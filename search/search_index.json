{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"spotpython","text":""},{"location":"#surrogate-model-based-optimization-and-hyperparameter-tuning-in-python","title":"Surrogate Model Based Optimization and Hyperparameter Tuning in Python","text":"<ul> <li>Documentation for spotpython see Hyperparameter Tuning Cookbook, a guide for scikit-learn, PyTorch, river, and spotpython.</li> <li>News and updates related to spotpython see SPOTSeven</li> </ul>"},{"location":"about/","title":"Contact/Privacy Policy","text":""},{"location":"about/#address","title":"Address","text":"<p>Prof. Dr. Thomas Bartz-Beielstein TH K\u00f6ln Raum 1.519 Steinm\u00fcllerallee 6 51643 Gummersbach +49 (0)2261 8196 6391 thomas.bartz-beielstein [at] th-koeln.de www.spotseven.de</p>"},{"location":"about/#privacy-policy","title":"Privacy Policy","text":"<p>We are very delighted that you have shown interest in our enterprise. Data protection is of a particularly high priority for the management of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. The use of the Internet pages of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is possible without any indication of personal data; however, if a data subject wants to use special enterprise services via our website, processing of personal data could become necessary. If the processing of personal data is necessary and there is no statutory basis for such processing, we generally obtain consent from the data subject.</p> <p>The processing of personal data, such as the name, address, e-mail address, or telephone number of a data subject shall always be in line with the General Data Protection Regulation (GDPR), and in accordance with the country-specific data protection regulations applicable to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. By means of this data protection declaration, our enterprise would like to inform the general public of the nature, scope, and purpose of the personal data we collect, use and process. Furthermore, data subjects are informed, by means of this data protection declaration, of the rights to which they are entitled.</p> <p>As the controller, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab has implemented numerous technical and organizational measures to ensure the most complete protection of personal data processed through this website. However, Internet-based data transmissions may in principle have security gaps, so absolute protection may not be guaranteed. For this reason, every data subject is free to transfer personal data to us via alternative means, e.g. by telephone.</p> <ol> <li>Definitions</li> </ol> <p>The data protection declaration of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is based on the terms used by the European legislator for the adoption of the General Data Protection Regulation (GDPR). Our data protection declaration should be legible and understandable for the general public, as well as our customers and business partners. To ensure this, we would like to first explain the terminology used.</p> <p>In this data protection declaration, we use, inter alia, the following terms:</p> <p>a)    Personal data</p> <p>Personal data means any information relating to an identified or identifiable natural person (\u201cdata subject\u201d). An identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.</p> <p>b) Data subject</p> <p>Data subject is any identified or identifiable natural person, whose personal data is processed by the controller responsible for the processing.</p> <p>c)    Processing</p> <p>Processing is any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.</p> <p>d)    Restriction of processing</p> <p>Restriction of processing is the marking of stored personal data with the aim of limiting their processing in the future.</p> <p>e)    Profiling</p> <p>Profiling means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person\u2019s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements.</p> <p>f)     Pseudonymisation</p> <p>Pseudonymisation is the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person.</p> <p>g)    Controller or controller responsible for the processing</p> <p>Controller or controller responsible for the processing is the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law.</p> <p>h)    Processor</p> <p>Processor is a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller.</p> <p>i)      Recipient</p> <p>Recipient is a natural or legal person, public authority, agency or another body, to which the personal data are disclosed, whether a third party or not. However, public authorities which may receive personal data in the framework of a particular inquiry in accordance with Union or Member State law shall not be regarded as recipients; the processing of those data by those public authorities shall be in compliance with the applicable data protection rules according to the purposes of the processing.</p> <p>j)      Third party</p> <p>Third party is a natural or legal person, public authority, agency or body other than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data.</p> <p>k)    Consent</p> <p>Consent of the data subject is any freely given, specific, informed and unambiguous indication of the data subject\u2019s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.</p> <ol> <li>Name and Address of the controller</li> </ol> <p>Controller for the purposes of the General Data Protection Regulation (GDPR), other data protection laws applicable in Member states of the European Union and other provisions related to data protection is:</p> <p>TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab</p> <p>Steinm\u00fcllerallee 1</p> <p>51643 Gummersbach</p> <p>Deutschland</p> <p>Phone: +49 2261 81966391</p> <p>Email: thomas.bartz-beielstein@th-koeln.de</p> <p>Website: www.spotseven.de</p> <ol> <li>Collection of general data and information</li> </ol> <p>The website of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab collects a series of general data and information when a data subject or automated system calls up the website. This general data and information are stored in the server log files. Collected may be (1) the browser types and versions used, (2) the operating system used by the accessing system, (3) the website from which an accessing system reaches our website (so-called referrers), (4) the sub-websites, (5) the date and time of access to the Internet site, (6) an Internet protocol address (IP address), (7) the Internet service provider of the accessing system, and (8) any other similar data and information that may be used in the event of attacks on our information technology systems.</p> <p>When using these general data and information, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab does not draw any conclusions about the data subject. Rather, this information is needed to (1) deliver the content of our website correctly, (2) optimize the content of our website as well as its advertisement, (3) ensure the long-term viability of our information technology systems and website technology, and (4) provide law enforcement authorities with the information necessary for criminal prosecution in case of a cyber-attack. Therefore, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab analyzes anonymously collected data and information statistically, with the aim of increasing the data protection and data security of our enterprise, and to ensure an optimal level of protection for the personal data we process. The anonymous data of the server log files are stored separately from all personal data provided by a data subject.</p> <ol> <li>Comments function in the blog on the website</li> </ol> <p>The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab offers users the possibility to leave individual comments on individual blog contributions on a blog, which is on the website of the controller. A blog is a web-based, publicly-accessible portal, through which one or more people called bloggers or web-bloggers may post articles or write down thoughts in so-called blogposts. Blogposts may usually be commented by third parties.</p> <p>If a data subject leaves a comment on the blog published on this website, the comments made by the data subject are also stored and published, as well as information on the date of the commentary and on the user\u2019s (pseudonym) chosen by the data subject. In addition, the IP address assigned by the Internet service provider (ISP) to the data subject is also logged. This storage of the IP address takes place for security reasons, and in case the data subject violates the rights of third parties, or posts illegal content through a given comment. The storage of these personal data is, therefore, in the own interest of the data controller, so that he can exculpate in the event of an infringement. This collected personal data will not be passed to third parties, unless such a transfer is required by law or serves the aim of the defense of the data controller.</p> <ol> <li>Routine erasure and blocking of personal data</li> </ol> <p>The data controller shall process and store the personal data of the data subject only for the period necessary to achieve the purpose of storage, or as far as this is granted by the European legislator or other legislators in laws or regulations to which the controller is subject to.</p> <p>If the storage purpose is not applicable, or if a storage period prescribed by the European legislator or another competent legislator expires, the personal data are routinely blocked or erased in accordance with legal requirements.</p> <ol> <li>Rights of the data subject</li> </ol> <p>a) Right of confirmation</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller the confirmation as to whether or not personal data concerning him or her are being processed. If a data subject wishes to avail himself of this right of confirmation, he or she may, at any time, contact our Data Protection Officer or another employee of the controller.</p> <p>b) Right of access</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller free information about his or her personal data stored at any time and a copy of this information. Furthermore, the European directives and regulations grant the data subject access to the following information:</p> <p>the purposes of the processing; the categories of personal data concerned; the recipients or categories of recipients to whom the personal data have been or will be disclosed, in particular recipients in third countries or international organisations; where possible, the envisaged period for which the personal data will be stored, or, if not possible, the criteria used to determine that period; the existence of the right to request from the controller rectification or erasure of personal data, or restriction of processing of personal data concerning the data subject, or to object to such processing; the existence of the right to lodge a complaint with a supervisory authority; where the personal data are not collected from the data subject, any available information as to their source; the existence of automated decision-making, including profiling, referred to in Article 22(1) and (4) of the GDPR and, at least in those cases, meaningful information about the logic involved, as well as the significance and envisaged consequences of such processing for the data subject. Furthermore, the data subject shall have a right to obtain information as to whether personal data are transferred to a third country or to an international organisation. Where this is the case, the data subject shall have the right to be informed of the appropriate safeguards relating to the transfer.</p> <p>If a data subject wishes to avail himself of this right of access, he or she may at any time contact our Data Protection Officer or another employee of the controller.</p> <p>c) Right to rectification</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller without undue delay the rectification of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement.</p> <p>If a data subject wishes to exercise this right to rectification, he or she may, at any time, contact our Data Protection Officer or another employee of the controller.</p> <p>d) Right to erasure (Right to be forgotten)</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies, as long as the processing is not necessary:</p> <p>The personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed. The data subject withdraws consent to which the processing is based according to point (a) of Article 6(1) of the GDPR, or point (a) of Article 9(2) of the GDPR, and where there is no other legal ground for the processing. The data subject objects to the processing pursuant to Article 21(1) of the GDPR and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2) of the GDPR. The personal data have been unlawfully processed. The personal data must be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject. The personal data have been collected in relation to the offer of information society services referred to in Article 8(1) of the GDPR. If one of the aforementioned reasons applies, and a data subject wishes to request the erasure of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee shall promptly ensure that the erasure request is complied with immediately.</p> <p>Where the controller has made personal data public and is obliged pursuant to Article 17(1) to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform other controllers processing the personal data that the data subject has requested erasure by such controllers of any links to, or copy or replication of, those personal data, as far as processing is not required. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the necessary measures in individual cases.</p> <p>e) Right of restriction of processing</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller restriction of processing where one of the following applies:</p> <p>The accuracy of the personal data is contested by the data subject, for a period enabling the controller to verify the accuracy of the personal data. The processing is unlawful and the data subject opposes the erasure of the personal data and requests instead the restriction of their use instead. The controller no longer needs the personal data for the purposes of the processing, but they are required by the data subject for the establishment, exercise or defence of legal claims. The data subject has objected to processing pursuant to Article 21(1) of the GDPR pending the verification whether the legitimate grounds of the controller override those of the data subject. If one of the aforementioned conditions is met, and a data subject wishes to request the restriction of the processing of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the restriction of the processing.</p> <p>f) Right to data portability</p> <p>Each data subject shall have the right granted by the European legislator, to receive the personal data concerning him or her, which was provided to a controller, in a structured, commonly used and machine-readable format. He or she shall have the right to transmit those data to another controller without hindrance from the controller to which the personal data have been provided, as long as the processing is based on consent pursuant to point (a) of Article 6(1) of the GDPR or point (a) of Article 9(2) of the GDPR, or on a contract pursuant to point (b) of Article 6(1) of the GDPR, and the processing is carried out by automated means, as long as the processing is not necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller.</p> <p>Furthermore, in exercising his or her right to data portability pursuant to Article 20(1) of the GDPR, the data subject shall have the right to have personal data transmitted directly from one controller to another, where technically feasible and when doing so does not adversely affect the rights and freedoms of others.</p> <p>In order to assert the right to data portability, the data subject may at any time contact the Data Protection Officer designated by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee.</p> <p>g) Right to object</p> <p>Each data subject shall have the right granted by the European legislator to object, on grounds relating to his or her particular situation, at any time, to processing of personal data concerning him or her, which is based on point (e) or (f) of Article 6(1) of the GDPR. This also applies to profiling based on these provisions.</p> <p>The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall no longer process the personal data in the event of the objection, unless we can demonstrate compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject, or for the establishment, exercise or defence of legal claims.</p> <p>If the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab processes personal data for direct marketing purposes, the data subject shall have the right to object at any time to processing of personal data concerning him or her for such marketing. This applies to profiling to the extent that it is related to such direct marketing. If the data subject objects to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab to the processing for direct marketing purposes, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab will no longer process the personal data for these purposes.</p> <p>In addition, the data subject has the right, on grounds relating to his or her particular situation, to object to processing of personal data concerning him or her by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab for scientific or historical research purposes, or for statistical purposes pursuant to Article 89(1) of the GDPR, unless the processing is necessary for the performance of a task carried out for reasons of public interest.</p> <p>In order to exercise the right to object, the data subject may directly contact the Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee. In addition, the data subject is free in the context of the use of information society services, and notwithstanding Directive 2002/58/EC, to use his or her right to object by automated means using technical specifications.</p> <p>h) Automated individual decision-making, including profiling</p> <p>Each data subject shall have the right granted by the European legislator not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her, or similarly significantly affects him or her, as long as the decision (1) is not is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) is not authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, or (3) is not based on the data subject\u2019s explicit consent.</p> <p>If the decision (1) is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) it is based on the data subject\u2019s explicit consent, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall implement suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and contest the decision.</p> <p>If the data subject wishes to exercise the rights concerning automated individual decision-making, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller.</p> <p>i) Right to withdraw data protection consent</p> <p>Each data subject shall have the right granted by the European legislator to withdraw his or her consent to processing of his or her personal data at any time.</p> <p>f the data subject wishes to exercise the right to withdraw the consent, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller.</p> <ol> <li>Data protection provisions about the application and use of Facebook</li> </ol> <p>On this website, the controller has integrated components of the enterprise Facebook. Facebook is a social network.</p> <p>A social network is a place for social meetings on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Facebook allows social network users to include the creation of private profiles, upload photos, and network through friend requests.</p> <p>The operating company of Facebook is Facebook, Inc., 1 Hacker Way, Menlo Park, CA 94025, United States. If a person lives outside of the United States or Canada, the controller is the Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Ireland.</p> <p>With each call-up to one of the individual pages of this Internet website, which is operated by the controller and into which a Facebook component (Facebook plug-ins) was integrated, the web browser on the information technology system of the data subject is automatically prompted to download display of the corresponding Facebook component from Facebook through the Facebook component. An overview of all the Facebook Plug-ins may be accessed under https://developers.facebook.com/docs/plugins/. During the course of this technical procedure, Facebook is made aware of what specific sub-site of our website was visited by the data subject.</p> <p>If the data subject is logged in at the same time on Facebook, Facebook detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-site of our Internet page was visited by the data subject. This information is collected through the Facebook component and associated with the respective Facebook account of the data subject. If the data subject clicks on one of the Facebook buttons integrated into our website, e.g. the \u201cLike\u201d button, or if the data subject submits a comment, then Facebook matches this information with the personal Facebook user account of the data subject and stores the personal data.</p> <p>Facebook always receives, through the Facebook component, information about a visit to our website by the data subject, whenever the data subject is logged in at the same time on Facebook during the time of the call-up to our website. This occurs regardless of whether the data subject clicks on the Facebook component or not. If such a transmission of information to Facebook is not desirable for the data subject, then he or she may prevent this by logging off from their Facebook account before a call-up to our website is made.</p> <p>The data protection guideline published by Facebook, which is available at https://facebook.com/about/privacy/, provides information about the collection, processing and use of personal data by Facebook. In addition, it is explained there what setting options Facebook offers to protect the privacy of the data subject. In addition, different configuration options are made available to allow the elimination of data transmission to Facebook, e.g. the Facebook blocker of the provider Webgraph, which may be obtained under http://webgraph.com/resources/facebookblocker/. These applications may be used by the data subject to eliminate a data transmission to Facebook.</p> <ol> <li>Data protection provisions about the application and use of Google+</li> </ol> <p>On this website, the controller has integrated the Google+ button as a component. Google+ is a so-called social network. A social network is a social meeting place on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Google+ allows users of the social network to include the creation of private profiles, upload photos and network through friend requests.</p> <p>The operating company of Google+ is Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this website, which is operated by the controller and on which a Google+ button has been integrated, the Internet browser on the information technology system of the data subject automatically downloads a display of the corresponding Google+ button of Google through the respective Google+ button component. During the course of this technical procedure, Google is made aware of what specific sub-page of our website was visited by the data subject. More detailed information about Google+ is available under https://developers.google.com/+/.</p> <p>If the data subject is logged in at the same time to Google+, Google recognizes with each call-up to our website by the data subject and for the entire duration of his or her stay on our Internet site, which specific sub-pages of our Internet page were visited by the data subject. This information is collected through the Google+ button and Google matches this with the respective Google+ account associated with the data subject.</p> <p>If the data subject clicks on the Google+ button integrated on our website and thus gives a Google+ 1 recommendation, then Google assigns this information to the personal Google+ user account of the data subject and stores the personal data. Google stores the Google+ 1 recommendation of the data subject, making it publicly available in accordance with the terms and conditions accepted by the data subject in this regard. Subsequently, a Google+ 1 recommendation given by the data subject on this website together with other personal data, such as the Google+ account name used by the data subject and the stored photo, is stored and processed on other Google services, such as search-engine results of the Google search engine, the Google account of the data subject or in other places, e.g. on Internet pages, or in relation to advertisements. Google is also able to link the visit to this website with other personal data stored on Google. Google further records this personal information with the purpose of improving or optimizing the various Google services.</p> <p>Through the Google+ button, Google receives information that the data subject visited our website, if the data subject at the time of the call-up to our website is logged in to Google+. This occurs regardless of whether the data subject clicks or doesn\u2019t click on the Google+ button.</p> <p>If the data subject does not wish to transmit personal data to Google, he or she may prevent such transmission by logging out of his Google+ account before calling up our website.</p> <p>Further information and the data protection provisions of Google may be retrieved under https://www.google.com/intl/en/policies/privacy/. More references from Google about the Google+ 1 button may be obtained under https://developers.google.com/+/web/buttons-policy.</p> <ol> <li>Data protection provisions about the application and use of Jetpack for WordPress</li> </ol> <p>On this website, the controller has integrated Jetpack. Jetpack is a WordPress plug-in, which provides additional features to the operator of a website based on WordPress. Jetpack allows the Internet site operator, inter alia, an overview of the visitors of the site. By displaying related posts and publications, or the ability to share content on the page, it is also possible to increase visitor numbers. In addition, security features are integrated into Jetpack, so a Jetpack-using site is better protected against brute-force attacks. Jetpack also optimizes and accelerates the loading of images on the website.</p> <p>The operating company of Jetpack Plug-Ins for WordPress is the Automattic Inc., 132 Hawthorne Street, San Francisco, CA 94107, UNITED STATES. The operating enterprise uses the tracking technology created by Quantcast Inc., 201 Third Street, San Francisco, CA 94103, UNITED STATES.</p> <p>Jetpack sets a cookie on the information technology system used by the data subject. The definition of cookies is explained above. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Jetpack component was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to submit data through the Jetpack component for analysis purposes to Automattic. During the course of this technical procedure Automattic receives data that is used to create an overview of website visits. The data obtained in this way serves the analysis of the behaviour of the data subject, which has access to the Internet page of the controller and is analyzed with the aim to optimize the website. The data collected through the Jetpack component is not used to identify the data subject without a prior obtaining of a separate express consent of the data subject. The data comes also to the notice of Quantcast. Quantcast uses the data for the same purposes as Automattic.</p> <p>The data subject can, as stated above, prevent the setting of cookies through our website at any time by means of a corresponding adjustment of the web browser used and thus permanently deny the setting of cookies. Such an adjustment to the Internet browser used would also prevent Automattic/Quantcast from setting a cookie on the information technology system of the data subject. In addition, cookies already in use by Automattic/Quantcast may be deleted at any time via a web browser or other software programs.</p> <p>In addition, the data subject has the possibility of objecting to a collection of data relating to a use of this Internet site that are generated by the Jetpack cookie as well as the processing of these data by Automattic/Quantcast and the chance to preclude any such. For this purpose, the data subject must press the \u2018opt-out\u2019 button under the link https://www.quantcast.com/opt-out/ which sets an opt-out cookie. The opt-out cookie set with this purpose is placed on the information technology system used by the data subject. If the cookies are deleted on the system of the data subject, then the data subject must call up the link again and set a new opt-out cookie.</p> <p>With the setting of the opt-out cookie, however, the possibility exists that the websites of the controller are not fully usable anymore by the data subject.</p> <p>The applicable data protection provisions of Automattic may be accessed under https://automattic.com/privacy/. The applicable data protection provisions of Quantcast can be accessed under https://www.quantcast.com/privacy/.</p> <ol> <li>Data protection provisions about the application and use of LinkedIn</li> </ol> <p>The controller has integrated components of the LinkedIn Corporation on this website. LinkedIn is a web-based social network that enables users with existing business contacts to connect and to make new business contacts. Over 400 million registered people in more than 200 countries use LinkedIn. Thus, LinkedIn is currently the largest platform for business contacts and one of the most visited websites in the world.</p> <p>The operating company of LinkedIn is LinkedIn Corporation, 2029 Stierlin Court Mountain View, CA 94043, UNITED STATES. For privacy matters outside of the UNITED STATES LinkedIn Ireland, Privacy Policy Issues, Wilton Plaza, Wilton Place, Dublin 2, Ireland, is responsible.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a LinkedIn component (LinkedIn plug-in) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to the download of a display of the corresponding LinkedIn component of LinkedIn. Further information about the LinkedIn plug-in may be accessed under https://developer.linkedin.com/plugins. During the course of this technical procedure, LinkedIn gains knowledge of what specific sub-page of our website was visited by the data subject.</p> <p>If the data subject is logged in at the same time on LinkedIn, LinkedIn detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-page of our Internet page was visited by the data subject. This information is collected through the LinkedIn component and associated with the respective LinkedIn account of the data subject. If the data subject clicks on one of the LinkedIn buttons integrated on our website, then LinkedIn assigns this information to the personal LinkedIn user account of the data subject and stores the personal data.</p> <p>LinkedIn receives information via the LinkedIn component that the data subject has visited our website, provided that the data subject is logged in at LinkedIn at the time of the call-up to our website. This occurs regardless of whether the person clicks on the LinkedIn button or not. If such a transmission of information to LinkedIn is not desirable for the data subject, then he or she may prevent this by logging off from their LinkedIn account before a call-up to our website is made.</p> <p>LinkedIn provides under https://www.linkedin.com/psettings/guest-controls the possibility to unsubscribe from e-mail messages, SMS messages and targeted ads, as well as the ability to manage ad settings. LinkedIn also uses affiliates such as Eire, Google Analytics, BlueKai, DoubleClick, Nielsen, Comscore, Eloqua, and Lotame. The setting of such cookies may be denied under https://www.linkedin.com/legal/cookie-policy. The applicable privacy policy for LinkedIn is available under https://www.linkedin.com/legal/privacy-policy. The LinkedIn Cookie Policy is available under https://www.linkedin.com/legal/cookie-policy.</p> <ol> <li>Data protection provisions about the application and use of Twitter</li> </ol> <p>On this website, the controller has integrated components of Twitter. Twitter is a multilingual, publicly-accessible microblogging service on which users may publish and spread so-called \u2018tweets,\u2019 e.g. short messages, which are limited to 140 characters. These short messages are available for everyone, including those who are not logged on to Twitter. The tweets are also displayed to so-called followers of the respective user. Followers are other Twitter users who follow a user\u2019s tweets. Furthermore, Twitter allows you to address a wide audience via hashtags, links or retweets.</p> <p>The operating company of Twitter is Twitter, Inc., 1355 Market Street, Suite 900, San Francisco, CA 94103, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Twitter component (Twitter button) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding Twitter component of Twitter. Further information about the Twitter buttons is available under https://about.twitter.com/de/resources/buttons. During the course of this technical procedure, Twitter gains knowledge of what specific sub-page of our website was visited by the data subject. The purpose of the integration of the Twitter component is a retransmission of the contents of this website to allow our users to introduce this web page to the digital world and increase our visitor numbers.</p> <p>If the data subject is logged in at the same time on Twitter, Twitter detects with every call-up to our website by the data subject and for the entire duration of their stay on our Internet site which specific sub-page of our Internet page was visited by the data subject. This information is collected through the Twitter component and associated with the respective Twitter account of the data subject. If the data subject clicks on one of the Twitter buttons integrated on our website, then Twitter assigns this information to the personal Twitter user account of the data subject and stores the personal data.</p> <p>Twitter receives information via the Twitter component that the data subject has visited our website, provided that the data subject is logged in on Twitter at the time of the call-up to our website. This occurs regardless of whether the person clicks on the Twitter component or not. If such a transmission of information to Twitter is not desirable for the data subject, then he or she may prevent this by logging off from their Twitter account before a call-up to our website is made.</p> <p>The applicable data protection provisions of Twitter may be accessed under https://twitter.com/privacy?lang=en.</p> <ol> <li>Data protection provisions about the application and use of YouTube</li> </ol> <p>On this website, the controller has integrated components of YouTube. YouTube is an Internet video portal that enables video publishers to set video clips and other users free of charge, which also provides free viewing, review and commenting on them. YouTube allows you to publish all kinds of videos, so you can access both full movies and TV broadcasts, as well as music videos, trailers, and videos made by users via the Internet portal.</p> <p>The operating company of YouTube is YouTube, LLC, 901 Cherry Ave., San Bruno, CA 94066, UNITED STATES. The YouTube, LLC is a subsidiary of Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a YouTube component (YouTube video) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding YouTube component. Further information about YouTube may be obtained under https://www.youtube.com/yt/about/en/. During the course of this technical procedure, YouTube and Google gain knowledge of what specific sub-page of our website was visited by the data subject.</p> <p>If the data subject is logged in on YouTube, YouTube recognizes with each call-up to a sub-page that contains a YouTube video, which specific sub-page of our Internet site was visited by the data subject. This information is collected by YouTube and Google and assigned to the respective YouTube account of the data subject.</p> <p>YouTube and Google will receive information through the YouTube component that the data subject has visited our website, if the data subject at the time of the call to our website is logged in on YouTube; this occurs regardless of whether the person clicks on a YouTube video or not. If such a transmission of this information to YouTube and Google is not desirable for the data subject, the delivery may be prevented if the data subject logs off from their own YouTube account before a call-up to our website is made.</p> <p>YouTube\u2019s data protection provisions, available at https://www.google.com/intl/en/policies/privacy/, provide information about the collection, processing and use of personal data by YouTube and Google.</p> <ol> <li>Legal basis for the processing</li> </ol> <p>Art. 6(1) lit. a GDPR serves as the legal basis for processing operations for which we obtain consent for a specific processing purpose. If the processing of personal data is necessary for the performance of a contract to which the data subject is party, as is the case, for example, when processing operations are necessary for the supply of goods or to provide any other service, the processing is based on Article 6(1) lit. b GDPR. The same applies to such processing operations which are necessary for carrying out pre-contractual measures, for example in the case of inquiries concerning our products or services. Is our company subject to a legal obligation by which processing of personal data is required, such as for the fulfillment of tax obligations, the processing is based on Art. 6(1) lit. c GDPR. In rare cases, the processing of personal data may be necessary to protect the vital interests of the data subject or of another natural person. This would be the case, for example, if a visitor were injured in our company and his name, age, health insurance data or other vital information would have to be passed on to a doctor, hospital or other third party. Then the processing would be based on Art. 6(1) lit. d GDPR. Finally, processing operations could be based on Article 6(1) lit. f GDPR. This legal basis is used for processing operations which are not covered by any of the abovementioned legal grounds, if processing is necessary for the purposes of the legitimate interests pursued by our company or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data. Such processing operations are particularly permissible because they have been specifically mentioned by the European legislator. He considered that a legitimate interest could be assumed if the data subject is a client of the controller (Recital 47 Sentence 2 GDPR).</p> <ol> <li>The legitimate interests pursued by the controller or by a third party</li> </ol> <p>Where the processing of personal data is based on Article 6(1) lit. f GDPR our legitimate interest is to carry out our business in favor of the well-being of all our employees and the shareholders.</p> <ol> <li>Period for which the personal data will be stored</li> </ol> <p>The criteria used to determine the period of storage of personal data is the respective statutory retention period. After expiration of that period, the corresponding data is routinely deleted, as long as it is no longer necessary for the fulfillment of the contract or the initiation of a contract.</p> <ol> <li>Provision of personal data as statutory or contractual requirement; Requirement necessary to enter into a contract; Obligation of the data subject to provide the personal data; possible consequences of failure to provide such data</li> </ol> <p>We clarify that the provision of personal data is partly required by law (e.g. tax regulations) or can also result from contractual provisions (e.g. information on the contractual partner). Sometimes it may be necessary to conclude a contract that the data subject provides us with personal data, which must subsequently be processed by us. The data subject is, for example, obliged to provide us with personal data when our company signs a contract with him or her. The non-provision of the personal data would have the consequence that the contract with the data subject could not be concluded. Before personal data is provided by the data subject, the data subject must contact our Data Protection Officer. Our Data Protection Officer clarifies to the data subject whether the provision of the personal data is required by law or contract or is necessary for the conclusion of the contract, whether there is an obligation to provide the personal data and the consequences of non-provision of the personal data.</p> <ol> <li>Existence of automated decision-making</li> </ol> <p>As a responsible company, we do not use automatic decision-making or profiling.</p> <p>This Privacy Policy has been generated by the Privacy Policy Generator of the External Data Protection Officers that was developed in cooperation with RC GmbH, which sells used notebooks and the Media Law Lawyers from WBS-LAW.</p>"},{"location":"download/","title":"Install spotpython","text":"<pre><code>pip install spotpython\n</code></pre>"},{"location":"examples/","title":"SPOT Examples","text":""},{"location":"examples/#simple-spotpython-run","title":"Simple spotpython run","text":"<pre><code>import numpy as np\nfrom spotpython.spot import spot\nfrom spotpython.fun.objectivefunctions import analytical\nfrom spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n\nfun = analytical().fun_branin\nfun_control = fun_control_init(lower = np.array([-5, 0]),\n                               upper = np.array([10, 15]),\n                               fun_evals=20)\ndesign_control = design_control_init(init_size=10)\nsurrogate_control = surrogate_control_init(n_theta=2)\nS = spot.Spot(fun=fun, fun_control=fun_control, design_control=design_control)\nS.run()\n</code></pre> <pre><code>spotpython tuning: 3.146824136952164 [######----] 55.00% \nspotpython tuning: 3.146824136952164 [######----] 60.00% \nspotpython tuning: 3.146824136952164 [######----] 65.00% \nspotpython tuning: 3.146824136952164 [#######---] 70.00% \nspotpython tuning: 1.1487233101571483 [########--] 75.00% \nspotpython tuning: 1.0236891516766402 [########--] 80.00% \nspotpython tuning: 0.41994270072214057 [########--] 85.00% \nspotpython tuning: 0.40193544341108023 [#########-] 90.00% \nspotpython tuning: 0.3991519598268951 [##########] 95.00% \nspotpython tuning: 0.3991519598268951 [##########] 100.00% Done...\n</code></pre> <pre><code>S.print_results()\n</code></pre> <pre><code>min y: 0.3991519598268951\nx0: 3.1546575195040987\nx1: 2.285931113926263\n</code></pre> <pre><code>S.plot_progress(log_y=True)\n</code></pre> <pre><code>S.surrogate.plot()\n</code></pre>"},{"location":"examples/#further-examples","title":"Further Examples","text":"<p>Examples can be found in the Hyperparameter Tuning Cookbook, e.g., Documentation of the Sequential Parameter Optimization.</p>"},{"location":"hyperparameter-tuning-cookbook/","title":"Hyperparameter Tuning Cookbook","text":"<p>The following is a cookbook of hyperparameter tuning recipes. It is not meant to be exhaustive, but instead act as a place to capture a number of the common patterns used in hyperparameter tuning.</p> <p>Hyperparameter Tuning Cookbook</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>spotpython<ul> <li>budget<ul> <li>ocba</li> </ul> </li> <li>build<ul> <li>kriging</li> <li>surrogates</li> </ul> </li> <li>data<ul> <li>base</li> <li>california</li> <li>california_housing</li> <li>csvdataset</li> <li>diabetes</li> <li>friedman</li> <li>lightcrossvalidationdatamodule</li> <li>lightdatamodule</li> <li>mnistdatamodule</li> <li>pkldataset</li> <li>torchdata</li> <li>vbdp</li> </ul> </li> <li>design<ul> <li>designs</li> <li>factorial</li> <li>spacefilling</li> </ul> </li> <li>fun<ul> <li>hyperlight</li> <li>hypersklearn</li> <li>hypertorch</li> <li>objectivefunctions</li> </ul> </li> <li>hyperdict<ul> <li>light_hyper_dict</li> <li>sklearn_hyper_dict</li> <li>torch_hyper_dict</li> </ul> </li> <li>hyperparameters<ul> <li>architecture</li> <li>categorical</li> <li>optimizer</li> <li>update</li> <li>values</li> </ul> </li> <li>light<ul> <li>cifar10<ul> <li>cifar10datamodule</li> </ul> </li> <li>classification<ul> <li>netlightbasemapk</li> </ul> </li> <li>cnn<ul> <li>googlenet</li> <li>inceptionblock</li> <li>netcnnbase</li> </ul> </li> <li>cvmodel</li> <li>litmodel</li> <li>loadmodel</li> <li>predictmodel</li> <li>regression<ul> <li>netlightregression</li> <li>nn_condnet_regressor</li> <li>nn_linear_regressor</li> <li>nn_resnet_regressor</li> <li>nn_transformer_regressor</li> <li>pos_enc</li> <li>rnnlightregression</li> <li>transformerlightregression</li> </ul> </li> <li>testmodel</li> <li>trainmodel</li> <li>transformer<ul> <li>attention</li> <li>encoder</li> <li>encoderblock</li> <li>multiheadattention</li> <li>positionalEncoding</li> <li>positionalEncodingBasic</li> <li>skiplinear</li> <li>transformerlightpredictor</li> </ul> </li> </ul> </li> <li>plot<ul> <li>contour</li> <li>ts</li> <li>validation</li> <li>xai</li> <li>xy</li> </ul> </li> <li>sklearn<ul> <li>traintest</li> </ul> </li> <li>spot<ul> <li>spot</li> </ul> </li> <li>torch<ul> <li>activation</li> <li>cosinewarmupcheduler</li> <li>dataframedataset</li> <li>dimensions</li> <li>mapk</li> <li>netcifar10</li> <li>netcore</li> <li>netfashionMNIST</li> <li>netregression</li> <li>netvbdp</li> <li>traintest</li> </ul> </li> <li>utils<ul> <li>aggregate</li> <li>classes</li> <li>compare</li> <li>convert</li> <li>device</li> <li>eda</li> <li>file</li> <li>init</li> <li>math</li> <li>metrics</li> <li>numpy2json</li> <li>progress</li> <li>repair</li> <li>scaler</li> <li>split</li> <li>tensorboard</li> <li>time</li> <li>transform</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/spotpython/budget/ocba/","title":"ocba","text":"<p>OCBA: Optimal Computing Budget Allocation</p>"},{"location":"reference/spotpython/budget/ocba/#spotpython.budget.ocba.get_ocba","title":"<code>get_ocba(means, vars, delta, verbose=False)</code>","text":"<p>Optimal Computer Budget Allocation (OCBA)</p> <p>This function calculates the budget recommendations for a given set of means, variances, and incremental budget using the OCBA algorithm.</p> References <p>[1]: Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization: An Optimal Computer Budget Allocation, pp. 49 and pp. 215 [2]: C.S.M Currie and T. Monks: How to choose the best setup for a system. A tutorial for the Simulation Workshop 2021, see: https://colab.research.google.com/github/TomMonks/sim-tools/blob/master/examples/sw21_tutorial.ipynb and https://github.com/TomMonks/sim-tools</p> <p>Parameters:</p> Name Type Description Default <code>means</code> <code>array</code> <p>An array of means.</p> required <code>vars</code> <code>array</code> <p>An array of variances.</p> required <code>delta</code> <code>int</code> <p>The incremental budget.</p> required <code>verbose</code> <code>bool</code> <p>If True, print the results.</p> <code>False</code> <p>Returns:</p> Type Description <code>array</code> <p>An array of budget recommendations.</p> Note <p>The implementation is based on the pseudo-code in the Chen et al. (p. 49), see [1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import copy\n    import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.budget.ocba import get_ocba\n    # Example is based on the example from the book:\n    # Chun-Hung Chen and Loo Hay Lee:\n    #     Stochastic Simulation Optimization: An Optimal Computer Budget Allocation,\n    #     pp. 49 and pp. 215\n    #     p. 49:\n    #     mean_y = np.array([1,2,3,4,5])\n    #     var_y = np.array([1,1,9,9,4])\n    #     get_ocba(mean_y, var_y, 50)\n    #     [11  9 19  9  2]\n    fun = analytical().fun_linear\n    fun_control = {\"sigma\": 0.001,\n                \"seed\": 123}\n    spot_1_noisy = spot.Spot(fun=fun,\n                    lower = np.array([-1]),\n                    upper = np.array([1]),\n                    fun_evals = 20,\n                    fun_repeats = 2,\n                    noise = True,\n                    ocba_delta=1,\n                    seed=123,\n                    show_models=False,\n                    fun_control = fun_control,\n                    design_control={\"init_size\": 3,\n                                    \"repeats\": 2},\n                    surrogate_control={\"noise\": True})\n    spot_1_noisy.run()\n    spot_2 = copy.deepcopy(spot_1_noisy)\n    spot_2.mean_y = np.array([1,2,3,4,5])\n    spot_2.var_y = np.array([1,1,9,9,4])\n    n = 50\n    o = get_ocba(spot_2.mean_y, spot_2.var_y, n)\n    assert sum(o) == 50\n    assert (o == np.array([[11, 9, 19, 9, 2]])).all()\n    o\n    spotpython tuning: -1.000367786651468 [####------] 45.00%\n    spotpython tuning: -1.000989121350348 [######----] 60.00%\n    spotpython tuning: -1.000989121350348 [########--] 75.00%\n    spotpython tuning: -1.000989121350348 [#########-] 90.00%\n    spotpython tuning: -1.000989121350348 [##########] 100.00% Done...\n    array([11,  9, 19,  9,  2])\n</code></pre> Source code in <code>spotpython/budget/ocba.py</code> <pre><code>def get_ocba(means, vars, delta, verbose=False) -&gt; array:\n    \"\"\"\n    Optimal Computer Budget Allocation (OCBA)\n\n    This function calculates the budget recommendations for a given set of means,\n    variances, and incremental budget using the OCBA algorithm.\n\n    References:\n        [1]: Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization: An Optimal Computer Budget Allocation,\n        pp. 49 and pp. 215\n        [2]: C.S.M Currie and T. Monks: How to choose the best setup for a system.\n        A tutorial for the Simulation Workshop 2021, see:\n        https://colab.research.google.com/github/TomMonks/sim-tools/blob/master/examples/sw21_tutorial.ipynb\n        and\n        https://github.com/TomMonks/sim-tools\n\n    Args:\n        means (numpy.array):\n            An array of means.\n        vars (numpy.array):\n            An array of variances.\n        delta (int):\n            The incremental budget.\n        verbose (bool):\n            If True, print the results.\n\n    Returns:\n        (numpy.array): An array of budget recommendations.\n\n    Note:\n        The implementation is based on the pseudo-code in the Chen et al. (p. 49), see [1].\n\n    Examples:\n        &gt;&gt;&gt; import copy\n            import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n            from spotpython.budget.ocba import get_ocba\n            # Example is based on the example from the book:\n            # Chun-Hung Chen and Loo Hay Lee:\n            #     Stochastic Simulation Optimization: An Optimal Computer Budget Allocation,\n            #     pp. 49 and pp. 215\n            #     p. 49:\n            #     mean_y = np.array([1,2,3,4,5])\n            #     var_y = np.array([1,1,9,9,4])\n            #     get_ocba(mean_y, var_y, 50)\n            #     [11  9 19  9  2]\n            fun = analytical().fun_linear\n            fun_control = {\"sigma\": 0.001,\n                        \"seed\": 123}\n            spot_1_noisy = spot.Spot(fun=fun,\n                            lower = np.array([-1]),\n                            upper = np.array([1]),\n                            fun_evals = 20,\n                            fun_repeats = 2,\n                            noise = True,\n                            ocba_delta=1,\n                            seed=123,\n                            show_models=False,\n                            fun_control = fun_control,\n                            design_control={\"init_size\": 3,\n                                            \"repeats\": 2},\n                            surrogate_control={\"noise\": True})\n            spot_1_noisy.run()\n            spot_2 = copy.deepcopy(spot_1_noisy)\n            spot_2.mean_y = np.array([1,2,3,4,5])\n            spot_2.var_y = np.array([1,1,9,9,4])\n            n = 50\n            o = get_ocba(spot_2.mean_y, spot_2.var_y, n)\n            assert sum(o) == 50\n            assert (o == np.array([[11, 9, 19, 9, 2]])).all()\n            o\n            spotpython tuning: -1.000367786651468 [####------] 45.00%\n            spotpython tuning: -1.000989121350348 [######----] 60.00%\n            spotpython tuning: -1.000989121350348 [########--] 75.00%\n            spotpython tuning: -1.000989121350348 [#########-] 90.00%\n            spotpython tuning: -1.000989121350348 [##########] 100.00% Done...\n            array([11,  9, 19,  9,  2])\n    \"\"\"\n    if np.all(vars &gt; 0) and (means.shape[0] &gt; 2):\n        n_designs = means.shape[0]\n        allocations = zeros(n_designs, int32)\n        ratios = zeros(n_designs, float64)\n        budget = delta\n        ranks = get_ranks(means)\n        best, second_best = argpartition(ranks, 2)[:2]\n        ratios[second_best] = 1.0\n        select = [i for i in range(n_designs) if i not in [best, second_best]]\n        temp = (means[best] - means[second_best]) / (means[best] - means[select])\n        ratios[select] = square(temp) * (vars[select] / vars[second_best])\n        select = [i for i in range(n_designs) if i not in [best]]\n        temp = (square(ratios[select]) / vars[select]).sum()\n        ratios[best] = sqrt(vars[best] * temp)\n        more_runs = full(n_designs, True, dtype=bool)\n        add_budget = zeros(n_designs, dtype=float)\n        more_alloc = True\n        if verbose:\n            print(\"\\nIn get_ocba():\")\n            print(f\"means: {means}\")\n            print(f\"vars: {vars}\")\n            print(f\"delta: {delta}\")\n            print(f\"n_designs: {n_designs}\")\n            print(f\"Allocations: {allocations}\")\n            print(f\"Ratios: {ratios}\")\n            print(f\"Budget: {budget}\")\n            print(f\"Ranks: {ranks}\")\n            print(f\"Best: {best}\")\n            print(f\"Second best: {second_best}\")\n            print(f\"Select: {select}\")\n            print(f\"Temp: {temp}\")\n            print(f\"More runs: {more_runs}\")\n            print(f\"Add budget: {add_budget}\")\n            print(f\"More allocations: {more_alloc}\")\n        while more_alloc:\n            more_alloc = False\n            ratio_s = (more_runs * ratios).sum()\n            add_budget[more_runs] = (budget / ratio_s) * ratios[more_runs]\n            add_budget = around(add_budget).astype(int)\n            mask = add_budget &lt; allocations\n            add_budget[mask] = allocations[mask]\n            more_runs[mask] = 0\n            if verbose:\n                print(\"\\nIn more_alloc:\")\n                print(f\"ratio_s: {ratio_s}\")\n                print(f\"more_runs: {more_runs}\")\n                print(f\"add_budget: {add_budget}\")\n            if mask.sum() &gt; 0:\n                more_alloc = True\n            if more_alloc:\n                budget = allocations.sum() + delta\n                budget -= (add_budget * ~more_runs).sum()\n        t_budget = add_budget.sum()\n        add_budget[best] += allocations.sum() + delta - t_budget\n        return add_budget - allocations\n    else:\n        return None\n</code></pre>"},{"location":"reference/spotpython/budget/ocba/#spotpython.budget.ocba.get_ocba_X","title":"<code>get_ocba_X(X, means, vars, delta, verbose=False)</code>","text":"<p>This function calculates the OCBA allocation and repeats the input array X along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input array to be repeated.</p> required <code>means</code> <code>list</code> <p>List of means for each alternative.</p> required <code>vars</code> <code>list</code> <p>List of variances for each alternative.</p> required <code>delta</code> <code>float</code> <p>Indifference zone parameter.</p> required <code>verbose</code> <code>bool</code> <p>If True, print the results.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Repeated array of X along the specified axis based on the OCBA allocation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.budget.ocba import get_ocba_X\n    from spotpython.utils.aggregate import aggregate_mean_var\n    import numpy as np\n    X = np.array([[1,2,3],\n                [1,2,3],\n                [4,5,6],\n                [4,5,6],\n                [4,5,6],\n                [7,8,9],\n                [7,8,9],])\n    y = np.array([1,2,30,40, 40, 500, 600  ])\n    Z = aggregate_mean_var(X=X, y=y)\n    mean_X = Z[0]\n    mean_y = Z[1]\n    var_y = Z[2]\n    print(f\"X: {X}\")\n    print(f\"y: {y}\")\n    print(f\"mean_X: {mean_X}\")\n    print(f\"mean_y: {mean_y}\")\n    print(f\"var_y: {var_y}\")\n    delta = 5\n    X_new = get_ocba_X(X=mean_X, means=mean_y, vars=var_y, delta=delta,verbose=True)\n    X_new\n    array([[4., 5., 6.],\n           [4., 5., 6.],\n           [4., 5., 6.],\n           [7., 8., 9.],\n           [7., 8., 9.]])\n</code></pre> Source code in <code>spotpython/budget/ocba.py</code> <pre><code>def get_ocba_X(X, means, vars, delta, verbose=False) -&gt; float64:\n    \"\"\"\n    This function calculates the OCBA allocation and repeats the input array X along the specified axis.\n\n    Args:\n        X (numpy.ndarray): Input array to be repeated.\n        means (list): List of means for each alternative.\n        vars (list): List of variances for each alternative.\n        delta (float): Indifference zone parameter.\n        verbose (bool): If True, print the results.\n\n    Returns:\n        (numpy.ndarray): Repeated array of X along the specified axis based on the OCBA allocation.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.budget.ocba import get_ocba_X\n            from spotpython.utils.aggregate import aggregate_mean_var\n            import numpy as np\n            X = np.array([[1,2,3],\n                        [1,2,3],\n                        [4,5,6],\n                        [4,5,6],\n                        [4,5,6],\n                        [7,8,9],\n                        [7,8,9],])\n            y = np.array([1,2,30,40, 40, 500, 600  ])\n            Z = aggregate_mean_var(X=X, y=y)\n            mean_X = Z[0]\n            mean_y = Z[1]\n            var_y = Z[2]\n            print(f\"X: {X}\")\n            print(f\"y: {y}\")\n            print(f\"mean_X: {mean_X}\")\n            print(f\"mean_y: {mean_y}\")\n            print(f\"var_y: {var_y}\")\n            delta = 5\n            X_new = get_ocba_X(X=mean_X, means=mean_y, vars=var_y, delta=delta,verbose=True)\n            X_new\n            array([[4., 5., 6.],\n                   [4., 5., 6.],\n                   [4., 5., 6.],\n                   [7., 8., 9.],\n                   [7., 8., 9.]])\n\n    \"\"\"\n    if np.all(vars &gt; 0) and (means.shape[0] &gt; 2):\n        o = get_ocba(means=means, vars=vars, delta=delta, verbose=verbose)\n        return repeat(X, o, axis=0)\n    else:\n        return None\n</code></pre>"},{"location":"reference/spotpython/build/kriging/","title":"kriging","text":""},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging","title":"<code>Kriging</code>","text":"<p>               Bases: <code>surrogates</code></p> <p>Kriging surrogate.</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>class Kriging(surrogates):\n    \"\"\"Kriging surrogate.\n    \"\"\"\n    def __init__(\n            self: object,\n            noise: bool = False,\n            var_type: List[str] = [\"num\"],\n            name: str = \"kriging\",\n            seed: int = 124,\n            model_optimizer=None,\n            model_fun_evals: Optional[int] = None,\n            min_theta: float = -3.0,\n            max_theta: float = 2.0,\n            n_theta: int = 1,\n            theta_init_zero: bool = True,\n            p_val: float = 2.0,\n            n_p: int = 1,\n            optim_p: bool = False,\n            min_Lambda: float = 1e-9,\n            max_Lambda: float = 1.,\n            log_level: int = 50,\n            spot_writer=None,\n            counter=None,\n            metric_factorial=\"canberra\",\n            **kwargs\n    ):\n        \"\"\"\n        Initialize the Kriging surrogate.\n\n        Args:\n            noise (bool): Use regression instead of interpolation kriging. Defaults to False.\n            var_type (List[str]):\n                Variable type. Can be either \"num\" (numerical) or \"factor\" (factor).\n                Defaults to [\"num\"].\n            name (str):\n                Surrogate name. Defaults to \"kriging\".\n            seed (int):\n                Random seed. Defaults to 124.\n            model_optimizer (Optional[object]):\n                Optimizer on the surrogate. If None, differential_evolution is selected.\n            model_fun_evals (Optional[int]):\n                Number of iterations used by the optimizer on the surrogate.\n            min_theta (float):\n                Min log10 theta value. Defaults to -3.\n            max_theta (float):\n                Max log10 theta value. Defaults to 2.\n            n_theta (int):\n                Number of theta values. Defaults to 1.\n            theta_init_zero (bool):\n                Initialize theta with zero. Defaults to True.\n            p_val (float):\n                p value. Used as an initial value if optim_p = True. Otherwise as a constant. Defaults to 2.\n            n_p (int):\n                Number of p values. Defaults to 1.\n            optim_p (bool):\n                Determines whether p should be optimized. Deafults to False.\n            min_Lambda (float):\n                Min Lambda value. Defaults to 1e-9.\n            max_Lambda (float):\n                Max Lambda value. Defaults to 1.\n            log_level (int):\n                Logging level, e.g., 20 is \"INFO\". Defaults to 50 (\"CRITICAL\").\n            spot_writer (Optional[object]):\n                Spot writer. Defaults to None.\n            counter (Optional[int]):\n                Counter. Defaults to None.\n            metric_factorial (str):\n                Metric for factorial. Defaults to \"canberra\". Can be \"euclidean\",\n                \"cityblock\", seuclidean\", \"sqeuclidean\", \"cosine\",\n                \"correlation\", \"hamming\", \"jaccard\", \"jensenshannon\",\n                \"chebyshev\", \"canberra\", \"braycurtis\", \"mahalanobis\", \"matching\".\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                import matplotlib.pyplot as plt\n                from numpy import linspace, arange\n                rng = np.random.RandomState(1)\n                X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n                y = np.squeeze(X * np.sin(X))\n                training_indices = rng.choice(arange(y.size), size=6, replace=False)\n                X_train, y_train = X[training_indices], y[training_indices]\n                S = Kriging(name='kriging', seed=124)\n                S.fit(X_train, y_train)\n                mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n                plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n                plt.scatter(X_train, y_train, label=\"Observations\")\n                plt.plot(X, mean_prediction, label=\"Mean prediction\")\n                plt.fill_between(\n                    X.ravel(),\n                    mean_prediction - 1.96 * std_prediction,\n                    mean_prediction + 1.96 * std_prediction,\n                    alpha=0.5,\n                    label=r\"95% confidence interval\",\n                    )\n                plt.legend()\n                plt.xlabel(\"$x$\")\n                plt.ylabel(\"$f(x)$\")\n                _ = plt.title(\"Gaussian process regression on noise-free dataset\")\n                plt.show()\n\n        References:\n            https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html\n            [[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)]\n            scikit-learn: Gaussian Processes regression: basic introductory example\n\n        \"\"\"\n        super().__init__(name, seed, log_level)\n\n        self.noise = noise\n        self.var_type = var_type\n        self.name = name\n        self.seed = seed\n        self.log_level = log_level\n        self.spot_writer = spot_writer\n        self.counter = counter\n        self.metric_factorial = metric_factorial\n\n        self.sigma = 0\n        self.eps = sqrt(spacing(1))\n        self.min_theta = min_theta\n        self.max_theta = max_theta\n        self.min_p = 1\n        self.max_p = 2\n        self.min_Lambda = min_Lambda\n        self.max_Lambda = max_Lambda\n        self.n_theta = n_theta\n        self.p_val = p_val\n        self.n_p = n_p\n        self.optim_p = optim_p\n        self.theta_init_zero = theta_init_zero\n        # Psi matrix condition:\n        self.cnd_Psi = 0\n        self.inf_Psi = False\n\n        self.model_optimizer = model_optimizer\n        if self.model_optimizer is None:\n            self.model_optimizer = differential_evolution\n        self.model_fun_evals = model_fun_evals\n        # differential evolution uses maxiter = 1000\n        # and sets the number of function evaluations to\n        # (maxiter + 1) * popsize * N, which results in\n        # 1000 * 15 * k, because the default popsize is 15 and\n        # N is the number of parameters. This seems to be quite large:\n        # for k=2 these are 30 000 iterations. Therefore we set this value to\n        # 100\n        if self.model_fun_evals is None:\n            self.model_fun_evals = 100\n\n        # Logging information\n        self.log[\"negLnLike\"] = []\n        self.log[\"theta\"] = []\n        self.log[\"p\"] = []\n        self.log[\"Lambda\"] = []\n        # Logger\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def exp_imp(self, y0: float, s0: float) -&gt; float:\n        \"\"\"\n        Calculates the expected improvement for a given function value and error in coded units.\n\n        Args:\n            self (object): The Kriging object.\n            y0 (float): The function value in coded units.\n            s0 (float): The error value.\n\n        Returns:\n            float: The expected improvement value.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                S = Kriging(name='kriging', seed=124)\n                S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n                S.exp_imp(1.0, 0.0)\n                0.0\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                S = Kriging(name='kriging', seed=124)\n                S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n                # assert S.exp_imp(0.0, 1.0) == 1/np.sqrt(2*np.pi)\n                # which is approx. 0.3989422804014327\n                S.exp_imp(0.0, 1.0)\n                0.3989422804014327\n        \"\"\"\n        # We do not use the min y values, but the aggragated mean values\n        # y_min = min(self.nat_y)\n        y_min = min(self.aggregated_mean_y)\n        if s0 &lt;= 0.0:\n            EI = 0.0\n        elif s0 &gt; 0.0:\n            EI_one = (y_min - y0) * (\n                    0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * ((y_min - y0) / s0))\n            )\n            EI_two = (s0 * (1.0 / sqrt(2.0 * pi))) * (\n                exp(-(1.0 / 2.0) * ((y_min - y0) ** 2.0 / s0 ** 2.0))\n            )\n            EI = EI_one + EI_two\n        return EI\n\n    def set_de_bounds(self) -&gt; None:\n        \"\"\"\n        Determine search bounds for model_optimizer, e.g., differential evolution.\n        This method sets the attribute `de_bounds` of the object to a list of lists,\n        where each inner list represents the lower and upper bounds for a parameter\n        being optimized. The number of inner lists is determined by the number of\n        parameters being optimized (`n_theta` and `n_p`), as well as whether noise is\n        being considered (`noise`).\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                S = Kriging(name='kriging', seed=124)\n                S.set_de_bounds()\n                print(S.de_bounds)\n                [[-3.0, 2.0]]\n\n        Returns:\n            None\n        \"\"\"\n        logger.debug(\"In set_de_bounds(): self.min_theta: %s\", self.min_theta)\n        logger.debug(\"In set_de_bounds(): self.max_theta: %s\", self.max_theta)\n        logger.debug(\"In set_de_bounds(): self.n_theta: %s\", self.n_theta)\n        logger.debug(\"In set_de_bounds(): self.optim_p: %s\", self.optim_p)\n        logger.debug(\"In set_de_bounds(): self.min_p: %s\", self.min_p)\n        logger.debug(\"In set_de_bounds(): self.max_p: %s\", self.max_p)\n        logger.debug(\"In set_de_bounds(): self.n_p: %s\", self.n_p)\n        logger.debug(\"In set_de_bounds(): self.noise: %s\", self.noise)\n        logger.debug(\"In set_de_bounds(): self.min_Lambda: %s\", self.min_Lambda)\n        logger.debug(\"In set_de_bounds(): self.max_Lambda: %s\", self.max_Lambda)\n\n        de_bounds = [[self.min_theta, self.max_theta] for _ in range(self.n_theta)]\n        if self.optim_p:\n            de_bounds += [[self.min_p, self.max_p] for _ in range(self.n_p)]\n            if self.noise:\n                de_bounds.append([self.min_Lambda, self.max_Lambda])\n        else:\n            if self.noise:\n                de_bounds.append([self.min_Lambda, self.max_Lambda])\n        self.de_bounds = de_bounds\n        logger.debug(\"In set_de_bounds(): self.de_bounds: %s\", self.de_bounds)\n\n    def extract_from_bounds(self, new_theta_p_Lambda: np.ndarray) -&gt; None:\n        \"\"\"\n        Extract `theta`, `p`, and `Lambda` from bounds. The kriging object stores\n        `theta` as an array,  `p` as an array, and `Lambda` as a float.\n\n        Args:\n            self (object): The Kriging object.\n            new_theta_p_Lambda (np.ndarray):\n                1d-array with theta, p, and Lambda values. Order is important.\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.build.kriging import Kriging\n                import logging\n                logging.basicConfig(level=logging.DEBUG)\n                # Define the number of theta and p parameters\n                num_theta = 2\n                num_p = 3\n                # Initialize the Kriging model\n                kriging_model = Kriging(\n                    name='kriging',\n                    seed=124,\n                    n_theta=num_theta,\n                    n_p=num_p,\n                    optim_p=True,\n                    noise=True\n                )\n                # Create bounds array\n                bounds_array = np.array([1, 2, 3, 4, 5, 6])\n                # Extract parameters from given bounds\n                kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n                # Assertions to check if parameters are correctly extracted\n                assert np.array_equal(kriging_model.theta,\n                    [1, 2]), f\"Expected theta to be [1, 2] but got {kriging_model.theta}\"\n                assert np.array_equal(kriging_model.p,\n                    [3, 4, 5]), f\"Expected p to be [3, 4, 5] but got {kriging_model.p}\"\n                assert kriging_model.Lambda == 6, f\"Expected Lambda to be 6 but got {kriging_model.Lambda}\"\n                print(\"All assertions passed!\")\n        \"\"\"\n        logger.debug(\"Extracting parameters from: %s\", new_theta_p_Lambda)\n\n        # Validate array length\n        expected_length = self.n_theta\n        if self.optim_p:\n            expected_length += self.n_p\n        if self.noise:\n            expected_length += 1\n\n        if len(new_theta_p_Lambda) &lt; expected_length:\n            logger.error(\"Input array is too short. Expected at least %d elements, got %d.\",\n                         expected_length, len(new_theta_p_Lambda))\n            raise ValueError(f\"Input array must have at least {expected_length} elements.\")\n\n        # Extract theta\n        self.theta = new_theta_p_Lambda[:self.n_theta]\n        logger.debug(\"Extracted theta: %s\", self.theta)\n\n        if self.optim_p:\n            # Extract p if optim_p is True\n            self.p = new_theta_p_Lambda[self.n_theta:self.n_theta + self.n_p]\n            logger.debug(\"Extracted p: %s\", self.p)\n\n        if self.noise:\n            # Extract Lambda\n            lambda_index = self.n_theta + (self.n_p if self.optim_p else 0)\n            self.Lambda = new_theta_p_Lambda[lambda_index]\n            logger.debug(\"Extracted Lambda: %s\", self.Lambda)\n\n    def optimize_model(self) -&gt; Union[List[float], Tuple[float]]:\n        \"\"\"\n        Optimize the model using the specified model_optimizer.\n\n        This method uses the specified model_optimizer to optimize the\n        likelihood function (`fun_likelihood`) with respect to the model parameters.\n        The optimization is performed within the bounds specified by the attribute\n        `de_bounds`.\n        The result of the optimization is returned as a list or tuple of optimized parameter values.\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                n=2\n                p=2\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n                S.initialize_variables(nat_X, nat_y)\n                S.set_variable_types()\n                S.set_theta_values()\n                S.initialize_matrices()\n                S.set_de_bounds()\n                new_theta_p_Lambda = S.optimize_model()\n                print(new_theta_p_Lambda)\n                [0.12167915 1.49467909 1.82808259 1.69648798 0.79564346]\n\n        Returns:\n            result[\"x\"] (Union[List[float], Tuple[float]]):\n                A list or tuple of optimized parameter values.\n        \"\"\"\n        logger.debug(\"Entering optimize_model.\")\n        if not callable(self.model_optimizer):\n            logger.error(\"model_optimizer is not callable.\")\n            raise ValueError(\"model_optimizer must be a callable function or method.\")\n\n        optimizer_strategies: Dict[str, Dict] = {\n            'dual_annealing': {'func': self.fun_likelihood, 'bounds': self.de_bounds},\n            'differential_evolution': {\n                'func': self.fun_likelihood,\n                'bounds': self.de_bounds,\n                'maxiter': self.model_fun_evals,\n                'seed': self.seed\n            },\n            'direct': {\n                'func': self.fun_likelihood,\n                'bounds': self.de_bounds,\n                'eps': 1e-2\n            },\n            'shgo': {'func': self.fun_likelihood, 'bounds': self.de_bounds},\n            'basinhopping': {'func': self.fun_likelihood, 'x0': np.mean(self.de_bounds, axis=1)}\n        }\n\n        optimizer_name = self.model_optimizer.__name__\n        logger.debug(\"Optimizer selected: %s\", optimizer_name)\n\n        if optimizer_name not in optimizer_strategies:\n            logger.info(\"Using default options for optimizer: %s\", optimizer_name)\n            optimizer_args = {'func': self.fun_likelihood, 'bounds': self.de_bounds}\n        else:\n            optimizer_args = optimizer_strategies[optimizer_name]\n\n        logger.debug(\"Parameters for optimization: %s\", optimizer_args)\n\n        try:\n            result = self.model_optimizer(**optimizer_args)\n        except Exception as e:\n            logger.error(\"Optimization failed due to error: %s\", str(e))\n            raise\n\n        if \"x\" not in result:\n            logger.error(\"Optimization result does not contain 'x'. Result: %s\", result)\n            raise ValueError(\"The optimization result does not contain the expected 'x' key.\")\n        logger.debug(\"Optimization result: %s\", result)\n        optimized_parameters = list(result[\"x\"])\n        logger.debug(\"Extracted optimized parameters: %s\", optimized_parameters)\n        return optimized_parameters\n\n    def update_log(self) -&gt; None:\n        \"\"\"\n        Update the log with the current values of negLnLike, theta, p, and Lambda.\n        This method appends the current values of negLnLike, theta, p (if optim_p is True),\n        and Lambda (if noise is True)\n        to their respective lists in the log dictionary.\n        It also updates the log_length attribute with the current length\n        of the negLnLike list in the log.\n        If spot_writer is not None, this method also writes the current values of\n        negLnLike, theta, p (if optim_p is True),\n        and Lambda (if noise is True) to the spot_writer object.\n\n        Args:\n            self (object): The Kriging object.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                n=2\n                p=2\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n                S.initialize_variables(nat_X, nat_y)\n                S.set_variable_types()\n                S.set_theta_values()\n                S.initialize_matrices()\n                S.set_de_bounds()\n                new_theta_p_Lambda = S.optimize_model()\n                S.update_log()\n                print(S.log)\n                {'negLnLike': array([-1.38629436]),\n                 'theta': array([-1.14525993,  1.6123372 ]),\n                  'p': array([1.84444406, 1.74590865]),\n                  'Lambda': array([0.44268472])}\n\n        \"\"\"\n        self.log[\"negLnLike\"] = append(self.log[\"negLnLike\"], self.negLnLike)\n        self.log[\"theta\"] = append(self.log[\"theta\"], self.theta)\n        if self.optim_p:\n            self.log[\"p\"] = append(self.log[\"p\"], self.p)\n        if self.noise:\n            self.log[\"Lambda\"] = append(self.log[\"Lambda\"], self.Lambda)\n        # get the length of the log\n        self.log_length = len(self.log[\"negLnLike\"])\n        if self.spot_writer is not None:\n            negLnLike = self.negLnLike.copy()\n            self.spot_writer.add_scalar(\"spot_negLnLike\", negLnLike, self.counter+self.log_length)\n            # add the self.n_theta theta values to the writer with one key \"theta\",\n            # i.e, the same key for all theta values\n            theta = self.theta.copy()\n            self.spot_writer.add_scalars(\"spot_theta\", {f\"theta_{i}\": theta[i] for i in range(self.n_theta)},\n                                         self.counter+self.log_length)\n            if self.noise:\n                Lambda = self.Lambda.copy()\n                self.spot_writer.add_scalar(\"spot_Lambda\", Lambda, self.counter+self.log_length)\n            if self.optim_p:\n                p = self.p.copy()\n                self.spot_writer.add_scalars(\"spot_p\",\n                                             {f\"p_{i}\": p[i] for i in range(self.n_p)}, self.counter+self.log_length)\n            self.spot_writer.flush()\n\n    def fit(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; object:\n        \"\"\"\n        Fits the hyperparameters (`theta`, `p`, `Lambda`) of the Kriging model.\n        The function computes the following internal values:\n        1. `theta`, `p`, and `Lambda` values via optimization of the function `fun_likelihood()`.\n        2. Correlation matrix `Psi` via `buildPsi()`.\n        3. U matrix via `buildU()`.\n\n        Args:\n            self (object): The Kriging object.\n            nat_X (np.ndarray): Sample points.\n            nat_y (np.ndarray): Function values.\n\n        Returns:\n            object: Fitted estimator.\n\n        Attributes:\n            theta (np.ndarray): Kriging theta values. Shape (k,).\n            p (np.ndarray): Kriging p values. Shape (k,).\n            LnDetPsi (np.float64): Determinant Psi matrix.\n            Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n            psi (np.ndarray): psi vector. Shape (n,).\n            one (np.ndarray): vector of ones. Shape (n,).\n            mu (np.float64): Kriging expected mean value mu.\n            U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n            SigmaSqr (np.float64): Sigma squared value.\n            Lambda (float): lambda noise value.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 0], [1, 0]])\n                nat_y = np.array([1, 2])\n                S = Kriging()\n                S.fit(nat_X, nat_y)\n                print(S.Psi)\n                [[1.00000001 1.        ]\n                [1.         1.00000001]]\n\n        \"\"\"\n        logger.debug(\"In fit(): nat_X: %s\", nat_X)\n        logger.debug(\"In fit(): nat_y: %s\", nat_y)\n        self.initialize_variables(nat_X, nat_y)\n        self.set_variable_types()\n        self.set_theta_values()\n        self.initialize_matrices()\n        # build_Psi() and build_U() are called in fun_likelihood\n        self.set_de_bounds()\n        # Finally, set new theta and p values and update the surrogate again\n        # for new_theta_p_Lambda in de_results[\"x\"]:\n        new_theta_p_Lambda = self.optimize_model()\n        self.extract_from_bounds(new_theta_p_Lambda)\n        self.build_Psi()\n        self.build_U()\n        # TODO: check if the following line is necessary!\n        self.likelihood()\n        self.update_log()\n\n    def initialize_variables(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; None:\n        \"\"\"\n        Initialize variables for the class instance.\n        This method takes in the independent and dependent variable data as input\n        and initializes the class instance variables.\n        It creates deep copies of the input data and stores them in the\n        instance variables `nat_X` and `nat_y`.\n        It also calculates the number of observations `n` and\n        the number of independent variables `k` from the shape of `nat_X`.\n        Finally, it creates empty arrays with the same shape as `nat_X`\n        and `nat_y` and stores them in the instance variables `cod_X` and `cod_y`.\n\n        Args:\n            self (object): The Kriging object.\n            nat_X (np.ndarray): The independent variable data.\n            nat_y (np.ndarray): The dependent variable data.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                S = Kriging()\n                S.initialize_variables(nat_X, nat_y)\n                print(f\"S.nat_X: {S.nat_X}\")\n                print(f\"S.nat_y: {S.nat_y}\")\n                S.nat_X: [[1 2]\n                          [3 4]]\n                S.nat_y: [1 2]\n\n        \"\"\"\n        # Validate input dimensions\n        if nat_X.ndim != 2 or nat_y.ndim != 1:\n            raise ValueError(\"nat_X must be a 2D array and nat_y must be a 1D array.\")\n        if nat_X.shape[0] != nat_y.shape[0]:\n            raise ValueError(\"The number of samples in nat_X and nat_y must be equal.\")\n\n        # Initialize instance variables\n        self.nat_X = copy.deepcopy(nat_X)\n        self.nat_y = copy.deepcopy(nat_y)\n        self.n, self.k = self.nat_X.shape\n\n        # Calculate and store min and max of X\n        self.min_X = np.min(self.nat_X, axis=0)\n        self.max_X = np.max(self.nat_X, axis=0)\n\n        # Calculate the aggregated mean of y\n        _, aggregated_mean_y, _ = aggregate_mean_var(X=self.nat_X, y=self.nat_y)\n        self.aggregated_mean_y = np.copy(aggregated_mean_y)\n\n        # Logging the initialized variables\n        logger.debug(\"In initialize_variables(): self.nat_X: %s\", self.nat_X)\n        logger.debug(\"In initialize_variables(): self.nat_y: %s\", self.nat_y)\n        logger.debug(\"In initialize_variables(): self.aggregated_mean_y: %s\", self.aggregated_mean_y)\n        logger.debug(\"In initialize_variables(): self.min_X: %s\", self.min_X)\n        logger.debug(\"In initialize_variables(): self.max_X: %s\", self.max_X)\n        logger.debug(\"In initialize_variables(): self.n: %d\", self.n)\n        logger.debug(\"In initialize_variables(): self.k: %d\", self.k)\n\n    def set_variable_types(self) -&gt; None:\n        \"\"\"\n        Set the variable types for the class instance.\n        This method sets the variable types for the class instance based\n        on the `var_type` attribute. If the length of `var_type` is less\n        than `k`, all variable types are forced to 'num' and a warning is logged.\n        The method then creates Boolean masks for each variable\n        type ('num', 'factor', 'int', 'ordered') using numpy arrays, e.g.,\n        `num_mask = array([ True,  True])` if two numerical variables are present.\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                n=2\n                p=2\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n                S.initialize_variables(nat_X, nat_y)\n                S.set_variable_types()\n                assert S.var_type == ['num', 'num']\n                assert S.var_type == ['num', 'num']\n                assert S.num_mask.all() == True\n                assert S.factor_mask.all() == False\n                assert S.int_mask.all() == False\n                assert S.ordered_mask.all() == True\n\n        Returns:\n            None\n        \"\"\"\n        logger.debug(\"In set_variable_types(): self.k: %s\", self.k)\n        logger.debug(\"In set_variable_types(): self.var_type: %s\", self.var_type)\n\n        # Ensure var_type has appropriate length by defaulting to 'num'\n        if len(self.var_type) &lt; self.k:\n            self.var_type = ['num'] * self.k  # Corrected to fill with 'num' instead of duplicating\n            logger.warning(\"In set_variable_types(): All variable types forced to 'num'.\")\n            logger.debug(\"In set_variable_types(): self.var_type: %s\", self.var_type)\n        # Create masks for each type using numpy vectorized operations\n        var_type_array = np.array(self.var_type)\n        self.num_mask = (var_type_array == \"num\")\n        self.factor_mask = (var_type_array == \"factor\")\n        self.int_mask = (var_type_array == \"int\")\n        self.ordered_mask = np.isin(var_type_array, [\"int\", \"num\", \"float\"])\n        logger.debug(\"In set_variable_types(): self.num_mask: %s\", self.num_mask)\n        logger.debug(\"In set_variable_types(): self.factor_mask: %s\", self.factor_mask)\n        logger.debug(\"In set_variable_types(): self.int_mask: %s\", self.int_mask)\n        logger.debug(\"In set_variable_types(): self.ordered_mask: %s\", self.ordered_mask)\n\n    def set_theta_values(self) -&gt; None:\n        \"\"\"\n        Set the theta values for the class instance.\n\n        This method sets the theta values for the class instance based\n        on the `n_theta` and `k` attributes. If `n_theta` is greater than\n        `k`, `n_theta` is set to `k` and a warning is logged.\n        The method then initializes the `theta` attribute as a list\n        of zeros with length `n_theta`.\n        The `x0_theta` attribute is also initialized as a list of ones\n        with length `n_theta`, multiplied by `n / (100 * k)`.\n\n        Args:\n            self (object): The Kriging object.\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                from numpy import array\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                n=2\n                p=2\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n                S.initialize_variables(nat_X, nat_y)\n                S.set_variable_types()\n                S.set_theta_values()\n                assert S.theta.all() == array([0., 0.]).all()\n        \"\"\"\n        logger.debug(\"In set_theta_values(): self.k: %s\", self.k)\n        logger.debug(\"In set_theta_values(): self.n_theta: %s\", self.n_theta)\n\n        # Adjust `n_theta` if it exceeds `k`\n        if self.n_theta &gt; self.k:\n            self.n_theta = self.k\n            logger.warning(\"Too few theta values or more theta values than dimensions. `n_theta` set to `k`.\")\n            logger.debug(\"In set_theta_values(): self.n_theta reset to: %s\", self.n_theta)\n\n        # Initialize theta values\n        if hasattr(self, \"theta_init_zero\") and self.theta_init_zero:\n            self.theta = np.zeros(self.n_theta, dtype=float)\n            logger.debug(\"Theta initialized to zeros: %s\", self.theta)\n        else:\n            logger.debug(\"In set_theta_values(): self.n: %s\", self.n)\n            self.theta = np.ones(self.n_theta, dtype=float) * self.n / (100 * self.k)\n            logger.debug(\"Theta initialized based on n and k: %s\", self.theta)\n\n    def initialize_matrices(self) -&gt; None:\n        \"\"\"\n        Initialize the matrices for the class instance.\n\n        This method initializes several matrices and attributes for the class instance.\n        The `p` attribute is initialized as a list of ones with length `n_p`, multiplied by 2.0.\n        The `pen_val` attribute is initialized as the natural logarithm of the\n        variance of `nat_y`, multiplied by `n`, plus 1e4.\n        The `negLnLike`, `LnDetPsi`, `mu`, `U`, `SigmaSqr`, and `Lambda` attributes are all set to None.\n        The `gen` attribute is initialized using the `SpaceFilling` function with arguments `k` and `seed`.\n        The `Psi` attribute is initialized as a zero matrix with shape `(n, n)` and dtype `float64`.\n        The `psi` attribute is initialized as a zero matrix with shape `(n, 1)`.\n        The `one` attribute is initialized as a list of ones with length `n`.\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                from numpy import log, var\n                nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n                nat_y = np.array([1, 2, 3])\n                n=3\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n                S.initialize_variables(nat_X, nat_y)\n                S.set_variable_types()\n                S.set_theta_values()\n                S.initialize_matrices()\n                # if var(self.nat_y) is &gt; 0, then self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n                # else self.pen_val = self.n * var(self.nat_y) + 1e4\n                assert S.pen_val == nat_X.shape[0] * log(var(S.nat_y)) + 1e4\n                assert S.Psi.shape == (n, n)\n\n        Returns:\n            None\n        \"\"\"\n        logger.debug(\"In initialize_matrices(): self.n_p: %s\", self.n_p)\n\n        # Initialize p\n        self.p = np.ones(self.n_p) * self.p_val\n        logger.debug(\"In initialize_matrices(): self.p: %s\", self.p)\n\n        # Calculate variance of nat_y\n        y_variance = var(self.nat_y)\n        logger.debug(\"In initialize_matrices(): var(self.nat_y): %s\", y_variance)\n\n        # Set penalty value based on variance\n        if y_variance &gt; 0:\n            self.pen_val = self.n * log(y_variance) + 1e4\n        else:\n            self.pen_val = self.n * y_variance + 1e4\n        logger.debug(\"In initialize_matrices(): self.pen_val: %s\", self.pen_val)\n\n        # Initialize other attributes\n        self.negLnLike = None\n        self.LnDetPsi = None\n        self.mu = None\n        self.U = None\n        self.SigmaSqr = None\n        self.Lambda = None\n\n        # Initialize generator\n        self.gen = SpaceFilling(k=self.k, seed=self.seed)\n        logger.debug(\"In initialize_matrices(): self.gen: %s\", self.gen)\n\n        # Initialize matrix Psi and vector psi\n        self.Psi = np.zeros((self.n, self.n), dtype=np.float64)\n        logger.debug(\"In initialize_matrices(): self.Psi shape: %s\", self.Psi.shape)\n\n        self.psi = np.zeros((self.n, 1), dtype=np.float64)\n        logger.debug(\"In initialize_matrices(): self.psi shape: %s\", self.psi.shape)\n\n        # Initialize one\n        self.one = np.ones(self.n, dtype=np.float64)\n        logger.debug(\"In initialize_matrices(): self.one: %s\", self.one)\n\n    def fun_likelihood(self, new_theta_p_Lambda: np.ndarray) -&gt; float:\n        \"\"\"\n        Compute log likelihood for a set of hyperparameters (theta, p, Lambda).\n\n        This method computes the log likelihood for a set of hyperparameters\n        (theta, p, Lambda) using several internal methods for matrix construction\n        and likelihood evaluation. It handles potential errors by returning a\n        penalty value for non-computable states.\n\n        Args:\n            new_theta_p_Lambda (np.ndarray): An array containing `theta`, `p`, and `Lambda` values.\n\n        Returns:\n            float: The negative log likelihood or the penalty value if computation fails.\n\n        Attributes:\n            theta (np.ndarray): Kriging theta values. Shape (k,).\n            p (np.ndarray): Kriging p values. Shape (k,).\n            Lambda (float): lambda noise value.\n            Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n            U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n            negLnLike (float): Negative log likelihood of the surface at the specified hyperparameters.\n            pen_val (float): Penalty value.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[0], [1]])\n                nat_y = np.array([0, 1])\n                n=1\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n                S.initialize_variables(nat_X, nat_y)\n                S.set_variable_types()\n                print(S.nat_X)\n                print(S.nat_y)\n                S.set_theta_values()\n                print(f\"S.theta: {S.theta}\")\n                S.initialize_matrices()\n                S.set_de_bounds()\n                new_theta_p_Lambda = S.optimize_model()\n                S.extract_from_bounds(new_theta_p_Lambda)\n                print(f\"S.theta: {S.theta}\")\n                S.build_Psi()\n                print(f\"S.Psi: {S.Psi}\")\n                S.build_U()\n                print(f\"S.U:{S.U}\")\n                S.likelihood()\n                S.negLnLike\n                    [[0]\n                    [1]]\n                    [0 1]\n                    S.theta: [0.]\n                    S.theta: [1.60036366]\n                    S.Psi: [[1.00000001e+00 4.96525625e-18]\n                    [4.96525625e-18 1.00000001e+00]]\n                    S.U:[[1.00000001e+00 4.96525622e-18]\n                    [0.00000000e+00 1.00000001e+00]]\n                    -1.3862943611198906\n        \"\"\"\n        # Extract hyperparameters\n        self.extract_from_bounds(new_theta_p_Lambda)\n        # Check transformed theta values\n        theta_scaled = np.power(10.0, self.theta)\n        if self.__is_any__(theta_scaled, 0):\n            logger.warning(\"Failure in fun_likelihood: 10^theta == 0. Setting negLnLike to %s\", self.pen_val)\n            return self.pen_val\n        # Build Psi matrix and check its condition\n        self.build_Psi()\n        if getattr(self, 'inf_Psi', False) or getattr(self, 'cnd_Psi', float('inf')) &gt; 1e9:\n            logger.warning(\"Failure in fun_likelihood: Psi is ill-conditioned: %s\", getattr(self, 'cnd_Psi', 'unknown'))\n            logger.warning(\"Setting negLnLike to: %s\", self.pen_val)\n            return self.pen_val\n        # Build U matrix and handle exceptions\n        try:\n            self.build_U()\n        except Exception as error:\n            logger.error(\"Error in fun_likelihood(). Call to build_U() failed: %s\", error)\n            logger.error(\"Setting negLnLike to %.2f.\", self.pen_val)\n            return self.pen_val\n\n        # Calculate likelihood\n        self.likelihood()\n        return self.negLnLike\n\n    def __is_any__(self, x: Union[np.ndarray, Any], v: Any) -&gt; bool:\n        \"\"\"\n        Check if any element in `x` is equal to `v`.\n\n        This method checks if any element in the input array-like `x`\n        is equal to the given value `v`. Converts inputs to numpy arrays as necessary.\n\n        Args:\n            x (Union[np.ndarray, Any]): The input array-like object to check.\n            v (Any): The value to check for in `x`.\n\n        Returns:\n            bool: True if any element in `x` is equal to `v`, False otherwise.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                from numpy import power\n                import numpy as np\n                nat_X = np.array([[0], [1]])\n                nat_y = np.array([0, 1])\n                n=1\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n                S.initialize_variables(nat_X, nat_y)\n                S.set_variable_types()\n                S.set_theta_values()\n                print(f\"S.theta: {S.theta}\")\n                print(S.__is_any__(power(10.0, S.theta), 0))\n                print(S.__is_any__(S.theta, 0))\n                    S.theta: [0.]\n                    False\n                    True\n        \"\"\"\n\n        if not isinstance(x, np.ndarray):\n            x = np.array([x])  # Wrap scalar x in an array\n        return np.any(x == v)\n\n    def build_Psi(self) -&gt; None:\n        \"\"\"\n        Constructs a new (n x n) correlation matrix Psi to reflect new data\n        or a change in hyperparameters.\n        This method uses `theta`, `p`, and coded `X` values to construct the\n        correlation matrix as described in [Forr08a, p.57].\n\n        Attributes:\n            Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n            cnd_Psi (float): Condition number of Psi.\n            inf_Psi (bool): True if Psi is infinite, False otherwise.\n\n        Raises:\n            LinAlgError: If building Psi fails.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[0], [1]])\n                nat_y = np.array([0, 1])\n                n=1\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n                S.initialize_variables(nat_X, nat_y)\n                S.set_variable_types()\n                print(S.nat_X)\n                print(S.nat_y)\n                S.set_theta_values()\n                print(f\"S.theta: {S.theta}\")\n                S.initialize_matrices()\n                S.set_de_bounds()\n                new_theta_p_Lambda = S.optimize_model()\n                S.extract_from_bounds(new_theta_p_Lambda)\n                print(f\"S.theta: {S.theta}\")\n                S.build_Psi()\n                print(f\"S.Psi: {S.Psi}\")\n                    [[0]\n                    [1]]\n                    [0 1]\n                    S.theta: [0.]\n                    S.theta: [1.60036366]\n                    S.Psi: [[1.00000001e+00 4.96525625e-18]\n                    [4.96525625e-18 1.00000001e+00]]\n        \"\"\"\n        try:\n            n = self.n\n            k = self.k\n            theta = np.power(10.0, self.theta)\n\n            # Ensure theta has the correct length\n            if self.n_theta == 1:\n                theta = theta * np.ones(k)\n\n            # Initialize the Psi matrix\n            self.Psi = np.zeros((n, n), dtype=np.float64)\n\n            # Calculate the distance matrix using ordered variables\n            if self.ordered_mask.any():\n                X_ordered = self.nat_X[:, self.ordered_mask]\n                D_ordered = squareform(\n                    pdist(X_ordered, metric='sqeuclidean', w=theta[self.ordered_mask])\n                )\n                self.Psi += D_ordered\n\n            # Add the contribution of factor variables to the distance matrix\n            if self.factor_mask.any():\n                X_factor = self.nat_X[:, self.factor_mask]\n                D_factor = squareform(\n                    pdist(X_factor, metric=self.metric_factorial, w=theta[self.factor_mask])\n                )\n                self.Psi += D_factor\n\n            # Calculate correlation from distance\n            self.Psi = np.exp(-self.Psi)\n\n            # Adjust diagonal elements for noise or minimum epsilon\n            diag_indices = np.diag_indices_from(self.Psi)\n            if self.noise:\n                self.Psi[diag_indices] += self.Lambda\n                logger.debug(\"Noise level Lambda applied to diagonal: %s\", self.Lambda)\n            else:\n                self.Psi[diag_indices] += self.eps\n\n            # Check for infinite values\n            self.inf_Psi = np.isinf(self.Psi).any()\n\n            # Calculate condition number\n            self.cnd_Psi = cond(self.Psi)\n            logger.debug(\"Condition number of Psi: %f\", self.cnd_Psi)\n\n        except LinAlgError as err:\n            logger.error(\"Building Psi failed. Error: %s, Type: %s\", err, type(err))\n            raise\n\n    def build_U(self, scipy: bool = True) -&gt; None:\n        \"\"\"\n        Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57].\n        This method uses either `scipy_cholesky` or numpy's `cholesky` to perform the Cholesky factorization of Psi.\n\n        Args:\n            self (object):\n                The Kriging object.\n            scipy (bool):\n                If True, use `scipy_cholesky`.\n                If False, use numpy's `cholesky`.\n                Defaults to True.\n\n        Returns:\n            None\n\n        Raises:\n            LinAlgError:\n                If Cholesky factorization fails for Psi.\n\n        Attributes:\n            U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[0], [1]])\n                nat_y = np.array([0, 1])\n                n=1\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n                S.initialize_variables(nat_X, nat_y)\n                S.set_variable_types()\n                print(S.nat_X)\n                print(S.nat_y)\n                S.set_theta_values()\n                print(f\"S.theta: {S.theta}\")\n                S.initialize_matrices()\n                S.set_de_bounds()\n                new_theta_p_Lambda = S.optimize_model()\n                S.extract_from_bounds(new_theta_p_Lambda)\n                print(f\"S.theta: {S.theta}\")\n                S.build_Psi()\n                print(f\"S.Psi: {S.Psi}\")\n                S.build_U()\n                print(f\"S.U:{S.U}\")\n                    [[0]\n                    [1]]\n                    [0 1]\n                    S.theta: [0.]\n                    S.theta: [1.60036366]\n                    S.Psi: [[1.00000001e+00 4.96525625e-18]\n                    [4.96525625e-18 1.00000001e+00]]\n                    S.U:[[1.00000001e+00 4.96525622e-18]\n                    [0.00000000e+00 1.00000001e+00]]\n        \"\"\"\n        try:\n            self.U = scipy_cholesky(self.Psi, lower=True) if scipy else cholesky(self.Psi)\n            self.U = self.U.T\n        except LinAlgError as err:\n            print(f\"build_U() Cholesky failed for Psi:\\n {self.Psi}. {err=}, {type(err)=}\")\n\n    def likelihood(self) -&gt; None:\n        \"\"\"\n        Calculate the negative concentrated log-likelihood.\n        Implements equation (2.32) from [Forr08a] to compute the negative of the\n        concentrated log-likelihood. Updates `mu`, `SigmaSqr`, `LnDetPsi`, and `negLnLike`.\n\n        Note:\n            Requires prior calls to `build_Psi` and `build_U`.\n\n        Attributes:\n            mu (np.float64): Kriging expected mean value mu.\n            SigmaSqr (np.float64): Sigma squared value.\n            LnDetPsi (np.float64): Logarithm of the determinant of Psi matrix.\n            negLnLike (float): Negative log likelihood of the surface at the specified hyperparameters.\n\n        Raises:\n            LinAlgError: If matrix operations fail.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[1], [2]])\n                nat_y = np.array([5, 10])\n                n=2\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n                S.initialize_variables(nat_X, nat_y)\n                S.set_variable_types()\n                S.set_theta_values()\n                S.initialize_matrices()\n                S.build_Psi()\n                S.build_U()\n                S.likelihood()\n                assert np.allclose(S.mu, 7.5, atol=1e-6)\n                E = np.exp(1)\n                sigma2 = E / (E**2 - 1) * (25/4 + 25/4*E)\n                assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)\n                print(f\"S.LnDetPsi:{S.LnDetPsi}\")\n                print(f\"S.negLnLike:{S.negLnLike}\")\n                    S.LnDetPsi:-0.1454134234019476\n                    S.negLnLike:2.2185498738611282\n        \"\"\"\n        try:\n            # Solving linear equations for needed components\n            U_T_inv_one = solve(self.U.T, self.one)\n            U_T_inv_nat_y = solve(self.U.T, self.nat_y)\n            # Mean calculation: (2.20) in [Forr08a]\n            self.mu = (self.one.T @ solve(self.U, U_T_inv_nat_y)) / (self.one.T @ solve(self.U, U_T_inv_one))\n            # Residuals\n            cod_y_minus_mu = self.nat_y - self.one * self.mu\n            # Sigma squared calculation: (2.31) in [Forr08a]\n            self.SigmaSqr = (cod_y_minus_mu.T @ solve(self.U, solve(self.U.T, cod_y_minus_mu))) / self.n\n            # Log determinant of Psi: (2.32) in [Forr08a]\n            self.LnDetPsi = 2.0 * np.sum(np.log(np.abs(np.diag(self.U))))\n            # Negative log-likelihood calculation: simplified from (2.32)\n            self.negLnLike = 0.5 * (self.n * np.log(self.SigmaSqr) + self.LnDetPsi)\n            logger.debug(\"Likelihood calculated: mu=%s, SigmaSqr=%s, LnDetPsi=%s, negLnLike=%s\",\n                         self.mu, self.SigmaSqr, self.LnDetPsi, self.negLnLike)\n        except LinAlgError as error:\n            logger.error(\"LinAlgError in likelihood calculation: %s\", error)\n            raise\n\n    def plot(self, show: Optional[bool] = True) -&gt; None:\n        \"\"\"\n        This function plots 1D and 2D surrogates.\n\n        Args:\n            self (object):\n                The Kriging object.\n            show (bool):\n                If `True`, the plots are displayed.\n                If `False`, `plt.show()` should be called outside this function.\n\n        Returns:\n            None\n\n        Note:\n            * This method provides only a basic plot. For more advanced plots,\n                use the `plot_contour()` method of the `Spot` class.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init, design_control_init\n                # 1-dimensional example\n                fun = analytical().fun_sphere\n                fun_control=fun_control_init(lower = np.array([-1]),\n                                            upper = np.array([1]),\n                                            noise=False)\n                design_control=design_control_init(init_size=10)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control)\n                S.initialize_design()\n                S.update_stats()\n                S.fit_surrogate()\n                S.surrogate.plot()\n                # 2-dimensional example\n                fun = analytical().fun_sphere\n                fun_control=fun_control_init(lower = np.array([-1, -1]),\n                                            upper = np.array([1, 1]),\n                                            noise=False)\n                design_control=design_control_init(init_size=10)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control)\n                S.initialize_design()\n                S.update_stats()\n                S.fit_surrogate()\n                S.surrogate.plot()\n        \"\"\"\n        if self.k == 1:\n            # TODO: Improve plot (add conf. interval etc.)\n            fig = pylab.figure(figsize=(9, 6))\n            n_grid = 100\n            x = linspace(\n                self.min_X[0], self.max_X[0], num=n_grid\n            )\n            y = self.predict(x)\n            plt.figure()\n            plt.plot(x, y, \"k\")\n            if show:\n                plt.show()\n\n        if self.k == 2:\n            fig = pylab.figure(figsize=(9, 6))\n            n_grid = 100\n            x = linspace(\n                self.min_X[0], self.max_X[0], num=n_grid\n            )\n            y = linspace(\n                self.min_X[1], self.max_X[1], num=n_grid\n            )\n            X, Y = meshgrid(x, y)\n            # Predict based on the optimized results\n            zz = array(\n                [self.predict(array([x, y]), return_val=\"all\") for x, y in zip(ravel(X), ravel(Y))]\n            )\n            zs = zz[:, 0, :]\n            zse = zz[:, 1, :]\n            Z = zs.reshape(X.shape)\n            Ze = zse.reshape(X.shape)\n\n            nat_point_X = self.nat_X[:, 0]\n            nat_point_Y = self.nat_X[:, 1]\n            contour_levels = 30\n            ax = fig.add_subplot(224)\n            # plot predicted values:\n            pylab.contourf(X, Y, Ze, contour_levels, cmap=\"jet\")\n            pylab.title(\"Error\")\n            pylab.colorbar()\n            # plot observed points:\n            pylab.plot(nat_point_X, nat_point_Y, \"ow\")\n            #\n            ax = fig.add_subplot(223)\n            # plot predicted values:\n            plt.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\")\n            plt.title(\"Surrogate\")\n            # plot observed points:\n            pylab.plot(nat_point_X, nat_point_Y, \"ow\", zorder=3)\n            pylab.colorbar()\n            #\n            ax = fig.add_subplot(221, projection=\"3d\")\n            ax.plot_surface(X, Y, Z, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n            #\n            ax = fig.add_subplot(222, projection=\"3d\")\n            ax.plot_surface(X, Y, Ze, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n            #\n            pylab.show()\n\n    def predict(self, nat_X: ndarray, return_val: str = \"y\") -&gt; Union[float, Tuple[float, float]]:\n        \"\"\"\n        This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X.\n\n        Args:\n            self (object): The Kriging object.\n            nat_X (ndarray): Design variable to evaluate in natural units.\n            return_val (str): Specifies which prediction values to return. It can be \"y\", \"s\", \"ei\", or \"all\".\n\n        Returns:\n            Union[float, Tuple[float, float, float]]: Depending on `return_val`, returns the predicted value,\n            predicted error, expected improvement, or all.\n\n        Raises:\n            TypeError: If `nat_X` is not an ndarray or doesn't match expected dimensions.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                from numpy import linspace, arange\n                rng = np.random.RandomState(1)\n                X = linspace(start=0, stop=10, num=1_0).reshape(-1, 1)\n                y = np.squeeze(X * np.sin(X))\n                training_indices = rng.choice(arange(y.size), size=6, replace=False)\n                X_train, y_train = X[training_indices], y[training_indices]\n                S = Kriging(name='kriging', seed=124)\n                S.fit(X_train, y_train)\n                mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n                print(f\"mean_prediction: {mean_prediction}\")\n                print(f\"std_prediction: {std_prediction}\")\n                print(f\"s_ei: {s_ei}\")\n        \"\"\"\n        if not isinstance(nat_X, ndarray):\n            raise TypeError(f\"Expected an ndarray, got {type(nat_X)} instead.\")\n\n        try:\n            X = nat_X.reshape(-1, self.nat_X.shape[1])\n            X = repair_non_numeric(X, self.var_type)\n        except Exception as e:\n            raise TypeError(\"Input to predict was not convertible to the size of X\") from e\n\n        y, s, ei = self.predict_coded_batch(X)\n\n        if return_val == \"y\":\n            return y\n        elif return_val == \"s\":\n            return s\n        elif return_val == \"ei\":\n            return -ei\n        elif return_val == \"all\":\n            return y, s, -ei\n        else:\n            raise ValueError(f\"Invalid return_val: {return_val}. Supported values are 'y', 's', 'ei', 'all'.\")\n\n    def predict_coded(self, cod_x: np.ndarray) -&gt; Tuple[float, float, float]:\n        \"\"\"\n        Kriging prediction of one point in coded units as described in (2.20) in [Forr08a].\n        The error is returned as well. The method is used in `predict`.\n\n        Args:\n            self (object): The Kriging object.\n            cod_x (np.ndarray): Point in coded units to make prediction at.\n\n        Returns:\n            Tuple[float, float, float]: Predicted value, predicted error, and expected improvement.\n\n        Note:\n            Uses attributes such as `self.mu` and `self.SigmaSqr` that are expected\n            to be calculated by `likelihood`.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                from numpy import linspace, arange, empty\n                rng = np.random.RandomState(1)\n                X = linspace(start=0, stop=10, num=10).reshape(-1, 1)\n                y = np.squeeze(X * np.sin(X))\n                training_indices = rng.choice(arange(y.size), size=6, replace=False)\n                X_train, y_train = X[training_indices], y[training_indices]\n                S = Kriging(name='kriging', seed=124)\n                S.fit(X_train, y_train)\n                n = X.shape[0]\n                y = empty(n, dtype=float)\n                s = empty(n, dtype=float)\n                ei = empty(n, dtype=float)\n                for i in range(n):\n                    y_coded, s_coded, ei_coded = S.predict_coded(X[i, :])\n                    y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n                    s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n                    ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n                print(f\"y: {y}\")\n                print(f\"s: {s}\")\n                print(f\"ei: {-1.0*ei}\")\n        \"\"\"\n        self.build_psi_vec(cod_x)\n        mu_adj = self.mu\n        psi = self.psi\n\n        # Calculate the prediction\n        U_T_inv = solve(self.U.T, self.nat_y - self.one.dot(mu_adj))\n        f = mu_adj + psi.T.dot(solve(self.U, U_T_inv))[0]\n\n        Lambda = self.Lambda if self.noise else 0.0\n\n        # Calculate the estimated error\n        SSqr = self.SigmaSqr * (1 + Lambda - psi.T.dot(solve(self.U, solve(self.U.T, psi))))\n        SSqr = power(abs(SSqr), 0.5)[0]\n\n        # Calculate expected improvement\n        EI = self.exp_imp(y0=f, s0=SSqr)\n\n        return f, SSqr, EI\n\n    def predict_coded_batch(self, X: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Vectorized prediction for batch input using coded units.\n\n        Args:\n            X (np.ndarray): Input array of coded points.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]:\n                Arrays of predicted values, predicted errors, and expected improvements.\n        \"\"\"\n        n = X.shape[0]\n        y = np.empty(n, dtype=float)\n        s = np.empty(n, dtype=float)\n        ei = np.empty(n, dtype=float)\n\n        for i in range(n):\n            y_coded, s_coded, ei_coded = self.predict_coded(X[i, :])\n            y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n            s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n            ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n\n        return y, s, ei\n\n    def build_psi_vec(self, cod_x: np.ndarray) -&gt; None:\n        \"\"\"\n        Build the psi vector required for predictive methods.\n\n        Args:\n            cod_x (ndarray): Point to calculate the psi vector for.\n\n        Returns:\n            None\n\n        Modifies:\n            self.psi (np.ndarray): Updates the psi vector.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.build.kriging import Kriging\n                X_train = np.array([[1., 2.],\n                                    [2., 4.],\n                                    [3., 6.]])\n                y_train = np.array([1., 2., 3.])\n                S = Kriging(name='kriging',\n                            seed=123,\n                            log_level=50,\n                            n_theta=1,\n                            noise=False,\n                            cod_type=\"norm\")\n                S.fit(X_train, y_train)\n                # force theta to simple values:\n                S.theta = np.array([0.0])\n                nat_X = np.array([1., 0.])\n                S.psi = np.zeros((S.n, 1))\n                S.build_psi_vec(nat_X)\n                res = np.array([[np.exp(-4)],\n                    [np.exp(-17)],\n                    [np.exp(-40)]])\n                assert np.array_equal(S.psi, res)\n                print(f\"S.psi: {S.psi}\")\n                print(f\"Control value res: {res}\")\n        \"\"\"\n        logger.debug(\"Building psi vector for point: %s\", cod_x)\n        try:\n            self.psi = np.zeros((self.n, 1))\n            theta_scaled = np.power(10.0, self.theta)\n            if self.n_theta == 1:\n                theta_scaled = theta_scaled * np.ones(self.k)\n\n            D = np.zeros(self.n)\n\n            # Compute ordered distance contributions\n            if self.ordered_mask.any():\n                X_ordered = self.nat_X[:, self.ordered_mask]\n                x_ordered = cod_x[self.ordered_mask]\n                D += cdist(x_ordered.reshape(1, -1),\n                           X_ordered,\n                           metric='sqeuclidean',\n                           w=theta_scaled[self.ordered_mask]).ravel()\n            logger.debug(\"Distance D after ordered mask: %s\", D)\n            # Compute factor distance contributions\n            if self.factor_mask.any():\n                X_factor = self.nat_X[:, self.factor_mask]\n                x_factor = cod_x[self.factor_mask]\n                D += cdist(x_factor.reshape(1, -1),\n                           X_factor,\n                           metric=self.metric_factorial,\n                           w=theta_scaled[self.factor_mask]).ravel()\n            logger.debug(\"Distance D after factor mask: %s\", D)\n\n            self.psi = np.exp(-D).reshape(-1, 1)\n\n        except np.linalg.LinAlgError as err:\n            logger.error(\"Building psi failed due to a linear algebra error: %s. Error type: %s\", err, type(err))\n\n    def weighted_exp_imp(self, cod_x: np.ndarray, w: float) -&gt; float:\n        \"\"\"\n        Weighted expected improvement. Currently not used in `spotpython`\n\n        Args:\n            self (object): The Kriging object.\n            cod_x (np.ndarray): A coded design vector.\n            w (float): Weight.\n\n        Returns:\n            EI (float): Weighted expected improvement.\n\n        References:\n            [Sobester et al. 2005].\n        \"\"\"\n        y0, s0 = self.predict_coded(cod_x)\n        y_min = min(self.nat_y)\n        if s0 &lt;= 0.0:\n            EI = 0.0\n        else:\n            y_min_y0 = y_min - y0\n            EI_one = w * (\n                    y_min_y0\n                    * (0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * (y_min_y0 / s0)))\n            )\n            EI_two = (\n                    (1.0 - w)\n                    * (s0 * (1.0 / sqrt(2.0 * pi)))\n                    * (exp(-(1.0 / 2.0) * ((y_min_y0) ** 2.0 / s0 ** 2.0)))\n            )\n            EI = EI_one + EI_two\n        return EI\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.__init__","title":"<code>__init__(noise=False, var_type=['num'], name='kriging', seed=124, model_optimizer=None, model_fun_evals=None, min_theta=-3.0, max_theta=2.0, n_theta=1, theta_init_zero=True, p_val=2.0, n_p=1, optim_p=False, min_Lambda=1e-09, max_Lambda=1.0, log_level=50, spot_writer=None, counter=None, metric_factorial='canberra', **kwargs)</code>","text":"<p>Initialize the Kriging surrogate.</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>bool</code> <p>Use regression instead of interpolation kriging. Defaults to False.</p> <code>False</code> <code>var_type</code> <code>List[str]</code> <p>Variable type. Can be either \u201cnum\u201d (numerical) or \u201cfactor\u201d (factor). Defaults to [\u201cnum\u201d].</p> <code>['num']</code> <code>name</code> <code>str</code> <p>Surrogate name. Defaults to \u201ckriging\u201d.</p> <code>'kriging'</code> <code>seed</code> <code>int</code> <p>Random seed. Defaults to 124.</p> <code>124</code> <code>model_optimizer</code> <code>Optional[object]</code> <p>Optimizer on the surrogate. If None, differential_evolution is selected.</p> <code>None</code> <code>model_fun_evals</code> <code>Optional[int]</code> <p>Number of iterations used by the optimizer on the surrogate.</p> <code>None</code> <code>min_theta</code> <code>float</code> <p>Min log10 theta value. Defaults to -3.</p> <code>-3.0</code> <code>max_theta</code> <code>float</code> <p>Max log10 theta value. Defaults to 2.</p> <code>2.0</code> <code>n_theta</code> <code>int</code> <p>Number of theta values. Defaults to 1.</p> <code>1</code> <code>theta_init_zero</code> <code>bool</code> <p>Initialize theta with zero. Defaults to True.</p> <code>True</code> <code>p_val</code> <code>float</code> <p>p value. Used as an initial value if optim_p = True. Otherwise as a constant. Defaults to 2.</p> <code>2.0</code> <code>n_p</code> <code>int</code> <p>Number of p values. Defaults to 1.</p> <code>1</code> <code>optim_p</code> <code>bool</code> <p>Determines whether p should be optimized. Deafults to False.</p> <code>False</code> <code>min_Lambda</code> <code>float</code> <p>Min Lambda value. Defaults to 1e-9.</p> <code>1e-09</code> <code>max_Lambda</code> <code>float</code> <p>Max Lambda value. Defaults to 1.</p> <code>1.0</code> <code>log_level</code> <code>int</code> <p>Logging level, e.g., 20 is \u201cINFO\u201d. Defaults to 50 (\u201cCRITICAL\u201d).</p> <code>50</code> <code>spot_writer</code> <code>Optional[object]</code> <p>Spot writer. Defaults to None.</p> <code>None</code> <code>counter</code> <code>Optional[int]</code> <p>Counter. Defaults to None.</p> <code>None</code> <code>metric_factorial</code> <code>str</code> <p>Metric for factorial. Defaults to \u201ccanberra\u201d. Can be \u201ceuclidean\u201d, \u201ccityblock\u201d, seuclidean\u201d, \u201csqeuclidean\u201d, \u201ccosine\u201d, \u201ccorrelation\u201d, \u201chamming\u201d, \u201cjaccard\u201d, \u201cjensenshannon\u201d, \u201cchebyshev\u201d, \u201ccanberra\u201d, \u201cbraycurtis\u201d, \u201cmahalanobis\u201d, \u201cmatching\u201d.</p> <code>'canberra'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from numpy import linspace, arange\n    rng = np.random.RandomState(1)\n    X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n    y = np.squeeze(X * np.sin(X))\n    training_indices = rng.choice(arange(y.size), size=6, replace=False)\n    X_train, y_train = X[training_indices], y[training_indices]\n    S = Kriging(name='kriging', seed=124)\n    S.fit(X_train, y_train)\n    mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n    plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n    plt.scatter(X_train, y_train, label=\"Observations\")\n    plt.plot(X, mean_prediction, label=\"Mean prediction\")\n    plt.fill_between(\n        X.ravel(),\n        mean_prediction - 1.96 * std_prediction,\n        mean_prediction + 1.96 * std_prediction,\n        alpha=0.5,\n        label=r\"95% confidence interval\",\n        )\n    plt.legend()\n    plt.xlabel(\"$x$\")\n    plt.ylabel(\"$f(x)$\")\n    _ = plt.title(\"Gaussian process regression on noise-free dataset\")\n    plt.show()\n</code></pre> References <p>https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html [1] scikit-learn: Gaussian Processes regression: basic introductory example</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def __init__(\n        self: object,\n        noise: bool = False,\n        var_type: List[str] = [\"num\"],\n        name: str = \"kriging\",\n        seed: int = 124,\n        model_optimizer=None,\n        model_fun_evals: Optional[int] = None,\n        min_theta: float = -3.0,\n        max_theta: float = 2.0,\n        n_theta: int = 1,\n        theta_init_zero: bool = True,\n        p_val: float = 2.0,\n        n_p: int = 1,\n        optim_p: bool = False,\n        min_Lambda: float = 1e-9,\n        max_Lambda: float = 1.,\n        log_level: int = 50,\n        spot_writer=None,\n        counter=None,\n        metric_factorial=\"canberra\",\n        **kwargs\n):\n    \"\"\"\n    Initialize the Kriging surrogate.\n\n    Args:\n        noise (bool): Use regression instead of interpolation kriging. Defaults to False.\n        var_type (List[str]):\n            Variable type. Can be either \"num\" (numerical) or \"factor\" (factor).\n            Defaults to [\"num\"].\n        name (str):\n            Surrogate name. Defaults to \"kriging\".\n        seed (int):\n            Random seed. Defaults to 124.\n        model_optimizer (Optional[object]):\n            Optimizer on the surrogate. If None, differential_evolution is selected.\n        model_fun_evals (Optional[int]):\n            Number of iterations used by the optimizer on the surrogate.\n        min_theta (float):\n            Min log10 theta value. Defaults to -3.\n        max_theta (float):\n            Max log10 theta value. Defaults to 2.\n        n_theta (int):\n            Number of theta values. Defaults to 1.\n        theta_init_zero (bool):\n            Initialize theta with zero. Defaults to True.\n        p_val (float):\n            p value. Used as an initial value if optim_p = True. Otherwise as a constant. Defaults to 2.\n        n_p (int):\n            Number of p values. Defaults to 1.\n        optim_p (bool):\n            Determines whether p should be optimized. Deafults to False.\n        min_Lambda (float):\n            Min Lambda value. Defaults to 1e-9.\n        max_Lambda (float):\n            Max Lambda value. Defaults to 1.\n        log_level (int):\n            Logging level, e.g., 20 is \"INFO\". Defaults to 50 (\"CRITICAL\").\n        spot_writer (Optional[object]):\n            Spot writer. Defaults to None.\n        counter (Optional[int]):\n            Counter. Defaults to None.\n        metric_factorial (str):\n            Metric for factorial. Defaults to \"canberra\". Can be \"euclidean\",\n            \"cityblock\", seuclidean\", \"sqeuclidean\", \"cosine\",\n            \"correlation\", \"hamming\", \"jaccard\", \"jensenshannon\",\n            \"chebyshev\", \"canberra\", \"braycurtis\", \"mahalanobis\", \"matching\".\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            import matplotlib.pyplot as plt\n            from numpy import linspace, arange\n            rng = np.random.RandomState(1)\n            X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n            y = np.squeeze(X * np.sin(X))\n            training_indices = rng.choice(arange(y.size), size=6, replace=False)\n            X_train, y_train = X[training_indices], y[training_indices]\n            S = Kriging(name='kriging', seed=124)\n            S.fit(X_train, y_train)\n            mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n            plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n            plt.scatter(X_train, y_train, label=\"Observations\")\n            plt.plot(X, mean_prediction, label=\"Mean prediction\")\n            plt.fill_between(\n                X.ravel(),\n                mean_prediction - 1.96 * std_prediction,\n                mean_prediction + 1.96 * std_prediction,\n                alpha=0.5,\n                label=r\"95% confidence interval\",\n                )\n            plt.legend()\n            plt.xlabel(\"$x$\")\n            plt.ylabel(\"$f(x)$\")\n            _ = plt.title(\"Gaussian process regression on noise-free dataset\")\n            plt.show()\n\n    References:\n        https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html\n        [[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)]\n        scikit-learn: Gaussian Processes regression: basic introductory example\n\n    \"\"\"\n    super().__init__(name, seed, log_level)\n\n    self.noise = noise\n    self.var_type = var_type\n    self.name = name\n    self.seed = seed\n    self.log_level = log_level\n    self.spot_writer = spot_writer\n    self.counter = counter\n    self.metric_factorial = metric_factorial\n\n    self.sigma = 0\n    self.eps = sqrt(spacing(1))\n    self.min_theta = min_theta\n    self.max_theta = max_theta\n    self.min_p = 1\n    self.max_p = 2\n    self.min_Lambda = min_Lambda\n    self.max_Lambda = max_Lambda\n    self.n_theta = n_theta\n    self.p_val = p_val\n    self.n_p = n_p\n    self.optim_p = optim_p\n    self.theta_init_zero = theta_init_zero\n    # Psi matrix condition:\n    self.cnd_Psi = 0\n    self.inf_Psi = False\n\n    self.model_optimizer = model_optimizer\n    if self.model_optimizer is None:\n        self.model_optimizer = differential_evolution\n    self.model_fun_evals = model_fun_evals\n    # differential evolution uses maxiter = 1000\n    # and sets the number of function evaluations to\n    # (maxiter + 1) * popsize * N, which results in\n    # 1000 * 15 * k, because the default popsize is 15 and\n    # N is the number of parameters. This seems to be quite large:\n    # for k=2 these are 30 000 iterations. Therefore we set this value to\n    # 100\n    if self.model_fun_evals is None:\n        self.model_fun_evals = 100\n\n    # Logging information\n    self.log[\"negLnLike\"] = []\n    self.log[\"theta\"] = []\n    self.log[\"p\"] = []\n    self.log[\"Lambda\"] = []\n    # Logger\n    logger.setLevel(self.log_level)\n    logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.__is_any__","title":"<code>__is_any__(x, v)</code>","text":"<p>Check if any element in <code>x</code> is equal to <code>v</code>.</p> <p>This method checks if any element in the input array-like <code>x</code> is equal to the given value <code>v</code>. Converts inputs to numpy arrays as necessary.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, Any]</code> <p>The input array-like object to check.</p> required <code>v</code> <code>Any</code> <p>The value to check for in <code>x</code>.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if any element in <code>x</code> is equal to <code>v</code>, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    from numpy import power\n    import numpy as np\n    nat_X = np.array([[0], [1]])\n    nat_y = np.array([0, 1])\n    n=1\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n    S.initialize_variables(nat_X, nat_y)\n    S.set_variable_types()\n    S.set_theta_values()\n    print(f\"S.theta: {S.theta}\")\n    print(S.__is_any__(power(10.0, S.theta), 0))\n    print(S.__is_any__(S.theta, 0))\n        S.theta: [0.]\n        False\n        True\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def __is_any__(self, x: Union[np.ndarray, Any], v: Any) -&gt; bool:\n    \"\"\"\n    Check if any element in `x` is equal to `v`.\n\n    This method checks if any element in the input array-like `x`\n    is equal to the given value `v`. Converts inputs to numpy arrays as necessary.\n\n    Args:\n        x (Union[np.ndarray, Any]): The input array-like object to check.\n        v (Any): The value to check for in `x`.\n\n    Returns:\n        bool: True if any element in `x` is equal to `v`, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            from numpy import power\n            import numpy as np\n            nat_X = np.array([[0], [1]])\n            nat_y = np.array([0, 1])\n            n=1\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n            S.initialize_variables(nat_X, nat_y)\n            S.set_variable_types()\n            S.set_theta_values()\n            print(f\"S.theta: {S.theta}\")\n            print(S.__is_any__(power(10.0, S.theta), 0))\n            print(S.__is_any__(S.theta, 0))\n                S.theta: [0.]\n                False\n                True\n    \"\"\"\n\n    if not isinstance(x, np.ndarray):\n        x = np.array([x])  # Wrap scalar x in an array\n    return np.any(x == v)\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.build_Psi","title":"<code>build_Psi()</code>","text":"<p>Constructs a new (n x n) correlation matrix Psi to reflect new data or a change in hyperparameters. This method uses <code>theta</code>, <code>p</code>, and coded <code>X</code> values to construct the correlation matrix as described in [Forr08a, p.57].</p> <p>Attributes:</p> Name Type Description <code>Psi</code> <code>matrix</code> <p>Correlation matrix Psi. Shape (n,n).</p> <code>cnd_Psi</code> <code>float</code> <p>Condition number of Psi.</p> <code>inf_Psi</code> <code>bool</code> <p>True if Psi is infinite, False otherwise.</p> <p>Raises:</p> Type Description <code>LinAlgError</code> <p>If building Psi fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[0], [1]])\n    nat_y = np.array([0, 1])\n    n=1\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n    S.initialize_variables(nat_X, nat_y)\n    S.set_variable_types()\n    print(S.nat_X)\n    print(S.nat_y)\n    S.set_theta_values()\n    print(f\"S.theta: {S.theta}\")\n    S.initialize_matrices()\n    S.set_de_bounds()\n    new_theta_p_Lambda = S.optimize_model()\n    S.extract_from_bounds(new_theta_p_Lambda)\n    print(f\"S.theta: {S.theta}\")\n    S.build_Psi()\n    print(f\"S.Psi: {S.Psi}\")\n        [[0]\n        [1]]\n        [0 1]\n        S.theta: [0.]\n        S.theta: [1.60036366]\n        S.Psi: [[1.00000001e+00 4.96525625e-18]\n        [4.96525625e-18 1.00000001e+00]]\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def build_Psi(self) -&gt; None:\n    \"\"\"\n    Constructs a new (n x n) correlation matrix Psi to reflect new data\n    or a change in hyperparameters.\n    This method uses `theta`, `p`, and coded `X` values to construct the\n    correlation matrix as described in [Forr08a, p.57].\n\n    Attributes:\n        Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n        cnd_Psi (float): Condition number of Psi.\n        inf_Psi (bool): True if Psi is infinite, False otherwise.\n\n    Raises:\n        LinAlgError: If building Psi fails.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[0], [1]])\n            nat_y = np.array([0, 1])\n            n=1\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n            S.initialize_variables(nat_X, nat_y)\n            S.set_variable_types()\n            print(S.nat_X)\n            print(S.nat_y)\n            S.set_theta_values()\n            print(f\"S.theta: {S.theta}\")\n            S.initialize_matrices()\n            S.set_de_bounds()\n            new_theta_p_Lambda = S.optimize_model()\n            S.extract_from_bounds(new_theta_p_Lambda)\n            print(f\"S.theta: {S.theta}\")\n            S.build_Psi()\n            print(f\"S.Psi: {S.Psi}\")\n                [[0]\n                [1]]\n                [0 1]\n                S.theta: [0.]\n                S.theta: [1.60036366]\n                S.Psi: [[1.00000001e+00 4.96525625e-18]\n                [4.96525625e-18 1.00000001e+00]]\n    \"\"\"\n    try:\n        n = self.n\n        k = self.k\n        theta = np.power(10.0, self.theta)\n\n        # Ensure theta has the correct length\n        if self.n_theta == 1:\n            theta = theta * np.ones(k)\n\n        # Initialize the Psi matrix\n        self.Psi = np.zeros((n, n), dtype=np.float64)\n\n        # Calculate the distance matrix using ordered variables\n        if self.ordered_mask.any():\n            X_ordered = self.nat_X[:, self.ordered_mask]\n            D_ordered = squareform(\n                pdist(X_ordered, metric='sqeuclidean', w=theta[self.ordered_mask])\n            )\n            self.Psi += D_ordered\n\n        # Add the contribution of factor variables to the distance matrix\n        if self.factor_mask.any():\n            X_factor = self.nat_X[:, self.factor_mask]\n            D_factor = squareform(\n                pdist(X_factor, metric=self.metric_factorial, w=theta[self.factor_mask])\n            )\n            self.Psi += D_factor\n\n        # Calculate correlation from distance\n        self.Psi = np.exp(-self.Psi)\n\n        # Adjust diagonal elements for noise or minimum epsilon\n        diag_indices = np.diag_indices_from(self.Psi)\n        if self.noise:\n            self.Psi[diag_indices] += self.Lambda\n            logger.debug(\"Noise level Lambda applied to diagonal: %s\", self.Lambda)\n        else:\n            self.Psi[diag_indices] += self.eps\n\n        # Check for infinite values\n        self.inf_Psi = np.isinf(self.Psi).any()\n\n        # Calculate condition number\n        self.cnd_Psi = cond(self.Psi)\n        logger.debug(\"Condition number of Psi: %f\", self.cnd_Psi)\n\n    except LinAlgError as err:\n        logger.error(\"Building Psi failed. Error: %s, Type: %s\", err, type(err))\n        raise\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.build_U","title":"<code>build_U(scipy=True)</code>","text":"<p>Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57]. This method uses either <code>scipy_cholesky</code> or numpy\u2019s <code>cholesky</code> to perform the Cholesky factorization of Psi.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>scipy</code> <code>bool</code> <p>If True, use <code>scipy_cholesky</code>. If False, use numpy\u2019s <code>cholesky</code>. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>LinAlgError</code> <p>If Cholesky factorization fails for Psi.</p> <p>Attributes:</p> Name Type Description <code>U</code> <code>matrix</code> <p>Kriging U matrix, Cholesky decomposition. Shape (n,n).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[0], [1]])\n    nat_y = np.array([0, 1])\n    n=1\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n    S.initialize_variables(nat_X, nat_y)\n    S.set_variable_types()\n    print(S.nat_X)\n    print(S.nat_y)\n    S.set_theta_values()\n    print(f\"S.theta: {S.theta}\")\n    S.initialize_matrices()\n    S.set_de_bounds()\n    new_theta_p_Lambda = S.optimize_model()\n    S.extract_from_bounds(new_theta_p_Lambda)\n    print(f\"S.theta: {S.theta}\")\n    S.build_Psi()\n    print(f\"S.Psi: {S.Psi}\")\n    S.build_U()\n    print(f\"S.U:{S.U}\")\n        [[0]\n        [1]]\n        [0 1]\n        S.theta: [0.]\n        S.theta: [1.60036366]\n        S.Psi: [[1.00000001e+00 4.96525625e-18]\n        [4.96525625e-18 1.00000001e+00]]\n        S.U:[[1.00000001e+00 4.96525622e-18]\n        [0.00000000e+00 1.00000001e+00]]\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def build_U(self, scipy: bool = True) -&gt; None:\n    \"\"\"\n    Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57].\n    This method uses either `scipy_cholesky` or numpy's `cholesky` to perform the Cholesky factorization of Psi.\n\n    Args:\n        self (object):\n            The Kriging object.\n        scipy (bool):\n            If True, use `scipy_cholesky`.\n            If False, use numpy's `cholesky`.\n            Defaults to True.\n\n    Returns:\n        None\n\n    Raises:\n        LinAlgError:\n            If Cholesky factorization fails for Psi.\n\n    Attributes:\n        U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[0], [1]])\n            nat_y = np.array([0, 1])\n            n=1\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n            S.initialize_variables(nat_X, nat_y)\n            S.set_variable_types()\n            print(S.nat_X)\n            print(S.nat_y)\n            S.set_theta_values()\n            print(f\"S.theta: {S.theta}\")\n            S.initialize_matrices()\n            S.set_de_bounds()\n            new_theta_p_Lambda = S.optimize_model()\n            S.extract_from_bounds(new_theta_p_Lambda)\n            print(f\"S.theta: {S.theta}\")\n            S.build_Psi()\n            print(f\"S.Psi: {S.Psi}\")\n            S.build_U()\n            print(f\"S.U:{S.U}\")\n                [[0]\n                [1]]\n                [0 1]\n                S.theta: [0.]\n                S.theta: [1.60036366]\n                S.Psi: [[1.00000001e+00 4.96525625e-18]\n                [4.96525625e-18 1.00000001e+00]]\n                S.U:[[1.00000001e+00 4.96525622e-18]\n                [0.00000000e+00 1.00000001e+00]]\n    \"\"\"\n    try:\n        self.U = scipy_cholesky(self.Psi, lower=True) if scipy else cholesky(self.Psi)\n        self.U = self.U.T\n    except LinAlgError as err:\n        print(f\"build_U() Cholesky failed for Psi:\\n {self.Psi}. {err=}, {type(err)=}\")\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.build_psi_vec","title":"<code>build_psi_vec(cod_x)</code>","text":"<p>Build the psi vector required for predictive methods.</p> <p>Parameters:</p> Name Type Description Default <code>cod_x</code> <code>ndarray</code> <p>Point to calculate the psi vector for.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Modifies <p>self.psi (np.ndarray): Updates the psi vector.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.build.kriging import Kriging\n    X_train = np.array([[1., 2.],\n                        [2., 4.],\n                        [3., 6.]])\n    y_train = np.array([1., 2., 3.])\n    S = Kriging(name='kriging',\n                seed=123,\n                log_level=50,\n                n_theta=1,\n                noise=False,\n                cod_type=\"norm\")\n    S.fit(X_train, y_train)\n    # force theta to simple values:\n    S.theta = np.array([0.0])\n    nat_X = np.array([1., 0.])\n    S.psi = np.zeros((S.n, 1))\n    S.build_psi_vec(nat_X)\n    res = np.array([[np.exp(-4)],\n        [np.exp(-17)],\n        [np.exp(-40)]])\n    assert np.array_equal(S.psi, res)\n    print(f\"S.psi: {S.psi}\")\n    print(f\"Control value res: {res}\")\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def build_psi_vec(self, cod_x: np.ndarray) -&gt; None:\n    \"\"\"\n    Build the psi vector required for predictive methods.\n\n    Args:\n        cod_x (ndarray): Point to calculate the psi vector for.\n\n    Returns:\n        None\n\n    Modifies:\n        self.psi (np.ndarray): Updates the psi vector.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.build.kriging import Kriging\n            X_train = np.array([[1., 2.],\n                                [2., 4.],\n                                [3., 6.]])\n            y_train = np.array([1., 2., 3.])\n            S = Kriging(name='kriging',\n                        seed=123,\n                        log_level=50,\n                        n_theta=1,\n                        noise=False,\n                        cod_type=\"norm\")\n            S.fit(X_train, y_train)\n            # force theta to simple values:\n            S.theta = np.array([0.0])\n            nat_X = np.array([1., 0.])\n            S.psi = np.zeros((S.n, 1))\n            S.build_psi_vec(nat_X)\n            res = np.array([[np.exp(-4)],\n                [np.exp(-17)],\n                [np.exp(-40)]])\n            assert np.array_equal(S.psi, res)\n            print(f\"S.psi: {S.psi}\")\n            print(f\"Control value res: {res}\")\n    \"\"\"\n    logger.debug(\"Building psi vector for point: %s\", cod_x)\n    try:\n        self.psi = np.zeros((self.n, 1))\n        theta_scaled = np.power(10.0, self.theta)\n        if self.n_theta == 1:\n            theta_scaled = theta_scaled * np.ones(self.k)\n\n        D = np.zeros(self.n)\n\n        # Compute ordered distance contributions\n        if self.ordered_mask.any():\n            X_ordered = self.nat_X[:, self.ordered_mask]\n            x_ordered = cod_x[self.ordered_mask]\n            D += cdist(x_ordered.reshape(1, -1),\n                       X_ordered,\n                       metric='sqeuclidean',\n                       w=theta_scaled[self.ordered_mask]).ravel()\n        logger.debug(\"Distance D after ordered mask: %s\", D)\n        # Compute factor distance contributions\n        if self.factor_mask.any():\n            X_factor = self.nat_X[:, self.factor_mask]\n            x_factor = cod_x[self.factor_mask]\n            D += cdist(x_factor.reshape(1, -1),\n                       X_factor,\n                       metric=self.metric_factorial,\n                       w=theta_scaled[self.factor_mask]).ravel()\n        logger.debug(\"Distance D after factor mask: %s\", D)\n\n        self.psi = np.exp(-D).reshape(-1, 1)\n\n    except np.linalg.LinAlgError as err:\n        logger.error(\"Building psi failed due to a linear algebra error: %s. Error type: %s\", err, type(err))\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.exp_imp","title":"<code>exp_imp(y0, s0)</code>","text":"<p>Calculates the expected improvement for a given function value and error in coded units.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>y0</code> <code>float</code> <p>The function value in coded units.</p> required <code>s0</code> <code>float</code> <p>The error value.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The expected improvement value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    S = Kriging(name='kriging', seed=124)\n    S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n    S.exp_imp(1.0, 0.0)\n    0.0\n&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    S = Kriging(name='kriging', seed=124)\n    S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n    # assert S.exp_imp(0.0, 1.0) == 1/np.sqrt(2*np.pi)\n    # which is approx. 0.3989422804014327\n    S.exp_imp(0.0, 1.0)\n    0.3989422804014327\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def exp_imp(self, y0: float, s0: float) -&gt; float:\n    \"\"\"\n    Calculates the expected improvement for a given function value and error in coded units.\n\n    Args:\n        self (object): The Kriging object.\n        y0 (float): The function value in coded units.\n        s0 (float): The error value.\n\n    Returns:\n        float: The expected improvement value.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            S = Kriging(name='kriging', seed=124)\n            S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n            S.exp_imp(1.0, 0.0)\n            0.0\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            S = Kriging(name='kriging', seed=124)\n            S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n            # assert S.exp_imp(0.0, 1.0) == 1/np.sqrt(2*np.pi)\n            # which is approx. 0.3989422804014327\n            S.exp_imp(0.0, 1.0)\n            0.3989422804014327\n    \"\"\"\n    # We do not use the min y values, but the aggragated mean values\n    # y_min = min(self.nat_y)\n    y_min = min(self.aggregated_mean_y)\n    if s0 &lt;= 0.0:\n        EI = 0.0\n    elif s0 &gt; 0.0:\n        EI_one = (y_min - y0) * (\n                0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * ((y_min - y0) / s0))\n        )\n        EI_two = (s0 * (1.0 / sqrt(2.0 * pi))) * (\n            exp(-(1.0 / 2.0) * ((y_min - y0) ** 2.0 / s0 ** 2.0))\n        )\n        EI = EI_one + EI_two\n    return EI\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.extract_from_bounds","title":"<code>extract_from_bounds(new_theta_p_Lambda)</code>","text":"<p>Extract <code>theta</code>, <code>p</code>, and <code>Lambda</code> from bounds. The kriging object stores <code>theta</code> as an array,  <code>p</code> as an array, and <code>Lambda</code> as a float.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>new_theta_p_Lambda</code> <code>ndarray</code> <p>1d-array with theta, p, and Lambda values. Order is important.</p> required <p>Returns:     None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.build.kriging import Kriging\n    import logging\n    logging.basicConfig(level=logging.DEBUG)\n    # Define the number of theta and p parameters\n    num_theta = 2\n    num_p = 3\n    # Initialize the Kriging model\n    kriging_model = Kriging(\n        name='kriging',\n        seed=124,\n        n_theta=num_theta,\n        n_p=num_p,\n        optim_p=True,\n        noise=True\n    )\n    # Create bounds array\n    bounds_array = np.array([1, 2, 3, 4, 5, 6])\n    # Extract parameters from given bounds\n    kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n    # Assertions to check if parameters are correctly extracted\n    assert np.array_equal(kriging_model.theta,\n        [1, 2]), f\"Expected theta to be [1, 2] but got {kriging_model.theta}\"\n    assert np.array_equal(kriging_model.p,\n        [3, 4, 5]), f\"Expected p to be [3, 4, 5] but got {kriging_model.p}\"\n    assert kriging_model.Lambda == 6, f\"Expected Lambda to be 6 but got {kriging_model.Lambda}\"\n    print(\"All assertions passed!\")\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def extract_from_bounds(self, new_theta_p_Lambda: np.ndarray) -&gt; None:\n    \"\"\"\n    Extract `theta`, `p`, and `Lambda` from bounds. The kriging object stores\n    `theta` as an array,  `p` as an array, and `Lambda` as a float.\n\n    Args:\n        self (object): The Kriging object.\n        new_theta_p_Lambda (np.ndarray):\n            1d-array with theta, p, and Lambda values. Order is important.\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.build.kriging import Kriging\n            import logging\n            logging.basicConfig(level=logging.DEBUG)\n            # Define the number of theta and p parameters\n            num_theta = 2\n            num_p = 3\n            # Initialize the Kriging model\n            kriging_model = Kriging(\n                name='kriging',\n                seed=124,\n                n_theta=num_theta,\n                n_p=num_p,\n                optim_p=True,\n                noise=True\n            )\n            # Create bounds array\n            bounds_array = np.array([1, 2, 3, 4, 5, 6])\n            # Extract parameters from given bounds\n            kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n            # Assertions to check if parameters are correctly extracted\n            assert np.array_equal(kriging_model.theta,\n                [1, 2]), f\"Expected theta to be [1, 2] but got {kriging_model.theta}\"\n            assert np.array_equal(kriging_model.p,\n                [3, 4, 5]), f\"Expected p to be [3, 4, 5] but got {kriging_model.p}\"\n            assert kriging_model.Lambda == 6, f\"Expected Lambda to be 6 but got {kriging_model.Lambda}\"\n            print(\"All assertions passed!\")\n    \"\"\"\n    logger.debug(\"Extracting parameters from: %s\", new_theta_p_Lambda)\n\n    # Validate array length\n    expected_length = self.n_theta\n    if self.optim_p:\n        expected_length += self.n_p\n    if self.noise:\n        expected_length += 1\n\n    if len(new_theta_p_Lambda) &lt; expected_length:\n        logger.error(\"Input array is too short. Expected at least %d elements, got %d.\",\n                     expected_length, len(new_theta_p_Lambda))\n        raise ValueError(f\"Input array must have at least {expected_length} elements.\")\n\n    # Extract theta\n    self.theta = new_theta_p_Lambda[:self.n_theta]\n    logger.debug(\"Extracted theta: %s\", self.theta)\n\n    if self.optim_p:\n        # Extract p if optim_p is True\n        self.p = new_theta_p_Lambda[self.n_theta:self.n_theta + self.n_p]\n        logger.debug(\"Extracted p: %s\", self.p)\n\n    if self.noise:\n        # Extract Lambda\n        lambda_index = self.n_theta + (self.n_p if self.optim_p else 0)\n        self.Lambda = new_theta_p_Lambda[lambda_index]\n        logger.debug(\"Extracted Lambda: %s\", self.Lambda)\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.fit","title":"<code>fit(nat_X, nat_y)</code>","text":"<p>Fits the hyperparameters (<code>theta</code>, <code>p</code>, <code>Lambda</code>) of the Kriging model. The function computes the following internal values: 1. <code>theta</code>, <code>p</code>, and <code>Lambda</code> values via optimization of the function <code>fun_likelihood()</code>. 2. Correlation matrix <code>Psi</code> via <code>buildPsi()</code>. 3. U matrix via <code>buildU()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>nat_X</code> <code>ndarray</code> <p>Sample points.</p> required <code>nat_y</code> <code>ndarray</code> <p>Function values.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>Fitted estimator.</p> <p>Attributes:</p> Name Type Description <code>theta</code> <code>ndarray</code> <p>Kriging theta values. Shape (k,).</p> <code>p</code> <code>ndarray</code> <p>Kriging p values. Shape (k,).</p> <code>LnDetPsi</code> <code>float64</code> <p>Determinant Psi matrix.</p> <code>Psi</code> <code>matrix</code> <p>Correlation matrix Psi. Shape (n,n).</p> <code>psi</code> <code>ndarray</code> <p>psi vector. Shape (n,).</p> <code>one</code> <code>ndarray</code> <p>vector of ones. Shape (n,).</p> <code>mu</code> <code>float64</code> <p>Kriging expected mean value mu.</p> <code>U</code> <code>matrix</code> <p>Kriging U matrix, Cholesky decomposition. Shape (n,n).</p> <code>SigmaSqr</code> <code>float64</code> <p>Sigma squared value.</p> <code>Lambda</code> <code>float</code> <p>lambda noise value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[1, 0], [1, 0]])\n    nat_y = np.array([1, 2])\n    S = Kriging()\n    S.fit(nat_X, nat_y)\n    print(S.Psi)\n    [[1.00000001 1.        ]\n    [1.         1.00000001]]\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def fit(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; object:\n    \"\"\"\n    Fits the hyperparameters (`theta`, `p`, `Lambda`) of the Kriging model.\n    The function computes the following internal values:\n    1. `theta`, `p`, and `Lambda` values via optimization of the function `fun_likelihood()`.\n    2. Correlation matrix `Psi` via `buildPsi()`.\n    3. U matrix via `buildU()`.\n\n    Args:\n        self (object): The Kriging object.\n        nat_X (np.ndarray): Sample points.\n        nat_y (np.ndarray): Function values.\n\n    Returns:\n        object: Fitted estimator.\n\n    Attributes:\n        theta (np.ndarray): Kriging theta values. Shape (k,).\n        p (np.ndarray): Kriging p values. Shape (k,).\n        LnDetPsi (np.float64): Determinant Psi matrix.\n        Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n        psi (np.ndarray): psi vector. Shape (n,).\n        one (np.ndarray): vector of ones. Shape (n,).\n        mu (np.float64): Kriging expected mean value mu.\n        U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n        SigmaSqr (np.float64): Sigma squared value.\n        Lambda (float): lambda noise value.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[1, 0], [1, 0]])\n            nat_y = np.array([1, 2])\n            S = Kriging()\n            S.fit(nat_X, nat_y)\n            print(S.Psi)\n            [[1.00000001 1.        ]\n            [1.         1.00000001]]\n\n    \"\"\"\n    logger.debug(\"In fit(): nat_X: %s\", nat_X)\n    logger.debug(\"In fit(): nat_y: %s\", nat_y)\n    self.initialize_variables(nat_X, nat_y)\n    self.set_variable_types()\n    self.set_theta_values()\n    self.initialize_matrices()\n    # build_Psi() and build_U() are called in fun_likelihood\n    self.set_de_bounds()\n    # Finally, set new theta and p values and update the surrogate again\n    # for new_theta_p_Lambda in de_results[\"x\"]:\n    new_theta_p_Lambda = self.optimize_model()\n    self.extract_from_bounds(new_theta_p_Lambda)\n    self.build_Psi()\n    self.build_U()\n    # TODO: check if the following line is necessary!\n    self.likelihood()\n    self.update_log()\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.fun_likelihood","title":"<code>fun_likelihood(new_theta_p_Lambda)</code>","text":"<p>Compute log likelihood for a set of hyperparameters (theta, p, Lambda).</p> <p>This method computes the log likelihood for a set of hyperparameters (theta, p, Lambda) using several internal methods for matrix construction and likelihood evaluation. It handles potential errors by returning a penalty value for non-computable states.</p> <p>Parameters:</p> Name Type Description Default <code>new_theta_p_Lambda</code> <code>ndarray</code> <p>An array containing <code>theta</code>, <code>p</code>, and <code>Lambda</code> values.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The negative log likelihood or the penalty value if computation fails.</p> <p>Attributes:</p> Name Type Description <code>theta</code> <code>ndarray</code> <p>Kriging theta values. Shape (k,).</p> <code>p</code> <code>ndarray</code> <p>Kriging p values. Shape (k,).</p> <code>Lambda</code> <code>float</code> <p>lambda noise value.</p> <code>Psi</code> <code>matrix</code> <p>Correlation matrix Psi. Shape (n,n).</p> <code>U</code> <code>matrix</code> <p>Kriging U matrix, Cholesky decomposition. Shape (n,n).</p> <code>negLnLike</code> <code>float</code> <p>Negative log likelihood of the surface at the specified hyperparameters.</p> <code>pen_val</code> <code>float</code> <p>Penalty value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[0], [1]])\n    nat_y = np.array([0, 1])\n    n=1\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n    S.initialize_variables(nat_X, nat_y)\n    S.set_variable_types()\n    print(S.nat_X)\n    print(S.nat_y)\n    S.set_theta_values()\n    print(f\"S.theta: {S.theta}\")\n    S.initialize_matrices()\n    S.set_de_bounds()\n    new_theta_p_Lambda = S.optimize_model()\n    S.extract_from_bounds(new_theta_p_Lambda)\n    print(f\"S.theta: {S.theta}\")\n    S.build_Psi()\n    print(f\"S.Psi: {S.Psi}\")\n    S.build_U()\n    print(f\"S.U:{S.U}\")\n    S.likelihood()\n    S.negLnLike\n        [[0]\n        [1]]\n        [0 1]\n        S.theta: [0.]\n        S.theta: [1.60036366]\n        S.Psi: [[1.00000001e+00 4.96525625e-18]\n        [4.96525625e-18 1.00000001e+00]]\n        S.U:[[1.00000001e+00 4.96525622e-18]\n        [0.00000000e+00 1.00000001e+00]]\n        -1.3862943611198906\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def fun_likelihood(self, new_theta_p_Lambda: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute log likelihood for a set of hyperparameters (theta, p, Lambda).\n\n    This method computes the log likelihood for a set of hyperparameters\n    (theta, p, Lambda) using several internal methods for matrix construction\n    and likelihood evaluation. It handles potential errors by returning a\n    penalty value for non-computable states.\n\n    Args:\n        new_theta_p_Lambda (np.ndarray): An array containing `theta`, `p`, and `Lambda` values.\n\n    Returns:\n        float: The negative log likelihood or the penalty value if computation fails.\n\n    Attributes:\n        theta (np.ndarray): Kriging theta values. Shape (k,).\n        p (np.ndarray): Kriging p values. Shape (k,).\n        Lambda (float): lambda noise value.\n        Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n        U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n        negLnLike (float): Negative log likelihood of the surface at the specified hyperparameters.\n        pen_val (float): Penalty value.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[0], [1]])\n            nat_y = np.array([0, 1])\n            n=1\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n            S.initialize_variables(nat_X, nat_y)\n            S.set_variable_types()\n            print(S.nat_X)\n            print(S.nat_y)\n            S.set_theta_values()\n            print(f\"S.theta: {S.theta}\")\n            S.initialize_matrices()\n            S.set_de_bounds()\n            new_theta_p_Lambda = S.optimize_model()\n            S.extract_from_bounds(new_theta_p_Lambda)\n            print(f\"S.theta: {S.theta}\")\n            S.build_Psi()\n            print(f\"S.Psi: {S.Psi}\")\n            S.build_U()\n            print(f\"S.U:{S.U}\")\n            S.likelihood()\n            S.negLnLike\n                [[0]\n                [1]]\n                [0 1]\n                S.theta: [0.]\n                S.theta: [1.60036366]\n                S.Psi: [[1.00000001e+00 4.96525625e-18]\n                [4.96525625e-18 1.00000001e+00]]\n                S.U:[[1.00000001e+00 4.96525622e-18]\n                [0.00000000e+00 1.00000001e+00]]\n                -1.3862943611198906\n    \"\"\"\n    # Extract hyperparameters\n    self.extract_from_bounds(new_theta_p_Lambda)\n    # Check transformed theta values\n    theta_scaled = np.power(10.0, self.theta)\n    if self.__is_any__(theta_scaled, 0):\n        logger.warning(\"Failure in fun_likelihood: 10^theta == 0. Setting negLnLike to %s\", self.pen_val)\n        return self.pen_val\n    # Build Psi matrix and check its condition\n    self.build_Psi()\n    if getattr(self, 'inf_Psi', False) or getattr(self, 'cnd_Psi', float('inf')) &gt; 1e9:\n        logger.warning(\"Failure in fun_likelihood: Psi is ill-conditioned: %s\", getattr(self, 'cnd_Psi', 'unknown'))\n        logger.warning(\"Setting negLnLike to: %s\", self.pen_val)\n        return self.pen_val\n    # Build U matrix and handle exceptions\n    try:\n        self.build_U()\n    except Exception as error:\n        logger.error(\"Error in fun_likelihood(). Call to build_U() failed: %s\", error)\n        logger.error(\"Setting negLnLike to %.2f.\", self.pen_val)\n        return self.pen_val\n\n    # Calculate likelihood\n    self.likelihood()\n    return self.negLnLike\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.initialize_matrices","title":"<code>initialize_matrices()</code>","text":"<p>Initialize the matrices for the class instance.</p> <p>This method initializes several matrices and attributes for the class instance. The <code>p</code> attribute is initialized as a list of ones with length <code>n_p</code>, multiplied by 2.0. The <code>pen_val</code> attribute is initialized as the natural logarithm of the variance of <code>nat_y</code>, multiplied by <code>n</code>, plus 1e4. The <code>negLnLike</code>, <code>LnDetPsi</code>, <code>mu</code>, <code>U</code>, <code>SigmaSqr</code>, and <code>Lambda</code> attributes are all set to None. The <code>gen</code> attribute is initialized using the <code>SpaceFilling</code> function with arguments <code>k</code> and <code>seed</code>. The <code>Psi</code> attribute is initialized as a zero matrix with shape <code>(n, n)</code> and dtype <code>float64</code>. The <code>psi</code> attribute is initialized as a zero matrix with shape <code>(n, 1)</code>. The <code>one</code> attribute is initialized as a list of ones with length <code>n</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    from numpy import log, var\n    nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n    nat_y = np.array([1, 2, 3])\n    n=3\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n    S.initialize_variables(nat_X, nat_y)\n    S.set_variable_types()\n    S.set_theta_values()\n    S.initialize_matrices()\n    # if var(self.nat_y) is &gt; 0, then self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n    # else self.pen_val = self.n * var(self.nat_y) + 1e4\n    assert S.pen_val == nat_X.shape[0] * log(var(S.nat_y)) + 1e4\n    assert S.Psi.shape == (n, n)\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def initialize_matrices(self) -&gt; None:\n    \"\"\"\n    Initialize the matrices for the class instance.\n\n    This method initializes several matrices and attributes for the class instance.\n    The `p` attribute is initialized as a list of ones with length `n_p`, multiplied by 2.0.\n    The `pen_val` attribute is initialized as the natural logarithm of the\n    variance of `nat_y`, multiplied by `n`, plus 1e4.\n    The `negLnLike`, `LnDetPsi`, `mu`, `U`, `SigmaSqr`, and `Lambda` attributes are all set to None.\n    The `gen` attribute is initialized using the `SpaceFilling` function with arguments `k` and `seed`.\n    The `Psi` attribute is initialized as a zero matrix with shape `(n, n)` and dtype `float64`.\n    The `psi` attribute is initialized as a zero matrix with shape `(n, 1)`.\n    The `one` attribute is initialized as a list of ones with length `n`.\n\n    Args:\n        self (object): The Kriging object.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            from numpy import log, var\n            nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n            nat_y = np.array([1, 2, 3])\n            n=3\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n            S.initialize_variables(nat_X, nat_y)\n            S.set_variable_types()\n            S.set_theta_values()\n            S.initialize_matrices()\n            # if var(self.nat_y) is &gt; 0, then self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n            # else self.pen_val = self.n * var(self.nat_y) + 1e4\n            assert S.pen_val == nat_X.shape[0] * log(var(S.nat_y)) + 1e4\n            assert S.Psi.shape == (n, n)\n\n    Returns:\n        None\n    \"\"\"\n    logger.debug(\"In initialize_matrices(): self.n_p: %s\", self.n_p)\n\n    # Initialize p\n    self.p = np.ones(self.n_p) * self.p_val\n    logger.debug(\"In initialize_matrices(): self.p: %s\", self.p)\n\n    # Calculate variance of nat_y\n    y_variance = var(self.nat_y)\n    logger.debug(\"In initialize_matrices(): var(self.nat_y): %s\", y_variance)\n\n    # Set penalty value based on variance\n    if y_variance &gt; 0:\n        self.pen_val = self.n * log(y_variance) + 1e4\n    else:\n        self.pen_val = self.n * y_variance + 1e4\n    logger.debug(\"In initialize_matrices(): self.pen_val: %s\", self.pen_val)\n\n    # Initialize other attributes\n    self.negLnLike = None\n    self.LnDetPsi = None\n    self.mu = None\n    self.U = None\n    self.SigmaSqr = None\n    self.Lambda = None\n\n    # Initialize generator\n    self.gen = SpaceFilling(k=self.k, seed=self.seed)\n    logger.debug(\"In initialize_matrices(): self.gen: %s\", self.gen)\n\n    # Initialize matrix Psi and vector psi\n    self.Psi = np.zeros((self.n, self.n), dtype=np.float64)\n    logger.debug(\"In initialize_matrices(): self.Psi shape: %s\", self.Psi.shape)\n\n    self.psi = np.zeros((self.n, 1), dtype=np.float64)\n    logger.debug(\"In initialize_matrices(): self.psi shape: %s\", self.psi.shape)\n\n    # Initialize one\n    self.one = np.ones(self.n, dtype=np.float64)\n    logger.debug(\"In initialize_matrices(): self.one: %s\", self.one)\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.initialize_variables","title":"<code>initialize_variables(nat_X, nat_y)</code>","text":"<p>Initialize variables for the class instance. This method takes in the independent and dependent variable data as input and initializes the class instance variables. It creates deep copies of the input data and stores them in the instance variables <code>nat_X</code> and <code>nat_y</code>. It also calculates the number of observations <code>n</code> and the number of independent variables <code>k</code> from the shape of <code>nat_X</code>. Finally, it creates empty arrays with the same shape as <code>nat_X</code> and <code>nat_y</code> and stores them in the instance variables <code>cod_X</code> and <code>cod_y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>nat_X</code> <code>ndarray</code> <p>The independent variable data.</p> required <code>nat_y</code> <code>ndarray</code> <p>The dependent variable data.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[1, 2], [3, 4]])\n    nat_y = np.array([1, 2])\n    S = Kriging()\n    S.initialize_variables(nat_X, nat_y)\n    print(f\"S.nat_X: {S.nat_X}\")\n    print(f\"S.nat_y: {S.nat_y}\")\n    S.nat_X: [[1 2]\n              [3 4]]\n    S.nat_y: [1 2]\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def initialize_variables(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; None:\n    \"\"\"\n    Initialize variables for the class instance.\n    This method takes in the independent and dependent variable data as input\n    and initializes the class instance variables.\n    It creates deep copies of the input data and stores them in the\n    instance variables `nat_X` and `nat_y`.\n    It also calculates the number of observations `n` and\n    the number of independent variables `k` from the shape of `nat_X`.\n    Finally, it creates empty arrays with the same shape as `nat_X`\n    and `nat_y` and stores them in the instance variables `cod_X` and `cod_y`.\n\n    Args:\n        self (object): The Kriging object.\n        nat_X (np.ndarray): The independent variable data.\n        nat_y (np.ndarray): The dependent variable data.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[1, 2], [3, 4]])\n            nat_y = np.array([1, 2])\n            S = Kriging()\n            S.initialize_variables(nat_X, nat_y)\n            print(f\"S.nat_X: {S.nat_X}\")\n            print(f\"S.nat_y: {S.nat_y}\")\n            S.nat_X: [[1 2]\n                      [3 4]]\n            S.nat_y: [1 2]\n\n    \"\"\"\n    # Validate input dimensions\n    if nat_X.ndim != 2 or nat_y.ndim != 1:\n        raise ValueError(\"nat_X must be a 2D array and nat_y must be a 1D array.\")\n    if nat_X.shape[0] != nat_y.shape[0]:\n        raise ValueError(\"The number of samples in nat_X and nat_y must be equal.\")\n\n    # Initialize instance variables\n    self.nat_X = copy.deepcopy(nat_X)\n    self.nat_y = copy.deepcopy(nat_y)\n    self.n, self.k = self.nat_X.shape\n\n    # Calculate and store min and max of X\n    self.min_X = np.min(self.nat_X, axis=0)\n    self.max_X = np.max(self.nat_X, axis=0)\n\n    # Calculate the aggregated mean of y\n    _, aggregated_mean_y, _ = aggregate_mean_var(X=self.nat_X, y=self.nat_y)\n    self.aggregated_mean_y = np.copy(aggregated_mean_y)\n\n    # Logging the initialized variables\n    logger.debug(\"In initialize_variables(): self.nat_X: %s\", self.nat_X)\n    logger.debug(\"In initialize_variables(): self.nat_y: %s\", self.nat_y)\n    logger.debug(\"In initialize_variables(): self.aggregated_mean_y: %s\", self.aggregated_mean_y)\n    logger.debug(\"In initialize_variables(): self.min_X: %s\", self.min_X)\n    logger.debug(\"In initialize_variables(): self.max_X: %s\", self.max_X)\n    logger.debug(\"In initialize_variables(): self.n: %d\", self.n)\n    logger.debug(\"In initialize_variables(): self.k: %d\", self.k)\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.likelihood","title":"<code>likelihood()</code>","text":"<p>Calculate the negative concentrated log-likelihood. Implements equation (2.32) from [Forr08a] to compute the negative of the concentrated log-likelihood. Updates <code>mu</code>, <code>SigmaSqr</code>, <code>LnDetPsi</code>, and <code>negLnLike</code>.</p> Note <p>Requires prior calls to <code>build_Psi</code> and <code>build_U</code>.</p> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float64</code> <p>Kriging expected mean value mu.</p> <code>SigmaSqr</code> <code>float64</code> <p>Sigma squared value.</p> <code>LnDetPsi</code> <code>float64</code> <p>Logarithm of the determinant of Psi matrix.</p> <code>negLnLike</code> <code>float</code> <p>Negative log likelihood of the surface at the specified hyperparameters.</p> <p>Raises:</p> Type Description <code>LinAlgError</code> <p>If matrix operations fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[1], [2]])\n    nat_y = np.array([5, 10])\n    n=2\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n    S.initialize_variables(nat_X, nat_y)\n    S.set_variable_types()\n    S.set_theta_values()\n    S.initialize_matrices()\n    S.build_Psi()\n    S.build_U()\n    S.likelihood()\n    assert np.allclose(S.mu, 7.5, atol=1e-6)\n    E = np.exp(1)\n    sigma2 = E / (E**2 - 1) * (25/4 + 25/4*E)\n    assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)\n    print(f\"S.LnDetPsi:{S.LnDetPsi}\")\n    print(f\"S.negLnLike:{S.negLnLike}\")\n        S.LnDetPsi:-0.1454134234019476\n        S.negLnLike:2.2185498738611282\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def likelihood(self) -&gt; None:\n    \"\"\"\n    Calculate the negative concentrated log-likelihood.\n    Implements equation (2.32) from [Forr08a] to compute the negative of the\n    concentrated log-likelihood. Updates `mu`, `SigmaSqr`, `LnDetPsi`, and `negLnLike`.\n\n    Note:\n        Requires prior calls to `build_Psi` and `build_U`.\n\n    Attributes:\n        mu (np.float64): Kriging expected mean value mu.\n        SigmaSqr (np.float64): Sigma squared value.\n        LnDetPsi (np.float64): Logarithm of the determinant of Psi matrix.\n        negLnLike (float): Negative log likelihood of the surface at the specified hyperparameters.\n\n    Raises:\n        LinAlgError: If matrix operations fail.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[1], [2]])\n            nat_y = np.array([5, 10])\n            n=2\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n            S.initialize_variables(nat_X, nat_y)\n            S.set_variable_types()\n            S.set_theta_values()\n            S.initialize_matrices()\n            S.build_Psi()\n            S.build_U()\n            S.likelihood()\n            assert np.allclose(S.mu, 7.5, atol=1e-6)\n            E = np.exp(1)\n            sigma2 = E / (E**2 - 1) * (25/4 + 25/4*E)\n            assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)\n            print(f\"S.LnDetPsi:{S.LnDetPsi}\")\n            print(f\"S.negLnLike:{S.negLnLike}\")\n                S.LnDetPsi:-0.1454134234019476\n                S.negLnLike:2.2185498738611282\n    \"\"\"\n    try:\n        # Solving linear equations for needed components\n        U_T_inv_one = solve(self.U.T, self.one)\n        U_T_inv_nat_y = solve(self.U.T, self.nat_y)\n        # Mean calculation: (2.20) in [Forr08a]\n        self.mu = (self.one.T @ solve(self.U, U_T_inv_nat_y)) / (self.one.T @ solve(self.U, U_T_inv_one))\n        # Residuals\n        cod_y_minus_mu = self.nat_y - self.one * self.mu\n        # Sigma squared calculation: (2.31) in [Forr08a]\n        self.SigmaSqr = (cod_y_minus_mu.T @ solve(self.U, solve(self.U.T, cod_y_minus_mu))) / self.n\n        # Log determinant of Psi: (2.32) in [Forr08a]\n        self.LnDetPsi = 2.0 * np.sum(np.log(np.abs(np.diag(self.U))))\n        # Negative log-likelihood calculation: simplified from (2.32)\n        self.negLnLike = 0.5 * (self.n * np.log(self.SigmaSqr) + self.LnDetPsi)\n        logger.debug(\"Likelihood calculated: mu=%s, SigmaSqr=%s, LnDetPsi=%s, negLnLike=%s\",\n                     self.mu, self.SigmaSqr, self.LnDetPsi, self.negLnLike)\n    except LinAlgError as error:\n        logger.error(\"LinAlgError in likelihood calculation: %s\", error)\n        raise\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.optimize_model","title":"<code>optimize_model()</code>","text":"<p>Optimize the model using the specified model_optimizer.</p> <p>This method uses the specified model_optimizer to optimize the likelihood function (<code>fun_likelihood</code>) with respect to the model parameters. The optimization is performed within the bounds specified by the attribute <code>de_bounds</code>. The result of the optimization is returned as a list or tuple of optimized parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[1, 2], [3, 4]])\n    nat_y = np.array([1, 2])\n    n=2\n    p=2\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n    S.initialize_variables(nat_X, nat_y)\n    S.set_variable_types()\n    S.set_theta_values()\n    S.initialize_matrices()\n    S.set_de_bounds()\n    new_theta_p_Lambda = S.optimize_model()\n    print(new_theta_p_Lambda)\n    [0.12167915 1.49467909 1.82808259 1.69648798 0.79564346]\n</code></pre> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[float]]</code> <p>result[\u201cx\u201d] (Union[List[float], Tuple[float]]): A list or tuple of optimized parameter values.</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def optimize_model(self) -&gt; Union[List[float], Tuple[float]]:\n    \"\"\"\n    Optimize the model using the specified model_optimizer.\n\n    This method uses the specified model_optimizer to optimize the\n    likelihood function (`fun_likelihood`) with respect to the model parameters.\n    The optimization is performed within the bounds specified by the attribute\n    `de_bounds`.\n    The result of the optimization is returned as a list or tuple of optimized parameter values.\n\n    Args:\n        self (object): The Kriging object.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[1, 2], [3, 4]])\n            nat_y = np.array([1, 2])\n            n=2\n            p=2\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n            S.initialize_variables(nat_X, nat_y)\n            S.set_variable_types()\n            S.set_theta_values()\n            S.initialize_matrices()\n            S.set_de_bounds()\n            new_theta_p_Lambda = S.optimize_model()\n            print(new_theta_p_Lambda)\n            [0.12167915 1.49467909 1.82808259 1.69648798 0.79564346]\n\n    Returns:\n        result[\"x\"] (Union[List[float], Tuple[float]]):\n            A list or tuple of optimized parameter values.\n    \"\"\"\n    logger.debug(\"Entering optimize_model.\")\n    if not callable(self.model_optimizer):\n        logger.error(\"model_optimizer is not callable.\")\n        raise ValueError(\"model_optimizer must be a callable function or method.\")\n\n    optimizer_strategies: Dict[str, Dict] = {\n        'dual_annealing': {'func': self.fun_likelihood, 'bounds': self.de_bounds},\n        'differential_evolution': {\n            'func': self.fun_likelihood,\n            'bounds': self.de_bounds,\n            'maxiter': self.model_fun_evals,\n            'seed': self.seed\n        },\n        'direct': {\n            'func': self.fun_likelihood,\n            'bounds': self.de_bounds,\n            'eps': 1e-2\n        },\n        'shgo': {'func': self.fun_likelihood, 'bounds': self.de_bounds},\n        'basinhopping': {'func': self.fun_likelihood, 'x0': np.mean(self.de_bounds, axis=1)}\n    }\n\n    optimizer_name = self.model_optimizer.__name__\n    logger.debug(\"Optimizer selected: %s\", optimizer_name)\n\n    if optimizer_name not in optimizer_strategies:\n        logger.info(\"Using default options for optimizer: %s\", optimizer_name)\n        optimizer_args = {'func': self.fun_likelihood, 'bounds': self.de_bounds}\n    else:\n        optimizer_args = optimizer_strategies[optimizer_name]\n\n    logger.debug(\"Parameters for optimization: %s\", optimizer_args)\n\n    try:\n        result = self.model_optimizer(**optimizer_args)\n    except Exception as e:\n        logger.error(\"Optimization failed due to error: %s\", str(e))\n        raise\n\n    if \"x\" not in result:\n        logger.error(\"Optimization result does not contain 'x'. Result: %s\", result)\n        raise ValueError(\"The optimization result does not contain the expected 'x' key.\")\n    logger.debug(\"Optimization result: %s\", result)\n    optimized_parameters = list(result[\"x\"])\n    logger.debug(\"Extracted optimized parameters: %s\", optimized_parameters)\n    return optimized_parameters\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.plot","title":"<code>plot(show=True)</code>","text":"<p>This function plots 1D and 2D surrogates.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>show</code> <code>bool</code> <p>If <code>True</code>, the plots are displayed. If <code>False</code>, <code>plt.show()</code> should be called outside this function.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Note <ul> <li>This method provides only a basic plot. For more advanced plots,     use the <code>plot_contour()</code> method of the <code>Spot</code> class.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init, design_control_init\n    # 1-dimensional example\n    fun = analytical().fun_sphere\n    fun_control=fun_control_init(lower = np.array([-1]),\n                                upper = np.array([1]),\n                                noise=False)\n    design_control=design_control_init(init_size=10)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control)\n    S.initialize_design()\n    S.update_stats()\n    S.fit_surrogate()\n    S.surrogate.plot()\n    # 2-dimensional example\n    fun = analytical().fun_sphere\n    fun_control=fun_control_init(lower = np.array([-1, -1]),\n                                upper = np.array([1, 1]),\n                                noise=False)\n    design_control=design_control_init(init_size=10)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control)\n    S.initialize_design()\n    S.update_stats()\n    S.fit_surrogate()\n    S.surrogate.plot()\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def plot(self, show: Optional[bool] = True) -&gt; None:\n    \"\"\"\n    This function plots 1D and 2D surrogates.\n\n    Args:\n        self (object):\n            The Kriging object.\n        show (bool):\n            If `True`, the plots are displayed.\n            If `False`, `plt.show()` should be called outside this function.\n\n    Returns:\n        None\n\n    Note:\n        * This method provides only a basic plot. For more advanced plots,\n            use the `plot_contour()` method of the `Spot` class.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init, design_control_init\n            # 1-dimensional example\n            fun = analytical().fun_sphere\n            fun_control=fun_control_init(lower = np.array([-1]),\n                                        upper = np.array([1]),\n                                        noise=False)\n            design_control=design_control_init(init_size=10)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control)\n            S.initialize_design()\n            S.update_stats()\n            S.fit_surrogate()\n            S.surrogate.plot()\n            # 2-dimensional example\n            fun = analytical().fun_sphere\n            fun_control=fun_control_init(lower = np.array([-1, -1]),\n                                        upper = np.array([1, 1]),\n                                        noise=False)\n            design_control=design_control_init(init_size=10)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control)\n            S.initialize_design()\n            S.update_stats()\n            S.fit_surrogate()\n            S.surrogate.plot()\n    \"\"\"\n    if self.k == 1:\n        # TODO: Improve plot (add conf. interval etc.)\n        fig = pylab.figure(figsize=(9, 6))\n        n_grid = 100\n        x = linspace(\n            self.min_X[0], self.max_X[0], num=n_grid\n        )\n        y = self.predict(x)\n        plt.figure()\n        plt.plot(x, y, \"k\")\n        if show:\n            plt.show()\n\n    if self.k == 2:\n        fig = pylab.figure(figsize=(9, 6))\n        n_grid = 100\n        x = linspace(\n            self.min_X[0], self.max_X[0], num=n_grid\n        )\n        y = linspace(\n            self.min_X[1], self.max_X[1], num=n_grid\n        )\n        X, Y = meshgrid(x, y)\n        # Predict based on the optimized results\n        zz = array(\n            [self.predict(array([x, y]), return_val=\"all\") for x, y in zip(ravel(X), ravel(Y))]\n        )\n        zs = zz[:, 0, :]\n        zse = zz[:, 1, :]\n        Z = zs.reshape(X.shape)\n        Ze = zse.reshape(X.shape)\n\n        nat_point_X = self.nat_X[:, 0]\n        nat_point_Y = self.nat_X[:, 1]\n        contour_levels = 30\n        ax = fig.add_subplot(224)\n        # plot predicted values:\n        pylab.contourf(X, Y, Ze, contour_levels, cmap=\"jet\")\n        pylab.title(\"Error\")\n        pylab.colorbar()\n        # plot observed points:\n        pylab.plot(nat_point_X, nat_point_Y, \"ow\")\n        #\n        ax = fig.add_subplot(223)\n        # plot predicted values:\n        plt.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\")\n        plt.title(\"Surrogate\")\n        # plot observed points:\n        pylab.plot(nat_point_X, nat_point_Y, \"ow\", zorder=3)\n        pylab.colorbar()\n        #\n        ax = fig.add_subplot(221, projection=\"3d\")\n        ax.plot_surface(X, Y, Z, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n        #\n        ax = fig.add_subplot(222, projection=\"3d\")\n        ax.plot_surface(X, Y, Ze, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n        #\n        pylab.show()\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.predict","title":"<code>predict(nat_X, return_val='y')</code>","text":"<p>This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>nat_X</code> <code>ndarray</code> <p>Design variable to evaluate in natural units.</p> required <code>return_val</code> <code>str</code> <p>Specifies which prediction values to return. It can be \u201cy\u201d, \u201cs\u201d, \u201cei\u201d, or \u201call\u201d.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>Union[float, Tuple[float, float]]</code> <p>Union[float, Tuple[float, float, float]]: Depending on <code>return_val</code>, returns the predicted value,</p> <code>Union[float, Tuple[float, float]]</code> <p>predicted error, expected improvement, or all.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>nat_X</code> is not an ndarray or doesn\u2019t match expected dimensions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    from numpy import linspace, arange\n    rng = np.random.RandomState(1)\n    X = linspace(start=0, stop=10, num=1_0).reshape(-1, 1)\n    y = np.squeeze(X * np.sin(X))\n    training_indices = rng.choice(arange(y.size), size=6, replace=False)\n    X_train, y_train = X[training_indices], y[training_indices]\n    S = Kriging(name='kriging', seed=124)\n    S.fit(X_train, y_train)\n    mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n    print(f\"mean_prediction: {mean_prediction}\")\n    print(f\"std_prediction: {std_prediction}\")\n    print(f\"s_ei: {s_ei}\")\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def predict(self, nat_X: ndarray, return_val: str = \"y\") -&gt; Union[float, Tuple[float, float]]:\n    \"\"\"\n    This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X.\n\n    Args:\n        self (object): The Kriging object.\n        nat_X (ndarray): Design variable to evaluate in natural units.\n        return_val (str): Specifies which prediction values to return. It can be \"y\", \"s\", \"ei\", or \"all\".\n\n    Returns:\n        Union[float, Tuple[float, float, float]]: Depending on `return_val`, returns the predicted value,\n        predicted error, expected improvement, or all.\n\n    Raises:\n        TypeError: If `nat_X` is not an ndarray or doesn't match expected dimensions.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            from numpy import linspace, arange\n            rng = np.random.RandomState(1)\n            X = linspace(start=0, stop=10, num=1_0).reshape(-1, 1)\n            y = np.squeeze(X * np.sin(X))\n            training_indices = rng.choice(arange(y.size), size=6, replace=False)\n            X_train, y_train = X[training_indices], y[training_indices]\n            S = Kriging(name='kriging', seed=124)\n            S.fit(X_train, y_train)\n            mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n            print(f\"mean_prediction: {mean_prediction}\")\n            print(f\"std_prediction: {std_prediction}\")\n            print(f\"s_ei: {s_ei}\")\n    \"\"\"\n    if not isinstance(nat_X, ndarray):\n        raise TypeError(f\"Expected an ndarray, got {type(nat_X)} instead.\")\n\n    try:\n        X = nat_X.reshape(-1, self.nat_X.shape[1])\n        X = repair_non_numeric(X, self.var_type)\n    except Exception as e:\n        raise TypeError(\"Input to predict was not convertible to the size of X\") from e\n\n    y, s, ei = self.predict_coded_batch(X)\n\n    if return_val == \"y\":\n        return y\n    elif return_val == \"s\":\n        return s\n    elif return_val == \"ei\":\n        return -ei\n    elif return_val == \"all\":\n        return y, s, -ei\n    else:\n        raise ValueError(f\"Invalid return_val: {return_val}. Supported values are 'y', 's', 'ei', 'all'.\")\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.predict_coded","title":"<code>predict_coded(cod_x)</code>","text":"<p>Kriging prediction of one point in coded units as described in (2.20) in [Forr08a]. The error is returned as well. The method is used in <code>predict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>cod_x</code> <code>ndarray</code> <p>Point in coded units to make prediction at.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float, float]</code> <p>Tuple[float, float, float]: Predicted value, predicted error, and expected improvement.</p> Note <p>Uses attributes such as <code>self.mu</code> and <code>self.SigmaSqr</code> that are expected to be calculated by <code>likelihood</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    from numpy import linspace, arange, empty\n    rng = np.random.RandomState(1)\n    X = linspace(start=0, stop=10, num=10).reshape(-1, 1)\n    y = np.squeeze(X * np.sin(X))\n    training_indices = rng.choice(arange(y.size), size=6, replace=False)\n    X_train, y_train = X[training_indices], y[training_indices]\n    S = Kriging(name='kriging', seed=124)\n    S.fit(X_train, y_train)\n    n = X.shape[0]\n    y = empty(n, dtype=float)\n    s = empty(n, dtype=float)\n    ei = empty(n, dtype=float)\n    for i in range(n):\n        y_coded, s_coded, ei_coded = S.predict_coded(X[i, :])\n        y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n        s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n        ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n    print(f\"y: {y}\")\n    print(f\"s: {s}\")\n    print(f\"ei: {-1.0*ei}\")\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def predict_coded(self, cod_x: np.ndarray) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    Kriging prediction of one point in coded units as described in (2.20) in [Forr08a].\n    The error is returned as well. The method is used in `predict`.\n\n    Args:\n        self (object): The Kriging object.\n        cod_x (np.ndarray): Point in coded units to make prediction at.\n\n    Returns:\n        Tuple[float, float, float]: Predicted value, predicted error, and expected improvement.\n\n    Note:\n        Uses attributes such as `self.mu` and `self.SigmaSqr` that are expected\n        to be calculated by `likelihood`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            from numpy import linspace, arange, empty\n            rng = np.random.RandomState(1)\n            X = linspace(start=0, stop=10, num=10).reshape(-1, 1)\n            y = np.squeeze(X * np.sin(X))\n            training_indices = rng.choice(arange(y.size), size=6, replace=False)\n            X_train, y_train = X[training_indices], y[training_indices]\n            S = Kriging(name='kriging', seed=124)\n            S.fit(X_train, y_train)\n            n = X.shape[0]\n            y = empty(n, dtype=float)\n            s = empty(n, dtype=float)\n            ei = empty(n, dtype=float)\n            for i in range(n):\n                y_coded, s_coded, ei_coded = S.predict_coded(X[i, :])\n                y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n                s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n                ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n            print(f\"y: {y}\")\n            print(f\"s: {s}\")\n            print(f\"ei: {-1.0*ei}\")\n    \"\"\"\n    self.build_psi_vec(cod_x)\n    mu_adj = self.mu\n    psi = self.psi\n\n    # Calculate the prediction\n    U_T_inv = solve(self.U.T, self.nat_y - self.one.dot(mu_adj))\n    f = mu_adj + psi.T.dot(solve(self.U, U_T_inv))[0]\n\n    Lambda = self.Lambda if self.noise else 0.0\n\n    # Calculate the estimated error\n    SSqr = self.SigmaSqr * (1 + Lambda - psi.T.dot(solve(self.U, solve(self.U.T, psi))))\n    SSqr = power(abs(SSqr), 0.5)[0]\n\n    # Calculate expected improvement\n    EI = self.exp_imp(y0=f, s0=SSqr)\n\n    return f, SSqr, EI\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.predict_coded_batch","title":"<code>predict_coded_batch(X)</code>","text":"<p>Vectorized prediction for batch input using coded units.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input array of coded points.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: Arrays of predicted values, predicted errors, and expected improvements.</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def predict_coded_batch(self, X: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Vectorized prediction for batch input using coded units.\n\n    Args:\n        X (np.ndarray): Input array of coded points.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]:\n            Arrays of predicted values, predicted errors, and expected improvements.\n    \"\"\"\n    n = X.shape[0]\n    y = np.empty(n, dtype=float)\n    s = np.empty(n, dtype=float)\n    ei = np.empty(n, dtype=float)\n\n    for i in range(n):\n        y_coded, s_coded, ei_coded = self.predict_coded(X[i, :])\n        y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n        s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n        ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n\n    return y, s, ei\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.set_de_bounds","title":"<code>set_de_bounds()</code>","text":"<p>Determine search bounds for model_optimizer, e.g., differential evolution. This method sets the attribute <code>de_bounds</code> of the object to a list of lists, where each inner list represents the lower and upper bounds for a parameter being optimized. The number of inner lists is determined by the number of parameters being optimized (<code>n_theta</code> and <code>n_p</code>), as well as whether noise is being considered (<code>noise</code>).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    S = Kriging(name='kriging', seed=124)\n    S.set_de_bounds()\n    print(S.de_bounds)\n    [[-3.0, 2.0]]\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def set_de_bounds(self) -&gt; None:\n    \"\"\"\n    Determine search bounds for model_optimizer, e.g., differential evolution.\n    This method sets the attribute `de_bounds` of the object to a list of lists,\n    where each inner list represents the lower and upper bounds for a parameter\n    being optimized. The number of inner lists is determined by the number of\n    parameters being optimized (`n_theta` and `n_p`), as well as whether noise is\n    being considered (`noise`).\n\n    Args:\n        self (object): The Kriging object.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            S = Kriging(name='kriging', seed=124)\n            S.set_de_bounds()\n            print(S.de_bounds)\n            [[-3.0, 2.0]]\n\n    Returns:\n        None\n    \"\"\"\n    logger.debug(\"In set_de_bounds(): self.min_theta: %s\", self.min_theta)\n    logger.debug(\"In set_de_bounds(): self.max_theta: %s\", self.max_theta)\n    logger.debug(\"In set_de_bounds(): self.n_theta: %s\", self.n_theta)\n    logger.debug(\"In set_de_bounds(): self.optim_p: %s\", self.optim_p)\n    logger.debug(\"In set_de_bounds(): self.min_p: %s\", self.min_p)\n    logger.debug(\"In set_de_bounds(): self.max_p: %s\", self.max_p)\n    logger.debug(\"In set_de_bounds(): self.n_p: %s\", self.n_p)\n    logger.debug(\"In set_de_bounds(): self.noise: %s\", self.noise)\n    logger.debug(\"In set_de_bounds(): self.min_Lambda: %s\", self.min_Lambda)\n    logger.debug(\"In set_de_bounds(): self.max_Lambda: %s\", self.max_Lambda)\n\n    de_bounds = [[self.min_theta, self.max_theta] for _ in range(self.n_theta)]\n    if self.optim_p:\n        de_bounds += [[self.min_p, self.max_p] for _ in range(self.n_p)]\n        if self.noise:\n            de_bounds.append([self.min_Lambda, self.max_Lambda])\n    else:\n        if self.noise:\n            de_bounds.append([self.min_Lambda, self.max_Lambda])\n    self.de_bounds = de_bounds\n    logger.debug(\"In set_de_bounds(): self.de_bounds: %s\", self.de_bounds)\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.set_theta_values","title":"<code>set_theta_values()</code>","text":"<p>Set the theta values for the class instance.</p> <p>This method sets the theta values for the class instance based on the <code>n_theta</code> and <code>k</code> attributes. If <code>n_theta</code> is greater than <code>k</code>, <code>n_theta</code> is set to <code>k</code> and a warning is logged. The method then initializes the <code>theta</code> attribute as a list of zeros with length <code>n_theta</code>. The <code>x0_theta</code> attribute is also initialized as a list of ones with length <code>n_theta</code>, multiplied by <code>n / (100 * k)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Returns:     None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    from numpy import array\n    nat_X = np.array([[1, 2], [3, 4]])\n    nat_y = np.array([1, 2])\n    n=2\n    p=2\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n    S.initialize_variables(nat_X, nat_y)\n    S.set_variable_types()\n    S.set_theta_values()\n    assert S.theta.all() == array([0., 0.]).all()\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def set_theta_values(self) -&gt; None:\n    \"\"\"\n    Set the theta values for the class instance.\n\n    This method sets the theta values for the class instance based\n    on the `n_theta` and `k` attributes. If `n_theta` is greater than\n    `k`, `n_theta` is set to `k` and a warning is logged.\n    The method then initializes the `theta` attribute as a list\n    of zeros with length `n_theta`.\n    The `x0_theta` attribute is also initialized as a list of ones\n    with length `n_theta`, multiplied by `n / (100 * k)`.\n\n    Args:\n        self (object): The Kriging object.\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            from numpy import array\n            nat_X = np.array([[1, 2], [3, 4]])\n            nat_y = np.array([1, 2])\n            n=2\n            p=2\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n            S.initialize_variables(nat_X, nat_y)\n            S.set_variable_types()\n            S.set_theta_values()\n            assert S.theta.all() == array([0., 0.]).all()\n    \"\"\"\n    logger.debug(\"In set_theta_values(): self.k: %s\", self.k)\n    logger.debug(\"In set_theta_values(): self.n_theta: %s\", self.n_theta)\n\n    # Adjust `n_theta` if it exceeds `k`\n    if self.n_theta &gt; self.k:\n        self.n_theta = self.k\n        logger.warning(\"Too few theta values or more theta values than dimensions. `n_theta` set to `k`.\")\n        logger.debug(\"In set_theta_values(): self.n_theta reset to: %s\", self.n_theta)\n\n    # Initialize theta values\n    if hasattr(self, \"theta_init_zero\") and self.theta_init_zero:\n        self.theta = np.zeros(self.n_theta, dtype=float)\n        logger.debug(\"Theta initialized to zeros: %s\", self.theta)\n    else:\n        logger.debug(\"In set_theta_values(): self.n: %s\", self.n)\n        self.theta = np.ones(self.n_theta, dtype=float) * self.n / (100 * self.k)\n        logger.debug(\"Theta initialized based on n and k: %s\", self.theta)\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.set_variable_types","title":"<code>set_variable_types()</code>","text":"<p>Set the variable types for the class instance. This method sets the variable types for the class instance based on the <code>var_type</code> attribute. If the length of <code>var_type</code> is less than <code>k</code>, all variable types are forced to \u2018num\u2019 and a warning is logged. The method then creates Boolean masks for each variable type (\u2018num\u2019, \u2018factor\u2019, \u2018int\u2019, \u2018ordered\u2019) using numpy arrays, e.g., <code>num_mask = array([ True,  True])</code> if two numerical variables are present.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    nat_X = np.array([[1, 2], [3, 4]])\n    nat_y = np.array([1, 2])\n    n=2\n    p=2\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n    S.initialize_variables(nat_X, nat_y)\n    S.set_variable_types()\n    assert S.var_type == ['num', 'num']\n    assert S.var_type == ['num', 'num']\n    assert S.num_mask.all() == True\n    assert S.factor_mask.all() == False\n    assert S.int_mask.all() == False\n    assert S.ordered_mask.all() == True\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def set_variable_types(self) -&gt; None:\n    \"\"\"\n    Set the variable types for the class instance.\n    This method sets the variable types for the class instance based\n    on the `var_type` attribute. If the length of `var_type` is less\n    than `k`, all variable types are forced to 'num' and a warning is logged.\n    The method then creates Boolean masks for each variable\n    type ('num', 'factor', 'int', 'ordered') using numpy arrays, e.g.,\n    `num_mask = array([ True,  True])` if two numerical variables are present.\n\n    Args:\n        self (object): The Kriging object.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            nat_X = np.array([[1, 2], [3, 4]])\n            nat_y = np.array([1, 2])\n            n=2\n            p=2\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n            S.initialize_variables(nat_X, nat_y)\n            S.set_variable_types()\n            assert S.var_type == ['num', 'num']\n            assert S.var_type == ['num', 'num']\n            assert S.num_mask.all() == True\n            assert S.factor_mask.all() == False\n            assert S.int_mask.all() == False\n            assert S.ordered_mask.all() == True\n\n    Returns:\n        None\n    \"\"\"\n    logger.debug(\"In set_variable_types(): self.k: %s\", self.k)\n    logger.debug(\"In set_variable_types(): self.var_type: %s\", self.var_type)\n\n    # Ensure var_type has appropriate length by defaulting to 'num'\n    if len(self.var_type) &lt; self.k:\n        self.var_type = ['num'] * self.k  # Corrected to fill with 'num' instead of duplicating\n        logger.warning(\"In set_variable_types(): All variable types forced to 'num'.\")\n        logger.debug(\"In set_variable_types(): self.var_type: %s\", self.var_type)\n    # Create masks for each type using numpy vectorized operations\n    var_type_array = np.array(self.var_type)\n    self.num_mask = (var_type_array == \"num\")\n    self.factor_mask = (var_type_array == \"factor\")\n    self.int_mask = (var_type_array == \"int\")\n    self.ordered_mask = np.isin(var_type_array, [\"int\", \"num\", \"float\"])\n    logger.debug(\"In set_variable_types(): self.num_mask: %s\", self.num_mask)\n    logger.debug(\"In set_variable_types(): self.factor_mask: %s\", self.factor_mask)\n    logger.debug(\"In set_variable_types(): self.int_mask: %s\", self.int_mask)\n    logger.debug(\"In set_variable_types(): self.ordered_mask: %s\", self.ordered_mask)\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.update_log","title":"<code>update_log()</code>","text":"<p>Update the log with the current values of negLnLike, theta, p, and Lambda. This method appends the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to their respective lists in the log dictionary. It also updates the log_length attribute with the current length of the negLnLike list in the log. If spot_writer is not None, this method also writes the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to the spot_writer object.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[1, 2], [3, 4]])\n    nat_y = np.array([1, 2])\n    n=2\n    p=2\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n    S.initialize_variables(nat_X, nat_y)\n    S.set_variable_types()\n    S.set_theta_values()\n    S.initialize_matrices()\n    S.set_de_bounds()\n    new_theta_p_Lambda = S.optimize_model()\n    S.update_log()\n    print(S.log)\n    {'negLnLike': array([-1.38629436]),\n     'theta': array([-1.14525993,  1.6123372 ]),\n      'p': array([1.84444406, 1.74590865]),\n      'Lambda': array([0.44268472])}\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def update_log(self) -&gt; None:\n    \"\"\"\n    Update the log with the current values of negLnLike, theta, p, and Lambda.\n    This method appends the current values of negLnLike, theta, p (if optim_p is True),\n    and Lambda (if noise is True)\n    to their respective lists in the log dictionary.\n    It also updates the log_length attribute with the current length\n    of the negLnLike list in the log.\n    If spot_writer is not None, this method also writes the current values of\n    negLnLike, theta, p (if optim_p is True),\n    and Lambda (if noise is True) to the spot_writer object.\n\n    Args:\n        self (object): The Kriging object.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[1, 2], [3, 4]])\n            nat_y = np.array([1, 2])\n            n=2\n            p=2\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n            S.initialize_variables(nat_X, nat_y)\n            S.set_variable_types()\n            S.set_theta_values()\n            S.initialize_matrices()\n            S.set_de_bounds()\n            new_theta_p_Lambda = S.optimize_model()\n            S.update_log()\n            print(S.log)\n            {'negLnLike': array([-1.38629436]),\n             'theta': array([-1.14525993,  1.6123372 ]),\n              'p': array([1.84444406, 1.74590865]),\n              'Lambda': array([0.44268472])}\n\n    \"\"\"\n    self.log[\"negLnLike\"] = append(self.log[\"negLnLike\"], self.negLnLike)\n    self.log[\"theta\"] = append(self.log[\"theta\"], self.theta)\n    if self.optim_p:\n        self.log[\"p\"] = append(self.log[\"p\"], self.p)\n    if self.noise:\n        self.log[\"Lambda\"] = append(self.log[\"Lambda\"], self.Lambda)\n    # get the length of the log\n    self.log_length = len(self.log[\"negLnLike\"])\n    if self.spot_writer is not None:\n        negLnLike = self.negLnLike.copy()\n        self.spot_writer.add_scalar(\"spot_negLnLike\", negLnLike, self.counter+self.log_length)\n        # add the self.n_theta theta values to the writer with one key \"theta\",\n        # i.e, the same key for all theta values\n        theta = self.theta.copy()\n        self.spot_writer.add_scalars(\"spot_theta\", {f\"theta_{i}\": theta[i] for i in range(self.n_theta)},\n                                     self.counter+self.log_length)\n        if self.noise:\n            Lambda = self.Lambda.copy()\n            self.spot_writer.add_scalar(\"spot_Lambda\", Lambda, self.counter+self.log_length)\n        if self.optim_p:\n            p = self.p.copy()\n            self.spot_writer.add_scalars(\"spot_p\",\n                                         {f\"p_{i}\": p[i] for i in range(self.n_p)}, self.counter+self.log_length)\n        self.spot_writer.flush()\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.weighted_exp_imp","title":"<code>weighted_exp_imp(cod_x, w)</code>","text":"<p>Weighted expected improvement. Currently not used in <code>spotpython</code></p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>cod_x</code> <code>ndarray</code> <p>A coded design vector.</p> required <code>w</code> <code>float</code> <p>Weight.</p> required <p>Returns:</p> Name Type Description <code>EI</code> <code>float</code> <p>Weighted expected improvement.</p> References <p>[Sobester et al. 2005].</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def weighted_exp_imp(self, cod_x: np.ndarray, w: float) -&gt; float:\n    \"\"\"\n    Weighted expected improvement. Currently not used in `spotpython`\n\n    Args:\n        self (object): The Kriging object.\n        cod_x (np.ndarray): A coded design vector.\n        w (float): Weight.\n\n    Returns:\n        EI (float): Weighted expected improvement.\n\n    References:\n        [Sobester et al. 2005].\n    \"\"\"\n    y0, s0 = self.predict_coded(cod_x)\n    y_min = min(self.nat_y)\n    if s0 &lt;= 0.0:\n        EI = 0.0\n    else:\n        y_min_y0 = y_min - y0\n        EI_one = w * (\n                y_min_y0\n                * (0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * (y_min_y0 / s0)))\n        )\n        EI_two = (\n                (1.0 - w)\n                * (s0 * (1.0 / sqrt(2.0 * pi)))\n                * (exp(-(1.0 / 2.0) * ((y_min_y0) ** 2.0 / s0 ** 2.0)))\n        )\n        EI = EI_one + EI_two\n    return EI\n</code></pre>"},{"location":"reference/spotpython/build/surrogates/","title":"surrogates","text":""},{"location":"reference/spotpython/build/surrogates/#spotpython.build.surrogates.surrogates","title":"<code>surrogates</code>","text":"<p>Super class for all surrogate model classes (e.g., Kriging)</p> Source code in <code>spotpython/build/surrogates.py</code> <pre><code>class surrogates:\n    \"\"\"\n    Super class for all surrogate model classes (e.g., Kriging)\n    \"\"\"\n    def __init__(self, name=\"\", seed=123, verbosity=0):\n        self.name = name\n        self.seed = seed\n        self.rng = default_rng(self.seed)\n        self.log = {}\n        self.verbosity = verbosity\n</code></pre>"},{"location":"reference/spotpython/data/base/","title":"base","text":""},{"location":"reference/spotpython/data/base/#spotpython.data.base.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all configurations.</p> <p>All configurations inherit from this class, be they stored in a file or generated on the fly.</p> <p>Attributes:</p> Name Type Description <code>desc</code> <code>str</code> <p>The description from the docstring.</p> <code>_repr_content</code> <code>dict</code> <p>The items that are displayed in the repr method.</p> Source code in <code>spotpython/data/base.py</code> <pre><code>class Config(abc.ABC):\n    \"\"\"Base class for all configurations.\n\n    All configurations inherit from this class, be they stored in a file or generated on the fly.\n\n    Attributes:\n        desc (str): The description from the docstring.\n        _repr_content (dict): The items that are displayed in the __repr__ method.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize a Config object.\"\"\"\n        pass\n\n    @property\n    def desc(self) -&gt; str:\n        \"\"\"Return the description from the docstring.\n\n        Returns:\n            str: The description from the docstring.\n\n        Examples:\n            &gt;&gt;&gt; class MyConfig(Config):\n            ...     '''My configuration class.'''\n            ...     pass\n            &gt;&gt;&gt; MyConfig().desc\n            'My configuration class.'\n        \"\"\"\n        desc = re.split(pattern=r\"\\w+\\n\\s{4}\\-{3,}\", string=self.__doc__, maxsplit=0)[0]\n        return inspect.cleandoc(desc)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The items that are displayed in the __repr__ method.\n\n        This property can be overridden in order to modify the output of the __repr__ method.\n\n        Returns:\n            dict: A dictionary containing the items to be displayed in the __repr__ method.\n\n        Examples:\n            &gt;&gt;&gt; class MyConfig(Config):\n            ...     '''My configuration class.'''\n            ...     pass\n            &gt;&gt;&gt; MyConfig()._repr_content\n            {'Name': 'MyConfig'}\n        \"\"\"\n        content = {}\n        content[\"Name\"] = self.__class__.__name__\n        return content\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Config.desc","title":"<code>desc: str</code>  <code>property</code>","text":"<p>Return the description from the docstring.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The description from the docstring.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyConfig(Config):\n...     '''My configuration class.'''\n...     pass\n&gt;&gt;&gt; MyConfig().desc\n'My configuration class.'\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Config.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a Config object.</p> Source code in <code>spotpython/data/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize a Config object.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all datasets.</p> <p>All datasets inherit from this class, be they stored in a file or generated on the fly.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d</p> required <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> <code>None</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset, only applies to classification datasets.</p> <code>None</code> <code>n_outputs</code> <code>int</code> <p>Number of outputs the target is made of, only applies to multi-output datasets.</p> <code>None</code> <code>sparse</code> <code>bool</code> <p>Whether the dataset is sparse or not.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>desc</code> <code>str</code> <p>The description from the docstring.</p> <code>_repr_content</code> <code>dict</code> <p>The items that are displayed in the repr method.</p> Source code in <code>spotpython/data/base.py</code> <pre><code>class Dataset(abc.ABC):\n    \"\"\"Base class for all datasets.\n\n    All datasets inherit from this class, be they stored in a file or generated on the fly.\n\n    Args:\n        task (str): Type of task the dataset is meant for. Should be one of:\n            - \"Regression\"\n            - \"Binary classification\"\n            - \"Multi-class classification\"\n            - \"Multi-output binary classification\"\n            - \"Multi-output regression\"\n        n_features (int): Number of features in the dataset.\n        n_samples (int, optional): Number of samples in the dataset.\n        n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets.\n        n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets.\n        sparse (bool, optional): Whether the dataset is sparse or not.\n\n    Attributes:\n        desc (str): The description from the docstring.\n        _repr_content (dict): The items that are displayed in the __repr__ method.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        n_features: int,\n        n_samples: Optional[int] = None,\n        n_classes: Optional[int] = None,\n        n_outputs: Optional[int] = None,\n        sparse: bool = False,\n    ):\n        \"\"\"Initialize a Dataset object.\n\n        Args:\n            task (str): Type of task the dataset is meant for. Should be one of:\n                - \"Regression\"\n                - \"Binary classification\"\n                - \"Multi-class classification\"\n                - \"Multi-output binary classification\"\n                - \"Multi-output regression\"\n            n_features (int): Number of features in the dataset.\n            n_samples (int, optional): Number of samples in the dataset. Defaults to None.\n            n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets.\n                Defaults to None.\n            n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets.\n                Defaults to None.\n            sparse (bool, optional): Whether the dataset is sparse or not. Defaults to False.\n        \"\"\"\n        self.task = task\n        self.n_features = n_features\n        self.n_samples = n_samples\n        self.n_outputs = n_outputs\n        self.n_classes = n_classes\n        self.sparse = sparse\n\n    @abc.abstractmethod\n    def __iter__(self):\n        \"\"\"Abstract method for iterating over samples in the dataset.\"\"\"\n        raise NotImplementedError\n\n    def take(self, k: int) -&gt; itertools.islice:\n        \"\"\"Iterate over the k samples.\n\n        Args:\n            k (int): The number of samples to iterate over.\n\n        Returns:\n            itertools.islice: An iterator over the first k samples in the dataset.\n\n        Examples:\n            &gt;&gt;&gt; class MyDataset(Dataset):\n            ...     def __init__(self):\n            ...         super().__init__('Regression', 10)\n            ...     def __iter__(self):\n            ...         yield from range(10)\n            &gt;&gt;&gt; list(MyDataset().take(5))\n            [0, 1, 2, 3, 4]\n        \"\"\"\n        return itertools.islice(self, k)\n\n    @property\n    def desc(self) -&gt; str:\n        \"\"\"Return the description from the docstring.\n\n        Returns:\n            str: The description from the docstring.\n\n        Examples:\n            &gt;&gt;&gt; class MyDataset(Dataset):\n            ...     '''My dataset class.'''\n            ...     def __init__(self):\n            ...         super().__init__('Regression', 10)\n            ...     def __iter__(self):\n            ...         yield from range(10)\n            &gt;&gt;&gt; MyDataset().desc\n            'My dataset class.'\n        \"\"\"\n        desc = re.split(pattern=r\"\\w+\\n\\s{4}\\-{3,}\", string=self.__doc__, maxsplit=0)[0]\n        return inspect.cleandoc(desc)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The items that are displayed in the __repr__ method.\n\n        This property can be overridden in order to modify the output of the __repr__ method.\n\n        Returns:\n            dict: A dictionary containing the items to be displayed in the __repr__ method.\n        \"\"\"\n\n        content = {}\n        content[\"Name\"] = self.__class__.__name__\n        content[\"Task\"] = self.task\n        if isinstance(self, SyntheticDataset) and self.n_samples is None:\n            content[\"Samples\"] = \"\u221e\"\n        elif self.n_samples:\n            content[\"Samples\"] = f\"{self.n_samples:,}\"\n        if self.n_features:\n            content[\"Features\"] = f\"{self.n_features:,}\"\n        if self.n_outputs:\n            content[\"Outputs\"] = f\"{self.n_outputs:,}\"\n        if self.n_classes:\n            content[\"Classes\"] = f\"{self.n_classes:,}\"\n        content[\"Sparse\"] = str(self.sparse)\n\n        return content\n\n    def __repr__(self):\n        l_len = max(map(len, self._repr_content.keys()))\n        r_len = max(map(len, self._repr_content.values()))\n\n        out = f\"{self.desc}\\n\\n\" + \"\\n\".join(k.rjust(l_len) + \"  \" + v.ljust(r_len) for k, v in self._repr_content.items())\n\n        if \"Parameters\\n    ----------\" in self.__doc__:\n            params = re.split(\n                r\"\\w+\\n\\s{4}\\-{3,}\",\n                re.split(\"Parameters\\n    ----------\", self.__doc__)[1],\n            )[0].rstrip()\n            out += f\"\\n\\nParameters\\n----------{params}\"\n\n        return out\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Dataset.desc","title":"<code>desc: str</code>  <code>property</code>","text":"<p>Return the description from the docstring.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The description from the docstring.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyDataset(Dataset):\n...     '''My dataset class.'''\n...     def __init__(self):\n...         super().__init__('Regression', 10)\n...     def __iter__(self):\n...         yield from range(10)\n&gt;&gt;&gt; MyDataset().desc\n'My dataset class.'\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Dataset.__init__","title":"<code>__init__(task, n_features, n_samples=None, n_classes=None, n_outputs=None, sparse=False)</code>","text":"<p>Initialize a Dataset object.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d</p> required <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset. Defaults to None.</p> <code>None</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset, only applies to classification datasets. Defaults to None.</p> <code>None</code> <code>n_outputs</code> <code>int</code> <p>Number of outputs the target is made of, only applies to multi-output datasets. Defaults to None.</p> <code>None</code> <code>sparse</code> <code>bool</code> <p>Whether the dataset is sparse or not. Defaults to False.</p> <code>False</code> Source code in <code>spotpython/data/base.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    n_features: int,\n    n_samples: Optional[int] = None,\n    n_classes: Optional[int] = None,\n    n_outputs: Optional[int] = None,\n    sparse: bool = False,\n):\n    \"\"\"Initialize a Dataset object.\n\n    Args:\n        task (str): Type of task the dataset is meant for. Should be one of:\n            - \"Regression\"\n            - \"Binary classification\"\n            - \"Multi-class classification\"\n            - \"Multi-output binary classification\"\n            - \"Multi-output regression\"\n        n_features (int): Number of features in the dataset.\n        n_samples (int, optional): Number of samples in the dataset. Defaults to None.\n        n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets.\n            Defaults to None.\n        n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets.\n            Defaults to None.\n        sparse (bool, optional): Whether the dataset is sparse or not. Defaults to False.\n    \"\"\"\n    self.task = task\n    self.n_features = n_features\n    self.n_samples = n_samples\n    self.n_outputs = n_outputs\n    self.n_classes = n_classes\n    self.sparse = sparse\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Dataset.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Abstract method for iterating over samples in the dataset.</p> Source code in <code>spotpython/data/base.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n    \"\"\"Abstract method for iterating over samples in the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Dataset.take","title":"<code>take(k)</code>","text":"<p>Iterate over the k samples.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of samples to iterate over.</p> required <p>Returns:</p> Type Description <code>islice</code> <p>itertools.islice: An iterator over the first k samples in the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyDataset(Dataset):\n...     def __init__(self):\n...         super().__init__('Regression', 10)\n...     def __iter__(self):\n...         yield from range(10)\n&gt;&gt;&gt; list(MyDataset().take(5))\n[0, 1, 2, 3, 4]\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>def take(self, k: int) -&gt; itertools.islice:\n    \"\"\"Iterate over the k samples.\n\n    Args:\n        k (int): The number of samples to iterate over.\n\n    Returns:\n        itertools.islice: An iterator over the first k samples in the dataset.\n\n    Examples:\n        &gt;&gt;&gt; class MyDataset(Dataset):\n        ...     def __init__(self):\n        ...         super().__init__('Regression', 10)\n        ...     def __iter__(self):\n        ...         yield from range(10)\n        &gt;&gt;&gt; list(MyDataset().take(5))\n        [0, 1, 2, 3, 4]\n    \"\"\"\n    return itertools.islice(self, k)\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.FileConfig","title":"<code>FileConfig</code>","text":"<p>               Bases: <code>Config</code></p> <p>Base class for configurations that are stored in a local file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The file\u2019s name.</p> required <code>directory</code> <code>Optional[str]</code> <p>The directory where the file is contained. Defaults to the location of the <code>datasets</code> module.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra config parameters to pass as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FileConfig</code> <p>A FileConfig object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>class FileConfig(Config):\n    \"\"\"Base class for configurations that are stored in a local file.\n\n    Args:\n        filename (str): The file's name.\n        directory (Optional[str]):\n            The directory where the file is contained.\n            Defaults to the location of the `datasets` module.\n        desc (dict): Extra config parameters to pass as keyword arguments.\n\n    Returns:\n        (FileConfig): A FileConfig object.\n\n    Examples:\n        &gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n    \"\"\"\n\n    def __init__(self, filename: str, directory: Optional[str] = None, **desc):\n        super().__init__(**desc)\n        self.filename = filename\n        self.directory = directory\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"The path to the configuration file.\n\n        Returns:\n            pathlib.Path: The path to the configuration file.\n\n        Examples:\n            &gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; config.path\n            PosixPath('/path/to/directory/config.json')\n        \"\"\"\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The content of the string representation of the FileConfig object.\n\n        Returns:\n            dict: A dictionary containing the content of the string representation of the FileConfig object.\n\n        Examples:\n            &gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; config._repr_content\n            {'Path': '/path/to/directory/config.json'}\n        \"\"\"\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.FileConfig.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>The path to the configuration file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the configuration file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n&gt;&gt;&gt; config.path\nPosixPath('/path/to/directory/config.json')\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.FileDataset","title":"<code>FileDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for datasets that are stored in a local file.</p> <p>Small datasets that are part of the spotriver package inherit from this class.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The file\u2019s name.</p> required <code>directory</code> <code>Optional[str]</code> <p>The directory where the file is contained. Defaults to the location of the <code>datasets</code> module.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra dataset parameters to pass as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FileDataset</code> <p>A FileDataset object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>class FileDataset(Dataset):\n    \"\"\"Base class for datasets that are stored in a local file.\n\n    Small datasets that are part of the spotriver package inherit from this class.\n\n    Args:\n        filename (str): The file's name.\n        directory (Optional[str]):\n            The directory where the file is contained.\n            Defaults to the location of the `datasets` module.\n        desc (dict): Extra dataset parameters to pass as keyword arguments.\n\n    Returns:\n        (FileDataset): A FileDataset object.\n\n    Examples:\n        &gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n    \"\"\"\n\n    def __init__(self, filename: str, directory: Optional[str] = None, **desc):\n        super().__init__(**desc)\n        self.filename = filename\n        self.directory = directory\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"The path to the dataset file.\n\n        Returns:\n            pathlib.Path: The path to the dataset file.\n\n        Examples:\n            &gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; dataset.path\n            PosixPath('/path/to/directory/dataset.csv')\n        \"\"\"\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The content of the string representation of the FileDataset object.\n\n        Returns:\n            dict: A dictionary containing the content of the string representation of the FileDataset object.\n\n        Examples:\n            &gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; dataset._repr_content\n            {'Path': '/path/to/directory/dataset.csv'}\n        \"\"\"\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.FileDataset.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>The path to the dataset file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the dataset file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n&gt;&gt;&gt; dataset.path\nPosixPath('/path/to/directory/dataset.csv')\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.GenericFileDataset","title":"<code>GenericFileDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for datasets that are stored in a local file.</p> <p>Small datasets that are part of the spotriver package inherit from this class.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The file\u2019s name.</p> required <code>target</code> <code>str</code> <p>The name of the target variable.</p> required <code>converters</code> <code>dict</code> <p>A dictionary specifying how to convert the columns of the dataset. Defaults to None.</p> <code>None</code> <code>parse_dates</code> <code>list</code> <p>A list of columns to parse as dates. Defaults to None.</p> <code>None</code> <code>directory</code> <code>str</code> <p>The directory where the file is contained. Defaults to the location of the <code>datasets</code> module.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra dataset parameters to pass as keyword arguments.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.datasets import Iris\n&gt;&gt;&gt; dataset = Iris()\n&gt;&gt;&gt; for x, y in dataset:\n...     print(x, y)\n...     break\n({'sepal_length': 5.1,\n  'sepal_width': 3.5,\n  'petal_length': 1.4,\n  'petal_width': 0.2},\n  'setosa')\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>class GenericFileDataset(Dataset):\n    \"\"\"Base class for datasets that are stored in a local file.\n\n    Small datasets that are part of the spotriver package inherit from this class.\n\n    Args:\n        filename (str): The file's name.\n        target (str): The name of the target variable.\n        converters (dict):\n            A dictionary specifying how to convert the columns of the dataset. Defaults to None.\n        parse_dates (list): A list of columns to parse as dates. Defaults to None.\n        directory (str):\n            The directory where the file is contained. Defaults to the location of the `datasets` module.\n        desc (dict): Extra dataset parameters to pass as keyword arguments.\n\n\n    Examples:\n\n        &gt;&gt;&gt; from river.datasets import Iris\n        &gt;&gt;&gt; dataset = Iris()\n        &gt;&gt;&gt; for x, y in dataset:\n        ...     print(x, y)\n        ...     break\n        ({'sepal_length': 5.1,\n          'sepal_width': 3.5,\n          'petal_length': 1.4,\n          'petal_width': 0.2},\n          'setosa')\n\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str,\n        target: str,\n        converters: dict = None,\n        parse_dates: list = None,\n        directory: str = None,\n        **desc: dict,\n    ):\n        super().__init__(**desc)\n        self.filename = filename\n        self.directory = directory\n        self.target = target\n        self.converters = converters\n        self.parse_dates = parse_dates\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"Returns the path where the dataset is stored.\"\"\"\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self):\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.GenericFileDataset.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>Returns the path where the dataset is stored.</p>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.RemoteDataset","title":"<code>RemoteDataset</code>","text":"<p>               Bases: <code>FileDataset</code></p> <p>Base class for datasets that are stored in a remote file.</p> <p>Medium and large datasets that are not part of the river package inherit from this class.</p> <p>The filename doesn\u2019t have to be provided if unpack is False. Indeed in the latter case the filename will be inferred from the URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL the dataset is located at.</p> required <code>size</code> <code>int</code> <p>The expected download size.</p> required <code>unpack</code> <code>bool</code> <p>Whether to unpack the download or not. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>An optional name to given to the file if the file is unpacked. Defaults to None.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra dataset parameters to pass as keyword arguments.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.datasets import AirlinePassengers\n&gt;&gt;&gt; dataset = AirlinePassengers()\n&gt;&gt;&gt; for x, y in dataset:\n...     print(x, y)\n...     break\n({'month': datetime.datetime(1949, 1, 1, 0, 0)}, 112)\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>class RemoteDataset(FileDataset):\n    \"\"\"Base class for datasets that are stored in a remote file.\n\n    Medium and large datasets that are not part of the river package inherit from this class.\n\n    The filename doesn't have to be provided if unpack is False. Indeed in the latter case the\n    filename will be inferred from the URL.\n\n    Args:\n        url (str): The URL the dataset is located at.\n        size (int): The expected download size.\n        unpack (bool): Whether to unpack the download or not. Defaults to True.\n        filename (str):\n            An optional name to given to the file if the file is unpacked. Defaults to None.\n        desc (dict): Extra dataset parameters to pass as keyword arguments.\n\n    Examples:\n\n        &gt;&gt;&gt; from river.datasets import AirlinePassengers\n        &gt;&gt;&gt; dataset = AirlinePassengers()\n        &gt;&gt;&gt; for x, y in dataset:\n        ...     print(x, y)\n        ...     break\n        ({'month': datetime.datetime(1949, 1, 1, 0, 0)}, 112)\n\n    \"\"\"\n\n    def __init__(self, url: str, size: int, unpack: bool = True, filename: str = None, **desc: dict):\n        if filename is None:\n            filename = path.basename(url)\n\n        super().__init__(filename=filename, **desc)\n        self.url = url\n        self.size = size\n        self.unpack = unpack\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"Returns the path where the dataset is stored.\"\"\"\n        return pathlib.Path(get_data_home(), self.__class__.__name__, self.filename)\n\n    def download(self, force: bool = False, verbose: bool = True) -&gt; None:\n        \"\"\"Downloads the dataset.\n\n        Args:\n            force (bool):\n                Whether to force the download even if the data is already downloaded.\n                Defaults to False.\n            verbose (bool):\n                Whether to display information about the download. Defaults to True.\n\n        \"\"\"\n        if not force and self.is_downloaded:\n            return\n\n        # Determine where to download the archive\n        directory = self.path.parent\n        directory.mkdir(parents=True, exist_ok=True)\n        archive_path = directory.joinpath(path.basename(self.url))\n\n        with request.urlopen(self.url) as r:\n            # Notify the user\n            if verbose:\n                meta = r.info()\n                try:\n                    n_bytes = int(meta[\"Content-Length\"])\n                    msg = f\"Downloading {self.url} ({n_bytes})\"\n                except KeyError:\n                    msg = f\"Downloading {self.url}\"\n                print(msg)\n\n            # Now dump the contents of the requests\n            with open(archive_path, \"wb\") as f:\n                shutil.copyfileobj(r, f)\n\n        if not self.unpack:\n            return\n\n        if verbose:\n            print(f\"Uncompressing into {directory}\")\n\n        if archive_path.suffix.endswith(\"zip\"):\n            with zipfile.ZipFile(archive_path, \"r\") as zf:\n                zf.extractall(directory)\n\n        elif archive_path.suffix.endswith((\"gz\", \"tar\")):\n            mode = \"r:\" if archive_path.suffix.endswith(\"tar\") else \"r:gz\"\n            tar = tarfile.open(archive_path, mode)\n            tar.extractall(directory)\n            tar.close()\n\n        else:\n            raise RuntimeError(f\"Unhandled extension type: {archive_path.suffix}\")\n\n        # Delete the archive file now that it has been uncompressed\n        archive_path.unlink()\n\n    @abc.abstractmethod\n    def _iter(self):\n        pass\n\n    @property\n    def is_downloaded(self) -&gt; bool:\n        \"\"\"Indicate whether or not the data has been correctly downloaded.\"\"\"\n        if self.path.exists():\n            if self.path.is_file():\n                return self.path.stat().st_size == self.size\n            return sum(f.stat().st_size for f in self.path.glob(\"**/*\") if f.is_file())\n\n        return False\n\n    def __iter__(self):\n        \"\"\"Iterates over the samples of a dataset.\"\"\"\n        if not self.is_downloaded:\n            self.download(verbose=True)\n        if not self.is_downloaded:\n            raise RuntimeError(\"Something went wrong during the download\")\n        yield from self._iter()\n\n    @property\n    def _repr_content(self):\n        content = super()._repr_content\n        content[\"URL\"] = self.url\n        content[\"Size\"] = self.size\n        content[\"Downloaded\"] = str(self.is_downloaded)\n        return content\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.RemoteDataset.is_downloaded","title":"<code>is_downloaded: bool</code>  <code>property</code>","text":"<p>Indicate whether or not the data has been correctly downloaded.</p>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.RemoteDataset.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>Returns the path where the dataset is stored.</p>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.RemoteDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over the samples of a dataset.</p> Source code in <code>spotpython/data/base.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterates over the samples of a dataset.\"\"\"\n    if not self.is_downloaded:\n        self.download(verbose=True)\n    if not self.is_downloaded:\n        raise RuntimeError(\"Something went wrong during the download\")\n    yield from self._iter()\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.RemoteDataset.download","title":"<code>download(force=False, verbose=True)</code>","text":"<p>Downloads the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>Whether to force the download even if the data is already downloaded. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to display information about the download. Defaults to True.</p> <code>True</code> Source code in <code>spotpython/data/base.py</code> <pre><code>def download(self, force: bool = False, verbose: bool = True) -&gt; None:\n    \"\"\"Downloads the dataset.\n\n    Args:\n        force (bool):\n            Whether to force the download even if the data is already downloaded.\n            Defaults to False.\n        verbose (bool):\n            Whether to display information about the download. Defaults to True.\n\n    \"\"\"\n    if not force and self.is_downloaded:\n        return\n\n    # Determine where to download the archive\n    directory = self.path.parent\n    directory.mkdir(parents=True, exist_ok=True)\n    archive_path = directory.joinpath(path.basename(self.url))\n\n    with request.urlopen(self.url) as r:\n        # Notify the user\n        if verbose:\n            meta = r.info()\n            try:\n                n_bytes = int(meta[\"Content-Length\"])\n                msg = f\"Downloading {self.url} ({n_bytes})\"\n            except KeyError:\n                msg = f\"Downloading {self.url}\"\n            print(msg)\n\n        # Now dump the contents of the requests\n        with open(archive_path, \"wb\") as f:\n            shutil.copyfileobj(r, f)\n\n    if not self.unpack:\n        return\n\n    if verbose:\n        print(f\"Uncompressing into {directory}\")\n\n    if archive_path.suffix.endswith(\"zip\"):\n        with zipfile.ZipFile(archive_path, \"r\") as zf:\n            zf.extractall(directory)\n\n    elif archive_path.suffix.endswith((\"gz\", \"tar\")):\n        mode = \"r:\" if archive_path.suffix.endswith(\"tar\") else \"r:gz\"\n        tar = tarfile.open(archive_path, mode)\n        tar.extractall(directory)\n        tar.close()\n\n    else:\n        raise RuntimeError(f\"Unhandled extension type: {archive_path.suffix}\")\n\n    # Delete the archive file now that it has been uncompressed\n    archive_path.unlink()\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.SyntheticDataset","title":"<code>SyntheticDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A synthetic dataset.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d</p> required <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset, only applies to classification datasets.</p> <code>None</code> <code>n_outputs</code> <code>int</code> <p>Number of outputs the target is made of, only applies to multi-output datasets.</p> <code>None</code> <code>sparse</code> <code>bool</code> <p>Whether the dataset is sparse or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>SyntheticDataset</code> <p>A synthetic dataset object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n&gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                    n_features=4,\n                                    n_samples=100,\n                                    n_classes=2,\n                                    n_outputs=1,\n                                    sparse=False)\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>class SyntheticDataset(Dataset):\n    \"\"\"A synthetic dataset.\n\n    Args:\n        task (str): Type of task the dataset is meant for. Should be one of:\n            - \"Regression\"\n            - \"Binary classification\"\n            - \"Multi-class classification\"\n            - \"Multi-output binary classification\"\n            - \"Multi-output regression\"\n        n_features (int): Number of features in the dataset.\n        n_samples (int): Number of samples in the dataset.\n        n_classes (int): Number of classes in the dataset, only applies to classification datasets.\n        n_outputs (int): Number of outputs the target is made of, only applies to multi-output datasets.\n        sparse (bool): Whether the dataset is sparse or not.\n\n    Returns:\n        (SyntheticDataset): A synthetic dataset object.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import make_classification\n        &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n        &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                            n_features=4,\n                                            n_samples=100,\n                                            n_classes=2,\n                                            n_outputs=1,\n                                            sparse=False)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        n_features: int,\n        n_samples: int,\n        n_classes: Union[int, None] = None,\n        n_outputs: Union[int, None] = None,\n        sparse: bool = False,\n    ):\n        pass\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the SyntheticDataset object.\n\n        Returns:\n            str: A string representation of the SyntheticDataset object.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.datasets import make_classification\n            &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n            &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                                n_features=4,\n                                                n_samples=100,\n                                                n_classes=2,\n                                                n_outputs=1,\n                                                sparse=False)\n            &gt;&gt;&gt; print(dataset)\n            Synthetic data generator\n\n            Configuration\n            -------------\n                task  Binary classification\n          n_features  4\n           n_samples  100\n           n_classes  2\n           n_outputs  1\n              sparse  False\n        \"\"\"\n        l_len_prop = max(map(len, self._repr_content.keys()))\n        r_len_prop = max(map(len, self._repr_content.values()))\n        params = self._get_params()\n        l_len_config = max(map(len, params.keys()))\n        r_len_config = max(map(len, map(str, params.values())))\n\n        out = (\n            \"Synthetic data generator\\n\\n\"\n            + \"\\n\".join(k.rjust(l_len_prop) + \"  \" + v.ljust(r_len_prop) for k, v in self._repr_content.items())\n            + \"\\n\\nConfiguration\\n-------------\\n\"\n            + \"\\n\".join(k.rjust(l_len_config) + \"  \" + str(v).ljust(r_len_config) for k, v in params.items())\n        )\n\n        return out\n\n    def _get_params(self) -&gt; typing.Dict[str, typing.Any]:\n        \"\"\"Return the parameters that were used during initialization.\n\n        Returns:\n            dict: A dictionary containing the parameters that were used during initialization.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.datasets import make_classification\n            &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n            &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                            n_features=4,\n                                            n_samples=100,\n                                            n_classes=2,\n                                            n_outputs=1,\n                                            sparse=False)\n            &gt;&gt;&gt; dataset._get_params()\n            {'task': 'Binary classification',\n             'n_features': 4,\n             'n_samples': 100,\n             'n_classes': 2,\n             'n_outputs': 1,\n             'sparse': False}\n        \"\"\"\n        return {name: getattr(self, name) for name, param in inspect.signature(self.__init__).parameters.items() if param.kind != param.VAR_KEYWORD}  # type: ignore\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.SyntheticDataset.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the SyntheticDataset object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the SyntheticDataset object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n&gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                    n_features=4,\n                                    n_samples=100,\n                                    n_classes=2,\n                                    n_outputs=1,\n                                    sparse=False)\n&gt;&gt;&gt; print(dataset)\nSynthetic data generator\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.SyntheticDataset.__repr__--configuration","title":"Configuration","text":"<pre><code>task  Binary classification\n</code></pre> <p>n_features  4    n_samples  100    n_classes  2    n_outputs  1       sparse  False</p> Source code in <code>spotpython/data/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the SyntheticDataset object.\n\n    Returns:\n        str: A string representation of the SyntheticDataset object.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import make_classification\n        &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n        &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                            n_features=4,\n                                            n_samples=100,\n                                            n_classes=2,\n                                            n_outputs=1,\n                                            sparse=False)\n        &gt;&gt;&gt; print(dataset)\n        Synthetic data generator\n\n        Configuration\n        -------------\n            task  Binary classification\n      n_features  4\n       n_samples  100\n       n_classes  2\n       n_outputs  1\n          sparse  False\n    \"\"\"\n    l_len_prop = max(map(len, self._repr_content.keys()))\n    r_len_prop = max(map(len, self._repr_content.values()))\n    params = self._get_params()\n    l_len_config = max(map(len, params.keys()))\n    r_len_config = max(map(len, map(str, params.values())))\n\n    out = (\n        \"Synthetic data generator\\n\\n\"\n        + \"\\n\".join(k.rjust(l_len_prop) + \"  \" + v.ljust(r_len_prop) for k, v in self._repr_content.items())\n        + \"\\n\\nConfiguration\\n-------------\\n\"\n        + \"\\n\".join(k.rjust(l_len_config) + \"  \" + str(v).ljust(r_len_config) for k, v in params.items())\n    )\n\n    return out\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.get_data_home","title":"<code>get_data_home(data_home=None)</code>","text":"<p>Return the location where remote datasets are to be stored.</p> <p>By default the data directory is set to a folder named \u2018spotriver_data\u2019 in the user home folder. Alternatively, it can be set by the \u2018SPOTRIVER_DATA\u2019 environment variable or programmatically by giving an explicit folder path. The \u2018~\u2019 symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotriver data directory. If <code>None</code>, the default path is <code>~/spotriver_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotriver data directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; get_data_home()\nPosixPath('/home/user/spotriver_data')\n&gt;&gt;&gt; get_data_home(Path('/tmp/spotriver_data'))\nPosixPath('/tmp/spotriver_data')\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where remote datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotriver_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTRIVER_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotriver data directory. If `None`, the default path\n            is `~/spotriver_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotriver data directory.\n\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotriver_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotriver_data'))\n        PosixPath('/tmp/spotriver_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\"SPOTRIVER_DATA\", Path.home() / \"spotriver_data\")\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"reference/spotpython/data/california/","title":"california","text":""},{"location":"reference/spotpython/data/california/#spotpython.data.california.CaliforniaHousing","title":"<code>CaliforniaHousing</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for regression. A toy data set from scikit-learn. Features:     * MedInc median income in block group     * HouseAge median house age in block group     * AveRooms average number of rooms per household     * AveBedrms average number of bedrooms per household     * Population block group population     * AveOccup average number of household members     * Latitude block group latitude     * Longitude block group longitude The target variable is the median house value for California districts, expressed in hundreds of thousands of Dollars ($100,000). Samples total: 20640, Dimensionality: 8, Features: real, Target: real 0.15 - 5. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).</p> <p>Parameters:</p> Name Type Description Default <code>feature_type</code> <code>dtype</code> <p>The data type of the features. Defaults to torch.float.</p> <code>float</code> <code>target_type</code> <code>dtype</code> <p>The data type of the targets. Defaults to torch.long.</p> <code>float</code> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not. Defaults to True.</p> <code>True</code> <code>n_samples</code> <code>int</code> <p>The number of samples of the dataset. Defaults to None, which means the entire dataset is used.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>Tensor</code> <p>The data features.</p> <code>targets</code> <code>Tensor</code> <p>The data targets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    import torch\n    dataset = Diabetes(feature_type=torch.float32, target_type=torch.float32)\n    # Set batch size for DataLoader\n    batch_size = 5\n    # Create DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    # Iterate over the data in the DataLoader\n    for batch in dataloader:\n        inputs, targets = batch\n        print(f\"Batch Size: {inputs.size(0)}\")\n        print(\"---------------\")\n        print(f\"Inputs: {inputs}\")\n        print(f\"Targets: {targets}\")\n</code></pre> Source code in <code>spotpython/data/california.py</code> <pre><code>class CaliforniaHousing(Dataset):\n    \"\"\"\n    A PyTorch Dataset for regression. A toy data set from scikit-learn.\n    Features:\n        * MedInc median income in block group\n        * HouseAge median house age in block group\n        * AveRooms average number of rooms per household\n        * AveBedrms average number of bedrooms per household\n        * Population block group population\n        * AveOccup average number of household members\n        * Latitude block group latitude\n        * Longitude block group longitude\n    The target variable is the median house value for California districts,\n    expressed in hundreds of thousands of Dollars ($100,000).\n    Samples total: 20640, Dimensionality: 8, Features: real, Target: real 0.15 - 5.\n    This dataset was derived from the 1990 U.S. census, using one row per census block group.\n    A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data\n    (a block group typically has a population of 600 to 3,000 people).\n\n    Args:\n        feature_type (torch.dtype): The data type of the features. Defaults to torch.float.\n        target_type (torch.dtype): The data type of the targets. Defaults to torch.long.\n        train (bool): Whether the dataset is for training or not. Defaults to True.\n        n_samples (int): The number of samples of the dataset. Defaults to None, which means the entire dataset is used.\n\n    Attributes:\n        data (Tensor): The data features.\n        targets (Tensor): The data targets.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            import torch\n            dataset = Diabetes(feature_type=torch.float32, target_type=torch.float32)\n            # Set batch size for DataLoader\n            batch_size = 5\n            # Create DataLoader\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n            # Iterate over the data in the DataLoader\n            for batch in dataloader:\n                inputs, targets = batch\n                print(f\"Batch Size: {inputs.size(0)}\")\n                print(\"---------------\")\n                print(f\"Inputs: {inputs}\")\n                print(f\"Targets: {targets}\")\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_type: torch.dtype = torch.float,\n        target_type: torch.dtype = torch.float,\n        train: bool = True,\n        n_samples: int = None,\n    ) -&gt; None:\n        super().__init__()\n        self.feature_type = feature_type\n        self.target_type = target_type\n        self.train = train\n        self.n_samples = n_samples\n        self.data, self.targets = self._load_data()\n\n    def _load_data(self) -&gt; tuple:\n        \"\"\"Loads the data from scikit-learn and returns the features and targets.\n\n        Returns:\n            tuple: A tuple containing the features and targets.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n                dataset = Diabetes()\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([442, 10])\n                torch.Size([442])\n        \"\"\"\n        feature_df, target_df = fetch_california_housing(return_X_y=True, as_frame=True)\n        if self.n_samples is not None:\n            feature_df = feature_df[: self.n_samples]\n            target_df = target_df[: self.n_samples]\n        # Convert DataFrames to PyTorch tensors\n        feature_tensor = torch.tensor(feature_df.values, dtype=self.feature_type)\n        target_tensor = torch.tensor(target_df.values, dtype=self.target_type)\n\n        return feature_tensor, target_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n                dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([11, 65])\n                torch.Size([11])\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(len(dataset))\n            60000\n\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the dataset.\n\n        Returns:\n            str: A string representation of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(dataset)\n            Split: Train\n\n        \"\"\"\n        split = \"Train\" if self.train else \"Test\"\n        return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotpython/data/california/#spotpython.data.california.CaliforniaHousing.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n    dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n    print(dataset.data.shape)\n    print(dataset.targets.shape)\n    torch.Size([11, 65])\n    torch.Size([11])\n</code></pre> Source code in <code>spotpython/data/california.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n            dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n            print(dataset.data.shape)\n            print(dataset.targets.shape)\n            torch.Size([11, 65])\n            torch.Size([11])\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotpython/data/california/#spotpython.data.california.CaliforniaHousing.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(len(dataset))\n60000\n</code></pre> Source code in <code>spotpython/data/california.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(len(dataset))\n        60000\n\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/california/#spotpython.data.california.CaliforniaHousing.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string representation of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(dataset)\nSplit: Train\n</code></pre> Source code in <code>spotpython/data/california.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the dataset.\n\n    Returns:\n        str: A string representation of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(dataset)\n        Split: Train\n\n    \"\"\"\n    split = \"Train\" if self.train else \"Test\"\n    return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotpython/data/california_housing/","title":"california_housing","text":""},{"location":"reference/spotpython/data/california_housing/#spotpython.data.california_housing.CaliforniaHousing","title":"<code>CaliforniaHousing</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for regression. A toy data set from scikit-learn. Data Set Characteristics: * Number of Instances: 20640 * Number of Attributes: 8 numeric, predictive attributes and the target * Attribute Information:     - MedInc median income in block group     - HouseAge median house age in block group     - AveRooms average number of rooms per household     - AveBedrms average number of bedrooms per household     - Population block group population     - AveOccup average number of household members     - Latitude block group latitude     - Longitude block group longitude * Missing Attribute Values: None * Target: The target variable is the median house value for California districts,     expressed in hundreds of thousands of dollars ($100,000).     This dataset was obtained from the StatLib repository:     https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html     This dataset was derived from the 1990 U.S. census, using one row per census block group.     A block group is the smallest geographical unit for which the U.S. Census Bureau publishes     sample data (a block group typically has a population of 600 to 3,000 people).     A household is a group of people residing within a home. Since the average number of rooms     and bedrooms in this dataset are provided per household, these columns may take surprisingly     large values for block groups with few households and many empty houses, such as vacation resorts.</p> <p>Parameters:</p> Name Type Description Default <code>feature_type</code> <code>dtype</code> <p>The data type of the features. Defaults to torch.float.</p> <code>float</code> <code>target_type</code> <code>dtype</code> <p>The data type of the targets. Defaults to torch.long.</p> <code>float</code> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>Tensor</code> <p>The data features.</p> <code>targets</code> <code>Tensor</code> <p>The data targets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.california_housing import CaliforniaHousing\n    import torch\n    dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n    # Set batch size for DataLoader\n    batch_size = 5\n    # Create DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    # Iterate over the data in the DataLoader\n    for batch in dataloader:\n        inputs, targets = batch\n        print(f\"Batch Size: {inputs.size(0)}\")\n        print(\"---------------\")\n        print(f\"Inputs: {inputs}\")\n        print(f\"Targets: {targets}\")\n</code></pre> Source code in <code>spotpython/data/california_housing.py</code> <pre><code>class CaliforniaHousing(Dataset):\n    \"\"\"\n    A PyTorch Dataset for regression. A toy data set from scikit-learn.\n    Data Set Characteristics:\n    * Number of Instances: 20640\n    * Number of Attributes: 8 numeric, predictive attributes and the target\n    * Attribute Information:\n        - MedInc median income in block group\n        - HouseAge median house age in block group\n        - AveRooms average number of rooms per household\n        - AveBedrms average number of bedrooms per household\n        - Population block group population\n        - AveOccup average number of household members\n        - Latitude block group latitude\n        - Longitude block group longitude\n    * Missing Attribute Values: None\n    * Target: The target variable is the median house value for California districts,\n        expressed in hundreds of thousands of dollars ($100,000).\n        This dataset was obtained from the StatLib repository:\n        https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n        This dataset was derived from the 1990 U.S. census, using one row per census block group.\n        A block group is the smallest geographical unit for which the U.S. Census Bureau publishes\n        sample data (a block group typically has a population of 600 to 3,000 people).\n        A household is a group of people residing within a home. Since the average number of rooms\n        and bedrooms in this dataset are provided per household, these columns may take surprisingly\n        large values for block groups with few households and many empty houses, such as vacation resorts.\n\n    Args:\n        feature_type (torch.dtype): The data type of the features. Defaults to torch.float.\n        target_type (torch.dtype): The data type of the targets. Defaults to torch.long.\n        train (bool): Whether the dataset is for training or not. Defaults to True.\n\n    Attributes:\n        data (Tensor): The data features.\n        targets (Tensor): The data targets.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.california_housing import CaliforniaHousing\n            import torch\n            dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n            # Set batch size for DataLoader\n            batch_size = 5\n            # Create DataLoader\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n            # Iterate over the data in the DataLoader\n            for batch in dataloader:\n                inputs, targets = batch\n                print(f\"Batch Size: {inputs.size(0)}\")\n                print(\"---------------\")\n                print(f\"Inputs: {inputs}\")\n                print(f\"Targets: {targets}\")\n    \"\"\"\n\n    def __init__(self, feature_type: torch.dtype = torch.float, target_type: torch.dtype = torch.float, train: bool = True) -&gt; None:\n        super().__init__()\n        self.feature_type = feature_type\n        self.target_type = target_type\n        self.train = train\n        self.names = self.get_names()\n        self.data, self.targets = self._load_data()\n\n    def _load_data(self) -&gt; tuple:\n        \"\"\"Loads the data from scikit-learn and returns the features and targets.\n\n        Returns:\n            tuple: A tuple containing the features and targets.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n                dataset = CaliforniaHousing()\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([20640, 8])\n                torch.Size([20640])\n        \"\"\"\n        feature_df, target_df = fetch_california_housing(return_X_y=True, as_frame=True)\n        # Convert DataFrames to PyTorch tensors\n        feature_tensor = torch.tensor(feature_df.values, dtype=self.feature_type)\n        target_tensor = torch.tensor(target_df.values, dtype=self.target_type)\n\n        return feature_tensor, target_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n                dataset = CaliforniaHousing()\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([20640, 8])\n                torch.Size([20640])\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n                dataset = CaliforniaHousing()\n                print(len(dataset))\n                20640\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the dataset.\n\n        Returns:\n            str: A string representation of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(dataset)\n            Split: Train\n\n        \"\"\"\n        split = \"Train\" if self.train else \"Test\"\n        return f\"Split: {split}\"\n\n    def get_names(self) -&gt; list:\n        \"\"\"\n        Returns the names of the features.\n\n        Returns:\n            list: A list containing the names of the features.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n                dataset = CaliforniaHousing()\n                print(dataset.get_names())\n                ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n        \"\"\"\n        housing = fetch_california_housing()\n        return housing.feature_names\n</code></pre>"},{"location":"reference/spotpython/data/california_housing/#spotpython.data.california_housing.CaliforniaHousing.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n    dataset = CaliforniaHousing()\n    print(dataset.data.shape)\n    print(dataset.targets.shape)\n    torch.Size([20640, 8])\n    torch.Size([20640])\n</code></pre> Source code in <code>spotpython/data/california_housing.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n            dataset = CaliforniaHousing()\n            print(dataset.data.shape)\n            print(dataset.targets.shape)\n            torch.Size([20640, 8])\n            torch.Size([20640])\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotpython/data/california_housing/#spotpython.data.california_housing.CaliforniaHousing.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n    dataset = CaliforniaHousing()\n    print(len(dataset))\n    20640\n</code></pre> Source code in <code>spotpython/data/california_housing.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n            dataset = CaliforniaHousing()\n            print(len(dataset))\n            20640\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/california_housing/#spotpython.data.california_housing.CaliforniaHousing.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string representation of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(dataset)\nSplit: Train\n</code></pre> Source code in <code>spotpython/data/california_housing.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the dataset.\n\n    Returns:\n        str: A string representation of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(dataset)\n        Split: Train\n\n    \"\"\"\n    split = \"Train\" if self.train else \"Test\"\n    return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotpython/data/california_housing/#spotpython.data.california_housing.CaliforniaHousing.get_names","title":"<code>get_names()</code>","text":"<p>Returns the names of the features.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing the names of the features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n    dataset = CaliforniaHousing()\n    print(dataset.get_names())\n    ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n</code></pre> Source code in <code>spotpython/data/california_housing.py</code> <pre><code>def get_names(self) -&gt; list:\n    \"\"\"\n    Returns the names of the features.\n\n    Returns:\n        list: A list containing the names of the features.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n            dataset = CaliforniaHousing()\n            print(dataset.get_names())\n            ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n    \"\"\"\n    housing = fetch_california_housing()\n    return housing.feature_names\n</code></pre>"},{"location":"reference/spotpython/data/csvdataset/","title":"csvdataset","text":""},{"location":"reference/spotpython/data/csvdataset/#spotpython.data.csvdataset.CSVDataset","title":"<code>CSVDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for handling CSV data.</p> Source code in <code>spotpython/data/csvdataset.py</code> <pre><code>class CSVDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset for handling CSV data.\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"data.csv\",\n        directory: None = None,\n        feature_type: torch.dtype = torch.float,\n        target_column: str = \"y\",\n        target_type: torch.dtype = torch.float,\n        train: bool = True,\n        rmNA=True,\n        dropId=False,\n        oe=OrdinalEncoder(),\n        le=LabelEncoder(),\n        **desc,\n    ) -&gt; None:\n        super().__init__()\n        self.filename = filename\n        self.directory = directory\n        self.feature_type = feature_type\n        self.target_type = target_type\n        self.target_column = target_column\n        self.train = train\n        self.rmNA = rmNA\n        self.dropId = dropId\n        self.oe = oe\n        self.le = le\n        self.data, self.targets = self._load_data()\n\n    @property\n    def path(self):\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self):\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n\n    def _load_data(self) -&gt; tuple:\n        df = pd.read_csv(self.path, index_col=False)\n\n        # Remove rows with NA if specified\n        if self.rmNA:\n            df = df.dropna()\n\n        # Drop the id column if specified\n        if self.dropId and \"id\" in df.columns:\n            df = df.drop(columns=[\"id\"])\n\n        # Split DataFrame into feature and target DataFrames\n        feature_df = df.drop(columns=[self.target_column])\n\n        # Identify non-numerical columns in the feature DataFrame\n        non_numerical_columns = feature_df.select_dtypes(exclude=[\"number\"]).columns.tolist()\n\n        # Apply OrdinalEncoder to non-numerical feature columns\n        if non_numerical_columns:\n            if self.oe is None:\n                raise ValueError(f\"\\n!!! non_numerical_columns in data: {non_numerical_columns}\" \"\\nOrdinalEncoder object oe must be provided for encoding non-numerical columns\")\n            feature_df[non_numerical_columns] = self.oe.fit_transform(feature_df[non_numerical_columns])\n\n        target_df = df[self.target_column]\n\n        # Check if the target column is non-numerical using dtype\n        if not pd.api.types.is_numeric_dtype(target_df):\n            if self.le is None:\n                raise ValueError(f\"\\n!!! The target column '{self.target_column}' is non-numerical\" \"\\nLabelEncoder object le must be provided for encoding non-numerical target\")\n            target_df = self.le.fit_transform(target_df)\n\n        # Convert DataFrames to NumPy arrays and then to PyTorch tensors\n        feature_array = feature_df.to_numpy()\n        target_array = target_df\n\n        feature_tensor = torch.tensor(feature_array, dtype=self.feature_type)\n        target_tensor = torch.tensor(target_array, dtype=self.target_type)\n\n        return feature_tensor, target_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n                dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([11, 65])\n                torch.Size([11])\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(len(dataset))\n            60000\n\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the dataset.\n\n        Returns:\n            str: A string representation of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(dataset)\n            Split: Train\n\n        \"\"\"\n        split = \"Train\" if self.train else \"Test\"\n        return f\"Split: {split}\"\n\n    def __ncols__(self) -&gt; int:\n        \"\"\"\n        Returns the number of columns in the dataset.\n\n        Returns:\n            int: The number of columns in the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n                import torch\n                from torch.utils.data import DataLoader\n                dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n                print(dataset.__ncols__())\n                64\n        \"\"\"\n        return self.data.size(1)\n</code></pre>"},{"location":"reference/spotpython/data/csvdataset/#spotpython.data.csvdataset.CSVDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n    dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n    print(dataset.data.shape)\n    print(dataset.targets.shape)\n    torch.Size([11, 65])\n    torch.Size([11])\n</code></pre> Source code in <code>spotpython/data/csvdataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n            dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n            print(dataset.data.shape)\n            print(dataset.targets.shape)\n            torch.Size([11, 65])\n            torch.Size([11])\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotpython/data/csvdataset/#spotpython.data.csvdataset.CSVDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(len(dataset))\n60000\n</code></pre> Source code in <code>spotpython/data/csvdataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(len(dataset))\n        60000\n\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/csvdataset/#spotpython.data.csvdataset.CSVDataset.__ncols__","title":"<code>__ncols__()</code>","text":"<p>Returns the number of columns in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of columns in the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    print(dataset.__ncols__())\n    64\n</code></pre> Source code in <code>spotpython/data/csvdataset.py</code> <pre><code>def __ncols__(self) -&gt; int:\n    \"\"\"\n    Returns the number of columns in the dataset.\n\n    Returns:\n        int: The number of columns in the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            print(dataset.__ncols__())\n            64\n    \"\"\"\n    return self.data.size(1)\n</code></pre>"},{"location":"reference/spotpython/data/csvdataset/#spotpython.data.csvdataset.CSVDataset.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string representation of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(dataset)\nSplit: Train\n</code></pre> Source code in <code>spotpython/data/csvdataset.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the dataset.\n\n    Returns:\n        str: A string representation of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(dataset)\n        Split: Train\n\n    \"\"\"\n    split = \"Train\" if self.train else \"Test\"\n    return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotpython/data/diabetes/","title":"diabetes","text":""},{"location":"reference/spotpython/data/diabetes/#spotpython.data.diabetes.Diabetes","title":"<code>Diabetes</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for regression. A toy data set from scikit-learn. Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. Number of Instances: 442 Number of Attributes:First 10 columns are numeric predictive values. Target: Column 11 is a quantitative measure of disease progression one year after baseline. Attribute Information:     * age age in years     * sex     * bmi body mass index     * bp average blood pressure     * s1 tc, total serum cholesterol     * s2 ldl, low-density lipoproteins     * s3 hdl, high-density lipoproteins     * s4 tch, total cholesterol / HDL     * s5 ltg, possibly log of serum triglycerides level     * s6 glu, blood sugar level</p> <p>Parameters:</p> Name Type Description Default <code>feature_type</code> <code>dtype</code> <p>The data type of the features. Defaults to torch.float.</p> <code>float</code> <code>target_type</code> <code>dtype</code> <p>The data type of the targets. Defaults to torch.long.</p> <code>float</code> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>Tensor</code> <p>The data features.</p> <code>targets</code> <code>Tensor</code> <p>The data targets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    import torch\n    dataset = Diabetes(feature_type=torch.float32, target_type=torch.float32)\n    # Set batch size for DataLoader\n    batch_size = 5\n    # Create DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    # Iterate over the data in the DataLoader\n    for batch in dataloader:\n        inputs, targets = batch\n        print(f\"Batch Size: {inputs.size(0)}\")\n        print(\"---------------\")\n        print(f\"Inputs: {inputs}\")\n        print(f\"Targets: {targets}\")\n</code></pre> Source code in <code>spotpython/data/diabetes.py</code> <pre><code>class Diabetes(Dataset):\n    \"\"\"\n    A PyTorch Dataset for regression. A toy data set from scikit-learn.\n    Ten baseline variables, age, sex, body mass index, average blood pressure,\n    and six blood serum measurements were obtained for each of n = 442 diabetes patients,\n    as well as the response of interest,\n    a quantitative measure of disease progression one year after baseline.\n    Number of Instances: 442\n    Number of Attributes:First 10 columns are numeric predictive values.\n    Target: Column 11 is a quantitative measure of disease progression one year after baseline.\n    Attribute Information:\n        * age age in years\n        * sex\n        * bmi body mass index\n        * bp average blood pressure\n        * s1 tc, total serum cholesterol\n        * s2 ldl, low-density lipoproteins\n        * s3 hdl, high-density lipoproteins\n        * s4 tch, total cholesterol / HDL\n        * s5 ltg, possibly log of serum triglycerides level\n        * s6 glu, blood sugar level\n\n    Args:\n        feature_type (torch.dtype): The data type of the features. Defaults to torch.float.\n        target_type (torch.dtype): The data type of the targets. Defaults to torch.long.\n        train (bool): Whether the dataset is for training or not. Defaults to True.\n\n    Attributes:\n        data (Tensor): The data features.\n        targets (Tensor): The data targets.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            import torch\n            dataset = Diabetes(feature_type=torch.float32, target_type=torch.float32)\n            # Set batch size for DataLoader\n            batch_size = 5\n            # Create DataLoader\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n            # Iterate over the data in the DataLoader\n            for batch in dataloader:\n                inputs, targets = batch\n                print(f\"Batch Size: {inputs.size(0)}\")\n                print(\"---------------\")\n                print(f\"Inputs: {inputs}\")\n                print(f\"Targets: {targets}\")\n    \"\"\"\n\n    def __init__(self, feature_type: torch.dtype = torch.float, target_type: torch.dtype = torch.float, train: bool = True) -&gt; None:\n        super().__init__()\n        self.feature_type = feature_type\n        self.target_type = target_type\n        self.train = train\n        self.names = self.get_names()\n        self.data, self.targets = self._load_data()\n\n    def _load_data(self) -&gt; tuple:\n        \"\"\"Loads the data from scikit-learn and returns the features and targets.\n\n        Returns:\n            tuple: A tuple containing the features and targets.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n                dataset = Diabetes()\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([442, 10])\n                torch.Size([442])\n        \"\"\"\n        feature_df, target_df = load_diabetes(return_X_y=True, as_frame=True)\n        # Convert DataFrames to PyTorch tensors\n        feature_tensor = torch.tensor(feature_df.values, dtype=self.feature_type)\n        target_tensor = torch.tensor(target_df.values, dtype=self.target_type)\n\n        return feature_tensor, target_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n                dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([11, 65])\n                torch.Size([11])\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(len(dataset))\n            60000\n\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the dataset.\n\n        Returns:\n            str: A string representation of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(dataset)\n            Split: Train\n\n        \"\"\"\n        split = \"Train\" if self.train else \"Test\"\n        return f\"Split: {split}\"\n\n    def get_names(self) -&gt; list:\n        \"\"\"\n        Returns the names of the features.\n\n        Returns:\n            list: A list containing the names of the features.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n                dataset = Diabetes()\n                print(dataset.get_names())\n                [\"age\", \"sex\", \"bmi\", \"bp\", \"tc\", \"ldl\", \"hdl\", \"tch\", \"ltg\", \"glu\"]\n        \"\"\"\n        return [\"age\", \"sex\", \"bmi\", \"bp\", \"s1_tc\", \"s2_ldl\", \"s3_hdl\", \"s4_tch\", \"s5_ltg\", \"s6_glu\"]\n</code></pre>"},{"location":"reference/spotpython/data/diabetes/#spotpython.data.diabetes.Diabetes.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n    dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n    print(dataset.data.shape)\n    print(dataset.targets.shape)\n    torch.Size([11, 65])\n    torch.Size([11])\n</code></pre> Source code in <code>spotpython/data/diabetes.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n            dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n            print(dataset.data.shape)\n            print(dataset.targets.shape)\n            torch.Size([11, 65])\n            torch.Size([11])\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotpython/data/diabetes/#spotpython.data.diabetes.Diabetes.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(len(dataset))\n60000\n</code></pre> Source code in <code>spotpython/data/diabetes.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(len(dataset))\n        60000\n\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/diabetes/#spotpython.data.diabetes.Diabetes.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string representation of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(dataset)\nSplit: Train\n</code></pre> Source code in <code>spotpython/data/diabetes.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the dataset.\n\n    Returns:\n        str: A string representation of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(dataset)\n        Split: Train\n\n    \"\"\"\n    split = \"Train\" if self.train else \"Test\"\n    return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotpython/data/diabetes/#spotpython.data.diabetes.Diabetes.get_names","title":"<code>get_names()</code>","text":"<p>Returns the names of the features.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing the names of the features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n    dataset = Diabetes()\n    print(dataset.get_names())\n    [\"age\", \"sex\", \"bmi\", \"bp\", \"tc\", \"ldl\", \"hdl\", \"tch\", \"ltg\", \"glu\"]\n</code></pre> Source code in <code>spotpython/data/diabetes.py</code> <pre><code>def get_names(self) -&gt; list:\n    \"\"\"\n    Returns the names of the features.\n\n    Returns:\n        list: A list containing the names of the features.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n            dataset = Diabetes()\n            print(dataset.get_names())\n            [\"age\", \"sex\", \"bmi\", \"bp\", \"tc\", \"ldl\", \"hdl\", \"tch\", \"ltg\", \"glu\"]\n    \"\"\"\n    return [\"age\", \"sex\", \"bmi\", \"bp\", \"s1_tc\", \"s2_ldl\", \"s3_hdl\", \"s4_tch\", \"s5_ltg\", \"s6_glu\"]\n</code></pre>"},{"location":"reference/spotpython/data/friedman/","title":"friedman","text":""},{"location":"reference/spotpython/data/friedman/#spotpython.data.friedman.FriedmanDriftDataset","title":"<code>FriedmanDriftDataset</code>","text":"<p>Friedman Drift Dataset.</p> Source code in <code>spotpython/data/friedman.py</code> <pre><code>class FriedmanDriftDataset:\n    \"\"\"Friedman Drift Dataset.\"\"\"\n\n    def __init__(self, n_samples=100, change_point1=50, change_point2=75, seed=None, constant=False) -&gt; None:\n        \"\"\"Constructor for the Friedman Drift Dataset.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            change_point1 (int): The index of the first change point.\n            change_point2 (int): The index of the second change point.\n            seed (int): The seed for the random number generator.\n            constant (bool): If True, only the first feature is set to 1 and all others are set to 0.\n\n        Returns:\n            None (None): None\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.friedman import FriedmanDriftDataset\n                data_generator = FriedmanDriftDataset(n_samples=100,\n                    seed=42, change_point1=50, change_point2=75, constant=False)\n                data = [data for data in data_generator]\n                indices = [i for _, _, i in data]\n                values = {f\"x{i}\": [] for i in range(5)}\n                values[\"y\"] = []\n                for x, y, _ in data:\n                    for i in range(5):\n                        values[f\"x{i}\"].append(x[i])\n                    values[\"y\"].append(y)\n                plt.figure(figsize=(10, 6))\n                for label, series in values.items():\n                    plt.plot(indices, series, label=label)\n                plt.xlabel('Index')\n                plt.ylabel('Value')\n                plt.title('')\n                plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n                plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n                plt.legend()\n                plt.grid(True)\n                plt.show()\n        \"\"\"\n        self.n_samples = n_samples\n        self._change_point1 = change_point1\n        self._change_point2 = change_point2\n        self.seed = seed\n        self.index = 0\n        self.rng = random.Random(self.seed)\n        self.constant = constant\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.index &gt;= self.n_samples:  # Specifying end of generation\n            raise StopIteration\n        if self.constant:\n            # x[0] is set to 1, all others to 0\n            x = {0: 1}\n            x.update({i: 0 for i in range(1, 10)})  # All x[i] are 0 for i &gt; 0\n        else:\n            x = {i: self.rng.uniform(a=0, b=1) for i in range(10)}\n        y = self._global_recurring_abrupt_gen(x, self.index) + self.rng.gauss(mu=0, sigma=1)\n        result = (x, y, self.index)\n        self.index += 1\n        return result\n\n    def _global_recurring_abrupt_gen(self, x, index):\n        if index &lt; self._change_point1 or index &gt;= self._change_point2:\n            return 10 * math.sin(math.pi * x[0] * x[1]) + 20 * (x[2] - 0.5) ** 2 + 10 * x[3] + 5 * x[4]\n        else:\n            return 10 * math.sin(math.pi * x[3] * x[5]) + 20 * (x[1] - 0.5) ** 2 + 10 * x[0] + 5 * x[2]\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n\n        \"\"\"\n        return self.n_samples\n</code></pre>"},{"location":"reference/spotpython/data/friedman/#spotpython.data.friedman.FriedmanDriftDataset.__init__","title":"<code>__init__(n_samples=100, change_point1=50, change_point2=75, seed=None, constant=False)</code>","text":"<p>Constructor for the Friedman Drift Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> <code>100</code> <code>change_point1</code> <code>int</code> <p>The index of the first change point.</p> <code>50</code> <code>change_point2</code> <code>int</code> <p>The index of the second change point.</p> <code>75</code> <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> <code>None</code> <code>constant</code> <code>bool</code> <p>If True, only the first feature is set to 1 and all others are set to 0.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.friedman import FriedmanDriftDataset\n    data_generator = FriedmanDriftDataset(n_samples=100,\n        seed=42, change_point1=50, change_point2=75, constant=False)\n    data = [data for data in data_generator]\n    indices = [i for _, _, i in data]\n    values = {f\"x{i}\": [] for i in range(5)}\n    values[\"y\"] = []\n    for x, y, _ in data:\n        for i in range(5):\n            values[f\"x{i}\"].append(x[i])\n        values[\"y\"].append(y)\n    plt.figure(figsize=(10, 6))\n    for label, series in values.items():\n        plt.plot(indices, series, label=label)\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.title('')\n    plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n    plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n</code></pre> Source code in <code>spotpython/data/friedman.py</code> <pre><code>def __init__(self, n_samples=100, change_point1=50, change_point2=75, seed=None, constant=False) -&gt; None:\n    \"\"\"Constructor for the Friedman Drift Dataset.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        change_point1 (int): The index of the first change point.\n        change_point2 (int): The index of the second change point.\n        seed (int): The seed for the random number generator.\n        constant (bool): If True, only the first feature is set to 1 and all others are set to 0.\n\n    Returns:\n        None (None): None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.friedman import FriedmanDriftDataset\n            data_generator = FriedmanDriftDataset(n_samples=100,\n                seed=42, change_point1=50, change_point2=75, constant=False)\n            data = [data for data in data_generator]\n            indices = [i for _, _, i in data]\n            values = {f\"x{i}\": [] for i in range(5)}\n            values[\"y\"] = []\n            for x, y, _ in data:\n                for i in range(5):\n                    values[f\"x{i}\"].append(x[i])\n                values[\"y\"].append(y)\n            plt.figure(figsize=(10, 6))\n            for label, series in values.items():\n                plt.plot(indices, series, label=label)\n            plt.xlabel('Index')\n            plt.ylabel('Value')\n            plt.title('')\n            plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n            plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n            plt.legend()\n            plt.grid(True)\n            plt.show()\n    \"\"\"\n    self.n_samples = n_samples\n    self._change_point1 = change_point1\n    self._change_point2 = change_point2\n    self.seed = seed\n    self.index = 0\n    self.rng = random.Random(self.seed)\n    self.constant = constant\n</code></pre>"},{"location":"reference/spotpython/data/friedman/#spotpython.data.friedman.FriedmanDriftDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> Source code in <code>spotpython/data/friedman.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n\n    \"\"\"\n    return self.n_samples\n</code></pre>"},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/","title":"lightcrossvalidationdatamodule","text":""},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/#spotpython.data.lightcrossvalidationdatamodule.LightCrossValidationDataModule","title":"<code>LightCrossValidationDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for handling cross-validation data splits.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The size of the batch. Defaults to 64.</p> <code>64</code> <code>k</code> <code>int</code> <p>The fold number. Defaults to 1.</p> <code>1</code> <code>split_seed</code> <code>int</code> <p>The random seed for splitting the data. Defaults to 42.</p> <code>42</code> <code>num_splits</code> <code>int</code> <p>The number of splits for cross-validation. Defaults to 10.</p> <code>10</code> <code>data_dir</code> <code>str</code> <p>The path to the dataset. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <code>num_workers</code> <code>int</code> <p>The number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory for data loading. Defaults to False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>data_train</code> <code>Optional[Dataset]</code> <p>The training dataset.</p> <code>data_val</code> <code>Optional[Dataset]</code> <p>The validation dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n&gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n&gt;&gt;&gt; data_module.setup()\n&gt;&gt;&gt; print(f\"Training set size: {len(data_module.data_train)}\")\nTraining set size: 45000\n&gt;&gt;&gt; print(f\"Validation set size: {len(data_module.data_val)}\")\nValidation set size: 5000\n&gt;&gt;&gt; print(f\"Test set size: {len(data_module.data_test)}\")\nTest set size: 10000\n</code></pre> Source code in <code>spotpython/data/lightcrossvalidationdatamodule.py</code> <pre><code>class LightCrossValidationDataModule(L.LightningDataModule):\n    \"\"\"\n    A LightningDataModule for handling cross-validation data splits.\n\n    Args:\n        batch_size (int): The size of the batch. Defaults to 64.\n        k (int): The fold number. Defaults to 1.\n        split_seed (int): The random seed for splitting the data. Defaults to 42.\n        num_splits (int): The number of splits for cross-validation. Defaults to 10.\n        data_dir (str): The path to the dataset. Defaults to \"./data\".\n        num_workers (int): The number of workers for data loading. Defaults to 0.\n        pin_memory (bool): Whether to pin memory for data loading. Defaults to False.\n\n    Attributes:\n        data_train (Optional[Dataset]): The training dataset.\n        data_val (Optional[Dataset]): The validation dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n        &gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; print(f\"Training set size: {len(data_module.data_train)}\")\n        Training set size: 45000\n        &gt;&gt;&gt; print(f\"Validation set size: {len(data_module.data_val)}\")\n        Validation set size: 5000\n        &gt;&gt;&gt; print(f\"Test set size: {len(data_module.data_test)}\")\n        Test set size: 10000\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size=64,\n        dataset=None,\n        k: int = 1,\n        split_seed: int = 42,\n        num_splits: int = 10,\n        data_dir: str = \"./data\",\n        num_workers: int = 0,\n        pin_memory: bool = False,\n        scaler: Optional[object] = None,\n    ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.data_full = dataset\n        self.data_dir = data_dir\n        self.num_workers = num_workers\n        self.k = k\n        self.split_seed = split_seed\n        self.num_splits = num_splits\n        self.pin_memory = pin_memory\n        self.scaler = scaler\n        self.save_hyperparameters(logger=False)\n        assert 0 &lt;= self.k &lt; self.num_splits, \"incorrect fold number\"\n\n        # no data transformations\n        self.transforms = None\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Prepares the data for use.\"\"\"\n        # download\n        pass\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Sets up the data for use.\n\n        Args:\n            stage (Optional[str]): The current stage. Defaults to None.\n        \"\"\"\n        if not self.data_train and not self.data_val:\n            dataset_full = self.data_full\n            kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n            all_splits = [k for k in kf.split(dataset_full)]\n            train_indexes, val_indexes = all_splits[self.hparams.k]\n            train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n            self.data_train = Subset(dataset_full, train_indexes)\n            print(f\"Train Dataset Size: {len(self.data_train)}\")\n            self.data_val = Subset(dataset_full, val_indexes)\n            print(f\"Val Dataset Size: {len(self.data_val)}\")\n\n        if self.scaler is not None:\n            # Fit the scaler on training data and transform both train and val data\n            scaler_train_data = torch.stack([self.data_train[i][0] for i in range(len(self.data_train))]).squeeze(1)\n            self.scaler.fit(scaler_train_data)\n            self.data_train = [(self.scaler.transform(data), target) for data, target in self.data_train]\n            data_tensors_train = [data.clone().detach() for data, target in self.data_train]\n            target_tensors_train = [target.clone().detach() for data, target in self.data_train]\n            self.data_train = TensorDataset(torch.stack(data_tensors_train).squeeze(1), torch.stack(target_tensors_train))\n            self.data_val = [(self.scaler.transform(data), target) for data, target in self.data_val]\n            data_tensors_val = [data.clone().detach() for data, target in self.data_val]\n            target_tensors_val = [target.clone().detach() for data, target in self.data_val]\n            self.data_val = TensorDataset(torch.stack(data_tensors_val).squeeze(1), torch.stack(target_tensors_val))\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the training dataloader.\n\n        Returns:\n            DataLoader: The training dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n            &gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n            &gt;&gt;&gt; data_module.setup()\n            &gt;&gt;&gt; train_dataloader = data_module.train_dataloader()\n            &gt;&gt;&gt; print(f\"Training set size: {len(train_dataloader.dataset)}\")\n            Training set size: 45000\n\n        \"\"\"\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the validation dataloader.\n\n        Returns:\n            DataLoader: The validation dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n            &gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n            &gt;&gt;&gt; data_module.setup()\n            &gt;&gt;&gt; val_dataloader = data_module.val_dataloader()\n            &gt;&gt;&gt; print(f\"Validation set size: {len(val_dataloader.dataset)}\")\n            Validation set size: 5000\n        \"\"\"\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n        )\n</code></pre>"},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/#spotpython.data.lightcrossvalidationdatamodule.LightCrossValidationDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepares the data for use.</p> Source code in <code>spotpython/data/lightcrossvalidationdatamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Prepares the data for use.\"\"\"\n    # download\n    pass\n</code></pre>"},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/#spotpython.data.lightcrossvalidationdatamodule.LightCrossValidationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Sets up the data for use.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>The current stage. Defaults to None.</p> <code>None</code> Source code in <code>spotpython/data/lightcrossvalidationdatamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Sets up the data for use.\n\n    Args:\n        stage (Optional[str]): The current stage. Defaults to None.\n    \"\"\"\n    if not self.data_train and not self.data_val:\n        dataset_full = self.data_full\n        kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n        all_splits = [k for k in kf.split(dataset_full)]\n        train_indexes, val_indexes = all_splits[self.hparams.k]\n        train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n        self.data_train = Subset(dataset_full, train_indexes)\n        print(f\"Train Dataset Size: {len(self.data_train)}\")\n        self.data_val = Subset(dataset_full, val_indexes)\n        print(f\"Val Dataset Size: {len(self.data_val)}\")\n\n    if self.scaler is not None:\n        # Fit the scaler on training data and transform both train and val data\n        scaler_train_data = torch.stack([self.data_train[i][0] for i in range(len(self.data_train))]).squeeze(1)\n        self.scaler.fit(scaler_train_data)\n        self.data_train = [(self.scaler.transform(data), target) for data, target in self.data_train]\n        data_tensors_train = [data.clone().detach() for data, target in self.data_train]\n        target_tensors_train = [target.clone().detach() for data, target in self.data_train]\n        self.data_train = TensorDataset(torch.stack(data_tensors_train).squeeze(1), torch.stack(target_tensors_train))\n        self.data_val = [(self.scaler.transform(data), target) for data, target in self.data_val]\n        data_tensors_val = [data.clone().detach() for data, target in self.data_val]\n        target_tensors_val = [target.clone().detach() for data, target in self.data_val]\n        self.data_val = TensorDataset(torch.stack(data_tensors_val).squeeze(1), torch.stack(target_tensors_val))\n</code></pre>"},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/#spotpython.data.lightcrossvalidationdatamodule.LightCrossValidationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The training dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n&gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n&gt;&gt;&gt; data_module.setup()\n&gt;&gt;&gt; train_dataloader = data_module.train_dataloader()\n&gt;&gt;&gt; print(f\"Training set size: {len(train_dataloader.dataset)}\")\nTraining set size: 45000\n</code></pre> Source code in <code>spotpython/data/lightcrossvalidationdatamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the training dataloader.\n\n    Returns:\n        DataLoader: The training dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n        &gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; train_dataloader = data_module.train_dataloader()\n        &gt;&gt;&gt; print(f\"Training set size: {len(train_dataloader.dataset)}\")\n        Training set size: 45000\n\n    \"\"\"\n    return DataLoader(\n        dataset=self.data_train,\n        batch_size=self.hparams.batch_size,\n        num_workers=self.hparams.num_workers,\n        pin_memory=self.hparams.pin_memory,\n        shuffle=True,\n    )\n</code></pre>"},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/#spotpython.data.lightcrossvalidationdatamodule.LightCrossValidationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The validation dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n&gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n&gt;&gt;&gt; data_module.setup()\n&gt;&gt;&gt; val_dataloader = data_module.val_dataloader()\n&gt;&gt;&gt; print(f\"Validation set size: {len(val_dataloader.dataset)}\")\nValidation set size: 5000\n</code></pre> Source code in <code>spotpython/data/lightcrossvalidationdatamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the validation dataloader.\n\n    Returns:\n        DataLoader: The validation dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n        &gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; val_dataloader = data_module.val_dataloader()\n        &gt;&gt;&gt; print(f\"Validation set size: {len(val_dataloader.dataset)}\")\n        Validation set size: 5000\n    \"\"\"\n    return DataLoader(\n        dataset=self.data_val,\n        batch_size=self.hparams.batch_size,\n        num_workers=self.hparams.num_workers,\n        pin_memory=self.hparams.pin_memory,\n    )\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/","title":"lightdatamodule","text":""},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule","title":"<code>LightDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for handling data.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size. Required.</p> required <code>dataset</code> <code>Dataset</code> <p>The dataset from the torch.utils.data Dataset class. It  must implement three functions: init, len, and getitem. Required.</p> required <code>test_size</code> <code>float</code> <p>The test size. if test_size is float, then train_size is 1 - test_size. If test_size is int, then train_size is len(data_full) - test_size. Train size will be split into train and validation sets. So if test size is 0.7, the 0.7 train size will be split into 0.7 * 0.7 = 0.49 train set amd 0.7 * 0.3 = 0.21 validation set.</p> required <code>test_seed</code> <code>int</code> <p>The test seed. Defaults to 42.</p> <code>42</code> <code>num_workers</code> <code>int</code> <p>The number of workers. Defaults to 0.</p> <code>0</code> <code>scaler</code> <code>object</code> <p>The spot scaler object (e.g. TorchStandardScaler). Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>data_full</code> <code>Dataset</code> <p>The full dataset.</p> <code>data_test</code> <code>Dataset</code> <p>The test dataset.</p> <code>data_train</code> <code>Dataset</code> <p>The training dataset.</p> <code>data_val</code> <code>Dataset</code> <p>The validation dataset.</p> <code>num_workers</code> <code>int</code> <p>The number of workers.</p> <code>test_seed</code> <code>int</code> <p>The test seed.</p> <code>test_size</code> <code>float</code> <p>The test size.</p> <p>Methods:</p> Name Description <code>prepare_data</code> <p>Usually used for downloading the data. Here: Does nothing, i.e., pass.</p> <code>setup</code> <p>Optional[str] = None): Performs the training, validation, and test split.</p> <code>train_dataloader</code> <p>Returns a DataLoader instance for the training set.</p> <code>val_dataloader</code> <p>Returns a DataLoader instance for the validation set.</p> <code>test_dataloader</code> <p>Returns a DataLoader instance for the test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    from spotpython.utils.scaler import TorchStandardScaler\n    import torch\n    # data.csv is simple csv file with 11 samples\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    scaler = TorchStandardScaler()\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n    data_module.setup()\n    print(f\"Training set size: {len(data_module.data_train)}\")\n    print(f\"Validation set size: {len(data_module.data_val)}\")\n    print(f\"Test set size: {len(data_module.data_test)}\")\n    full_train_size: 0.5\n    val_size: 0.25\n    train_size: 0.25\n    test_size: 0.5\n    Training set size: 3\n    Validation set size: 3\n    Test set size: 6\n</code></pre> References <p>See https://lightning.ai/docs/pytorch/stable/data/datamodule.html</p> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>class LightDataModule(L.LightningDataModule):\n    \"\"\"\n    A LightningDataModule for handling data.\n\n    Args:\n        batch_size (int):\n            The batch size. Required.\n        dataset (torch.utils.data.Dataset):\n            The dataset from the torch.utils.data Dataset class.\n            It  must implement three functions: __init__, __len__, and __getitem__.\n            Required.\n        test_size (float):\n            The test size. if test_size is float, then train_size is 1 - test_size.\n            If test_size is int, then train_size is len(data_full) - test_size.\n            Train size will be split into train and validation sets.\n            So if test size is 0.7, the 0.7 train size will be split into 0.7 * 0.7 = 0.49 train set\n            amd 0.7 * 0.3 = 0.21 validation set.\n        test_seed (int):\n            The test seed. Defaults to 42.\n        num_workers (int):\n            The number of workers. Defaults to 0.\n        scaler (object):\n            The spot scaler object (e.g. TorchStandardScaler). Defaults to None.\n\n    Attributes:\n        batch_size (int): The batch size.\n        data_full (Dataset): The full dataset.\n        data_test (Dataset): The test dataset.\n        data_train (Dataset): The training dataset.\n        data_val (Dataset): The validation dataset.\n        num_workers (int): The number of workers.\n        test_seed (int): The test seed.\n        test_size (float): The test size.\n\n    Methods:\n        prepare_data(self):\n            Usually used for downloading the data. Here: Does nothing, i.e., pass.\n        setup(self, stage: Optional[str] = None):\n            Performs the training, validation, and test split.\n        train_dataloader():\n            Returns a DataLoader instance for the training set.\n        val_dataloader():\n            Returns a DataLoader instance for the validation set.\n        test_dataloader():\n            Returns a DataLoader instance for the test set.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            from spotpython.utils.scaler import TorchStandardScaler\n            import torch\n            # data.csv is simple csv file with 11 samples\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            scaler = TorchStandardScaler()\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n            data_module.setup()\n            print(f\"Training set size: {len(data_module.data_train)}\")\n            print(f\"Validation set size: {len(data_module.data_val)}\")\n            print(f\"Test set size: {len(data_module.data_test)}\")\n            full_train_size: 0.5\n            val_size: 0.25\n            train_size: 0.25\n            test_size: 0.5\n            Training set size: 3\n            Validation set size: 3\n            Test set size: 6\n\n    References:\n        See https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        dataset: object,\n        test_size: float,\n        test_seed: int = 42,\n        num_workers: int = 0,\n        scaler: Optional[object] = None,\n        verbosity: int = 0,\n    ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.data_full = dataset\n        self.test_size = test_size\n        self.test_seed = test_seed\n        self.num_workers = num_workers\n        self.scaler = scaler\n        self.verbosity = verbosity\n\n    def transform_dataset(self, dataset) -&gt; TensorDataset:\n        \"\"\"Applies the scaler transformation to the dataset.\n\n        Args:\n            dataset (List[Tuple[torch.Tensor, Any]]): The dataset to transform, consisting of data and target pairs.\n\n        Returns:\n            TensorDataset: A PyTorch TensorDataset containing the transformed and cloned data and targets.\n\n        Raises:\n            ValueError: If the input data is not correctly formatted for transformation.\n        \"\"\"\n        try:\n            # Perform transformations on the data in a single iteration\n            transformed_data = [(self.scaler.transform(data), target) for data, target in dataset]\n            # Clone and detach data tensors\n            data_tensors = [data.clone().detach() for data, _ in transformed_data]\n            target_tensors = [target.clone().detach() for _, target in transformed_data]\n            # Create a TensorDataset from the processed data\n            return TensorDataset(torch.stack(data_tensors).squeeze(1), torch.stack(target_tensors))\n        except Exception as e:\n            raise ValueError(f\"Error transforming dataset: {e}\")\n\n    def handle_scaling_and_transform(self) -&gt; None:\n        \"\"\"\n        Fits the scaler on the training data and transforms both training and validation datasets.\n        This function is only called when self.scaler is not None.\n        \"\"\"\n        # Ensure self.scaler is not None before proceeding\n        if self.scaler is None:\n            raise ValueError(\"Scaler object is required to perform scaling and transformation.\")\n        # Fit the scaler on training data\n        scaler_train_data = torch.stack([self.data_train[i][0] for i in range(len(self.data_train))]).squeeze(1)\n        if self.verbosity &gt; 0:\n            print(scaler_train_data.shape)\n        self.scaler.fit(scaler_train_data)\n        # Transform the training data\n        self.data_train = self.transform_dataset(self.data_train)\n        # Transform the validation data\n        self.data_val = self.transform_dataset(self.data_val)\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Prepares the data for use.\"\"\"\n        # download\n        pass\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Splits the data for use in training, validation, and testing.\n        Uses torch.utils.data.random_split() to split the data.\n        Splitting is based on the test_size and test_seed.\n        The test_size can be a float or an int.\n        If a spotpython scaler object is defined, the data will be scaled.\n\n        Args:\n            stage (Optional[str]):\n                The current stage. Can be \"fit\" (for training and validation), \"test\" (testing),\n                or None (for all three stages). Defaults to None.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n                from spotpython.data.csvdataset import CSVDataset\n                import torch\n                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n                data_module.setup()\n                print(f\"Training set size: {len(data_module.data_train)}\")\n                Training set size: 3\n\n        \"\"\"\n        full_train_size, val_size, train_size, test_size = calculate_data_split(\n            test_size=self.test_size,\n            full_size=len(self.data_full),\n            verbosity=self.verbosity,\n            stage=stage,\n        )\n\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            if self.verbosity &gt; 0:\n                print(f\"train_size: {train_size}, val_size: {val_size} used for train &amp; val data.\")\n            generator_fit = torch.Generator().manual_seed(self.test_seed)\n            self.data_train, self.data_val, _ = random_split(self.data_full, [train_size, val_size, test_size], generator=generator_fit)\n            # Handle scaling and transformation if scaler is provided\n            if self.scaler is not None:\n                self.handle_scaling_and_transform()\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            if self.verbosity &gt; 0:\n                print(f\"test_size: {test_size} used for test dataset.\")\n            generator_test = torch.Generator().manual_seed(self.test_seed)\n            self.data_test, _ = random_split(self.data_full, [test_size, full_train_size], generator=generator_test)\n            if self.scaler is not None:\n                # Transform the test data\n                self.data_test = self.transform_dataset(self.data_test)\n\n        # Assign pred dataset for use in dataloader(s)\n        if stage == \"predict\" or stage is None:\n            if self.verbosity &gt; 0:\n                print(f\"test_size: {test_size} used for predict dataset.\")\n            generator_predict = torch.Generator().manual_seed(self.test_seed)\n            self.data_predict, _ = random_split(self.data_full, [test_size, full_train_size], generator=generator_predict)\n            if self.scaler is not None:\n                # Transform the predict data\n                self.data_predict = self.transform_dataset(self.data_predict)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the training dataloader, i.e., a pytorch DataLoader instance\n        using the training dataset.\n\n        Returns:\n            DataLoader: The training dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n                from spotpython.data.csvdataset import CSVDataset\n                import torch\n                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n                data_module.setup()\n                print(f\"Training set size: {len(data_module.data_train)}\")\n                Training set size: 3\n\n        \"\"\"\n        if self.verbosity &gt; 0:\n            print(f\"LightDataModule.train_dataloader(). data_train size: {len(self.data_train)}\")\n        # print(f\"LightDataModule: train_dataloader(). batch_size: {self.batch_size}\")\n        # print(f\"LightDataModule: train_dataloader(). num_workers: {self.num_workers}\")\n        return DataLoader(self.data_train, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the validation dataloader, i.e., a pytorch DataLoader instance\n        using the validation dataset.\n\n        Returns:\n            DataLoader: The validation dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n                from spotpython.data.csvdataset import CSVDataset\n                import torch\n                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n                data_module.setup()\n                print(f\"Training set size: {len(data_module.data_val)}\")\n                Training set size: 3\n        \"\"\"\n        if self.verbosity &gt; 0:\n            print(f\"LightDataModule.val_dataloader(). Val. set size: {len(self.data_val)}\")\n        # print(f\"LightDataModule: val_dataloader(). batch_size: {self.batch_size}\")\n        # print(f\"LightDataModule: val_dataloader(). num_workers: {self.num_workers}\")\n        return DataLoader(self.data_val, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the test dataloader, i.e., a pytorch DataLoader instance\n        using the test dataset.\n\n        Returns:\n            DataLoader: The test dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n                from spotpython.data.csvdataset import CSVDataset\n                import torch\n                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n                data_module.setup()\n                print(f\"Test set size: {len(data_module.data_test)}\")\n                Test set size: 6\n\n        \"\"\"\n        if self.verbosity &gt; 0:\n            print(f\"LightDataModule.test_dataloader(). Test set size: {len(self.data_test)}\")\n        # print(f\"LightDataModule: test_dataloader(). batch_size: {self.batch_size}\")\n        # print(f\"LightDataModule: test_dataloader(). num_workers: {self.num_workers}\")\n        return DataLoader(self.data_test, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def predict_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the predict dataloader, i.e., a pytorch DataLoader instance\n        using the predict dataset.\n\n        Returns:\n            DataLoader: The predict dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n                from spotpython.data.csvdataset import CSVDataset\n                import torch\n                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n                data_module.setup()\n                print(f\"Predict set size: {len(data_module.data_predict)}\")\n                Predict set size: 6\n\n        \"\"\"\n        if self.verbosity &gt; 0:\n            print(f\"LightDataModule.predict_dataloader(). Predict set size: {len(self.data_predict)}\")\n        # print(f\"LightDataModule: predict_dataloader(). batch_size: {self.batch_size}\")\n        # print(f\"LightDataModule: predict_dataloader(). num_workers: {self.num_workers}\")\n        return DataLoader(self.data_predict, batch_size=len(self.data_predict), num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.handle_scaling_and_transform","title":"<code>handle_scaling_and_transform()</code>","text":"<p>Fits the scaler on the training data and transforms both training and validation datasets. This function is only called when self.scaler is not None.</p> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def handle_scaling_and_transform(self) -&gt; None:\n    \"\"\"\n    Fits the scaler on the training data and transforms both training and validation datasets.\n    This function is only called when self.scaler is not None.\n    \"\"\"\n    # Ensure self.scaler is not None before proceeding\n    if self.scaler is None:\n        raise ValueError(\"Scaler object is required to perform scaling and transformation.\")\n    # Fit the scaler on training data\n    scaler_train_data = torch.stack([self.data_train[i][0] for i in range(len(self.data_train))]).squeeze(1)\n    if self.verbosity &gt; 0:\n        print(scaler_train_data.shape)\n    self.scaler.fit(scaler_train_data)\n    # Transform the training data\n    self.data_train = self.transform_dataset(self.data_train)\n    # Transform the validation data\n    self.data_val = self.transform_dataset(self.data_val)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns the predict dataloader, i.e., a pytorch DataLoader instance using the predict dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The predict dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    import torch\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n    data_module.setup()\n    print(f\"Predict set size: {len(data_module.data_predict)}\")\n    Predict set size: 6\n</code></pre> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the predict dataloader, i.e., a pytorch DataLoader instance\n    using the predict dataset.\n\n    Returns:\n        DataLoader: The predict dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            import torch\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n            data_module.setup()\n            print(f\"Predict set size: {len(data_module.data_predict)}\")\n            Predict set size: 6\n\n    \"\"\"\n    if self.verbosity &gt; 0:\n        print(f\"LightDataModule.predict_dataloader(). Predict set size: {len(self.data_predict)}\")\n    # print(f\"LightDataModule: predict_dataloader(). batch_size: {self.batch_size}\")\n    # print(f\"LightDataModule: predict_dataloader(). num_workers: {self.num_workers}\")\n    return DataLoader(self.data_predict, batch_size=len(self.data_predict), num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepares the data for use.</p> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Prepares the data for use.\"\"\"\n    # download\n    pass\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Splits the data for use in training, validation, and testing. Uses torch.utils.data.random_split() to split the data. Splitting is based on the test_size and test_seed. The test_size can be a float or an int. If a spotpython scaler object is defined, the data will be scaled.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>The current stage. Can be \u201cfit\u201d (for training and validation), \u201ctest\u201d (testing), or None (for all three stages). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    import torch\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n    data_module.setup()\n    print(f\"Training set size: {len(data_module.data_train)}\")\n    Training set size: 3\n</code></pre> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Splits the data for use in training, validation, and testing.\n    Uses torch.utils.data.random_split() to split the data.\n    Splitting is based on the test_size and test_seed.\n    The test_size can be a float or an int.\n    If a spotpython scaler object is defined, the data will be scaled.\n\n    Args:\n        stage (Optional[str]):\n            The current stage. Can be \"fit\" (for training and validation), \"test\" (testing),\n            or None (for all three stages). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            import torch\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n            data_module.setup()\n            print(f\"Training set size: {len(data_module.data_train)}\")\n            Training set size: 3\n\n    \"\"\"\n    full_train_size, val_size, train_size, test_size = calculate_data_split(\n        test_size=self.test_size,\n        full_size=len(self.data_full),\n        verbosity=self.verbosity,\n        stage=stage,\n    )\n\n    # Assign train/val datasets for use in dataloaders\n    if stage == \"fit\" or stage is None:\n        if self.verbosity &gt; 0:\n            print(f\"train_size: {train_size}, val_size: {val_size} used for train &amp; val data.\")\n        generator_fit = torch.Generator().manual_seed(self.test_seed)\n        self.data_train, self.data_val, _ = random_split(self.data_full, [train_size, val_size, test_size], generator=generator_fit)\n        # Handle scaling and transformation if scaler is provided\n        if self.scaler is not None:\n            self.handle_scaling_and_transform()\n\n    # Assign test dataset for use in dataloader(s)\n    if stage == \"test\" or stage is None:\n        if self.verbosity &gt; 0:\n            print(f\"test_size: {test_size} used for test dataset.\")\n        generator_test = torch.Generator().manual_seed(self.test_seed)\n        self.data_test, _ = random_split(self.data_full, [test_size, full_train_size], generator=generator_test)\n        if self.scaler is not None:\n            # Transform the test data\n            self.data_test = self.transform_dataset(self.data_test)\n\n    # Assign pred dataset for use in dataloader(s)\n    if stage == \"predict\" or stage is None:\n        if self.verbosity &gt; 0:\n            print(f\"test_size: {test_size} used for predict dataset.\")\n        generator_predict = torch.Generator().manual_seed(self.test_seed)\n        self.data_predict, _ = random_split(self.data_full, [test_size, full_train_size], generator=generator_predict)\n        if self.scaler is not None:\n            # Transform the predict data\n            self.data_predict = self.transform_dataset(self.data_predict)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader, i.e., a pytorch DataLoader instance using the test dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The test dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    import torch\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n    data_module.setup()\n    print(f\"Test set size: {len(data_module.data_test)}\")\n    Test set size: 6\n</code></pre> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the test dataloader, i.e., a pytorch DataLoader instance\n    using the test dataset.\n\n    Returns:\n        DataLoader: The test dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            import torch\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n            data_module.setup()\n            print(f\"Test set size: {len(data_module.data_test)}\")\n            Test set size: 6\n\n    \"\"\"\n    if self.verbosity &gt; 0:\n        print(f\"LightDataModule.test_dataloader(). Test set size: {len(self.data_test)}\")\n    # print(f\"LightDataModule: test_dataloader(). batch_size: {self.batch_size}\")\n    # print(f\"LightDataModule: test_dataloader(). num_workers: {self.num_workers}\")\n    return DataLoader(self.data_test, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader, i.e., a pytorch DataLoader instance using the training dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The training dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    import torch\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n    data_module.setup()\n    print(f\"Training set size: {len(data_module.data_train)}\")\n    Training set size: 3\n</code></pre> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the training dataloader, i.e., a pytorch DataLoader instance\n    using the training dataset.\n\n    Returns:\n        DataLoader: The training dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            import torch\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n            data_module.setup()\n            print(f\"Training set size: {len(data_module.data_train)}\")\n            Training set size: 3\n\n    \"\"\"\n    if self.verbosity &gt; 0:\n        print(f\"LightDataModule.train_dataloader(). data_train size: {len(self.data_train)}\")\n    # print(f\"LightDataModule: train_dataloader(). batch_size: {self.batch_size}\")\n    # print(f\"LightDataModule: train_dataloader(). num_workers: {self.num_workers}\")\n    return DataLoader(self.data_train, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.transform_dataset","title":"<code>transform_dataset(dataset)</code>","text":"<p>Applies the scaler transformation to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>List[Tuple[Tensor, Any]]</code> <p>The dataset to transform, consisting of data and target pairs.</p> required <p>Returns:</p> Name Type Description <code>TensorDataset</code> <code>TensorDataset</code> <p>A PyTorch TensorDataset containing the transformed and cloned data and targets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data is not correctly formatted for transformation.</p> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def transform_dataset(self, dataset) -&gt; TensorDataset:\n    \"\"\"Applies the scaler transformation to the dataset.\n\n    Args:\n        dataset (List[Tuple[torch.Tensor, Any]]): The dataset to transform, consisting of data and target pairs.\n\n    Returns:\n        TensorDataset: A PyTorch TensorDataset containing the transformed and cloned data and targets.\n\n    Raises:\n        ValueError: If the input data is not correctly formatted for transformation.\n    \"\"\"\n    try:\n        # Perform transformations on the data in a single iteration\n        transformed_data = [(self.scaler.transform(data), target) for data, target in dataset]\n        # Clone and detach data tensors\n        data_tensors = [data.clone().detach() for data, _ in transformed_data]\n        target_tensors = [target.clone().detach() for _, target in transformed_data]\n        # Create a TensorDataset from the processed data\n        return TensorDataset(torch.stack(data_tensors).squeeze(1), torch.stack(target_tensors))\n    except Exception as e:\n        raise ValueError(f\"Error transforming dataset: {e}\")\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader, i.e., a pytorch DataLoader instance using the validation dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The validation dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    import torch\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n    data_module.setup()\n    print(f\"Training set size: {len(data_module.data_val)}\")\n    Training set size: 3\n</code></pre> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the validation dataloader, i.e., a pytorch DataLoader instance\n    using the validation dataset.\n\n    Returns:\n        DataLoader: The validation dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            import torch\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n            data_module.setup()\n            print(f\"Training set size: {len(data_module.data_val)}\")\n            Training set size: 3\n    \"\"\"\n    if self.verbosity &gt; 0:\n        print(f\"LightDataModule.val_dataloader(). Val. set size: {len(self.data_val)}\")\n    # print(f\"LightDataModule: val_dataloader(). batch_size: {self.batch_size}\")\n    # print(f\"LightDataModule: val_dataloader(). num_workers: {self.num_workers}\")\n    return DataLoader(self.data_val, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/data/mnistdatamodule/","title":"mnistdatamodule","text":""},{"location":"reference/spotpython/data/pkldataset/","title":"pkldataset","text":""},{"location":"reference/spotpython/data/pkldataset/#spotpython.data.pkldataset.PKLDataset","title":"<code>PKLDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset Class for handling pickle (*.pkl) data.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename of the pkl file. Defaults to \u201cdata.pkl\u201d.</p> <code>'data.pkl'</code> <code>directory</code> <code>str</code> <p>The directory where the pkl file is located. Defaults to None.</p> <code>None</code> <code>feature_type</code> <code>dtype</code> <p>The data type of the features. Defaults to torch.float.</p> <code>float</code> <code>target_column</code> <code>str</code> <p>The name of the target column. Defaults to \u201cy\u201d.</p> <code>'y'</code> <code>target_type</code> <code>dtype</code> <p>The data type of the targets. Defaults to torch.long.</p> <code>float</code> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not. Defaults to True.</p> <code>True</code> <code>rmNA</code> <code>bool</code> <p>Whether to remove rows with NA values or not. Defaults to True.</p> <code>True</code> <code>**desc</code> <code>Any</code> <p>Additional arguments to be passed to the base class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The filename of the pkl file.</p> <code>directory</code> <code>str</code> <p>The directory where the pkl file is located.</p> <code>feature_type</code> <code>dtype</code> <p>The data type of the features. Defaults to torch.float.</p> <code>target_column</code> <code>str</code> <p>The name of the target column.</p> <code>target_type</code> <code>dtype</code> <p>The data type of the targets. Defaults to torch.float.</p> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not.</p> <code>rmNA</code> <code>bool</code> <p>Whether to remove rows with NA values or not.</p> <code>data</code> <code>Tensor</code> <p>The features.</p> <code>targets</code> <code>Tensor</code> <p>The targets.</p> Notes <ul> <li><code>spotpython</code> comes with a sample pkl file, which is located at <code>spotpython/data/pkldataset.pkl</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    # Set batch size for DataLoader\n    batch_size = 5\n    # Create DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    # Iterate over the data in the DataLoader\n    for batch in dataloader:\n        inputs, targets = batch\n        print(f\"Batch Size: {inputs.size(0)}\")\n        print(\"---------------\")\n        print(f\"Inputs: {inputs}\")\n        print(f\"Targets: {targets}\")\n        break\n    Batch Size: 5\n    ---------------\n    Inputs: tensor([[1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n            0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n            [1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n            1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n            0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n            [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n            1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n            0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n    Targets: tensor([ 0,  1,  6,  9, 10])\n&gt;&gt;&gt; # Load the data from a different directory:\n&gt;&gt;&gt; # Similar to the above example, but with a different target column, full path, and different data type\n&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\",\n                        filename=\"data_sensitive.pkl\",\n                        target_column='N',\n                        feature_type=torch.float32,\n                        target_type=torch.float32,\n                        rmNA=True)\n</code></pre> Source code in <code>spotpython/data/pkldataset.py</code> <pre><code>class PKLDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset Class for handling pickle (*.pkl) data.\n\n    Args:\n        filename (str):\n            The filename of the pkl file. Defaults to \"data.pkl\".\n        directory (str):\n            The directory where the pkl file is located. Defaults to None.\n        feature_type (torch.dtype):\n            The data type of the features. Defaults to torch.float.\n        target_column (str):\n            The name of the target column. Defaults to \"y\".\n        target_type (torch.dtype):\n            The data type of the targets. Defaults to torch.long.\n        train (bool):\n            Whether the dataset is for training or not. Defaults to True.\n        rmNA (bool):\n            Whether to remove rows with NA values or not. Defaults to True.\n        **desc (Any):\n            Additional arguments to be passed to the base class.\n\n    Attributes:\n        filename (str):\n            The filename of the pkl file.\n        directory (str):\n            The directory where the pkl file is located.\n        feature_type (torch.dtype):\n            The data type of the features.\n            Defaults to torch.float.\n        target_column (str):\n            The name of the target column.\n        target_type (torch.dtype):\n            The data type of the targets.\n            Defaults to torch.float.\n        train (bool):\n            Whether the dataset is for training or not.\n        rmNA (bool):\n            Whether to remove rows with NA values or not.\n        data (torch.Tensor):\n            The features.\n        targets (torch.Tensor):\n            The targets.\n\n    Notes:\n        * `spotpython` comes with a sample pkl file, which is located at `spotpython/data/pkldataset.pkl`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            # Set batch size for DataLoader\n            batch_size = 5\n            # Create DataLoader\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n            # Iterate over the data in the DataLoader\n            for batch in dataloader:\n                inputs, targets = batch\n                print(f\"Batch Size: {inputs.size(0)}\")\n                print(\"---------------\")\n                print(f\"Inputs: {inputs}\")\n                print(f\"Targets: {targets}\")\n                break\n            Batch Size: 5\n            ---------------\n            Inputs: tensor([[1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n                    0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n                    [1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n                    0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n                    [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n                    1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n                    0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n            Targets: tensor([ 0,  1,  6,  9, 10])\n        &gt;&gt;&gt; # Load the data from a different directory:\n        &gt;&gt;&gt; # Similar to the above example, but with a different target column, full path, and different data type\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\",\n                                filename=\"data_sensitive.pkl\",\n                                target_column='N',\n                                feature_type=torch.float32,\n                                target_type=torch.float32,\n                                rmNA=True)\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"data.pkl\",\n        directory: None = None,\n        feature_type: torch.dtype = torch.float,\n        target_column: str = \"y\",\n        target_type: torch.dtype = torch.float,\n        train: bool = True,\n        rmNA=True,\n        oe=OrdinalEncoder(),\n        le=LabelEncoder(),\n        **desc,\n    ) -&gt; None:\n        super().__init__()\n        self.filename = filename\n        self.directory = directory\n        self.feature_type = feature_type\n        self.target_type = target_type\n        self.target_column = target_column\n        self.train = train\n        self.rmNA = rmNA\n        self.oe = oe\n        self.le = le\n        self.data, self.targets = self._load_data()\n\n    @property\n    def path(self):\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self):\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n\n    def _load_data(self) -&gt; tuple:\n        # ensure that self.target_type and self.feature_type are the same torch types\n        if self.target_type != self.feature_type:\n            raise ValueError(\"target_type and feature_type must be the same torch type\")\n        with open(self.path, \"rb\") as f:\n            df = pd.read_pickle(f)\n        # rm rows with NA\n        if self.rmNA:\n            df = df.dropna()\n\n        # Split DataFrame into feature and target DataFrames\n        feature_df = df.drop(columns=[self.target_column])\n\n        # Identify non-numerical columns in the feature DataFrame\n        non_numerical_columns = feature_df.select_dtypes(exclude=[\"number\"]).columns.tolist()\n\n        # Apply OrdinalEncoder to non-numerical feature columns\n        if non_numerical_columns:\n            feature_df[non_numerical_columns] = self.oe.fit_transform(feature_df[non_numerical_columns])\n\n        target_df = df[self.target_column]\n\n        # Check if the target column is non-numerical using dtype\n        if not pd.api.types.is_numeric_dtype(target_df):\n            target_df = self.le.fit_transform(target_df)\n\n        # Convert DataFrames to NumPy arrays and then to PyTorch tensors\n        feature_array = feature_df.to_numpy()\n        target_array = target_df\n\n        feature_tensor = torch.tensor(feature_array, dtype=self.feature_type)\n        target_tensor = torch.tensor(target_array, dtype=self.target_type)\n\n        return feature_tensor, target_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n                import torch\n                from torch.utils.data import DataLoader\n                dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([11, 64])\n                torch.Size([11])\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n                import torch\n                from torch.utils.data import DataLoader\n                dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n                print(len(dataset))\n                11\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string with the filename and directory of the dataset.\n\n        Returns:\n            str: A string with the filename and directory of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n                import torch\n                from torch.utils.data import DataLoader\n                dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n                print(dataset)\n        \"\"\"\n        return \"filename={}, directory={}\".format(self.filename, self.directory)\n\n    def __ncols__(self) -&gt; int:\n        \"\"\"\n        Returns the number of columns in the dataset.\n\n        Returns:\n            int: The number of columns in the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n                import torch\n                from torch.utils.data import DataLoader\n                dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n                print(dataset.__ncols__())\n                64\n        \"\"\"\n        return self.data.size(1)\n</code></pre>"},{"location":"reference/spotpython/data/pkldataset/#spotpython.data.pkldataset.PKLDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    print(dataset.data.shape)\n    print(dataset.targets.shape)\n    torch.Size([11, 64])\n    torch.Size([11])\n</code></pre> Source code in <code>spotpython/data/pkldataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            print(dataset.data.shape)\n            print(dataset.targets.shape)\n            torch.Size([11, 64])\n            torch.Size([11])\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotpython/data/pkldataset/#spotpython.data.pkldataset.PKLDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    print(len(dataset))\n    11\n</code></pre> Source code in <code>spotpython/data/pkldataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            print(len(dataset))\n            11\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/pkldataset/#spotpython.data.pkldataset.PKLDataset.__ncols__","title":"<code>__ncols__()</code>","text":"<p>Returns the number of columns in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of columns in the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    print(dataset.__ncols__())\n    64\n</code></pre> Source code in <code>spotpython/data/pkldataset.py</code> <pre><code>def __ncols__(self) -&gt; int:\n    \"\"\"\n    Returns the number of columns in the dataset.\n\n    Returns:\n        int: The number of columns in the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            print(dataset.__ncols__())\n            64\n    \"\"\"\n    return self.data.size(1)\n</code></pre>"},{"location":"reference/spotpython/data/pkldataset/#spotpython.data.pkldataset.PKLDataset.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string with the filename and directory of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string with the filename and directory of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    print(dataset)\n</code></pre> Source code in <code>spotpython/data/pkldataset.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string with the filename and directory of the dataset.\n\n    Returns:\n        str: A string with the filename and directory of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            print(dataset)\n    \"\"\"\n    return \"filename={}, directory={}\".format(self.filename, self.directory)\n</code></pre>"},{"location":"reference/spotpython/data/torchdata/","title":"torchdata","text":""},{"location":"reference/spotpython/data/torchdata/#spotpython.data.torchdata.load_data_cifar10","title":"<code>load_data_cifar10(data_dir='./data')</code>","text":"<p>Load the CIFAR-10 dataset.     This function loads the CIFAR-10 dataset using the torchvision library.     The data is split into a training set and a test set.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>The directory where the data is stored. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <p>Returns:</p> Type Description <code>Tuple[CIFAR10, CIFAR10]</code> <p>Tuple[datasets.CIFAR10, datasets.CIFAR10]: A tuple containing the training set and the test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; trainset, testset = load_data_cifar10()\n&gt;&gt;&gt; print(f\"Training set size: {len(trainset)}\")\nTraining set size: 50000\n&gt;&gt;&gt; print(f\"Test set size: {len(testset)}\")\nTest set size: 10000\n</code></pre> Source code in <code>spotpython/data/torchdata.py</code> <pre><code>def load_data_cifar10(data_dir: str = \"./data\") -&gt; Tuple[datasets.CIFAR10, datasets.CIFAR10]:\n    \"\"\"Load the CIFAR-10 dataset.\n        This function loads the CIFAR-10 dataset using the torchvision library.\n        The data is split into a training set and a test set.\n\n    Args:\n        data_dir (str):\n            The directory where the data is stored. Defaults to \"./data\".\n\n    Returns:\n        Tuple[datasets.CIFAR10, datasets.CIFAR10]:\n            A tuple containing the training set and the test set.\n\n    Examples:\n        &gt;&gt;&gt; trainset, testset = load_data_cifar10()\n        &gt;&gt;&gt; print(f\"Training set size: {len(trainset)}\")\n        Training set size: 50000\n        &gt;&gt;&gt; print(f\"Test set size: {len(testset)}\")\n        Test set size: 10000\n\n    \"\"\"\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n    trainset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n\n    testset = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n\n    return trainset, testset\n</code></pre>"},{"location":"reference/spotpython/data/vbdp/","title":"vbdp","text":""},{"location":"reference/spotpython/data/vbdp/#spotpython.data.vbdp.affinity_propagation_features","title":"<code>affinity_propagation_features(X)</code>","text":"<p>Clusters the features of a dataframe using Affinity Propagation.</p> <p>This function takes a dataframe with features and clusters them using the Affinity Propagation algorithm. The resulting dataframe contains the original features as well as a new feature representing the cluster labels.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>A dataframe with features.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the original features and a new cluster feature.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n&gt;&gt;&gt; df\n    a      b      c\n0  True   True   False\n1  False  True   False\n2  True   False  True\n&gt;&gt;&gt; affinity_propagation_features(df)\nEstimated number of clusters: 3\n    a      b      c  cluster\n0  True   True   False       0\n1  False  True   False       1\n2  True   False  True        2\n</code></pre> Source code in <code>spotpython/data/vbdp.py</code> <pre><code>def affinity_propagation_features(X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Clusters the features of a dataframe using Affinity Propagation.\n\n    This function takes a dataframe with features and clusters them using the\n    Affinity Propagation algorithm. The resulting dataframe contains the original\n    features as well as a new feature representing the cluster labels.\n\n    Args:\n        X (pd.DataFrame):\n            A dataframe with features.\n\n    Returns:\n        (pd.DataFrame):\n            A dataframe with the original features and a new cluster feature.\n\n    Examples:\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n        &gt;&gt;&gt; df\n            a      b      c\n        0  True   True   False\n        1  False  True   False\n        2  True   False  True\n        &gt;&gt;&gt; affinity_propagation_features(df)\n        Estimated number of clusters: 3\n            a      b      c  cluster\n        0  True   True   False       0\n        1  False  True   False       1\n        2  True   False  True        2\n    \"\"\"\n    D = manhattan_distances(X)\n    af = AffinityPropagation(random_state=0, affinity=\"precomputed\").fit(D)\n    cluster_centers_indices = af.cluster_centers_indices_\n    n_clusters_ = len(cluster_centers_indices)\n    print(\"Estimated number of clusters: %d\" % n_clusters_)\n    X[\"cluster\"] = af.labels_\n    return X\n</code></pre>"},{"location":"reference/spotpython/data/vbdp/#spotpython.data.vbdp.cluster_features","title":"<code>cluster_features(X)</code>","text":"<p>Clusters the features of a dataframe based on similarity.</p> <p>This function takes a dataframe with features and clusters them based on similarity. The resulting dataframe contains the original features as well as new features representing the clusters.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>A dataframe with features.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the original features and new cluster features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n&gt;&gt;&gt; df\n    a      b      c\n0  True   True  False\n1 False   True  False\n2  True  False   True\n&gt;&gt;&gt; cluster_features(df)\n    a      b      c  c_0  c_1  c_2  c_3\n0  True   True  False    0    0    0    0\n1 False   True  False    0    0    0    0\n2  True  False   True    0    0    0    0\n</code></pre> Source code in <code>spotpython/data/vbdp.py</code> <pre><code>def cluster_features(X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Clusters the features of a dataframe based on similarity.\n\n    This function takes a dataframe with features and clusters them based on similarity.\n    The resulting dataframe contains the original features as well as new features representing the clusters.\n\n    Args:\n        X (pd.DataFrame): A dataframe with features.\n\n    Returns:\n        (pd.DataFrame): A dataframe with the original features and new cluster features.\n\n    Examples:\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n        &gt;&gt;&gt; df\n            a      b      c\n        0  True   True  False\n        1 False   True  False\n        2  True  False   True\n        &gt;&gt;&gt; cluster_features(df)\n            a      b      c  c_0  c_1  c_2  c_3\n        0  True   True  False    0    0    0    0\n        1 False   True  False    0    0    0    0\n        2  True  False   True    0    0    0    0\n    \"\"\"\n    c_0 = X.columns[X.columns.str.contains(\"pain\")]\n    c_1 = X.columns[X.columns.str.contains(\"inflammation\")]\n    c_2 = X.columns[X.columns.str.contains(\"bleed\")]\n    c_3 = X.columns[X.columns.str.contains(\"skin\")]\n    X[\"c_0\"] = X[c_0].sum(axis=1)\n    X[\"c_1\"] = X[c_1].sum(axis=1)\n    X[\"c_2\"] = X[c_2].sum(axis=1)\n    X[\"c_3\"] = X[c_3].sum(axis=1)\n    return X\n</code></pre>"},{"location":"reference/spotpython/design/designs/","title":"designs","text":""},{"location":"reference/spotpython/design/designs/#spotpython.design.designs.Designs","title":"<code>Designs</code>","text":"<p>Super class for all design classes (factorial and space-filling).</p> <p>Attributes:</p> Name Type Description <code>designs</code> <code>List</code> <p>A list of design instances.</p> <code>k</code> <code>int</code> <p>The dimension of the design.</p> <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> <code>rng</code> <code>Generator</code> <p>A random number generator instance.</p> Source code in <code>spotpython/design/designs.py</code> <pre><code>class Designs:\n    \"\"\"\n    Super class for all design classes (factorial and space-filling).\n\n    Attributes:\n        designs (List): A list of design instances.\n        k (int): The dimension of the design.\n        seed (int): The seed for the random number generator.\n        rng (Generator): A random number generator instance.\n    \"\"\"\n\n    def __init__(self, k: int, seed: int = 123) -&gt; None:\n        \"\"\"\n        Initializes a Designs object with the given dimension and seed.\n\n        Args:\n            k (int): The dimension of the design.\n            seed (int): The seed for the random number generator. Defaults to 123.\n\n        Raises:\n            ValueError: If 'k' is not an integer.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.design.designs import Designs\n            &gt;&gt;&gt; designs = Designs(k=2, seed=123)\n            &gt;&gt;&gt; designs.get_dim()\n            2\n        \"\"\"\n        if not isinstance(k, int):\n            raise ValueError(\"The dimension of the design must be an integer.\")\n\n        self.k: int = k\n        self.seed: int = seed\n        self.rng = default_rng(self.seed)\n        self.designs: List = []\n\n    def get_dim(self) -&gt; int:\n        \"\"\"\n        Returns the dimension of the design.\n\n        Returns:\n            int: The dimension of the design.\n        \"\"\"\n        return self.k\n</code></pre>"},{"location":"reference/spotpython/design/designs/#spotpython.design.designs.Designs.__init__","title":"<code>__init__(k, seed=123)</code>","text":"<p>Initializes a Designs object with the given dimension and seed.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The dimension of the design.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 123.</p> <code>123</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If \u2018k\u2019 is not an integer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.design.designs import Designs\n&gt;&gt;&gt; designs = Designs(k=2, seed=123)\n&gt;&gt;&gt; designs.get_dim()\n2\n</code></pre> Source code in <code>spotpython/design/designs.py</code> <pre><code>def __init__(self, k: int, seed: int = 123) -&gt; None:\n    \"\"\"\n    Initializes a Designs object with the given dimension and seed.\n\n    Args:\n        k (int): The dimension of the design.\n        seed (int): The seed for the random number generator. Defaults to 123.\n\n    Raises:\n        ValueError: If 'k' is not an integer.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.design.designs import Designs\n        &gt;&gt;&gt; designs = Designs(k=2, seed=123)\n        &gt;&gt;&gt; designs.get_dim()\n        2\n    \"\"\"\n    if not isinstance(k, int):\n        raise ValueError(\"The dimension of the design must be an integer.\")\n\n    self.k: int = k\n    self.seed: int = seed\n    self.rng = default_rng(self.seed)\n    self.designs: List = []\n</code></pre>"},{"location":"reference/spotpython/design/designs/#spotpython.design.designs.Designs.get_dim","title":"<code>get_dim()</code>","text":"<p>Returns the dimension of the design.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The dimension of the design.</p> Source code in <code>spotpython/design/designs.py</code> <pre><code>def get_dim(self) -&gt; int:\n    \"\"\"\n    Returns the dimension of the design.\n\n    Returns:\n        int: The dimension of the design.\n    \"\"\"\n    return self.k\n</code></pre>"},{"location":"reference/spotpython/design/factorial/","title":"factorial","text":""},{"location":"reference/spotpython/design/factorial/#spotpython.design.factorial.factorial","title":"<code>factorial</code>","text":"<p>               Bases: <code>designs</code></p> <p>Super class for factorial designs.</p> <p>Attributes:</p> Name Type Description <code>k</code> <code>int</code> <p>The number of factors.</p> <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> Source code in <code>spotpython/design/factorial.py</code> <pre><code>class factorial(designs):\n    \"\"\"\n    Super class for factorial designs.\n\n    Attributes:\n        k (int): The number of factors.\n        seed (int): The seed for the random number generator.\n    \"\"\"\n\n    def __init__(self, k: int = 2, seed: int = 123) -&gt; None:\n        \"\"\"\n        Initializes a factorial design object.\n\n        Args:\n            k (int): The number of factors. Defaults to 2.\n            seed (int): The seed for the random number generator. Defaults to 123.\n        \"\"\"\n        super().__init__(k, seed)\n\n    def full_factorial(self, p: int) -&gt; \"np.ndarray\":\n        \"\"\"\n        Generates a full factorial design.\n\n        Args:\n            p (int): The number of levels for each factor.\n\n        Returns:\n            numpy.ndarray: A 2D array representing the full factorial design.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.design.factorial import factorial\n                factorial_design = factorial(k=2)\n                factorial_design.full_factorial(p=2)\n                array([[0., 0.],\n                    [0., 1.],\n                    [1., 0.],\n                    [1., 1.]])\n        \"\"\"\n        i = (slice(0, 1, p * 1j),) * self.k\n        return mgrid[i].reshape(self.k, p**self.k).T\n</code></pre>"},{"location":"reference/spotpython/design/factorial/#spotpython.design.factorial.factorial.__init__","title":"<code>__init__(k=2, seed=123)</code>","text":"<p>Initializes a factorial design object.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of factors. Defaults to 2.</p> <code>2</code> <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 123.</p> <code>123</code> Source code in <code>spotpython/design/factorial.py</code> <pre><code>def __init__(self, k: int = 2, seed: int = 123) -&gt; None:\n    \"\"\"\n    Initializes a factorial design object.\n\n    Args:\n        k (int): The number of factors. Defaults to 2.\n        seed (int): The seed for the random number generator. Defaults to 123.\n    \"\"\"\n    super().__init__(k, seed)\n</code></pre>"},{"location":"reference/spotpython/design/factorial/#spotpython.design.factorial.factorial.full_factorial","title":"<code>full_factorial(p)</code>","text":"<p>Generates a full factorial design.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>int</code> <p>The number of levels for each factor.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: A 2D array representing the full factorial design.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.design.factorial import factorial\n    factorial_design = factorial(k=2)\n    factorial_design.full_factorial(p=2)\n    array([[0., 0.],\n        [0., 1.],\n        [1., 0.],\n        [1., 1.]])\n</code></pre> Source code in <code>spotpython/design/factorial.py</code> <pre><code>def full_factorial(self, p: int) -&gt; \"np.ndarray\":\n    \"\"\"\n    Generates a full factorial design.\n\n    Args:\n        p (int): The number of levels for each factor.\n\n    Returns:\n        numpy.ndarray: A 2D array representing the full factorial design.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.design.factorial import factorial\n            factorial_design = factorial(k=2)\n            factorial_design.full_factorial(p=2)\n            array([[0., 0.],\n                [0., 1.],\n                [1., 0.],\n                [1., 1.]])\n    \"\"\"\n    i = (slice(0, 1, p * 1j),) * self.k\n    return mgrid[i].reshape(self.k, p**self.k).T\n</code></pre>"},{"location":"reference/spotpython/design/spacefilling/","title":"spacefilling","text":""},{"location":"reference/spotpython/design/spacefilling/#spotpython.design.spacefilling.SpaceFilling","title":"<code>SpaceFilling</code>","text":"<p>               Bases: <code>Designs</code></p> <p>A class for generating space-filling designs using Latin Hypercube Sampling.</p> Source code in <code>spotpython/design/spacefilling.py</code> <pre><code>class SpaceFilling(Designs):\n    \"\"\"\n    A class for generating space-filling designs using Latin Hypercube Sampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        k: int,\n        scramble: bool = True,\n        strength: int = 1,\n        optimization: Optional[Union[None, str]] = None,\n        seed: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a SpaceFilling design class.\n        Based on scipy.stats.qmc's LatinHypercube method.\n\n        Args:\n            k (int):\n                Dimension of the parameter space.\n            scramble (bool, optional):\n                When False, center samples within cells of a multi-dimensional grid.\n                Otherwise, samples are randomly placed within cells of the grid.\n                Note:\n                    Setting `scramble=False` does not ensure deterministic output. For that, use the `seed` parameter.\n                Default is True.\n            optimization (Optional[Union[None, str]]):\n                Whether to use an optimization scheme to improve the quality after sampling.\n                Note that this is a post-processing step that does not guarantee that all\n                properties of the sample will be conserved.\n                Defaults to None.\n                Options:\n                    - \"random-cd\": Random permutations of coordinates to lower the centered discrepancy. The best sample based on the centered discrepancy is constantly updated.\n                    Centered discrepancy-based sampling shows better space-filling\n                    robustness toward 2D and 3D subprojections compared to using other discrepancy measures.\n                    - \"lloyd\": Perturb samples using a modified Lloyd-Max algorithm. The process converges to equally spaced samples.\n            strength (Optional[int]):\n                Strength of the LHS. `strength=1` produces a plain LHS while `strength=2` produces an orthogonal array based LHS of strength 2.\n                In that case, only `n=p**2` points can be sampled, with `p` a prime number.\n                It also constrains `d &lt;= p + 1`.\n                Defaults to 1.\n            seed (int, optional):\n                Seed for the random number generator. Defaults to 123.\n        \"\"\"\n        super().__init__(k=k, seed=seed)\n        self.sampler = LatinHypercube(d=self.k, scramble=scramble, strength=strength, optimization=optimization, seed=seed)\n\n    def scipy_lhd(\n        self,\n        n: int,\n        repeats: int = 1,\n        lower: Optional[Union[int, float, np.ndarray]] = None,\n        upper: Optional[Union[int, float, np.ndarray]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Latin hypercube sampling based on scipy.\n\n        Args:\n            n (int): Number of samples.\n            repeats (int): Number of repeats (replicates).\n            lower (int, float, or np.ndarray, optional): Lower bound. Defaults to a zero vector.\n            upper (int, float, or np.ndarray, optional): Upper bound. Defaults to a one vector.\n\n        Returns:\n            np.ndarray: Latin hypercube design with specified dimensions and boundaries.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.design.spacefilling import SpaceFilling\n            &gt;&gt;&gt; lhd = SpaceFilling(k=2, seed=123)\n            &gt;&gt;&gt; lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0, 0]), upper=np.array([1, 1]))\n            array([[0.66352963, 0.5892358 ],\n                   [0.66352963, 0.5892358 ],\n                   [0.55592803, 0.96312564],\n                   [0.55592803, 0.96312564],\n                   [0.16481882, 0.0375811 ],\n                   [0.16481882, 0.0375811 ],\n                   [0.215331  , 0.34468512],\n                   [0.215331  , 0.34468512],\n                   [0.83604909, 0.62202146],\n                   [0.83604909, 0.62202146]])\n        \"\"\"\n        if lower is None:\n            lower = np.zeros(self.k)\n        if upper is None:\n            upper = np.ones(self.k)\n\n        sample = self.sampler.random(n=n)\n        des = scale(sample, lower, upper)\n        return np.repeat(des, repeats, axis=0)\n</code></pre>"},{"location":"reference/spotpython/design/spacefilling/#spotpython.design.spacefilling.SpaceFilling.__init__","title":"<code>__init__(k, scramble=True, strength=1, optimization=None, seed=123)</code>","text":"<p>Initializes a SpaceFilling design class. Based on scipy.stats.qmc\u2019s LatinHypercube method.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Dimension of the parameter space.</p> required <code>scramble</code> <code>bool</code> <p>When False, center samples within cells of a multi-dimensional grid. Otherwise, samples are randomly placed within cells of the grid. Note:     Setting <code>scramble=False</code> does not ensure deterministic output. For that, use the <code>seed</code> parameter. Default is True.</p> <code>True</code> <code>optimization</code> <code>Optional[Union[None, str]]</code> <p>Whether to use an optimization scheme to improve the quality after sampling. Note that this is a post-processing step that does not guarantee that all properties of the sample will be conserved. Defaults to None. Options:     - \u201crandom-cd\u201d: Random permutations of coordinates to lower the centered discrepancy. The best sample based on the centered discrepancy is constantly updated.     Centered discrepancy-based sampling shows better space-filling     robustness toward 2D and 3D subprojections compared to using other discrepancy measures.     - \u201clloyd\u201d: Perturb samples using a modified Lloyd-Max algorithm. The process converges to equally spaced samples.</p> <code>None</code> <code>strength</code> <code>Optional[int]</code> <p>Strength of the LHS. <code>strength=1</code> produces a plain LHS while <code>strength=2</code> produces an orthogonal array based LHS of strength 2. In that case, only <code>n=p**2</code> points can be sampled, with <code>p</code> a prime number. It also constrains <code>d &lt;= p + 1</code>. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>int</code> <p>Seed for the random number generator. Defaults to 123.</p> <code>123</code> Source code in <code>spotpython/design/spacefilling.py</code> <pre><code>def __init__(\n    self,\n    k: int,\n    scramble: bool = True,\n    strength: int = 1,\n    optimization: Optional[Union[None, str]] = None,\n    seed: int = 123,\n) -&gt; None:\n    \"\"\"\n    Initializes a SpaceFilling design class.\n    Based on scipy.stats.qmc's LatinHypercube method.\n\n    Args:\n        k (int):\n            Dimension of the parameter space.\n        scramble (bool, optional):\n            When False, center samples within cells of a multi-dimensional grid.\n            Otherwise, samples are randomly placed within cells of the grid.\n            Note:\n                Setting `scramble=False` does not ensure deterministic output. For that, use the `seed` parameter.\n            Default is True.\n        optimization (Optional[Union[None, str]]):\n            Whether to use an optimization scheme to improve the quality after sampling.\n            Note that this is a post-processing step that does not guarantee that all\n            properties of the sample will be conserved.\n            Defaults to None.\n            Options:\n                - \"random-cd\": Random permutations of coordinates to lower the centered discrepancy. The best sample based on the centered discrepancy is constantly updated.\n                Centered discrepancy-based sampling shows better space-filling\n                robustness toward 2D and 3D subprojections compared to using other discrepancy measures.\n                - \"lloyd\": Perturb samples using a modified Lloyd-Max algorithm. The process converges to equally spaced samples.\n        strength (Optional[int]):\n            Strength of the LHS. `strength=1` produces a plain LHS while `strength=2` produces an orthogonal array based LHS of strength 2.\n            In that case, only `n=p**2` points can be sampled, with `p` a prime number.\n            It also constrains `d &lt;= p + 1`.\n            Defaults to 1.\n        seed (int, optional):\n            Seed for the random number generator. Defaults to 123.\n    \"\"\"\n    super().__init__(k=k, seed=seed)\n    self.sampler = LatinHypercube(d=self.k, scramble=scramble, strength=strength, optimization=optimization, seed=seed)\n</code></pre>"},{"location":"reference/spotpython/design/spacefilling/#spotpython.design.spacefilling.SpaceFilling.scipy_lhd","title":"<code>scipy_lhd(n, repeats=1, lower=None, upper=None)</code>","text":"<p>Latin hypercube sampling based on scipy.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples.</p> required <code>repeats</code> <code>int</code> <p>Number of repeats (replicates).</p> <code>1</code> <code>lower</code> <code>int, float, or np.ndarray</code> <p>Lower bound. Defaults to a zero vector.</p> <code>None</code> <code>upper</code> <code>int, float, or np.ndarray</code> <p>Upper bound. Defaults to a one vector.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Latin hypercube design with specified dimensions and boundaries.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.design.spacefilling import SpaceFilling\n&gt;&gt;&gt; lhd = SpaceFilling(k=2, seed=123)\n&gt;&gt;&gt; lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0, 0]), upper=np.array([1, 1]))\narray([[0.66352963, 0.5892358 ],\n       [0.66352963, 0.5892358 ],\n       [0.55592803, 0.96312564],\n       [0.55592803, 0.96312564],\n       [0.16481882, 0.0375811 ],\n       [0.16481882, 0.0375811 ],\n       [0.215331  , 0.34468512],\n       [0.215331  , 0.34468512],\n       [0.83604909, 0.62202146],\n       [0.83604909, 0.62202146]])\n</code></pre> Source code in <code>spotpython/design/spacefilling.py</code> <pre><code>def scipy_lhd(\n    self,\n    n: int,\n    repeats: int = 1,\n    lower: Optional[Union[int, float, np.ndarray]] = None,\n    upper: Optional[Union[int, float, np.ndarray]] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Latin hypercube sampling based on scipy.\n\n    Args:\n        n (int): Number of samples.\n        repeats (int): Number of repeats (replicates).\n        lower (int, float, or np.ndarray, optional): Lower bound. Defaults to a zero vector.\n        upper (int, float, or np.ndarray, optional): Upper bound. Defaults to a one vector.\n\n    Returns:\n        np.ndarray: Latin hypercube design with specified dimensions and boundaries.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.design.spacefilling import SpaceFilling\n        &gt;&gt;&gt; lhd = SpaceFilling(k=2, seed=123)\n        &gt;&gt;&gt; lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0, 0]), upper=np.array([1, 1]))\n        array([[0.66352963, 0.5892358 ],\n               [0.66352963, 0.5892358 ],\n               [0.55592803, 0.96312564],\n               [0.55592803, 0.96312564],\n               [0.16481882, 0.0375811 ],\n               [0.16481882, 0.0375811 ],\n               [0.215331  , 0.34468512],\n               [0.215331  , 0.34468512],\n               [0.83604909, 0.62202146],\n               [0.83604909, 0.62202146]])\n    \"\"\"\n    if lower is None:\n        lower = np.zeros(self.k)\n    if upper is None:\n        upper = np.ones(self.k)\n\n    sample = self.sampler.random(n=n)\n    des = scale(sample, lower, upper)\n    return np.repeat(des, repeats, axis=0)\n</code></pre>"},{"location":"reference/spotpython/fun/hyperlight/","title":"hyperlight","text":""},{"location":"reference/spotpython/fun/hyperlight/#spotpython.fun.hyperlight.HyperLight","title":"<code>HyperLight</code>","text":"<p>Hyperparameter Tuning for Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for the random number generator. See Numpy Random Sampling.</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for the random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n&gt;&gt;&gt; print(hyper_light.seed)\n    126\n</code></pre> Source code in <code>spotpython/fun/hyperlight.py</code> <pre><code>class HyperLight:\n    \"\"\"\n    Hyperparameter Tuning for Lightning.\n\n    Args:\n        seed (int): seed for the random number generator. See Numpy Random Sampling.\n        log_level (int): log level for the logger.\n\n    Attributes:\n        seed (int): seed for the random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the hyperparameter tuning.\n        log_level (int): log level for the logger.\n\n    Examples:\n        &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n        &gt;&gt;&gt; print(hyper_light.seed)\n            126\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50) -&gt; None:\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.log_level = log_level\n        logger.setLevel(log_level)\n        logger.info(f\"Starting the logger at level {log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray, fun_control: dict) -&gt; np.ndarray:\n        \"\"\"\n        Checks the shape of the input array X and raises an exception if it is not valid.\n\n        Args:\n            X (np.ndarray):\n                input array.\n            fun_control (dict):\n                dictionary containing control parameters for the hyperparameter tuning.\n\n        Returns:\n            np.ndarray:\n                input array with valid shape.\n\n        Raises:\n            Exception:\n                if the shape of the input array is not valid.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.utils.init import fun_control_init\n                from spotpython.light.regression.netlightregression import NetLightRegression\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.hyperparameters.values import add_core_model_to_fun_control\n                from spotpython.fun.hyperlight import HyperLight\n                from spotpython.hyperparameters.values import get_var_name\n                fun_control = fun_control_init()\n                add_core_model_to_fun_control(core_model=NetLightRegression,\n                                            fun_control=fun_control,\n                                            hyper_dict=LightHyperDict)\n                hyper_light = HyperLight(seed=126, log_level=50)\n                n_hyperparams = len(get_var_name(fun_control))\n                # generate a random np.array X with shape (2, n_hyperparams)\n                X = np.random.rand(2, n_hyperparams)\n                X == hyper_light.check_X_shape(X, fun_control)\n                array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n                [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(get_var_name(fun_control)):\n            raise Exception(\"Invalid shape of input array X.\")\n        return X\n\n    def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluates the function for the given input array X and control parameters.\n        Calls the train_model function from spotpython.light.trainmodel\n        to train the model and evaluate the results.\n\n        Args:\n            X (np.ndarray):\n                input array.\n            fun_control (dict):\n                dictionary containing control parameters for the hyperparameter tuning.\n\n        Returns:\n            (np.ndarray):\n                array containing the evaluation results.\n\n        Examples:\n            &gt;&gt;&gt; from math import inf\n                import numpy as np\n                from spotpython.data.diabetes import Diabetes\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.fun.hyperlight import HyperLight\n                from spotpython.utils.init import fun_control_init\n                from spotpython.utils.eda import gen_design_table\n                from spotpython.spot import spot\n                from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n                PREFIX=\"000\"\n                data_set = Diabetes()\n                fun_control = fun_control_init(\n                    PREFIX=PREFIX,\n                    save_experiment=True,\n                    fun_evals=inf,\n                    max_time=1,\n                    data_set = data_set,\n                    core_model_name=\"light.regression.NNLinearRegressor\",\n                    hyperdict=LightHyperDict,\n                    _L_in=10,\n                    _L_out=1,\n                    TENSORBOARD_CLEAN=True,\n                    tensorboard_log=True,\n                    seed=42,)\n                print(gen_design_table(fun_control))\n                X = get_default_hyperparameters_as_array(fun_control)\n                # set epochs to 2^8:\n                X[0, 1] = 8\n                # set patience to 2^10:\n                X[0, 7] = 10\n                print(f\"X: {X}\")\n                # combine X and X to a np.array with shape (2, n_hyperparams)\n                # so that two values are returned\n                X = np.vstack((X, X))\n                hyper_light = HyperLight(seed=125, log_level=50)\n                hyper_light.fun(X, fun_control)\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        self.check_X_shape(X=X, fun_control=fun_control)\n        var_dict = assign_values(X, get_var_name(fun_control))\n        # type information and transformations are considered in generate_one_config_from_var_dict:\n        for config in generate_one_config_from_var_dict(var_dict, fun_control):\n            if fun_control[\"show_config\"]:\n                print(\"\\nIn fun(): config:\")\n                pprint.pprint(config)\n            logger.debug(f\"\\nconfig: {config}\")\n            # extract parameters like epochs, batch_size, lr, etc. from config\n            # config_id = generate_config_id(config)\n            try:\n                logger.debug(\"fun: Calling train_model\")\n                df_eval = train_model(config, fun_control)\n                logger.debug(\"fun: train_model returned\")\n            except Exception as err:\n                if fun_control[\"verbosity\"] &gt; 0:\n                    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n                    pprint.pprint(fun_control)\n                    print(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                    print(\"Setting df_eval to np.nan\\n\")\n                logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                logger.error(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            # Multiply results by the weights. Positive weights mean that the result is to be minimized.\n            # Negative weights mean that the result is to be maximized, e.g., accuracy.\n            z_val = fun_control[\"weights\"] * df_eval\n            # Append, since several configurations can be evaluated at once.\n            z_res = np.append(z_res, z_val)\n        return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/hyperlight/#spotpython.fun.hyperlight.HyperLight.check_X_shape","title":"<code>check_X_shape(X, fun_control)</code>","text":"<p>Checks the shape of the input array X and raises an exception if it is not valid.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: input array with valid shape.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if the shape of the input array is not valid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.init import fun_control_init\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    from spotpython.fun.hyperlight import HyperLight\n    from spotpython.hyperparameters.values import get_var_name\n    fun_control = fun_control_init()\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n    hyper_light = HyperLight(seed=126, log_level=50)\n    n_hyperparams = len(get_var_name(fun_control))\n    # generate a random np.array X with shape (2, n_hyperparams)\n    X = np.random.rand(2, n_hyperparams)\n    X == hyper_light.check_X_shape(X, fun_control)\n    array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n    [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n</code></pre> Source code in <code>spotpython/fun/hyperlight.py</code> <pre><code>def check_X_shape(self, X: np.ndarray, fun_control: dict) -&gt; np.ndarray:\n    \"\"\"\n    Checks the shape of the input array X and raises an exception if it is not valid.\n\n    Args:\n        X (np.ndarray):\n            input array.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        np.ndarray:\n            input array with valid shape.\n\n    Raises:\n        Exception:\n            if the shape of the input array is not valid.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.init import fun_control_init\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            from spotpython.fun.hyperlight import HyperLight\n            from spotpython.hyperparameters.values import get_var_name\n            fun_control = fun_control_init()\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n            hyper_light = HyperLight(seed=126, log_level=50)\n            n_hyperparams = len(get_var_name(fun_control))\n            # generate a random np.array X with shape (2, n_hyperparams)\n            X = np.random.rand(2, n_hyperparams)\n            X == hyper_light.check_X_shape(X, fun_control)\n            array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n            [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(get_var_name(fun_control)):\n        raise Exception(\"Invalid shape of input array X.\")\n    return X\n</code></pre>"},{"location":"reference/spotpython/fun/hyperlight/#spotpython.fun.hyperlight.HyperLight.fun","title":"<code>fun(X, fun_control=None)</code>","text":"<p>Evaluates the function for the given input array X and control parameters. Calls the train_model function from spotpython.light.trainmodel to train the model and evaluate the results.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing the evaluation results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from math import inf\n    import numpy as np\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.fun.hyperlight import HyperLight\n    from spotpython.utils.init import fun_control_init\n    from spotpython.utils.eda import gen_design_table\n    from spotpython.spot import spot\n    from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n    PREFIX=\"000\"\n    data_set = Diabetes()\n    fun_control = fun_control_init(\n        PREFIX=PREFIX,\n        save_experiment=True,\n        fun_evals=inf,\n        max_time=1,\n        data_set = data_set,\n        core_model_name=\"light.regression.NNLinearRegressor\",\n        hyperdict=LightHyperDict,\n        _L_in=10,\n        _L_out=1,\n        TENSORBOARD_CLEAN=True,\n        tensorboard_log=True,\n        seed=42,)\n    print(gen_design_table(fun_control))\n    X = get_default_hyperparameters_as_array(fun_control)\n    # set epochs to 2^8:\n    X[0, 1] = 8\n    # set patience to 2^10:\n    X[0, 7] = 10\n    print(f\"X: {X}\")\n    # combine X and X to a np.array with shape (2, n_hyperparams)\n    # so that two values are returned\n    X = np.vstack((X, X))\n    hyper_light = HyperLight(seed=125, log_level=50)\n    hyper_light.fun(X, fun_control)\n</code></pre> Source code in <code>spotpython/fun/hyperlight.py</code> <pre><code>def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates the function for the given input array X and control parameters.\n    Calls the train_model function from spotpython.light.trainmodel\n    to train the model and evaluate the results.\n\n    Args:\n        X (np.ndarray):\n            input array.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        (np.ndarray):\n            array containing the evaluation results.\n\n    Examples:\n        &gt;&gt;&gt; from math import inf\n            import numpy as np\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.fun.hyperlight import HyperLight\n            from spotpython.utils.init import fun_control_init\n            from spotpython.utils.eda import gen_design_table\n            from spotpython.spot import spot\n            from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n            PREFIX=\"000\"\n            data_set = Diabetes()\n            fun_control = fun_control_init(\n                PREFIX=PREFIX,\n                save_experiment=True,\n                fun_evals=inf,\n                max_time=1,\n                data_set = data_set,\n                core_model_name=\"light.regression.NNLinearRegressor\",\n                hyperdict=LightHyperDict,\n                _L_in=10,\n                _L_out=1,\n                TENSORBOARD_CLEAN=True,\n                tensorboard_log=True,\n                seed=42,)\n            print(gen_design_table(fun_control))\n            X = get_default_hyperparameters_as_array(fun_control)\n            # set epochs to 2^8:\n            X[0, 1] = 8\n            # set patience to 2^10:\n            X[0, 7] = 10\n            print(f\"X: {X}\")\n            # combine X and X to a np.array with shape (2, n_hyperparams)\n            # so that two values are returned\n            X = np.vstack((X, X))\n            hyper_light = HyperLight(seed=125, log_level=50)\n            hyper_light.fun(X, fun_control)\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    self.check_X_shape(X=X, fun_control=fun_control)\n    var_dict = assign_values(X, get_var_name(fun_control))\n    # type information and transformations are considered in generate_one_config_from_var_dict:\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        if fun_control[\"show_config\"]:\n            print(\"\\nIn fun(): config:\")\n            pprint.pprint(config)\n        logger.debug(f\"\\nconfig: {config}\")\n        # extract parameters like epochs, batch_size, lr, etc. from config\n        # config_id = generate_config_id(config)\n        try:\n            logger.debug(\"fun: Calling train_model\")\n            df_eval = train_model(config, fun_control)\n            logger.debug(\"fun: train_model returned\")\n        except Exception as err:\n            if fun_control[\"verbosity\"] &gt; 0:\n                print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n                pprint.pprint(fun_control)\n                print(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                print(\"Setting df_eval to np.nan\\n\")\n            logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n            logger.error(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        # Multiply results by the weights. Positive weights mean that the result is to be minimized.\n        # Negative weights mean that the result is to be maximized, e.g., accuracy.\n        z_val = fun_control[\"weights\"] * df_eval\n        # Append, since several configurations can be evaluated at once.\n        z_res = np.append(z_res, z_val)\n    return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/hypersklearn/","title":"hypersklearn","text":""},{"location":"reference/spotpython/fun/hypersklearn/#spotpython.fun.hypersklearn.HyperSklearn","title":"<code>HyperSklearn</code>","text":"<p>Hyperparameter Tuning for Sklearn.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed. See Numpy Random Sampling</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for logger. Default is 50.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> <code>log_level</code> <code>int</code> <p>log level for logger.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.hypersklearn import HyperSklearn\n&gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n&gt;&gt;&gt; print(hyper_sklearn.seed)\n126\n</code></pre> Source code in <code>spotpython/fun/hypersklearn.py</code> <pre><code>class HyperSklearn:\n    \"\"\"\n    Hyperparameter Tuning for Sklearn.\n\n    Args:\n        seed (int): seed.\n            See Numpy Random Sampling\n        log_level (int): log level for logger. Default is 50.\n\n    Attributes:\n        seed (int): seed for random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the function.\n        log_level (int): log level for logger.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.hypersklearn import HyperSklearn\n        &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n        &gt;&gt;&gt; print(hyper_sklearn.seed)\n        126\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50):\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\n            \"seed\": None,\n            \"data\": None,\n            \"step\": 10_000,\n            \"horizon\": None,\n            \"grace_period\": None,\n            \"metric_river\": None,\n            \"metric_sklearn\": mean_absolute_error,\n            \"weights\": array([1, 0, 0]),\n            \"weight_coeff\": 0.0,\n            \"log_level\": log_level,\n            \"var_name\": [],\n            \"var_type\": [],\n            \"prep_model\": None,\n            \"predict_proba\": False,\n        }\n        self.log_level = self.fun_control[\"log_level\"]\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Check the shape of the input array X.\n\n        Args:\n            X (np.ndarray): input array.\n\n        Raises:\n            Exception: if the second dimension of X does not match the length of var_name in fun_control.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.hypersklearn import HyperSklearn\n            &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n            &gt;&gt;&gt; hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"]\n            &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]]))\n            &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2]]))\n            Traceback (most recent call last):\n            ...\n            Exception\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(self.fun_control[\"var_name\"]):\n            raise Exception\n\n    def get_sklearn_df_eval_preds(self, model) -&gt; tuple:\n        \"\"\"\n        Get evaluation and prediction dataframes for a given model.\n        Args:\n            model (sklearn model): sklearn model.\n\n        Returns:\n            (tuple): tuple containing evaluation and prediction dataframes.\n\n        Raises:\n            Exception: if call to evaluate_model fails.\n\n        \"\"\"\n        try:\n            df_eval, df_preds = self.evaluate_model(model, self.fun_control)\n        except Exception as err:\n            print(f\"Error in get_sklearn_df_eval_preds(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n            print(\"Setting df_eval and df.preds to np.nan\")\n            df_eval = np.nan\n            df_preds = np.nan\n        return df_eval, df_preds\n\n    def fun_sklearn(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluate a sklearn model using hyperparameters specified in X.\n\n        Args:\n            X (np.ndarray): input array containing hyperparameters.\n            fun_control (dict): dictionary containing control parameters for the function. Default is None.\n\n        Returns:\n            (np.ndarray): array containing evaluation results.\n\n        Raises:\n            Exception: if call to evaluate_model fails.\n\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        self.fun_control.update(fun_control)\n        self.check_X_shape(X)\n        var_dict = assign_values(X, self.fun_control[\"var_name\"])\n        for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n            if self.fun_control[\"prep_model\"] is not None:\n                model = make_pipeline(self.fun_control[\"prep_model\"](), self.fun_control[\"core_model\"](**config))\n            else:\n                model = self.fun_control[\"core_model\"](**config)\n            try:\n                eval_type = fun_control[\"eval\"]\n                if eval_type == \"eval_test\":\n                    df_eval, _ = evaluate_model(model, self.fun_control)\n                elif eval_type == \"eval_oob_score\":\n                    df_eval, _ = evaluate_model_oob(model, self.fun_control)\n                elif eval_type == \"train_cv\":\n                    df_eval, _ = evaluate_cv(model, self.fun_control)\n                else:  # None or \"evaluate_hold_out\":\n                    df_eval, _ = evaluate_hold_out(model, self.fun_control)\n            except Exception as err:\n                print(f\"Error in fun_sklearn(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n                print(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            z_res = np.append(z_res, fun_control[\"weights\"] * df_eval)\n        return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/hypersklearn/#spotpython.fun.hypersklearn.HyperSklearn.check_X_shape","title":"<code>check_X_shape(X)</code>","text":"<p>Check the shape of the input array X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>if the second dimension of X does not match the length of var_name in fun_control.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.hypersklearn import HyperSklearn\n&gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n&gt;&gt;&gt; hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"]\n&gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]]))\n&gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2]]))\nTraceback (most recent call last):\n...\nException\n</code></pre> Source code in <code>spotpython/fun/hypersklearn.py</code> <pre><code>def check_X_shape(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Check the shape of the input array X.\n\n    Args:\n        X (np.ndarray): input array.\n\n    Raises:\n        Exception: if the second dimension of X does not match the length of var_name in fun_control.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.hypersklearn import HyperSklearn\n        &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n        &gt;&gt;&gt; hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"]\n        &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]]))\n        &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2]]))\n        Traceback (most recent call last):\n        ...\n        Exception\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(self.fun_control[\"var_name\"]):\n        raise Exception\n</code></pre>"},{"location":"reference/spotpython/fun/hypersklearn/#spotpython.fun.hypersklearn.HyperSklearn.fun_sklearn","title":"<code>fun_sklearn(X, fun_control=None)</code>","text":"<p>Evaluate a sklearn model using hyperparameters specified in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array containing hyperparameters.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing evaluation results.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if call to evaluate_model fails.</p> Source code in <code>spotpython/fun/hypersklearn.py</code> <pre><code>def fun_sklearn(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluate a sklearn model using hyperparameters specified in X.\n\n    Args:\n        X (np.ndarray): input array containing hyperparameters.\n        fun_control (dict): dictionary containing control parameters for the function. Default is None.\n\n    Returns:\n        (np.ndarray): array containing evaluation results.\n\n    Raises:\n        Exception: if call to evaluate_model fails.\n\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    self.fun_control.update(fun_control)\n    self.check_X_shape(X)\n    var_dict = assign_values(X, self.fun_control[\"var_name\"])\n    for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n        if self.fun_control[\"prep_model\"] is not None:\n            model = make_pipeline(self.fun_control[\"prep_model\"](), self.fun_control[\"core_model\"](**config))\n        else:\n            model = self.fun_control[\"core_model\"](**config)\n        try:\n            eval_type = fun_control[\"eval\"]\n            if eval_type == \"eval_test\":\n                df_eval, _ = evaluate_model(model, self.fun_control)\n            elif eval_type == \"eval_oob_score\":\n                df_eval, _ = evaluate_model_oob(model, self.fun_control)\n            elif eval_type == \"train_cv\":\n                df_eval, _ = evaluate_cv(model, self.fun_control)\n            else:  # None or \"evaluate_hold_out\":\n                df_eval, _ = evaluate_hold_out(model, self.fun_control)\n        except Exception as err:\n            print(f\"Error in fun_sklearn(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n            print(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        z_res = np.append(z_res, fun_control[\"weights\"] * df_eval)\n    return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/hypersklearn/#spotpython.fun.hypersklearn.HyperSklearn.get_sklearn_df_eval_preds","title":"<code>get_sklearn_df_eval_preds(model)</code>","text":"<p>Get evaluation and prediction dataframes for a given model. Args:     model (sklearn model): sklearn model.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>tuple containing evaluation and prediction dataframes.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if call to evaluate_model fails.</p> Source code in <code>spotpython/fun/hypersklearn.py</code> <pre><code>def get_sklearn_df_eval_preds(self, model) -&gt; tuple:\n    \"\"\"\n    Get evaluation and prediction dataframes for a given model.\n    Args:\n        model (sklearn model): sklearn model.\n\n    Returns:\n        (tuple): tuple containing evaluation and prediction dataframes.\n\n    Raises:\n        Exception: if call to evaluate_model fails.\n\n    \"\"\"\n    try:\n        df_eval, df_preds = self.evaluate_model(model, self.fun_control)\n    except Exception as err:\n        print(f\"Error in get_sklearn_df_eval_preds(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n        print(\"Setting df_eval and df.preds to np.nan\")\n        df_eval = np.nan\n        df_preds = np.nan\n    return df_eval, df_preds\n</code></pre>"},{"location":"reference/spotpython/fun/hypertorch/","title":"hypertorch","text":""},{"location":"reference/spotpython/fun/hypertorch/#spotpython.fun.hypertorch.HyperTorch","title":"<code>HyperTorch</code>","text":"<p>Hyperparameter Tuning for Torch.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for random number generator. See Numpy Random Sampling</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for logger. Default is 50.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> <code>log_level</code> <code>int</code> <p>log level for logger.</p> Source code in <code>spotpython/fun/hypertorch.py</code> <pre><code>class HyperTorch:\n    \"\"\"\n    Hyperparameter Tuning for Torch.\n\n    Args:\n        seed (int): seed for random number generator.\n            See Numpy Random Sampling\n        log_level (int): log level for logger. Default is 50.\n\n    Attributes:\n        seed (int): seed for random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the function.\n        log_level (int): log level for logger.\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50):\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\n            \"seed\": None,\n            \"data\": None,\n            \"step\": 10_000,\n            \"horizon\": None,\n            \"grace_period\": None,\n            \"metric_river\": None,\n            \"metric_sklearn\": None,\n            \"weights\": array([1, 0, 0]),\n            \"weight_coeff\": 0.0,\n            \"log_level\": log_level,\n            \"var_name\": [],\n            \"var_type\": [],\n        }\n        self.log_level = self.fun_control[\"log_level\"]\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Check the shape of the input array X.\n\n        Args:\n            X (np.ndarray): input array.\n\n        Raises:\n            Exception: if the second dimension of X does not match the length of var_name in fun_control.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n            &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n            &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]]))\n            &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([1, 2]))\n            Traceback (most recent call last):\n            ...\n            Exception\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(self.fun_control[\"var_name\"]):\n            raise Exception\n\n    def fun_torch(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Function to be optimized.\n\n        Args:\n            X (np.ndarray): input array.\n            fun_control (dict): dictionary containing control parameters for the function.\n        Returns:\n            np.ndarray: output array.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n            &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n            &gt;&gt;&gt; hyper_torch.fun_torch(np.array([[1, 2], [3, 4]]))\n\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        self.fun_control.update(fun_control)\n        self.check_X_shape(X)\n        var_dict = assign_values(X, self.fun_control[\"var_name\"])\n        for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n            print(f\"\\nconfig: {config}\")\n            config_id = generate_config_id(config)\n            if self.fun_control[\"prep_model\"] is not None:\n                model = make_pipeline(self.fun_control[\"prep_model\"], self.fun_control[\"core_model\"](**config))\n            else:\n                model = self.fun_control[\"core_model\"](**config)\n            try:\n                if self.fun_control[\"eval\"] == \"train_cv\":\n                    df_eval, _ = evaluate_cv(\n                        model,\n                        dataset=fun_control[\"train\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n                elif self.fun_control[\"eval\"] == \"test_cv\":\n                    df_eval, _ = evaluate_cv(\n                        model,\n                        dataset=fun_control[\"test\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n                elif self.fun_control[\"eval\"] == \"test_hold_out\":\n                    df_eval, _ = evaluate_hold_out(\n                        model,\n                        train_dataset=fun_control[\"train\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        loss_function=self.fun_control[\"loss_function\"],\n                        metric=self.fun_control[\"metric_torch\"],\n                        test_dataset=fun_control[\"test\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        path=self.fun_control[\"path\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n                else:  # eval == \"train_hold_out\"\n                    df_eval, _ = evaluate_hold_out(\n                        model,\n                        train_dataset=fun_control[\"train\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        loss_function=self.fun_control[\"loss_function\"],\n                        metric=self.fun_control[\"metric_torch\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        path=self.fun_control[\"path\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n            except Exception as err:\n                print(f\"Error in fun_torch(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n                print(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            z_val = fun_control[\"weights\"] * df_eval\n            if self.fun_control[\"spot_writer\"] is not None:\n                writer = self.fun_control[\"spot_writer\"]\n                writer.add_hparams(config, {\"fun_torch: loss\": z_val})\n                writer.flush()\n            z_res = np.append(z_res, z_val)\n        return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/hypertorch/#spotpython.fun.hypertorch.HyperTorch.check_X_shape","title":"<code>check_X_shape(X)</code>","text":"<p>Check the shape of the input array X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>if the second dimension of X does not match the length of var_name in fun_control.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n&gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n&gt;&gt;&gt; hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]]))\n&gt;&gt;&gt; hyper_torch.check_X_shape(np.array([1, 2]))\nTraceback (most recent call last):\n...\nException\n</code></pre> Source code in <code>spotpython/fun/hypertorch.py</code> <pre><code>def check_X_shape(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Check the shape of the input array X.\n\n    Args:\n        X (np.ndarray): input array.\n\n    Raises:\n        Exception: if the second dimension of X does not match the length of var_name in fun_control.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n        &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n        &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]]))\n        &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([1, 2]))\n        Traceback (most recent call last):\n        ...\n        Exception\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(self.fun_control[\"var_name\"]):\n        raise Exception\n</code></pre>"},{"location":"reference/spotpython/fun/hypertorch/#spotpython.fun.hypertorch.HyperTorch.fun_torch","title":"<code>fun_torch(X, fun_control=None)</code>","text":"<p>Function to be optimized.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> <code>None</code> <p>Returns:     np.ndarray: output array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n&gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n&gt;&gt;&gt; hyper_torch.fun_torch(np.array([[1, 2], [3, 4]]))\n</code></pre> Source code in <code>spotpython/fun/hypertorch.py</code> <pre><code>def fun_torch(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Function to be optimized.\n\n    Args:\n        X (np.ndarray): input array.\n        fun_control (dict): dictionary containing control parameters for the function.\n    Returns:\n        np.ndarray: output array.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n        &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n        &gt;&gt;&gt; hyper_torch.fun_torch(np.array([[1, 2], [3, 4]]))\n\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    self.fun_control.update(fun_control)\n    self.check_X_shape(X)\n    var_dict = assign_values(X, self.fun_control[\"var_name\"])\n    for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n        print(f\"\\nconfig: {config}\")\n        config_id = generate_config_id(config)\n        if self.fun_control[\"prep_model\"] is not None:\n            model = make_pipeline(self.fun_control[\"prep_model\"], self.fun_control[\"core_model\"](**config))\n        else:\n            model = self.fun_control[\"core_model\"](**config)\n        try:\n            if self.fun_control[\"eval\"] == \"train_cv\":\n                df_eval, _ = evaluate_cv(\n                    model,\n                    dataset=fun_control[\"train\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n            elif self.fun_control[\"eval\"] == \"test_cv\":\n                df_eval, _ = evaluate_cv(\n                    model,\n                    dataset=fun_control[\"test\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n            elif self.fun_control[\"eval\"] == \"test_hold_out\":\n                df_eval, _ = evaluate_hold_out(\n                    model,\n                    train_dataset=fun_control[\"train\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    loss_function=self.fun_control[\"loss_function\"],\n                    metric=self.fun_control[\"metric_torch\"],\n                    test_dataset=fun_control[\"test\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    path=self.fun_control[\"path\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n            else:  # eval == \"train_hold_out\"\n                df_eval, _ = evaluate_hold_out(\n                    model,\n                    train_dataset=fun_control[\"train\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    loss_function=self.fun_control[\"loss_function\"],\n                    metric=self.fun_control[\"metric_torch\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    path=self.fun_control[\"path\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n        except Exception as err:\n            print(f\"Error in fun_torch(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n            print(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        z_val = fun_control[\"weights\"] * df_eval\n        if self.fun_control[\"spot_writer\"] is not None:\n            writer = self.fun_control[\"spot_writer\"]\n            writer.add_hparams(config, {\"fun_torch: loss\": z_val})\n            writer.flush()\n        z_res = np.append(z_res, z_val)\n    return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/","title":"objectivefunctions","text":""},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical","title":"<code>analytical</code>","text":"<p>Class for analytical test functions.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>float</code> <p>Offset value. Defaults to 0.0.</p> <code>0.0</code> <code>hz</code> <code>float</code> <p>Horizontal value. Defaults to 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Seed value for random number generation. Defaults to 126.</p> <code>126</code> Notes <p>See Numpy Random Sampling</p> <p>Attributes:</p> Name Type Description <code>offset</code> <code>float</code> <p>Offset value.</p> <code>hz</code> <code>float</code> <p>Horizontal value.</p> <code>sigma</code> <code>float</code> <p>Noise level.</p> <code>seed</code> <code>int</code> <p>Seed value for random number generation.</p> <code>rng</code> <code>Generator</code> <p>Numpy random number generator object.</p> <code>fun_control</code> <code>dict</code> <p>Dictionary containing control parameters for the function.</p> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>class analytical:\n    \"\"\"\n    Class for analytical test functions.\n\n    Args:\n        offset (float):\n            Offset value. Defaults to 0.0.\n        hz (float):\n            Horizontal value. Defaults to 0.\n        seed (int):\n            Seed value for random number generation. Defaults to 126.\n\n    Notes:\n        See [Numpy Random Sampling](https://numpy.org/doc/stable/reference/random/index.html#random-quick-start)\n\n    Attributes:\n        offset (float):\n            Offset value.\n        hz (float):\n            Horizontal value.\n        sigma (float):\n            Noise level.\n        seed (int):\n            Seed value for random number generation.\n        rng (Generator):\n            Numpy random number generator object.\n        fun_control (dict):\n            Dictionary containing control parameters for the function.\n    \"\"\"\n\n    def __init__(self, offset: float = 0.0, hz: float = 0, sigma=0.0, seed: int = 126) -&gt; None:\n        self.offset = offset\n        self.hz = hz\n        self.sigma = sigma\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\"sigma\": sigma, \"seed\": None, \"sel_var\": None}\n\n    def __repr__(self) -&gt; str:\n        return f\"analytical(offset={self.offset}, hz={self.hz}, seed={self.seed})\"\n\n    def add_noise(self, y: List[float]) -&gt; np.ndarray:\n        \"\"\"\n        Adds noise to the input data.\n        This method takes in a list of float values y as input and adds noise to\n        the data using a random number generator. The method returns a numpy array\n        containing the noisy data.\n\n        Args:\n            self (analytical): analytical class object.\n            y (List[float]): Input data.\n\n        Returns:\n            np.ndarray: Noisy data.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n                import numpy as np\n                y = np.array([1, 2, 3, 4, 5])\n                fun = analytical(sigma=1.0, seed=123)\n                fun.add_noise(y)\n            array([0.01087865, 1.63221335, 4.28792526, 4.19397442, 5.9202309 ])\n\n        \"\"\"\n        # Use own rng:\n        if self.fun_control[\"seed\"] is not None:\n            rng = default_rng(seed=self.fun_control[\"seed\"])\n        # Use class rng:\n        else:\n            rng = self.rng\n        noise_y = np.array([], dtype=float)\n        for y_i in y:\n            noise_y = np.append(\n                noise_y,\n                y_i + rng.normal(loc=0, scale=self.fun_control[\"sigma\"], size=1),\n            )\n        return noise_y\n\n    def fun_branin_factor(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"\n        Calculates the Branin function of (x1, x2) with an additional factor based on the value of x3.\n        If x3 = 1, the value of the Branin function is increased by 10.\n        If x3 = 2, the value of the Branin function is decreased by 10.\n        Otherwise, the value of the Branin function is not changed.\n\n        Args:\n            X (np.ndarray):\n                A 2D numpy array with shape (n, 3) where n is the number of samples.\n            fun_control (Optional[Dict]):\n                A dictionary containing control parameters for the function.\n                If None, self.fun_control is used. Defaults to None.\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n                import numpy as np\n                X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n                fun = analytical()\n                fun.fun_branin_factor(X)\n                array([55.60211264, 65.60211264, 45.60211264])\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        if len(X.shape) == 1:\n            X = np.array([X])\n        if X.shape[1] != 3:\n            raise Exception(\"X must have shape (n, 3)\")\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        x3 = X[:, 2]\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        r = 6\n        s = 10\n        t = 1 / (8 * np.pi)\n        y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n        for j in range(X.shape[0]):\n            if x3[j] == 1:\n                y[j] = y[j] + 10\n            elif x3[j] == 2:\n                y[j] = y[j] - 10\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_linear(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Linear function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_linear(X)\n            array([ 6., 15.])\n\n        \"\"\"\n        if fun_control is not None:\n            self.fun_control = fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        X = np.atleast_2d(X)\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, np.sum(X[i]))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_sphere(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Sphere function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_sphere(X)\n            array([14., 77.])\n\n        \"\"\"\n        if fun_control is not None:\n            self.fun_control = fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        X = np.atleast_2d(X)\n        offset = np.ones(X.shape[1]) * self.offset\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, np.sum((X[i] - offset) ** 2))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_cubed(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Cubed function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_cubed(X)\n            array([ 0., 27.])\n        \"\"\"\n\n        if fun_control is None:\n            fun_control = self.fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n\n        X = np.atleast_2d(X)\n        offset = np.ones(X.shape[1]) * self.offset\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, np.sum((X[i] - offset) ** 3))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_forrester(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Forrester function. Function used by [Forr08a, p.83].\n           f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].\n           Starts with three sample points at x=0, x=0.5, and x=1.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_forrester(X)\n            array([  0.        ,  11.99999999])\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n\n        X = np.atleast_2d(X)\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, (6.0 * X[i] - 2) ** 2 * np.sin(12 * X[i] - 4))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_branin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        r\"\"\"Branin function. The 2-dim Branin function is defined as\n            $$\n            y = a (x_2 - b x_1^2 + c x_1 - r) ^2 + s (1 - t) \\cos(x_1) + s,\n            $$\n            where values of $a, b, c, r, s$ and $t$ are:\n            $a = 1$, $b = 5.1 / (4\\pi^2)$, $c = 5 / \\pi$, $r = 6$, $s = 10$ and $t = 1 / (8\\pi)$.\n            It has three global minima with $f(x) = 0.39788736$ at\n            $$\n            (-\\pi, 12.275),\n            $$\n            $$\n            (\\pi, 2.275),\n            $$\n            and\n            $$\n            (9.42478, 2.475).\n            $$\n            Input domain: This function is usually evaluated on the square $x_1 \\in [-5, 10] \\times x_2 \\in [0, 15]$.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n                pi = np.pi\n                X = np.array([[0,0],\n                    [-pi, 12.275],\n                    [pi, 2.275],\n                    [9.42478, 2.475]])\n                fun = analytical()\n                fun.fun_branin(X)\n                array([55.60211264,  0.39788736,  0.39788736,  0.39788736])\n\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        if X.shape[1] != 2:\n            raise Exception\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        r = 6\n        s = 10\n        t = 1 / (8 * np.pi)\n        y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_branin_modified(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Modified Branin function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_branin_modified(X)\n            array([  0.        ,  11.99999999])\n\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n\n        if X.shape[1] != 2:\n            raise Exception\n        x = X[:, 0]\n        y = X[:, 1]\n        X1 = 15 * x - 5\n        X2 = 15 * y\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        d = 6\n        e = 10\n        ff = 1 / (8 * np.pi)\n        y = (a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e) + 5 * x\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def branin_noise(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Branin function with noise.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.branin_noise(X)\n            array([  0.        ,  11.99999999])\n\n        \"\"\"\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n\n        if X.shape[1] != 2:\n            raise Exception\n        x = X[:, 0]\n        y = X[:, 1]\n        X1 = 15 * x - 5\n        X2 = 15 * y\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        d = 6\n        e = 10\n        ff = 1 / (8 * np.pi)\n        noiseFree = (a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e) + 5 * x\n        noise_y = []\n        for i in noiseFree:\n            noise_y.append(i + np.random.standard_normal() * 15)\n        return np.array(noise_y)\n\n    def fun_sin_cos(self, X, fun_control=None):\n        \"\"\"Sinusoidal function.\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_sin_cos(X)\n            array([-1.        , -0.41614684])\n        \"\"\"\n\n        if fun_control is None:\n            fun_control = self.fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        if X.shape[1] != 2:\n            raise Exception\n        x0 = X[:, 0]\n        x1 = X[:, 1]\n        y = 2.0 * np.sin(x0 + self.hz) + 0.5 * np.cos(x1 + self.hz)\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    # def fun_forrester_2(self, X):\n    #     \"\"\"\n    #     Function used by [Forr08a, p.83].\n    #     f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].\n    #     Starts with three sample points at x=0, x=0.5, and x=1.\n\n    #     Args:\n    #         X (flooat): input values (1-dim)\n\n    #     Returns:\n    #         float: function value\n    #     \"\"\"\n    #     try:\n    #         X.shape[1]\n    #     except ValueError:\n    #         X = np.array(X)\n\n    # X = np.atleast_2d(X)\n    #     # y = X[:, 1]\n    #     y = (6.0 * X - 2) ** 2 * np.sin(12 * X - 4)\n    #     if self.sigma != 0:\n    #         noise_y = np.array([], dtype=float)\n    #         for i in y:\n    #             noise_y = np.append(\n    #                 noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1)\n    #             )\n    #         return noise_y\n    #     else:\n    #         return y\n\n    def fun_runge(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k &gt;= 1.\n           Interval: -5 &lt;= x &lt;= 5\n\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_runge(X)\n            array([0.0625    , 0.015625  , 0.00390625])\n\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n        offset = np.ones(X.shape[1]) * self.offset\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, (1 / (1 + np.sum((X[i] - offset) ** 2))))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_wingwt(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        r\"\"\"Wing weight function. Example from Forrester et al. to understand the weight\n            of an unpainted light aircraft wing as a function of nine design and operational parameters:\n            $W=0.036 S_W^{0.758}  Wfw^{0.0035} ( A / (\\cos^2 \\Lambda))^{0.6} q^{0.006}  \\lambda^{0.04} ( (100 Rtc)/(\\cos\n              \\Lambda) ))^{-0.3} (Nz Wdg)^{0.49}$\n\n        | Symbol    | Parameter                              | Baseline | Minimum | Maximum |\n        |-----------|----------------------------------------|----------|---------|---------|\n        | $S_W$     | Wing area ($ft^2$)                     | 174      | 150     | 200     |\n        | $W_{fw}$  | Weight of fuel in wing (lb)            | 252      | 220     | 300     |\n        | $A$       | Aspect ratio                          | 7.52     | 6       | 10      |\n        | $\\Lambda$ | Quarter-chord sweep (deg)              | 0        | -10     | 10      |\n        | $q$       | Dynamic pressure at cruise ($lb/ft^2$) | 34       | 16      | 45      |\n        | $\\lambda$ | Taper ratio                            | 0.672    | 0.5     | 1       |\n        | $R_{tc}$  | Aerofoil thickness to chord ratio      | 0.12     | 0.08    | 0.18    |\n        | $N_z$     | Ultimate load factor                   | 3.8      | 2.5     | 6       |\n        | $W_{dg}$  | Flight design gross weight (lb)         | 2000     | 1700    | 2500    |\n        | $W_p$     | paint weight (lb/ft^2)                   | 0.064 |   0.025  | 0.08    |\n\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_wingwt(X)\n            array([0.0625    , 0.015625  , 0.00390625])\n\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        #\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            Sw = X[i, 0] * (200 - 150) + 150\n            Wfw = X[i, 1] * (300 - 220) + 220\n            A = X[i, 2] * (10 - 6) + 6\n            L = (X[i, 3] * (10 - (-10)) - 10) * np.pi / 180\n            q = X[i, 4] * (45 - 16) + 16\n            la = X[i, 5] * (1 - 0.5) + 0.5\n            Rtc = X[i, 6] * (0.18 - 0.08) + 0.08\n            Nz = X[i, 7] * (6 - 2.5) + 2.5\n            Wdg = X[i, 8] * (2500 - 1700) + 1700\n            Wp = X[i, 9] * (0.08 - 0.025) + 0.025\n            # calculation on natural scale\n            W = 0.036 * Sw**0.758 * Wfw**0.0035 * (A / np.cos(L) ** 2) ** 0.6 * q**0.006\n            W = W * la**0.04 * (100 * Rtc / np.cos(L)) ** (-0.3) * (Nz * Wdg) ** (0.49) + Sw * Wp\n            y = np.append(y, W)\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_xsin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Example function.\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_xsin(X)\n            array([0.84147098, 0.90929743, 0.14112001])\n\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        X = np.atleast_2d(X)\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, X[i] * np.sin(1.0 / X[i]))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_rosen(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Rosenbrock function.\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_rosen(X)\n            array([24,  0])\n        \"\"\"\n\n        if fun_control is None:\n            fun_control = self.fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        if X.shape[1] != 2:\n            raise Exception\n        x0 = X[:, 0]\n        x1 = X[:, 1]\n        b = 10\n        y = (x0 - 1) ** 2 + b * (x1 - x0**2) ** 2\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_random_error(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Return errors for testing spot stability.\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_random_error(X)\n            array([24,  0])\n\n        \"\"\"\n        if fun_control is not None:\n            self.fun_control = fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            # provoke error:\n            if random() &lt; 0.1:\n                y = np.append(y, np.nan)\n            else:\n                y = np.append(y, np.sum(X[i]))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            print(y)\n            return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.add_noise","title":"<code>add_noise(y)</code>","text":"<p>Adds noise to the input data. This method takes in a list of float values y as input and adds noise to the data using a random number generator. The method returns a numpy array containing the noisy data.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>analytical</code> <p>analytical class object.</p> required <code>y</code> <code>List[float]</code> <p>Input data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Noisy data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n    import numpy as np\n    y = np.array([1, 2, 3, 4, 5])\n    fun = analytical(sigma=1.0, seed=123)\n    fun.add_noise(y)\narray([0.01087865, 1.63221335, 4.28792526, 4.19397442, 5.9202309 ])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def add_noise(self, y: List[float]) -&gt; np.ndarray:\n    \"\"\"\n    Adds noise to the input data.\n    This method takes in a list of float values y as input and adds noise to\n    the data using a random number generator. The method returns a numpy array\n    containing the noisy data.\n\n    Args:\n        self (analytical): analytical class object.\n        y (List[float]): Input data.\n\n    Returns:\n        np.ndarray: Noisy data.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            import numpy as np\n            y = np.array([1, 2, 3, 4, 5])\n            fun = analytical(sigma=1.0, seed=123)\n            fun.add_noise(y)\n        array([0.01087865, 1.63221335, 4.28792526, 4.19397442, 5.9202309 ])\n\n    \"\"\"\n    # Use own rng:\n    if self.fun_control[\"seed\"] is not None:\n        rng = default_rng(seed=self.fun_control[\"seed\"])\n    # Use class rng:\n    else:\n        rng = self.rng\n    noise_y = np.array([], dtype=float)\n    for y_i in y:\n        noise_y = np.append(\n            noise_y,\n            y_i + rng.normal(loc=0, scale=self.fun_control[\"sigma\"], size=1),\n        )\n    return noise_y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.branin_noise","title":"<code>branin_noise(X, fun_control=None)</code>","text":"<p>Branin function with noise.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.branin_noise(X)\narray([  0.        ,  11.99999999])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def branin_noise(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Branin function with noise.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.branin_noise(X)\n        array([  0.        ,  11.99999999])\n\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n\n    if X.shape[1] != 2:\n        raise Exception\n    x = X[:, 0]\n    y = X[:, 1]\n    X1 = 15 * x - 5\n    X2 = 15 * y\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    d = 6\n    e = 10\n    ff = 1 / (8 * np.pi)\n    noiseFree = (a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e) + 5 * x\n    noise_y = []\n    for i in noiseFree:\n        noise_y.append(i + np.random.standard_normal() * 15)\n    return np.array(noise_y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_branin","title":"<code>fun_branin(X, fun_control=None)</code>","text":"<p>Branin function. The 2-dim Branin function is defined as     $$     y = a (x_2 - b x_1^2 + c x_1 - r) ^2 + s (1 - t) \\cos(x_1) + s,     $$     where values of \\(a, b, c, r, s\\) and \\(t\\) are:     \\(a = 1\\), \\(b = 5.1 / (4\\pi^2)\\), \\(c = 5 / \\pi\\), \\(r = 6\\), \\(s = 10\\) and \\(t = 1 / (8\\pi)\\).     It has three global minima with \\(f(x) = 0.39788736\\) at     $$     (-\\pi, 12.275),     $$     $$     (\\pi, 2.275),     $$     and     $$     (9.42478, 2.475).     $$     Input domain: This function is usually evaluated on the square \\(x_1 \\in [-5, 10] \\times x_2 \\in [0, 15]\\).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n    pi = np.pi\n    X = np.array([[0,0],\n        [-pi, 12.275],\n        [pi, 2.275],\n        [9.42478, 2.475]])\n    fun = analytical()\n    fun.fun_branin(X)\n    array([55.60211264,  0.39788736,  0.39788736,  0.39788736])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_branin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    r\"\"\"Branin function. The 2-dim Branin function is defined as\n        $$\n        y = a (x_2 - b x_1^2 + c x_1 - r) ^2 + s (1 - t) \\cos(x_1) + s,\n        $$\n        where values of $a, b, c, r, s$ and $t$ are:\n        $a = 1$, $b = 5.1 / (4\\pi^2)$, $c = 5 / \\pi$, $r = 6$, $s = 10$ and $t = 1 / (8\\pi)$.\n        It has three global minima with $f(x) = 0.39788736$ at\n        $$\n        (-\\pi, 12.275),\n        $$\n        $$\n        (\\pi, 2.275),\n        $$\n        and\n        $$\n        (9.42478, 2.475).\n        $$\n        Input domain: This function is usually evaluated on the square $x_1 \\in [-5, 10] \\times x_2 \\in [0, 15]$.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            pi = np.pi\n            X = np.array([[0,0],\n                [-pi, 12.275],\n                [pi, 2.275],\n                [9.42478, 2.475]])\n            fun = analytical()\n            fun.fun_branin(X)\n            array([55.60211264,  0.39788736,  0.39788736,  0.39788736])\n\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    if X.shape[1] != 2:\n        raise Exception\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    r = 6\n    s = 10\n    t = 1 / (8 * np.pi)\n    y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_branin_factor","title":"<code>fun_branin_factor(X, fun_control=None)</code>","text":"<p>Calculates the Branin function of (x1, x2) with an additional factor based on the value of x3. If x3 = 1, the value of the Branin function is increased by 10. If x3 = 2, the value of the Branin function is decreased by 10. Otherwise, the value of the Branin function is not changed.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D numpy array with shape (n, 3) where n is the number of samples.</p> required <code>fun_control</code> <code>Optional[Dict]</code> <p>A dictionary containing control parameters for the function. If None, self.fun_control is used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n    import numpy as np\n    X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n    fun = analytical()\n    fun.fun_branin_factor(X)\n    array([55.60211264, 65.60211264, 45.60211264])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_branin_factor(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the Branin function of (x1, x2) with an additional factor based on the value of x3.\n    If x3 = 1, the value of the Branin function is increased by 10.\n    If x3 = 2, the value of the Branin function is decreased by 10.\n    Otherwise, the value of the Branin function is not changed.\n\n    Args:\n        X (np.ndarray):\n            A 2D numpy array with shape (n, 3) where n is the number of samples.\n        fun_control (Optional[Dict]):\n            A dictionary containing control parameters for the function.\n            If None, self.fun_control is used. Defaults to None.\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            import numpy as np\n            X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n            fun = analytical()\n            fun.fun_branin_factor(X)\n            array([55.60211264, 65.60211264, 45.60211264])\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    if len(X.shape) == 1:\n        X = np.array([X])\n    if X.shape[1] != 3:\n        raise Exception(\"X must have shape (n, 3)\")\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n    x3 = X[:, 2]\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    r = 6\n    s = 10\n    t = 1 / (8 * np.pi)\n    y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n    for j in range(X.shape[0]):\n        if x3[j] == 1:\n            y[j] = y[j] + 10\n        elif x3[j] == 2:\n            y[j] = y[j] - 10\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_branin_modified","title":"<code>fun_branin_modified(X, fun_control=None)</code>","text":"<p>Modified Branin function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_branin_modified(X)\narray([  0.        ,  11.99999999])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_branin_modified(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Modified Branin function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_branin_modified(X)\n        array([  0.        ,  11.99999999])\n\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n\n    if X.shape[1] != 2:\n        raise Exception\n    x = X[:, 0]\n    y = X[:, 1]\n    X1 = 15 * x - 5\n    X2 = 15 * y\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    d = 6\n    e = 10\n    ff = 1 / (8 * np.pi)\n    y = (a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e) + 5 * x\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_cubed","title":"<code>fun_cubed(X, fun_control=None)</code>","text":"<p>Cubed function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_cubed(X)\narray([ 0., 27.])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_cubed(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Cubed function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_cubed(X)\n        array([ 0., 27.])\n    \"\"\"\n\n    if fun_control is None:\n        fun_control = self.fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n\n    X = np.atleast_2d(X)\n    offset = np.ones(X.shape[1]) * self.offset\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, np.sum((X[i] - offset) ** 3))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_forrester","title":"<code>fun_forrester(X, fun_control=None)</code>","text":"<p>Forrester function. Function used by [Forr08a, p.83].    f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].    Starts with three sample points at x=0, x=0.5, and x=1.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_forrester(X)\narray([  0.        ,  11.99999999])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_forrester(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Forrester function. Function used by [Forr08a, p.83].\n       f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].\n       Starts with three sample points at x=0, x=0.5, and x=1.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_forrester(X)\n        array([  0.        ,  11.99999999])\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n\n    X = np.atleast_2d(X)\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, (6.0 * X[i] - 2) ** 2 * np.sin(12 * X[i] - 4))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_linear","title":"<code>fun_linear(X, fun_control=None)</code>","text":"<p>Linear function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_linear(X)\narray([ 6., 15.])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_linear(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Linear function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_linear(X)\n        array([ 6., 15.])\n\n    \"\"\"\n    if fun_control is not None:\n        self.fun_control = fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    X = np.atleast_2d(X)\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, np.sum(X[i]))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_random_error","title":"<code>fun_random_error(X, fun_control=None)</code>","text":"<p>Return errors for testing spot stability. Args:     X (array): input     fun_control (dict): dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_random_error(X)\narray([24,  0])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_random_error(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Return errors for testing spot stability.\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_random_error(X)\n        array([24,  0])\n\n    \"\"\"\n    if fun_control is not None:\n        self.fun_control = fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        # provoke error:\n        if random() &lt; 0.1:\n            y = np.append(y, np.nan)\n        else:\n            y = np.append(y, np.sum(X[i]))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        print(y)\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_rosen","title":"<code>fun_rosen(X, fun_control=None)</code>","text":"<p>Rosenbrock function. Args:     X (array): input     fun_control (dict): dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_rosen(X)\narray([24,  0])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_rosen(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Rosenbrock function.\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_rosen(X)\n        array([24,  0])\n    \"\"\"\n\n    if fun_control is None:\n        fun_control = self.fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    if X.shape[1] != 2:\n        raise Exception\n    x0 = X[:, 0]\n    x1 = X[:, 1]\n    b = 10\n    y = (x0 - 1) ** 2 + b * (x1 - x0**2) ** 2\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_runge","title":"<code>fun_runge(X, fun_control=None)</code>","text":"<p>Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k &gt;= 1.    Interval: -5 &lt;= x &lt;= 5</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_runge(X)\narray([0.0625    , 0.015625  , 0.00390625])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_runge(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k &gt;= 1.\n       Interval: -5 &lt;= x &lt;= 5\n\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_runge(X)\n        array([0.0625    , 0.015625  , 0.00390625])\n\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    offset = np.ones(X.shape[1]) * self.offset\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, (1 / (1 + np.sum((X[i] - offset) ** 2))))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_sin_cos","title":"<code>fun_sin_cos(X, fun_control=None)</code>","text":"<p>Sinusoidal function. Args:     X (array):         input     fun_control (dict):         dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_sin_cos(X)\narray([-1.        , -0.41614684])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_sin_cos(self, X, fun_control=None):\n    \"\"\"Sinusoidal function.\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_sin_cos(X)\n        array([-1.        , -0.41614684])\n    \"\"\"\n\n    if fun_control is None:\n        fun_control = self.fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    if X.shape[1] != 2:\n        raise Exception\n    x0 = X[:, 0]\n    x1 = X[:, 1]\n    y = 2.0 * np.sin(x0 + self.hz) + 0.5 * np.cos(x1 + self.hz)\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_sphere","title":"<code>fun_sphere(X, fun_control=None)</code>","text":"<p>Sphere function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_sphere(X)\narray([14., 77.])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_sphere(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Sphere function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_sphere(X)\n        array([14., 77.])\n\n    \"\"\"\n    if fun_control is not None:\n        self.fun_control = fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    X = np.atleast_2d(X)\n    offset = np.ones(X.shape[1]) * self.offset\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, np.sum((X[i] - offset) ** 2))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_wingwt","title":"<code>fun_wingwt(X, fun_control=None)</code>","text":"<p>Wing weight function. Example from Forrester et al. to understand the weight     of an unpainted light aircraft wing as a function of nine design and operational parameters:     \\(W=0.036 S_W^{0.758}  Wfw^{0.0035} ( A / (\\cos^2 \\Lambda))^{0.6} q^{0.006}  \\lambda^{0.04} ( (100 Rtc)/(\\cos       \\Lambda) ))^{-0.3} (Nz Wdg)^{0.49}\\)</p> Symbol Parameter Baseline Minimum Maximum \\(S_W\\) Wing area (\\(ft^2\\)) 174 150 200 \\(W_{fw}\\) Weight of fuel in wing (lb) 252 220 300 \\(A\\) Aspect ratio 7.52 6 10 \\(\\Lambda\\) Quarter-chord sweep (deg) 0 -10 10 \\(q\\) Dynamic pressure at cruise (\\(lb/ft^2\\)) 34 16 45 \\(\\lambda\\) Taper ratio 0.672 0.5 1 \\(R_{tc}\\) Aerofoil thickness to chord ratio 0.12 0.08 0.18 \\(N_z\\) Ultimate load factor 3.8 2.5 6 \\(W_{dg}\\) Flight design gross weight (lb) 2000 1700 2500 \\(W_p\\) paint weight (lb/ft^2) 0.064 0.025 0.08 <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_wingwt(X)\narray([0.0625    , 0.015625  , 0.00390625])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_wingwt(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    r\"\"\"Wing weight function. Example from Forrester et al. to understand the weight\n        of an unpainted light aircraft wing as a function of nine design and operational parameters:\n        $W=0.036 S_W^{0.758}  Wfw^{0.0035} ( A / (\\cos^2 \\Lambda))^{0.6} q^{0.006}  \\lambda^{0.04} ( (100 Rtc)/(\\cos\n          \\Lambda) ))^{-0.3} (Nz Wdg)^{0.49}$\n\n    | Symbol    | Parameter                              | Baseline | Minimum | Maximum |\n    |-----------|----------------------------------------|----------|---------|---------|\n    | $S_W$     | Wing area ($ft^2$)                     | 174      | 150     | 200     |\n    | $W_{fw}$  | Weight of fuel in wing (lb)            | 252      | 220     | 300     |\n    | $A$       | Aspect ratio                          | 7.52     | 6       | 10      |\n    | $\\Lambda$ | Quarter-chord sweep (deg)              | 0        | -10     | 10      |\n    | $q$       | Dynamic pressure at cruise ($lb/ft^2$) | 34       | 16      | 45      |\n    | $\\lambda$ | Taper ratio                            | 0.672    | 0.5     | 1       |\n    | $R_{tc}$  | Aerofoil thickness to chord ratio      | 0.12     | 0.08    | 0.18    |\n    | $N_z$     | Ultimate load factor                   | 3.8      | 2.5     | 6       |\n    | $W_{dg}$  | Flight design gross weight (lb)         | 2000     | 1700    | 2500    |\n    | $W_p$     | paint weight (lb/ft^2)                   | 0.064 |   0.025  | 0.08    |\n\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_wingwt(X)\n        array([0.0625    , 0.015625  , 0.00390625])\n\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    #\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        Sw = X[i, 0] * (200 - 150) + 150\n        Wfw = X[i, 1] * (300 - 220) + 220\n        A = X[i, 2] * (10 - 6) + 6\n        L = (X[i, 3] * (10 - (-10)) - 10) * np.pi / 180\n        q = X[i, 4] * (45 - 16) + 16\n        la = X[i, 5] * (1 - 0.5) + 0.5\n        Rtc = X[i, 6] * (0.18 - 0.08) + 0.08\n        Nz = X[i, 7] * (6 - 2.5) + 2.5\n        Wdg = X[i, 8] * (2500 - 1700) + 1700\n        Wp = X[i, 9] * (0.08 - 0.025) + 0.025\n        # calculation on natural scale\n        W = 0.036 * Sw**0.758 * Wfw**0.0035 * (A / np.cos(L) ** 2) ** 0.6 * q**0.006\n        W = W * la**0.04 * (100 * Rtc / np.cos(L)) ** (-0.3) * (Nz * Wdg) ** (0.49) + Sw * Wp\n        y = np.append(y, W)\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.analytical.fun_xsin","title":"<code>fun_xsin(X, fun_control=None)</code>","text":"<p>Example function. Args:     X (array): input     fun_control (dict): dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_xsin(X)\narray([0.84147098, 0.90929743, 0.14112001])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_xsin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Example function.\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_xsin(X)\n        array([0.84147098, 0.90929743, 0.14112001])\n\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    X = np.atleast_2d(X)\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, X[i] * np.sin(1.0 / X[i]))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotpython/hyperdict/light_hyper_dict/","title":"light_hyper_dict","text":""},{"location":"reference/spotpython/hyperdict/light_hyper_dict/#spotpython.hyperdict.light_hyper_dict.LightHyperDict","title":"<code>LightHyperDict</code>","text":"<p>               Bases: <code>FileConfig</code></p> <p>Lightning hyperparameter dictionary.</p> <p>This class extends the FileConfig class to provide a dictionary for storing hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The name of the file where the hyperparameters are stored.</p> Source code in <code>spotpython/hyperdict/light_hyper_dict.py</code> <pre><code>class LightHyperDict(base.FileConfig):\n    \"\"\"Lightning hyperparameter dictionary.\n\n    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n\n    Attributes:\n        filename (str):\n            The name of the file where the hyperparameters are stored.\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"light_hyper_dict.json\",\n        directory: None = None,\n    ) -&gt; None:\n        super().__init__(filename=filename, directory=directory)\n        self.filename = filename\n        self.directory = directory\n        self.hyper_dict = self.load()\n\n    @property\n    def path(self):\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    def load(self) -&gt; dict:\n        \"\"\"Load the hyperparameters from the file.\n\n        Returns:\n            dict: A dictionary containing the hyperparameters.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                lhd = LightHyperDict()\n                lhd.hyper_dict\n                {'NetLightRegression': {'l1': {'type': 'int',\n                'default': 3,\n                'transform': 'transform_power_2_int',\n                'lower': 3,\n                'upper': 8},\n                'epochs': {'type': 'int',\n                'default': 4,\n                'transform': 'transform_power_2_int',\n                'lower': 4,\n                'upper': 9},\n                ...\n                'transform': 'None',\n                'class_name': 'torch.optim',\n                'core_model_parameter_type': 'str',\n                'lower': 0,\n                'upper': 11}}}\n            # Assume the user specified file `user_hyper_dict.json` is in the `./hyperdict/` directory.\n            &gt;&gt;&gt; user_lhd = LightHyperDict(filename='user_hyper_dict.json', directory='./hyperdict/')\n        \"\"\"\n        with open(self.path, \"r\") as f:\n            d = json.load(f)\n        return d\n</code></pre>"},{"location":"reference/spotpython/hyperdict/light_hyper_dict/#spotpython.hyperdict.light_hyper_dict.LightHyperDict.load","title":"<code>load()</code>","text":"<p>Load the hyperparameters from the file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    lhd = LightHyperDict()\n    lhd.hyper_dict\n    {'NetLightRegression': {'l1': {'type': 'int',\n    'default': 3,\n    'transform': 'transform_power_2_int',\n    'lower': 3,\n    'upper': 8},\n    'epochs': {'type': 'int',\n    'default': 4,\n    'transform': 'transform_power_2_int',\n    'lower': 4,\n    'upper': 9},\n    ...\n    'transform': 'None',\n    'class_name': 'torch.optim',\n    'core_model_parameter_type': 'str',\n    'lower': 0,\n    'upper': 11}}}\n# Assume the user specified file `user_hyper_dict.json` is in the `./hyperdict/` directory.\n&gt;&gt;&gt; user_lhd = LightHyperDict(filename='user_hyper_dict.json', directory='./hyperdict/')\n</code></pre> Source code in <code>spotpython/hyperdict/light_hyper_dict.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"Load the hyperparameters from the file.\n\n    Returns:\n        dict: A dictionary containing the hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            lhd = LightHyperDict()\n            lhd.hyper_dict\n            {'NetLightRegression': {'l1': {'type': 'int',\n            'default': 3,\n            'transform': 'transform_power_2_int',\n            'lower': 3,\n            'upper': 8},\n            'epochs': {'type': 'int',\n            'default': 4,\n            'transform': 'transform_power_2_int',\n            'lower': 4,\n            'upper': 9},\n            ...\n            'transform': 'None',\n            'class_name': 'torch.optim',\n            'core_model_parameter_type': 'str',\n            'lower': 0,\n            'upper': 11}}}\n        # Assume the user specified file `user_hyper_dict.json` is in the `./hyperdict/` directory.\n        &gt;&gt;&gt; user_lhd = LightHyperDict(filename='user_hyper_dict.json', directory='./hyperdict/')\n    \"\"\"\n    with open(self.path, \"r\") as f:\n        d = json.load(f)\n    return d\n</code></pre>"},{"location":"reference/spotpython/hyperdict/sklearn_hyper_dict/","title":"sklearn_hyper_dict","text":""},{"location":"reference/spotpython/hyperdict/sklearn_hyper_dict/#spotpython.hyperdict.sklearn_hyper_dict.SklearnHyperDict","title":"<code>SklearnHyperDict</code>","text":"<p>               Bases: <code>FileConfig</code></p> <p>Scikit-learn hyperparameter dictionary.</p> <p>This class extends the FileConfig class to provide a dictionary for storing hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The name of the file where the hyperparameters are stored.</p> Source code in <code>spotpython/hyperdict/sklearn_hyper_dict.py</code> <pre><code>class SklearnHyperDict(base.FileConfig):\n    \"\"\"Scikit-learn hyperparameter dictionary.\n\n    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n\n    Attributes:\n        filename (str): The name of the file where the hyperparameters are stored.\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"sklearn_hyper_dict.json\",\n        directory: None = None,\n    ) -&gt; None:\n        super().__init__(filename=filename, directory=directory)\n        self.filename = filename\n        self.directory = directory\n        self.hyper_dict = self.load()\n\n    @property\n    def path(self):\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    def load(self) -&gt; dict:\n        \"\"\"Load the hyperparameters from the file.\n\n        Returns:\n            (dict): A dictionary containing the hyperparameters.\n        Examples:\n            &gt;&gt;&gt; shd = SklearnHyperDict()\n            &gt;&gt;&gt; hyperparams = shd.load()\n            &gt;&gt;&gt; print(hyperparams)\n            {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n        \"\"\"\n        with open(self.path, \"r\") as f:\n            d = json.load(f)\n        return d\n</code></pre>"},{"location":"reference/spotpython/hyperdict/sklearn_hyper_dict/#spotpython.hyperdict.sklearn_hyper_dict.SklearnHyperDict.load","title":"<code>load()</code>","text":"<p>Load the hyperparameters from the file.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the hyperparameters.</p> <p>Examples:     &gt;&gt;&gt; shd = SklearnHyperDict()     &gt;&gt;&gt; hyperparams = shd.load()     &gt;&gt;&gt; print(hyperparams)     {\u2018learning_rate\u2019: 0.001, \u2018batch_size\u2019: 32, \u2018epochs\u2019: 10}</p> Source code in <code>spotpython/hyperdict/sklearn_hyper_dict.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"Load the hyperparameters from the file.\n\n    Returns:\n        (dict): A dictionary containing the hyperparameters.\n    Examples:\n        &gt;&gt;&gt; shd = SklearnHyperDict()\n        &gt;&gt;&gt; hyperparams = shd.load()\n        &gt;&gt;&gt; print(hyperparams)\n        {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n    \"\"\"\n    with open(self.path, \"r\") as f:\n        d = json.load(f)\n    return d\n</code></pre>"},{"location":"reference/spotpython/hyperdict/torch_hyper_dict/","title":"torch_hyper_dict","text":""},{"location":"reference/spotpython/hyperdict/torch_hyper_dict/#spotpython.hyperdict.torch_hyper_dict.TorchHyperDict","title":"<code>TorchHyperDict</code>","text":"<p>               Bases: <code>FileConfig</code></p> <p>PyTorch hyperparameter dictionary.</p> <p>This class extends the FileConfig class to provide a dictionary for storing hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The name of the file where the hyperparameters are stored.</p> Source code in <code>spotpython/hyperdict/torch_hyper_dict.py</code> <pre><code>class TorchHyperDict(base.FileConfig):\n    \"\"\"PyTorch hyperparameter dictionary.\n\n    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n\n    Attributes:\n        filename (str): The name of the file where the hyperparameters are stored.\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"torch_hyper_dict.json\",\n        directory: None = None,\n    ) -&gt; None:\n        super().__init__(filename=filename, directory=directory)\n        self.filename = filename\n        self.directory = directory\n        self.hyper_dict = self.load()\n\n    @property\n    def path(self):\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    def load(self) -&gt; dict:\n        \"\"\"Load the hyperparameters from the file.\n\n        Returns:\n            (dict): A dictionary containing the hyperparameters.\n        Examples:\n            &gt;&gt;&gt; thd = TorchHyperDict()\n            &gt;&gt;&gt; hyperparams = thd.load()\n            &gt;&gt;&gt; print(hyperparams)\n            {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n        \"\"\"\n        with open(self.path, \"r\") as f:\n            d = json.load(f)\n        return d\n</code></pre>"},{"location":"reference/spotpython/hyperdict/torch_hyper_dict/#spotpython.hyperdict.torch_hyper_dict.TorchHyperDict.load","title":"<code>load()</code>","text":"<p>Load the hyperparameters from the file.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the hyperparameters.</p> <p>Examples:     &gt;&gt;&gt; thd = TorchHyperDict()     &gt;&gt;&gt; hyperparams = thd.load()     &gt;&gt;&gt; print(hyperparams)     {\u2018learning_rate\u2019: 0.001, \u2018batch_size\u2019: 32, \u2018epochs\u2019: 10}</p> Source code in <code>spotpython/hyperdict/torch_hyper_dict.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"Load the hyperparameters from the file.\n\n    Returns:\n        (dict): A dictionary containing the hyperparameters.\n    Examples:\n        &gt;&gt;&gt; thd = TorchHyperDict()\n        &gt;&gt;&gt; hyperparams = thd.load()\n        &gt;&gt;&gt; print(hyperparams)\n        {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n    \"\"\"\n    with open(self.path, \"r\") as f:\n        d = json.load(f)\n    return d\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/architecture/","title":"architecture","text":""},{"location":"reference/spotpython/hyperparameters/architecture/#spotpython.hyperparameters.architecture.get_hidden_sizes","title":"<code>get_hidden_sizes(_L_in, l1, n=10)</code>","text":"<p>Generates a list of hidden sizes for a neural network with a given input size and l1 regularization. The list is generated by dividing the input size by 2 until the minimum size is reached.</p> <p>Parameters:</p> Name Type Description Default <code>_L_in</code> <code>int</code> <p>input size.</p> required <code>l1</code> <code>int</code> <p>l1 regularization.</p> required <code>n</code> <code>int</code> <p>number of hidden sizes to generate.</p> <code>10</code> <p>Returns:</p> Type Description <code>list</code> <p>list of hidden sizes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.architecture import get_hidden_sizes\n    _L_in = 10\n    l1 = 10\n    n = 10\n    get_hidden_sizes(_L_in, l1, n)\n    [10, 5, 2, 1, 1, 1, 1, 1, 1, 1]\n</code></pre> Source code in <code>spotpython/hyperparameters/architecture.py</code> <pre><code>def get_hidden_sizes(_L_in, l1, n=10) -&gt; list:\n    \"\"\"\n    Generates a list of hidden sizes for a neural network with a given input size and l1 regularization.\n    The list is generated by dividing the input size by 2 until the minimum size is reached.\n\n    Args:\n        _L_in (int):\n            input size.\n        l1 (int):\n            l1 regularization.\n        n (int):\n            number of hidden sizes to generate.\n\n    Returns:\n        (list):\n            list of hidden sizes.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.architecture import get_hidden_sizes\n            _L_in = 10\n            l1 = 10\n            n = 10\n            get_hidden_sizes(_L_in, l1, n)\n            [10, 5, 2, 1, 1, 1, 1, 1, 1, 1]\n    \"\"\"\n    if l1 &lt; 4:\n        raise ValueError(\"l1 must be at least 4\")\n    n_low = _L_in // 4\n    n_high = max(l1, 2 * n_low)\n    hidden_sizes = generate_div2_list(n_high, n_low)\n    # keep only the first n values of hidden_sizes list\n    if len(hidden_sizes) &gt; n:\n        hidden_sizes = hidden_sizes[:n]\n    return hidden_sizes\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/categorical/","title":"categorical","text":""},{"location":"reference/spotpython/hyperparameters/categorical/#spotpython.hyperparameters.categorical.add_missing_elements","title":"<code>add_missing_elements(a, b)</code>","text":"<p>Add missing elements from list a to list b. Arguments:     a (list): List of elements to check.     b (list): List of elements to add to.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of elements with missing elements from list a added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; a = [1, 4]\n    b = [1, 2]\n    add_missing_elements(a, b)\n    [1, 2, 4]\n</code></pre> Source code in <code>spotpython/hyperparameters/categorical.py</code> <pre><code>def add_missing_elements(a: list, b: list) -&gt; list:\n    \"\"\"Add missing elements from list a to list b.\n    Arguments:\n        a (list): List of elements to check.\n        b (list): List of elements to add to.\n\n    Returns:\n        list: List of elements with missing elements from list a added.\n\n    Examples:\n        &gt;&gt;&gt; a = [1, 4]\n            b = [1, 2]\n            add_missing_elements(a, b)\n            [1, 2, 4]\n    \"\"\"\n    for element in a:\n        if element not in b:\n            b.append(element)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/categorical/#spotpython.hyperparameters.categorical.find_closest_key","title":"<code>find_closest_key(integer_value, encoding_dict)</code>","text":"<p>Given an integer value and an encoding dictionary that maps keys to binary values, this function finds the key in the dictionary whose binary value is closest to the binary representation of the integer value.</p> <p>Parameters:</p> Name Type Description Default <code>integer_value</code> <code>int</code> <p>The integer value to find the closest key for.</p> required <code>encoding_dict</code> <code>dict</code> <p>The encoding dictionary that maps keys to binary values.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The key in the encoding dictionary whose binary value is</p> <code>str</code> <p>closest to the binary representation of the integer value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; encoding_dict = {'A': [1, 0, 0], 'B': [0, 1, 0], 'C': [0, 0, 1]}\n    find_closest_key(6, encoding_dict)\n    'B'\n</code></pre> Source code in <code>spotpython/hyperparameters/categorical.py</code> <pre><code>def find_closest_key(integer_value: int, encoding_dict: dict) -&gt; str:\n    \"\"\"\n    Given an integer value and an encoding dictionary that maps keys to binary values,\n    this function finds the key in the dictionary whose binary value is closest to the binary\n    representation of the integer value.\n\n    Arguments:\n        integer_value (int): The integer value to find the closest key for.\n        encoding_dict (dict): The encoding dictionary that maps keys to binary values.\n\n    Returns:\n        str: The key in the encoding dictionary whose binary value is\n        closest to the binary representation of the integer value.\n\n    Examples:\n        &gt;&gt;&gt; encoding_dict = {'A': [1, 0, 0], 'B': [0, 1, 0], 'C': [0, 0, 1]}\n            find_closest_key(6, encoding_dict)\n            'B'\n    \"\"\"\n    binary_value = [int(x) for x in format(integer_value, f\"0{len(list(encoding_dict.values())[0])}b\")]\n    min_distance = float(\"inf\")\n    closest_key = None\n    for key, encoded_value in encoding_dict.items():\n        distance = sum([x != y for x, y in zip(binary_value, encoded_value)])\n        if distance &lt; min_distance:\n            min_distance = distance\n            closest_key = key\n    return closest_key\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/categorical/#spotpython.hyperparameters.categorical.get_one_hot","title":"<code>get_one_hot(alg, hyper_param, d=None, filename='data.json')</code>","text":"<p>Get one hot encoded values for a hyper parameter of an algorithm. Arguments:     alg (str): Name of the algorithm.     hyper_param (str): Name of the hyper parameter.     d (dict): Dictionary of algorithms and their hyperparameters.     filename (str): Name of the file containing the dictionary. Returns:     dict: Dictionary of hyper parameter values and their one hot encoded values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; alg = \"HoeffdingAdaptiveTreeClassifier\"\n    hyper_param = \"split_criterion\"\n    d = {\n        \"HoeffdingAdaptiveTreeClassifier\": {\n            \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"],\n            \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"],\n            \"bootstrap_sampling\": [\"0\", \"1\"]\n            },\n            \"HoeffdingTreeClassifier\": {\n                \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"],\n                \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"],\n                \"binary_split\": [\"0\", \"1\"],\n                \"stop_mem_management\": [\"0\", \"1\"]\n            }\n        }\n    get_one_hot(alg, hyper_param, d)\n    {'gini': [1, 0, 0], 'info_gain': [0, 1, 0], 'hellinger': [0, 0, 1]}\n</code></pre> Source code in <code>spotpython/hyperparameters/categorical.py</code> <pre><code>def get_one_hot(alg: str, hyper_param: str, d: dict = None, filename: str = \"data.json\") -&gt; dict:\n    \"\"\"Get one hot encoded values for a hyper parameter of an algorithm.\n    Arguments:\n        alg (str): Name of the algorithm.\n        hyper_param (str): Name of the hyper parameter.\n        d (dict): Dictionary of algorithms and their hyperparameters.\n        filename (str): Name of the file containing the dictionary.\n    Returns:\n        dict: Dictionary of hyper parameter values and their one hot encoded values.\n\n    Examples:\n        &gt;&gt;&gt; alg = \"HoeffdingAdaptiveTreeClassifier\"\n            hyper_param = \"split_criterion\"\n            d = {\n                \"HoeffdingAdaptiveTreeClassifier\": {\n                    \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"],\n                    \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"],\n                    \"bootstrap_sampling\": [\"0\", \"1\"]\n                    },\n                    \"HoeffdingTreeClassifier\": {\n                        \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"],\n                        \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"],\n                        \"binary_split\": [\"0\", \"1\"],\n                        \"stop_mem_management\": [\"0\", \"1\"]\n                    }\n                }\n            get_one_hot(alg, hyper_param, d)\n            {'gini': [1, 0, 0], 'info_gain': [0, 1, 0], 'hellinger': [0, 0, 1]}\n    \"\"\"\n    if d is None:\n        with open(filename, \"r\") as f:\n            d = json.load(f)\n    values = d[alg][hyper_param]\n    one_hot_encoded_values = one_hot_encode(values)\n    return one_hot_encoded_values\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/categorical/#spotpython.hyperparameters.categorical.one_hot_encode","title":"<code>one_hot_encode(strings)</code>","text":"<p>One hot encode a list of strings. Arguments:     strings (list): List of strings to encode. Returns:     dict: Dictionary of strings and their one hot encoded values. Examples:     &gt;&gt;&gt; one_hot_encode([\u2018a\u2019, \u2018b\u2019, \u2018c\u2019])     {\u2018a\u2019: [1, 0, 0], \u2018b\u2019: [0, 1, 0], \u2018c\u2019: [0, 0, 1]}</p> Source code in <code>spotpython/hyperparameters/categorical.py</code> <pre><code>def one_hot_encode(strings) -&gt; dict:\n    \"\"\"One hot encode a list of strings.\n    Arguments:\n        strings (list): List of strings to encode.\n    Returns:\n        dict: Dictionary of strings and their one hot encoded values.\n    Examples:\n        &gt;&gt;&gt; one_hot_encode(['a', 'b', 'c'])\n        {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]}\n    \"\"\"\n    n = len(strings)\n    encoding_dict = {}\n    for i, string in enumerate(strings):\n        one_hot_encoded_value = [0] * n\n        one_hot_encoded_value[i] = 1\n        encoding_dict[string] = one_hot_encoded_value\n    return encoding_dict\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/categorical/#spotpython.hyperparameters.categorical.sum_encoded_values","title":"<code>sum_encoded_values(strings, encoding_dict)</code>","text":"<p>Sum the encoded values of a list of strings.</p> <p>Parameters:</p> Name Type Description Default <code>strings</code> <code>list</code> <p>List of strings to encode.</p> required <code>encoding_dict</code> <code>dict</code> <p>Dictionary of strings and their one hot encoded values.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Decimal value of the sum of the encoded values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; encoding_dict = {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]}\n    sum_encoded_values(['a', 'b', 'c'], encoding_dict)\n    7\n    sum_encoded_values(['a', 'c'], encoding_dict)\n    5\n</code></pre> Source code in <code>spotpython/hyperparameters/categorical.py</code> <pre><code>def sum_encoded_values(strings, encoding_dict) -&gt; int:\n    \"\"\"Sum the encoded values of a list of strings.\n\n    Args:\n        strings (list): List of strings to encode.\n        encoding_dict (dict): Dictionary of strings and their one hot encoded values.\n\n    Returns:\n        int: Decimal value of the sum of the encoded values.\n\n    Examples:\n        &gt;&gt;&gt; encoding_dict = {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]}\n            sum_encoded_values(['a', 'b', 'c'], encoding_dict)\n            7\n            sum_encoded_values(['a', 'c'], encoding_dict)\n            5\n    \"\"\"\n    result = [0] * len(list(encoding_dict.values())[0])\n    for string in strings:\n        encoded_value = encoding_dict.get(string)\n        if encoded_value:\n            result = [sum(x) for x in zip(result, encoded_value)]\n    decimal_result = 0\n    for i, value in enumerate(result[::-1]):\n        decimal_result += value * (2**i)\n    return decimal_result\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/optimizer/","title":"optimizer","text":""},{"location":"reference/spotpython/hyperparameters/optimizer/#spotpython.hyperparameters.optimizer.optimizer_handler","title":"<code>optimizer_handler(optimizer_name, params, lr_mult=1.0, **kwargs)</code>","text":"<p>Returns an instance of the specified optimizer. See Notes below for supported optimizers.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_name</code> <code>str</code> <p>The name of the optimizer to use.</p> required <code>params</code> <code>list or Tensor</code> <p>The parameters to optimize.</p> required <code>lr_mult</code> <code>float</code> <p>A multiplier for the learning rate. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the optimizer.</p> <code>{}</code> Notes <p>The following optimizers are supported (see also: https://pytorch.org/docs/stable/optim.html#base-class):</p> <pre><code>* Adadelta\n* Adagrad\n* Adam\n* AdamW\n* SparseAdam\n* ASGD\n* LBFGS\n* NAdam\n* RAdam\n* RMSprop\n* Rprop\n* SGD\n</code></pre> <p>Returns:</p> Type Description <code>Optimizer</code> <p>An instance of the specified optimizer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.netlightregression import NetLightRegression\n    from torch import nn\n    import lightning as L\n    BATCH_SIZE = 8\n    lr_mult=0.1\n    dataset = Diabetes()\n    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    # First example: Adam\n    net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                    initialization='xavier', act_fn=nn.ReLU(),\n                                    optimizer='Adam', dropout_prob=0.1, lr_mult=lr_mult,\n                                    patience=5, _L_in=10, _L_out=1)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n    trainer.fit(net_light_base, train_loader)\n    # Adam uses a lr which is calculated as lr=lr_mult * 0.001, so this value\n    # should be 0.1 * 0.001 = 0.0001\n    trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*0.001\n    # Second example: Adadelta\n    net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                    initialization='xavier', act_fn=nn.ReLU(),\n                                    optimizer='Adadelta', dropout_prob=0.1, lr_mult=lr_mult,\n                                    patience=5, _L_in=10, _L_out=1)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n    trainer.fit(net_light_base, train_loader)\n    # Adadelta uses a lr which is calculated as lr=lr_mult * 1.0, so this value\n    # should be 1.0 * 0.1 = 0.1\n    trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*1.0\n</code></pre> Source code in <code>spotpython/hyperparameters/optimizer.py</code> <pre><code>def optimizer_handler(optimizer_name: str, params: Union[list, torch.Tensor], lr_mult: float = 1.0, **kwargs: Any) -&gt; torch.optim.Optimizer:\n    \"\"\"Returns an instance of the specified optimizer. See Notes below for supported optimizers.\n\n    Args:\n        optimizer_name (str):\n            The name of the optimizer to use.\n        params (list or torch.Tensor):\n            The parameters to optimize.\n        lr_mult (float, optional):\n            A multiplier for the learning rate. Defaults to 1.0.\n        **kwargs:\n            Additional keyword arguments for the optimizer.\n\n    Notes:\n        The following optimizers are supported (see also: https://pytorch.org/docs/stable/optim.html#base-class):\n\n            * Adadelta\n            * Adagrad\n            * Adam\n            * AdamW\n            * SparseAdam\n            * ASGD\n            * LBFGS\n            * NAdam\n            * RAdam\n            * RMSprop\n            * Rprop\n            * SGD\n\n    Returns:\n        (torch.optim.Optimizer):\n            An instance of the specified optimizer.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.netlightregression import NetLightRegression\n            from torch import nn\n            import lightning as L\n            BATCH_SIZE = 8\n            lr_mult=0.1\n            dataset = Diabetes()\n            train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            # First example: Adam\n            net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=lr_mult,\n                                            patience=5, _L_in=10, _L_out=1)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n            trainer.fit(net_light_base, train_loader)\n            # Adam uses a lr which is calculated as lr=lr_mult * 0.001, so this value\n            # should be 0.1 * 0.001 = 0.0001\n            trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*0.001\n            # Second example: Adadelta\n            net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adadelta', dropout_prob=0.1, lr_mult=lr_mult,\n                                            patience=5, _L_in=10, _L_out=1)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n            trainer.fit(net_light_base, train_loader)\n            # Adadelta uses a lr which is calculated as lr=lr_mult * 1.0, so this value\n            # should be 1.0 * 0.1 = 0.1\n            trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*1.0\n    \"\"\"\n    if optimizer_name == \"Adadelta\":\n        return torch.optim.Adadelta(\n            params,\n            lr=lr_mult * 1.0,\n            rho=0.9,\n            eps=1e-06,\n            weight_decay=0,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"Adagrad\":\n        return torch.optim.Adagrad(\n            params,\n            lr=lr_mult * 0.01,\n            lr_decay=0,\n            weight_decay=0,\n            initial_accumulator_value=0,\n            eps=1e-10,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"Adam\":\n        return torch.optim.Adam(\n            params,\n            lr=lr_mult * 0.001,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            amsgrad=False,\n            foreach=None,\n            maximize=False,\n            capturable=False,\n            # differentiable=False,\n            fused=None,\n        )\n    elif optimizer_name == \"AdamW\":\n        return torch.optim.AdamW(\n            params,\n            lr=lr_mult * 0.001,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0.01,\n            amsgrad=False,\n            foreach=None,\n            maximize=False,\n            capturable=False,\n            # differentiable=False,\n            # fused=None,\n        )\n    elif optimizer_name == \"SparseAdam\":\n        return torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, maximize=False)\n    elif optimizer_name == \"Adamax\":\n        return torch.optim.Adamax(\n            params,\n            lr=lr_mult * 0.002,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"ASGD\":\n        return torch.optim.ASGD(\n            params,\n            lr=lr_mult * 0.01,\n            lambd=0.0001,\n            alpha=0.75,\n            t0=1000000.0,\n            weight_decay=0,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"LBFGS\":\n        return torch.optim.LBFGS(\n            params,\n            lr=lr_mult * 1,\n            max_iter=20,\n            max_eval=None,\n            tolerance_grad=1e-07,\n            tolerance_change=1e-09,\n            history_size=100,\n            line_search_fn=None,\n        )\n    elif optimizer_name == \"NAdam\":\n        return torch.optim.NAdam(\n            params,\n            lr=lr_mult * 0.002,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            momentum_decay=0.004,\n            foreach=None,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"RAdam\":\n        return torch.optim.RAdam(\n            params,\n            lr=0.001,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            foreach=None,\n            # differentiable=False\n        )\n    elif optimizer_name == \"RMSprop\":\n        return torch.optim.RMSprop(\n            params,\n            lr=lr_mult * 0.01,\n            alpha=0.99,\n            eps=1e-08,\n            weight_decay=0,\n            momentum=0,\n            centered=False,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"Rprop\":\n        return torch.optim.Rprop(\n            params,\n            lr=lr_mult * 0.01,\n            etas=(0.5, 1.2),\n            step_sizes=(1e-06, 50),\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"SGD\":\n        return torch.optim.SGD(\n            params,\n            lr=lr_mult * 1e-3,\n            momentum=0,\n            dampening=0,\n            weight_decay=0,\n            nesterov=False,\n            maximize=False,\n            foreach=None,\n            # differentiable=False,\n        )\n    else:\n        raise ValueError(f\"Optimizer {optimizer_name} not supported\")\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/","title":"update","text":""},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.add_core_model_to_fun_control","title":"<code>add_core_model_to_fun_control(fun_control, core_model, hyper_dict=None, filename=None)</code>","text":"<p>Add the core model to the function control dictionary. It updates the keys \u201ccore_model\u201d, \u201ccore_model_hyper_dict\u201d, \u201cvar_type\u201d, \u201cvar_name\u201d in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>core_model</code> <code>class</code> <p>The core model.</p> required <code>hyper_dict</code> <code>dict</code> <p>The hyper parameter dictionary. Optional. Default is None. If no hyper_dict is provided, the function will try to load the hyper_dict from the file specified by filename.</p> <code>None</code> <code>filename</code> <code>str</code> <p>The name of the json file that contains the hyper parameter dictionary. Optional. Default is None. If no filename is provided, the function will try to load the hyper_dict from the hyper_dict dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated fun_control dictionary.</p> Notes <p>The function adds the following keys to the fun_control dictionary: \u201ccore_model\u201d: The core model. \u201ccore_model_hyper_dict\u201d: The hyper parameter dictionary for the core model. \u201ccore_model_hyper_dict_default\u201d: The hyper parameter dictionary for the core model. \u201cvar_type\u201d: A list of variable types. \u201cvar_name\u201d: A list of variable names. The original hyperparameters of the core model are stored in the \u201ccore_model_hyper_dict_default\u201d key. These remain unmodified, while the \u201ccore_model_hyper_dict\u201d key is modified during the tuning process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                hyper_dict=LightHyperDict)\n    # or, if a user wants to use a custom hyper_dict:\n&gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                filename=\"./hyperdict/user_hyper_dict.json\")\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def add_core_model_to_fun_control(fun_control, core_model, hyper_dict=None, filename=None) -&gt; dict:\n    \"\"\"Add the core model to the function control dictionary. It updates the keys \"core_model\",\n    \"core_model_hyper_dict\", \"var_type\", \"var_name\" in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        core_model (class):\n            The core model.\n        hyper_dict (dict):\n            The hyper parameter dictionary. Optional. Default is None. If no hyper_dict is provided,\n            the function will try to load the hyper_dict from the file specified by filename.\n        filename (str):\n            The name of the json file that contains the hyper parameter dictionary.\n            Optional. Default is None. If no filename is provided, the function will try to load the\n            hyper_dict from the hyper_dict dictionary.\n\n    Returns:\n        (dict):\n            The updated fun_control dictionary.\n\n    Notes:\n        The function adds the following keys to the fun_control dictionary:\n        \"core_model\": The core model.\n        \"core_model_hyper_dict\": The hyper parameter dictionary for the core model.\n        \"core_model_hyper_dict_default\": The hyper parameter dictionary for the core model.\n        \"var_type\": A list of variable types.\n        \"var_name\": A list of variable names.\n        The original hyperparameters of the core model are stored in the \"core_model_hyper_dict_default\" key.\n        These remain unmodified, while the \"core_model_hyper_dict\" key is modified during the tuning process.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        hyper_dict=LightHyperDict)\n            # or, if a user wants to use a custom hyper_dict:\n        &gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        filename=\"./hyperdict/user_hyper_dict.json\")\n\n    \"\"\"\n    fun_control.update({\"core_model\": core_model})\n    if filename is None:\n        new_hyper_dict = hyper_dict().load()\n    else:\n        with open(filename, \"r\") as f:\n            new_hyper_dict = json.load(f)\n    fun_control.update({\"core_model_hyper_dict\": new_hyper_dict[core_model.__name__]})\n    fun_control.update({\"core_model_hyper_dict_default\": copy.deepcopy(new_hyper_dict[core_model.__name__])})\n    var_type = get_var_type(fun_control)\n    var_name = get_var_name(fun_control)\n    lower = get_bound_values(fun_control, \"lower\", as_list=False)\n    upper = get_bound_values(fun_control, \"upper\", as_list=False)\n    fun_control.update({\"var_type\": var_type, \"var_name\": var_name, \"lower\": lower, \"upper\": upper})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.assign_values","title":"<code>assign_values(X, var_list)</code>","text":"<p>This function takes an np.array X and a list of variable names as input arguments and returns a dictionary where the keys are the variable names and the values are assigned from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>A 2D numpy array where each column represents a variable.</p> required <code>var_list</code> <code>list</code> <p>A list of strings representing variable names.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are assigned from X.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.hyperparameters.values import assign_values\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; var_list = ['a', 'b']\n&gt;&gt;&gt; result = assign_values(X, var_list)\n&gt;&gt;&gt; print(result)\n{'a': array([1, 3, 5]), 'b': array([2, 4, 6])}\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def assign_values(X: np.array, var_list: list) -&gt; dict:\n    \"\"\"\n    This function takes an np.array X and a list of variable names as input arguments\n    and returns a dictionary where the keys are the variable names and the values are assigned from X.\n\n    Args:\n        X (np.array):\n            A 2D numpy array where each column represents a variable.\n        var_list (list):\n            A list of strings representing variable names.\n\n    Returns:\n        dict:\n            A dictionary where keys are variable names and values are assigned from X.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import assign_values\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; var_list = ['a', 'b']\n        &gt;&gt;&gt; result = assign_values(X, var_list)\n        &gt;&gt;&gt; print(result)\n        {'a': array([1, 3, 5]), 'b': array([2, 4, 6])}\n    \"\"\"\n    result = {}\n    for i, var_name in enumerate(var_list):\n        result[var_name] = X[:, i]\n    return result\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.convert_keys","title":"<code>convert_keys(d, var_type)</code>","text":"<p>Convert values in a dictionary to integers based on a list of variable types. This function takes a dictionary <code>d</code> and a list of variable types <code>var_type</code> as arguments. For each key in the dictionary, if the corresponding entry in <code>var_type</code> is not equal to <code>\"num\"</code>, the value associated with that key is converted to an integer.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>The input dictionary.</p> required <code>var_type</code> <code>list</code> <p>A list of variable types. If the entry is not <code>\"num\"</code> the corresponding value will be converted to the type <code>\"int\"</code>.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Union[int, float]]</code> <p>The modified dictionary with values converted to integers based on <code>var_type</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import convert_keys\n&gt;&gt;&gt; d = {'a': '1.1', 'b': '2', 'c': '3.1'}\n&gt;&gt;&gt; var_type = [\"int\", \"num\", \"int\"]\n&gt;&gt;&gt; convert_keys(d, var_type)\n{'a': 1, 'b': '2', 'c': 3}\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def convert_keys(d: Dict[str, Union[int, float, str]], var_type: List[str]) -&gt; Dict[str, Union[int, float]]:\n    \"\"\"Convert values in a dictionary to integers based on a list of variable types.\n    This function takes a dictionary `d` and a list of variable types `var_type` as arguments.\n    For each key in the dictionary,\n    if the corresponding entry in `var_type` is not equal to `\"num\"`,\n    the value associated with that key is converted to an integer.\n\n    Args:\n        d (dict): The input dictionary.\n        var_type (list):\n            A list of variable types. If the entry is not `\"num\"` the corresponding\n            value will be converted to the type `\"int\"`.\n\n    Returns:\n        dict: The modified dictionary with values converted to integers based on `var_type`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import convert_keys\n        &gt;&gt;&gt; d = {'a': '1.1', 'b': '2', 'c': '3.1'}\n        &gt;&gt;&gt; var_type = [\"int\", \"num\", \"int\"]\n        &gt;&gt;&gt; convert_keys(d, var_type)\n        {'a': 1, 'b': '2', 'c': 3}\n    \"\"\"\n    keys = list(d.keys())\n    for i in range(len(keys)):\n        if var_type[i] not in [\"num\", \"float\"]:\n            d[keys[i]] = int(d[keys[i]])\n    return d\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.create_model","title":"<code>create_model(config, fun_control, **kwargs)</code>","text":"<p>Creates a model for the given configuration and control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>dictionary containing the configuration for the hyperparameter tuning.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <code>**kwargs</code> <code>Any</code> <p>additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>object</code> <p>model object.</p> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def create_model(config, fun_control, **kwargs) -&gt; object:\n    \"\"\"\n    Creates a model for the given configuration and control parameters.\n\n    Args:\n        config (dict):\n            dictionary containing the configuration for the hyperparameter tuning.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n        **kwargs (Any):\n            additional keyword arguments.\n\n    Returns:\n        (object):\n            model object.\n    \"\"\"\n    return fun_control[\"core_model\"](**config, **kwargs)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.generate_one_config_from_var_dict","title":"<code>generate_one_config_from_var_dict(var_dict, fun_control, default=False)</code>","text":"<p>Generate one configuration from a dictionary of variables (as a generator).</p> <p>This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d.</p> required <code>default</code> <code>bool</code> <p>A boolean value indicating whether to use the default values from fun_control.</p> <code>False</code> <p>Returns:</p> Type Description <code>Generator[Dict[str, Union[int, float]], None, None]</code> <p>Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.hyperparameters.values import generate_one_config_from_var_dict\n&gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n&gt;&gt;&gt; fun_control = {\"var_type\": [\"int\", \"num\"]}\n&gt;&gt;&gt; list(generate_one_config_from_var_dict(var_dict, fun_control))\n[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def generate_one_config_from_var_dict(\n    var_dict: Dict[str, np.ndarray],\n    fun_control: Dict[str, Union[List[str], str]],\n    default: bool = False,\n) -&gt; Generator[Dict[str, Union[int, float]], None, None]:\n    \"\"\"Generate one configuration from a dictionary of variables (as a generator).\n\n    This function takes a dictionary of variables as input arguments and returns a generator\n    that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Args:\n        var_dict (dict):\n            A dictionary where keys are variable names and values are numpy arrays.\n        fun_control (dict):\n            A dictionary which (at least) has an entry with the following key:\n            \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding\n            value will be converted to the type \"int\".\n        default (bool):\n            A boolean value indicating whether to use the default values from fun_control.\n\n    Returns:\n        Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import generate_one_config_from_var_dict\n        &gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n        &gt;&gt;&gt; fun_control = {\"var_type\": [\"int\", \"num\"]}\n        &gt;&gt;&gt; list(generate_one_config_from_var_dict(var_dict, fun_control))\n        [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n    \"\"\"\n    for values in iterate_dict_values(var_dict):\n        values = convert_keys(values, fun_control[\"var_type\"])\n        values = get_dict_with_levels_and_types(fun_control=fun_control, v=values, default=default)\n        values = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values)\n        yield values\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_bound_values","title":"<code>get_bound_values(fun_control, bound, as_list=False)</code>","text":"<p>Generate a list or array from a dictionary. This function takes the values from the keys \u201cbound\u201d in the fun_control[\u201ccore_model_hyper_dict\u201d] dictionary and returns a list or array of the values in the same order as the keys in the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>A dictionary containing a key \u201ccore_model_hyper_dict\u201d which is a dictionary with keys that have either an \u201cupper\u201d or \u201clower\u201d value.</p> required <code>bound</code> <code>str</code> <p>Either \u201cupper\u201d or \u201clower\u201d, indicating which value to extract from the inner dictionary.</p> required <code>as_list</code> <code>bool</code> <p>If True, return a list. If False, return a numpy array. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List, ndarray]</code> <p>list or np.ndarray: A list or array of the extracted values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If bound is not \u201cupper\u201d or \u201clower\u201d.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_bound_values\n&gt;&gt;&gt; fun_control = {\"core_model_hyper_dict\": {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}}}\n&gt;&gt;&gt; get_bound_values(fun_control, \"upper\", as_list=True)\n[1, 2]\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_bound_values(fun_control: dict, bound: str, as_list: bool = False) -&gt; Union[List, np.ndarray]:\n    \"\"\"Generate a list or array from a dictionary.\n    This function takes the values from the keys \"bound\" in the\n    fun_control[\"core_model_hyper_dict\"] dictionary and returns a list or array of the values\n    in the same order as the keys in the dictionary.\n\n    Args:\n        fun_control (dict):\n            A dictionary containing a key \"core_model_hyper_dict\"\n            which is a dictionary with keys that have either an \"upper\" or \"lower\" value.\n        bound (str):\n            Either \"upper\" or \"lower\",\n            indicating which value to extract from the inner dictionary.\n        as_list (bool):\n            If True, return a list.\n            If False, return a numpy array. Default is False.\n\n    Returns:\n        list or np.ndarray:\n            A list or array of the extracted values.\n\n    Raises:\n        ValueError:\n            If bound is not \"upper\" or \"lower\".\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_bound_values\n        &gt;&gt;&gt; fun_control = {\"core_model_hyper_dict\": {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}}}\n        &gt;&gt;&gt; get_bound_values(fun_control, \"upper\", as_list=True)\n        [1, 2]\n    \"\"\"\n    # Throw value error if bound is not upper or lower:\n    if bound not in [\"upper\", \"lower\"]:\n        raise ValueError(\"bound must be either 'upper' or 'lower'\")\n    # check if key \"core_model_hyper_dict\" exists in fun_control:\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        d = fun_control[\"core_model_hyper_dict\"]\n        b = []\n        for key, value in d.items():\n            b.append(value[bound])\n        if as_list:\n            return b\n        else:\n            return np.array(b)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_control_key_value","title":"<code>get_control_key_value(control_dict=None, key=None)</code>","text":"<p>This function gets the key value pair from the control_dict dictionary. If the key does not exist, return None. If the control_dict dictionary is None, return None.</p> <p>Parameters:</p> Name Type Description Default <code>control_dict</code> <code>dict</code> <p>control_dict dictionary</p> <code>None</code> <code>key</code> <code>str</code> <p>key</p> <code>None</code> <p>Returns:</p> Name Type Description <code>value</code> <code>Any</code> <p>value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import get_control_key_value\n    control_dict = fun_control_init()\n    get_control_key_value(control_dict=control_dict,\n                    key=\"key\")\n    \"value\"\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_control_key_value(control_dict=None, key=None) -&gt; Any:\n    \"\"\"\n    This function gets the key value pair from the control_dict dictionary.\n    If the key does not exist, return None.\n    If the control_dict dictionary is None, return None.\n\n    Args:\n        control_dict (dict):\n            control_dict dictionary\n        key (str): key\n\n    Returns:\n        value (Any):\n            value\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import get_control_key_value\n            control_dict = fun_control_init()\n            get_control_key_value(control_dict=control_dict,\n                            key=\"key\")\n            \"value\"\n    \"\"\"\n    if control_dict is None:\n        return None\n    else:\n        # check if key \"core_model_hyper_dict\" exists in fun_control:\n        if \"core_model_hyper_dict\" in control_dict.keys():\n            if key == \"lower\":\n                lower = get_bound_values(fun_control=control_dict, bound=\"lower\")\n                return lower\n            if key == \"upper\":\n                upper = get_bound_values(fun_control=control_dict, bound=\"upper\")\n                return upper\n            if key == \"var_name\":\n                var_name = get_var_name(fun_control=control_dict)\n                return var_name\n            if key == \"var_type\":\n                var_type = get_var_type(fun_control=control_dict)\n                return var_type\n            if key == \"transform\":\n                transform = get_transform(fun_control=control_dict)\n                return transform\n        # check if key exists in control_dict:\n        elif control_dict is None or key not in control_dict.keys():\n            return None\n        else:\n            return control_dict[key]\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_core_model_parameter_type_from_var_name","title":"<code>get_core_model_parameter_type_from_var_name(fun_control, var_name)</code>","text":"<p>Extracts the core_model_parameter_type value from a dictionary for a specified key.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The dictionary containing the information.</p> required <code>var_name</code> <code>str</code> <p>The key for which to extract the core_model_parameter_type value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The core_model_parameter_type value if available, else None.</p> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_core_model_parameter_type_from_var_name(fun_control, var_name) -&gt; str:\n    \"\"\"\n    Extracts the core_model_parameter_type value from a dictionary for a specified key.\n\n    Args:\n        fun_control (dict):\n            The dictionary containing the information.\n        var_name (str):\n            The key for which to extract the core_model_parameter_type value.\n\n    Returns:\n        (str):\n            The core_model_parameter_type value if available, else None.\n    \"\"\"\n    # Check if the key exists in the dictionary and it has a 'core_model_parameter_type' entry\n    if var_name in fun_control[\"core_model_hyper_dict\"] and \"core_model_parameter_type\" in fun_control[\"core_model_hyper_dict\"][var_name]:\n        return fun_control[\"core_model_hyper_dict\"][var_name][\"core_model_parameter_type\"]\n    else:\n        return None\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_default_hyperparameters_as_array","title":"<code>get_default_hyperparameters_as_array(fun_control)</code>","text":"<p>Get the default hyper parameters as array.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The default hyper parameters as array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    from spotpython.hyperparameters.values import (\n        get_default_hyperparameters_as_array,\n        add_core_model_to_fun_control)\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    get_default_hyperparameters_as_array(fun_control)\n    array([0, 0, 0, 0, 0])\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_default_hyperparameters_as_array(fun_control) -&gt; np.array:\n    \"\"\"Get the default hyper parameters as array.\n\n    Args:\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (np.array):\n            The default hyper parameters as array.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            from spotpython.hyperparameters.values import (\n                get_default_hyperparameters_as_array,\n                add_core_model_to_fun_control)\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            get_default_hyperparameters_as_array(fun_control)\n            array([0, 0, 0, 0, 0])\n    \"\"\"\n    X0 = get_default_values(fun_control)\n    X0 = replace_levels_with_positions(fun_control[\"core_model_hyper_dict_default\"], X0)\n    if X0 is None:\n        return None\n    else:\n        X0 = get_values_from_dict(X0)\n        X0 = np.array([X0])\n        X0.shape[1]\n        return X0\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_default_values","title":"<code>get_default_values(fun_control)</code>","text":"<p>Get the values from the \u201cdefault\u201d keys from the dictionary fun_control as a dict. If the key of the value has as \u201ctype\u201d the value \u201cint\u201d or \u201cfloat\u201d, convert the value to the corresponding type.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with levels and types</p> required <p>Returns:</p> Name Type Description <code>new_dict</code> <code>dict</code> <p>dictionary with default values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n    d = {\"core_model_hyper_dict\":{\n        \"leaf_prediction\": {\n            \"levels\": [\"mean\", \"model\", \"adaptive\"],\n            \"type\": \"factor\",\n            \"default\": \"mean\",\n            \"core_model_parameter_type\": \"str\"},\n        \"leaf_model\": {\n            \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n            \"type\": \"factor\",\n            \"default\": \"LinearRegression\",\n            \"core_model_parameter_type\": \"instance\"},\n        \"splitter\": {\n            \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n            \"type\": \"factor\",\n            \"default\": \"EBSTSplitter\",\n            \"core_model_parameter_type\": \"instance()\"},\n        \"binary_split\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"},\n        \"stop_mem_management\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"}}}\n    get_default_values(d)\n    {'leaf_prediction': 'mean',\n    'leaf_model': 'linear_model.LinearRegression',\n    'splitter': 'EBSTSplitter',\n    'binary_split': 0,\n    'stop_mem_management': 0}\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_default_values(fun_control) -&gt; dict:\n    \"\"\"Get the values from the \"default\" keys from the dictionary fun_control as a dict.\n    If the key of the value has as \"type\" the value \"int\" or \"float\", convert the value to the corresponding type.\n\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n\n    Returns:\n        new_dict (dict):\n            dictionary with default values\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n            d = {\"core_model_hyper_dict\":{\n                \"leaf_prediction\": {\n                    \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                    \"type\": \"factor\",\n                    \"default\": \"mean\",\n                    \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}}\n            get_default_values(d)\n            {'leaf_prediction': 'mean',\n            'leaf_model': 'linear_model.LinearRegression',\n            'splitter': 'EBSTSplitter',\n            'binary_split': 0,\n            'stop_mem_management': 0}\n    \"\"\"\n    d = fun_control[\"core_model_hyper_dict_default\"]\n    new_dict = {}\n    for key, value in d.items():\n        if value[\"type\"] == \"int\":\n            new_dict[key] = int(value[\"default\"])\n        elif value[\"type\"] == \"float\":\n            new_dict[key] = float(value[\"default\"])\n        else:\n            new_dict[key] = value[\"default\"]\n    return new_dict\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_dict_with_levels_and_types","title":"<code>get_dict_with_levels_and_types(fun_control, v, default=False)</code>","text":"<p>Get dictionary with levels and types. The function maps the numerical output of the hyperparameter optimization to the corresponding levels of the hyperparameter needed by the core model, i.e., the tuned algorithm. The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. If the key value in the dictionary is 0, it takes the first value from the list, if it is 1, it takes the second and so on. If a key is not in fun_control, it takes the key from v. If the core_model_parameter_type value is instance, it returns the class of the value from the module via getattr(\u201cclass\u201d, value).</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>Dict[str, Any]</code> <p>A dictionary containing information about the core model hyperparameters.</p> required <code>v</code> <code>Dict[str, Any]</code> <p>A dictionary containing the numerical output of the hyperparameter optimization.</p> required <code>default</code> <code>bool</code> <p>A boolean value indicating whether to use the default values from fun_control.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fun_control = {\n...     \"core_model_hyper_dict\": {\n...         \"leaf_prediction\": {\n...             \"levels\": [\"mean\", \"model\", \"adaptive\"],\n...             \"type\": \"factor\",\n...             \"default\": \"mean\",\n...             \"core_model_parameter_type\": \"str\"\n...         },\n...         \"leaf_model\": {\n...             \"levels\": [\n...                 \"linear_model.LinearRegression\",\n...                 \"linear_model.PARegressor\",\n...                 \"linear_model.Perceptron\"\n...             ],\n...             \"type\": \"factor\",\n...             \"default\": \"LinearRegression\",\n...             \"core_model_parameter_type\": \"instance\"\n...         },\n...         \"splitter\": {\n...             \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n...             \"type\": \"factor\",\n...             \"default\": \"EBSTSplitter\",\n...             \"core_model_parameter_type\": \"instance()\"\n...         },\n...         \"binary_split\": {\n...             \"levels\": [0, 1],\n...             \"type\": \"factor\",\n...             \"default\": 0,\n...             \"core_model_parameter_type\": \"bool\"\n...         },\n...         \"stop_mem_management\": {\n...             \"levels\": [0, 1],\n...             \"type\": \"factor\",\n...             \"default\": 0,\n...             \"core_model_parameter_type\": \"bool\"\n...         }\n...     }\n... }\n&gt;&gt;&gt; v = {\n...     'grace_period': 200,\n...     'max_depth': 10,\n...     'delta': 1e-07,\n...     'tau': 0.05,\n...     'leaf_prediction': 0,\n...     'leaf_model': 0,\n...     'model_selector_decay': 0.95,\n...     'splitter': 1,\n...     'min_samples_split': 9,\n...     'binary_split': 0,\n...     'max_size': 500.0\n... }\n&gt;&gt;&gt; get_dict_with_levels_and_types(fun_control, v)\n{\n    'grace_period': 200,\n    'max_depth': 10,\n    'delta': 1e-07,\n    'tau': 0.05,\n    'leaf_prediction': 'mean',\n    'leaf_model': linear_model.LinearRegression,\n    'model_selector_decay': 0.95,\n    'splitter': TEBSTSplitter,\n    'min_samples_split': 9,\n    'binary_split': False,\n    'max_size': 500.0\n}\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_dict_with_levels_and_types(fun_control: Dict[str, Any], v: Dict[str, Any], default=False) -&gt; Dict[str, Any]:\n    \"\"\"Get dictionary with levels and types.\n    The function maps the numerical output of the hyperparameter optimization to the corresponding levels\n    of the hyperparameter needed by the core model, i.e., the tuned algorithm.\n    The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v\n    but with the values of the levels of the keys from fun_control.\n    If the key value in the dictionary is 0, it takes the first value from the list,\n    if it is 1, it takes the second and so on.\n    If a key is not in fun_control, it takes the key from v.\n    If the core_model_parameter_type value is instance, it returns the class of the value from the module\n    via getattr(\"class\", value).\n\n    Args:\n        fun_control (Dict[str, Any]):\n            A dictionary containing information about the core model hyperparameters.\n        v (Dict[str, Any]):\n            A dictionary containing the numerical output of the hyperparameter optimization.\n        default (bool):\n            A boolean value indicating whether to use the default values from fun_control.\n\n    Returns:\n        Dict[str, Any]:\n            A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control.\n\n    Examples:\n        &gt;&gt;&gt; fun_control = {\n        ...     \"core_model_hyper_dict\": {\n        ...         \"leaf_prediction\": {\n        ...             \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        ...             \"type\": \"factor\",\n        ...             \"default\": \"mean\",\n        ...             \"core_model_parameter_type\": \"str\"\n        ...         },\n        ...         \"leaf_model\": {\n        ...             \"levels\": [\n        ...                 \"linear_model.LinearRegression\",\n        ...                 \"linear_model.PARegressor\",\n        ...                 \"linear_model.Perceptron\"\n        ...             ],\n        ...             \"type\": \"factor\",\n        ...             \"default\": \"LinearRegression\",\n        ...             \"core_model_parameter_type\": \"instance\"\n        ...         },\n        ...         \"splitter\": {\n        ...             \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n        ...             \"type\": \"factor\",\n        ...             \"default\": \"EBSTSplitter\",\n        ...             \"core_model_parameter_type\": \"instance()\"\n        ...         },\n        ...         \"binary_split\": {\n        ...             \"levels\": [0, 1],\n        ...             \"type\": \"factor\",\n        ...             \"default\": 0,\n        ...             \"core_model_parameter_type\": \"bool\"\n        ...         },\n        ...         \"stop_mem_management\": {\n        ...             \"levels\": [0, 1],\n        ...             \"type\": \"factor\",\n        ...             \"default\": 0,\n        ...             \"core_model_parameter_type\": \"bool\"\n        ...         }\n        ...     }\n        ... }\n        &gt;&gt;&gt; v = {\n        ...     'grace_period': 200,\n        ...     'max_depth': 10,\n        ...     'delta': 1e-07,\n        ...     'tau': 0.05,\n        ...     'leaf_prediction': 0,\n        ...     'leaf_model': 0,\n        ...     'model_selector_decay': 0.95,\n        ...     'splitter': 1,\n        ...     'min_samples_split': 9,\n        ...     'binary_split': 0,\n        ...     'max_size': 500.0\n        ... }\n        &gt;&gt;&gt; get_dict_with_levels_and_types(fun_control, v)\n        {\n            'grace_period': 200,\n            'max_depth': 10,\n            'delta': 1e-07,\n            'tau': 0.05,\n            'leaf_prediction': 'mean',\n            'leaf_model': linear_model.LinearRegression,\n            'model_selector_decay': 0.95,\n            'splitter': TEBSTSplitter,\n            'min_samples_split': 9,\n            'binary_split': False,\n            'max_size': 500.0\n        }\n    \"\"\"\n    if default:\n        d = fun_control[\"core_model_hyper_dict_default\"]\n    else:\n        d = fun_control[\"core_model_hyper_dict\"]\n    new_dict = {}\n    for key, value in v.items():\n        if key in d and d[key][\"type\"] == \"factor\":\n            if d[key][\"core_model_parameter_type\"] == \"instance\":\n                if \"class_name\" in d[key]:\n                    mdl = d[key][\"class_name\"]\n                c = d[key][\"levels\"][value]\n                new_dict[key] = class_for_name(mdl, c)\n            elif d[key][\"core_model_parameter_type\"] == \"instance()\":\n                mdl = d[key][\"class_name\"]\n                c = d[key][\"levels\"][value]\n                k = class_for_name(mdl, c)\n                new_dict[key] = k()\n            else:\n                new_dict[key] = d[key][\"levels\"][value]\n        else:\n            new_dict[key] = v[key]\n    return new_dict\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_ith_hyperparameter_name_from_fun_control","title":"<code>get_ith_hyperparameter_name_from_fun_control(fun_control, key, i)</code>","text":"<p>Get the ith hyperparameter name from the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>key</code> <code>str</code> <p>key</p> required <code>i</code> <code>int</code> <p>index</p> required <p>Returns:</p> Type Description <code>str</code> <p>hyperparameter name</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.device import getDevice\n    from spotpython.utils.init import fun_control_init\n    from spotpython.utils.file import get_experiment_name\n    import numpy as np\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.hyperparameters.values import set_control_hyperparameter_value\n    experiment_name = get_experiment_name(prefix=\"000\")\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        TENSORBOARD_CLEAN=True,\n        device=getDevice(),\n        enable_progress_bar=False,\n        fun_evals=15,\n        log_level=10,\n        max_time=1,\n        num_workers=0,\n        show_progress=True,\n        tolerance_x=np.sqrt(np.spacing(1)),\n        )\n    dataset = Diabetes()\n    set_control_key_value(control_dict=fun_control,\n                            key=\"data_set\",\n                            value=dataset,\n                            replace=True)\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n</code></pre> <pre><code>set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\nget_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0)\nAdam\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_ith_hyperparameter_name_from_fun_control(fun_control, key, i):\n    \"\"\"\n    Get the ith hyperparameter name from the fun_control dictionary.\n\n    Args:\n        fun_control (dict): fun_control dictionary\n        key (str): key\n        i (int): index\n\n    Returns:\n        (str): hyperparameter name\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.device import getDevice\n            from spotpython.utils.init import fun_control_init\n            from spotpython.utils.file import get_experiment_name\n            import numpy as np\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.hyperparameters.values import set_control_hyperparameter_value\n            experiment_name = get_experiment_name(prefix=\"000\")\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                TENSORBOARD_CLEAN=True,\n                device=getDevice(),\n                enable_progress_bar=False,\n                fun_evals=15,\n                log_level=10,\n                max_time=1,\n                num_workers=0,\n                show_progress=True,\n                tolerance_x=np.sqrt(np.spacing(1)),\n                )\n            dataset = Diabetes()\n            set_control_key_value(control_dict=fun_control,\n                                    key=\"data_set\",\n                                    value=dataset,\n                                    replace=True)\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n\n            set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n            set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n            get_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0)\n            Adam\n\n    \"\"\"\n    if \"core_model_hyper_dict\" in fun_control:\n        if key in fun_control[\"core_model_hyper_dict\"]:\n            if \"levels\" in fun_control[\"core_model_hyper_dict\"][key]:\n                if i &lt; len(fun_control[\"core_model_hyper_dict\"][key][\"levels\"]):\n                    return fun_control[\"core_model_hyper_dict\"][key][\"levels\"][i]\n    return None\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_one_config_from_X","title":"<code>get_one_config_from_X(X, fun_control=None)</code>","text":"<p>Get one config from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The config dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_config_from_X(X, fun_control)\n    {'leaf_prediction': 'mean',\n    'leaf_model': 'NBAdaptive',\n    'splitter': 'HoeffdingAdaptiveTreeSplitter',\n    'binary_split': 'info_gain',\n    'stop_mem_management': False}\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_one_config_from_X(X, fun_control=None):\n    \"\"\"Get one config from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (dict):\n            The config dictionary.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_config_from_X(X, fun_control)\n            {'leaf_prediction': 'mean',\n            'leaf_model': 'NBAdaptive',\n            'splitter': 'HoeffdingAdaptiveTreeSplitter',\n            'binary_split': 'info_gain',\n            'stop_mem_management': False}\n    \"\"\"\n    var_dict = assign_values(X, fun_control[\"var_name\"])\n    config = return_conf_list_from_var_dict(var_dict, fun_control)[0]\n    return config\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_one_core_model_from_X","title":"<code>get_one_core_model_from_X(X, fun_control=None, default=False)</code>","text":"<p>Get one core model from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <code>default</code> <code>bool</code> <p>A boolean value indicating whether to use the default values from fun_control.</p> <code>False</code> <p>Returns:</p> Type Description <code>class</code> <p>The core model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=fun_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_core_model_from_X(X, fun_control)\n    HoeffdingAdaptiveTreeRegressor()\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_one_core_model_from_X(\n    X,\n    fun_control=None,\n    default=False,\n):\n    \"\"\"Get one core model from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n        default (bool):\n            A boolean value indicating whether to use the default values from fun_control.\n\n    Returns:\n        (class):\n            The core model.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=fun_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_core_model_from_X(X, fun_control)\n            HoeffdingAdaptiveTreeRegressor()\n    \"\"\"\n    var_dict = assign_values(X, fun_control[\"var_name\"])\n    # var_dict = assign_values(X, get_var_name(fun_control))\n    config = return_conf_list_from_var_dict(var_dict, fun_control, default=default)[0]\n    core_model = fun_control[\"core_model\"](**config)\n    return core_model\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_one_river_model_from_X","title":"<code>get_one_river_model_from_X(X, fun_control=None)</code>","text":"<p>Get one river model from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>class</code> <p>The river model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_river_model_from_X(X, fun_control)\n    HoeffdingAdaptiveTreeRegressor()\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_one_river_model_from_X(X, fun_control=None):\n    \"\"\"Get one river model from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (class):\n            The river model.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_river_model_from_X(X, fun_control)\n            HoeffdingAdaptiveTreeRegressor()\n    \"\"\"\n    core_model = get_one_core_model_from_X(X=X, fun_control=fun_control)\n    if fun_control[\"prep_model\"] is not None:\n        model = compose.Pipeline(fun_control[\"prep_model\"], core_model)\n    else:\n        model = core_model\n    return model\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_one_sklearn_model_from_X","title":"<code>get_one_sklearn_model_from_X(X, fun_control=None)</code>","text":"<p>Get one sklearn model from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>class</code> <p>The sklearn model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n    from spotriver.data.sklearn_hyper_dict import SklearnHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=LinearRegression,\n        fun_control=func_control,\n        hyper_dict=SklearnHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_sklearn_model_from_X(X, fun_control)\n    LinearRegression()\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_one_sklearn_model_from_X(X, fun_control=None):\n    \"\"\"Get one sklearn model from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (class):\n            The sklearn model.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            from spotriver.data.sklearn_hyper_dict import SklearnHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=LinearRegression,\n                fun_control=func_control,\n                hyper_dict=SklearnHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_sklearn_model_from_X(X, fun_control)\n            LinearRegression()\n    \"\"\"\n    core_model = get_one_core_model_from_X(X=X, fun_control=fun_control)\n    if fun_control[\"prep_model\"] is not None:\n        model = make_pipeline(fun_control[\"prep_model\"], core_model)\n    else:\n        model = core_model\n    return model\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_transform","title":"<code>get_transform(fun_control)</code>","text":"<p>Get the transformations of the values from the dictionary fun_control as a list.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with levels and types</p> required <p>Returns:</p> Type Description <code>list</code> <p>list with transformations</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_transform\n    d = {\"core_model_hyper_dict\":{\n    \"leaf_prediction\": {\n        \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        \"type\": \"factor\",\n        \"default\": \"mean\",\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"str\"},\n    \"leaf_model\": {\n        \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n        \"type\": \"factor\",\n        \"default\": \"LinearRegression\",\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"instance\"},\n    \"splitter\": {\n        \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n        \"type\": \"factor\",\n        \"default\": \"EBSTSplitter\",\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"instance()\"},\n    \"binary_split\": {\n        \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"bool\"},\n    \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"bool\"}}}\n</code></pre> <pre><code>get_transform(d)\n['None', 'None', 'None', 'None', 'None']\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_transform(fun_control) -&gt; list:\n    \"\"\"Get the transformations of the values from the dictionary fun_control as a list.\n\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n\n    Returns:\n        (list):\n            list with transformations\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_transform\n            d = {\"core_model_hyper_dict\":{\n            \"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"str\"},\n            \"leaf_model\": {\n                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                \"type\": \"factor\",\n                \"default\": \"LinearRegression\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"instance\"},\n            \"splitter\": {\n                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                \"type\": \"factor\",\n                \"default\": \"EBSTSplitter\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"instance()\"},\n            \"binary_split\": {\n                \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"bool\"},\n            \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"bool\"}}}\n\n            get_transform(d)\n            ['None', 'None', 'None', 'None', 'None']\n    \"\"\"\n    return list(fun_control[\"core_model_hyper_dict\"][key][\"transform\"] for key in fun_control[\"core_model_hyper_dict\"].keys())\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_tuned_architecture","title":"<code>get_tuned_architecture(spot_tuner, fun_control, force_minX=False)</code>","text":"<p>Returns the tuned architecture. If the spot tuner has noise, it returns the architecture with the lowest mean (.min_mean_X), otherwise it returns the architecture with the lowest value (.min_X).</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <code>force_minX</code> <code>bool</code> <p>If True, return the architecture with the lowest value (.min_X).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the tuned architecture.</p> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_tuned_architecture(spot_tuner, fun_control, force_minX=False) -&gt; dict:\n    \"\"\"\n    Returns the tuned architecture. If the spot tuner has noise,\n    it returns the architecture with the lowest mean (.min_mean_X),\n    otherwise it returns the architecture with the lowest value (.min_X).\n\n    Args:\n        spot_tuner (object):\n            spot tuner object.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n        force_minX (bool):\n            If True, return the architecture with the lowest value (.min_X).\n\n    Returns:\n        (dict):\n            dictionary containing the tuned architecture.\n    \"\"\"\n    if not spot_tuner.noise or force_minX:\n        X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1, -1))\n    else:\n        # noise or force_minX is False:\n        X = spot_tuner.to_all_dim(spot_tuner.min_mean_X.reshape(1, -1))\n    config = get_one_config_from_X(X, fun_control)\n    return config\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_tuned_hyperparameters","title":"<code>get_tuned_hyperparameters(spot_tuner, fun_control=None)</code>","text":"<p>Get the tuned hyperparameters from the spot tuner. This is just a wrapper function for the spot <code>get_tuned_hyperparameters</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning. Optional. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the tuned hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.device import getDevice\n    from math import inf\n    from spotpython.utils.init import fun_control_init\n    import numpy as np\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperparameters.values import get_tuned_hyperparameters\n    MAX_TIME = 1\n    FUN_EVALS = 10\n    INIT_SIZE = 5\n    WORKERS = 0\n    PREFIX=\"037\"\n    DEVICE = getDevice()\n    DEVICES = 1\n    TEST_SIZE = 0.4\n    TORCH_METRIC = \"mean_squared_error\"\n    dataset = Diabetes()\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        _torchmetric=TORCH_METRIC,\n        PREFIX=PREFIX,\n        TENSORBOARD_CLEAN=True,\n        data_set=dataset,\n        device=DEVICE,\n        enable_progress_bar=False,\n        fun_evals=FUN_EVALS,\n        log_level=50,\n        max_time=MAX_TIME,\n        num_workers=WORKERS,\n        show_progress=True,\n        test_size=TEST_SIZE,\n        tolerance_x=np.sqrt(np.spacing(1)),\n        )\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                hyper_dict=LightHyperDict)\n    from spotpython.hyperparameters.values import set_control_hyperparameter_value\n    set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n    set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n    set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n    set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                    \"Adam\",\n                    \"RAdam\",\n                ])\n    set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n    set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n    set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n    set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                    \"ReLU\",\n                    \"LeakyReLU\"\n                ] )\n    from spotpython.utils.init import design_control_init, surrogate_control_init\n    design_control = design_control_init(init_size=INIT_SIZE)\n    surrogate_control = surrogate_control_init(noise=True,\n                                                n_theta=2)\n    from spotpython.fun.hyperlight import HyperLight\n    fun = HyperLight(log_level=50).fun\n    from spotpython.spot import spot\n    spot_tuner = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control)\n    spot_tuner.run()\n    get_tuned_hyperparameters(spot_tuner)\n        {'l1': 7.0,\n        'epochs': 5.0,\n        'batch_size': 4.0,\n        'act_fn': 0.0,\n        'optimizer': 0.0,\n        'dropout_prob': 0.01,\n        'lr_mult': 5.0,\n        'patience': 3.0,\n        'initialization': 1.0}\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_tuned_hyperparameters(spot_tuner, fun_control=None) -&gt; dict:\n    \"\"\"\n    Get the tuned hyperparameters from the spot tuner.\n    This is just a wrapper function for the spot `get_tuned_hyperparameters` method.\n\n    Args:\n        spot_tuner (object):\n            spot tuner object.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n            Optional. Default is None.\n\n    Returns:\n        (dict):\n            dictionary containing the tuned hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.device import getDevice\n            from math import inf\n            from spotpython.utils.init import fun_control_init\n            import numpy as np\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperparameters.values import get_tuned_hyperparameters\n            MAX_TIME = 1\n            FUN_EVALS = 10\n            INIT_SIZE = 5\n            WORKERS = 0\n            PREFIX=\"037\"\n            DEVICE = getDevice()\n            DEVICES = 1\n            TEST_SIZE = 0.4\n            TORCH_METRIC = \"mean_squared_error\"\n            dataset = Diabetes()\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                _torchmetric=TORCH_METRIC,\n                PREFIX=PREFIX,\n                TENSORBOARD_CLEAN=True,\n                data_set=dataset,\n                device=DEVICE,\n                enable_progress_bar=False,\n                fun_evals=FUN_EVALS,\n                log_level=50,\n                max_time=MAX_TIME,\n                num_workers=WORKERS,\n                show_progress=True,\n                test_size=TEST_SIZE,\n                tolerance_x=np.sqrt(np.spacing(1)),\n                )\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        hyper_dict=LightHyperDict)\n            from spotpython.hyperparameters.values import set_control_hyperparameter_value\n            set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n            set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n            set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n            set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                            \"Adam\",\n                            \"RAdam\",\n                        ])\n            set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n            set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n            set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n            set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                            \"ReLU\",\n                            \"LeakyReLU\"\n                        ] )\n            from spotpython.utils.init import design_control_init, surrogate_control_init\n            design_control = design_control_init(init_size=INIT_SIZE)\n            surrogate_control = surrogate_control_init(noise=True,\n                                                        n_theta=2)\n            from spotpython.fun.hyperlight import HyperLight\n            fun = HyperLight(log_level=50).fun\n            from spotpython.spot import spot\n            spot_tuner = spot.Spot(fun=fun,\n                                fun_control=fun_control,\n                                design_control=design_control,\n                                surrogate_control=surrogate_control)\n            spot_tuner.run()\n            get_tuned_hyperparameters(spot_tuner)\n                {'l1': 7.0,\n                'epochs': 5.0,\n                'batch_size': 4.0,\n                'act_fn': 0.0,\n                'optimizer': 0.0,\n                'dropout_prob': 0.01,\n                'lr_mult': 5.0,\n                'patience': 3.0,\n                'initialization': 1.0}\n    \"\"\"\n    return spot_tuner.get_tuned_hyperparameters(fun_control=fun_control)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_values_from_dict","title":"<code>get_values_from_dict(dictionary)</code>","text":"<p>Get the values from a dictionary as an array. Generate an np.array that contains the values of the keys of a dictionary in the same order as the keys of the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>dictionary with values</p> required <p>Returns:</p> Type Description <code>array</code> <p>array with values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_values_from_dict\n&gt;&gt;&gt; d = {\"a\": 1, \"b\": 2, \"c\": 3}\n&gt;&gt;&gt; get_values_from_dict(d)\narray([1, 2, 3])\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_values_from_dict(dictionary) -&gt; np.array:\n    \"\"\"Get the values from a dictionary as an array.\n    Generate an np.array that contains the values of the keys of a dictionary\n    in the same order as the keys of the dictionary.\n\n    Args:\n        dictionary (dict):\n            dictionary with values\n\n    Returns:\n        (np.array):\n            array with values\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_values_from_dict\n        &gt;&gt;&gt; d = {\"a\": 1, \"b\": 2, \"c\": 3}\n        &gt;&gt;&gt; get_values_from_dict(d)\n        array([1, 2, 3])\n    \"\"\"\n    return np.array(list(dictionary.values()))\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_var_name","title":"<code>get_var_name(fun_control)</code>","text":"<p>Get the names of the values from the dictionary fun_control as a list. If no \u201ccore_model_hyper_dict\u201d key exists in fun_control, return None.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with names</p> required <p>Returns:</p> Type Description <code>list</code> <p>ist with names</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_name\n    fun_control = {\"core_model_hyper_dict\":{\n                \"leaf_prediction\": {\n                    \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                    \"type\": \"factor\",\n                    \"default\": \"mean\",\n                    \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\",\n                                \"linear_model.PARegressor\",\n                                \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}}\n    get_var_name(fun_control)\n    ['leaf_prediction',\n        'leaf_model',\n        'splitter',\n        'binary_split',\n        'stop_mem_management']\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_var_name(fun_control) -&gt; list:\n    \"\"\"Get the names of the values from the dictionary fun_control as a list.\n    If no \"core_model_hyper_dict\" key exists in fun_control, return None.\n\n    Args:\n        fun_control (dict):\n            dictionary with names\n\n    Returns:\n        (list):\n            ist with names\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_name\n            fun_control = {\"core_model_hyper_dict\":{\n                        \"leaf_prediction\": {\n                            \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                            \"type\": \"factor\",\n                            \"default\": \"mean\",\n                            \"core_model_parameter_type\": \"str\"},\n                        \"leaf_model\": {\n                            \"levels\": [\"linear_model.LinearRegression\",\n                                        \"linear_model.PARegressor\",\n                                        \"linear_model.Perceptron\"],\n                            \"type\": \"factor\",\n                            \"default\": \"LinearRegression\",\n                            \"core_model_parameter_type\": \"instance\"},\n                        \"splitter\": {\n                            \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                            \"type\": \"factor\",\n                            \"default\": \"EBSTSplitter\",\n                            \"core_model_parameter_type\": \"instance()\"},\n                        \"binary_split\": {\n                            \"levels\": [0, 1],\n                            \"type\": \"factor\",\n                            \"default\": 0,\n                            \"core_model_parameter_type\": \"bool\"},\n                        \"stop_mem_management\": {\n                            \"levels\": [0, 1],\n                            \"type\": \"factor\",\n                            \"default\": 0,\n                            \"core_model_parameter_type\": \"bool\"}}}\n            get_var_name(fun_control)\n            ['leaf_prediction',\n                'leaf_model',\n                'splitter',\n                'binary_split',\n                'stop_mem_management']\n    \"\"\"\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        return list(fun_control[\"core_model_hyper_dict\"].keys())\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_var_type","title":"<code>get_var_type(fun_control)</code>","text":"<p>Get the types of the values from the dictionary fun_control as a list. If no \u201ccore_model_hyper_dict\u201d key exists in fun_control, return None.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with levels and types</p> required <p>Returns:</p> Type Description <code>list</code> <p>list with types</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_type\n    d = {\"core_model_hyper_dict\":{\n    \"leaf_prediction\": {\n        \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        \"type\": \"factor\",\n        \"default\": \"mean\",\n        \"core_model_parameter_type\": \"str\"},\n    \"leaf_model\": {\n        \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n        \"type\": \"factor\",\n        \"default\": \"LinearRegression\",\n        \"core_model_parameter_type\": \"instance\"},\n    \"splitter\": {\n        \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n        \"type\": \"factor\",\n        \"default\": \"EBSTSplitter\",\n        \"core_model_parameter_type\": \"instance()\"},\n    \"binary_split\": {\n        \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"core_model_parameter_type\": \"bool\"},\n    \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"core_model_parameter_type\": \"bool\"}}}\n</code></pre> <pre><code>get_var_type(d)\n['factor', 'factor', 'factor', 'factor', 'factor']\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_var_type(fun_control) -&gt; list:\n    \"\"\"\n    Get the types of the values from the dictionary fun_control as a list.\n    If no \"core_model_hyper_dict\" key exists in fun_control, return None.\n\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n\n    Returns:\n        (list):\n            list with types\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_type\n            d = {\"core_model_hyper_dict\":{\n            \"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"core_model_parameter_type\": \"str\"},\n            \"leaf_model\": {\n                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                \"type\": \"factor\",\n                \"default\": \"LinearRegression\",\n                \"core_model_parameter_type\": \"instance\"},\n            \"splitter\": {\n                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                \"type\": \"factor\",\n                \"default\": \"EBSTSplitter\",\n                \"core_model_parameter_type\": \"instance()\"},\n            \"binary_split\": {\n                \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"core_model_parameter_type\": \"bool\"},\n            \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"core_model_parameter_type\": \"bool\"}}}\n\n            get_var_type(d)\n            ['factor', 'factor', 'factor', 'factor', 'factor']\n    \"\"\"\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        return list(fun_control[\"core_model_hyper_dict\"][key][\"type\"] for key in fun_control[\"core_model_hyper_dict\"].keys())\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.get_var_type_from_var_name","title":"<code>get_var_type_from_var_name(fun_control, var_name)</code>","text":"<p>This function gets the variable type from the variable name.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>var_name</code> <code>str</code> <p>variable name</p> required <p>Returns:</p> Type Description <code>str</code> <p>variable type</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import get_var_type_from_var_name\n    control_dict = fun_control_init()\n    get_var_type_from_var_name(var_name=\"max_depth\",\n                    fun_control=control_dict)\n    \"int\"\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def get_var_type_from_var_name(fun_control, var_name) -&gt; str:\n    \"\"\"\n    This function gets the variable type from the variable name.\n\n    Args:\n        fun_control (dict): fun_control dictionary\n        var_name (str): variable name\n\n    Returns:\n        (str): variable type\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import get_var_type_from_var_name\n            control_dict = fun_control_init()\n            get_var_type_from_var_name(var_name=\"max_depth\",\n                            fun_control=control_dict)\n            \"int\"\n    \"\"\"\n    var_type_list = get_control_key_value(control_dict=fun_control, key=\"var_type\")\n    var_name_list = get_control_key_value(control_dict=fun_control, key=\"var_name\")\n    return var_type_list[var_name_list.index(var_name)]\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.iterate_dict_values","title":"<code>iterate_dict_values(var_dict)</code>","text":"<p>Iterate over the values of a dictionary of variables. This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <p>Returns:</p> Type Description <code>Generator[Dict[str, Union[int, float]], None, None]</code> <p>Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.hyperparameters.values import iterate_dict_values\n&gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n&gt;&gt;&gt; list(iterate_dict_values(var_dict))\n[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def iterate_dict_values(var_dict: Dict[str, np.ndarray]) -&gt; Generator[Dict[str, Union[int, float]], None, None]:\n    \"\"\"Iterate over the values of a dictionary of variables.\n    This function takes a dictionary of variables as input arguments and returns a generator that\n    yields dictionaries with the values from the arrays in the input dictionary.\n\n    Args:\n        var_dict (dict): A dictionary where keys are variable names and values are numpy arrays.\n\n    Returns:\n        Generator[dict]:\n            A generator that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import iterate_dict_values\n        &gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n        &gt;&gt;&gt; list(iterate_dict_values(var_dict))\n        [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n    \"\"\"\n    n = len(next(iter(var_dict.values())))\n    for i in range(n):\n        yield {key: value[i] for key, value in var_dict.items()}\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.modify_boolean_hyper_parameter_levels","title":"<code>modify_boolean_hyper_parameter_levels(fun_control, hyperparameter, levels)</code>","text":"<p>This function modifies the levels of a boolean hyperparameter in the fun_control dictionary. It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>levels</code> <code>list</code> <p>list of levels</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def modify_boolean_hyper_parameter_levels(fun_control, hyperparameter, levels) -&gt; None:\n    \"\"\"\n    This function modifies the levels of a boolean hyperparameter in the fun_control dictionary.\n    It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        levels (list):\n            list of levels\n\n    Returns:\n        None.\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"levels\": levels})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": levels[0]})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": levels[1]})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.modify_hyper_parameter_bounds","title":"<code>modify_hyper_parameter_bounds(fun_control, hyperparameter, bounds)</code>","text":"<p>Modify the bounds of a hyperparameter in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>bounds</code> <code>list</code> <p>list of two bound values. The first value represents the lower bound and the second value represents the upper bound.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n    fun_control = {}\n    core_model  = HoeffdingTreeRegressor\n    fun_control.update({\"core_model\": core_model})\n    fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n    bounds = [3, 11]\n    fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds)\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def modify_hyper_parameter_bounds(fun_control, hyperparameter, bounds) -&gt; None:\n    \"\"\"\n    Modify the bounds of a hyperparameter in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        bounds (list):\n            list of two bound values. The first value represents the lower bound\n            and the second value represents the upper bound.\n\n    Returns:\n        None.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n            fun_control = {}\n            core_model  = HoeffdingTreeRegressor\n            fun_control.update({\"core_model\": core_model})\n            fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n            bounds = [3, 11]\n            fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds)\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": bounds[0]})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": bounds[1]})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.modify_hyper_parameter_levels","title":"<code>modify_hyper_parameter_levels(fun_control, hyperparameter, levels)</code>","text":"<p>This function modifies the levels of a hyperparameter in the fun_control dictionary. It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>levels</code> <code>list</code> <p>list of levels</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fun_control = {}\n    from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n    core_model  = HoeffdingTreeRegressor\n    fun_control.update({\"core_model\": core_model})\n    fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n    levels = [\"mean\", \"model\"]\n    fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels)\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def modify_hyper_parameter_levels(fun_control, hyperparameter, levels) -&gt; None:\n    \"\"\"\n    This function modifies the levels of a hyperparameter in the fun_control dictionary.\n    It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        levels (list):\n            list of levels\n\n    Returns:\n        None.\n\n    Examples:\n        &gt;&gt;&gt; fun_control = {}\n            from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n            core_model  = HoeffdingTreeRegressor\n            fun_control.update({\"core_model\": core_model})\n            fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n            levels = [\"mean\", \"model\"]\n            fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels)\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"levels\": levels})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": 0})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": len(levels) - 1})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.replace_levels_with_positions","title":"<code>replace_levels_with_positions(hyper_dict, hyper_dict_values)</code>","text":"<p>Replace the levels with the position in the levels list. The function that takes two dictionaries. The first contains as hyperparameters as keys. If the hyperparameter has the key \u201clevels\u201d, then the value of the corresponding hyperparameter in the second dictionary is replaced by the position of the value in the list of levels. The function returns a dictionary with the same keys as the second dictionary. For example, if the second dictionary is {\u201ca\u201d: 1, \u201cb\u201d: \u201cmodel1\u201d, \u201cc\u201d: 3} and the first dictionary is {     \u201ca\u201d: {\u201ctype\u201d: \u201cint\u201d},     \u201cb\u201d: {\u201clevels\u201d: [\u201cmodel4\u201d, \u201cmodel5\u201d, \u201cmodel1\u201d]},     \u201cd\u201d: {\u201ctype\u201d: \u201cfloat\u201d}}, then the function should return {\u201ca\u201d: 1, \u201cb\u201d: 2, \u201cc\u201d: 3}.</p> <p>Parameters:</p> Name Type Description Default <code>hyper_dict</code> <code>dict</code> <p>dictionary with levels</p> required <code>hyper_dict_values</code> <code>dict</code> <p>dictionary with values</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dictionary with values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import replace_levels_with_positions\n    hyper_dict = {\"leaf_prediction\": {\n        \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        \"type\": \"factor\",\n        \"default\": \"mean\",\n        \"core_model_parameter_type\": \"str\"},\n        \"leaf_model\": {\n            \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n            \"type\": \"factor\",\n            \"default\": \"LinearRegression\",\n            \"core_model_parameter_type\": \"instance\"},\n        \"splitter\": {\n            \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n            \"type\": \"factor\",\n            \"default\": \"EBSTSplitter\",\n            \"core_model_parameter_type\": \"instance()\"},\n        \"binary_split\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"},\n        \"stop_mem_management\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"}}\n    hyper_dict_values = {\"leaf_prediction\": \"mean\",\n        \"leaf_model\": \"linear_model.LinearRegression\",\n        \"splitter\": \"EBSTSplitter\",\n        \"binary_split\": 0,\n        \"stop_mem_management\": 0}\n    replace_levels_with_position(hyper_dict, hyper_dict_values)\n        {'leaf_prediction': 0,\n        'leaf_model': 0,\n        'splitter': 0,\n        'binary_split': 0,\n        'stop_mem_management': 0}\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def replace_levels_with_positions(hyper_dict, hyper_dict_values) -&gt; dict:\n    \"\"\"Replace the levels with the position in the levels list.\n    The function that takes two dictionaries.\n    The first contains as hyperparameters as keys.\n    If the hyperparameter has the key \"levels\",\n    then the value of the corresponding hyperparameter in the second dictionary is\n    replaced by the position of the value in the list of levels.\n    The function returns a dictionary with the same keys as the second dictionary.\n    For example, if the second dictionary is {\"a\": 1, \"b\": \"model1\", \"c\": 3}\n    and the first dictionary is {\n        \"a\": {\"type\": \"int\"},\n        \"b\": {\"levels\": [\"model4\", \"model5\", \"model1\"]},\n        \"d\": {\"type\": \"float\"}},\n    then the function should return {\"a\": 1, \"b\": 2, \"c\": 3}.\n\n    Args:\n        hyper_dict (dict):\n            dictionary with levels\n        hyper_dict_values (dict):\n            dictionary with values\n\n    Returns:\n        (dict):\n            dictionary with values\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import replace_levels_with_positions\n            hyper_dict = {\"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}\n            hyper_dict_values = {\"leaf_prediction\": \"mean\",\n                \"leaf_model\": \"linear_model.LinearRegression\",\n                \"splitter\": \"EBSTSplitter\",\n                \"binary_split\": 0,\n                \"stop_mem_management\": 0}\n            replace_levels_with_position(hyper_dict, hyper_dict_values)\n                {'leaf_prediction': 0,\n                'leaf_model': 0,\n                'splitter': 0,\n                'binary_split': 0,\n                'stop_mem_management': 0}\n    \"\"\"\n    hyper_dict_values_new = copy.deepcopy(hyper_dict_values)\n    # generate an error if the following code fails and write an error message:\n    try:\n        for key, value in hyper_dict_values.items():\n            if key in hyper_dict.keys():\n                if \"levels\" in hyper_dict[key].keys():\n                    hyper_dict_values_new[key] = hyper_dict[key][\"levels\"].index(value)\n    except Exception as e:\n        print(\"!!! Warning: \", e)\n        print(\"Did you modify lower and upper bounds so that the default values are not included?\")\n        print(\"Returning 'None'.\")\n        return None\n    return hyper_dict_values_new\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.return_conf_list_from_var_dict","title":"<code>return_conf_list_from_var_dict(var_dict, fun_control, default=False)</code>","text":"<p>Return a list of configurations from a dictionary of variables.</p> <p>This function takes a dictionary of variables and a dictionary of function control as input arguments. It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries of hyper parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, Union[int, float]]]</code> <p>A list of dictionaries of hyper parameter values. Transformations are applied to the values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.hyperparameters.values import return_conf_list_from_var_dict\n&gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n&gt;&gt;&gt; fun_control = {'var_type': ['int', 'int']}\n&gt;&gt;&gt; return_conf_list_from_var_dict(var_dict, fun_control)\n[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def return_conf_list_from_var_dict(\n    var_dict: Dict[str, np.ndarray],\n    fun_control: Dict[str, Union[List[str], str]],\n    default: bool = False,\n) -&gt; List[Dict[str, Union[int, float]]]:\n    \"\"\"Return a list of configurations from a dictionary of variables.\n\n    This function takes a dictionary of variables and a dictionary of function control as input arguments.\n    It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries\n    of hyper parameter values.\n\n    Args:\n        var_dict (dict): A dictionary where keys are variable names and values are numpy arrays.\n        fun_control (dict): A dictionary which (at least) has an entry with the following key:\n            \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding\n            value will be converted to the type \"int\".\n\n    Returns:\n        list: A list of dictionaries of hyper parameter values. Transformations are applied to the values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import return_conf_list_from_var_dict\n        &gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n        &gt;&gt;&gt; fun_control = {'var_type': ['int', 'int']}\n        &gt;&gt;&gt; return_conf_list_from_var_dict(var_dict, fun_control)\n        [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n    \"\"\"\n    conf_list = []\n    for values in generate_one_config_from_var_dict(var_dict, fun_control, default=default):\n        conf_list.append(values)\n    return conf_list\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.set_control_hyperparameter_value","title":"<code>set_control_hyperparameter_value(control_dict, hyperparameter, value)</code>","text":"<p>This function sets the hyperparameter values depending on the var_type via modify_hyperameter_levels or modify_hyperparameter_bounds in the control_dict dictionary. If the hyperparameter is a factor, it calls modify_hyper_parameter_levels. Otherwise, it calls modify_hyper_parameter_bounds.</p> <p>Parameters:</p> Name Type Description Default <code>control_dict</code> <code>dict</code> <p>control_dict dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>key</p> required <code>value</code> <code>Any</code> <p>value</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def set_control_hyperparameter_value(control_dict, hyperparameter, value) -&gt; None:\n    \"\"\"\n    This function sets the hyperparameter values depending on the var_type\n    via modify_hyperameter_levels or modify_hyperparameter_bounds in the control_dict dictionary.\n    If the hyperparameter is a factor, it calls modify_hyper_parameter_levels.\n    Otherwise, it calls modify_hyper_parameter_bounds.\n\n    Args:\n        control_dict (dict):\n            control_dict dictionary\n        hyperparameter (str): key\n        value (Any): value\n\n    Returns:\n        None.\n\n    \"\"\"\n    print(f\"Setting hyperparameter {hyperparameter} to value {value}.\")\n    vt = get_var_type_from_var_name(fun_control=control_dict, var_name=hyperparameter)\n    print(f\"Variable type is {vt}.\")\n    core_type = get_core_model_parameter_type_from_var_name(fun_control=control_dict, var_name=hyperparameter)\n    print(f\"Core type is {core_type}.\")\n    if vt == \"factor\" and core_type != \"bool\":\n        print(\"Calling modify_hyper_parameter_levels().\")\n        modify_hyper_parameter_levels(fun_control=control_dict, hyperparameter=hyperparameter, levels=value)\n    elif vt == \"factor\" and core_type == \"bool\":\n        print(\"Calling modify_boolean_hyper_parameter_levels().\")\n        modify_boolean_hyper_parameter_levels(fun_control=control_dict, hyperparameter=hyperparameter, levels=value)\n    else:\n        print(\"Calling modify_hyper_parameter_bounds().\")\n        modify_hyper_parameter_bounds(fun_control=control_dict, hyperparameter=hyperparameter, bounds=value)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.set_control_key_value","title":"<code>set_control_key_value(control_dict, key, value, replace=False)</code>","text":"<p>This function sets the key value pair in the control_dict dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>control_dict</code> <code>dict</code> <p>control_dict dictionary</p> required <code>key</code> <code>str</code> <p>key</p> required <code>value</code> <code>Any</code> <p>value</p> required <code>replace</code> <code>bool</code> <p>replace value if key already exists. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>key</p> <code>value</code> <code>Any</code> <p>value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_control_key_value\n    control_dict = fun_control_init()\n    set_control_key_value(control_dict=control_dict,\n                  key=\"key\",\n                  value=\"value\")\n    control_dict[\"key\"]\n</code></pre> Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def set_control_key_value(control_dict, key, value, replace=False) -&gt; None:\n    \"\"\"\n    This function sets the key value pair in the control_dict dictionary.\n\n    Args:\n        control_dict (dict):\n            control_dict dictionary\n        key (str): key\n        value (Any): value\n        replace (bool): replace value if key already exists. Default is False.\n\n    Returns:\n        None.\n\n    Attributes:\n        key (str): key\n        value (Any): value\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_control_key_value\n            control_dict = fun_control_init()\n            set_control_key_value(control_dict=control_dict,\n                          key=\"key\",\n                          value=\"value\")\n            control_dict[\"key\"]\n\n    \"\"\"\n    if replace:\n        control_dict.update({key: value})\n    else:\n        if key not in control_dict.keys():\n            control_dict.update({key: value})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/update/#spotpython.hyperparameters.update.update_fun_control_with_hyper_num_cat_dicts","title":"<code>update_fun_control_with_hyper_num_cat_dicts(fun_control, num_dict, cat_dict, dict)</code>","text":"<p>Update an existing fun_control dictionary with new hyperparameter values. All values from the hyperparameter dict (dict) are updated in the fun_control dictionary using the num_dict and cat_dict dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary. This dictionary is updated with the new hyperparameter values.</p> required <code>num_dict</code> <code>dict</code> <p>The dictionary containing the numerical hyperparameter values, which are used to update the fun_control dictionary.</p> required <code>cat_dict</code> <code>dict</code> <p>The dictionary containing the categorical hyperparameter values, which are used to update the fun_control dictionary.</p> required <code>dict</code> <code>dict</code> <p>The dictionary containing the \u201cold\u201d hyperparameter values.</p> required Source code in <code>spotpython/hyperparameters/update.py</code> <pre><code>def update_fun_control_with_hyper_num_cat_dicts(fun_control, num_dict, cat_dict, dict):\n    \"\"\"\n    Update an existing fun_control dictionary with new hyperparameter values.\n    All values from the hyperparameter dict (dict) are updated in the fun_control dictionary\n    using the num_dict and cat_dict dictionaries.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary. This dictionary is updated with the new hyperparameter values.\n        num_dict (dict):\n            The dictionary containing the numerical hyperparameter values, which\n            are used to update the fun_control dictionary.\n        cat_dict (dict):\n            The dictionary containing the categorical hyperparameter values, which\n            are used to update the fun_control dictionary.\n        dict (dict):\n            The dictionary containing the \"old\" hyperparameter values.\n    \"\"\"\n    for i, (key, value) in enumerate(dict.items()):\n        if dict[key][\"type\"] == \"int\":\n            set_control_hyperparameter_value(\n                fun_control,\n                key,\n                [\n                    int(num_dict[key][\"lower\"]),\n                    int(num_dict[key][\"upper\"]),\n                ],\n            )\n        if (dict[key][\"type\"] == \"factor\") and (dict[key][\"core_model_parameter_type\"] == \"bool\"):\n            set_control_hyperparameter_value(\n                fun_control,\n                key,\n                [\n                    int(num_dict[key][\"lower\"]),\n                    int(num_dict[key][\"upper\"]),\n                ],\n            )\n        if dict[key][\"type\"] == \"float\":\n            set_control_hyperparameter_value(\n                fun_control,\n                key,\n                [\n                    float(num_dict[key][\"lower\"]),\n                    float(num_dict[key][\"upper\"]),\n                ],\n            )\n        if dict[key][\"type\"] == \"factor\" and dict[key][\"core_model_parameter_type\"] != \"bool\":\n            fle = cat_dict[key][\"levels\"]\n            # convert the string to a list of strings\n            fle = fle.split()\n            set_control_hyperparameter_value(fun_control, key, fle)\n            fun_control[\"core_model_hyper_dict\"][key].update({\"upper\": len(fle) - 1})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/","title":"values","text":""},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.add_core_model_to_fun_control","title":"<code>add_core_model_to_fun_control(fun_control, core_model, hyper_dict=None, filename=None)</code>","text":"<p>Add the core model to the function control dictionary. It updates the keys \u201ccore_model\u201d, \u201ccore_model_hyper_dict\u201d, \u201cvar_type\u201d, \u201cvar_name\u201d in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>core_model</code> <code>class</code> <p>The core model.</p> required <code>hyper_dict</code> <code>dict</code> <p>The hyper parameter dictionary. Optional. Default is None. If no hyper_dict is provided, the function will try to load the hyper_dict from the file specified by filename.</p> <code>None</code> <code>filename</code> <code>str</code> <p>The name of the json file that contains the hyper parameter dictionary. Optional. Default is None. If no filename is provided, the function will try to load the hyper_dict from the hyper_dict dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated fun_control dictionary.</p> Notes <p>The function adds the following keys to the fun_control dictionary: \u201ccore_model\u201d: The core model. \u201ccore_model_hyper_dict\u201d: The hyper parameter dictionary for the core model. \u201ccore_model_hyper_dict_default\u201d: The hyper parameter dictionary for the core model. \u201cvar_type\u201d: A list of variable types. \u201cvar_name\u201d: A list of variable names. The original hyperparameters of the core model are stored in the \u201ccore_model_hyper_dict_default\u201d key. These remain unmodified, while the \u201ccore_model_hyper_dict\u201d key is modified during the tuning process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                hyper_dict=LightHyperDict)\n    # or, if a user wants to use a custom hyper_dict:\n&gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                filename=\"./hyperdict/user_hyper_dict.json\")\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def add_core_model_to_fun_control(fun_control, core_model, hyper_dict=None, filename=None) -&gt; dict:\n    \"\"\"Add the core model to the function control dictionary. It updates the keys \"core_model\",\n    \"core_model_hyper_dict\", \"var_type\", \"var_name\" in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        core_model (class):\n            The core model.\n        hyper_dict (dict):\n            The hyper parameter dictionary. Optional. Default is None. If no hyper_dict is provided,\n            the function will try to load the hyper_dict from the file specified by filename.\n        filename (str):\n            The name of the json file that contains the hyper parameter dictionary.\n            Optional. Default is None. If no filename is provided, the function will try to load the\n            hyper_dict from the hyper_dict dictionary.\n\n    Returns:\n        (dict):\n            The updated fun_control dictionary.\n\n    Notes:\n        The function adds the following keys to the fun_control dictionary:\n        \"core_model\": The core model.\n        \"core_model_hyper_dict\": The hyper parameter dictionary for the core model.\n        \"core_model_hyper_dict_default\": The hyper parameter dictionary for the core model.\n        \"var_type\": A list of variable types.\n        \"var_name\": A list of variable names.\n        The original hyperparameters of the core model are stored in the \"core_model_hyper_dict_default\" key.\n        These remain unmodified, while the \"core_model_hyper_dict\" key is modified during the tuning process.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        hyper_dict=LightHyperDict)\n            # or, if a user wants to use a custom hyper_dict:\n        &gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        filename=\"./hyperdict/user_hyper_dict.json\")\n\n    \"\"\"\n    fun_control.update({\"core_model\": core_model})\n    if filename is None:\n        new_hyper_dict = hyper_dict().load()\n    else:\n        with open(filename, \"r\") as f:\n            new_hyper_dict = json.load(f)\n    fun_control.update({\"core_model_hyper_dict\": new_hyper_dict[core_model.__name__]})\n    fun_control.update({\"core_model_hyper_dict_default\": copy.deepcopy(new_hyper_dict[core_model.__name__])})\n    var_type = get_var_type(fun_control)\n    var_name = get_var_name(fun_control)\n    lower = get_bound_values(fun_control, \"lower\", as_list=False)\n    upper = get_bound_values(fun_control, \"upper\", as_list=False)\n    fun_control.update({\"var_type\": var_type, \"var_name\": var_name, \"lower\": lower, \"upper\": upper})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.assign_values","title":"<code>assign_values(X, var_list)</code>","text":"<p>This function takes an np.array X and a list of variable names as input arguments and returns a dictionary where the keys are the variable names and the values are assigned from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>A 2D numpy array where each column represents a variable.</p> required <code>var_list</code> <code>list</code> <p>A list of strings representing variable names.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are assigned from X.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.hyperparameters.values import assign_values\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; var_list = ['a', 'b']\n&gt;&gt;&gt; result = assign_values(X, var_list)\n&gt;&gt;&gt; print(result)\n{'a': array([1, 3, 5]), 'b': array([2, 4, 6])}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def assign_values(X: np.array, var_list: list) -&gt; dict:\n    \"\"\"\n    This function takes an np.array X and a list of variable names as input arguments\n    and returns a dictionary where the keys are the variable names and the values are assigned from X.\n\n    Args:\n        X (np.array):\n            A 2D numpy array where each column represents a variable.\n        var_list (list):\n            A list of strings representing variable names.\n\n    Returns:\n        dict:\n            A dictionary where keys are variable names and values are assigned from X.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import assign_values\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; var_list = ['a', 'b']\n        &gt;&gt;&gt; result = assign_values(X, var_list)\n        &gt;&gt;&gt; print(result)\n        {'a': array([1, 3, 5]), 'b': array([2, 4, 6])}\n    \"\"\"\n    result = {}\n    for i, var_name in enumerate(var_list):\n        result[var_name] = X[:, i]\n    return result\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.convert_keys","title":"<code>convert_keys(d, var_type)</code>","text":"<p>Convert values in a dictionary to integers based on a list of variable types. This function takes a dictionary <code>d</code> and a list of variable types <code>var_type</code> as arguments. For each key in the dictionary, if the corresponding entry in <code>var_type</code> is not equal to <code>\"num\"</code>, the value associated with that key is converted to an integer.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>The input dictionary.</p> required <code>var_type</code> <code>list</code> <p>A list of variable types. If the entry is not <code>\"num\"</code> the corresponding value will be converted to the type <code>\"int\"</code>.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Union[int, float]]</code> <p>The modified dictionary with values converted to integers based on <code>var_type</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import convert_keys\n&gt;&gt;&gt; d = {'a': '1.1', 'b': '2', 'c': '3.1'}\n&gt;&gt;&gt; var_type = [\"int\", \"num\", \"int\"]\n&gt;&gt;&gt; convert_keys(d, var_type)\n{'a': 1, 'b': '2', 'c': 3}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def convert_keys(d: Dict[str, Union[int, float, str]], var_type: List[str]) -&gt; Dict[str, Union[int, float]]:\n    \"\"\"Convert values in a dictionary to integers based on a list of variable types.\n    This function takes a dictionary `d` and a list of variable types `var_type` as arguments.\n    For each key in the dictionary,\n    if the corresponding entry in `var_type` is not equal to `\"num\"`,\n    the value associated with that key is converted to an integer.\n\n    Args:\n        d (dict): The input dictionary.\n        var_type (list):\n            A list of variable types. If the entry is not `\"num\"` the corresponding\n            value will be converted to the type `\"int\"`.\n\n    Returns:\n        dict: The modified dictionary with values converted to integers based on `var_type`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import convert_keys\n        &gt;&gt;&gt; d = {'a': '1.1', 'b': '2', 'c': '3.1'}\n        &gt;&gt;&gt; var_type = [\"int\", \"num\", \"int\"]\n        &gt;&gt;&gt; convert_keys(d, var_type)\n        {'a': 1, 'b': '2', 'c': 3}\n    \"\"\"\n    keys = list(d.keys())\n    for i in range(len(keys)):\n        if var_type[i] not in [\"num\", \"float\"]:\n            d[keys[i]] = int(d[keys[i]])\n    return d\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.create_model","title":"<code>create_model(config, fun_control, **kwargs)</code>","text":"<p>Creates a model for the given configuration and control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>dictionary containing the configuration for the hyperparameter tuning.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <code>**kwargs</code> <code>Any</code> <p>additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>object</code> <p>model object.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def create_model(config, fun_control, **kwargs) -&gt; object:\n    \"\"\"\n    Creates a model for the given configuration and control parameters.\n\n    Args:\n        config (dict):\n            dictionary containing the configuration for the hyperparameter tuning.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n        **kwargs (Any):\n            additional keyword arguments.\n\n    Returns:\n        (object):\n            model object.\n    \"\"\"\n    return fun_control[\"core_model\"](**config, **kwargs)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.generate_one_config_from_var_dict","title":"<code>generate_one_config_from_var_dict(var_dict, fun_control, default=False)</code>","text":"<p>Generate one configuration from a dictionary of variables (as a generator).</p> <p>This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d.</p> required <code>default</code> <code>bool</code> <p>A boolean value indicating whether to use the default values from fun_control.</p> <code>False</code> <p>Returns:</p> Type Description <code>Generator[Dict[str, Union[int, float]], None, None]</code> <p>Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.hyperparameters.values import generate_one_config_from_var_dict\n&gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n&gt;&gt;&gt; fun_control = {\"var_type\": [\"int\", \"num\"]}\n&gt;&gt;&gt; list(generate_one_config_from_var_dict(var_dict, fun_control))\n[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def generate_one_config_from_var_dict(\n    var_dict: Dict[str, np.ndarray],\n    fun_control: Dict[str, Union[List[str], str]],\n    default: bool = False,\n) -&gt; Generator[Dict[str, Union[int, float]], None, None]:\n    \"\"\"Generate one configuration from a dictionary of variables (as a generator).\n\n    This function takes a dictionary of variables as input arguments and returns a generator\n    that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Args:\n        var_dict (dict):\n            A dictionary where keys are variable names and values are numpy arrays.\n        fun_control (dict):\n            A dictionary which (at least) has an entry with the following key:\n            \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding\n            value will be converted to the type \"int\".\n        default (bool):\n            A boolean value indicating whether to use the default values from fun_control.\n\n    Returns:\n        Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import generate_one_config_from_var_dict\n        &gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n        &gt;&gt;&gt; fun_control = {\"var_type\": [\"int\", \"num\"]}\n        &gt;&gt;&gt; list(generate_one_config_from_var_dict(var_dict, fun_control))\n        [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n    \"\"\"\n    for values in iterate_dict_values(var_dict):\n        values = convert_keys(values, fun_control[\"var_type\"])\n        values = get_dict_with_levels_and_types(fun_control=fun_control, v=values, default=default)\n        values = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values)\n        yield values\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_bound_values","title":"<code>get_bound_values(fun_control, bound, as_list=False)</code>","text":"<p>Generate a list or array from a dictionary. This function takes the values from the keys \u201cbound\u201d in the fun_control[\u201ccore_model_hyper_dict\u201d] dictionary and returns a list or array of the values in the same order as the keys in the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>A dictionary containing a key \u201ccore_model_hyper_dict\u201d which is a dictionary with keys that have either an \u201cupper\u201d or \u201clower\u201d value.</p> required <code>bound</code> <code>str</code> <p>Either \u201cupper\u201d or \u201clower\u201d, indicating which value to extract from the inner dictionary.</p> required <code>as_list</code> <code>bool</code> <p>If True, return a list. If False, return a numpy array. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List, ndarray]</code> <p>list or np.ndarray: A list or array of the extracted values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If bound is not \u201cupper\u201d or \u201clower\u201d.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_bound_values\n&gt;&gt;&gt; fun_control = {\"core_model_hyper_dict\": {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}}}\n&gt;&gt;&gt; get_bound_values(fun_control, \"upper\", as_list=True)\n[1, 2]\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_bound_values(fun_control: dict, bound: str, as_list: bool = False) -&gt; Union[List, np.ndarray]:\n    \"\"\"Generate a list or array from a dictionary.\n    This function takes the values from the keys \"bound\" in the\n    fun_control[\"core_model_hyper_dict\"] dictionary and returns a list or array of the values\n    in the same order as the keys in the dictionary.\n\n    Args:\n        fun_control (dict):\n            A dictionary containing a key \"core_model_hyper_dict\"\n            which is a dictionary with keys that have either an \"upper\" or \"lower\" value.\n        bound (str):\n            Either \"upper\" or \"lower\",\n            indicating which value to extract from the inner dictionary.\n        as_list (bool):\n            If True, return a list.\n            If False, return a numpy array. Default is False.\n\n    Returns:\n        list or np.ndarray:\n            A list or array of the extracted values.\n\n    Raises:\n        ValueError:\n            If bound is not \"upper\" or \"lower\".\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_bound_values\n        &gt;&gt;&gt; fun_control = {\"core_model_hyper_dict\": {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}}}\n        &gt;&gt;&gt; get_bound_values(fun_control, \"upper\", as_list=True)\n        [1, 2]\n    \"\"\"\n    # Throw value error if bound is not upper or lower:\n    if bound not in [\"upper\", \"lower\"]:\n        raise ValueError(\"bound must be either 'upper' or 'lower'\")\n    # check if key \"core_model_hyper_dict\" exists in fun_control:\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        d = fun_control[\"core_model_hyper_dict\"]\n        b = []\n        for key, value in d.items():\n            b.append(value[bound])\n        if as_list:\n            return b\n        else:\n            return np.array(b)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_control_key_value","title":"<code>get_control_key_value(control_dict=None, key=None)</code>","text":"<p>This function gets the key value pair from the control_dict dictionary. If the key does not exist, return None. If the control_dict dictionary is None, return None.</p> <p>Parameters:</p> Name Type Description Default <code>control_dict</code> <code>dict</code> <p>control_dict dictionary</p> <code>None</code> <code>key</code> <code>str</code> <p>key</p> <code>None</code> <p>Returns:</p> Name Type Description <code>value</code> <code>Any</code> <p>value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import get_control_key_value\n    control_dict = fun_control_init()\n    get_control_key_value(control_dict=control_dict,\n                    key=\"key\")\n    \"value\"\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_control_key_value(control_dict=None, key=None) -&gt; Any:\n    \"\"\"\n    This function gets the key value pair from the control_dict dictionary.\n    If the key does not exist, return None.\n    If the control_dict dictionary is None, return None.\n\n    Args:\n        control_dict (dict):\n            control_dict dictionary\n        key (str): key\n\n    Returns:\n        value (Any):\n            value\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import get_control_key_value\n            control_dict = fun_control_init()\n            get_control_key_value(control_dict=control_dict,\n                            key=\"key\")\n            \"value\"\n    \"\"\"\n    if control_dict is None:\n        return None\n    else:\n        # check if key \"core_model_hyper_dict\" exists in fun_control:\n        if \"core_model_hyper_dict\" in control_dict.keys():\n            if key == \"lower\":\n                lower = get_bound_values(fun_control=control_dict, bound=\"lower\")\n                return lower\n            if key == \"upper\":\n                upper = get_bound_values(fun_control=control_dict, bound=\"upper\")\n                return upper\n            if key == \"var_name\":\n                var_name = get_var_name(fun_control=control_dict)\n                return var_name\n            if key == \"var_type\":\n                var_type = get_var_type(fun_control=control_dict)\n                return var_type\n            if key == \"transform\":\n                transform = get_transform(fun_control=control_dict)\n                return transform\n        # check if key exists in control_dict:\n        elif control_dict is None or key not in control_dict.keys():\n            return None\n        else:\n            return control_dict[key]\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_core_model_from_name","title":"<code>get_core_model_from_name(core_model_name)</code>","text":"<p>Returns the sklearn or spotpython lightning core model name and instance from a core model name.</p> <p>Parameters:</p> Name Type Description Default <code>core_model_name</code> <code>str</code> <p>The full name of the core model in the format \u2018module.Model\u2019.</p> required <p>Returns:</p> Type Description <code>(str, object)</code> <p>A tuple containing the core model name and an instance of the core model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model_name, model_instance = get_core_model_from_name(\"light.regression.NNLinearRegressor\")\n    print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")\n        Model Name:\n        NNLinearRegressor,\n        Model Instance:\n        &lt;class 'spotpython.light.regression.nn_linear_regressor.NNLinearRegressor'&gt;\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_core_model_from_name(core_model_name: str) -&gt; tuple:\n    \"\"\"\n    Returns the sklearn or spotpython lightning core model name and instance from a core model name.\n\n    Args:\n        core_model_name (str): The full name of the core model in the format 'module.Model'.\n\n    Returns:\n        (str, object): A tuple containing the core model name and an instance of the core model.\n\n    Examples:\n        &gt;&gt;&gt; model_name, model_instance = get_core_model_from_name(\"light.regression.NNLinearRegressor\")\n            print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")\n                Model Name:\n                NNLinearRegressor,\n                Model Instance:\n                &lt;class 'spotpython.light.regression.nn_linear_regressor.NNLinearRegressor'&gt;\n    \"\"\"\n    # Split the model name into its components\n    name_parts = core_model_name.split(\".\")\n    if len(name_parts) &lt; 2:\n        raise ValueError(f\"Invalid core model name: {core_model_name}. Expected format: 'module.ModelName'.\")\n    module_name = name_parts[0]\n    model_name = name_parts[1]\n    try:\n        # Try to get the model from the sklearn library\n        core_model_instance = getattr(getattr(sklearn, module_name), model_name)\n        return model_name, core_model_instance\n    except AttributeError:\n        try:\n            # Try to get the model from the spotpython library\n            submodule_name = name_parts[1]\n            model_name = name_parts[2] if len(name_parts) == 3 else model_name\n            print(f\"module_name: {module_name}\")\n            print(f\"submodule_name: {submodule_name}\")\n            print(f\"model_name: {model_name}\")\n            core_model_instance = getattr(getattr(getattr(spotpython, module_name), submodule_name), model_name)\n            return model_name, core_model_instance\n        except AttributeError:\n            raise ValueError(f\"Model '{core_model_name}' not found in either 'sklearn' or 'spotpython lightning' libraries.\")\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_core_model_parameter_type_from_var_name","title":"<code>get_core_model_parameter_type_from_var_name(fun_control, var_name)</code>","text":"<p>Extracts the core_model_parameter_type value from a dictionary for a specified key.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The dictionary containing the information.</p> required <code>var_name</code> <code>str</code> <p>The key for which to extract the core_model_parameter_type value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The core_model_parameter_type value if available, else None.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_core_model_parameter_type_from_var_name(fun_control, var_name) -&gt; str:\n    \"\"\"\n    Extracts the core_model_parameter_type value from a dictionary for a specified key.\n\n    Args:\n        fun_control (dict):\n            The dictionary containing the information.\n        var_name (str):\n            The key for which to extract the core_model_parameter_type value.\n\n    Returns:\n        (str):\n            The core_model_parameter_type value if available, else None.\n    \"\"\"\n    # Check if the key exists in the dictionary and it has a 'core_model_parameter_type' entry\n    if var_name in fun_control[\"core_model_hyper_dict\"] and \"core_model_parameter_type\" in fun_control[\"core_model_hyper_dict\"][var_name]:\n        return fun_control[\"core_model_hyper_dict\"][var_name][\"core_model_parameter_type\"]\n    else:\n        return None\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_default_hyperparameters_as_array","title":"<code>get_default_hyperparameters_as_array(fun_control)</code>","text":"<p>Get the default hyper parameters as array.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The default hyper parameters as array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    from spotpython.hyperparameters.values import (\n        get_default_hyperparameters_as_array,\n        add_core_model_to_fun_control)\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    get_default_hyperparameters_as_array(fun_control)\n    array([0, 0, 0, 0, 0])\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_default_hyperparameters_as_array(fun_control) -&gt; np.array:\n    \"\"\"Get the default hyper parameters as array.\n\n    Args:\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (np.array):\n            The default hyper parameters as array.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            from spotpython.hyperparameters.values import (\n                get_default_hyperparameters_as_array,\n                add_core_model_to_fun_control)\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            get_default_hyperparameters_as_array(fun_control)\n            array([0, 0, 0, 0, 0])\n    \"\"\"\n    X0 = get_default_values(fun_control)\n    X0 = replace_levels_with_positions(fun_control[\"core_model_hyper_dict_default\"], X0)\n    if X0 is None:\n        return None\n    else:\n        X0 = get_values_from_dict(X0)\n        X0 = np.array([X0])\n        X0.shape[1]\n        return X0\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_default_values","title":"<code>get_default_values(fun_control)</code>","text":"<p>Get the values from the \u201cdefault\u201d keys from the dictionary fun_control as a dict. If the key of the value has as \u201ctype\u201d the value \u201cint\u201d or \u201cfloat\u201d, convert the value to the corresponding type.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with levels and types</p> required <p>Returns:</p> Name Type Description <code>new_dict</code> <code>dict</code> <p>dictionary with default values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n    d = {\"core_model_hyper_dict\":{\n        \"leaf_prediction\": {\n            \"levels\": [\"mean\", \"model\", \"adaptive\"],\n            \"type\": \"factor\",\n            \"default\": \"mean\",\n            \"core_model_parameter_type\": \"str\"},\n        \"leaf_model\": {\n            \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n            \"type\": \"factor\",\n            \"default\": \"LinearRegression\",\n            \"core_model_parameter_type\": \"instance\"},\n        \"splitter\": {\n            \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n            \"type\": \"factor\",\n            \"default\": \"EBSTSplitter\",\n            \"core_model_parameter_type\": \"instance()\"},\n        \"binary_split\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"},\n        \"stop_mem_management\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"}}}\n    get_default_values(d)\n    {'leaf_prediction': 'mean',\n    'leaf_model': 'linear_model.LinearRegression',\n    'splitter': 'EBSTSplitter',\n    'binary_split': 0,\n    'stop_mem_management': 0}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_default_values(fun_control) -&gt; dict:\n    \"\"\"Get the values from the \"default\" keys from the dictionary fun_control as a dict.\n    If the key of the value has as \"type\" the value \"int\" or \"float\", convert the value to the corresponding type.\n\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n\n    Returns:\n        new_dict (dict):\n            dictionary with default values\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n            d = {\"core_model_hyper_dict\":{\n                \"leaf_prediction\": {\n                    \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                    \"type\": \"factor\",\n                    \"default\": \"mean\",\n                    \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}}\n            get_default_values(d)\n            {'leaf_prediction': 'mean',\n            'leaf_model': 'linear_model.LinearRegression',\n            'splitter': 'EBSTSplitter',\n            'binary_split': 0,\n            'stop_mem_management': 0}\n    \"\"\"\n    d = fun_control[\"core_model_hyper_dict_default\"]\n    new_dict = {}\n    for key, value in d.items():\n        if value[\"type\"] == \"int\":\n            new_dict[key] = int(value[\"default\"])\n        elif value[\"type\"] == \"float\":\n            new_dict[key] = float(value[\"default\"])\n        else:\n            new_dict[key] = value[\"default\"]\n    return new_dict\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_dict_with_levels_and_types","title":"<code>get_dict_with_levels_and_types(fun_control, v, default=False)</code>","text":"<p>Get dictionary with levels and types. The function maps the numerical output of the hyperparameter optimization to the corresponding levels of the hyperparameter needed by the core model, i.e., the tuned algorithm. The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. If the key value in the dictionary is 0, it takes the first value from the list, if it is 1, it takes the second and so on. If a key is not in fun_control, it takes the key from v. If the core_model_parameter_type value is instance, it returns the class of the value from the module via getattr(\u201cclass\u201d, value).</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>Dict[str, Any]</code> <p>A dictionary containing information about the core model hyperparameters.</p> required <code>v</code> <code>Dict[str, Any]</code> <p>A dictionary containing the numerical output of the hyperparameter optimization.</p> required <code>default</code> <code>bool</code> <p>A boolean value indicating whether to use the default values from fun_control.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fun_control = {\n...     \"core_model_hyper_dict\": {\n...         \"leaf_prediction\": {\n...             \"levels\": [\"mean\", \"model\", \"adaptive\"],\n...             \"type\": \"factor\",\n...             \"default\": \"mean\",\n...             \"core_model_parameter_type\": \"str\"\n...         },\n...         \"leaf_model\": {\n...             \"levels\": [\n...                 \"linear_model.LinearRegression\",\n...                 \"linear_model.PARegressor\",\n...                 \"linear_model.Perceptron\"\n...             ],\n...             \"type\": \"factor\",\n...             \"default\": \"LinearRegression\",\n...             \"core_model_parameter_type\": \"instance\"\n...         },\n...         \"splitter\": {\n...             \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n...             \"type\": \"factor\",\n...             \"default\": \"EBSTSplitter\",\n...             \"core_model_parameter_type\": \"instance()\"\n...         },\n...         \"binary_split\": {\n...             \"levels\": [0, 1],\n...             \"type\": \"factor\",\n...             \"default\": 0,\n...             \"core_model_parameter_type\": \"bool\"\n...         },\n...         \"stop_mem_management\": {\n...             \"levels\": [0, 1],\n...             \"type\": \"factor\",\n...             \"default\": 0,\n...             \"core_model_parameter_type\": \"bool\"\n...         }\n...     }\n... }\n&gt;&gt;&gt; v = {\n...     'grace_period': 200,\n...     'max_depth': 10,\n...     'delta': 1e-07,\n...     'tau': 0.05,\n...     'leaf_prediction': 0,\n...     'leaf_model': 0,\n...     'model_selector_decay': 0.95,\n...     'splitter': 1,\n...     'min_samples_split': 9,\n...     'binary_split': 0,\n...     'max_size': 500.0\n... }\n&gt;&gt;&gt; get_dict_with_levels_and_types(fun_control, v)\n{\n    'grace_period': 200,\n    'max_depth': 10,\n    'delta': 1e-07,\n    'tau': 0.05,\n    'leaf_prediction': 'mean',\n    'leaf_model': linear_model.LinearRegression,\n    'model_selector_decay': 0.95,\n    'splitter': TEBSTSplitter,\n    'min_samples_split': 9,\n    'binary_split': False,\n    'max_size': 500.0\n}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_dict_with_levels_and_types(fun_control: Dict[str, Any], v: Dict[str, Any], default=False) -&gt; Dict[str, Any]:\n    \"\"\"Get dictionary with levels and types.\n    The function maps the numerical output of the hyperparameter optimization to the corresponding levels\n    of the hyperparameter needed by the core model, i.e., the tuned algorithm.\n    The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v\n    but with the values of the levels of the keys from fun_control.\n    If the key value in the dictionary is 0, it takes the first value from the list,\n    if it is 1, it takes the second and so on.\n    If a key is not in fun_control, it takes the key from v.\n    If the core_model_parameter_type value is instance, it returns the class of the value from the module\n    via getattr(\"class\", value).\n\n    Args:\n        fun_control (Dict[str, Any]):\n            A dictionary containing information about the core model hyperparameters.\n        v (Dict[str, Any]):\n            A dictionary containing the numerical output of the hyperparameter optimization.\n        default (bool):\n            A boolean value indicating whether to use the default values from fun_control.\n\n    Returns:\n        Dict[str, Any]:\n            A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control.\n\n    Examples:\n        &gt;&gt;&gt; fun_control = {\n        ...     \"core_model_hyper_dict\": {\n        ...         \"leaf_prediction\": {\n        ...             \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        ...             \"type\": \"factor\",\n        ...             \"default\": \"mean\",\n        ...             \"core_model_parameter_type\": \"str\"\n        ...         },\n        ...         \"leaf_model\": {\n        ...             \"levels\": [\n        ...                 \"linear_model.LinearRegression\",\n        ...                 \"linear_model.PARegressor\",\n        ...                 \"linear_model.Perceptron\"\n        ...             ],\n        ...             \"type\": \"factor\",\n        ...             \"default\": \"LinearRegression\",\n        ...             \"core_model_parameter_type\": \"instance\"\n        ...         },\n        ...         \"splitter\": {\n        ...             \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n        ...             \"type\": \"factor\",\n        ...             \"default\": \"EBSTSplitter\",\n        ...             \"core_model_parameter_type\": \"instance()\"\n        ...         },\n        ...         \"binary_split\": {\n        ...             \"levels\": [0, 1],\n        ...             \"type\": \"factor\",\n        ...             \"default\": 0,\n        ...             \"core_model_parameter_type\": \"bool\"\n        ...         },\n        ...         \"stop_mem_management\": {\n        ...             \"levels\": [0, 1],\n        ...             \"type\": \"factor\",\n        ...             \"default\": 0,\n        ...             \"core_model_parameter_type\": \"bool\"\n        ...         }\n        ...     }\n        ... }\n        &gt;&gt;&gt; v = {\n        ...     'grace_period': 200,\n        ...     'max_depth': 10,\n        ...     'delta': 1e-07,\n        ...     'tau': 0.05,\n        ...     'leaf_prediction': 0,\n        ...     'leaf_model': 0,\n        ...     'model_selector_decay': 0.95,\n        ...     'splitter': 1,\n        ...     'min_samples_split': 9,\n        ...     'binary_split': 0,\n        ...     'max_size': 500.0\n        ... }\n        &gt;&gt;&gt; get_dict_with_levels_and_types(fun_control, v)\n        {\n            'grace_period': 200,\n            'max_depth': 10,\n            'delta': 1e-07,\n            'tau': 0.05,\n            'leaf_prediction': 'mean',\n            'leaf_model': linear_model.LinearRegression,\n            'model_selector_decay': 0.95,\n            'splitter': TEBSTSplitter,\n            'min_samples_split': 9,\n            'binary_split': False,\n            'max_size': 500.0\n        }\n    \"\"\"\n    if default:\n        d = fun_control[\"core_model_hyper_dict_default\"]\n    else:\n        d = fun_control[\"core_model_hyper_dict\"]\n    new_dict = {}\n    for key, value in v.items():\n        if key in d and d[key][\"type\"] == \"factor\":\n            if d[key][\"core_model_parameter_type\"] == \"instance\":\n                if \"class_name\" in d[key]:\n                    mdl = d[key][\"class_name\"]\n                c = d[key][\"levels\"][value]\n                new_dict[key] = class_for_name(mdl, c)\n            elif d[key][\"core_model_parameter_type\"] == \"instance()\":\n                mdl = d[key][\"class_name\"]\n                c = d[key][\"levels\"][value]\n                k = class_for_name(mdl, c)\n                new_dict[key] = k()\n            # bool() introduced to convert 0 and 1 to False and True in v0.14.54\n            elif d[key][\"core_model_parameter_type\"] == \"bool\":\n                new_dict[key] = bool(d[key][\"levels\"][value])\n            else:\n                new_dict[key] = d[key][\"levels\"][value]\n        else:\n            new_dict[key] = v[key]\n    return new_dict\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_ith_hyperparameter_name_from_fun_control","title":"<code>get_ith_hyperparameter_name_from_fun_control(fun_control, key, i)</code>","text":"<p>Get the ith hyperparameter name from the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>key</code> <code>str</code> <p>key</p> required <code>i</code> <code>int</code> <p>index</p> required <p>Returns:</p> Type Description <code>str</code> <p>hyperparameter name</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.device import getDevice\n    from spotpython.utils.init import fun_control_init\n    from spotpython.utils.file import get_experiment_name\n    import numpy as np\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.hyperparameters.values import set_control_hyperparameter_value\n    experiment_name = get_experiment_name(prefix=\"000\")\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        TENSORBOARD_CLEAN=True,\n        device=getDevice(),\n        enable_progress_bar=False,\n        fun_evals=15,\n        log_level=10,\n        max_time=1,\n        num_workers=0,\n        show_progress=True,\n        tolerance_x=np.sqrt(np.spacing(1)),\n        )\n    dataset = Diabetes()\n    set_control_key_value(control_dict=fun_control,\n                            key=\"data_set\",\n                            value=dataset,\n                            replace=True)\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n</code></pre> <pre><code>set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\nget_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0)\nAdam\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_ith_hyperparameter_name_from_fun_control(fun_control, key, i):\n    \"\"\"\n    Get the ith hyperparameter name from the fun_control dictionary.\n\n    Args:\n        fun_control (dict): fun_control dictionary\n        key (str): key\n        i (int): index\n\n    Returns:\n        (str): hyperparameter name\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.device import getDevice\n            from spotpython.utils.init import fun_control_init\n            from spotpython.utils.file import get_experiment_name\n            import numpy as np\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.hyperparameters.values import set_control_hyperparameter_value\n            experiment_name = get_experiment_name(prefix=\"000\")\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                TENSORBOARD_CLEAN=True,\n                device=getDevice(),\n                enable_progress_bar=False,\n                fun_evals=15,\n                log_level=10,\n                max_time=1,\n                num_workers=0,\n                show_progress=True,\n                tolerance_x=np.sqrt(np.spacing(1)),\n                )\n            dataset = Diabetes()\n            set_control_key_value(control_dict=fun_control,\n                                    key=\"data_set\",\n                                    value=dataset,\n                                    replace=True)\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n\n            set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n            set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n            get_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0)\n            Adam\n\n    \"\"\"\n    if \"core_model_hyper_dict\" in fun_control:\n        if key in fun_control[\"core_model_hyper_dict\"]:\n            if \"levels\" in fun_control[\"core_model_hyper_dict\"][key]:\n                if i &lt; len(fun_control[\"core_model_hyper_dict\"][key][\"levels\"]):\n                    return fun_control[\"core_model_hyper_dict\"][key][\"levels\"][i]\n    return None\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_metric_sklearn","title":"<code>get_metric_sklearn(metric_name)</code>","text":"<p>Returns the sklearn metric from the metric name.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The name of the metric.</p> required <p>Returns:</p> Type Description <code>object</code> <p>sklearn.metrics (object): The sklearn metric.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_metric_sklearn(metric_name) -&gt; object:\n    \"\"\"\n    Returns the sklearn metric from the metric name.\n\n    Args:\n        metric_name (str): The name of the metric.\n\n    Returns:\n        sklearn.metrics (object): The sklearn metric.\n    \"\"\"\n    metric_sklearn = getattr(sklearn.metrics, metric_name)\n    return metric_sklearn\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_one_config_from_X","title":"<code>get_one_config_from_X(X, fun_control=None)</code>","text":"<p>Get one config from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The config dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_config_from_X(X, fun_control)\n    {'leaf_prediction': 'mean',\n    'leaf_model': 'NBAdaptive',\n    'splitter': 'HoeffdingAdaptiveTreeSplitter',\n    'binary_split': 'info_gain',\n    'stop_mem_management': False}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_one_config_from_X(X, fun_control=None):\n    \"\"\"Get one config from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (dict):\n            The config dictionary.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_config_from_X(X, fun_control)\n            {'leaf_prediction': 'mean',\n            'leaf_model': 'NBAdaptive',\n            'splitter': 'HoeffdingAdaptiveTreeSplitter',\n            'binary_split': 'info_gain',\n            'stop_mem_management': False}\n    \"\"\"\n    var_dict = assign_values(X, fun_control[\"var_name\"])\n    config = return_conf_list_from_var_dict(var_dict, fun_control)[0]\n    return config\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_one_core_model_from_X","title":"<code>get_one_core_model_from_X(X, fun_control=None, default=False)</code>","text":"<p>Get one core model from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <code>default</code> <code>bool</code> <p>A boolean value indicating whether to use the default values from fun_control.</p> <code>False</code> <p>Returns:</p> Type Description <code>class</code> <p>The core model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=fun_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_core_model_from_X(X, fun_control)\n    HoeffdingAdaptiveTreeRegressor()\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_one_core_model_from_X(\n    X,\n    fun_control=None,\n    default=False,\n):\n    \"\"\"Get one core model from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n        default (bool):\n            A boolean value indicating whether to use the default values from fun_control.\n\n    Returns:\n        (class):\n            The core model.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=fun_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_core_model_from_X(X, fun_control)\n            HoeffdingAdaptiveTreeRegressor()\n    \"\"\"\n    var_dict = assign_values(X, fun_control[\"var_name\"])\n    # var_dict = assign_values(X, get_var_name(fun_control))\n    config = return_conf_list_from_var_dict(var_dict, fun_control, default=default)[0]\n    core_model = fun_control[\"core_model\"](**config)\n    return core_model\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_one_river_model_from_X","title":"<code>get_one_river_model_from_X(X, fun_control=None)</code>","text":"<p>Get one river model from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>class</code> <p>The river model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_river_model_from_X(X, fun_control)\n    HoeffdingAdaptiveTreeRegressor()\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_one_river_model_from_X(X, fun_control=None):\n    \"\"\"Get one river model from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (class):\n            The river model.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_river_model_from_X(X, fun_control)\n            HoeffdingAdaptiveTreeRegressor()\n    \"\"\"\n    core_model = get_one_core_model_from_X(X=X, fun_control=fun_control)\n    if fun_control[\"prep_model\"] is not None:\n        model = compose.Pipeline(fun_control[\"prep_model\"], core_model)\n    else:\n        model = core_model\n    return model\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_one_sklearn_model_from_X","title":"<code>get_one_sklearn_model_from_X(X, fun_control=None)</code>","text":"<p>Get one sklearn model from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>class</code> <p>The sklearn model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n    from spotriver.data.sklearn_hyper_dict import SklearnHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=LinearRegression,\n        fun_control=func_control,\n        hyper_dict=SklearnHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_sklearn_model_from_X(X, fun_control)\n    LinearRegression()\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_one_sklearn_model_from_X(X, fun_control=None):\n    \"\"\"Get one sklearn model from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (class):\n            The sklearn model.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            from spotriver.data.sklearn_hyper_dict import SklearnHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=LinearRegression,\n                fun_control=func_control,\n                hyper_dict=SklearnHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_sklearn_model_from_X(X, fun_control)\n            LinearRegression()\n    \"\"\"\n    core_model = get_one_core_model_from_X(X=X, fun_control=fun_control)\n    if fun_control[\"prep_model\"] is not None:\n        model = make_pipeline(fun_control[\"prep_model\"], core_model)\n    else:\n        model = core_model\n    return model\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_prep_model","title":"<code>get_prep_model(prepmodel_name)</code>","text":"<p>Get the sklearn preprocessing model from the name.</p> <p>Parameters:</p> Name Type Description Default <code>prepmodel_name</code> <code>str</code> <p>The name of the preprocessing model.</p> required <p>Returns:</p> Type Description <code>object</code> <p>river.preprocessing (object): The river preprocessing model.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_prep_model(prepmodel_name) -&gt; object:\n    \"\"\"\n    Get the sklearn preprocessing model from the name.\n\n    Args:\n        prepmodel_name (str): The name of the preprocessing model.\n\n    Returns:\n        river.preprocessing (object): The river preprocessing model.\n\n    \"\"\"\n    if prepmodel_name == \"None\":\n        prepmodel = None\n    else:\n        prepmodel = getattr(sklearn.preprocessing, prepmodel_name)\n    return prepmodel\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_river_core_model_from_name","title":"<code>get_river_core_model_from_name(core_model_name)</code>","text":"<p>Returns the river core model name and instance from a core model name.</p> <p>Parameters:</p> Name Type Description Default <code>core_model_name</code> <code>str</code> <p>The full name of the core model in the format \u2018module.Model\u2019.</p> required <p>Returns:</p> Type Description <code>(str, object)</code> <p>A tuple containing the core model name and an instance of the core model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_core_model_from_name\n    model_name, model_instance = get_core_model_from_name('tree.HoeffdingTreeRegressor')\n    print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")\n        Model Name:\n        HoeffdingTreeRegressor,\n        Model Instance:\n        &lt;class 'river.tree.hoeffding_tree_regressor.HoeffdingTreeRegressor'&gt;\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_river_core_model_from_name(core_model_name: str) -&gt; tuple:\n    \"\"\"\n    Returns the river core model name and instance from a core model name.\n\n    Args:\n        core_model_name (str): The full name of the core model in the format 'module.Model'.\n\n    Returns:\n        (str, object): A tuple containing the core model name and an instance of the core model.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_core_model_from_name\n            model_name, model_instance = get_core_model_from_name('tree.HoeffdingTreeRegressor')\n            print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")\n                Model Name:\n                HoeffdingTreeRegressor,\n                Model Instance:\n                &lt;class 'river.tree.hoeffding_tree_regressor.HoeffdingTreeRegressor'&gt;\n    \"\"\"\n    # Split the model name into its components\n    name_parts = core_model_name.split(\".\")\n    if len(name_parts) &lt; 2:\n        raise ValueError(f\"Invalid core model name: {core_model_name}. Expected format: 'module.ModelName'.\")\n    module_name = name_parts[0]\n    model_name = name_parts[1]\n    try:\n        # Try to get the model from the river library\n        core_model_instance = getattr(getattr(river, module_name), model_name)\n        return model_name, core_model_instance\n    except AttributeError:\n        raise ValueError(f\"Model '{core_model_name}' not found in either 'river' libraries.\")\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_river_prep_model","title":"<code>get_river_prep_model(prepmodel_name)</code>","text":"<p>Get the river preprocessing model from the name.</p> <p>Parameters:</p> Name Type Description Default <code>prepmodel_name</code> <code>str</code> <p>The name of the preprocessing model.</p> required <p>Returns:</p> Type Description <code>object</code> <p>river.preprocessing (object): The river preprocessing model.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_river_prep_model(prepmodel_name) -&gt; object:\n    \"\"\"\n    Get the river preprocessing model from the name.\n\n    Args:\n        prepmodel_name (str): The name of the preprocessing model.\n\n    Returns:\n        river.preprocessing (object): The river preprocessing model.\n\n    \"\"\"\n    if prepmodel_name == \"None\":\n        prepmodel = None\n    else:\n        prepmodel = getattr(river.preprocessing, prepmodel_name)\n    return prepmodel\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_sklearn_scaler","title":"<code>get_sklearn_scaler(scaler_name)</code>","text":"<p>Get the sklearn scaler model from the name.</p> <p>Parameters:</p> Name Type Description Default <code>scaler_name</code> <code>str</code> <p>The name of the preprocessing model.</p> required <p>Returns:</p> Type Description <code>object</code> <p>sklearn.preprocessing (object): The sklearn scaler.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_sklearn_scaler(scaler_name) -&gt; object:\n    \"\"\"\n    Get the sklearn scaler model from the name.\n\n    Args:\n        scaler_name (str): The name of the preprocessing model.\n\n    Returns:\n        sklearn.preprocessing (object): The sklearn scaler.\n\n    \"\"\"\n    if scaler_name == \"None\":\n        scaler = None\n    else:\n        scaler = getattr(sklearn.preprocessing, scaler_name)\n    return scaler\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_transform","title":"<code>get_transform(fun_control)</code>","text":"<p>Get the transformations of the values from the dictionary fun_control as a list.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with levels and types</p> required <p>Returns:</p> Type Description <code>list</code> <p>list with transformations</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_transform\n    d = {\"core_model_hyper_dict\":{\n    \"leaf_prediction\": {\n        \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        \"type\": \"factor\",\n        \"default\": \"mean\",\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"str\"},\n    \"leaf_model\": {\n        \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n        \"type\": \"factor\",\n        \"default\": \"LinearRegression\",\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"instance\"},\n    \"splitter\": {\n        \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n        \"type\": \"factor\",\n        \"default\": \"EBSTSplitter\",\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"instance()\"},\n    \"binary_split\": {\n        \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"bool\"},\n    \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"bool\"}}}\n</code></pre> <pre><code>get_transform(d)\n['None', 'None', 'None', 'None', 'None']\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_transform(fun_control) -&gt; list:\n    \"\"\"Get the transformations of the values from the dictionary fun_control as a list.\n\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n\n    Returns:\n        (list):\n            list with transformations\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_transform\n            d = {\"core_model_hyper_dict\":{\n            \"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"str\"},\n            \"leaf_model\": {\n                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                \"type\": \"factor\",\n                \"default\": \"LinearRegression\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"instance\"},\n            \"splitter\": {\n                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                \"type\": \"factor\",\n                \"default\": \"EBSTSplitter\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"instance()\"},\n            \"binary_split\": {\n                \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"bool\"},\n            \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"bool\"}}}\n\n            get_transform(d)\n            ['None', 'None', 'None', 'None', 'None']\n    \"\"\"\n    return list(fun_control[\"core_model_hyper_dict\"][key][\"transform\"] for key in fun_control[\"core_model_hyper_dict\"].keys())\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_tuned_architecture","title":"<code>get_tuned_architecture(spot_tuner, fun_control, force_minX=False)</code>","text":"<p>Returns the tuned architecture. If the spot tuner has noise, it returns the architecture with the lowest mean (.min_mean_X), otherwise it returns the architecture with the lowest value (.min_X).</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <code>force_minX</code> <code>bool</code> <p>If True, return the architecture with the lowest value (.min_X).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the tuned architecture.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_tuned_architecture(spot_tuner, fun_control, force_minX=False) -&gt; dict:\n    \"\"\"\n    Returns the tuned architecture. If the spot tuner has noise,\n    it returns the architecture with the lowest mean (.min_mean_X),\n    otherwise it returns the architecture with the lowest value (.min_X).\n\n    Args:\n        spot_tuner (object):\n            spot tuner object.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n        force_minX (bool):\n            If True, return the architecture with the lowest value (.min_X).\n\n    Returns:\n        (dict):\n            dictionary containing the tuned architecture.\n    \"\"\"\n    if not spot_tuner.noise or force_minX:\n        X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1, -1))\n    else:\n        # noise or force_minX is False:\n        X = spot_tuner.to_all_dim(spot_tuner.min_mean_X.reshape(1, -1))\n    config = get_one_config_from_X(X, fun_control)\n    return config\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_tuned_hyperparameters","title":"<code>get_tuned_hyperparameters(spot_tuner, fun_control=None)</code>","text":"<p>Get the tuned hyperparameters from the spot tuner. This is just a wrapper function for the spot <code>get_tuned_hyperparameters</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning. Optional. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the tuned hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.device import getDevice\n    from math import inf\n    from spotpython.utils.init import fun_control_init\n    import numpy as np\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperparameters.values import get_tuned_hyperparameters\n    MAX_TIME = 1\n    FUN_EVALS = 10\n    INIT_SIZE = 5\n    WORKERS = 0\n    PREFIX=\"037\"\n    DEVICE = getDevice()\n    DEVICES = 1\n    TEST_SIZE = 0.4\n    TORCH_METRIC = \"mean_squared_error\"\n    dataset = Diabetes()\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        _torchmetric=TORCH_METRIC,\n        PREFIX=PREFIX,\n        TENSORBOARD_CLEAN=True,\n        data_set=dataset,\n        device=DEVICE,\n        enable_progress_bar=False,\n        fun_evals=FUN_EVALS,\n        log_level=50,\n        max_time=MAX_TIME,\n        num_workers=WORKERS,\n        show_progress=True,\n        test_size=TEST_SIZE,\n        tolerance_x=np.sqrt(np.spacing(1)),\n        )\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                hyper_dict=LightHyperDict)\n    from spotpython.hyperparameters.values import set_control_hyperparameter_value\n    set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n    set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n    set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n    set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                    \"Adam\",\n                    \"RAdam\",\n                ])\n    set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n    set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n    set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n    set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                    \"ReLU\",\n                    \"LeakyReLU\"\n                ] )\n    from spotpython.utils.init import design_control_init, surrogate_control_init\n    design_control = design_control_init(init_size=INIT_SIZE)\n    surrogate_control = surrogate_control_init(noise=True,\n                                                n_theta=2)\n    from spotpython.fun.hyperlight import HyperLight\n    fun = HyperLight(log_level=50).fun\n    from spotpython.spot import spot\n    spot_tuner = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control)\n    spot_tuner.run()\n    get_tuned_hyperparameters(spot_tuner)\n        {'l1': 7.0,\n        'epochs': 5.0,\n        'batch_size': 4.0,\n        'act_fn': 0.0,\n        'optimizer': 0.0,\n        'dropout_prob': 0.01,\n        'lr_mult': 5.0,\n        'patience': 3.0,\n        'initialization': 1.0}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_tuned_hyperparameters(spot_tuner, fun_control=None) -&gt; dict:\n    \"\"\"\n    Get the tuned hyperparameters from the spot tuner.\n    This is just a wrapper function for the spot `get_tuned_hyperparameters` method.\n\n    Args:\n        spot_tuner (object):\n            spot tuner object.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n            Optional. Default is None.\n\n    Returns:\n        (dict):\n            dictionary containing the tuned hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.device import getDevice\n            from math import inf\n            from spotpython.utils.init import fun_control_init\n            import numpy as np\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperparameters.values import get_tuned_hyperparameters\n            MAX_TIME = 1\n            FUN_EVALS = 10\n            INIT_SIZE = 5\n            WORKERS = 0\n            PREFIX=\"037\"\n            DEVICE = getDevice()\n            DEVICES = 1\n            TEST_SIZE = 0.4\n            TORCH_METRIC = \"mean_squared_error\"\n            dataset = Diabetes()\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                _torchmetric=TORCH_METRIC,\n                PREFIX=PREFIX,\n                TENSORBOARD_CLEAN=True,\n                data_set=dataset,\n                device=DEVICE,\n                enable_progress_bar=False,\n                fun_evals=FUN_EVALS,\n                log_level=50,\n                max_time=MAX_TIME,\n                num_workers=WORKERS,\n                show_progress=True,\n                test_size=TEST_SIZE,\n                tolerance_x=np.sqrt(np.spacing(1)),\n                )\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        hyper_dict=LightHyperDict)\n            from spotpython.hyperparameters.values import set_control_hyperparameter_value\n            set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n            set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n            set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n            set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                            \"Adam\",\n                            \"RAdam\",\n                        ])\n            set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n            set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n            set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n            set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                            \"ReLU\",\n                            \"LeakyReLU\"\n                        ] )\n            from spotpython.utils.init import design_control_init, surrogate_control_init\n            design_control = design_control_init(init_size=INIT_SIZE)\n            surrogate_control = surrogate_control_init(noise=True,\n                                                        n_theta=2)\n            from spotpython.fun.hyperlight import HyperLight\n            fun = HyperLight(log_level=50).fun\n            from spotpython.spot import spot\n            spot_tuner = spot.Spot(fun=fun,\n                                fun_control=fun_control,\n                                design_control=design_control,\n                                surrogate_control=surrogate_control)\n            spot_tuner.run()\n            get_tuned_hyperparameters(spot_tuner)\n                {'l1': 7.0,\n                'epochs': 5.0,\n                'batch_size': 4.0,\n                'act_fn': 0.0,\n                'optimizer': 0.0,\n                'dropout_prob': 0.01,\n                'lr_mult': 5.0,\n                'patience': 3.0,\n                'initialization': 1.0}\n    \"\"\"\n    return spot_tuner.get_tuned_hyperparameters(fun_control=fun_control)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_values_from_dict","title":"<code>get_values_from_dict(dictionary)</code>","text":"<p>Get the values from a dictionary as an array. Generate an np.array that contains the values of the keys of a dictionary in the same order as the keys of the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>dictionary with values</p> required <p>Returns:</p> Type Description <code>array</code> <p>array with values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_values_from_dict\n&gt;&gt;&gt; d = {\"a\": 1, \"b\": 2, \"c\": 3}\n&gt;&gt;&gt; get_values_from_dict(d)\narray([1, 2, 3])\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_values_from_dict(dictionary) -&gt; np.array:\n    \"\"\"Get the values from a dictionary as an array.\n    Generate an np.array that contains the values of the keys of a dictionary\n    in the same order as the keys of the dictionary.\n\n    Args:\n        dictionary (dict):\n            dictionary with values\n\n    Returns:\n        (np.array):\n            array with values\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_values_from_dict\n        &gt;&gt;&gt; d = {\"a\": 1, \"b\": 2, \"c\": 3}\n        &gt;&gt;&gt; get_values_from_dict(d)\n        array([1, 2, 3])\n    \"\"\"\n    return np.array(list(dictionary.values()))\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_var_name","title":"<code>get_var_name(fun_control)</code>","text":"<p>Get the names of the values from the dictionary fun_control as a list. If no \u201ccore_model_hyper_dict\u201d key exists in fun_control, return None.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with names</p> required <p>Returns:</p> Type Description <code>list</code> <p>ist with names</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_name\n    fun_control = {\"core_model_hyper_dict\":{\n                \"leaf_prediction\": {\n                    \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                    \"type\": \"factor\",\n                    \"default\": \"mean\",\n                    \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\",\n                                \"linear_model.PARegressor\",\n                                \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}}\n    get_var_name(fun_control)\n    ['leaf_prediction',\n        'leaf_model',\n        'splitter',\n        'binary_split',\n        'stop_mem_management']\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_var_name(fun_control) -&gt; list:\n    \"\"\"Get the names of the values from the dictionary fun_control as a list.\n    If no \"core_model_hyper_dict\" key exists in fun_control, return None.\n\n    Args:\n        fun_control (dict):\n            dictionary with names\n\n    Returns:\n        (list):\n            ist with names\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_name\n            fun_control = {\"core_model_hyper_dict\":{\n                        \"leaf_prediction\": {\n                            \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                            \"type\": \"factor\",\n                            \"default\": \"mean\",\n                            \"core_model_parameter_type\": \"str\"},\n                        \"leaf_model\": {\n                            \"levels\": [\"linear_model.LinearRegression\",\n                                        \"linear_model.PARegressor\",\n                                        \"linear_model.Perceptron\"],\n                            \"type\": \"factor\",\n                            \"default\": \"LinearRegression\",\n                            \"core_model_parameter_type\": \"instance\"},\n                        \"splitter\": {\n                            \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                            \"type\": \"factor\",\n                            \"default\": \"EBSTSplitter\",\n                            \"core_model_parameter_type\": \"instance()\"},\n                        \"binary_split\": {\n                            \"levels\": [0, 1],\n                            \"type\": \"factor\",\n                            \"default\": 0,\n                            \"core_model_parameter_type\": \"bool\"},\n                        \"stop_mem_management\": {\n                            \"levels\": [0, 1],\n                            \"type\": \"factor\",\n                            \"default\": 0,\n                            \"core_model_parameter_type\": \"bool\"}}}\n            get_var_name(fun_control)\n            ['leaf_prediction',\n                'leaf_model',\n                'splitter',\n                'binary_split',\n                'stop_mem_management']\n    \"\"\"\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        return list(fun_control[\"core_model_hyper_dict\"].keys())\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_var_type","title":"<code>get_var_type(fun_control)</code>","text":"<p>Get the types of the values from the dictionary fun_control as a list. If no \u201ccore_model_hyper_dict\u201d key exists in fun_control, return None.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with levels and types</p> required <p>Returns:</p> Type Description <code>list</code> <p>list with types</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_type\n    d = {\"core_model_hyper_dict\":{\n    \"leaf_prediction\": {\n        \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        \"type\": \"factor\",\n        \"default\": \"mean\",\n        \"core_model_parameter_type\": \"str\"},\n    \"leaf_model\": {\n        \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n        \"type\": \"factor\",\n        \"default\": \"LinearRegression\",\n        \"core_model_parameter_type\": \"instance\"},\n    \"splitter\": {\n        \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n        \"type\": \"factor\",\n        \"default\": \"EBSTSplitter\",\n        \"core_model_parameter_type\": \"instance()\"},\n    \"binary_split\": {\n        \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"core_model_parameter_type\": \"bool\"},\n    \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"core_model_parameter_type\": \"bool\"}}}\n</code></pre> <pre><code>get_var_type(d)\n['factor', 'factor', 'factor', 'factor', 'factor']\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_var_type(fun_control) -&gt; list:\n    \"\"\"\n    Get the types of the values from the dictionary fun_control as a list.\n    If no \"core_model_hyper_dict\" key exists in fun_control, return None.\n\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n\n    Returns:\n        (list):\n            list with types\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_type\n            d = {\"core_model_hyper_dict\":{\n            \"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"core_model_parameter_type\": \"str\"},\n            \"leaf_model\": {\n                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                \"type\": \"factor\",\n                \"default\": \"LinearRegression\",\n                \"core_model_parameter_type\": \"instance\"},\n            \"splitter\": {\n                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                \"type\": \"factor\",\n                \"default\": \"EBSTSplitter\",\n                \"core_model_parameter_type\": \"instance()\"},\n            \"binary_split\": {\n                \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"core_model_parameter_type\": \"bool\"},\n            \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"core_model_parameter_type\": \"bool\"}}}\n\n            get_var_type(d)\n            ['factor', 'factor', 'factor', 'factor', 'factor']\n    \"\"\"\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        return list(fun_control[\"core_model_hyper_dict\"][key][\"type\"] for key in fun_control[\"core_model_hyper_dict\"].keys())\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_var_type_from_var_name","title":"<code>get_var_type_from_var_name(fun_control, var_name)</code>","text":"<p>This function gets the variable type from the variable name.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>var_name</code> <code>str</code> <p>variable name</p> required <p>Returns:</p> Type Description <code>str</code> <p>variable type</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import get_var_type_from_var_name\n    control_dict = fun_control_init()\n    get_var_type_from_var_name(var_name=\"max_depth\",\n                    fun_control=control_dict)\n    \"int\"\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_var_type_from_var_name(fun_control, var_name) -&gt; str:\n    \"\"\"\n    This function gets the variable type from the variable name.\n\n    Args:\n        fun_control (dict): fun_control dictionary\n        var_name (str): variable name\n\n    Returns:\n        (str): variable type\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import get_var_type_from_var_name\n            control_dict = fun_control_init()\n            get_var_type_from_var_name(var_name=\"max_depth\",\n                            fun_control=control_dict)\n            \"int\"\n    \"\"\"\n    var_type_list = get_control_key_value(control_dict=fun_control, key=\"var_type\")\n    var_name_list = get_control_key_value(control_dict=fun_control, key=\"var_name\")\n    return var_type_list[var_name_list.index(var_name)]\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.iterate_dict_values","title":"<code>iterate_dict_values(var_dict)</code>","text":"<p>Iterate over the values of a dictionary of variables. This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <p>Returns:</p> Type Description <code>Generator[Dict[str, Union[int, float]], None, None]</code> <p>Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.hyperparameters.values import iterate_dict_values\n&gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n&gt;&gt;&gt; list(iterate_dict_values(var_dict))\n[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def iterate_dict_values(var_dict: Dict[str, np.ndarray]) -&gt; Generator[Dict[str, Union[int, float]], None, None]:\n    \"\"\"Iterate over the values of a dictionary of variables.\n    This function takes a dictionary of variables as input arguments and returns a generator that\n    yields dictionaries with the values from the arrays in the input dictionary.\n\n    Args:\n        var_dict (dict): A dictionary where keys are variable names and values are numpy arrays.\n\n    Returns:\n        Generator[dict]:\n            A generator that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import iterate_dict_values\n        &gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n        &gt;&gt;&gt; list(iterate_dict_values(var_dict))\n        [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n    \"\"\"\n    n = len(next(iter(var_dict.values())))\n    for i in range(n):\n        yield {key: value[i] for key, value in var_dict.items()}\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.modify_boolean_hyper_parameter_levels","title":"<code>modify_boolean_hyper_parameter_levels(fun_control, hyperparameter, levels)</code>","text":"<p>This function modifies the levels of a boolean hyperparameter in the fun_control dictionary. It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>levels</code> <code>list</code> <p>list of levels</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def modify_boolean_hyper_parameter_levels(fun_control, hyperparameter, levels) -&gt; None:\n    \"\"\"\n    This function modifies the levels of a boolean hyperparameter in the fun_control dictionary.\n    It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        levels (list):\n            list of levels\n\n    Returns:\n        None.\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"levels\": levels})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": levels[0]})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": levels[1]})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.modify_hyper_parameter_bounds","title":"<code>modify_hyper_parameter_bounds(fun_control, hyperparameter, bounds)</code>","text":"<p>Modify the bounds of a hyperparameter in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>bounds</code> <code>list</code> <p>list of two bound values. The first value represents the lower bound and the second value represents the upper bound.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n    fun_control = {}\n    core_model  = HoeffdingTreeRegressor\n    fun_control.update({\"core_model\": core_model})\n    fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n    bounds = [3, 11]\n    fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds)\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def modify_hyper_parameter_bounds(fun_control, hyperparameter, bounds) -&gt; None:\n    \"\"\"\n    Modify the bounds of a hyperparameter in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        bounds (list):\n            list of two bound values. The first value represents the lower bound\n            and the second value represents the upper bound.\n\n    Returns:\n        None.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n            fun_control = {}\n            core_model  = HoeffdingTreeRegressor\n            fun_control.update({\"core_model\": core_model})\n            fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n            bounds = [3, 11]\n            fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds)\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": bounds[0]})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": bounds[1]})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.modify_hyper_parameter_levels","title":"<code>modify_hyper_parameter_levels(fun_control, hyperparameter, levels)</code>","text":"<p>This function modifies the levels of a hyperparameter in the fun_control dictionary. It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>levels</code> <code>list</code> <p>list of levels</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fun_control = {}\n    from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n    core_model  = HoeffdingTreeRegressor\n    fun_control.update({\"core_model\": core_model})\n    fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n    levels = [\"mean\", \"model\"]\n    fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels)\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def modify_hyper_parameter_levels(fun_control, hyperparameter, levels) -&gt; None:\n    \"\"\"\n    This function modifies the levels of a hyperparameter in the fun_control dictionary.\n    It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        levels (list):\n            list of levels\n\n    Returns:\n        None.\n\n    Examples:\n        &gt;&gt;&gt; fun_control = {}\n            from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n            core_model  = HoeffdingTreeRegressor\n            fun_control.update({\"core_model\": core_model})\n            fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n            levels = [\"mean\", \"model\"]\n            fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels)\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"levels\": levels})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": 0})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": len(levels) - 1})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.replace_levels_with_positions","title":"<code>replace_levels_with_positions(hyper_dict, hyper_dict_values)</code>","text":"<p>Replace the levels with the position in the levels list. The function that takes two dictionaries. The first contains as hyperparameters as keys. If the hyperparameter has the key \u201clevels\u201d, then the value of the corresponding hyperparameter in the second dictionary is replaced by the position of the value in the list of levels. The function returns a dictionary with the same keys as the second dictionary. For example, if the second dictionary is {\u201ca\u201d: 1, \u201cb\u201d: \u201cmodel1\u201d, \u201cc\u201d: 3} and the first dictionary is {     \u201ca\u201d: {\u201ctype\u201d: \u201cint\u201d},     \u201cb\u201d: {\u201clevels\u201d: [\u201cmodel4\u201d, \u201cmodel5\u201d, \u201cmodel1\u201d]},     \u201cd\u201d: {\u201ctype\u201d: \u201cfloat\u201d}}, then the function should return {\u201ca\u201d: 1, \u201cb\u201d: 2, \u201cc\u201d: 3}.</p> <p>Parameters:</p> Name Type Description Default <code>hyper_dict</code> <code>dict</code> <p>dictionary with levels</p> required <code>hyper_dict_values</code> <code>dict</code> <p>dictionary with values</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dictionary with values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import replace_levels_with_positions\n    hyper_dict = {\"leaf_prediction\": {\n        \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        \"type\": \"factor\",\n        \"default\": \"mean\",\n        \"core_model_parameter_type\": \"str\"},\n        \"leaf_model\": {\n            \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n            \"type\": \"factor\",\n            \"default\": \"LinearRegression\",\n            \"core_model_parameter_type\": \"instance\"},\n        \"splitter\": {\n            \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n            \"type\": \"factor\",\n            \"default\": \"EBSTSplitter\",\n            \"core_model_parameter_type\": \"instance()\"},\n        \"binary_split\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"},\n        \"stop_mem_management\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"}}\n    hyper_dict_values = {\"leaf_prediction\": \"mean\",\n        \"leaf_model\": \"linear_model.LinearRegression\",\n        \"splitter\": \"EBSTSplitter\",\n        \"binary_split\": 0,\n        \"stop_mem_management\": 0}\n    replace_levels_with_position(hyper_dict, hyper_dict_values)\n        {'leaf_prediction': 0,\n        'leaf_model': 0,\n        'splitter': 0,\n        'binary_split': 0,\n        'stop_mem_management': 0}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def replace_levels_with_positions(hyper_dict, hyper_dict_values) -&gt; dict:\n    \"\"\"Replace the levels with the position in the levels list.\n    The function that takes two dictionaries.\n    The first contains as hyperparameters as keys.\n    If the hyperparameter has the key \"levels\",\n    then the value of the corresponding hyperparameter in the second dictionary is\n    replaced by the position of the value in the list of levels.\n    The function returns a dictionary with the same keys as the second dictionary.\n    For example, if the second dictionary is {\"a\": 1, \"b\": \"model1\", \"c\": 3}\n    and the first dictionary is {\n        \"a\": {\"type\": \"int\"},\n        \"b\": {\"levels\": [\"model4\", \"model5\", \"model1\"]},\n        \"d\": {\"type\": \"float\"}},\n    then the function should return {\"a\": 1, \"b\": 2, \"c\": 3}.\n\n    Args:\n        hyper_dict (dict):\n            dictionary with levels\n        hyper_dict_values (dict):\n            dictionary with values\n\n    Returns:\n        (dict):\n            dictionary with values\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import replace_levels_with_positions\n            hyper_dict = {\"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}\n            hyper_dict_values = {\"leaf_prediction\": \"mean\",\n                \"leaf_model\": \"linear_model.LinearRegression\",\n                \"splitter\": \"EBSTSplitter\",\n                \"binary_split\": 0,\n                \"stop_mem_management\": 0}\n            replace_levels_with_position(hyper_dict, hyper_dict_values)\n                {'leaf_prediction': 0,\n                'leaf_model': 0,\n                'splitter': 0,\n                'binary_split': 0,\n                'stop_mem_management': 0}\n    \"\"\"\n    hyper_dict_values_new = copy.deepcopy(hyper_dict_values)\n    # generate an error if the following code fails and write an error message:\n    try:\n        for key, value in hyper_dict_values.items():\n            if key in hyper_dict.keys():\n                if \"levels\" in hyper_dict[key].keys():\n                    hyper_dict_values_new[key] = hyper_dict[key][\"levels\"].index(value)\n    except Exception as e:\n        print(\"!!! Warning: \", e)\n        print(\"Did you modify lower and upper bounds so that the default values are not included?\")\n        print(\"Returning 'None'.\")\n        return None\n    return hyper_dict_values_new\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.return_conf_list_from_var_dict","title":"<code>return_conf_list_from_var_dict(var_dict, fun_control, default=False)</code>","text":"<p>Return a list of configurations from a dictionary of variables.</p> <p>This function takes a dictionary of variables and a dictionary of function control as input arguments. It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries of hyper parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, Union[int, float]]]</code> <p>A list of dictionaries of hyper parameter values. Transformations are applied to the values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.hyperparameters.values import return_conf_list_from_var_dict\n&gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n&gt;&gt;&gt; fun_control = {'var_type': ['int', 'int']}\n&gt;&gt;&gt; return_conf_list_from_var_dict(var_dict, fun_control)\n[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def return_conf_list_from_var_dict(\n    var_dict: Dict[str, np.ndarray],\n    fun_control: Dict[str, Union[List[str], str]],\n    default: bool = False,\n) -&gt; List[Dict[str, Union[int, float]]]:\n    \"\"\"Return a list of configurations from a dictionary of variables.\n\n    This function takes a dictionary of variables and a dictionary of function control as input arguments.\n    It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries\n    of hyper parameter values.\n\n    Args:\n        var_dict (dict): A dictionary where keys are variable names and values are numpy arrays.\n        fun_control (dict): A dictionary which (at least) has an entry with the following key:\n            \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding\n            value will be converted to the type \"int\".\n\n    Returns:\n        list: A list of dictionaries of hyper parameter values. Transformations are applied to the values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import return_conf_list_from_var_dict\n        &gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n        &gt;&gt;&gt; fun_control = {'var_type': ['int', 'int']}\n        &gt;&gt;&gt; return_conf_list_from_var_dict(var_dict, fun_control)\n        [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n    \"\"\"\n    conf_list = []\n    for values in generate_one_config_from_var_dict(var_dict, fun_control, default=default):\n        conf_list.append(values)\n    return conf_list\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_boolean_hyperparameter_values","title":"<code>set_boolean_hyperparameter_values(fun_control, key, lower, upper)</code>","text":"<p>Set the boolean hyperparameter values in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>key</code> <code>str</code> <p>The key of the hyperparameter.</p> required <code>lower</code> <code>bool</code> <p>The lower bound of the hyperparameter.</p> required <code>upper</code> <code>bool</code> <p>The upper bound of the hyperparameter.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_boolean_hyperparameter_values\n    from spotpython.utils.eda import gen_design_table\n    fun_control = fun_control_init(\n        core_model_name=\"forest.AMFRegressor\",\n        hyperdict=RiverHyperDict,\n    )\n    print(\"Before modification:\")\n    print(gen_design_table(fun_control))\n    set_boolean_hyperparameter_values(fun_control, \"use_aggregation\", 0, 0)\n    print(\"After modification:\")\n    print(gen_design_table(fun_control))\n    Seed set to 123\n    Before modification:\n    | name            | type   |   default |   lower |   upper | transform   |\n    |-----------------|--------|-----------|---------|---------|-------------|\n    | n_estimators    | int    |        10 |     2   |    1000 | None        |\n    | step            | float  |         1 |     0.1 |      10 | None        |\n    | use_aggregation | factor |         1 |     0   |       1 | None        |\n    Setting hyperparameter use_aggregation to value [0, 0].\n    Variable type is factor.\n    Core type is bool.\n    Calling modify_boolean_hyper_parameter_levels().\n    After modification:\n    | name            | type   |   default |   lower |   upper | transform   |\n    |-----------------|--------|-----------|---------|---------|-------------|\n    | n_estimators    | int    |        10 |     2   |    1000 | None        |\n    | step            | float  |         1 |     0.1 |      10 | None        |\n    | use_aggregation | factor |         1 |     0   |       0 | None        |\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_boolean_hyperparameter_values(fun_control, key, lower, upper):\n    \"\"\"\n    Set the boolean hyperparameter values in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        key (str):\n            The key of the hyperparameter.\n        lower (bool):\n            The lower bound of the hyperparameter.\n        upper (bool):\n            The upper bound of the hyperparameter.\n\n    Examples:\n        &gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_boolean_hyperparameter_values\n            from spotpython.utils.eda import gen_design_table\n            fun_control = fun_control_init(\n                core_model_name=\"forest.AMFRegressor\",\n                hyperdict=RiverHyperDict,\n            )\n            print(\"Before modification:\")\n            print(gen_design_table(fun_control))\n            set_boolean_hyperparameter_values(fun_control, \"use_aggregation\", 0, 0)\n            print(\"After modification:\")\n            print(gen_design_table(fun_control))\n            Seed set to 123\n            Before modification:\n            | name            | type   |   default |   lower |   upper | transform   |\n            |-----------------|--------|-----------|---------|---------|-------------|\n            | n_estimators    | int    |        10 |     2   |    1000 | None        |\n            | step            | float  |         1 |     0.1 |      10 | None        |\n            | use_aggregation | factor |         1 |     0   |       1 | None        |\n            Setting hyperparameter use_aggregation to value [0, 0].\n            Variable type is factor.\n            Core type is bool.\n            Calling modify_boolean_hyper_parameter_levels().\n            After modification:\n            | name            | type   |   default |   lower |   upper | transform   |\n            |-----------------|--------|-----------|---------|---------|-------------|\n            | n_estimators    | int    |        10 |     2   |    1000 | None        |\n            | step            | float  |         1 |     0.1 |      10 | None        |\n            | use_aggregation | factor |         1 |     0   |       0 | None        |\n    \"\"\"\n    set_control_hyperparameter_value(\n        fun_control,\n        key,\n        [\n            lower,\n            upper,\n        ],\n    )\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_control_hyperparameter_value","title":"<code>set_control_hyperparameter_value(control_dict, hyperparameter, value)</code>","text":"<p>This function sets the hyperparameter values depending on the var_type via modify_hyperameter_levels or modify_hyperparameter_bounds in the control_dict dictionary. If the hyperparameter is a factor, it calls modify_hyper_parameter_levels. Otherwise, it calls modify_hyper_parameter_bounds.</p> <p>Parameters:</p> Name Type Description Default <code>control_dict</code> <code>dict</code> <p>control_dict dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>key</p> required <code>value</code> <code>Any</code> <p>value</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_control_hyperparameter_value(control_dict, hyperparameter, value) -&gt; None:\n    \"\"\"\n    This function sets the hyperparameter values depending on the var_type\n    via modify_hyperameter_levels or modify_hyperparameter_bounds in the control_dict dictionary.\n    If the hyperparameter is a factor, it calls modify_hyper_parameter_levels.\n    Otherwise, it calls modify_hyper_parameter_bounds.\n\n    Args:\n        control_dict (dict):\n            control_dict dictionary\n        hyperparameter (str): key\n        value (Any): value\n\n    Returns:\n        None.\n\n    \"\"\"\n    print(f\"Setting hyperparameter {hyperparameter} to value {value}.\")\n    vt = get_var_type_from_var_name(fun_control=control_dict, var_name=hyperparameter)\n    print(f\"Variable type is {vt}.\")\n    core_type = get_core_model_parameter_type_from_var_name(fun_control=control_dict, var_name=hyperparameter)\n    print(f\"Core type is {core_type}.\")\n    if vt == \"factor\" and core_type != \"bool\":\n        print(\"Calling modify_hyper_parameter_levels().\")\n        modify_hyper_parameter_levels(fun_control=control_dict, hyperparameter=hyperparameter, levels=value)\n    elif vt == \"factor\" and core_type == \"bool\":\n        print(\"Calling modify_boolean_hyper_parameter_levels().\")\n        modify_boolean_hyper_parameter_levels(fun_control=control_dict, hyperparameter=hyperparameter, levels=value)\n    else:\n        print(\"Calling modify_hyper_parameter_bounds().\")\n        modify_hyper_parameter_bounds(fun_control=control_dict, hyperparameter=hyperparameter, bounds=value)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_control_key_value","title":"<code>set_control_key_value(control_dict, key, value, replace=False)</code>","text":"<p>This function sets the key value pair in the control_dict dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>control_dict</code> <code>dict</code> <p>control_dict dictionary</p> required <code>key</code> <code>str</code> <p>key</p> required <code>value</code> <code>Any</code> <p>value</p> required <code>replace</code> <code>bool</code> <p>replace value if key already exists. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>key</p> <code>value</code> <code>Any</code> <p>value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_control_key_value\n    control_dict = fun_control_init()\n    set_control_key_value(control_dict=control_dict,\n                  key=\"key\",\n                  value=\"value\")\n    control_dict[\"key\"]\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_control_key_value(control_dict, key, value, replace=False) -&gt; None:\n    \"\"\"\n    This function sets the key value pair in the control_dict dictionary.\n\n    Args:\n        control_dict (dict):\n            control_dict dictionary\n        key (str): key\n        value (Any): value\n        replace (bool): replace value if key already exists. Default is False.\n\n    Returns:\n        None.\n\n    Attributes:\n        key (str): key\n        value (Any): value\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_control_key_value\n            control_dict = fun_control_init()\n            set_control_key_value(control_dict=control_dict,\n                          key=\"key\",\n                          value=\"value\")\n            control_dict[\"key\"]\n\n    \"\"\"\n    if replace:\n        control_dict.update({key: value})\n    else:\n        if key not in control_dict.keys():\n            control_dict.update({key: value})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_factor_hyperparameter_values","title":"<code>set_factor_hyperparameter_values(fun_control, key, levels)</code>","text":"<p>Set the factor hyperparameter values in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>key</code> <code>str</code> <p>The key of the hyperparameter.</p> required <code>levels</code> <code>list</code> <p>The levels of the hyperparameter.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_factor_hyperparameter_values\n    from spotpython.utils.eda import gen_design_table\n    fun_control = fun_control_init(\n        core_model_name=\"tree.HoeffdingTreeRegressor\",\n        hyperdict=RiverHyperDict,\n    )\n    print(\"Before modification:\")\n    print(gen_design_table(fun_control))\n    set_factor_hyperparameter_values(fun_control, \"leaf_model\", ['LinearRegression',\n                                                        'Perceptron'])\n    print(\"After modification:\")\n    print(gen_design_table(fun_control))\n        Seed set to 123\n        Before modification:\n        | name                   | type   | default          |   lower |    upper | transform              |\n        |------------------------|--------|------------------|---------|----------|------------------------|\n        | grace_period           | int    | 200              |  10     | 1000     | None                   |\n        | max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n        | delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n        | tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n        | leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n        | leaf_model             | factor | LinearRegression |   0     |    2     | None                   |\n        | model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n        | splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n        | min_samples_split      | int    | 5                |   2     |   10     | None                   |\n        | binary_split           | factor | 0                |   0     |    1     | None                   |\n        | max_size               | float  | 500.0            | 100     | 1000     | None                   |\n        | memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n        | stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n        | remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n        | merit_preprune         | factor | 1                |   0     |    1     | None                   |\n        After modification:\n        | name                   | type   | default          |   lower |    upper | transform              |\n        |------------------------|--------|------------------|---------|----------|------------------------|\n        | grace_period           | int    | 200              |  10     | 1000     | None                   |\n        | max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n        | delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n        | tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n        | leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n        | leaf_model             | factor | LinearRegression |   0     |    1     | None                   |\n        | model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n        | splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n        | min_samples_split      | int    | 5                |   2     |   10     | None                   |\n        | binary_split           | factor | 0                |   0     |    1     | None                   |\n        | max_size               | float  | 500.0            | 100     | 1000     | None                   |\n        | memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n        | stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n        | remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n        | merit_preprune         | factor | 1                |   0     |    1     | None                   |\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_factor_hyperparameter_values(fun_control, key, levels):\n    \"\"\"\n    Set the factor hyperparameter values in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        key (str):\n            The key of the hyperparameter.\n        levels (list):\n            The levels of the hyperparameter.\n\n    Examples:\n        &gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_factor_hyperparameter_values\n            from spotpython.utils.eda import gen_design_table\n            fun_control = fun_control_init(\n                core_model_name=\"tree.HoeffdingTreeRegressor\",\n                hyperdict=RiverHyperDict,\n            )\n            print(\"Before modification:\")\n            print(gen_design_table(fun_control))\n            set_factor_hyperparameter_values(fun_control, \"leaf_model\", ['LinearRegression',\n                                                                'Perceptron'])\n            print(\"After modification:\")\n            print(gen_design_table(fun_control))\n                Seed set to 123\n                Before modification:\n                | name                   | type   | default          |   lower |    upper | transform              |\n                |------------------------|--------|------------------|---------|----------|------------------------|\n                | grace_period           | int    | 200              |  10     | 1000     | None                   |\n                | max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n                | delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n                | tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n                | leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n                | leaf_model             | factor | LinearRegression |   0     |    2     | None                   |\n                | model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n                | splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n                | min_samples_split      | int    | 5                |   2     |   10     | None                   |\n                | binary_split           | factor | 0                |   0     |    1     | None                   |\n                | max_size               | float  | 500.0            | 100     | 1000     | None                   |\n                | memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n                | stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n                | remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n                | merit_preprune         | factor | 1                |   0     |    1     | None                   |\n                After modification:\n                | name                   | type   | default          |   lower |    upper | transform              |\n                |------------------------|--------|------------------|---------|----------|------------------------|\n                | grace_period           | int    | 200              |  10     | 1000     | None                   |\n                | max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n                | delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n                | tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n                | leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n                | leaf_model             | factor | LinearRegression |   0     |    1     | None                   |\n                | model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n                | splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n                | min_samples_split      | int    | 5                |   2     |   10     | None                   |\n                | binary_split           | factor | 0                |   0     |    1     | None                   |\n                | max_size               | float  | 500.0            | 100     | 1000     | None                   |\n                | memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n                | stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n                | remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n                | merit_preprune         | factor | 1                |   0     |    1     | None                   |\n    \"\"\"\n    # check if levels is a list of strings. If not, convert it to a list\n    if not isinstance(levels, list):\n        levels = [levels]\n    # check if levels is a list of strings. Othewise, issue a warning and return None\n    if not all(isinstance(x, str) for x in levels):\n        print(\"!!! Warning: levels should be a list of strings.\")\n        return None\n    # check if key \"core_model_hyper_dict\" exists in fun_control:\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        fun_control[\"core_model_hyper_dict\"][key].update({\"levels\": levels})\n        fun_control[\"core_model_hyper_dict\"][key].update({\"upper\": len(levels) - 1})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_float_hyperparameter_values","title":"<code>set_float_hyperparameter_values(fun_control, key, lower, upper)</code>","text":"<p>Set the float hyperparameter values in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>key</code> <code>str</code> <p>The key of the hyperparameter.</p> required <code>lower</code> <code>float</code> <p>The lower bound of the hyperparameter.</p> required <code>upper</code> <code>float</code> <p>The upper bound of the hyperparameter.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_float_hyperparameter_values\n    from spotpython.utils.eda import gen_design_table\n    fun_control = fun_control_init(\n        core_model_name=\"forest.AMFRegressor\",\n        hyperdict=RiverHyperDict,\n    )\n    print(\"Before modification:\")\n    print(gen_design_table(fun_control))\n    set_float_hyperparameter_values(fun_control, \"step\", 0.2, 5)\n    print(\"After modification:\")\n    print(gen_design_table(fun_control))\n    Seed set to 123\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_float_hyperparameter_values(fun_control, key, lower, upper) -&gt; None:\n    \"\"\"\n    Set the float hyperparameter values in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        key (str):\n            The key of the hyperparameter.\n        lower (float):\n            The lower bound of the hyperparameter.\n        upper (float):\n            The upper bound of the hyperparameter.\n\n    Examples:\n        &gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_float_hyperparameter_values\n            from spotpython.utils.eda import gen_design_table\n            fun_control = fun_control_init(\n                core_model_name=\"forest.AMFRegressor\",\n                hyperdict=RiverHyperDict,\n            )\n            print(\"Before modification:\")\n            print(gen_design_table(fun_control))\n            set_float_hyperparameter_values(fun_control, \"step\", 0.2, 5)\n            print(\"After modification:\")\n            print(gen_design_table(fun_control))\n            Seed set to 123\n    \"\"\"\n    set_control_hyperparameter_value(\n        fun_control,\n        key,\n        [\n            lower,\n            upper,\n        ],\n    )\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_hyperparameter","title":"<code>set_hyperparameter(fun_control, key, values)</code>","text":"<p>Set hyperparameter values in the fun_control dictionary based on the type of the values argument.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>key</code> <code>str</code> <p>The key of the hyperparameter.</p> required <code>values</code> <code>Union[int, float, bool, list]</code> <p>The values of the hyperparameter. This can be:     - For int and float: a list containing lower and upper bounds.     - For bool: a list containing two boolean values.     - For factor: a list of strings representing levels.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import set_hyperparameter\n&gt;&gt;&gt; fun_control = {\n        \"core_model_hyper_dict\": {\n            \"n_estimators\": {\"type\": \"int\", \"default\": 10, \"lower\": 2, \"upper\": 1000},\n            \"step\": {\"type\": \"float\", \"default\": 1.0, \"lower\": 0.1, \"upper\": 10.0},\n            \"use_aggregation\": {\"type\": \"factor\", \"default\": 1, \"lower\": 0, \"upper\": 1, \"levels\": [0, 1]},\n            \"leaf_model\": {\"type\": \"factor\", \"default\": \"LinearRegression\", \"upper\": 2}\n        }\n    }\n&gt;&gt;&gt; set_hyperparameter(fun_control, \"n_estimators\", [2, 5])\n&gt;&gt;&gt; set_hyperparameter(fun_control, \"step\", [0.2, 5.0])\n&gt;&gt;&gt; set_hyperparameter(fun_control, \"use_aggregation\", [False, True])\n&gt;&gt;&gt; set_hyperparameter(fun_control, \"leaf_model\", [\"LinearRegression\", \"Perceptron\"])\n&gt;&gt;&gt; set_hyperparameter(fun_control, \"leaf_model\", \"LinearRegression\")\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_hyperparameter(fun_control, key, values):\n    \"\"\"\n    Set hyperparameter values in the fun_control dictionary based on the type of the values argument.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        key (str):\n            The key of the hyperparameter.\n        values (Union[int, float, bool, list]):\n            The values of the hyperparameter. This can be:\n                - For int and float: a list containing lower and upper bounds.\n                - For bool: a list containing two boolean values.\n                - For factor: a list of strings representing levels.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import set_hyperparameter\n        &gt;&gt;&gt; fun_control = {\n                \"core_model_hyper_dict\": {\n                    \"n_estimators\": {\"type\": \"int\", \"default\": 10, \"lower\": 2, \"upper\": 1000},\n                    \"step\": {\"type\": \"float\", \"default\": 1.0, \"lower\": 0.1, \"upper\": 10.0},\n                    \"use_aggregation\": {\"type\": \"factor\", \"default\": 1, \"lower\": 0, \"upper\": 1, \"levels\": [0, 1]},\n                    \"leaf_model\": {\"type\": \"factor\", \"default\": \"LinearRegression\", \"upper\": 2}\n                }\n            }\n        &gt;&gt;&gt; set_hyperparameter(fun_control, \"n_estimators\", [2, 5])\n        &gt;&gt;&gt; set_hyperparameter(fun_control, \"step\", [0.2, 5.0])\n        &gt;&gt;&gt; set_hyperparameter(fun_control, \"use_aggregation\", [False, True])\n        &gt;&gt;&gt; set_hyperparameter(fun_control, \"leaf_model\", [\"LinearRegression\", \"Perceptron\"])\n        &gt;&gt;&gt; set_hyperparameter(fun_control, \"leaf_model\", \"LinearRegression\")\n    \"\"\"\n    # if values is only a string  and not a list of strings, convert it to a list\n    if isinstance(values, str):\n        values = [values]\n    if isinstance(values, list):\n        if all(isinstance(v, int) for v in values):\n            _set_int_hyperparameter_values(fun_control, key, values[0], values[1])\n        elif all(isinstance(v, float) for v in values):\n            _set_float_hyperparameter_values(fun_control, key, values[0], values[1])\n        elif all(isinstance(v, bool) for v in values):\n            _set_boolean_hyperparameter_values(fun_control, key, values[0], values[1])\n        elif all(isinstance(v, str) for v in values):\n            _set_factor_hyperparameter_values(fun_control, key, values)\n        else:\n            raise ValueError(\"Invalid type in values list.\")\n    else:\n        raise TypeError(\"values should be a list.\")\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_int_hyperparameter_values","title":"<code>set_int_hyperparameter_values(fun_control, key, lower, upper)</code>","text":"<p>Set (modify) the integer hyperparameter values in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>key</code> <code>str</code> <p>The key of the hyperparameter.</p> required <code>lower</code> <code>int</code> <p>The lower bound of the hyperparameter.</p> required <code>upper</code> <code>int</code> <p>The upper bound of the hyperparameter.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_int_hyperparameter_values\n    from spotpython.utils.eda import gen_design_table\n    fun_control = fun_control_init(\n        core_model_name=\"forest.AMFRegressor\",\n        hyperdict=RiverHyperDict,\n    )\n    print(\"Before modification:\")\n    print(gen_design_table(fun_control))\n    set_int_hyperparameter_values(fun_control, \"n_estimators\", 2, 5)\n    print(\"After modification:\")\n    print(gen_design_table(fun_control))\n    Seed set to 123\n        Before modification:\n        | name            | type   |   default |   lower |   upper | transform   |\n        |-----------------|--------|-----------|---------|---------|-------------|\n        | n_estimators    | int    |        10 |     2   |    1000 | None        |\n        | step            | float  |         1 |     0.1 |      10 | None        |\n        | use_aggregation | factor |         1 |     0   |       1 | None        |\n        Setting hyperparameter n_estimators to value [2, 5].\n        Variable type is int.\n        Core type is None.\n        Calling modify_hyper_parameter_bounds().\n        After modification:\n        | name            | type   |   default |   lower |   upper | transform   |\n        |-----------------|--------|-----------|---------|---------|-------------|\n        | n_estimators    | int    |        10 |     2   |       5 | None        |\n        | step            | float  |         1 |     0.1 |      10 | None        |\n        | use_aggregation | factor |         1 |     0   |       1 | None        |\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_int_hyperparameter_values(fun_control, key, lower, upper) -&gt; None:\n    \"\"\"\n    Set (modify) the integer hyperparameter values in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        key (str):\n            The key of the hyperparameter.\n        lower (int):\n            The lower bound of the hyperparameter.\n        upper (int):\n            The upper bound of the hyperparameter.\n\n    Examples:\n        &gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_int_hyperparameter_values\n            from spotpython.utils.eda import gen_design_table\n            fun_control = fun_control_init(\n                core_model_name=\"forest.AMFRegressor\",\n                hyperdict=RiverHyperDict,\n            )\n            print(\"Before modification:\")\n            print(gen_design_table(fun_control))\n            set_int_hyperparameter_values(fun_control, \"n_estimators\", 2, 5)\n            print(\"After modification:\")\n            print(gen_design_table(fun_control))\n            Seed set to 123\n                Before modification:\n                | name            | type   |   default |   lower |   upper | transform   |\n                |-----------------|--------|-----------|---------|---------|-------------|\n                | n_estimators    | int    |        10 |     2   |    1000 | None        |\n                | step            | float  |         1 |     0.1 |      10 | None        |\n                | use_aggregation | factor |         1 |     0   |       1 | None        |\n                Setting hyperparameter n_estimators to value [2, 5].\n                Variable type is int.\n                Core type is None.\n                Calling modify_hyper_parameter_bounds().\n                After modification:\n                | name            | type   |   default |   lower |   upper | transform   |\n                |-----------------|--------|-----------|---------|---------|-------------|\n                | n_estimators    | int    |        10 |     2   |       5 | None        |\n                | step            | float  |         1 |     0.1 |      10 | None        |\n                | use_aggregation | factor |         1 |     0   |       1 | None        |\n    \"\"\"\n    set_control_hyperparameter_value(\n        fun_control,\n        key,\n        [\n            lower,\n            upper,\n        ],\n    )\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.update_fun_control_with_hyper_num_cat_dicts","title":"<code>update_fun_control_with_hyper_num_cat_dicts(fun_control, num_dict, cat_dict, dict)</code>","text":"<p>Update an existing fun_control dictionary with new hyperparameter values. All values from the hyperparameter dict (dict) are updated in the fun_control dictionary using the num_dict and cat_dict dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary. This dictionary is updated with the new hyperparameter values.</p> required <code>num_dict</code> <code>dict</code> <p>The dictionary containing the numerical hyperparameter values, which are used to update the fun_control dictionary.</p> required <code>cat_dict</code> <code>dict</code> <p>The dictionary containing the categorical hyperparameter values, which are used to update the fun_control dictionary.</p> required <code>dict</code> <code>dict</code> <p>The dictionary containing the \u201cold\u201d hyperparameter values.</p> required Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def update_fun_control_with_hyper_num_cat_dicts(fun_control, num_dict, cat_dict, dict):\n    \"\"\"\n    Update an existing fun_control dictionary with new hyperparameter values.\n    All values from the hyperparameter dict (dict) are updated in the fun_control dictionary\n    using the num_dict and cat_dict dictionaries.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary. This dictionary is updated with the new hyperparameter values.\n        num_dict (dict):\n            The dictionary containing the numerical hyperparameter values, which\n            are used to update the fun_control dictionary.\n        cat_dict (dict):\n            The dictionary containing the categorical hyperparameter values, which\n            are used to update the fun_control dictionary.\n        dict (dict):\n            The dictionary containing the \"old\" hyperparameter values.\n    \"\"\"\n    for i, (key, value) in enumerate(dict.items()):\n        if dict[key][\"type\"] == \"int\":\n            set_control_hyperparameter_value(\n                fun_control,\n                key,\n                [\n                    int(num_dict[key][\"lower\"]),\n                    int(num_dict[key][\"upper\"]),\n                ],\n            )\n        if (dict[key][\"type\"] == \"factor\") and (dict[key][\"core_model_parameter_type\"] == \"bool\"):\n            set_control_hyperparameter_value(\n                fun_control,\n                key,\n                [\n                    int(num_dict[key][\"lower\"]),\n                    int(num_dict[key][\"upper\"]),\n                ],\n            )\n        if dict[key][\"type\"] == \"float\":\n            set_control_hyperparameter_value(\n                fun_control,\n                key,\n                [\n                    float(num_dict[key][\"lower\"]),\n                    float(num_dict[key][\"upper\"]),\n                ],\n            )\n        if dict[key][\"type\"] == \"factor\" and dict[key][\"core_model_parameter_type\"] != \"bool\":\n            fle = cat_dict[key][\"levels\"]\n            # convert the string to a list of strings\n            fle = fle.split()\n            set_control_hyperparameter_value(fun_control, key, fle)\n            fun_control[\"core_model_hyper_dict\"][key].update({\"upper\": len(fle) - 1})\n</code></pre>"},{"location":"reference/spotpython/light/cvmodel/","title":"cvmodel","text":""},{"location":"reference/spotpython/light/cvmodel/#spotpython.light.cvmodel.cv_model","title":"<code>cv_model(config, fun_control)</code>","text":"<p>Performs k-fold cross-validation on a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean average precision at k (MAP@k) score of the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"_L_cond\": 0,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n...     \"k_folds\": 5,\n... }\n&gt;&gt;&gt; mapk_score = cv_model(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/cvmodel.py</code> <pre><code>def cv_model(config: dict, fun_control: dict) -&gt; float:\n    \"\"\"\n    Performs k-fold cross-validation on a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        (float): The mean average precision at k (MAP@k) score of the model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"_L_cond\": 0,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ...     \"k_folds\": 5,\n        ... }\n        &gt;&gt;&gt; mapk_score = cv_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _L_cond = fun_control[\"_L_cond\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"CV\" postfix to config_id\n    config_id = generate_config_id(config, timestamp=True) + \"_CV\"\n    results = []\n    num_folds = fun_control[\"k_folds\"]\n    split_seed = 12345\n\n    for k in range(num_folds):\n        print(\"k:\", k)\n\n        model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n\n        dm = LightCrossValidationDataModule(\n            k=k,\n            num_splits=num_folds,\n            split_seed=split_seed,\n            dataset=fun_control[\"data_set\"],\n            num_workers=fun_control[\"num_workers\"],\n            batch_size=config[\"batch_size\"],\n            data_dir=fun_control[\"DATASET_PATH\"],\n            scaler=fun_control[\"scaler\"],\n        )\n        dm.setup()\n        dm.prepare_data()\n\n        # TODO: Check if this is necessary:\n        # dm.setup()\n\n        # Init trainer\n        trainer = L.Trainer(\n            # Where to save models\n            default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n            max_epochs=model.hparams.epochs,\n            accelerator=fun_control[\"accelerator\"],\n            devices=fun_control[\"devices\"],\n            strategy=fun_control[\"strategy\"],\n            num_nodes=fun_control[\"num_nodes\"],\n            precision=fun_control[\"precision\"],\n            logger=TensorBoardLogger(\n                save_dir=fun_control[\"TENSORBOARD_PATH\"],\n                version=config_id,\n                default_hp_metric=True,\n                log_graph=fun_control[\"log_graph\"],\n            ),\n            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)],\n            enable_progress_bar=enable_progress_bar,\n        )\n        # Pass the datamodule as arg to trainer.fit to override model hooks :)\n        trainer.fit(model=model, datamodule=dm)\n        # Test best model on validation and test set\n        # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\n        verbose = fun_control[\"verbosity\"] &gt; 0\n        score = trainer.validate(model=model, datamodule=dm, verbose=verbose)\n        # unlist the result (from a list of one dict)\n        score = score[0]\n        print(f\"train_model result: {score}\")\n\n        results.append(score[\"val_loss\"])\n\n    score = sum(results) / num_folds\n    # print(f\"cv_model mapk result: {mapk_score}\")\n    return score\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/","title":"litmodel","text":""},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel","title":"<code>LitModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a simple neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>act_fn</code> <code>str</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>learning_rate</code> <code>float</code> <p>The learning rate for the optimizer.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>model</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; lit_model = LitModel(l1=128, epochs=10, batch_size=BATCH_SIZE, act_fn='relu', optimizer='adam')\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(lit_model, train_loader)\n</code></pre> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>class LitModel(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a simple neural network model.\n\n    Attributes:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        act_fn (str): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        learning_rate (float): The learning rate for the optimizer.\n        _L_in (int): The number of input features.\n        _L_out (int): The number of output classes.\n        model (nn.Sequential): The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; lit_model = LitModel(l1=128, epochs=10, batch_size=BATCH_SIZE, act_fn='relu', optimizer='adam')\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(lit_model, train_loader)\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        act_fn: str,\n        optimizer: str,\n        learning_rate: float = 2e-4,\n        _L_in: int = 28 * 28,\n        _L_out: int = 10,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the LitModel object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            act_fn (str): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            learning_rate (float, optional): The learning rate for the optimizer. Defaults to 2e-4.\n            _L_in (int, optional): The number of input features. Defaults to 28 * 28.\n            _L_out (int, optional): The number of output classes. Defaults to 10.\n\n        Returns:\n           (NoneType): None\n        \"\"\"\n        super().__init__()\n\n        # We take in input dimensions as parameters and use those to dynamically build model.\n        self._L_out = _L_out\n        self.l1 = l1\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.act_fn = act_fn\n        self.optimizer = optimizer\n        self.learning_rate = learning_rate\n\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(_L_in, l1),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(l1, l1),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(l1, _L_out),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the log probabilities for each class.\n        \"\"\"\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch: A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int) -&gt; None:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n\n        Returns:\n            None\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def test_step(self, batch: tuple, batch_idx: int) -&gt; tuple:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n\n        Returns:\n            tuple: A tuple containing the loss and accuracy for this batch.\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n        return loss, acc\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n        \"\"\"\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.__init__","title":"<code>__init__(l1, epochs, batch_size, act_fn, optimizer, learning_rate=0.0002, _L_in=28 * 28, _L_out=10, *args, **kwargs)</code>","text":"<p>Initializes the LitModel object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>act_fn</code> <code>str</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate for the optimizer. Defaults to 2e-4.</p> <code>0.0002</code> <code>_L_in</code> <code>int</code> <p>The number of input features. Defaults to 28 * 28.</p> <code>28 * 28</code> <code>_L_out</code> <code>int</code> <p>The number of output classes. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    act_fn: str,\n    optimizer: str,\n    learning_rate: float = 2e-4,\n    _L_in: int = 28 * 28,\n    _L_out: int = 10,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the LitModel object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        act_fn (str): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        learning_rate (float, optional): The learning rate for the optimizer. Defaults to 2e-4.\n        _L_in (int, optional): The number of input features. Defaults to 28 * 28.\n        _L_out (int, optional): The number of output classes. Defaults to 10.\n\n    Returns:\n       (NoneType): None\n    \"\"\"\n    super().__init__()\n\n    # We take in input dimensions as parameters and use those to dynamically build model.\n    self._L_out = _L_out\n    self.l1 = l1\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.act_fn = act_fn\n    self.optimizer = optimizer\n    self.learning_rate = learning_rate\n\n    self.model = nn.Sequential(\n        nn.Flatten(),\n        nn.Linear(_L_in, l1),\n        nn.ReLU(),\n        nn.Dropout(0.1),\n        nn.Linear(l1, l1),\n        nn.ReLU(),\n        nn.Dropout(0.1),\n        nn.Linear(l1, _L_out),\n    )\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n    \"\"\"\n    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the log probabilities for each class.</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the log probabilities for each class.\n    \"\"\"\n    x = self.model(x)\n    return F.log_softmax(x, dim=1)\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the loss and accuracy for this batch.</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int) -&gt; tuple:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n\n    Returns:\n        tuple: A tuple containing the loss and accuracy for this batch.\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.nll_loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n    self.log(\"val_loss\", loss, prog_bar=True)\n    self.log(\"val_acc\", acc, prog_bar=True)\n    return loss, acc\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch: A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.nll_loss(logits, y)\n    return loss\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int) -&gt; None:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n\n    Returns:\n        None\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.nll_loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n    self.log(\"val_loss\", loss, prog_bar=True)\n    self.log(\"val_acc\", acc, prog_bar=True)\n</code></pre>"},{"location":"reference/spotpython/light/loadmodel/","title":"loadmodel","text":""},{"location":"reference/spotpython/light/loadmodel/#spotpython.light.loadmodel.load_light_from_checkpoint","title":"<code>load_light_from_checkpoint(config, fun_control, postfix='_TEST')</code>","text":"<p>Loads a model from a checkpoint using the given configuration and function control parameters.</p> Notes <ul> <li><code>load_light_from_checkpoint</code> loads the last checkpoint of the model</li> <li>Randomness, dropout, etc\u2026 are disabled.</li> </ul> References <ul> <li>https://pytorch-lightning.readthedocs.io/en/0.8.5/weights_loading.html</li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <code>postfix</code> <code>str</code> <p>The postfix to append to the configuration ID when generating the checkpoint path. Default is \u201c_TEST\u201d. Can be set to \u201c_TRAIN\u201d for training checkpoints.</p> <code>'_TEST'</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The loaded model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"_torchmetric\": \"mean_squared_error\",\n...     \"core_model\": MyModel,\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; model = load_light_from_checkpoint(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/loadmodel.py</code> <pre><code>def load_light_from_checkpoint(config: dict, fun_control: dict, postfix: str = \"_TEST\") -&gt; Any:\n    \"\"\"\n    Loads a model from a checkpoint using the given configuration and function control parameters.\n\n    Notes:\n        * `load_light_from_checkpoint` loads the last checkpoint of the model\n        * Randomness, dropout, etc... are disabled.\n\n    References:\n        * https://pytorch-lightning.readthedocs.io/en/0.8.5/weights_loading.html\n\n    Args:\n        config (dict):\n            A dictionary containing the configuration parameters for the model.\n        fun_control (dict):\n            A dictionary containing the function control parameters.\n        postfix (str):\n            The postfix to append to the configuration ID when generating the checkpoint path.\n            Default is \"_TEST\". Can be set to \"_TRAIN\" for training checkpoints.\n\n    Returns:\n        Any: The loaded model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"_torchmetric\": \"mean_squared_error\",\n        ...     \"core_model\": MyModel,\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; model = load_light_from_checkpoint(config, fun_control)\n    \"\"\"\n    print(f\"config: {config}\")\n    # load a model from a checkpoint with the same config_id\n    # that was used in the test phase. Therefore, no timestamp is added.\n    config_id = generate_config_id(config, timestamp=False) + postfix\n    default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id, \"last.ckpt\")\n    print(f\"Loading model with {config_id} from {default_root_dir}\")\n    model = fun_control[\"core_model\"].load_from_checkpoint(\n        default_root_dir,\n        _L_in=fun_control[\"_L_in\"],\n        _L_out=fun_control[\"_L_out\"],\n        _L_cond=fun_control[\"_L_cond\"],\n        _torchmetric=fun_control[\"_torchmetric\"],\n    )\n    # disable randomness, dropout, etc...\n    print(f\"Model: {model}\")\n    model.eval()\n    return model\n</code></pre>"},{"location":"reference/spotpython/light/predictmodel/","title":"predictmodel","text":""},{"location":"reference/spotpython/light/predictmodel/#spotpython.light.predictmodel.predict_model","title":"<code>predict_model(config, fun_control)</code>","text":"<p>Predicts using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: The validation loss and the hyperparameter metric of the tested model.</p> Notes <ul> <li><code>test_model</code> saves the last checkpoint of the model from the training phase, which is called as follows:     <code>trainer.fit(model=model, datamodule=dm)</code>.</li> <li>The test result is evaluated with the following function call: <code>trainer.test(datamodule=dm, ckpt_path=\"last\")</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n     from spotpython.light.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n      get_default_hyperparameters_as_array)\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.hyperparameters.values import (get_var_name, assign_values,\n        generate_one_config_from_var_dict)\n    import spotpython.light.testmodel as tm\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,)\n    dataset = Diabetes()\n    set_control_key_value(control_dict=fun_control,\n                            key=\"data_set\",\n                            value=dataset)\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    var_dict = assign_values(X, get_var_name(fun_control))\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        y_test = tm.test_model(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/predictmodel.py</code> <pre><code>def predict_model(config: dict, fun_control: dict) -&gt; Tuple[float, float]:\n    \"\"\"\n    Predicts using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        Tuple[float, float]: The validation loss and the hyperparameter metric of the tested model.\n\n    Notes:\n        * `test_model` saves the last checkpoint of the model from the training phase, which is called as follows:\n            `trainer.fit(model=model, datamodule=dm)`.\n        * The test result is evaluated with the following function call:\n        `trainer.test(datamodule=dm, ckpt_path=\"last\")`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n             from spotpython.light.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n              get_default_hyperparameters_as_array)\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.hyperparameters.values import (get_var_name, assign_values,\n                generate_one_config_from_var_dict)\n            import spotpython.light.testmodel as tm\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,)\n            dataset = Diabetes()\n            set_control_key_value(control_dict=fun_control,\n                                    key=\"data_set\",\n                                    value=dataset)\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            var_dict = assign_values(X, get_var_name(fun_control))\n            for config in generate_one_config_from_var_dict(var_dict, fun_control):\n                y_test = tm.test_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _L_cond = fun_control[\"_L_cond\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"TEST\" postfix to config_id\n    # config id is unique. Since the model is loaded from a checkpoint,\n    # the config id is generated here without a timestamp. This differs from\n    # the config id generated in cvmodel.py and trainmodel.py.\n    config_id = generate_config_id(config, timestamp=False) + \"_TEST\"\n    dm = LightDataModule(\n        dataset=fun_control[\"data_set\"],\n        batch_size=config[\"batch_size\"],\n        num_workers=fun_control[\"num_workers\"],\n        test_size=fun_control[\"test_size\"],\n        test_seed=fun_control[\"test_seed\"],\n        scaler=fun_control[\"scaler\"],\n    )\n    # TODO: Check if this is necessary:\n    # dm.setup(stage=\"train\")\n    # Init model from datamodule's attributes\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=fun_control[\"accelerator\"],\n        devices=fun_control[\"devices\"],\n        strategy=fun_control[\"strategy\"],\n        num_nodes=fun_control[\"num_nodes\"],\n        precision=fun_control[\"precision\"],\n        logger=TensorBoardLogger(\n            save_dir=fun_control[\"TENSORBOARD_PATH\"],\n            version=config_id,\n            default_hp_metric=True,\n            log_graph=fun_control[\"log_graph\"],\n        ),\n        callbacks=[\n            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False),\n            ModelCheckpoint(dirpath=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id), save_last=True),  # Save the last checkpoint\n        ],\n        enable_progress_bar=enable_progress_bar,\n    )\n    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n    trainer.fit(model=model, datamodule=dm)\n\n    dm.setup(stage=\"predict\")\n    predictions = trainer.predict(model=model, datamodule=dm)\n    # predictions = trainer.predict(datamodule=dm)\n\n    # # Load the last checkpoint\n    # test_result = trainer.test(datamodule=dm, ckpt_path=\"last\")\n    # test_result = test_result[0]\n    # print(f\"test_model result: {test_result}\")\n    return predictions\n</code></pre>"},{"location":"reference/spotpython/light/testmodel/","title":"testmodel","text":""},{"location":"reference/spotpython/light/testmodel/#spotpython.light.testmodel.test_model","title":"<code>test_model(config, fun_control)</code>","text":"<p>Tests a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: The validation loss and the hyperparameter metric of the tested model.</p> Notes <ul> <li><code>test_model</code> saves the last checkpoint of the model from the training phase, which is called as follows:     <code>trainer.fit(model=model, datamodule=dm)</code>.</li> <li>The test result is evaluated with the following function call: <code>trainer.test(datamodule=dm, ckpt_path=\"last\")</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n     from spotpython.light.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n      get_default_hyperparameters_as_array)\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.hyperparameters.values import (get_var_name, assign_values,\n        generate_one_config_from_var_dict)\n    import spotpython.light.testmodel as tm\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\")\n    dataset = Diabetes()\n    set_control_key_value(control_dict=fun_control,\n                            key=\"data_set\",\n                            value=dataset)\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    var_dict = assign_values(X, get_var_name(fun_control))\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        y_test = tm.test_model(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/testmodel.py</code> <pre><code>def test_model(config: dict, fun_control: dict) -&gt; Tuple[float, float]:\n    \"\"\"\n    Tests a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        Tuple[float, float]: The validation loss and the hyperparameter metric of the tested model.\n\n    Notes:\n        * `test_model` saves the last checkpoint of the model from the training phase, which is called as follows:\n            `trainer.fit(model=model, datamodule=dm)`.\n        * The test result is evaluated with the following function call:\n        `trainer.test(datamodule=dm, ckpt_path=\"last\")`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n             from spotpython.light.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n              get_default_hyperparameters_as_array)\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.hyperparameters.values import (get_var_name, assign_values,\n                generate_one_config_from_var_dict)\n            import spotpython.light.testmodel as tm\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\")\n            dataset = Diabetes()\n            set_control_key_value(control_dict=fun_control,\n                                    key=\"data_set\",\n                                    value=dataset)\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            var_dict = assign_values(X, get_var_name(fun_control))\n            for config in generate_one_config_from_var_dict(var_dict, fun_control):\n                y_test = tm.test_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _L_cond = fun_control[\"_L_cond\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"TEST\" postfix to config_id\n    # config id is unique. Since the model is loaded from a checkpoint,\n    # the config id is generated here without a timestamp. This differs from\n    # the config id generated in cvmodel.py and trainmodel.py.\n    config_id = generate_config_id(config, timestamp=False) + \"_TEST\"\n    dm = LightDataModule(\n        dataset=fun_control[\"data_set\"],\n        batch_size=config[\"batch_size\"],\n        num_workers=fun_control[\"num_workers\"],\n        test_size=fun_control[\"test_size\"],\n        test_seed=fun_control[\"test_seed\"],\n        scaler=fun_control[\"scaler\"],\n    )\n    # TODO: Check if this is necessary:\n    # dm.setup()\n    # Init model from datamodule's attributes\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=fun_control[\"accelerator\"],\n        devices=fun_control[\"devices\"],\n        strategy=fun_control[\"strategy\"],\n        num_nodes=fun_control[\"num_nodes\"],\n        precision=fun_control[\"precision\"],\n        logger=TensorBoardLogger(\n            save_dir=fun_control[\"TENSORBOARD_PATH\"],\n            version=config_id,\n            default_hp_metric=True,\n            log_graph=fun_control[\"log_graph\"],\n        ),\n        callbacks=[\n            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False),\n            ModelCheckpoint(dirpath=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id), save_last=True),  # Save the last checkpoint\n        ],\n        enable_progress_bar=enable_progress_bar,\n    )\n    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n    trainer.fit(model=model, datamodule=dm)\n    # Load the last checkpoint\n    test_result = trainer.test(datamodule=dm, ckpt_path=\"last\")\n    test_result = test_result[0]\n    print(f\"test_model result: {test_result}\")\n    return test_result[\"val_loss\"], test_result[\"hp_metric\"]\n</code></pre>"},{"location":"reference/spotpython/light/trainmodel/","title":"trainmodel","text":""},{"location":"reference/spotpython/light/trainmodel/#spotpython.light.trainmodel.build_model_instance","title":"<code>build_model_instance(config, fun_control)</code>","text":"<p>Builds the core model using the configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Model configuration parameters.</p> required <code>fun_control</code> <code>dict</code> <p>Function control parameters.</p> required <p>Returns:</p> Type Description <code>LightningModule</code> <p>The constructed core model.</p> Source code in <code>spotpython/light/trainmodel.py</code> <pre><code>def build_model_instance(config: dict, fun_control: dict) -&gt; L.LightningModule:\n    \"\"\"\n    Builds the core model using the configuration and function control parameters.\n\n    Args:\n        config (dict): Model configuration parameters.\n        fun_control (dict): Function control parameters.\n\n    Returns:\n        The constructed core model.\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _L_cond = fun_control[\"_L_cond\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    return fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n</code></pre>"},{"location":"reference/spotpython/light/trainmodel/#spotpython.light.trainmodel.generate_config_id_with_timestamp","title":"<code>generate_config_id_with_timestamp(config, timestamp)</code>","text":"<p>Generates a configuration ID based on the given config and timestamp flag.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration parameters.</p> required <code>timestamp</code> <code>bool</code> <p>Indicates whether to include a timestamp in the config ID.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated configuration ID.</p> Source code in <code>spotpython/light/trainmodel.py</code> <pre><code>def generate_config_id_with_timestamp(config: dict, timestamp: bool) -&gt; str:\n    \"\"\"\n    Generates a configuration ID based on the given config and timestamp flag.\n\n    Args:\n        config (dict): The configuration parameters.\n        timestamp (bool): Indicates whether to include a timestamp in the config ID.\n\n    Returns:\n        str: The generated configuration ID.\n    \"\"\"\n    if timestamp:\n        # config id is unique. Since the model is not loaded from a checkpoint,\n        # the config id is generated here with a timestamp.\n        config_id = generate_config_id(config, timestamp=True)\n    else:\n        # config id is not time-dependent and therefore unique,\n        # so that the model can be loaded from a checkpoint,\n        # the config id is generated here without a timestamp.\n        config_id = generate_config_id(config, timestamp=False) + \"_TRAIN\"\n    return config_id\n</code></pre>"},{"location":"reference/spotpython/light/trainmodel/#spotpython.light.trainmodel.train_model","title":"<code>train_model(config, fun_control, timestamp=True)</code>","text":"<p>Trains a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <code>timestamp</code> <code>bool</code> <p>A boolean value indicating whether to include a timestamp in the config id. Default is True. If False, the string \u201c_TRAIN\u201d is appended to the config id.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The validation loss of the trained model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from math import inf\n    import numpy as np\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.utils.eda import gen_design_table\n    from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n    from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n    from spotpython.light.trainmodel import train_model\n    import pprint\n    PREFIX=\"000\"\n    data_set = Diabetes()\n    fun_control = fun_control_init(\n        PREFIX=PREFIX,\n        save_experiment=True,\n        fun_evals=inf,\n        max_time=1,\n        data_set = data_set,\n        core_model_name=\"light.regression.NNLinearRegressor\",\n        hyperdict=LightHyperDict,\n        _L_in=10,\n        _L_out=1,\n        TENSORBOARD_CLEAN=True,\n        tensorboard_log=True,\n        seed=42,)\n    print(gen_design_table(fun_control))\n    X = get_default_hyperparameters_as_array(fun_control)\n    # set epochs to 2^8:\n    # X[0, 1] = 8\n    # set patience to 2^10:\n    # X[0, 7] = 10\n    print(f\"X: {X}\")\n    # combine X and X to a np.array with shape (2, n_hyperparams)\n    # so that two values are returned\n    X = np.vstack((X, X))\n    var_dict = assign_values(X, get_var_name(fun_control))\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        pprint.pprint(config)\n        y = train_model(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/trainmodel.py</code> <pre><code>def train_model(config: dict, fun_control: dict, timestamp: bool = True) -&gt; float:\n    \"\"\"\n    Trains a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict):\n            A dictionary containing the configuration parameters for the model.\n        fun_control (dict):\n            A dictionary containing the function control parameters.\n        timestamp (bool):\n            A boolean value indicating whether to include a timestamp in the config id. Default is True.\n            If False, the string \"_TRAIN\" is appended to the config id.\n\n    Returns:\n        float: The validation loss of the trained model.\n\n    Examples:\n        &gt;&gt;&gt; from math import inf\n            import numpy as np\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.utils.eda import gen_design_table\n            from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n            from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n            from spotpython.light.trainmodel import train_model\n            import pprint\n            PREFIX=\"000\"\n            data_set = Diabetes()\n            fun_control = fun_control_init(\n                PREFIX=PREFIX,\n                save_experiment=True,\n                fun_evals=inf,\n                max_time=1,\n                data_set = data_set,\n                core_model_name=\"light.regression.NNLinearRegressor\",\n                hyperdict=LightHyperDict,\n                _L_in=10,\n                _L_out=1,\n                TENSORBOARD_CLEAN=True,\n                tensorboard_log=True,\n                seed=42,)\n            print(gen_design_table(fun_control))\n            X = get_default_hyperparameters_as_array(fun_control)\n            # set epochs to 2^8:\n            # X[0, 1] = 8\n            # set patience to 2^10:\n            # X[0, 7] = 10\n            print(f\"X: {X}\")\n            # combine X and X to a np.array with shape (2, n_hyperparams)\n            # so that two values are returned\n            X = np.vstack((X, X))\n            var_dict = assign_values(X, get_var_name(fun_control))\n            for config in generate_one_config_from_var_dict(var_dict, fun_control):\n                pprint.pprint(config)\n                y = train_model(config, fun_control)\n    \"\"\"\n    config_id = generate_config_id_with_timestamp(config=config, timestamp=timestamp)\n    model = build_model_instance(config, fun_control)\n    dm = LightDataModule(\n        dataset=fun_control[\"data_set\"],\n        batch_size=config[\"batch_size\"],\n        num_workers=fun_control[\"num_workers\"],\n        test_size=fun_control[\"test_size\"],\n        test_seed=fun_control[\"test_seed\"],\n        scaler=fun_control[\"scaler\"],\n    )\n    # TODO: Check if this is necessary or if this is handled by the trainer\n    # dm.setup()\n    # print(f\"train_model(): Test set size: {len(dm.data_test)}\")\n    # print(f\"train_model(): Train set size: {len(dm.data_train)}\")\n    # print(f\"train_model(): Batch size: {config['batch_size']}\")\n\n    # Callbacks\n    #\n    # EarlyStopping:\n    # Stop training when a monitored quantity has stopped improving.\n    # The EarlyStopping callback runs at the end of every validation epoch by default.\n    # However, the frequency of validation can be modified by setting various parameters\n    # in the Trainer, for example check_val_every_n_epoch and val_check_interval.\n    # It must be noted that the patience parameter counts the number of validation checks\n    # with no improvement, and not the number of training epochs.\n    # Therefore, with parameters check_val_every_n_epoch=10 and patience=3,\n    # the trainer will perform at least 40 training epochs before being stopped.\n    # Args:\n    # - monitor:\n    #   Quantity to be monitored. Default: 'val_loss'.\n    # - patience:\n    #   Number of validation checks with no improvement after which training will be stopped.\n    #   In spotpython, this is a hyperparameter.\n    # - mode (str):\n    #   one of {min, max}. If save_top_k != 0, the decision to overwrite the current save file\n    #   is made based on either the maximization or the minimization of the monitored quantity.\n    #   For 'val_acc', this should be 'max', for 'val_loss' this should be 'min', etc.\n    # - strict:\n    #   Set to False.\n    # - verbose:\n    #   If True, prints a message to the logger.\n    #\n    # ModelCheckpoint:\n    # Save the model periodically by monitoring a quantity.\n    # Every metric logged with log() or log_dict() is a candidate for the monitor key.\n    # spotpython uses ModelCheckpoint if timestamp is set to False. In this case, the\n    # config_id has no timestamp and ends with the unique string \"_TRAIN\". This\n    # enables loading the model from a checkpoint, because the config_id is unique.\n    # Args:\n    # - dirpath:\n    #   Path to the directory where the checkpoints will be saved.\n    # - monitor (str):\n    #   Quantity to monitor.\n    #   By default it is None which saves a checkpoint only for the last epoch.\n    # - verbose (bool):\n    #   If True, prints a message to the logger.\n    # - save_last (Union[bool, Literal['link'], None]):\n    #   When True, saves a last.ckpt copy whenever a checkpoint file gets saved.\n    #   Can be set to 'link' on a local filesystem to create a symbolic link.\n    #   This allows accessing the latest checkpoint in a deterministic manner.\n    #   Default: None.\n\n    callbacks = [EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)]\n    if not timestamp:\n        # add ModelCheckpoint only if timestamp is False\n        dirpath = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id)\n        callbacks.append(ModelCheckpoint(dirpath=dirpath, monitor=None, verbose=False, save_last=True))  # Save the last checkpoint\n\n    # Tensorboard logger. The tensorboard is passed to the trainer.\n    # See: https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.TensorBoardLogger.html\n    # It uses the following arguments:\n    # Args:\n    # - save_dir:\n    #   Where to save logs. Can be specified via fun_control[\"TENSORBOARD_PATH\"]\n    # - name:\n    #   Experiment name. Defaults to 'default'.\n    #   If it is the empty string then no per-experiment subdirectory is used.\n    #   Changed in spotpython 0.17.2 to the empty string.\n    # - version:\n    #   Experiment version. If version is not specified the logger inspects the save directory\n    #   for existing versions, then automatically assigns the next available version.\n    #   If it is a string then it is used as the run-specific subdirectory name,\n    #   otherwise 'version_${version}' is used. spotpython uses the config_id as version.\n    # - log_graph (bool):\n    #   Adds the computational graph to tensorboard.\n    #   This requires that the user has defined the self.example_input_array\n    #   attribute in their model. Set in spotpython to fun_control[\"log_graph\"].\n    # - default_hp_metric (bool):\n    #   Enables a placeholder metric with key hp_metric when log_hyperparams is called\n    #   without a metric (otherwise calls to log_hyperparams without a metric are ignored).\n    #   spotpython sets this to True.\n\n    # Init trainer. See: https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer\n    # Args used by spotpython (there are more):\n    # - default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n    #   Default: os.getcwd(). Can be remote file paths such as s3://mybucket/path or \u2018hdfs://path/\u2019\n    # - max_epochs: Stop training once this number of epochs is reached.\n    #   Disabled by default (None).\n    #   If both max_epochs and max_steps are not specified, defaults to max_epochs = 1000.\n    #   To enable infinite training, set max_epochs = -1.\n    # - accelerator: Supports passing different accelerator types\n    #   (\u201ccpu\u201d, \u201cgpu\u201d, \u201ctpu\u201d, \u201chpu\u201d, \u201cmps\u201d, \u201cauto\u201d) as well as custom accelerator instances.\n    # - devices: The devices to use. Can be set to a positive number (int or str),\n    #   a sequence of device indices (list or str), the value -1 to indicate all available devices\n    #   should be used, or \"auto\" for automatic selection based on the chosen accelerator.\n    #   Default: \"auto\".\n    # - strategy: Supports different training strategies with aliases as well custom strategies.\n    #   Default: \"auto\".\n    # - num_nodes: Number of GPU nodes for distributed training. Default: 1.\n    # - precision: Double precision (64, \u201864\u2019 or \u201864-true\u2019), full precision (32, \u201832\u2019 or \u201832-true\u2019),\n    #   16bit mixed precision (16, \u201816\u2019, \u201816-mixed\u2019) or bfloat16 mixed precision (\u2018bf16\u2019, \u2018bf16-mixed\u2019).\n    #   Can be used on CPU, GPU, TPUs, or HPUs. Default: '32-true'.\n    # - logger: Logger (or iterable collection of loggers) for experiment tracking.\n    #   A True value uses the default TensorBoardLogger if it is installed, otherwise CSVLogger.\n    #   False will disable logging. If multiple loggers are provided, local files (checkpoints,\n    #   profiler traces, etc.) are saved in the log_dir of the first logger. Default: True.\n    # - callbacks: List of callbacks to enable during training.Default: None.\n    # - enable_progress_bar: If True, enables the progress bar.\n    #   Whether to enable to progress bar by default. Default: True.\n    # - num_sanity_val_steps:\n    #   Sanity check runs n validation batches before starting the training routine.\n    #   Set it to -1 to run all batches in all validation dataloaders. Default: 2.\n    # - log_every_n_steps:\n    #   How often to log within steps. Default: 50.\n    # - gradient_clip_val:\n    #   The value at which to clip gradients. Passing gradient_clip_val=None\n    #   disables gradient clipping. If using Automatic Mixed Precision (AMP),\n    #   the gradients will be unscaled before. Default: None.\n    # - gradient_clip_algorithm (str):\n    #   The gradient clipping algorithm to use.\n    #   Pass gradient_clip_algorithm=\"value\" to clip by value,\n    #   and gradient_clip_algorithm=\"norm\" to clip by norm.\n    #   By default it will be set to \"norm\".\n\n    enable_progress_bar = fun_control[\"enable_progress_bar\"] or False\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=fun_control[\"accelerator\"],\n        devices=fun_control[\"devices\"],\n        strategy=fun_control[\"strategy\"],\n        num_nodes=fun_control[\"num_nodes\"],\n        precision=fun_control[\"precision\"],\n        logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True, log_graph=fun_control[\"log_graph\"], name=\"\"),\n        callbacks=callbacks,\n        enable_progress_bar=enable_progress_bar,\n        num_sanity_val_steps=fun_control[\"num_sanity_val_steps\"],\n        log_every_n_steps=fun_control[\"log_every_n_steps\"],\n        gradient_clip_val=None,\n        gradient_clip_algorithm=\"norm\",\n    )\n\n    # Fit the model\n    # Args:\n    # - model: Model to fit\n    # - datamodule: A LightningDataModule that defines the train_dataloader\n    #   hook. Pass the datamodule as arg to trainer.fit to override model hooks # :)\n    # - ckpt_path: Path/URL of the checkpoint from which training is resumed.\n    #   Could also be one of two special keywords \"last\" and \"hpc\".\n    #   If there is no checkpoint file at the path, an exception is raised.\n    trainer.fit(model=model, datamodule=dm, ckpt_path=None)\n\n    # Test best model on validation and test set\n\n    verbose = fun_control[\"verbosity\"] &gt; 0\n\n    # Validate the model\n    # Perform one evaluation epoch over the validation set.\n    # Args:\n    # - model: The model to validate.\n    # - datamodule: A LightningDataModule that defines the val_dataloader hook.\n    # - verbose: If True, prints the validation results.\n    # - ckpt_path: Path to a specific checkpoint to load for validation.\n    #   Either \"best\", \"last\", \"hpc\" or path to the checkpoint you wish to validate.\n    #   If None and the model instance was passed, use the current weights.\n    #   Otherwise, the best model checkpoint from the previous trainer.fit call will\n    #   be loaded if a checkpoint callback is configured.\n    # Returns:\n    # - List of dictionaries with metrics logged during the validation phase,\n    #   e.g., in model- or callback hooks like validation_step() etc.\n    #   The length of the list corresponds to the number of validation dataloaders used.\n    result = trainer.validate(model=model, datamodule=dm, ckpt_path=None, verbose=verbose)\n\n    # unlist the result (from a list of one dict)\n    result = result[0]\n    print(f\"train_model result: {result}\")\n    return result[\"val_loss\"]\n</code></pre>"},{"location":"reference/spotpython/light/cifar10/cifar10datamodule/","title":"cifar10datamodule","text":""},{"location":"reference/spotpython/light/cifar10/cifar10datamodule/#spotpython.light.cifar10.cifar10datamodule.CIFAR10DataModule","title":"<code>CIFAR10DataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for handling CIFAR10 data.</p> Torchvision provides many built-in datasets in the torchvision.datasets module, <p>as well as utility classes for building your own datasets. All datasets are subclasses of torch.utils.data.Dataset i.e, they have getitem and len methods implemented. Hence, they can all be passed to a torch.utils.data.DataLoader which can load multiple samples in parallel using torch.multiprocessing workers, see [1].</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The size of the batch.</p> required <code>data_dir</code> <code>str</code> <p>The directory where the data is stored. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <code>num_workers</code> <code>int</code> <p>The number of workers for data loading. Defaults to 0.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>data_train</code> <code>Dataset</code> <p>The training dataset.</p> <code>data_val</code> <code>Dataset</code> <p>The validation dataset.</p> <code>data_test</code> <code>Dataset</code> <p>The test dataset.</p> References <p>[1] https://pytorch.org/vision/stable/datasets.html</p> Source code in <code>spotpython/light/cifar10/cifar10datamodule.py</code> <pre><code>class CIFAR10DataModule(pl.LightningDataModule):\n    \"\"\"\n    A LightningDataModule for handling CIFAR10 data.\n\n    Note: Torchvision provides many built-in datasets in the torchvision.datasets module,\n        as well as utility classes for building your own datasets. All datasets are subclasses\n        of torch.utils.data.Dataset i.e, they have __getitem__ and __len__ methods implemented.\n        Hence, they can all be passed to a torch.utils.data.DataLoader which can load multiple\n        samples in parallel using torch.multiprocessing workers, see [1].\n\n    Args:\n        batch_size (int): The size of the batch.\n        data_dir (str): The directory where the data is stored. Defaults to \"./data\".\n        num_workers (int): The number of workers for data loading. Defaults to 0.\n\n    Attributes:\n        data_train (Dataset): The training dataset.\n        data_val (Dataset): The validation dataset.\n        data_test (Dataset): The test dataset.\n\n    References:\n        [1] [https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html)\n    \"\"\"\n\n    def __init__(self, batch_size: int, data_dir: str = \"./data\", num_workers: int = 0):\n        super().__init__()\n        self.batch_size = batch_size\n        self.data_dir = data_dir\n        self.num_workers = num_workers\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Prepares the data for use.\"\"\"\n        # download\n        CIFAR10(root=self.data_dir, train=True, download=True)\n        CIFAR10(root=self.data_dir, train=False, download=True)\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Sets up the data for use.\n\n        Args:\n            stage (Optional[str]): The current stage. Defaults to None.\n\n        \"\"\"\n        # Assign appropriate data transforms, see\n        # https://lightning.ai/docs/pytorch/latest/notebooks/course_UvA-DL/04-inception-resnet-densenet.html\n        DATA_MEANS = (0.49139968, 0.48215841, 0.44653091)\n        DATA_STDS = (0.24703223, 0.24348513, 0.26158784)\n        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(DATA_MEANS, DATA_STDS)])\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            data_full = CIFAR10(root=self.data_dir, train=True, transform=transform)\n            test_abs = int(len(data_full) * 0.6)\n            self.data_train, self.data_val = random_split(data_full, [test_abs, len(data_full) - test_abs])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.data_test = CIFAR10(root=self.data_dir, train=False, transform=transform)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the training dataloader.\n\n        Returns:\n            DataLoader: The training dataloader.\n\n        \"\"\"\n        print(\"train_dataloader: self.batch_size\", self.batch_size)\n        return DataLoader(self.data_train, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=self.num_workers)\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the validation dataloader.\n\n        Returns:\n            DataLoader: The validation dataloader.\n\n\n        \"\"\"\n        print(\"val_dataloader: self.batch_size\", self.batch_size)\n        return DataLoader(self.data_val, batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.num_workers)\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the test dataloader.\n\n        Returns:\n            DataLoader: The test dataloader.\n\n\n        \"\"\"\n        print(\"train_data_loader: self.batch_size\", self.batch_size)\n        return DataLoader(self.data_test, batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/light/cifar10/cifar10datamodule/#spotpython.light.cifar10.cifar10datamodule.CIFAR10DataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepares the data for use.</p> Source code in <code>spotpython/light/cifar10/cifar10datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Prepares the data for use.\"\"\"\n    # download\n    CIFAR10(root=self.data_dir, train=True, download=True)\n    CIFAR10(root=self.data_dir, train=False, download=True)\n</code></pre>"},{"location":"reference/spotpython/light/cifar10/cifar10datamodule/#spotpython.light.cifar10.cifar10datamodule.CIFAR10DataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Sets up the data for use.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>The current stage. Defaults to None.</p> <code>None</code> Source code in <code>spotpython/light/cifar10/cifar10datamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Sets up the data for use.\n\n    Args:\n        stage (Optional[str]): The current stage. Defaults to None.\n\n    \"\"\"\n    # Assign appropriate data transforms, see\n    # https://lightning.ai/docs/pytorch/latest/notebooks/course_UvA-DL/04-inception-resnet-densenet.html\n    DATA_MEANS = (0.49139968, 0.48215841, 0.44653091)\n    DATA_STDS = (0.24703223, 0.24348513, 0.26158784)\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(DATA_MEANS, DATA_STDS)])\n    # Assign train/val datasets for use in dataloaders\n    if stage == \"fit\" or stage is None:\n        data_full = CIFAR10(root=self.data_dir, train=True, transform=transform)\n        test_abs = int(len(data_full) * 0.6)\n        self.data_train, self.data_val = random_split(data_full, [test_abs, len(data_full) - test_abs])\n\n    # Assign test dataset for use in dataloader(s)\n    if stage == \"test\" or stage is None:\n        self.data_test = CIFAR10(root=self.data_dir, train=False, transform=transform)\n</code></pre>"},{"location":"reference/spotpython/light/cifar10/cifar10datamodule/#spotpython.light.cifar10.cifar10datamodule.CIFAR10DataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The test dataloader.</p> Source code in <code>spotpython/light/cifar10/cifar10datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the test dataloader.\n\n    Returns:\n        DataLoader: The test dataloader.\n\n\n    \"\"\"\n    print(\"train_data_loader: self.batch_size\", self.batch_size)\n    return DataLoader(self.data_test, batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/light/cifar10/cifar10datamodule/#spotpython.light.cifar10.cifar10datamodule.CIFAR10DataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The training dataloader.</p> Source code in <code>spotpython/light/cifar10/cifar10datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the training dataloader.\n\n    Returns:\n        DataLoader: The training dataloader.\n\n    \"\"\"\n    print(\"train_dataloader: self.batch_size\", self.batch_size)\n    return DataLoader(self.data_train, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/light/cifar10/cifar10datamodule/#spotpython.light.cifar10.cifar10datamodule.CIFAR10DataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The validation dataloader.</p> Source code in <code>spotpython/light/cifar10/cifar10datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the validation dataloader.\n\n    Returns:\n        DataLoader: The validation dataloader.\n\n\n    \"\"\"\n    print(\"val_dataloader: self.batch_size\", self.batch_size)\n    return DataLoader(self.data_val, batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/","title":"netlightbasemapk","text":""},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK","title":"<code>NetLightBaseMAPK</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS,\n                       train=True,\n                       download=True,\n                       transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data,\n                              batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                  epochs=10,\n                                  batch_size=BATCH_SIZE,\n                                  initialization='xavier',\n                                  act_fn=nn.ReLU(),\n                                  optimizer='Adam',\n                                  dropout_prob=0.1,\n                                  lr_mult=0.1,\n                                  patience=5)\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n</code></pre> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>class NetLightBaseMAPK(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a neural network model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS,\n                               train=True,\n                               download=True,\n                               transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data,\n                                      batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                          epochs=10,\n                                          batch_size=BATCH_SIZE,\n                                          initialization='xavier',\n                                          act_fn=nn.ReLU(),\n                                          optimizer='Adam',\n                                          dropout_prob=0.1,\n                                          lr_mult=0.1,\n                                          patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the NetLightBase object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 4.\n\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n        if self.hparams.l1 &lt; 4:\n            raise ValueError(\"l1 must be at least 4\")\n\n        hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n        self.train_mapk = MAPK(k=3)\n        self.valid_mapk = MAPK(k=3)\n        self.test_mapk = MAPK(k=3)\n\n        # Create the network based on the specified hidden sizes\n        layers = []\n        layer_sizes = [self._L_in] + hidden_sizes\n        layer_size_last = layer_sizes[0]\n        for layer_size in layer_sizes[1:]:\n            layers += [\n                nn.Linear(layer_size_last, layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n            layer_size_last = layer_size\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the probabilities for each class.\n\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                              epochs=10,\n                                              batch_size=BATCH_SIZE,\n                                              initialization='xavier', act_fn=nn.ReLU(),\n                                              optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                              patience=5)\n\n        \"\"\"\n        x = self.layers(x)\n        return F.softmax(x, dim=1)\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = F.cross_entropy(logits, y)\n        # self.train_mapk(logits, y)\n        # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False):\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            (NoneType): None\n\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())\n            &gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, val_loader)\n\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = F.cross_entropy(logits, y)\n        # loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n        self.valid_mapk(logits, y)\n        self.log(\"valid_mapk\", self.valid_mapk, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", loss, prog_bar=prog_bar)\n        self.log(\"val_acc\", acc, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; tuple:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            tuple: A tuple containing the loss and accuracy for this batch.\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n        self.test_mapk(logits, y)\n        self.log(\"test_mapk\", self.test_mapk, on_step=True, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", loss, prog_bar=prog_bar)\n        self.log(\"val_acc\", acc, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n        return loss, acc\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.__init__","title":"<code>__init__(l1, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, *args, **kwargs)</code>","text":"<p>Initializes the NetLightBase object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 4.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                    initialization='xavier', act_fn=nn.ReLU(),\n                                    optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                    patience=5)\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n</code></pre> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the NetLightBase object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 4.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n    if self.hparams.l1 &lt; 4:\n        raise ValueError(\"l1 must be at least 4\")\n\n    hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n    self.train_mapk = MAPK(k=3)\n    self.valid_mapk = MAPK(k=3)\n    self.test_mapk = MAPK(k=3)\n\n    # Create the network based on the specified hidden sizes\n    layers = []\n    layer_sizes = [self._L_in] + hidden_sizes\n    layer_size_last = layer_sizes[0]\n    for layer_size in layer_sizes[1:]:\n        layers += [\n            nn.Linear(layer_size_last, layer_size),\n            self.hparams.act_fn,\n            nn.Dropout(self.hparams.dropout_prob),\n        ]\n        layer_size_last = layer_size\n    layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n    # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n    self.layers = nn.Sequential(*layers)\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n    return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the probabilities for each class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                  epochs=10,\n                                  batch_size=BATCH_SIZE,\n                                  initialization='xavier', act_fn=nn.ReLU(),\n                                  optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                  patience=5)\n</code></pre> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the probabilities for each class.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                          epochs=10,\n                                          batch_size=BATCH_SIZE,\n                                          initialization='xavier', act_fn=nn.ReLU(),\n                                          optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                          patience=5)\n\n    \"\"\"\n    x = self.layers(x)\n    return F.softmax(x, dim=1)\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the loss and accuracy for this batch.</p> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; tuple:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing the loss and accuracy for this batch.\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = F.cross_entropy(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n    self.test_mapk(logits, y)\n    self.log(\"test_mapk\", self.test_mapk, on_step=True, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", loss, prog_bar=prog_bar)\n    self.log(\"val_acc\", acc, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n    return loss, acc\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                    epochs=10,\n                                    batch_size=BATCH_SIZE,\n                                    initialization='xavier', act_fn=nn.ReLU(),\n                                    optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                    patience=5)\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n</code></pre> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                            epochs=10,\n                                            batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = F.cross_entropy(logits, y)\n    # self.train_mapk(logits, y)\n    # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False)\n    return loss\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())\n&gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                    epochs=10,\n                                    batch_size=BATCH_SIZE,\n                                    initialization='xavier', act_fn=nn.ReLU(),\n                                    optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                    patience=5)\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(net_light_base, val_loader)\n</code></pre> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False):\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())\n        &gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                            epochs=10,\n                                            batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, val_loader)\n\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = F.cross_entropy(logits, y)\n    # loss = F.nll_loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n    self.valid_mapk(logits, y)\n    self.log(\"valid_mapk\", self.valid_mapk, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", loss, prog_bar=prog_bar)\n    self.log(\"val_acc\", acc, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n</code></pre>"},{"location":"reference/spotpython/light/cnn/googlenet/","title":"googlenet","text":""},{"location":"reference/spotpython/light/cnn/googlenet/#spotpython.light.cnn.googlenet.GoogleNet","title":"<code>GoogleNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>GoogleNet architecture</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of classes for the classification task. Defaults to 10.</p> <code>10</code> <code>act_fn_name</code> <code>str</code> <p>Name of the activation function. Defaults to \u201crelu\u201d.</p> <code>'relu'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>hparams</code> <code>SimpleNamespace</code> <p>Namespace containing the hyperparameters.</p> <code>input_net</code> <code>Sequential</code> <p>Input network.</p> <code>inception_blocks</code> <code>Sequential</code> <p>Inception blocks.</p> <code>output_net</code> <code>Sequential</code> <p>Output network.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of the GoogleNet architecture</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.cnn.googlenet import GoogleNet\n    import torch\n    import torch.nn as nn\n    model = GoogleNet()\n    x = torch.randn(1, 3, 32, 32)\n    y = model(x)\n    y.shape\n    torch.Size([1, 10])\n</code></pre> Source code in <code>spotpython/light/cnn/googlenet.py</code> <pre><code>class GoogleNet(nn.Module):\n    \"\"\"GoogleNet architecture\n\n    Args:\n        num_classes (int):\n            Number of classes for the classification task. Defaults to 10.\n        act_fn_name (str):\n            Name of the activation function. Defaults to \"relu\".\n        **kwargs (Any):\n            Additional keyword arguments.\n\n    Attributes:\n        hparams (SimpleNamespace):\n            Namespace containing the hyperparameters.\n        input_net (nn.Sequential):\n            Input network.\n        inception_blocks (nn.Sequential):\n            Inception blocks.\n        output_net (nn.Sequential):\n            Output network.\n\n    Returns:\n        (torch.Tensor):\n            Output tensor of the GoogleNet architecture\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.cnn.googlenet import GoogleNet\n            import torch\n            import torch.nn as nn\n            model = GoogleNet()\n            x = torch.randn(1, 3, 32, 32)\n            y = model(x)\n            y.shape\n            torch.Size([1, 10])\n    \"\"\"\n\n    def __init__(self, num_classes: int = 10, act_fn_name: str = \"relu\", **kwargs):\n        super().__init__()\n        # TODO: Replace this by act_fn handlers specified in the config file:\n        act_fn_by_name = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"leakyrelu\": nn.LeakyReLU, \"gelu\": nn.GELU}\n        self.hparams = SimpleNamespace(num_classes=num_classes, act_fn_name=act_fn_name, act_fn=act_fn_by_name[act_fn_name])\n        self._create_network()\n        self._init_params()\n\n    def _create_network(self):\n        # A first convolution on the original image to scale up the channel size\n        self.input_net = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), self.hparams.act_fn())\n        # Stacking inception blocks\n        self.inception_blocks = nn.Sequential(\n            InceptionBlock(\n                64,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                64,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 24, \"3x3\": 48, \"5x5\": 12, \"max\": 12},\n                act_fn=self.hparams.act_fn,\n            ),\n            nn.MaxPool2d(3, stride=2, padding=1),  # 32x32 =&gt; 16x16\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 24, \"3x3\": 48, \"5x5\": 12, \"max\": 12},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 16, \"3x3\": 48, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 16, \"3x3\": 48, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 32, \"3x3\": 48, \"5x5\": 24, \"max\": 24},\n                act_fn=self.hparams.act_fn,\n            ),\n            nn.MaxPool2d(3, stride=2, padding=1),  # 16x16 =&gt; 8x8\n            InceptionBlock(\n                128,\n                c_red={\"3x3\": 48, \"5x5\": 16},\n                c_out={\"1x1\": 32, \"3x3\": 64, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                128,\n                c_red={\"3x3\": 48, \"5x5\": 16},\n                c_out={\"1x1\": 32, \"3x3\": 64, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n        )\n        # Mapping to classification output\n        self.output_net = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Linear(128, self.hparams.num_classes))\n\n    def _init_params(self):\n        # We should initialize the\n        # convolutions according to the activation function\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, nonlinearity=self.hparams.act_fn_name)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.input_net(x)\n        x = self.inception_blocks(x)\n        x = self.output_net(x)\n        return x\n</code></pre>"},{"location":"reference/spotpython/light/cnn/inceptionblock/","title":"inceptionblock","text":""},{"location":"reference/spotpython/light/cnn/inceptionblock/#spotpython.light.cnn.inceptionblock.InceptionBlock","title":"<code>InceptionBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Inception block as used in GoogLeNet.</p> Notes <p>Description from P. Lippe:INCEPTION, RESNET AND DENSENET An Inception block applies four convolution blocks separately on the same feature map: a 1x1, 3x3, and 5x5 convolution, and a max pool operation. This allows the network to look at the same data with different receptive fields. Of course, learning only 5x5 convolution would be theoretically more powerful. However, this is not only more computation and memory heavy but also tends to overfit much easier. The 1x1 convolutions are used to reduce the number of input channels to the 3x3 and 5x5 convolutions, which reduces the number of parameters and computation.</p> <p>Parameters:</p> Name Type Description Default <code>c_in</code> <code>int</code> <p>Number of input feature maps from the previous layers</p> required <code>c_red</code> <code>dict</code> <p>Dictionary with keys \u201c3x3\u201d and \u201c5x5\u201d specifying the output of the dimensionality reducing 1x1 convolutions</p> required <code>c_out</code> <code>dict</code> <p>Dictionary with keys \u201c1x1\u201d, \u201c3x3\u201d, \u201c5x5\u201d, and \u201cmax\u201d</p> required <code>act_fn</code> <code>Module</code> <p>Activation class constructor (e.g. nn.ReLU)</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.cnn.googlenet import InceptionBlock\n    import torch\n    import torch.nn as nn\n    block = InceptionBlock(3,\n                {\"3x3\": 32, \"5x5\": 16},\n                {\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8},\n                nn.ReLU)\n    x = torch.randn(1, 3, 32, 32)\n    y = block(x)\n    y.shape\n    torch.Size([1, 64, 32, 32])\n</code></pre> Source code in <code>spotpython/light/cnn/inceptionblock.py</code> <pre><code>class InceptionBlock(nn.Module):\n    \"\"\"\n    Inception block as used in GoogLeNet.\n\n    Notes:\n        Description from\n        [P. Lippe:INCEPTION, RESNET AND DENSENET](https://lightning.ai/docs/pytorch/stable/)\n        An Inception block applies four convolution blocks separately on the same feature map:\n        a 1x1, 3x3, and 5x5 convolution, and a max pool operation.\n        This allows the network to look at the same data with different receptive fields.\n        Of course, learning only 5x5 convolution would be theoretically more powerful.\n        However, this is not only more computation and memory heavy but also tends to overfit much easier.\n        The 1x1 convolutions are used to reduce the number of input channels to the 3x3 and 5x5 convolutions,\n        which reduces the number of parameters and computation.\n\n    Args:\n        c_in (int):\n            Number of input feature maps from the previous layers\n        c_red (dict):\n            Dictionary with keys \"3x3\" and \"5x5\" specifying\n            the output of the dimensionality reducing 1x1 convolutions\n        c_out (dict):\n            Dictionary with keys \"1x1\", \"3x3\", \"5x5\", and \"max\"\n        act_fn (nn.Module):\n            Activation class constructor (e.g. nn.ReLU)\n\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.cnn.googlenet import InceptionBlock\n            import torch\n            import torch.nn as nn\n            block = InceptionBlock(3,\n                        {\"3x3\": 32, \"5x5\": 16},\n                        {\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8},\n                        nn.ReLU)\n            x = torch.randn(1, 3, 32, 32)\n            y = block(x)\n            y.shape\n            torch.Size([1, 64, 32, 32])\n\n    \"\"\"\n\n    def __init__(self, c_in, c_red: dict, c_out: dict, act_fn):\n        super().__init__()\n\n        # 1x1 convolution branch\n        self.conv_1x1 = nn.Sequential(nn.Conv2d(c_in, c_out[\"1x1\"], kernel_size=1), nn.BatchNorm2d(c_out[\"1x1\"]), act_fn())\n\n        # 3x3 convolution branch\n        self.conv_3x3 = nn.Sequential(\n            nn.Conv2d(c_in, c_red[\"3x3\"], kernel_size=1),\n            nn.BatchNorm2d(c_red[\"3x3\"]),\n            act_fn(),\n            nn.Conv2d(c_red[\"3x3\"], c_out[\"3x3\"], kernel_size=3, padding=1),\n            nn.BatchNorm2d(c_out[\"3x3\"]),\n            act_fn(),\n        )\n\n        # 5x5 convolution branch\n        self.conv_5x5 = nn.Sequential(\n            nn.Conv2d(c_in, c_red[\"5x5\"], kernel_size=1),\n            nn.BatchNorm2d(c_red[\"5x5\"]),\n            act_fn(),\n            nn.Conv2d(c_red[\"5x5\"], c_out[\"5x5\"], kernel_size=5, padding=2),\n            nn.BatchNorm2d(c_out[\"5x5\"]),\n            act_fn(),\n        )\n\n        # Max-pool branch\n        self.max_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n            nn.Conv2d(c_in, c_out[\"max\"], kernel_size=1),\n            nn.BatchNorm2d(c_out[\"max\"]),\n            act_fn(),\n        )\n\n    def forward(self, x) -&gt; torch.Tensor:\n        x_1x1 = self.conv_1x1(x)\n        x_3x3 = self.conv_3x3(x)\n        x_5x5 = self.conv_5x5(x)\n        x_max = self.max_pool(x)\n        x_out = torch.cat([x_1x1, x_3x3, x_5x5, x_max], dim=1)\n        return x_out\n</code></pre>"},{"location":"reference/spotpython/light/cnn/netcnnbase/","title":"netcnnbase","text":""},{"location":"reference/spotpython/light/cnn/netcnnbase/#spotpython.light.cnn.netcnnbase.NetCNNBase","title":"<code>NetCNNBase</code>","text":"<p>               Bases: <code>LightningModule</code></p> Source code in <code>spotpython/light/cnn/netcnnbase.py</code> <pre><code>class NetCNNBase(L.LightningModule):\n    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n        \"\"\"\n        Initializes the CNN model.\n\n        Args:\n            model_name (str): name of the model.\n            model_hparams (dict): dictionary containing the hyperparameters for the model.\n            optimizer_name (str): name of the optimizer.\n            optimizer_hparams (dict): dictionary containing the hyperparameters for the optimizer.\n\n        Returns:\n            (object): model object.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light.cnn.netcnnbase import NetCNNBase\n                from spotpython.light.cnn.googlenet import GoogleNet\n                import torch\n                import torch.nn as nn\n                model_hparams = {\"c_in\": 3, \"c_out\": 10, \"act_fn\": nn.ReLU, \"optimizer_name\": \"Adam\"}\n                fun_control = {\"core_model\": GoogleNet}\n                model = NetCNNBase(model_hparams, fun_control)\n                x = torch.randn(1, 3, 32, 32)\n                y = model(x)\n                y.shape\n                torch.Size([1, 10])\n\n        \"\"\"\n        super().__init__()\n        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n        self.save_hyperparameters()\n        print(f\"model_hparams: {model_hparams}\")\n        print(f\"self.hparams: {self.hparams}\")\n        # Create model\n        self.model = self.create_model(model_name, model_hparams)\n        # self.model = fun_control[\"core_model\"](**model_hparams)\n        print(f\"self.model: {self.model}\")\n        # Create loss module\n        self.loss_module = nn.CrossEntropyLoss()\n        # Example input for visualizing the graph in Tensorboard\n        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n\n    def forward(self, imgs):\n        # Forward function that is run when visualizing the graph\n        return self.model(imgs)\n\n    def configure_optimizers(self):\n        if self.hparams.optimizer_name == \"Adam\":\n            optimizer = optim.AdamW(self.parameters(), **self.hparams.optimizer_hparams)\n        elif self.hparams.optimizer_name == \"SGD\":\n            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n        else:\n            assert False, f'Unknown optimizer: \"{self.hparams.optimizer_name}\"'\n\n        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n        return [optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx):\n        # \"batch\" is the output of the training data loader.\n        imgs, labels = batch\n        preds = self.model(imgs)\n        loss = self.loss_module(preds, labels)\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n\n        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n        self.log(\"train_loss\", loss)\n        return loss  # Return tensor to call \".backward\" on\n\n    def validation_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs).argmax(dim=-1)\n        acc = (labels == preds).float().mean()\n        # By default logs it per epoch (weighted average over batches)\n        self.log(\"val_acc\", acc)\n\n    def test_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs).argmax(dim=-1)\n        acc = (labels == preds).float().mean()\n        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n        self.log(\"test_acc\", acc)\n\n    def create_model(self, model_name, model_hparams):\n        print(\"create_model: Starting\")\n        print(f\"model_name: {model_name}\")\n        print(f\"model_hparams: {model_hparams}\")\n        model_dict = {\"GoogleNet\": GoogleNet}\n        if model_name in model_dict:\n            return model_dict[model_name](**model_hparams)\n        else:\n            assert False, f'Unknown model name \"{model_name}\". Available models are: {str(model_dict.keys())}'\n</code></pre>"},{"location":"reference/spotpython/light/cnn/netcnnbase/#spotpython.light.cnn.netcnnbase.NetCNNBase.__init__","title":"<code>__init__(model_name, model_hparams, optimizer_name, optimizer_hparams)</code>","text":"<p>Initializes the CNN model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>name of the model.</p> required <code>model_hparams</code> <code>dict</code> <p>dictionary containing the hyperparameters for the model.</p> required <code>optimizer_name</code> <code>str</code> <p>name of the optimizer.</p> required <code>optimizer_hparams</code> <code>dict</code> <p>dictionary containing the hyperparameters for the optimizer.</p> required <p>Returns:</p> Type Description <code>object</code> <p>model object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.cnn.netcnnbase import NetCNNBase\n    from spotpython.light.cnn.googlenet import GoogleNet\n    import torch\n    import torch.nn as nn\n    model_hparams = {\"c_in\": 3, \"c_out\": 10, \"act_fn\": nn.ReLU, \"optimizer_name\": \"Adam\"}\n    fun_control = {\"core_model\": GoogleNet}\n    model = NetCNNBase(model_hparams, fun_control)\n    x = torch.randn(1, 3, 32, 32)\n    y = model(x)\n    y.shape\n    torch.Size([1, 10])\n</code></pre> Source code in <code>spotpython/light/cnn/netcnnbase.py</code> <pre><code>def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n    \"\"\"\n    Initializes the CNN model.\n\n    Args:\n        model_name (str): name of the model.\n        model_hparams (dict): dictionary containing the hyperparameters for the model.\n        optimizer_name (str): name of the optimizer.\n        optimizer_hparams (dict): dictionary containing the hyperparameters for the optimizer.\n\n    Returns:\n        (object): model object.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.cnn.netcnnbase import NetCNNBase\n            from spotpython.light.cnn.googlenet import GoogleNet\n            import torch\n            import torch.nn as nn\n            model_hparams = {\"c_in\": 3, \"c_out\": 10, \"act_fn\": nn.ReLU, \"optimizer_name\": \"Adam\"}\n            fun_control = {\"core_model\": GoogleNet}\n            model = NetCNNBase(model_hparams, fun_control)\n            x = torch.randn(1, 3, 32, 32)\n            y = model(x)\n            y.shape\n            torch.Size([1, 10])\n\n    \"\"\"\n    super().__init__()\n    # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n    self.save_hyperparameters()\n    print(f\"model_hparams: {model_hparams}\")\n    print(f\"self.hparams: {self.hparams}\")\n    # Create model\n    self.model = self.create_model(model_name, model_hparams)\n    # self.model = fun_control[\"core_model\"](**model_hparams)\n    print(f\"self.model: {self.model}\")\n    # Create loss module\n    self.loss_module = nn.CrossEntropyLoss()\n    # Example input for visualizing the graph in Tensorboard\n    self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/","title":"netlightregression","text":""},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression","title":"<code>NetLightRegression</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a regression neural network model. This is a very simple basic class. An enhanced version of this class is available as nn_linear_regression.py in the same directory.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function. If <code>None</code>, then \u201cmean_squared_error\u201d is used.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.netlightregression import NetLightRegression\n    from torch import nn\n    import lightning as L\n    PATH_DATASETS = './data'\n    BATCH_SIZE = 8\n    dataset = Diabetes()\n    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    batch_x, batch_y = next(iter(train_loader))\n    print(batch_x.shape)\n    print(batch_y.shape)\n    net_light_base = NetLightRegression(l1=128,\n                                        epochs=10,\n                                        batch_size=BATCH_SIZE,\n                                        initialization='xavier',\n                                        act_fn=nn.ReLU(),\n                                        optimizer='Adam',\n                                        dropout_prob=0.1,\n                                        lr_mult=0.1,\n                                        patience=5,\n                                        _L_in=10,\n                                        _L_out=1)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n    trainer.fit(net_light_base, train_loader)\n    trainer.validate(net_light_base, val_loader)\n    trainer.test(net_light_base, test_loader)\n    | Name   | Type       | Params | In sizes | Out sizes\n    -------------------------------------------------------------\n    0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n    -------------------------------------------------------------\n    15.9 K    Trainable params\n    0         Non-trainable params\n    15.9 K    Total params\n    0.064     Total estimated model params size (MB)\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        Validate metric           DataLoader 0\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            hp_metric              29010.7734375\n            val_loss               29010.7734375\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        Test metric             DataLoader 0\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            hp_metric              29010.7734375\n            val_loss               29010.7734375\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    [{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n</code></pre> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>class NetLightRegression(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a regression neural network model.\n    This is a very simple basic class. An enhanced version of this class is available\n    as nn_linear_regression.py in the same directory.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.netlightregression import NetLightRegression\n            from torch import nn\n            import lightning as L\n            PATH_DATASETS = './data'\n            BATCH_SIZE = 8\n            dataset = Diabetes()\n            train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            batch_x, batch_y = next(iter(train_loader))\n            print(batch_x.shape)\n            print(batch_y.shape)\n            net_light_base = NetLightRegression(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier',\n                                                act_fn=nn.ReLU(),\n                                                optimizer='Adam',\n                                                dropout_prob=0.1,\n                                                lr_mult=0.1,\n                                                patience=5,\n                                                _L_in=10,\n                                                _L_out=1)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n            trainer.fit(net_light_base, train_loader)\n            trainer.validate(net_light_base, val_loader)\n            trainer.test(net_light_base, test_loader)\n            | Name   | Type       | Params | In sizes | Out sizes\n            -------------------------------------------------------------\n            0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n            -------------------------------------------------------------\n            15.9 K    Trainable params\n            0         Non-trainable params\n            15.9 K    Total params\n            0.064     Total estimated model params size (MB)\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Validate metric           DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Test metric             DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            [{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the NetLightRegression object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 4.\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n        if self.hparams.l1 &lt; 4:\n            raise ValueError(\"l1 must be at least 4\")\n\n        # TODO: Implement a hidden_sizes generator function.\n        # This function is implemented in the updadated version of this class which\n        # is available as nn_linear_regression.py in the same directory.\n        hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n        # n_low = _L_in // 4\n        # # ensure that n_high is larger than n_low\n        # n_high = max(self.hparams.l1, 2 * n_low)\n        # hidden_sizes = generate_div2_list(n_high, n_low)\n\n        # Create the network based on the specified hidden sizes\n        layers = []\n        layer_sizes = [self._L_in] + hidden_sizes\n        layer_size_last = layer_sizes[0]\n        for layer_size in layer_sizes[1:]:\n            layers += [\n                nn.Linear(layer_size_last, layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n            layer_size_last = layer_size\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the output of the model.\n\n        \"\"\"\n        x = self.layers(x)\n        return x\n\n    def _calculate_loss(self, batch):\n        \"\"\"\n        Calculate the loss for the given batch.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            mode (str, optional): The mode of the model. Defaults to \"train\".\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        loss = self.metric(y_hat, y)\n        return loss\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n        return val_loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single prediction step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the prediction for this batch.\n        \"\"\"\n        x, y = batch\n        yhat = self(x)\n        y = y.view(len(y), 1)\n        yhat = yhat.view(len(yhat), 1)\n        print(f\"Predict step x: {x}\")\n        print(f\"Predict step y: {y}\")\n        print(f\"Predict step y_hat: {yhat}\")\n        # pred_loss = F.mse_loss(y_hat, y)\n        # pred loss not registered\n        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n        # MisconfigurationException: You are trying to `self.log()`\n        # but the loop's result collection is not registered yet.\n        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n        return (x, y, yhat)\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.__init__","title":"<code>__init__(l1, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, _torchmetric, *args, **kwargs)</code>","text":"<p>Initializes the NetLightRegression object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <p>_torchmetric (str):     The metric to use for the loss function. If <code>None</code>,     then \u201cmean_squared_error\u201d is used.</p> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 4.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    _torchmetric: str,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the NetLightRegression object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n    _torchmetric (str):\n        The metric to use for the loss function. If `None`,\n        then \"mean_squared_error\" is used.\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 4.\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    if _torchmetric is None:\n        _torchmetric = \"mean_squared_error\"\n    self._torchmetric = _torchmetric\n    self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_in))\n    if self.hparams.l1 &lt; 4:\n        raise ValueError(\"l1 must be at least 4\")\n\n    # TODO: Implement a hidden_sizes generator function.\n    # This function is implemented in the updadated version of this class which\n    # is available as nn_linear_regression.py in the same directory.\n    hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n    # n_low = _L_in // 4\n    # # ensure that n_high is larger than n_low\n    # n_high = max(self.hparams.l1, 2 * n_low)\n    # hidden_sizes = generate_div2_list(n_high, n_low)\n\n    # Create the network based on the specified hidden sizes\n    layers = []\n    layer_sizes = [self._L_in] + hidden_sizes\n    layer_size_last = layer_sizes[0]\n    for layer_size in layer_sizes[1:]:\n        layers += [\n            nn.Linear(layer_size_last, layer_size),\n            self.hparams.act_fn,\n            nn.Dropout(self.hparams.dropout_prob),\n        ]\n        layer_size_last = layer_size\n    layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n    # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n    self.layers = nn.Sequential(*layers)\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n    return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the output of the model.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the output of the model.\n\n    \"\"\"\n    x = self.layers(x)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.predict_step","title":"<code>predict_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the prediction for this batch.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single prediction step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the prediction for this batch.\n    \"\"\"\n    x, y = batch\n    yhat = self(x)\n    y = y.view(len(y), 1)\n    yhat = yhat.view(len(yhat), 1)\n    print(f\"Predict step x: {x}\")\n    print(f\"Predict step y: {y}\")\n    print(f\"Predict step y_hat: {yhat}\")\n    # pred_loss = F.mse_loss(y_hat, y)\n    # pred loss not registered\n    # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n    # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n    # MisconfigurationException: You are trying to `self.log()`\n    # but the loop's result collection is not registered yet.\n    # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n    # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n    return (x, y, yhat)\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n    # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/","title":"nn_condnet_regressor","text":""},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor","title":"<code>NNCondNetRegressor</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a conditional neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>batch_norm</code> <code>bool</code> <p>Whether to use batch normalization or not.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_L_cond</code> <code>int</code> <p>The number of neurons in the conditional hidden layer.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function. If <code>None</code>, then \u201cmean_squared_error\u201d is used.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.light.regression import NNLinearRegressor\n    from torch import nn\n    import lightning as L\n    import torch\n    from torch.utils.data import TensorDataset\n    PATH_DATASETS = './data'\n    BATCH_SIZE = 128\n    # generate data\n    num_samples = 1_000\n    input_dim = 10\n    X = torch.randn(num_samples, input_dim)  # random data for example\n    Y = torch.randn(num_samples, 1)  # random target for example\n    data_set = TensorDataset(X, Y)\n    train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    batch_x, batch_y = next(iter(train_loader))\n    print(batch_x.shape)\n    print(batch_y.shape)\n    net_light_base = NNLinearRegressor(l1=128,\n                                    batch_norm=True,\n                                        epochs=10,\n                                        batch_size=BATCH_SIZE,\n                                        initialization='xavier',\n                                        act_fn=nn.ReLU(),\n                                        optimizer='Adam',\n                                        dropout_prob=0.1,\n                                        lr_mult=0.1,\n                                        patience=5,\n                                        _L_in=input_dim,\n                                        _L_out=1,\n                                        _torchmetric=\"mean_squared_error\",)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n    trainer.fit(net_light_base, train_loader)\n    # validation and test should give the same result, because the data is the same\n    trainer.validate(net_light_base, val_loader)\n    trainer.test(net_light_base, test_loader)\n        GPU available: True (mps), used: True\n        TPU available: False, using: 0 TPU cores\n        HPU available: False, using: 0 HPUs\n</code></pre> <pre><code>    | Name   | Type       | Params | Mode  | In sizes  | Out sizes\n    ----------------------------------------------------------------------\n    0 | layers | Sequential | 20.8 K | train | [128, 10] | [128, 1]\n    ----------------------------------------------------------------------\n    20.8 K    Trainable params\n    0         Non-trainable params\n    20.8 K    Total params\n    0.083     Total estimated model params size (MB)\n    69        Modules in train mode\n    0         Modules in eval mode\n    torch.Size([128, 10])\n    torch.Size([128, 1])\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503      Validate metric      \u2503       DataLoader 0        \u2503\n    \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n    \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n    \u2502         val_loss          \u2502     81.1978988647461      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503        Test metric        \u2503       DataLoader 0        \u2503\n    \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n    \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n    \u2502         val_loss          \u2502     81.1978988647461      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    [{'val_loss': 81.1978988647461, 'hp_metric': 81.1978988647461}]\n</code></pre> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>class NNCondNetRegressor(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a conditional neural network model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        batch_norm (bool):\n            Whether to use batch normalization or not.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _L_cond (int):\n            The number of neurons in the conditional hidden layer.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.light.regression import NNLinearRegressor\n            from torch import nn\n            import lightning as L\n            import torch\n            from torch.utils.data import TensorDataset\n            PATH_DATASETS = './data'\n            BATCH_SIZE = 128\n            # generate data\n            num_samples = 1_000\n            input_dim = 10\n            X = torch.randn(num_samples, input_dim)  # random data for example\n            Y = torch.randn(num_samples, 1)  # random target for example\n            data_set = TensorDataset(X, Y)\n            train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            batch_x, batch_y = next(iter(train_loader))\n            print(batch_x.shape)\n            print(batch_y.shape)\n            net_light_base = NNLinearRegressor(l1=128,\n                                            batch_norm=True,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier',\n                                                act_fn=nn.ReLU(),\n                                                optimizer='Adam',\n                                                dropout_prob=0.1,\n                                                lr_mult=0.1,\n                                                patience=5,\n                                                _L_in=input_dim,\n                                                _L_out=1,\n                                                _torchmetric=\"mean_squared_error\",)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n            trainer.fit(net_light_base, train_loader)\n            # validation and test should give the same result, because the data is the same\n            trainer.validate(net_light_base, val_loader)\n            trainer.test(net_light_base, test_loader)\n                GPU available: True (mps), used: True\n                TPU available: False, using: 0 TPU cores\n                HPU available: False, using: 0 HPUs\n\n                | Name   | Type       | Params | Mode  | In sizes  | Out sizes\n                ----------------------------------------------------------------------\n                0 | layers | Sequential | 20.8 K | train | [128, 10] | [128, 1]\n                ----------------------------------------------------------------------\n                20.8 K    Trainable params\n                0         Non-trainable params\n                20.8 K    Total params\n                0.083     Total estimated model params size (MB)\n                69        Modules in train mode\n                0         Modules in eval mode\n                torch.Size([128, 10])\n                torch.Size([128, 1])\n                \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n                \u2503      Validate metric      \u2503       DataLoader 0        \u2503\n                \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n                \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n                \u2502         val_loss          \u2502     81.1978988647461      \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n                \u2503        Test metric        \u2503       DataLoader 0        \u2503\n                \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n                \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n                \u2502         val_loss          \u2502     81.1978988647461      \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                [{'val_loss': 81.1978988647461, 'hp_metric': 81.1978988647461}]\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        batch_norm: bool,\n        _L_cond: int,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the NNLinearRegressor object.\n\n        Args:\n            l1 (int):\n                The number of neurons in the first hidden layer.\n            epochs (int):\n                The number of epochs to train the model for.\n            batch_size (int):\n                The batch size to use during training.\n            initialization (str):\n                The initialization method to use for the weights.\n            act_fn (nn.Module):\n                The activation function to use in the hidden layers.\n            optimizer (str):\n                The optimizer to use during training.\n            dropout_prob (float):\n                The probability of dropping out a neuron during training.\n            lr_mult (float):\n                The learning rate multiplier for the optimizer.\n            patience (int):\n                The number of epochs to wait before early stopping.\n            batch_norm (bool):\n                Whether to use batch normalization or not.\n            _L_cond (int):\n                The number of neurons in the conditional hidden layer.\n            _L_in (int):\n                The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int):\n                The number of output classes. Not a hyperparameter, but needed to create the network.\n            _torchmetric (str):\n                The metric to use for the loss function. If `None`,\n                then \"mean_squared_error\" is used.\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 4.\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_cond = _L_cond\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_cond, _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_cond\", \"_L_in\", \"_L_out\", \"_torchmetric\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_cond + self._L_in))\n        if self.hparams.l1 &lt; 4:\n            raise ValueError(\"l1 must be at least 4\")\n        hidden_sizes = self._get_hidden_sizes()\n\n        # Conditional Layer\n        self.cond_layer = ConditionalLayer(self._L_in, self._L_cond, self.hparams.l1)\n\n        if batch_norm:\n            # Add batch normalization layers\n            layers = []\n            layer_sizes = [self.hparams.l1] + hidden_sizes\n            for i in range(len(layer_sizes) - 1):\n                current_layer_size = layer_sizes[i]\n                next_layer_size = layer_sizes[i + 1]\n                layers += [\n                    nn.Linear(current_layer_size, next_layer_size),\n                    nn.BatchNorm1d(next_layer_size),\n                    self.hparams.act_fn,\n                    nn.Dropout(self.hparams.dropout_prob),\n                ]\n            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        else:\n            layers = []\n            layer_sizes = [self.hparams.l1] + hidden_sizes\n            for i in range(len(layer_sizes) - 1):\n                current_layer_size = layer_sizes[i]\n                next_layer_size = layer_sizes[i + 1]\n                layers += [\n                    nn.Linear(current_layer_size, next_layer_size),\n                    self.hparams.act_fn,\n                    nn.Dropout(self.hparams.dropout_prob),\n                ]\n            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n\n        # Wrap the layers into a sequential container\n        self.layers = nn.Sequential(*layers)\n\n        # Initialization (Xavier, Kaiming, or Default)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            if self.hparams.initialization == \"xavier_uniform\":\n                nn.init.xavier_uniform_(module.weight)\n            elif self.hparams.initialization == \"xavier_normal\":\n                nn.init.xavier_normal_(module.weight)\n            elif self.hparams.initialization == \"kaiming_uniform\":\n                nn.init.kaiming_uniform_(module.weight)\n            elif self.hparams.initialization == \"kaiming_normal\":\n                nn.init.kaiming_normal_(module.weight)\n            else:  # \"Default\"\n                nn.init.uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def _generate_div2_list(self, n, n_min) -&gt; list:\n        result = []\n        current = n\n        repeats = 1\n        max_repeats = 4\n        while current &gt;= n_min:\n            result.extend([current] * min(repeats, max_repeats))\n            current = current // 2\n            repeats = repeats + 1\n        return result\n\n    def _get_hidden_sizes(self):\n        n_low = self._L_in // 4\n        n_high = max(self.hparams.l1, 2 * n_low)\n        hidden_sizes = self._generate_div2_list(n_high, n_low)\n        return hidden_sizes\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the output of the model.\n\n        \"\"\"\n        # exxtract the condition from the input\n        condition = x[:, : self._L_cond]\n        x_1 = x[:, self._L_cond :]\n        x_1 = self.cond_layer(x_1, condition)\n        x_1 = self.layers(x_1)\n        return x_1\n\n    def _calculate_loss(self, batch):\n        \"\"\"\n        Calculate the loss for the given batch.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            mode (str, optional): The mode of the model. Defaults to \"train\".\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        loss = self.metric(y_hat, y)\n        return loss\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n        return val_loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single prediction step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the prediction for this batch.\n        \"\"\"\n        x, y = batch\n        yhat = self(x)\n        y = y.view(len(y), 1)\n        yhat = yhat.view(len(yhat), 1)\n        print(f\"Predict step x: {x}\")\n        print(f\"Predict step y: {y}\")\n        print(f\"Predict step y_hat: {yhat}\")\n        # pred_loss = F.mse_loss(y_hat, y)\n        # pred loss not registered\n        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n        # MisconfigurationException: You are trying to `self.log()`\n        # but the loop's result collection is not registered yet.\n        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n        return (x, y, yhat)\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n        num_milestones = 3  # Number of milestones to divide the epochs\n        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n        lr_scheduler_config = {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.__init__","title":"<code>__init__(l1, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, batch_norm, _L_cond, _L_in, _L_out, _torchmetric, *args, **kwargs)</code>","text":"<p>Initializes the NNLinearRegressor object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>batch_norm</code> <code>bool</code> <p>Whether to use batch normalization or not.</p> required <code>_L_cond</code> <code>int</code> <p>The number of neurons in the conditional hidden layer.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function. If <code>None</code>, then \u201cmean_squared_error\u201d is used.</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 4.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    batch_norm: bool,\n    _L_cond: int,\n    _L_in: int,\n    _L_out: int,\n    _torchmetric: str,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the NNLinearRegressor object.\n\n    Args:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        batch_norm (bool):\n            Whether to use batch normalization or not.\n        _L_cond (int):\n            The number of neurons in the conditional hidden layer.\n        _L_in (int):\n            The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int):\n            The number of output classes. Not a hyperparameter, but needed to create the network.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 4.\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_cond = _L_cond\n    self._L_in = _L_in\n    self._L_out = _L_out\n    if _torchmetric is None:\n        _torchmetric = \"mean_squared_error\"\n    self._torchmetric = _torchmetric\n    self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n    # _L_cond, _L_in and _L_out are not hyperparameters, but are needed to create the network\n    # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n    self.save_hyperparameters(ignore=[\"_L_cond\", \"_L_in\", \"_L_out\", \"_torchmetric\"])\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_cond + self._L_in))\n    if self.hparams.l1 &lt; 4:\n        raise ValueError(\"l1 must be at least 4\")\n    hidden_sizes = self._get_hidden_sizes()\n\n    # Conditional Layer\n    self.cond_layer = ConditionalLayer(self._L_in, self._L_cond, self.hparams.l1)\n\n    if batch_norm:\n        # Add batch normalization layers\n        layers = []\n        layer_sizes = [self.hparams.l1] + hidden_sizes\n        for i in range(len(layer_sizes) - 1):\n            current_layer_size = layer_sizes[i]\n            next_layer_size = layer_sizes[i + 1]\n            layers += [\n                nn.Linear(current_layer_size, next_layer_size),\n                nn.BatchNorm1d(next_layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n    else:\n        layers = []\n        layer_sizes = [self.hparams.l1] + hidden_sizes\n        for i in range(len(layer_sizes) - 1):\n            current_layer_size = layer_sizes[i]\n            next_layer_size = layer_sizes[i + 1]\n            layers += [\n                nn.Linear(current_layer_size, next_layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n\n    # Wrap the layers into a sequential container\n    self.layers = nn.Sequential(*layers)\n\n    # Initialization (Xavier, Kaiming, or Default)\n    self.apply(self._init_weights)\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n    num_milestones = 3  # Number of milestones to divide the epochs\n    milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n    lr_scheduler_config = {\n        \"scheduler\": scheduler,\n        \"interval\": \"epoch\",\n        \"frequency\": 1,\n    }\n\n    return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the output of the model.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the output of the model.\n\n    \"\"\"\n    # exxtract the condition from the input\n    condition = x[:, : self._L_cond]\n    x_1 = x[:, self._L_cond :]\n    x_1 = self.cond_layer(x_1, condition)\n    x_1 = self.layers(x_1)\n    return x_1\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.predict_step","title":"<code>predict_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the prediction for this batch.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single prediction step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the prediction for this batch.\n    \"\"\"\n    x, y = batch\n    yhat = self(x)\n    y = y.view(len(y), 1)\n    yhat = yhat.view(len(yhat), 1)\n    print(f\"Predict step x: {x}\")\n    print(f\"Predict step y: {y}\")\n    print(f\"Predict step y_hat: {yhat}\")\n    # pred_loss = F.mse_loss(y_hat, y)\n    # pred loss not registered\n    # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n    # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n    # MisconfigurationException: You are trying to `self.log()`\n    # but the loop's result collection is not registered yet.\n    # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n    # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n    return (x, y, yhat)\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n    # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/","title":"nn_linear_regressor","text":""},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor","title":"<code>NNLinearRegressor</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a regression neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>batch_norm</code> <code>bool</code> <p>Whether to use batch normalization or not.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function. If <code>None</code>, then \u201cmean_squared_error\u201d is used.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.light.regression import NNLinearRegressor\n    from torch import nn\n    import lightning as L\n    import torch\n    from torch.utils.data import TensorDataset\n    PATH_DATASETS = './data'\n    BATCH_SIZE = 128\n    # generate data\n    num_samples = 1_000\n    input_dim = 10\n    X = torch.randn(num_samples, input_dim)  # random data for example\n    Y = torch.randn(num_samples, 1)  # random target for example\n    data_set = TensorDataset(X, Y)\n    train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    batch_x, batch_y = next(iter(train_loader))\n    print(batch_x.shape)\n    print(batch_y.shape)\n    net_light_base = NNLinearRegressor(l1=128,\n                                    batch_norm=True,\n                                        epochs=10,\n                                        batch_size=BATCH_SIZE,\n                                        initialization='xavier',\n                                        act_fn=nn.ReLU(),\n                                        optimizer='Adam',\n                                        dropout_prob=0.1,\n                                        lr_mult=0.1,\n                                        patience=5,\n                                        _L_in=input_dim,\n                                        _L_out=1,\n                                        _torchmetric=\"mean_squared_error\",)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n    trainer.fit(net_light_base, train_loader)\n    # validation and test should give the same result, because the data is the same\n    trainer.validate(net_light_base, val_loader)\n    trainer.test(net_light_base, test_loader)\n        GPU available: True (mps), used: True\n        TPU available: False, using: 0 TPU cores\n        HPU available: False, using: 0 HPUs\n</code></pre> <pre><code>    | Name   | Type       | Params | Mode  | In sizes  | Out sizes\n    ----------------------------------------------------------------------\n    0 | layers | Sequential | 20.8 K | train | [128, 10] | [128, 1]\n    ----------------------------------------------------------------------\n    20.8 K    Trainable params\n    0         Non-trainable params\n    20.8 K    Total params\n    0.083     Total estimated model params size (MB)\n    69        Modules in train mode\n    0         Modules in eval mode\n    torch.Size([128, 10])\n    torch.Size([128, 1])\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503      Validate metric      \u2503       DataLoader 0        \u2503\n    \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n    \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n    \u2502         val_loss          \u2502     81.1978988647461      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503        Test metric        \u2503       DataLoader 0        \u2503\n    \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n    \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n    \u2502         val_loss          \u2502     81.1978988647461      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    [{'val_loss': 81.1978988647461, 'hp_metric': 81.1978988647461}]\n</code></pre> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>class NNLinearRegressor(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a regression neural network model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        batch_norm (bool):\n            Whether to use batch normalization or not.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.light.regression import NNLinearRegressor\n            from torch import nn\n            import lightning as L\n            import torch\n            from torch.utils.data import TensorDataset\n            PATH_DATASETS = './data'\n            BATCH_SIZE = 128\n            # generate data\n            num_samples = 1_000\n            input_dim = 10\n            X = torch.randn(num_samples, input_dim)  # random data for example\n            Y = torch.randn(num_samples, 1)  # random target for example\n            data_set = TensorDataset(X, Y)\n            train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            batch_x, batch_y = next(iter(train_loader))\n            print(batch_x.shape)\n            print(batch_y.shape)\n            net_light_base = NNLinearRegressor(l1=128,\n                                            batch_norm=True,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier',\n                                                act_fn=nn.ReLU(),\n                                                optimizer='Adam',\n                                                dropout_prob=0.1,\n                                                lr_mult=0.1,\n                                                patience=5,\n                                                _L_in=input_dim,\n                                                _L_out=1,\n                                                _torchmetric=\"mean_squared_error\",)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n            trainer.fit(net_light_base, train_loader)\n            # validation and test should give the same result, because the data is the same\n            trainer.validate(net_light_base, val_loader)\n            trainer.test(net_light_base, test_loader)\n                GPU available: True (mps), used: True\n                TPU available: False, using: 0 TPU cores\n                HPU available: False, using: 0 HPUs\n\n                | Name   | Type       | Params | Mode  | In sizes  | Out sizes\n                ----------------------------------------------------------------------\n                0 | layers | Sequential | 20.8 K | train | [128, 10] | [128, 1]\n                ----------------------------------------------------------------------\n                20.8 K    Trainable params\n                0         Non-trainable params\n                20.8 K    Total params\n                0.083     Total estimated model params size (MB)\n                69        Modules in train mode\n                0         Modules in eval mode\n                torch.Size([128, 10])\n                torch.Size([128, 1])\n                \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n                \u2503      Validate metric      \u2503       DataLoader 0        \u2503\n                \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n                \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n                \u2502         val_loss          \u2502     81.1978988647461      \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n                \u2503        Test metric        \u2503       DataLoader 0        \u2503\n                \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n                \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n                \u2502         val_loss          \u2502     81.1978988647461      \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                [{'val_loss': 81.1978988647461, 'hp_metric': 81.1978988647461}]\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        batch_norm: bool,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n        if self.hparams.l1 &lt; 4:\n            raise ValueError(\"l1 must be at least 4\")\n        hidden_sizes = get_hidden_sizes(_L_in=self._L_in, l1=l1, n=10)\n\n        if batch_norm:\n            # Add batch normalization layers\n            layers = []\n            layer_sizes = [self._L_in] + hidden_sizes\n            for i in range(len(layer_sizes) - 1):\n                current_layer_size = layer_sizes[i]\n                next_layer_size = layer_sizes[i + 1]\n                layers += [\n                    nn.Linear(current_layer_size, next_layer_size),\n                    nn.BatchNorm1d(next_layer_size),\n                    self.hparams.act_fn,\n                    nn.Dropout(self.hparams.dropout_prob),\n                ]\n            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        else:\n            layers = []\n            layer_sizes = [self._L_in] + hidden_sizes\n            for i in range(len(layer_sizes) - 1):\n                current_layer_size = layer_sizes[i]\n                next_layer_size = layer_sizes[i + 1]\n                layers += [\n                    nn.Linear(current_layer_size, next_layer_size),\n                    self.hparams.act_fn,\n                    nn.Dropout(self.hparams.dropout_prob),\n                ]\n            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n\n        # Wrap the layers into a sequential container\n        self.layers = nn.Sequential(*layers)\n\n        # Initialization (Xavier, Kaiming, or Default)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            if self.hparams.initialization == \"xavier_uniform\":\n                nn.init.xavier_uniform_(module.weight)\n            elif self.hparams.initialization == \"xavier_normal\":\n                nn.init.xavier_normal_(module.weight)\n            elif self.hparams.initialization == \"kaiming_uniform\":\n                nn.init.kaiming_uniform_(module.weight)\n            elif self.hparams.initialization == \"kaiming_normal\":\n                nn.init.kaiming_normal_(module.weight)\n            else:  # \"Default\"\n                nn.init.uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the output of the model.\n\n        \"\"\"\n        x = self.layers(x)\n        return x\n\n    def _calculate_loss(self, batch):\n        \"\"\"\n        Calculate the loss for the given batch.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            mode (str, optional): The mode of the model. Defaults to \"train\".\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        loss = self.metric(y_hat, y)\n        return loss\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        loss = self._calculate_loss(batch)\n        # self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        return loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        return loss\n\n    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single prediction step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the prediction for this batch.\n        \"\"\"\n        x, y = batch\n        yhat = self(x)\n        y = y.view(len(y), 1)\n        yhat = yhat.view(len(yhat), 1)\n        print(f\"Predict step x: {x}\")\n        print(f\"Predict step y: {y}\")\n        print(f\"Predict step y_hat: {yhat}\")\n        # pred_loss = F.mse_loss(y_hat, y)\n        # pred loss not registered\n        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n        # MisconfigurationException: You are trying to `self.log()`\n        # but the loop's result collection is not registered yet.\n        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n        return (x, y, yhat)\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n        num_milestones = 3  # Number of milestones to divide the epochs\n        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n        lr_scheduler_config = {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n    num_milestones = 3  # Number of milestones to divide the epochs\n    milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n    lr_scheduler_config = {\n        \"scheduler\": scheduler,\n        \"interval\": \"epoch\",\n        \"frequency\": 1,\n    }\n\n    return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the output of the model.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the output of the model.\n\n    \"\"\"\n    x = self.layers(x)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.predict_step","title":"<code>predict_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the prediction for this batch.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single prediction step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the prediction for this batch.\n    \"\"\"\n    x, y = batch\n    yhat = self(x)\n    y = y.view(len(y), 1)\n    yhat = yhat.view(len(yhat), 1)\n    print(f\"Predict step x: {x}\")\n    print(f\"Predict step y: {y}\")\n    print(f\"Predict step y_hat: {yhat}\")\n    # pred_loss = F.mse_loss(y_hat, y)\n    # pred loss not registered\n    # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n    # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n    # MisconfigurationException: You are trying to `self.log()`\n    # but the loop's result collection is not registered yet.\n    # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n    # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n    return (x, y, yhat)\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    return loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    loss = self._calculate_loss(batch)\n    # self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n    return loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    return loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_resnet_regressor/","title":"nn_resnet_regressor","text":""},{"location":"reference/spotpython/light/regression/nn_transformer_regressor/","title":"nn_transformer_regressor","text":""},{"location":"reference/spotpython/light/regression/pos_enc/","title":"pos_enc","text":""},{"location":"reference/spotpython/light/regression/rnnlightregression/","title":"rnnlightregression","text":""},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression","title":"<code>RNNLightRegression</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a RNN model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function, e.g., \u201cmean_squared_error\u201d.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.netlightregression import NetLightRegression\n    from torch import nn\n    import lightning as L\n    PATH_DATASETS = './data'\n    BATCH_SIZE = 8\n    dataset = Diabetes()\n    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    batch_x, batch_y = next(iter(train_loader))\n    print(batch_x.shape)\n    print(batch_y.shape)\n    net_light_base = NetLightRegression(l1=128,\n                                        epochs=10,\n                                        batch_size=BATCH_SIZE,\n                                        initialization='xavier',\n                                        act_fn=nn.ReLU(),\n                                        optimizer='Adam',\n                                        dropout_prob=0.1,\n                                        lr_mult=0.1,\n                                        patience=5,\n                                        _L_in=10,\n                                        _L_out=1,\n                                        _torchmetric=\"mean_squared_error\")\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n    trainer.fit(net_light_base, train_loader)\n    trainer.validate(net_light_base, val_loader)\n    trainer.test(net_light_base, test_loader)\n</code></pre> <pre><code>  | Name   | Type       | Params | In sizes | Out sizes\n-------------------------------------------------------------\n0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n-------------------------------------------------------------\n15.9 K    Trainable params\n0         Non-trainable params\n15.9 K    Total params\n0.064     Total estimated model params size (MB)\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Validate metric           DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        hp_metric              29010.7734375\n        val_loss               29010.7734375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        hp_metric              29010.7734375\n        val_loss               29010.7734375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n[{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n</code></pre> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>class RNNLightRegression(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a RNN model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _torchmetric (str):\n            The metric to use for the loss function, e.g., \"mean_squared_error\".\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.netlightregression import NetLightRegression\n            from torch import nn\n            import lightning as L\n            PATH_DATASETS = './data'\n            BATCH_SIZE = 8\n            dataset = Diabetes()\n            train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            batch_x, batch_y = next(iter(train_loader))\n            print(batch_x.shape)\n            print(batch_y.shape)\n            net_light_base = NetLightRegression(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier',\n                                                act_fn=nn.ReLU(),\n                                                optimizer='Adam',\n                                                dropout_prob=0.1,\n                                                lr_mult=0.1,\n                                                patience=5,\n                                                _L_in=10,\n                                                _L_out=1,\n                                                _torchmetric=\"mean_squared_error\")\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n            trainer.fit(net_light_base, train_loader)\n            trainer.validate(net_light_base, val_loader)\n            trainer.test(net_light_base, test_loader)\n\n              | Name   | Type       | Params | In sizes | Out sizes\n            -------------------------------------------------------------\n            0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n            -------------------------------------------------------------\n            15.9 K    Trainable params\n            0         Non-trainable params\n            15.9 K    Total params\n            0.064     Total estimated model params size (MB)\n\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Validate metric           DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Test metric             DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n            [{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the NetLightRegression object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n            _torchmetric (str): The metric to use for the loss function, e.g., \"mean_squared_error\".\n\n        Returns:\n            (NoneType): None\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n\n        # Initialize RNN\n        # input_size = number of features is defined via _L_in\n        # output size via _L_out\n        # num_layers=1: only a single RNN and not stacked\n        rnn_units = self.hparams.l1\n        fc_units = self.hparams.l1\n\n        # # TODO: make this a hyperparameter\n        rnn_nonlinearity = \"relu\"\n\n        self.rnn_layer = nn.RNN(\n            input_size=self._L_in,\n            hidden_size=rnn_units,\n            num_layers=1,\n            nonlinearity=rnn_nonlinearity,\n            bias=True,\n            batch_first=True,\n            bidirectional=False,\n        )\n\n        # # Initialize Hidden- and Output-Layer\n        self.fc = nn.Linear(rnn_units, fc_units)\n        self.output_layer = nn.Linear(fc_units, self._L_out)\n\n        # # Initialize Activation Function and Dropouts\n        # dropout = [0.2, 0, 0]\n        # self.dropout1 = nn.Dropout(dropout[0])\n        # self.dropout2 = nn.Dropout(dropout[1])\n        # self.dropout3 = nn.Dropout(dropout[2])\n        # # TODO: use enhanced dropout management for different layers\n        self.dropout1 = nn.Dropout(self.hparams.dropout_prob)\n        self.dropout2 = nn.Dropout(self.hparams.dropout_prob // 10.0)\n        self.dropout3 = nn.Dropout(self.hparams.dropout_prob // 100.0)\n\n        # TODO: Enable different activation functions\n        # activation_fct = nn.ReLU()\n        # self.activation_fct = activation_fct\n        self.activation_fct = self.hparams.act_fn\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the output of the model.\n\n        \"\"\"\n        # print(f\"input: {x.shape}\")\n        x = self.dropout1(x)\n        # print(f\"dropout1: {x.shape}\")\n        x, _ = self.rnn_layer(x)\n        # print(f\"rnn_layer: {x.shape}\")\n        # x = x[:, -1, :]\n        # print(f\"slicing: {x.shape}\")\n        x = self.dropout2(x)\n        # print(f\"dropout2: {x.shape}\")\n        x = self.activation_fct(self.fc(x))\n        # print(f\"activation_fct: {x.shape}\")\n        x = self.dropout3(x)\n        # print(f\"dropout3: {x.shape}\")\n        x = self.output_layer(x)\n        # print(f\"output_layer: {x.shape}\")\n        return x\n\n    def training_step(self, batch: tuple, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        # reshape the tensor y to be a column vector (len(y) rows and 1 column)\n        y = y.view(len(y), 1)\n        # Note: the number of rows in x is equal to the number of rows in y\n        y_hat = self(x)\n        # Note: the number of rows in y_hat is equal to the number of rows in y\n        # train_loss = F.mse_loss(y_hat, y)\n        metric = getattr(torchmetrics.functional.regression, self._torchmetric)\n        train_loss = metric(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n        return train_loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        # reshape the tensor y to be a column vector (len(y) rows and 1 column)\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        val_loss = F.mse_loss(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        val_loss = F.mse_loss(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.__init__","title":"<code>__init__(l1, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, _torchmetric, *args, **kwargs)</code>","text":"<p>Initializes the NetLightRegression object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function, e.g., \u201cmean_squared_error\u201d.</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    _torchmetric: str,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the NetLightRegression object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n        _torchmetric (str): The metric to use for the loss function, e.g., \"mean_squared_error\".\n\n    Returns:\n        (NoneType): None\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    if _torchmetric is None:\n        _torchmetric = \"mean_squared_error\"\n    self._torchmetric = _torchmetric\n    self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_in))\n\n    # Initialize RNN\n    # input_size = number of features is defined via _L_in\n    # output size via _L_out\n    # num_layers=1: only a single RNN and not stacked\n    rnn_units = self.hparams.l1\n    fc_units = self.hparams.l1\n\n    # # TODO: make this a hyperparameter\n    rnn_nonlinearity = \"relu\"\n\n    self.rnn_layer = nn.RNN(\n        input_size=self._L_in,\n        hidden_size=rnn_units,\n        num_layers=1,\n        nonlinearity=rnn_nonlinearity,\n        bias=True,\n        batch_first=True,\n        bidirectional=False,\n    )\n\n    # # Initialize Hidden- and Output-Layer\n    self.fc = nn.Linear(rnn_units, fc_units)\n    self.output_layer = nn.Linear(fc_units, self._L_out)\n\n    # # Initialize Activation Function and Dropouts\n    # dropout = [0.2, 0, 0]\n    # self.dropout1 = nn.Dropout(dropout[0])\n    # self.dropout2 = nn.Dropout(dropout[1])\n    # self.dropout3 = nn.Dropout(dropout[2])\n    # # TODO: use enhanced dropout management for different layers\n    self.dropout1 = nn.Dropout(self.hparams.dropout_prob)\n    self.dropout2 = nn.Dropout(self.hparams.dropout_prob // 10.0)\n    self.dropout3 = nn.Dropout(self.hparams.dropout_prob // 100.0)\n\n    # TODO: Enable different activation functions\n    # activation_fct = nn.ReLU()\n    # self.activation_fct = activation_fct\n    self.activation_fct = self.hparams.act_fn\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n    return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the output of the model.</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the output of the model.\n\n    \"\"\"\n    # print(f\"input: {x.shape}\")\n    x = self.dropout1(x)\n    # print(f\"dropout1: {x.shape}\")\n    x, _ = self.rnn_layer(x)\n    # print(f\"rnn_layer: {x.shape}\")\n    # x = x[:, -1, :]\n    # print(f\"slicing: {x.shape}\")\n    x = self.dropout2(x)\n    # print(f\"dropout2: {x.shape}\")\n    x = self.activation_fct(self.fc(x))\n    # print(f\"activation_fct: {x.shape}\")\n    x = self.dropout3(x)\n    # print(f\"dropout3: {x.shape}\")\n    x = self.output_layer(x)\n    # print(f\"output_layer: {x.shape}\")\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    x, y = batch\n    y = y.view(len(y), 1)\n    y_hat = self(x)\n    val_loss = F.mse_loss(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.training_step","title":"<code>training_step(batch, prog_bar=False)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def training_step(self, batch: tuple, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    x, y = batch\n    # reshape the tensor y to be a column vector (len(y) rows and 1 column)\n    y = y.view(len(y), 1)\n    # Note: the number of rows in x is equal to the number of rows in y\n    y_hat = self(x)\n    # Note: the number of rows in y_hat is equal to the number of rows in y\n    # train_loss = F.mse_loss(y_hat, y)\n    metric = getattr(torchmetrics.functional.regression, self._torchmetric)\n    train_loss = metric(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n    # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n    return train_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    x, y = batch\n    # reshape the tensor y to be a column vector (len(y) rows and 1 column)\n    y = y.view(len(y), 1)\n    y_hat = self(x)\n    val_loss = F.mse_loss(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/","title":"transformerlightregression","text":""},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression","title":"<code>TransformerLightRegression</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a transformer-based regression neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function, e.g., \u201cmean_squared_error\u201d.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    from torch import nn\n    import lightning as L\n    PATH_DATASETS = './data'\n    BATCH_SIZE = 8\n    dataset = Diabetes()\n    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    batch_x, batch_y = next(iter(train_loader))\n    print(batch_x.shape)\n    print(batch_y.shape)\n    net_light_base = NetLightRegression2(l1=128,\n                                        epochs=10,\n                                        batch_size=BATCH_SIZE,\n                                        initialization='xavier',\n                                        act_fn=nn.ReLU(),\n                                        optimizer='Adam',\n                                        dropout_prob=0.1,\n                                        lr_mult=0.1,\n                                        patience=5,\n                                        _L_in=10,\n                                        _L_out=1)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n    trainer.fit(net_light_base, train_loader)\n    trainer.validate(net_light_base, val_loader)\n    trainer.test(net_light_base, test_loader)\n</code></pre> <pre><code>  | Name   | Type       | Params | In sizes | Out sizes\n-------------------------------------------------------------\n0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n-------------------------------------------------------------\n15.9 K    Trainable params\n0         Non-trainable params\n15.9 K    Total params\n0.064     Total estimated model params size (MB)\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Validate metric           DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        hp_metric              29010.7734375\n        val_loss               29010.7734375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        hp_metric              29010.7734375\n        val_loss               29010.7734375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n[{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n</code></pre> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>class TransformerLightRegression(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a transformer-based regression neural network model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _torchmetric (str):\n            The metric to use for the loss function, e.g., \"mean_squared_error\".\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            from torch import nn\n            import lightning as L\n            PATH_DATASETS = './data'\n            BATCH_SIZE = 8\n            dataset = Diabetes()\n            train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            batch_x, batch_y = next(iter(train_loader))\n            print(batch_x.shape)\n            print(batch_y.shape)\n            net_light_base = NetLightRegression2(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier',\n                                                act_fn=nn.ReLU(),\n                                                optimizer='Adam',\n                                                dropout_prob=0.1,\n                                                lr_mult=0.1,\n                                                patience=5,\n                                                _L_in=10,\n                                                _L_out=1)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n            trainer.fit(net_light_base, train_loader)\n            trainer.validate(net_light_base, val_loader)\n            trainer.test(net_light_base, test_loader)\n\n              | Name   | Type       | Params | In sizes | Out sizes\n            -------------------------------------------------------------\n            0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n            -------------------------------------------------------------\n            15.9 K    Trainable params\n            0         Non-trainable params\n            15.9 K    Total params\n            0.064     Total estimated model params size (MB)\n\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Validate metric           DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Test metric             DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n            [{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        d_mult: int,\n        dim_feedforward: int,\n        nhead: int,\n        num_layers: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the TransformerLightRegression object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n            _torchmetric (str): The metric to use for the loss function, e.g., \"mean_squared_error\".\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 4.\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        self.d_mult = d_mult\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n\n        # self.l1 = l1\n        # self.dim_feedforward = dim_feedforward\n        # self.nhead = nhead\n        # self.num_layers = num_layers\n        # self.act_fn = act_fn\n        # self.dropout_prob = dropout_prob\n\n        l_nodes = d_mult * nhead * 2\n        # Each of the _L_1 inputs is forwarded to d_model nodes,\n        # e.g., if _L_in = 90 and d_model = 4, then the input is forwarded to 360 nodes\n        # self.embed = SkipLinear(90, 360)\n        self.embed = SkipLinear(_L_in, _L_in * l_nodes)\n\n        # Positional encoding\n        # self.pos_enc = PositionalEncoding(d_model=4, dropout_prob=dropout_prob)\n        self.pos_enc = PositionalEncoding(d_model=l_nodes, dropout_prob=self.hparams.dropout_prob)\n\n        # Transformer encoder layer\n        # embed_dim \"d_model\" must be divisible by num_heads\n        print(f\"l_nodes: {l_nodes} must be divisible by nhead: {self.hparams.nhead} and 2.\")\n        # self.enc_layer = torch.nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=10, batch_first=True)\n        self.enc_layer = torch.nn.TransformerEncoderLayer(\n            d_model=l_nodes,\n            nhead=self.hparams.nhead,\n            dim_feedforward=self.hparams.dim_feedforward,\n            batch_first=True,\n        )\n\n        # Transformer encoder\n        # self.trans_enc = torch.nn.TransformerEncoder(self.enc_layer, num_layers=2)\n        self.trans_enc = torch.nn.TransformerEncoder(self.enc_layer, num_layers=self.hparams.num_layers)\n\n        n_low = _L_in // 4\n        # ensure that n_high is larger than n_low\n        n_high = max(self.hparams.l1, 2 * n_low)\n        hidden_sizes = generate_div2_list(n_high, n_low)\n\n        # Create the network based on the specified hidden sizes\n        layers = []\n        layer_sizes = [self._L_in * l_nodes] + hidden_sizes\n        layer_size_last = layer_sizes[0]\n        for layer_size in layer_sizes[1:]:\n            layers += [\n                nn.Linear(layer_size_last, layer_size),\n                nn.BatchNorm1d(layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n            layer_size_last = layer_size\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        l_nodes = self.hparams.d_mult * self.hparams.nhead * 2\n        z = self.embed(x)\n\n        # z = z.reshape(-1, 90, 4)\n        z = z.reshape(-1, self._L_in, l_nodes)\n\n        z = self.pos_enc(z)\n        z = self.trans_enc(z)\n\n        # flatten\n        # z = z.reshape(-1, 360)\n        z = z.reshape(-1, self._L_in * l_nodes)\n\n        z = self.layers(z)\n        return z\n\n    def training_step(self, batch: tuple, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        val_loss = F.mse_loss(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        # self.log(\"train_loss\", val_loss, prog_bar=prog_bar)\n        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n        return val_loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        val_loss = F.mse_loss(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        x, y = batch\n        y_hat = self(x)\n        y = y.view(len(y), 1)\n        val_loss = F.mse_loss(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single prediction step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the prediction for this batch.\n        \"\"\"\n        x, y = batch\n        yhat = self(x)\n        y = y.view(len(y), 1)\n        yhat = yhat.view(len(yhat), 1)\n        print(f\"Predict step x: {x}\")\n        print(f\"Predict step y: {y}\")\n        print(f\"Predict step y_hat: {yhat}\")\n        # pred_loss = F.mse_loss(y_hat, y)\n        # pred loss not registered\n        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n        # MisconfigurationException: You are trying to `self.log()`\n        # but the loop's result collection is not registered yet.\n        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n        return (x, y, yhat)\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.__init__","title":"<code>__init__(l1, d_mult, dim_feedforward, nhead, num_layers, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, _torchmetric, *args, **kwargs)</code>","text":"<p>Initializes the TransformerLightRegression object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function, e.g., \u201cmean_squared_error\u201d.</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 4.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    d_mult: int,\n    dim_feedforward: int,\n    nhead: int,\n    num_layers: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    _torchmetric: str,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the TransformerLightRegression object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n        _torchmetric (str): The metric to use for the loss function, e.g., \"mean_squared_error\".\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 4.\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    if _torchmetric is None:\n        _torchmetric = \"mean_squared_error\"\n    self._torchmetric = _torchmetric\n    self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n    self.d_mult = d_mult\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_in))\n\n    # self.l1 = l1\n    # self.dim_feedforward = dim_feedforward\n    # self.nhead = nhead\n    # self.num_layers = num_layers\n    # self.act_fn = act_fn\n    # self.dropout_prob = dropout_prob\n\n    l_nodes = d_mult * nhead * 2\n    # Each of the _L_1 inputs is forwarded to d_model nodes,\n    # e.g., if _L_in = 90 and d_model = 4, then the input is forwarded to 360 nodes\n    # self.embed = SkipLinear(90, 360)\n    self.embed = SkipLinear(_L_in, _L_in * l_nodes)\n\n    # Positional encoding\n    # self.pos_enc = PositionalEncoding(d_model=4, dropout_prob=dropout_prob)\n    self.pos_enc = PositionalEncoding(d_model=l_nodes, dropout_prob=self.hparams.dropout_prob)\n\n    # Transformer encoder layer\n    # embed_dim \"d_model\" must be divisible by num_heads\n    print(f\"l_nodes: {l_nodes} must be divisible by nhead: {self.hparams.nhead} and 2.\")\n    # self.enc_layer = torch.nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=10, batch_first=True)\n    self.enc_layer = torch.nn.TransformerEncoderLayer(\n        d_model=l_nodes,\n        nhead=self.hparams.nhead,\n        dim_feedforward=self.hparams.dim_feedforward,\n        batch_first=True,\n    )\n\n    # Transformer encoder\n    # self.trans_enc = torch.nn.TransformerEncoder(self.enc_layer, num_layers=2)\n    self.trans_enc = torch.nn.TransformerEncoder(self.enc_layer, num_layers=self.hparams.num_layers)\n\n    n_low = _L_in // 4\n    # ensure that n_high is larger than n_low\n    n_high = max(self.hparams.l1, 2 * n_low)\n    hidden_sizes = generate_div2_list(n_high, n_low)\n\n    # Create the network based on the specified hidden sizes\n    layers = []\n    layer_sizes = [self._L_in * l_nodes] + hidden_sizes\n    layer_size_last = layer_sizes[0]\n    for layer_size in layer_sizes[1:]:\n        layers += [\n            nn.Linear(layer_size_last, layer_size),\n            nn.BatchNorm1d(layer_size),\n            self.hparams.act_fn,\n            nn.Dropout(self.hparams.dropout_prob),\n        ]\n        layer_size_last = layer_size\n    layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n    # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n    self.layers = nn.Sequential(*layers)\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n    return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.predict_step","title":"<code>predict_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the prediction for this batch.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single prediction step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the prediction for this batch.\n    \"\"\"\n    x, y = batch\n    yhat = self(x)\n    y = y.view(len(y), 1)\n    yhat = yhat.view(len(yhat), 1)\n    print(f\"Predict step x: {x}\")\n    print(f\"Predict step y: {y}\")\n    print(f\"Predict step y_hat: {yhat}\")\n    # pred_loss = F.mse_loss(y_hat, y)\n    # pred loss not registered\n    # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n    # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n    # MisconfigurationException: You are trying to `self.log()`\n    # but the loop's result collection is not registered yet.\n    # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n    # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n    return (x, y, yhat)\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    x, y = batch\n    y_hat = self(x)\n    y = y.view(len(y), 1)\n    val_loss = F.mse_loss(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.training_step","title":"<code>training_step(batch, prog_bar=False)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def training_step(self, batch: tuple, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    x, y = batch\n    y = y.view(len(y), 1)\n    y_hat = self(x)\n    val_loss = F.mse_loss(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    # self.log(\"train_loss\", val_loss, prog_bar=prog_bar)\n    # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    x, y = batch\n    y = y.view(len(y), 1)\n    y_hat = self(x)\n    val_loss = F.mse_loss(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/transformer/attention/","title":"attention","text":""},{"location":"reference/spotpython/light/transformer/attention/#spotpython.light.transformer.attention.scaled_dot_product","title":"<code>scaled_dot_product(q, k, v, mask=None)</code>","text":"<pre><code>Compute scaled dot product attention.\nArgs:\n    q: Queries\n    k: Keys\n    v: Values\n    mask: Mask to apply to the attention logits\n\nReturns:\n    Tuple of (Values, Attention weights)\n\nExamples:\n&gt;&gt;&gt; from spotpython.light.transformer.attention import scaled_dot_product\n    seq_len, d_k = 1, 2\n    pl.seed_everything(42)\n    q = torch.randn(seq_len, d_k)\n    k = torch.randn(seq_len, d_k)\n    v = torch.randn(seq_len, d_k)\n    values, attention = scaled_dot_product(q, k, v)\n    print(\"Q\n</code></pre> <p>\u201d, q)         print(\u201cK \u201c, k)         print(\u201cV \u201c, v)         print(\u201cValues \u201c, values)         print(\u201cAttention \u201c, attention)</p> Source code in <code>spotpython/light/transformer/attention.py</code> <pre><code>def scaled_dot_product(q, k, v, mask=None):\n    \"\"\"\n    Compute scaled dot product attention.\n    Args:\n        q: Queries\n        k: Keys\n        v: Values\n        mask: Mask to apply to the attention logits\n\n    Returns:\n        Tuple of (Values, Attention weights)\n\n    Examples:\n    &gt;&gt;&gt; from spotpython.light.transformer.attention import scaled_dot_product\n        seq_len, d_k = 1, 2\n        pl.seed_everything(42)\n        q = torch.randn(seq_len, d_k)\n        k = torch.randn(seq_len, d_k)\n        v = torch.randn(seq_len, d_k)\n        values, attention = scaled_dot_product(q, k, v)\n        print(\"Q\\n\", q)\n        print(\"K\\n\", k)\n        print(\"V\\n\", v)\n        print(\"Values\\n\", values)\n        print(\"Attention\\n\", attention)\n    \"\"\"\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n    if mask is not None:\n        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n    attention = F.softmax(attn_logits, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention\n</code></pre>"},{"location":"reference/spotpython/light/transformer/encoder/","title":"encoder","text":""},{"location":"reference/spotpython/light/transformer/encoder/#spotpython.light.transformer.encoder.TransformerEncoder","title":"<code>TransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer encoder module. Consists of a stack of EncoderBlocks with layer norm at the end.</p> Source code in <code>spotpython/light/transformer/encoder.py</code> <pre><code>class TransformerEncoder(nn.Module):\n    \"\"\"Transformer encoder module.\n    Consists of a stack of EncoderBlocks with layer norm at the end.\n    \"\"\"\n\n    def __init__(self, num_layers, **block_args) -&gt; None:\n        \"\"\"Constructor.\n        Args:\n            num_layers: int, number of encoder blocks.\n            block_args: dict, arguments for EncoderBlock.\n\n        Returns:\n            None\n\n        Example:\n            &gt;&gt;&gt; from spotpython.light.transformer.encoder import TransformerEncoder\n            &gt;&gt;&gt; encoder = TransformerEncoder(num_layers=3,\n                                            model_dim=512,\n                                            num_heads=8,\n                                            dim_feedforward=2048,\n                                            dropout=0.1)\n            &gt;&gt;&gt; x = torch.rand(10, 32, 512)\n            &gt;&gt;&gt; encoder(x).shape\n            torch.Size([10, 32, 512])\n\n        \"\"\"\n        super().__init__()\n        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n\n    def forward(self, x, mask=None):\n        for layer in self.layers:\n            x = layer(x, mask=mask)\n        return x\n\n    def get_attention_maps(self, x, mask=None):\n        attention_maps = []\n        for layer in self.layers:\n            _, attn_map = layer.self_attn(x, mask=mask, return_attention=True)\n            attention_maps.append(attn_map)\n            x = layer(x)\n        return attention_maps\n</code></pre>"},{"location":"reference/spotpython/light/transformer/encoder/#spotpython.light.transformer.encoder.TransformerEncoder.__init__","title":"<code>__init__(num_layers, **block_args)</code>","text":"<p>Constructor. Args:     num_layers: int, number of encoder blocks.     block_args: dict, arguments for EncoderBlock.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>from spotpython.light.transformer.encoder import TransformerEncoder encoder = TransformerEncoder(num_layers=3,                                 model_dim=512,                                 num_heads=8,                                 dim_feedforward=2048,                                 dropout=0.1) x = torch.rand(10, 32, 512) encoder(x).shape torch.Size([10, 32, 512])</p> Source code in <code>spotpython/light/transformer/encoder.py</code> <pre><code>def __init__(self, num_layers, **block_args) -&gt; None:\n    \"\"\"Constructor.\n    Args:\n        num_layers: int, number of encoder blocks.\n        block_args: dict, arguments for EncoderBlock.\n\n    Returns:\n        None\n\n    Example:\n        &gt;&gt;&gt; from spotpython.light.transformer.encoder import TransformerEncoder\n        &gt;&gt;&gt; encoder = TransformerEncoder(num_layers=3,\n                                        model_dim=512,\n                                        num_heads=8,\n                                        dim_feedforward=2048,\n                                        dropout=0.1)\n        &gt;&gt;&gt; x = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; encoder(x).shape\n        torch.Size([10, 32, 512])\n\n    \"\"\"\n    super().__init__()\n    self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n</code></pre>"},{"location":"reference/spotpython/light/transformer/encoderblock/","title":"encoderblock","text":""},{"location":"reference/spotpython/light/transformer/encoderblock/#spotpython.light.transformer.encoderblock.EncoderBlock","title":"<code>EncoderBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>spotpython/light/transformer/encoderblock.py</code> <pre><code>class EncoderBlock(nn.Module):\n    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0) -&gt; None:\n        \"\"\"\n        Initializes the EncoderBlock object.\n\n        Args:\n            input_dim (int): The dimensionality of the input.\n            num_heads (int): The number of heads to use in the attention block.\n            dim_feedforward (int): The dimensionality of the hidden layer in the MLP.\n            dropout (float): The dropout probability to use in the dropout layers.\n\n        Returns:\n            None\n        \"\"\"\n        super().__init__()\n\n        # Attention layer\n        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n\n        # Two-layer MLP\n        self.linear_net = nn.Sequential(\n            nn.Linear(input_dim, dim_feedforward),\n            nn.Dropout(dropout),\n            nn.ReLU(inplace=True),\n            nn.Linear(dim_feedforward, input_dim),\n        )\n\n        # Layers to apply in between the main layers\n        self.norm1 = nn.LayerNorm(input_dim)\n        self.norm2 = nn.LayerNorm(input_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Attention part\n        attn_out = self.self_attn(x, mask=mask)\n        x = x + self.dropout(attn_out)\n        x = self.norm1(x)\n\n        # MLP part\n        linear_out = self.linear_net(x)\n        x = x + self.dropout(linear_out)\n        x = self.norm2(x)\n\n        return x\n</code></pre>"},{"location":"reference/spotpython/light/transformer/encoderblock/#spotpython.light.transformer.encoderblock.EncoderBlock.__init__","title":"<code>__init__(input_dim, num_heads, dim_feedforward, dropout=0.0)</code>","text":"<p>Initializes the EncoderBlock object.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The dimensionality of the input.</p> required <code>num_heads</code> <code>int</code> <p>The number of heads to use in the attention block.</p> required <code>dim_feedforward</code> <code>int</code> <p>The dimensionality of the hidden layer in the MLP.</p> required <code>dropout</code> <code>float</code> <p>The dropout probability to use in the dropout layers.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/light/transformer/encoderblock.py</code> <pre><code>def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0) -&gt; None:\n    \"\"\"\n    Initializes the EncoderBlock object.\n\n    Args:\n        input_dim (int): The dimensionality of the input.\n        num_heads (int): The number of heads to use in the attention block.\n        dim_feedforward (int): The dimensionality of the hidden layer in the MLP.\n        dropout (float): The dropout probability to use in the dropout layers.\n\n    Returns:\n        None\n    \"\"\"\n    super().__init__()\n\n    # Attention layer\n    self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n\n    # Two-layer MLP\n    self.linear_net = nn.Sequential(\n        nn.Linear(input_dim, dim_feedforward),\n        nn.Dropout(dropout),\n        nn.ReLU(inplace=True),\n        nn.Linear(dim_feedforward, input_dim),\n    )\n\n    # Layers to apply in between the main layers\n    self.norm1 = nn.LayerNorm(input_dim)\n    self.norm2 = nn.LayerNorm(input_dim)\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/multiheadattention/","title":"multiheadattention","text":""},{"location":"reference/spotpython/light/transformer/multiheadattention/#spotpython.light.transformer.multiheadattention.MultiheadAttention","title":"<code>MultiheadAttention</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>spotpython/light/transformer/multiheadattention.py</code> <pre><code>class MultiheadAttention(nn.Module):\n    def __init__(self, input_dim, embed_dim, num_heads):\n        \"\"\"Constructor.\n\n        Args:\n            input_dim (int): input dimensionality.\n            embed_dim (int): embedding dimensionality.\n            num_heads (int): number of heads.\n        \"\"\"\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"Embedding dim. must be 0 modulo number of heads.\"\n\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n\n        # Stack all weight matrices 1...h together for efficiency\n        # Note that in many implementations you see \"bias=False\" which is optional\n        self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n        self.o_proj = nn.Linear(embed_dim, embed_dim)\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # Original Transformer initialization, see PyTorch documentation\n        nn.init.xavier_uniform_(self.qkv_proj.weight)\n        self.qkv_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.o_proj.weight)\n        self.o_proj.bias.data.fill_(0)\n\n    def forward(self, x, mask=None, return_attention=False):\n        batch_size, seq_length, _ = x.size()\n        if mask is not None:\n            mask = expand_mask(mask)\n        qkv = self.qkv_proj(x)\n\n        # Separate Q, K, V from linear output\n        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3)  # [Batch, Head, SeqLen, Dims]\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Determine value outputs\n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n        values = values.permute(0, 2, 1, 3)  # [Batch, SeqLen, Head, Dims]\n        values = values.reshape(batch_size, seq_length, self.embed_dim)\n        o = self.o_proj(values)\n\n        if return_attention:\n            return o, attention\n        else:\n            return o\n</code></pre>"},{"location":"reference/spotpython/light/transformer/multiheadattention/#spotpython.light.transformer.multiheadattention.MultiheadAttention.__init__","title":"<code>__init__(input_dim, embed_dim, num_heads)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>input dimensionality.</p> required <code>embed_dim</code> <code>int</code> <p>embedding dimensionality.</p> required <code>num_heads</code> <code>int</code> <p>number of heads.</p> required Source code in <code>spotpython/light/transformer/multiheadattention.py</code> <pre><code>def __init__(self, input_dim, embed_dim, num_heads):\n    \"\"\"Constructor.\n\n    Args:\n        input_dim (int): input dimensionality.\n        embed_dim (int): embedding dimensionality.\n        num_heads (int): number of heads.\n    \"\"\"\n    super().__init__()\n    assert embed_dim % num_heads == 0, \"Embedding dim. must be 0 modulo number of heads.\"\n\n    self.embed_dim = embed_dim\n    self.num_heads = num_heads\n    self.head_dim = embed_dim // num_heads\n\n    # Stack all weight matrices 1...h together for efficiency\n    # Note that in many implementations you see \"bias=False\" which is optional\n    self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n    self.o_proj = nn.Linear(embed_dim, embed_dim)\n\n    self._reset_parameters()\n</code></pre>"},{"location":"reference/spotpython/light/transformer/multiheadattention/#spotpython.light.transformer.multiheadattention.expand_mask","title":"<code>expand_mask(mask)</code>","text":"<p>Helper function to support different mask shapes. Expands the mask to the correct shape for the MultiheadAttention layer. Output shape supports (batch_size, number of heads, seq length, seq length). If 2D: broadcasted over batch size and number of heads. If 3D: broadcasted over number of heads. If 4D: leave as is.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Tensor</code> <p>Mask tensor of shape (batch_size, seq_length, seq_length) or (seq_length, seq_length).</p> required Source code in <code>spotpython/light/transformer/multiheadattention.py</code> <pre><code>def expand_mask(mask):\n    \"\"\"\n    Helper function to support different mask shapes.\n    Expands the mask to the correct shape for the MultiheadAttention layer.\n    Output shape supports (batch_size, number of heads, seq length, seq length).\n    If 2D: broadcasted over batch size and number of heads.\n    If 3D: broadcasted over number of heads.\n    If 4D: leave as is.\n\n    Args:\n        mask (torch.Tensor):\n            Mask tensor of shape (batch_size, seq_length, seq_length) or (seq_length, seq_length).\n    \"\"\"\n    assert mask.ndim &gt;= 2, \"Mask must be &gt;= 2-dim. with seq_length x seq_length\"\n    if mask.ndim == 3:\n        mask = mask.unsqueeze(1)\n    while mask.ndim &lt; 4:\n        mask = mask.unsqueeze(0)\n    return mask\n</code></pre>"},{"location":"reference/spotpython/light/transformer/positionalEncoding/","title":"positionalEncoding","text":""},{"location":"reference/spotpython/light/transformer/positionalEncoding/#spotpython.light.transformer.positionalEncoding.PositionalEncoding","title":"<code>PositionalEncoding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Positional encoding module injects some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings, so that the two can be summed. Here, we use <code>sine</code> and <code>cosine</code> functions of different frequencies.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>the embedding dimension. Should be even.</p> required <code>dropout_prob</code> <code>float</code> <p>the dropout value</p> required <code>max_len</code> <code>int</code> <p>the maximum length of the incoming sequence. Usually related to the max batch_size. Can be larger as the batch size, e.g., if prediction is done on a single test set. Default: 12552</p> <code>12552</code> Shape <p>Input:     x: Tensor, shape <code>[seq_len, batch_size, embedding_dim]</code> Output:     Tensor, shape <code>[seq_len, batch_size, embedding_dim]</code></p> Notes <ul> <li><code>No return value, but torch</code>\u2019s method <code>register_buffer</code> is used to register the positional encodings.</li> <li>Code adapted from PyTorch: \u201cTransformer Tutorial\u201d</li> </ul> Reference <p>https://pytorch.org/tutorials/beginner/transformer_tutorial.html#positional-encoding</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.transformer.positionalEncoding import PositionalEncoding\n    import torch\n    # number of tensors\n    n = 3\n    # dimension of each tensor, should be even\n    k = 10\n    pe = PositionalEncoding(d_model=k, dropout_prob=0)\n    input = torch.zeros(1, n, k)\n    # Generate a tensor of size (1, 10, 4) with values from 1 to 10\n    for i in range(n):\n        input[0, i, :] = i\n    print(f\"Input shape: {input.shape}\")\n    print(f\"Input: {input}\")\n    output = pe(input)\n    print(f\"Output shape: {output.shape}\")\n    print(f\"Output: {output}\")\n    position: tensor([[    0],\n                    [    1],\n                    [    2],\n                    ...,\n                    [99997],\n                    [99998],\n                    [99999]])\n    div_term: tensor([1.0000e+00, 1.5849e-01, 2.5119e-02, 3.9811e-03, 6.3096e-04])\n    Input shape: torch.Size([1, 3, 10])\n    Input: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n            [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n            [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]])\n    Output shape: torch.Size([1, 3, 10])\n    Output: tensor([[[0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n            [1., 2., 1., 2., 1., 2., 1., 2., 1., 2.],\n            [2., 3., 2., 3., 2., 3., 2., 3., 2., 3.]]])\n</code></pre> Source code in <code>spotpython/light/transformer/positionalEncoding.py</code> <pre><code>class PositionalEncoding(nn.Module):\n    \"\"\"\n    Positional encoding module injects some information\n    about the relative or absolute position of the tokens in the sequence.\n    The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n    Here, we use ``sine`` and ``cosine`` functions of different frequencies.\n\n    Args:\n        d_model (int):\n            the embedding dimension. Should be even.\n        dropout_prob (float):\n            the dropout value\n        max_len (int):\n            the maximum length of the incoming sequence. Usually related to the max batch_size.\n            Can be larger as the batch size, e.g., if prediction is done on a single test set.\n            Default: 12552\n\n    Shape:\n        Input:\n            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n        Output:\n            Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n\n    Notes:\n        * `No return value, but torch`'s method `register_buffer` is used to register the positional encodings.\n        * Code adapted from PyTorch: \"Transformer Tutorial\"\n\n\n    Reference:\n        https://pytorch.org/tutorials/beginner/transformer_tutorial.html#positional-encoding\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.transformer.positionalEncoding import PositionalEncoding\n            import torch\n            # number of tensors\n            n = 3\n            # dimension of each tensor, should be even\n            k = 10\n            pe = PositionalEncoding(d_model=k, dropout_prob=0)\n            input = torch.zeros(1, n, k)\n            # Generate a tensor of size (1, 10, 4) with values from 1 to 10\n            for i in range(n):\n                input[0, i, :] = i\n            print(f\"Input shape: {input.shape}\")\n            print(f\"Input: {input}\")\n            output = pe(input)\n            print(f\"Output shape: {output.shape}\")\n            print(f\"Output: {output}\")\n            position: tensor([[    0],\n                            [    1],\n                            [    2],\n                            ...,\n                            [99997],\n                            [99998],\n                            [99999]])\n            div_term: tensor([1.0000e+00, 1.5849e-01, 2.5119e-02, 3.9811e-03, 6.3096e-04])\n            Input shape: torch.Size([1, 3, 10])\n            Input: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                    [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n                    [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]])\n            Output shape: torch.Size([1, 3, 10])\n            Output: tensor([[[0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n                    [1., 2., 1., 2., 1., 2., 1., 2., 1., 2.],\n                    [2., 3., 2., 3., 2., 3., 2., 3., 2., 3.]]])\n    \"\"\"\n\n    def __init__(self, d_model: int, dropout_prob: float, max_len: int = 12552) -&gt; None:\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout_prob)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Add positional encoding to the input tensor.\n\n        Arguments:\n            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n\n        Returns:\n            Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n\n        Raises:\n            IndexError: if the positional encoding cannot be added to the input tensor\n        \"\"\"\n        x = x + self.pe[: x.size(0)]\n        return self.dropout(x)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/positionalEncoding/#spotpython.light.transformer.positionalEncoding.PositionalEncoding.forward","title":"<code>forward(x)</code>","text":"<p>Add positional encoding to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor, shape <code>[seq_len, batch_size, embedding_dim]</code></p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor, shape <code>[seq_len, batch_size, embedding_dim]</code></p> <p>Raises:</p> Type Description <code>IndexError</code> <p>if the positional encoding cannot be added to the input tensor</p> Source code in <code>spotpython/light/transformer/positionalEncoding.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Add positional encoding to the input tensor.\n\n    Arguments:\n        x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n\n    Returns:\n        Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n\n    Raises:\n        IndexError: if the positional encoding cannot be added to the input tensor\n    \"\"\"\n    x = x + self.pe[: x.size(0)]\n    return self.dropout(x)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/positionalEncodingBasic/","title":"positionalEncodingBasic","text":""},{"location":"reference/spotpython/light/transformer/positionalEncodingBasic/#spotpython.light.transformer.positionalEncodingBasic.PositionalEncodingBasic","title":"<code>PositionalEncodingBasic</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>spotpython/light/transformer/positionalEncodingBasic.py</code> <pre><code>class PositionalEncodingBasic(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        \"\"\"\n        Inputs\n            d_model - Hidden dimensionality of the input.\n            max_len - Maximum length of a sequence to expect.\n        \"\"\"\n        super().__init__()\n\n        # Create matrix of [SeqLen, HiddenDim] representing\n        # the positional encoding for max_len inputs\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n\n        # register_buffer =&gt; Tensor which is not a parameter,\n        # but should be part of the modules state.\n        # Used for tensors that need to be on the same device as the module.\n        # persistent=False tells PyTorch to not add the buffer to the\n        # state dict (e.g. when we save the model)\n        self.register_buffer(\"pe\", pe, persistent=False)\n\n    def forward(self, x):\n        x = x + self.pe[:, : x.size(1)]\n        return x\n</code></pre>"},{"location":"reference/spotpython/light/transformer/positionalEncodingBasic/#spotpython.light.transformer.positionalEncodingBasic.PositionalEncodingBasic.__init__","title":"<code>__init__(d_model, max_len=5000)</code>","text":"<p>Inputs     d_model - Hidden dimensionality of the input.     max_len - Maximum length of a sequence to expect.</p> Source code in <code>spotpython/light/transformer/positionalEncodingBasic.py</code> <pre><code>def __init__(self, d_model, max_len=5000):\n    \"\"\"\n    Inputs\n        d_model - Hidden dimensionality of the input.\n        max_len - Maximum length of a sequence to expect.\n    \"\"\"\n    super().__init__()\n\n    # Create matrix of [SeqLen, HiddenDim] representing\n    # the positional encoding for max_len inputs\n    pe = torch.zeros(max_len, d_model)\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    pe = pe.unsqueeze(0)\n\n    # register_buffer =&gt; Tensor which is not a parameter,\n    # but should be part of the modules state.\n    # Used for tensors that need to be on the same device as the module.\n    # persistent=False tells PyTorch to not add the buffer to the\n    # state dict (e.g. when we save the model)\n    self.register_buffer(\"pe\", pe, persistent=False)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/skiplinear/","title":"skiplinear","text":""},{"location":"reference/spotpython/light/transformer/skiplinear/#spotpython.light.transformer.skiplinear.SkipLinear","title":"<code>SkipLinear</code>","text":"<p>               Bases: <code>Module</code></p> <p>A skip linear layer.</p> <p>Notes: Code adapted from James D. McCaffrey: \u201cRegression Using a PyTorch Neural Network with a Transformer Component\u201d</p> Reference <p>https://jamesmccaffrey.wordpress.com/2023/12/01/regression-using-a-pytorch-neural-network-with-a-transformer-component/</p> <p>Parameters:</p> Name Type Description Default <code>n_in</code> <code>int</code> <p>the input dimension</p> required <code>n_out</code> <code>int</code> <p>the output dimension</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.transformer.skiplinear import SkipLinear\n    import torch\n    n_in = 2\n    n_out = 4\n    sl = SkipLinear(n_in, n_out)\n    input = torch.zeros(1, n_in)\n    for i in range(n_in):\n        input[0, i] = i\n    print(f\"Input shape: {input.shape}\")\n    print(f\"Input: {input}\")\n    output = sl(input)\n    print(f\"Output shape: {output.shape}\")\n    print(f\"Output: {output}\")\n    print(sl.lst_modules)\n    for i in sl.lst_modules:\n        print(f\"weights: {i.weights}\")\n    Input shape: torch.Size([1, 2])\n    Input: tensor([[0., 1.]])\n    Output shape: torch.Size([1, 4])\n    Output: tensor([[ 0.0000,  0.0000, -0.0062, -0.0032]], grad_fn=&lt;ViewBackward0&gt;)\n    ModuleList(\n    (0-1): 2 x Core()\n    )\n    weights: Parameter containing:\n    tensor([[-0.0098],\n            [ 0.0038]], requires_grad=True)\n    weights: Parameter containing:\n    tensor([[0.0041],\n            [0.0074]], requires_grad=True)\n</code></pre> Source code in <code>spotpython/light/transformer/skiplinear.py</code> <pre><code>class SkipLinear(torch.nn.Module):\n    \"\"\"\n    A skip linear layer.\n\n    Notes:\n    Code adapted from James D. McCaffrey:\n    \"Regression Using a PyTorch Neural Network with a Transformer Component\"\n\n    Reference:\n        https://jamesmccaffrey.wordpress.com/2023/12/01/regression-using-a-pytorch-neural-network-with-a-transformer-component/\n\n    Args:\n        n_in (int):\n            the input dimension\n        n_out (int):\n            the output dimension\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.transformer.skiplinear import SkipLinear\n            import torch\n            n_in = 2\n            n_out = 4\n            sl = SkipLinear(n_in, n_out)\n            input = torch.zeros(1, n_in)\n            for i in range(n_in):\n                input[0, i] = i\n            print(f\"Input shape: {input.shape}\")\n            print(f\"Input: {input}\")\n            output = sl(input)\n            print(f\"Output shape: {output.shape}\")\n            print(f\"Output: {output}\")\n            print(sl.lst_modules)\n            for i in sl.lst_modules:\n                print(f\"weights: {i.weights}\")\n            Input shape: torch.Size([1, 2])\n            Input: tensor([[0., 1.]])\n            Output shape: torch.Size([1, 4])\n            Output: tensor([[ 0.0000,  0.0000, -0.0062, -0.0032]], grad_fn=&lt;ViewBackward0&gt;)\n            ModuleList(\n            (0-1): 2 x Core()\n            )\n            weights: Parameter containing:\n            tensor([[-0.0098],\n                    [ 0.0038]], requires_grad=True)\n            weights: Parameter containing:\n            tensor([[0.0041],\n                    [0.0074]], requires_grad=True)\n    \"\"\"\n\n    class Core(torch.nn.Module):\n        \"\"\"A simple linear layer with n outputs.\"\"\"\n\n        def __init__(self, n):\n            \"\"\"\n            Initialize the layer.\n\n            Args:\n                n (int): The number of output nodes.\n            \"\"\"\n            super().__init__()\n            # initialize with random weights using normal distribution\n            self.weights = torch.nn.Parameter(torch.randn(1, n))\n            # self.weights = torch.nn.Parameter(torch.rand(1, n) * 2 - 1)\n            self.linear = torch.nn.Linear(1, n)\n\n        def forward(self, x) -&gt; torch.Tensor:\n            \"\"\"\n            Forward pass through the layer.\n\n            Args:\n                x (torch.Tensor): The input tensor.\n\n            Returns:\n                torch.Tensor: The output of the layer.\n            \"\"\"\n            return self.linear(x)\n\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.n_in = n_in\n        self.n_out = n_out\n        if n_out % n_in != 0:\n            raise ValueError(\"n_out % n_in != 0\")\n        n = n_out // n_in  # num nodes per input\n\n        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for _ in range(n_in)])\n\n    def forward(self, x):\n        # We want to apply each module to a slice of the input tensor x and collect the outputs.\n        # This applies the i-th module to the i-th column of x, reshaped as a column vector.\n        # The result is a list of output tensors, which are then concatenated to form the final output.\n        lst_nodes = [self.lst_modules[i](x[:, i].unsqueeze(1)) for i in range(self.n_in)]\n        result = torch.cat(lst_nodes, dim=1)\n        return result.reshape(-1, self.n_out)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/skiplinear/#spotpython.light.transformer.skiplinear.SkipLinear.Core","title":"<code>Core</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple linear layer with n outputs.</p> Source code in <code>spotpython/light/transformer/skiplinear.py</code> <pre><code>class Core(torch.nn.Module):\n    \"\"\"A simple linear layer with n outputs.\"\"\"\n\n    def __init__(self, n):\n        \"\"\"\n        Initialize the layer.\n\n        Args:\n            n (int): The number of output nodes.\n        \"\"\"\n        super().__init__()\n        # initialize with random weights using normal distribution\n        self.weights = torch.nn.Parameter(torch.randn(1, n))\n        # self.weights = torch.nn.Parameter(torch.rand(1, n) * 2 - 1)\n        self.linear = torch.nn.Linear(1, n)\n\n    def forward(self, x) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the layer.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The output of the layer.\n        \"\"\"\n        return self.linear(x)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/skiplinear/#spotpython.light.transformer.skiplinear.SkipLinear.Core.__init__","title":"<code>__init__(n)</code>","text":"<p>Initialize the layer.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of output nodes.</p> required Source code in <code>spotpython/light/transformer/skiplinear.py</code> <pre><code>def __init__(self, n):\n    \"\"\"\n    Initialize the layer.\n\n    Args:\n        n (int): The number of output nodes.\n    \"\"\"\n    super().__init__()\n    # initialize with random weights using normal distribution\n    self.weights = torch.nn.Parameter(torch.randn(1, n))\n    # self.weights = torch.nn.Parameter(torch.rand(1, n) * 2 - 1)\n    self.linear = torch.nn.Linear(1, n)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/skiplinear/#spotpython.light.transformer.skiplinear.SkipLinear.Core.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output of the layer.</p> Source code in <code>spotpython/light/transformer/skiplinear.py</code> <pre><code>def forward(self, x) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the layer.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The output of the layer.\n    \"\"\"\n    return self.linear(x)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/transformerlightpredictor/","title":"transformerlightpredictor","text":""},{"location":"reference/spotpython/light/transformer/transformerlightpredictor/#spotpython.light.transformer.transformerlightpredictor.TransformerLightPredictor","title":"<code>TransformerLightPredictor</code>","text":"<p>               Bases: <code>LightningModule</code></p> Source code in <code>spotpython/light/transformer/transformerlightpredictor.py</code> <pre><code>class TransformerLightPredictor(L.LightningModule):\n    def __init__(\n        self,\n        l1: int,\n        d_mult: int,\n        dim_feedforward: int,\n        nhead: int,\n        num_layers: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        model_dim: int,\n        num_heads: int,\n        lr: float,\n        warmup: int,\n        max_iters: int,\n        input_dropout: float,\n        dropout: float,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the TransformerLightRegression object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int):\n                The number of input features. Not a hyperparameter, but needed to create the network. `input_dim`,\n                hidden dimensionality of the input.\n            _L_out (int):\n                The number of output classes. Not a hyperparameter, but needed to create the network. `num_classes`,\n                number of classes to predict per sequence element.\n            model_dim (int):\n                Hidden dimensionality to use inside the Transformer\n            num_heads (int):\n                Number of heads to use in the Multi-Head Attention blocks\n            num_layers (int):\n                Number of encoder blocks to use.\n            lr (float):\n                Learning rate in the optimizer\n            warmup (int):\n                Number of warmup steps. Usually between 50 and 500\n            max_iters (int):\n                Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n            input_dropout (float):\n                Dropout to apply on the input features\n            dropout (float):\n                Dropout to apply inside the Transformer\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        self.d_mult = d_mult\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n        self._create_model()\n\n    def _create_model(self):\n        # Input dim -&gt; Model dim\n        self.input_net = nn.Sequential(nn.Dropout(self.hparams.input_dropout), nn.Linear(self.hparams.input_dim, self.hparams.model_dim))\n        # Positional encoding for sequences\n        self.positional_encoding = PositionalEncodingBasic(d_model=self.hparams.model_dim)\n        # Transformer\n        self.transformer = TransformerEncoder(\n            num_layers=self.hparams.num_layers,\n            input_dim=self.hparams.model_dim,\n            dim_feedforward=2 * self.hparams.model_dim,\n            num_heads=self.hparams.num_heads,\n            dropout=self.hparams.dropout,\n        )\n        # Output classifier per sequence element\n        self.output_net = nn.Sequential(\n            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n            nn.LayerNorm(self.hparams.model_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(self.hparams.dropout),\n            nn.Linear(self.hparams.model_dim, self.hparams.num_classes),\n        )\n\n    def forward(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Inputs:\n            x - Input features of shape [Batch, SeqLen, input_dim]\n            mask - Mask to apply on the attention outputs (optional)\n            add_positional_encoding - If True, we add the positional encoding to the input.\n                                      Might not be desired for some tasks.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        x = self.transformer(x, mask=mask)\n        x = self.output_net(x)\n        return x\n\n    @torch.no_grad()\n    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Function for extracting the attention matrices of the whole Transformer for a single batch.\n        Input arguments same as the forward pass.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n        return attention_maps\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n\n        # Apply lr scheduler per step\n        lr_scheduler = CosineWarmupScheduler(optimizer, warmup=self.hparams.warmup, max_iters=self.hparams.max_iters)\n        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n\n    def training_step(self, batch, batch_idx):\n        raise NotImplementedError\n\n    def validation_step(self, batch, batch_idx):\n        raise NotImplementedError\n\n    def test_step(self, batch, batch_idx):\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/spotpython/light/transformer/transformerlightpredictor/#spotpython.light.transformer.transformerlightpredictor.TransformerLightPredictor.__init__","title":"<code>__init__(l1, d_mult, dim_feedforward, nhead, num_layers, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, model_dim, num_heads, lr, warmup, max_iters, input_dropout, dropout, *args, **kwargs)</code>","text":"<p>Initializes the TransformerLightRegression object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network. <code>input_dim</code>, hidden dimensionality of the input.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network. <code>num_classes</code>, number of classes to predict per sequence element.</p> required <code>model_dim</code> <code>int</code> <p>Hidden dimensionality to use inside the Transformer</p> required <code>num_heads</code> <code>int</code> <p>Number of heads to use in the Multi-Head Attention blocks</p> required <code>num_layers</code> <code>int</code> <p>Number of encoder blocks to use.</p> required <code>lr</code> <code>float</code> <p>Learning rate in the optimizer</p> required <code>warmup</code> <code>int</code> <p>Number of warmup steps. Usually between 50 and 500</p> required <code>max_iters</code> <code>int</code> <p>Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler</p> required <code>input_dropout</code> <code>float</code> <p>Dropout to apply on the input features</p> required <code>dropout</code> <code>float</code> <p>Dropout to apply inside the Transformer</p> required Source code in <code>spotpython/light/transformer/transformerlightpredictor.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    d_mult: int,\n    dim_feedforward: int,\n    nhead: int,\n    num_layers: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    model_dim: int,\n    num_heads: int,\n    lr: float,\n    warmup: int,\n    max_iters: int,\n    input_dropout: float,\n    dropout: float,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the TransformerLightRegression object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features. Not a hyperparameter, but needed to create the network. `input_dim`,\n            hidden dimensionality of the input.\n        _L_out (int):\n            The number of output classes. Not a hyperparameter, but needed to create the network. `num_classes`,\n            number of classes to predict per sequence element.\n        model_dim (int):\n            Hidden dimensionality to use inside the Transformer\n        num_heads (int):\n            Number of heads to use in the Multi-Head Attention blocks\n        num_layers (int):\n            Number of encoder blocks to use.\n        lr (float):\n            Learning rate in the optimizer\n        warmup (int):\n            Number of warmup steps. Usually between 50 and 500\n        max_iters (int):\n            Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n        input_dropout (float):\n            Dropout to apply on the input features\n        dropout (float):\n            Dropout to apply inside the Transformer\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    self.d_mult = d_mult\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_in))\n    self._create_model()\n</code></pre>"},{"location":"reference/spotpython/light/transformer/transformerlightpredictor/#spotpython.light.transformer.transformerlightpredictor.TransformerLightPredictor.forward","title":"<code>forward(x, mask=None, add_positional_encoding=True)</code>","text":"Inputs <p>x - Input features of shape [Batch, SeqLen, input_dim] mask - Mask to apply on the attention outputs (optional) add_positional_encoding - If True, we add the positional encoding to the input.                           Might not be desired for some tasks.</p> Source code in <code>spotpython/light/transformer/transformerlightpredictor.py</code> <pre><code>def forward(self, x, mask=None, add_positional_encoding=True):\n    \"\"\"\n    Inputs:\n        x - Input features of shape [Batch, SeqLen, input_dim]\n        mask - Mask to apply on the attention outputs (optional)\n        add_positional_encoding - If True, we add the positional encoding to the input.\n                                  Might not be desired for some tasks.\n    \"\"\"\n    x = self.input_net(x)\n    if add_positional_encoding:\n        x = self.positional_encoding(x)\n    x = self.transformer(x, mask=mask)\n    x = self.output_net(x)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/transformer/transformerlightpredictor/#spotpython.light.transformer.transformerlightpredictor.TransformerLightPredictor.get_attention_maps","title":"<code>get_attention_maps(x, mask=None, add_positional_encoding=True)</code>","text":"<p>Function for extracting the attention matrices of the whole Transformer for a single batch. Input arguments same as the forward pass.</p> Source code in <code>spotpython/light/transformer/transformerlightpredictor.py</code> <pre><code>@torch.no_grad()\ndef get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n    \"\"\"\n    Function for extracting the attention matrices of the whole Transformer for a single batch.\n    Input arguments same as the forward pass.\n    \"\"\"\n    x = self.input_net(x)\n    if add_positional_encoding:\n        x = self.positional_encoding(x)\n    attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n    return attention_maps\n</code></pre>"},{"location":"reference/spotpython/plot/contour/","title":"contour","text":""},{"location":"reference/spotpython/plot/contour/#spotpython.plot.contour.simple_contour","title":"<code>simple_contour(fun, min_x=-1, max_x=1, min_y=-1, max_y=1, min_z=None, max_z=None, n_samples=100, n_levels=30)</code>","text":"<p>Simple contour plot</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>_type_</code> <p>description</p> required <code>min_x</code> <code>int</code> <p>description. Defaults to -1.</p> <code>-1</code> <code>max_x</code> <code>int</code> <p>description. Defaults to 1.</p> <code>1</code> <code>min_y</code> <code>int</code> <p>description. Defaults to -1.</p> <code>-1</code> <code>max_y</code> <code>int</code> <p>description. Defaults to 1.</p> <code>1</code> <code>min_z</code> <code>int</code> <p>description. Defaults to 0.</p> <code>None</code> <code>max_z</code> <code>int</code> <p>description. Defaults to 1.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>description. Defaults to 100.</p> <code>100</code> <code>n_levels</code> <code>int</code> <p>description. Defaults to 5.</p> <code>30</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n    import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    fun = analytical().fun_branin\n    simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15)\n</code></pre> Source code in <code>spotpython/plot/contour.py</code> <pre><code>def simple_contour(\n    fun,\n    min_x=-1,\n    max_x=1,\n    min_y=-1,\n    max_y=1,\n    min_z=None,\n    max_z=None,\n    n_samples=100,\n    n_levels=30,\n):\n    \"\"\"\n    Simple contour plot\n\n    Args:\n        fun (_type_): _description_\n        min_x (int, optional): _description_. Defaults to -1.\n        max_x (int, optional): _description_. Defaults to 1.\n        min_y (int, optional): _description_. Defaults to -1.\n        max_y (int, optional): _description_. Defaults to 1.\n        min_z (int, optional): _description_. Defaults to 0.\n        max_z (int, optional): _description_. Defaults to 1.\n        n_samples (int, optional): _description_. Defaults to 100.\n        n_levels (int, optional): _description_. Defaults to 5.\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n            import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            fun = analytical().fun_branin\n            simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15)\n\n    \"\"\"\n    XX, YY = np.meshgrid(np.linspace(min_x, max_x, n_samples), np.linspace(min_y, max_y, n_samples))\n    zz = np.array([fun(np.array([xi, yi]).reshape(-1, 2)) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(n_samples, n_samples)\n    fig, ax = plt.subplots(figsize=(5, 2.7), layout=\"constrained\")\n    if min_z is None:\n        min_z = np.min(zz)\n    if max_z is None:\n        max_z = np.max(zz)\n    plt.contourf(\n        XX,\n        YY,\n        zz,\n        levels=np.linspace(min_z, max_z, n_levels),\n        zorder=1,\n        cmap=\"jet\",\n        vmin=min_z,\n        vmax=max_z,\n    )\n    plt.colorbar()\n</code></pre>"},{"location":"reference/spotpython/plot/ts/","title":"ts","text":""},{"location":"reference/spotpython/plot/ts/#spotpython.plot.ts.plot_friedman_drift_data","title":"<code>plot_friedman_drift_data(n_samples, seed, change_point1, change_point2, constant=True, show=True, filename=None)</code>","text":"<p>Plot the Friedman dataset with drifts at change_point1 and change_point2.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> required <code>seed</code> <code>int</code> <p>Seed for the random number generator.</p> required <code>change_point1</code> <code>int</code> <p>Index of the first drift point.</p> required <code>change_point2</code> <code>int</code> <p>Index of the second drift point.</p> required <code>constant</code> <code>bool</code> <p>If True, the drifts are constant. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>Name of the file to save the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.ts import plot_friedman_drift_data\n&gt;&gt;&gt; plot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\n&gt;&gt;&gt; plot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=True)\n</code></pre> Source code in <code>spotpython/plot/ts.py</code> <pre><code>def plot_friedman_drift_data(n_samples, seed, change_point1, change_point2, constant=True, show=True, filename=None) -&gt; None:\n    \"\"\"Plot the Friedman dataset with drifts at change_point1 and change_point2.\n\n    Args:\n        n_samples (int):\n            Number of samples to generate.\n        seed (int):\n            Seed for the random number generator.\n        change_point1 (int):\n            Index of the first drift point.\n        change_point2 (int):\n            Index of the second drift point.\n        constant (bool, optional):\n            If True, the drifts are constant. Defaults to True.\n        filename (str, optional):\n            Name of the file to save the plot. Defaults to None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.ts import plot_friedman_drift_data\n        &gt;&gt;&gt; plot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\n        &gt;&gt;&gt; plot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=True)\n    \"\"\"\n    data_generator = FriedmanDriftDataset(n_samples=n_samples, seed=seed, change_point1=change_point1, change_point2=change_point2, constant=constant)\n    data = [data for data in data_generator]\n    indices = [i for _, _, i in data]\n    values = {f\"x{i}\": [] for i in range(6)}\n    values[\"y\"] = []\n    for x, y, _ in data:\n        for i in range(6):\n            values[f\"x{i}\"].append(x[i])\n        values[\"y\"].append(y)\n\n    plt.figure(figsize=(10, 6))\n    for label, series in values.items():\n        plt.plot(indices, series, label=label)\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n    plt.axvline(x=change_point1, color=\"k\", linestyle=\"--\", label=\"Drift Point 1\")\n    plt.axvline(x=change_point2, color=\"r\", linestyle=\"--\", label=\"Drift Point 2\")\n    plt.legend()\n    plt.grid(True)\n    if filename is not None:\n        plt.savefig(filename)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/validation/","title":"validation","text":""},{"location":"reference/spotpython/plot/validation/#spotpython.plot.validation.plot_actual_vs_predicted","title":"<code>plot_actual_vs_predicted(y_test, y_pred, title=None, show=True, filename=None)</code>","text":"<p>Plot actual vs. predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>ndarray</code> <p>True values.</p> required <code>y_pred</code> <code>ndarray</code> <p>Predicted values.</p> required <code>title</code> <code>str</code> <p>Title of the plot. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>Name of the file to save the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n    from sklearn.linear_model import LinearRegression\n    from spotpython.plot.validation import plot_actual_vs_predicted\n    X, y = load_diabetes(return_X_y=True)\n    lr = LinearRegression()\n    lr.fit(X, y)\n    y_pred = lr.predict(X)\n    plot_actual_vs_predicted(y, y_pred)\n</code></pre> Source code in <code>spotpython/plot/validation.py</code> <pre><code>def plot_actual_vs_predicted(y_test, y_pred, title=None, show=True, filename=None) -&gt; None:\n    \"\"\"Plot actual vs. predicted values.\n\n    Args:\n        y_test (np.ndarray):\n            True values.\n        y_pred (np.ndarray):\n            Predicted values.\n        title (str, optional):\n            Title of the plot. Defaults to None.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to True.\n        filename (str, optional):\n            Name of the file to save the plot. Defaults to None.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n            from sklearn.linear_model import LinearRegression\n            from spotpython.plot.validation import plot_actual_vs_predicted\n            X, y = load_diabetes(return_X_y=True)\n            lr = LinearRegression()\n            lr.fit(X, y)\n            y_pred = lr.predict(X)\n            plot_actual_vs_predicted(y, y_pred)\n    \"\"\"\n    fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"actual_vs_predicted\",\n        subsample=100,\n        ax=axs[0],\n        random_state=0,\n        scatter_kwargs={\"alpha\": 0.5},\n    )\n    axs[0].set_title(\"Actual vs. Predicted values\")\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"residual_vs_predicted\",\n        subsample=100,\n        ax=axs[1],\n        random_state=0,\n    )\n    axs[1].set_title(\"Residuals vs. Predicted Values\")\n    if title is not None:\n        fig.suptitle(title)\n    plt.tight_layout()\n    if filename is not None:\n        plt.savefig(filename)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/validation/#spotpython.plot.validation.plot_confusion_matrix","title":"<code>plot_confusion_matrix(model=None, fun_control=None, df=None, title=None, target_names=None, y_true_name=None, y_pred_name=None, show=False, ax=None)</code>","text":"<p>Plotting a confusion matrix. If a model and the fun_control dictionary are passed, the confusion matrix is computed. If a dataframe is passed, the confusion matrix is computed from the dataframe. In this case, the names of the columns with the true and the predicted values must be specified. Default the dataframe is None.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Sklearn model. The model to be used for cross-validation. Defaults to None.</p> <code>None</code> <code>fun_control</code> <code>Dict</code> <p>Dictionary containing the data and the target column. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to None.</p> <code>None</code> <code>df</code> <code>DataFrame</code> <p>Dataframe containing the predictions and the target column. Defaults to None.</p> <code>None</code> <code>target_names</code> <code>List[str]</code> <p>List of target names. Defaults to None.</p> <code>None</code> <code>y_true_name</code> <code>str</code> <p>Name of the column with the true values if a dataframe is specified. Defaults to None.</p> <code>None</code> <code>y_pred_name</code> <code>str</code> <p>Name of the column with the predicted values if a dataframe is specified. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to False.</p> <code>False</code> <code>ax</code> <code>AxesSubplot</code> <p>Axes to plot the confusion matrix. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotpython/plot/validation.py</code> <pre><code>def plot_confusion_matrix(\n    model=None,\n    fun_control=None,\n    df=None,\n    title=None,\n    target_names=None,\n    y_true_name=None,\n    y_pred_name=None,\n    show=False,\n    ax=None,\n):\n    \"\"\"\n    Plotting a confusion matrix. If a model and the fun_control dictionary are passed,\n    the confusion matrix is computed. If a dataframe is passed, the confusion matrix is\n    computed from the dataframe. In this case, the names of the columns with the true and\n    the predicted values must be specified. Default the dataframe is None.\n\n    Args:\n        model (Any, optional):\n            Sklearn model. The model to be used for cross-validation. Defaults to None.\n        fun_control (Dict, optional):\n            Dictionary containing the data and the target column. Defaults to None.\n        title (str, optional):\n            Title of the plot. Defaults to None.\n        df (pd.DataFrame, optional):\n            Dataframe containing the predictions and the target column. Defaults to None.\n        target_names (List[str], optional):\n            List of target names. Defaults to None.\n        y_true_name (str, optional):\n            Name of the column with the true values if a dataframe is specified. Defaults to None.\n        y_pred_name (str, optional):\n            Name of the column with the predicted values if a dataframe is specified. Defaults to None.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to False.\n        ax (matplotlib.axes._subplots.AxesSubplot, optional):\n            Axes to plot the confusion matrix. Defaults to None.\n\n    Returns:\n        (NoneType): None\n\n    \"\"\"\n    if df is not None:\n        # assign the column y_true_name from df to y_true\n        y_true = df[y_true_name]\n        # assign the column y_pred_name from df to y_pred\n        y_pred = df[y_pred_name]\n    else:\n        X_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n        X_test, y_true = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 5))\n    ConfusionMatrixDisplay.from_predictions(y_true=y_true, y_pred=y_pred, ax=ax, colorbar=False)\n    if target_names is not None:\n        ax.xaxis.set_ticklabels(target_names)\n        ax.yaxis.set_ticklabels(target_names)\n    if title is not None:\n        _ = ax.set_title(title)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/validation/#spotpython.plot.validation.plot_cv_predictions","title":"<code>plot_cv_predictions(model, fun_control, show=True)</code>","text":"<p>Plots cross-validated predictions for regression.</p> <p>Uses <code>sklearn.model_selection.cross_val_predict</code> together with <code>sklearn.metrics.PredictionErrorDisplay</code> to visualize prediction errors. It is based on the example from the scikit-learn documentation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-download-auto-examples-model-selection-plot-cv-predict-py</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Sklearn model. The model to be used for cross-validation.</p> required <code>fun_control</code> <code>Dict</code> <p>Dictionary containing the data and the target column.</p> required <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; X, y = load_diabetes(return_X_y=True)\n&gt;&gt;&gt; lr = LinearRegression()\n&gt;&gt;&gt; plot_cv_predictions(lr, fun_control)\n</code></pre> Source code in <code>spotpython/plot/validation.py</code> <pre><code>def plot_cv_predictions(model: Any, fun_control: Dict, show=True) -&gt; None:\n    \"\"\"\n    Plots cross-validated predictions for regression.\n\n    Uses `sklearn.model_selection.cross_val_predict` together with\n    `sklearn.metrics.PredictionErrorDisplay` to visualize prediction errors.\n    It is based on the example from the scikit-learn documentation:\n    https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-download-auto-examples-model-selection-plot-cv-predict-py\n\n    Args:\n        model (Any):\n            Sklearn model. The model to be used for cross-validation.\n        fun_control (Dict):\n            Dictionary containing the data and the target column.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to True.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; X, y = load_diabetes(return_X_y=True)\n        &gt;&gt;&gt; lr = LinearRegression()\n        &gt;&gt;&gt; plot_cv_predictions(lr, fun_control)\n    \"\"\"\n    X_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n    # cross_val_predict returns an array of the same size of y\n    # where each entry is a prediction obtained by cross validation.\n    y_pred = cross_val_predict(model, X_test, y_test, cv=10)\n    fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"actual_vs_predicted\",\n        subsample=100,\n        ax=axs[0],\n        random_state=0,\n    )\n    axs[0].set_title(\"Actual vs. Predicted values\")\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"residual_vs_predicted\",\n        subsample=100,\n        ax=axs[1],\n        random_state=0,\n    )\n    axs[1].set_title(\"Residuals vs. Predicted Values\")\n    fig.suptitle(\"Plotting cross-validated predictions\")\n    plt.tight_layout()\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/validation/#spotpython.plot.validation.plot_roc","title":"<code>plot_roc(model_list, fun_control, alpha=0.8, model_names=None, show=True)</code>","text":"<p>Plots ROC curves for a list of models using the Visualization API from scikit-learn.</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>List[BaseEstimator]</code> <p>A list of scikit-learn models to plot ROC curves for.</p> required <code>fun_control</code> <code>Dict[str, Union[str, DataFrame]]</code> <p>A dictionary containing the train and test dataframes and the target column name.</p> required <code>alpha</code> <code>float</code> <p>The alpha value for the ROC curve. Defaults to 0.8.</p> <code>0.8</code> <code>model_names</code> <code>List[str]</code> <p>A list of names for the models. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n&gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X_train = iris.data[:100]\n&gt;&gt;&gt; y_train = iris.target[:100]\n&gt;&gt;&gt; X_test = iris.data[100:]\n&gt;&gt;&gt; y_test = iris.target[100:]\n&gt;&gt;&gt; train_df = pd.DataFrame(X_train, columns=iris.feature_names)\n&gt;&gt;&gt; train_df['target'] = y_train\n&gt;&gt;&gt; test_df = pd.DataFrame(X_test, columns=iris.feature_names)\n&gt;&gt;&gt; test_df['target'] = y_test\n&gt;&gt;&gt; fun_control = {\"train\": train_df, \"test\": test_df, \"target_column\": \"target\"}\n&gt;&gt;&gt; model_list = [LogisticRegression(), DecisionTreeClassifier()]\n&gt;&gt;&gt; model_names = [\"Logistic Regression\", \"Decision Tree\"]\n&gt;&gt;&gt; plot_roc(model_list, fun_control, model_names=model_names)\n</code></pre> Source code in <code>spotpython/plot/validation.py</code> <pre><code>def plot_roc(\n    model_list: List[BaseEstimator],\n    fun_control: Dict[str, Union[str, pd.DataFrame]],\n    alpha: float = 0.8,\n    model_names: List[str] = None,\n    show=True,\n) -&gt; None:\n    \"\"\"\n    Plots ROC curves for a list of models using the Visualization API from scikit-learn.\n\n    Args:\n        model_list (List[BaseEstimator]):\n            A list of scikit-learn models to plot ROC curves for.\n        fun_control (Dict[str, Union[str, pd.DataFrame]]):\n            A dictionary containing the train and test dataframes and the target column name.\n        alpha (float, optional):\n            The alpha value for the ROC curve. Defaults to 0.8.\n        model_names (List[str], optional):\n            A list of names for the models. Defaults to None.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to True.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n        &gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; X_train = iris.data[:100]\n        &gt;&gt;&gt; y_train = iris.target[:100]\n        &gt;&gt;&gt; X_test = iris.data[100:]\n        &gt;&gt;&gt; y_test = iris.target[100:]\n        &gt;&gt;&gt; train_df = pd.DataFrame(X_train, columns=iris.feature_names)\n        &gt;&gt;&gt; train_df['target'] = y_train\n        &gt;&gt;&gt; test_df = pd.DataFrame(X_test, columns=iris.feature_names)\n        &gt;&gt;&gt; test_df['target'] = y_test\n        &gt;&gt;&gt; fun_control = {\"train\": train_df, \"test\": test_df, \"target_column\": \"target\"}\n        &gt;&gt;&gt; model_list = [LogisticRegression(), DecisionTreeClassifier()]\n        &gt;&gt;&gt; model_names = [\"Logistic Regression\", \"Decision Tree\"]\n        &gt;&gt;&gt; plot_roc(model_list, fun_control, model_names=model_names)\n    \"\"\"\n    X_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n    X_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n    ax = plt.gca()\n    for i, model in enumerate(model_list):\n        model.fit(X_train, y_train)\n        if model_names is not None:\n            model_name = model_names[i]\n        else:\n            model_name = None\n        y_pred = model.predict(X_test)\n        RocCurveDisplay.from_predictions(y_test, y_pred, ax=ax, alpha=alpha, name=model_name)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/validation/#spotpython.plot.validation.plot_roc_from_dataframes","title":"<code>plot_roc_from_dataframes(df_list, alpha=0.8, model_names=None, target_column=None, show=True, title='', tkagg=False)</code>","text":"<p>Plot ROC curve for a list of dataframes from model evaluations.</p> <p>Parameters:</p> Name Type Description Default <code>df_list</code> <code>List[DataFrame]</code> <p>List of dataframes with results from models.</p> required <code>alpha</code> <code>float</code> <p>Transparency of the plotted lines.</p> <code>0.8</code> <code>model_names</code> <code>List[str]</code> <p>List of model names.</p> <code>None</code> <code>target_column</code> <code>str</code> <p>Name of the target column.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown.</p> <code>True</code> <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>''</code> <code>tkagg</code> <code>bool</code> <p>If True, the TkAgg backend is used. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n    from spotpython.plot.validation import plot_roc_from_dataframes\n    df1 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,0,0]})\n    df2 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,1,1]})\n    df_list = [df1, df2]\n    model_names = [\"Model 1\", \"Model 2\"]\n    plot_roc_from_dataframes(df_list, model_names=model_names, target_column=\"y\")\n</code></pre> Source code in <code>spotpython/plot/validation.py</code> <pre><code>def plot_roc_from_dataframes(\n    df_list: List[pd.DataFrame],\n    alpha: float = 0.8,\n    model_names: List[str] = None,\n    target_column: str = None,\n    show: bool = True,\n    title: str = \"\",\n    tkagg: bool = False,\n) -&gt; None:\n    \"\"\"\n    Plot ROC curve for a list of dataframes from model evaluations.\n\n    Args:\n        df_list:\n            List of dataframes with results from models.\n        alpha:\n            Transparency of the plotted lines.\n        model_names:\n            List of model names.\n        target_column:\n            Name of the target column.\n        show:\n            If True, the plot is shown.\n        title:\n            Title of the plot.\n        tkagg:\n            If True, the TkAgg backend is used.\n            Default is False.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n            from spotpython.plot.validation import plot_roc_from_dataframes\n            df1 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,0,0]})\n            df2 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,1,1]})\n            df_list = [df1, df2]\n            model_names = [\"Model 1\", \"Model 2\"]\n            plot_roc_from_dataframes(df_list, model_names=model_names, target_column=\"y\")\n\n    \"\"\"\n    if tkagg:\n        matplotlib.use(\"TkAgg\")\n    fig, ax = plt.subplots(figsize=(10, 5))\n    for i, df in enumerate(df_list):\n        y_test = df[target_column]\n        y_pred = df[\"Prediction\"]\n        if model_names is not None:\n            model_name = model_names[i]\n        else:\n            model_name = None\n        RocCurveDisplay.from_predictions(y_test, y_pred, ax=ax, alpha=alpha, name=model_name)\n    # add a title to the plot\n    ax.set_title(title)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/xai/","title":"xai","text":""},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.check_for_nans","title":"<code>check_for_nans(data, layer_index)</code>","text":"<p>Checks for NaN values in the tensor data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The tensor to check for NaN values.</p> required <code>layer_index</code> <code>int</code> <p>The index of the layer for logging purposes.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if NaNs are found, False otherwise.</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def check_for_nans(data, layer_index) -&gt; bool:\n    \"\"\"Checks for NaN values in the tensor data.\n\n    Args:\n        data (torch.Tensor): The tensor to check for NaN values.\n        layer_index (int): The index of the layer for logging purposes.\n\n    Returns:\n        bool: True if NaNs are found, False otherwise.\n    \"\"\"\n    if torch.isnan(data).any():\n        print(f\"NaN detected after layer {layer_index}\")\n        return True\n    return False\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_activations","title":"<code>get_activations(net, fun_control, batch_size, device='cpu', normalize=False)</code>","text":"<p>Computes the activations for each layer of the network, the mean activations, and the sizes of the activations for each layer.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>The neural network model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for the data loader.</p> required <code>device</code> <code>str</code> <p>The device to run the model on. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the activations, mean activations, and layer sizes for each layer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.xai import get_activations\n    import torch\n    import numpy as np\n    import torch.nn as nn\n    from spotpython.utils.init import fun_control_init\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (\n            get_default_hyperparameters_as_array, get_one_config_from_X)\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.plot.xai import get_gradients\n    fun_control = fun_control_init(\n        _L_in=10, # 10: diabetes\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\",\n        data_set=Diabetes(),\n        core_model=NNLinearRegressor,\n        hyperdict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    batch_size = 16\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n    activations, mean_activations, layer_sizes = get_activations(net=model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n    plot_nn_values_scatter(nn_values=activations, layer_sizes=layer_sizes, nn_values_names=\"Activations\")\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_activations(net, fun_control, batch_size, device=\"cpu\", normalize=False) -&gt; tuple:\n    \"\"\"\n    Computes the activations for each layer of the network, the mean activations,\n    and the sizes of the activations for each layer.\n\n    Args:\n        net (nn.Module): The neural network model.\n        fun_control (dict): A dictionary containing the dataset.\n        batch_size (int): The batch size for the data loader.\n        device (str): The device to run the model on. Defaults to \"cpu\".\n        normalize (bool): Whether to normalize the input data. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing the activations, mean activations, and layer sizes for each layer.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.xai import get_activations\n            import torch\n            import numpy as np\n            import torch.nn as nn\n            from spotpython.utils.init import fun_control_init\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (\n                    get_default_hyperparameters_as_array, get_one_config_from_X)\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.plot.xai import get_gradients\n            fun_control = fun_control_init(\n                _L_in=10, # 10: diabetes\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\",\n                data_set=Diabetes(),\n                core_model=NNLinearRegressor,\n                hyperdict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            config = get_one_config_from_X(X, fun_control)\n            _L_in = fun_control[\"_L_in\"]\n            _L_out = fun_control[\"_L_out\"]\n            _torchmetric = fun_control[\"_torchmetric\"]\n            batch_size = 16\n            model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n            activations, mean_activations, layer_sizes = get_activations(net=model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n            plot_nn_values_scatter(nn_values=activations, layer_sizes=layer_sizes, nn_values_names=\"Activations\")\n    \"\"\"\n    activations = {}\n    mean_activations = {}\n    layer_sizes = {}\n    net.eval()  # Set the model to evaluation mode\n\n    dataset = fun_control[\"data_set\"]\n    data_module = LightDataModule(\n        dataset=dataset,\n        batch_size=batch_size,\n        test_size=fun_control[\"test_size\"],\n        scaler=fun_control[\"scaler\"],\n        verbosity=10,\n    )\n    data_module.setup(stage=\"fit\")\n    train_loader = data_module.train_dataloader()\n    inputs, _ = next(iter(train_loader))\n    inputs = inputs.to(device)\n\n    if normalize:\n        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n\n    with torch.no_grad():\n        inputs = inputs.view(inputs.size(0), -1)\n        # Loop through all layers\n        for layer_index, layer in enumerate(net.layers[:-1]):\n            inputs = layer(inputs)  # Forward pass through the layer\n\n            # Check for NaNs\n            if check_for_nans(inputs, layer_index):\n                break\n\n            # Collect activations for Linear layers\n            if isinstance(layer, nn.Linear):\n                activations[layer_index] = inputs.view(-1).cpu().numpy()\n                mean_activations[layer_index] = inputs.mean(dim=0).cpu().numpy()\n                # Record the size of the activations and set the first dimension to 1\n                layer_size = np.array(inputs.size())\n                layer_size[0] = 1  # Set the first dimension to 1\n                layer_sizes[layer_index] = layer_size\n\n    return activations, mean_activations, layer_sizes\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_all_layers_conductance","title":"<code>get_all_layers_conductance(spot_tuner, fun_control, device='cpu', remove_spot_attributes=False)</code>","text":"<p>Get the conductance of all layers.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>The spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>remove_spot_attributes</code> <code>bool</code> <p>Whether to remove the spot attributes. Defaults to False.</p> <code>False</code> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_all_layers_conductance(spot_tuner, fun_control, device=\"cpu\", remove_spot_attributes=False) -&gt; dict:\n    \"\"\"\n    Get the conductance of all layers.\n\n    Args:\n        spot_tuner (object):\n            The spot tuner object.\n        fun_control (dict):\n            A dictionary with the function control.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        remove_spot_attributes (bool, optional):\n            Whether to remove the spot attributes. Defaults to False.\n    \"\"\"\n    config = get_tuned_architecture(spot_tuner, fun_control)\n    train_model(config, fun_control, timestamp=False)\n    model_loaded = load_light_from_checkpoint(config, fun_control, postfix=\"_TRAIN\")\n    if remove_spot_attributes:\n        removed_attributes, model = get_removed_attributes_and_base_net(net=model_loaded)\n    else:\n        model = model_loaded\n    model = model.to(device)\n    model.eval()\n    _, index, _ = get_weights(model, return_index=True)\n    layer_conductance = {}\n    for i in index:\n        layer_conductance[i] = get_layer_conductance(spot_tuner, fun_control, layer_idx=i)\n    return layer_conductance\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_attributions","title":"<code>get_attributions(spot_tuner, fun_control, attr_method='IntegratedGradients', baseline=None, abs_attr=True, n_rel=5, device='cpu', normalize=True, remove_spot_attributes=False)</code>","text":"<p>Get the attributions of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>The spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>attr_method</code> <code>str</code> <p>The attribution method. Defaults to \u201cIntegratedGradients\u201d.</p> <code>'IntegratedGradients'</code> <code>baseline</code> <code>Tensor</code> <p>The baseline for the attribution methods. Defaults to None.</p> <code>None</code> <code>abs_attr</code> <code>bool</code> <p>Whether the method should sort by the absolute attribution values. Defaults to True.</p> <code>True</code> <code>n_rel</code> <code>int</code> <p>The number of relevant features. Defaults to 5.</p> <code>5</code> <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to True.</p> <code>True</code> <code>remove_spot_attributes</code> <code>bool</code> <p>Whether to remove the spot attributes. If True, a torch model is created via <code>get_removed_attributes</code>. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame (object): A DataFrame with the attributions.</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_attributions(\n    spot_tuner,\n    fun_control,\n    attr_method=\"IntegratedGradients\",\n    baseline=None,\n    abs_attr=True,\n    n_rel=5,\n    device=\"cpu\",\n    normalize=True,\n    remove_spot_attributes=False,\n) -&gt; pd.DataFrame:\n    \"\"\"Get the attributions of a neural network.\n\n    Args:\n        spot_tuner (object):\n            The spot tuner object.\n        fun_control (dict):\n            A dictionary with the function control.\n        attr_method (str, optional):\n            The attribution method. Defaults to \"IntegratedGradients\".\n        baseline (torch.Tensor, optional):\n            The baseline for the attribution methods. Defaults to None.\n        abs_attr (bool, optional):\n            Whether the method should sort by the absolute attribution values. Defaults to True.\n        n_rel (int, optional):\n            The number of relevant features. Defaults to 5.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        normalize (bool, optional):\n            Whether to normalize the input data. Defaults to True.\n        remove_spot_attributes (bool, optional):\n            Whether to remove the spot attributes.\n            If True, a torch model is created via `get_removed_attributes`. Defaults to False.\n\n    Returns:\n        pd.DataFrame (object): A DataFrame with the attributions.\n    \"\"\"\n    try:\n        fun_control[\"data_set\"].names\n    except AttributeError:\n        fun_control[\"data_set\"].names = None\n    feature_names = fun_control[\"data_set\"].names\n    total_attributions = None\n    config = get_tuned_architecture(spot_tuner, fun_control)\n    train_model(config, fun_control, timestamp=False)\n    model_loaded = load_light_from_checkpoint(config, fun_control, postfix=\"_TRAIN\")\n    if remove_spot_attributes:\n        removed_attributes, model = get_removed_attributes_and_base_net(net=model_loaded)\n    else:\n        model = model_loaded\n    model = model.to(device)\n    model.eval()\n    # get feature names\n    dataset = fun_control[\"data_set\"]\n    try:\n        n_features = dataset.data.shape[1]\n    except AttributeError:\n        n_features = dataset.tensors[0].shape[1]\n    if feature_names is None:\n        feature_names = [f\"x{i}\" for i in range(n_features)]\n    # get batch size\n    batch_size = config[\"batch_size\"]\n    # test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    data_module = LightDataModule(\n        dataset=dataset,\n        batch_size=batch_size,\n        test_size=fun_control[\"test_size\"],\n        scaler=fun_control[\"scaler\"],\n        verbosity=10,\n    )\n    data_module.setup(stage=\"test\")\n    test_loader = data_module.test_dataloader()\n\n    if attr_method == \"IntegratedGradients\":\n        attr = IntegratedGradients(model)\n    elif attr_method == \"DeepLift\":\n        attr = DeepLift(model)\n    elif attr_method == \"GradientShap\":  # Todo: would need a baseline\n        if baseline is None:\n            raise ValueError(\"baseline cannot be 'None' for GradientShap\")\n        attr = GradientShap(model)\n    elif attr_method == \"FeatureAblation\":\n        attr = FeatureAblation(model)\n    else:\n        raise ValueError(\n            \"\"\"\n            Unsupported attribution method.\n            Please choose from 'IntegratedGradients', 'DeepLift', 'GradientShap', or 'FeatureAblation'.\n            \"\"\"\n        )\n    for inputs, _ in test_loader:\n        if normalize:\n            inputs = (inputs - inputs.mean()) / inputs.std()\n        inputs.requires_grad_()\n        attributions = attr.attribute(inputs, return_convergence_delta=False, baselines=baseline)\n        if total_attributions is None:\n            total_attributions = attributions\n        else:\n            if len(attributions) == len(total_attributions):\n                total_attributions += attributions\n\n    # Calculation of average attribution across all batches\n    avg_attributions = total_attributions.mean(dim=0).detach().numpy()\n\n    # Transformation to the absolute attribution values if abs_attr is True\n    # Get indices of the n most important features\n    if abs_attr is True:\n        abs_avg_attributions = abs(avg_attributions)\n        top_n_indices = abs_avg_attributions.argsort()[-n_rel:][::-1]\n    else:\n        top_n_indices = avg_attributions.argsort()[-n_rel:][::-1]\n\n    # Get the importance values for the top n features\n    top_n_importances = avg_attributions[top_n_indices]\n\n    df = pd.DataFrame(\n        {\n            \"Feature Index\": top_n_indices,\n            \"Feature\": [feature_names[i] for i in top_n_indices],\n            attr_method + \"Attribution\": top_n_importances,\n        }\n    )\n    return df\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_gradients","title":"<code>get_gradients(net, fun_control, batch_size, device='cpu', normalize=False)</code>","text":"<p>Get the gradients of a neural network and the size of each layer.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - grads: A dictionary with the gradients of the neural network. - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.xai import get_gradients\n    import torch\n    import numpy as np\n    import torch.nn as nn\n    from spotpython.utils.init import fun_control_init\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (\n            get_default_hyperparameters_as_array, get_one_config_from_X)\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.data.lightdatamodule import LightDataModule\n    # from spotpython.plot.xai import get_gradients\n    fun_control = fun_control_init(\n        _L_in=10, # 10: diabetes\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\",\n        data_set=Diabetes(),\n        core_model=NNLinearRegressor,\n        hyperdict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    batch_size = 16\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n    gradients, layer_sizes = get_gradients(net=model)\n    gradients, layer_sizes\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_gradients(net, fun_control, batch_size, device=\"cpu\", normalize=False) -&gt; tuple:\n    \"\"\"\n    Get the gradients of a neural network and the size of each layer.\n\n    Args:\n        net (object):\n            A neural network.\n        fun_control (dict):\n            A dictionary with the function control.\n        batch_size (int, optional):\n            The batch size.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        normalize (bool, optional):\n            Whether to normalize the input data. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing:\n            - grads: A dictionary with the gradients of the neural network.\n            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.xai import get_gradients\n            import torch\n            import numpy as np\n            import torch.nn as nn\n            from spotpython.utils.init import fun_control_init\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (\n                    get_default_hyperparameters_as_array, get_one_config_from_X)\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.data.lightdatamodule import LightDataModule\n            # from spotpython.plot.xai import get_gradients\n            fun_control = fun_control_init(\n                _L_in=10, # 10: diabetes\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\",\n                data_set=Diabetes(),\n                core_model=NNLinearRegressor,\n                hyperdict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            config = get_one_config_from_X(X, fun_control)\n            _L_in = fun_control[\"_L_in\"]\n            _L_out = fun_control[\"_L_out\"]\n            _torchmetric = fun_control[\"_torchmetric\"]\n            batch_size = 16\n            model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n            gradients, layer_sizes = get_gradients(net=model)\n            gradients, layer_sizes\n    \"\"\"\n    net.eval()\n    dataset = fun_control[\"data_set\"]\n    data_module = LightDataModule(\n        dataset=dataset,\n        batch_size=batch_size,\n        test_size=fun_control[\"test_size\"],\n        scaler=fun_control[\"scaler\"],\n        verbosity=10,\n    )\n    data_module.setup(stage=\"fit\")\n    train_loader = data_module.train_dataloader()\n    inputs, targets = next(iter(train_loader))\n    if normalize:\n        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    # Pass one batch through the network, and calculate the gradients for the weights\n    net.zero_grad()\n    preds = net(inputs)\n    preds = preds.squeeze(-1)  # Remove the last dimension if it's 1\n    loss = F.mse_loss(preds, targets)\n    loss.backward()\n\n    grads = {}\n    layer_sizes = {}\n    for name, params in net.named_parameters():\n        if \"weight\" in name:\n            # Collect gradient information\n            grads[name] = params.grad.view(-1).cpu().clone().numpy()\n            # Collect size information\n            layer_sizes[name] = np.array(params.size())\n\n    net.zero_grad()\n    return grads, layer_sizes\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_layer_conductance","title":"<code>get_layer_conductance(spot_tuner, fun_control, layer_idx, device='cpu', normalize=True, remove_spot_attributes=False)</code>","text":"<p>Compute the average layer conductance attributions for a specified layer in the model.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>Spot</code> <p>The spot tuner object containing the trained model.</p> required <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary containing the hyperparameters used to train the model.</p> required <code>layer_idx</code> <code>int</code> <p>Index of the layer for which to compute layer conductance attributions.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to True.</p> <code>True</code> <code>remove_spot_attributes</code> <code>bool</code> <p>Whether to remove the spot attributes. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: An array containing the average layer conductance attributions for the specified layer. The shape of the array corresponds to the shape of the attributions.</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_layer_conductance(spot_tuner, fun_control, layer_idx, device=\"cpu\", normalize=True, remove_spot_attributes=False) -&gt; np.ndarray:\n    \"\"\"\n    Compute the average layer conductance attributions for a specified layer in the model.\n\n    Args:\n        spot_tuner (spot.Spot):\n            The spot tuner object containing the trained model.\n        fun_control (dict):\n            The fun_control dictionary containing the hyperparameters used to train the model.\n        layer_idx (int):\n            Index of the layer for which to compute layer conductance attributions.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        normalize (bool, optional):\n            Whether to normalize the input data. Defaults to True.\n        remove_spot_attributes (bool, optional):\n            Whether to remove the spot attributes. Defaults to False.\n\n    Returns:\n        numpy.ndarray:\n            An array containing the average layer conductance attributions for the specified layer.\n            The shape of the array corresponds to the shape of the attributions.\n    \"\"\"\n    try:\n        fun_control[\"data_set\"].names\n    except AttributeError:\n        fun_control[\"data_set\"].names = None\n    feature_names = fun_control[\"data_set\"].names\n\n    config = get_tuned_architecture(spot_tuner, fun_control)\n    train_model(config, fun_control, timestamp=False)\n    model_loaded = load_light_from_checkpoint(config, fun_control, postfix=\"_TRAIN\")\n    if remove_spot_attributes:\n        removed_attributes, model = get_removed_attributes_and_base_net(net=model_loaded)\n    else:\n        model = model_loaded\n    model = model.to(device)\n    model.eval()\n\n    dataset = fun_control[\"data_set\"]\n    try:\n        n_features = dataset.data.shape[1]\n    except AttributeError:\n        n_features = dataset.tensors[0].shape[1]\n    if feature_names is None:\n        feature_names = [f\"x{i}\" for i in range(n_features)]\n    batch_size = config[\"batch_size\"]\n    test_loader = DataLoader(dataset, batch_size=batch_size)\n    total_layer_attributions = None\n    layers = model.layers\n    print(\"Conductance analysis for layer: \", layers[layer_idx])\n    lc = LayerConductance(model, layers[layer_idx])\n\n    for inputs, labels in test_loader:\n        if normalize:\n            inputs = (inputs - inputs.mean()) / inputs.std()\n        lc_attr_test = lc.attribute(inputs, n_steps=10, attribute_to_layer_input=True)\n        if total_layer_attributions is None:\n            total_layer_attributions = lc_attr_test\n        else:\n            if len(lc_attr_test) == len(total_layer_attributions):\n                total_layer_attributions += lc_attr_test\n\n    avg_layer_attributions = total_layer_attributions.mean(dim=0).detach().numpy()\n\n    return avg_layer_attributions\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_weights","title":"<code>get_weights(net, return_index=False)</code>","text":"<p>Get the weights of a neural network and the size of each layer.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>return_index</code> <code>bool</code> <p>Whether to return the index. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - weights: A dictionary with the weights of the neural network. - index: The layer index list (only if return_index is True). - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.xai import get_weights\n    import torch\n    import numpy as np\n    import torch.nn as nn\n    from spotpython.utils.init import fun_control_init\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (\n            get_default_hyperparameters_as_array, get_one_config_from_X)\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.plot.xai import get_gradients\n    fun_control = fun_control_init(\n        _L_in=10, # 10: diabetes\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\",\n        data_set=Diabetes(),\n        core_model=NNLinearRegressor,\n        hyperdict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    batch_size = 16\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n    weights, layer_sizes = get_weights(net=model)\n    weights, layer_sizes\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_weights(net, return_index=False) -&gt; tuple:\n    \"\"\"\n    Get the weights of a neural network and the size of each layer.\n\n    Args:\n        net (object):\n            A neural network.\n        return_index (bool, optional):\n            Whether to return the index. Defaults to False.\n\n    Returns:\n        tuple:\n            A tuple containing:\n            - weights: A dictionary with the weights of the neural network.\n            - index: The layer index list (only if return_index is True).\n            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.xai import get_weights\n            import torch\n            import numpy as np\n            import torch.nn as nn\n            from spotpython.utils.init import fun_control_init\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (\n                    get_default_hyperparameters_as_array, get_one_config_from_X)\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.plot.xai import get_gradients\n            fun_control = fun_control_init(\n                _L_in=10, # 10: diabetes\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\",\n                data_set=Diabetes(),\n                core_model=NNLinearRegressor,\n                hyperdict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            config = get_one_config_from_X(X, fun_control)\n            _L_in = fun_control[\"_L_in\"]\n            _L_out = fun_control[\"_L_out\"]\n            _torchmetric = fun_control[\"_torchmetric\"]\n            batch_size = 16\n            model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n            weights, layer_sizes = get_weights(net=model)\n            weights, layer_sizes\n    \"\"\"\n    weights = {}\n    index = []\n    layer_sizes = {}\n\n    for name, param in net.named_parameters():\n        if name.endswith(\".bias\"):\n            continue\n\n        # Extract layer number\n        layer_number = int(name.split(\".\")[1])\n        index.append(layer_number)\n\n        # Create dictionary key for this layer\n        key_name = f\"Layer {layer_number}\"\n\n        # Store weight information\n        weights[key_name] = param.detach().view(-1).cpu().numpy()\n\n        # Store layer size as a NumPy array\n        layer_sizes[key_name] = np.array(param.size())\n\n    if return_index:\n        return weights, index, layer_sizes\n    else:\n        return weights, layer_sizes\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_weights_conductance_last_layer","title":"<code>get_weights_conductance_last_layer(spot_tuner, fun_control, device='cpu', remove_spot_attributes=False)</code>","text":"<p>Get the weights and the conductance of the last layer.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>The spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>remove_spot_attributes</code> <code>bool</code> <p>Whether to remove the spot attributes. Defaults to False.</p> <code>False</code> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_weights_conductance_last_layer(spot_tuner, fun_control, device=\"cpu\", remove_spot_attributes=False) -&gt; tuple:\n    \"\"\"\n    Get the weights and the conductance of the last layer.\n\n    Args:\n        spot_tuner (object):\n            The spot tuner object.\n        fun_control (dict):\n            A dictionary with the function control.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        remove_spot_attributes (bool, optional):\n            Whether to remove the spot attributes. Defaults to False.\n    \"\"\"\n    config = get_tuned_architecture(spot_tuner, fun_control)\n    train_model(config, fun_control, timestamp=False)\n    model_loaded = load_light_from_checkpoint(config, fun_control, postfix=\"_TRAIN\")\n    if remove_spot_attributes:\n        removed_attributes, model = get_removed_attributes_and_base_net(net=model_loaded)\n    else:\n        model = model_loaded\n    model = model.to(device)\n    model.eval()\n\n    weights, index, _ = get_weights(model, return_index=True)\n    layer_idx = index[-1]\n    weights_last = weights[f\"Layer {layer_idx}\"]\n    weights_last\n    layer_conductance_last = get_layer_conductance(spot_tuner, fun_control, layer_idx=layer_idx)\n\n    return weights_last, layer_conductance_last\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.is_square","title":"<code>is_square(n)</code>","text":"<p>Check if a number is a square number.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the number is a square number, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_square(4)\nTrue\n&gt;&gt;&gt; is_square(5)\nFalse\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def is_square(n) -&gt; bool:\n    \"\"\"Check if a number is a square number.\n\n    Args:\n        n (int): The number to check.\n\n    Returns:\n        bool: True if the number is a square number, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; is_square(4)\n        True\n        &gt;&gt;&gt; is_square(5)\n        False\n    \"\"\"\n    return n == int(math.sqrt(n)) ** 2\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.plot_attributions","title":"<code>plot_attributions(df, attr_method='IntegratedGradients')</code>","text":"<p>Plot the attributions of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame with the attributions.</p> required <code>attr_method</code> <code>str</code> <p>The attribution method. Defaults to \u201cIntegratedGradients\u201d.</p> <code>'IntegratedGradients'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def plot_attributions(df, attr_method=\"IntegratedGradients\") -&gt; None:\n    \"\"\"\n    Plot the attributions of a neural network.\n\n    Args:\n        df (pd.DataFrame):\n            A DataFrame with the attributions.\n        attr_method (str, optional):\n            The attribution method. Defaults to \"IntegratedGradients\".\n\n    Returns:\n        None\n\n    \"\"\"\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=attr_method + \"Attribution\", y=\"Feature\", data=df, palette=\"viridis\", hue=\"Feature\")\n    plt.title(f\"Top {df.shape[0]} Features by {attr_method} Attribution\")\n    plt.xlabel(f\"{attr_method} Attribution Value\")\n    plt.ylabel(\"Feature\")\n    plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.plot_conductance_last_layer","title":"<code>plot_conductance_last_layer(weights_last, layer_conductance_last, figsize=(12, 6), show=True)</code>","text":"<p>Plot the conductance of the last layer.</p> <p>Parameters:</p> Name Type Description Default <code>weights_last</code> <code>ndarray</code> <p>The weights of the last layer.</p> required <code>layer_conductance_last</code> <code>ndarray</code> <p>The conductance of the last layer.</p> required <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 6).</p> <code>(12, 6)</code> <code>show</code> <code>bool</code> <p>Whether to show the plot. Defaults</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.plot.xai import plot_conductance_last_layer\n    weights_last = np.random.rand(10)\n    layer_conductance_last = np.random.rand(10)\n    plot_conductance_last_layer(weights_last, layer_conductance_last, show=True)\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def plot_conductance_last_layer(weights_last, layer_conductance_last, figsize=(12, 6), show=True) -&gt; None:\n    \"\"\"\n    Plot the conductance of the last layer.\n\n    Args:\n        weights_last (np.ndarray):\n            The weights of the last layer.\n        layer_conductance_last (np.ndarray):\n            The conductance of the last layer.\n        figsize (tuple, optional):\n            The figure size. Defaults to (12, 6).\n        show (bool, optional):\n            Whether to show the plot. Defaults\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.plot.xai import plot_conductance_last_layer\n            weights_last = np.random.rand(10)\n            layer_conductance_last = np.random.rand(10)\n            plot_conductance_last_layer(weights_last, layer_conductance_last, show=True)\n    \"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.bar(range(len(weights_last)), weights_last / weights_last.max(), label=\"Weights\", alpha=0.5)\n    ax.bar(\n        range(len(layer_conductance_last)),\n        layer_conductance_last / layer_conductance_last.max(),\n        label=\"Layer Conductance\",\n        alpha=0.5,\n    )\n    ax.set_xlabel(\"Weight Index\")\n    ax.set_ylabel(\"Normalized Value\")\n    ax.set_title(\"Layer Conductance vs. Weights\")\n    ax.legend()\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.plot_nn_values_hist","title":"<code>plot_nn_values_hist(nn_values, net, nn_values_names='', color='C0', columns=2)</code>","text":"<p>Plot the values of a neural network. Can be used to plot the weights, gradients, or activations of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>nn_values</code> <code>dict</code> <p>A dictionary with the values of the neural network. For example, the weights, gradients, or activations.</p> required <code>net</code> <code>object</code> <p>A neural network.</p> required <code>color</code> <code>str</code> <p>The color to use. Defaults to \u201cC0\u201d.</p> <code>'C0'</code> <code>columns</code> <code>int</code> <p>The number of columns. Defaults to 2.</p> <code>2</code> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def plot_nn_values_hist(nn_values, net, nn_values_names=\"\", color=\"C0\", columns=2) -&gt; None:\n    \"\"\"\n    Plot the values of a neural network.\n    Can be used to plot the weights, gradients, or activations of a neural network.\n\n    Args:\n        nn_values (dict):\n            A dictionary with the values of the neural network. For example,\n            the weights, gradients, or activations.\n        net (object):\n            A neural network.\n        color (str, optional):\n            The color to use. Defaults to \"C0\".\n        columns (int, optional):\n            The number of columns. Defaults to 2.\n\n    \"\"\"\n    n = len(nn_values)\n    print(f\"n:{n}\")\n    rows = n // columns + int(n % columns &gt; 0)\n    fig, ax = plt.subplots(rows, columns, figsize=(columns * 2.7, rows * 2.5))\n    fig_index = 0\n    for key in nn_values:\n        key_ax = ax[fig_index // columns][fig_index % columns]\n        sns.histplot(data=nn_values[key], bins=50, ax=key_ax, color=color, kde=True, stat=\"density\")\n        hidden_dim_str = r\"(%i $\\to$ %i)\" % (nn_values[key].shape[1], nn_values[key].shape[0]) if len(nn_values[key].shape) &gt; 1 else \"\"\n        key_ax.set_title(f\"{key} {hidden_dim_str}\")\n        # key_ax.set_title(f\"Layer {key} - {net.layers[key].__class__.__name__}\")\n        fig_index += 1\n    fig.suptitle(f\"{nn_values_names} distribution for activation function {net.hparams.act_fn}\", fontsize=14)\n    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n    plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.plot_nn_values_scatter","title":"<code>plot_nn_values_scatter(nn_values, layer_sizes, nn_values_names='', absolute=True, cmap='gray', figsize=(6, 6), return_reshaped=False, show=True, colorbar_orientation='auto')</code>","text":"<p>Plot the values of a neural network including a marker for padding values.</p> <p>Parameters:</p> Name Type Description Default <code>nn_values</code> <code>dict</code> <p>A dictionary with the values of the neural network. For example, the weights, gradients, or activations.</p> required <code>layer_sizes</code> <code>dict</code> <p>A dictionary with layer names as keys and their sizes as entries in NumPy array format.</p> required <code>nn_values_names</code> <code>str</code> <p>The name of the values. Defaults to \u201c\u201d.</p> <code>''</code> <code>absolute</code> <code>bool</code> <p>Whether to use the absolute values. Defaults to True.</p> <code>True</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cgray\u201d.</p> <code>'gray'</code> <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (6, 6).</p> <code>(6, 6)</code> <code>return_reshaped</code> <code>bool</code> <p>Whether to return the reshaped values. Defaults to False.</p> <code>False</code> <code>show</code> <code>bool</code> <p>Whether to show the plot. Defaults to True.</p> <code>True</code> <code>colorbar_orientation</code> <code>str</code> <p>The orientation of the colorbar. Can be \u201cauto\u201d, \u201chorizontal\u201d, \u201cvertical\u201d, or \u201cnone\u201d. \u201cauto\u201d will choose the orientation based on the geometry of the plot. \u201cnone\u201d will not show the colorbar. Defaults to \u201cauto\u201d.</p> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the reshaped values.</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def plot_nn_values_scatter(\n    nn_values,\n    layer_sizes,\n    nn_values_names=\"\",\n    absolute=True,\n    cmap=\"gray\",\n    figsize=(6, 6),\n    return_reshaped=False,\n    show=True,\n    colorbar_orientation=\"auto\",\n) -&gt; dict:\n    \"\"\"\n    Plot the values of a neural network including a marker for padding values.\n\n    Args:\n        nn_values (dict):\n            A dictionary with the values of the neural network. For example,\n            the weights, gradients, or activations.\n        layer_sizes (dict):\n            A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n        nn_values_names (str, optional):\n            The name of the values. Defaults to \"\".\n        absolute (bool, optional):\n            Whether to use the absolute values. Defaults to True.\n        cmap (str, optional):\n            The colormap to use. Defaults to \"gray\".\n        figsize (tuple, optional):\n            The figure size. Defaults to (6, 6).\n        return_reshaped (bool, optional):\n            Whether to return the reshaped values. Defaults to False.\n        show (bool, optional):\n            Whether to show the plot. Defaults to True.\n        colorbar_orientation (str, optional):\n            The orientation of the colorbar. Can be \"auto\", \"horizontal\", \"vertical\", or \"none\".\n            \"auto\" will choose the orientation based on the geometry of the plot.\n            \"none\" will not show the colorbar.\n            Defaults to \"auto\".\n\n    Returns:\n        dict: A dictionary with the reshaped values.\n    \"\"\"\n    if cmap == \"gray\":\n        cmap = \"gray\"\n    elif cmap == \"BlueWhiteRed\":\n        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"blue\", \"white\", \"red\"])\n    elif cmap == \"GreenYellowRed\":\n        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"green\", \"yellow\", \"red\"])\n    else:\n        cmap = \"viridis\"\n\n    res = {}\n    padding_marker = np.nan  # Use NaN as a special marker for padding\n    for layer, values in nn_values.items():\n        if layer not in layer_sizes:\n            print(f\"Layer {layer} size not defined, skipping.\")\n            continue\n\n        layer_shape = layer_sizes[layer]\n        height, width = layer_shape if len(layer_shape) == 2 else (layer_shape[0], 1)  # Support linear layers\n\n        print(f\"{len(values)} values in Layer {layer}. Geometry: ({height}, {width})\")\n\n        total_size = height * width\n        if len(values) &lt; total_size:\n            padding_needed = total_size - len(values)\n            print(f\"{padding_needed} padding values added to Layer {layer}.\")\n            values = np.append(values, [padding_marker] * padding_needed)  # Append padding values\n\n        if absolute:\n            reshaped_values = np.abs(values).reshape((height, width))\n            # Mark padding values distinctly by setting them back to NaN\n            reshaped_values[reshaped_values == np.abs(padding_marker)] = np.nan\n        else:\n            reshaped_values = values.reshape((height, width))\n\n        _, ax = plt.subplots(figsize=figsize)\n        cax = ax.imshow(reshaped_values, cmap=cmap, interpolation=\"nearest\")\n\n        for i in range(height):\n            for j in range(width):\n                if np.isnan(reshaped_values[i, j]):\n                    ax.text(j, i, \"P\", ha=\"center\", va=\"center\", color=\"red\")\n\n        if colorbar_orientation == \"auto\":\n            if height &lt; width:\n                plt.colorbar(cax, orientation=\"horizontal\", label=\"Value\")\n            else:\n                plt.colorbar(cax, orientation=\"vertical\", label=\"Value\")\n\n        if colorbar_orientation in [\"horizontal\", \"vertical\"]:\n            plt.colorbar(cax, orientation=colorbar_orientation, label=\"Value\")\n        plt.title(f\"{nn_values_names} Plot for {layer}\")\n        if show:\n            plt.show()\n\n        # Add reshaped_values to the dictionary res\n        res[layer] = reshaped_values\n\n    if return_reshaped:\n        return res\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.sort_layers","title":"<code>sort_layers(data_dict)</code>","text":"<p>Sorts a dictionary with keys in the format \u201cLayer X\u201d based on the numerical value X.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict</code> <p>A dictionary with keys in the format \u201cLayer X\u201d.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the keys sorted based on the numerical value X.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data_dict = {\n...     \"Layer 1\": [1, 2, 3],\n...     \"Layer 3\": [4, 5, 6],\n...     \"Layer 2\": [7, 8, 9]\n... }\n&gt;&gt;&gt; sort_layers(data_dict)\n{'Layer 1': [1, 2, 3], 'Layer 2': [7, 8, 9], 'Layer 3': [4,\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def sort_layers(data_dict) -&gt; dict:\n    \"\"\"\n    Sorts a dictionary with keys in the format \"Layer X\" based on the numerical value X.\n\n    Args:\n        data_dict (dict): A dictionary with keys in the format \"Layer X\".\n\n    Returns:\n        dict: A dictionary with the keys sorted based on the numerical value X.\n\n    Examples:\n        &gt;&gt;&gt; data_dict = {\n        ...     \"Layer 1\": [1, 2, 3],\n        ...     \"Layer 3\": [4, 5, 6],\n        ...     \"Layer 2\": [7, 8, 9]\n        ... }\n        &gt;&gt;&gt; sort_layers(data_dict)\n        {'Layer 1': [1, 2, 3], 'Layer 2': [7, 8, 9], 'Layer 3': [4,\n\n    \"\"\"\n    # Use a lambda function to extract the number X from \"Layer X\" and sort based on that number\n    sorted_items = sorted(data_dict.items(), key=lambda item: int(item[0].split()[1]))\n    # Create a new dictionary from the sorted items\n    sorted_dict = dict(sorted_items)\n    return sorted_dict\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_activations_distributions","title":"<code>visualize_activations_distributions(activations, net, color='C0', columns=4, bins=50, show=True)</code>","text":"<p>Plots the distribution of activations for each layer     that were determined via the get_activations function.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>dict</code> <p>A dictionary containing activations for each layer.</p> required <code>net</code> <code>Module</code> <p>The neural network model.</p> required <code>color</code> <code>str</code> <p>The color for the plot histogram. Defaults to \u201cC0\u201d.</p> <code>'C0'</code> <code>columns</code> <code>int</code> <p>The number of columns for the subplots. Defaults to 4.</p> <code>4</code> <code>bins</code> <code>int</code> <p>The number of bins for the histogram. Defaults to 50.</p> <code>50</code> <code>show</code> <code>bool</code> <p>Whether to show the plot. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_activations_distributions(activations, net, color=\"C0\", columns=4, bins=50, show=True) -&gt; None:\n    \"\"\"Plots the distribution of activations for each layer\n        that were determined via the get_activations function.\n\n    Args:\n        activations (dict): A dictionary containing activations for each layer.\n        net (nn.Module): The neural network model.\n        color (str): The color for the plot histogram. Defaults to \"C0\".\n        columns (int): The number of columns for the subplots. Defaults to 4.\n        bins (int): The number of bins for the histogram. Defaults to 50.\n        show (bool): Whether to show the plot. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    rows = math.ceil(len(activations) / columns)\n    fig, ax = plt.subplots(rows, columns, figsize=(columns * 2.7, rows * 2.5))\n    fig_index = 0\n    for key in activations:\n        key_ax = ax[fig_index // columns][fig_index % columns]\n        sns.histplot(data=activations[key], bins=bins, ax=key_ax, color=color, kde=True, stat=\"density\")\n        key_ax.set_title(f\"Layer {key} - {net.layers[key].__class__.__name__}\")\n        fig_index += 1\n    fig.suptitle(\"Activation distribution\", fontsize=14)\n    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n    if show:\n        plt.show()\n    plt.close()\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_gradient_distributions","title":"<code>visualize_gradient_distributions(net, fun_control, batch_size, device='cpu', color='C0', xlabel=None, stat='count', use_kde=True, columns=2, normalize=True)</code>","text":"<p>Plot the gradients distributions of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>color</code> <code>str</code> <p>The color to use. Defaults to \u201cC0\u201d.</p> <code>'C0'</code> <code>xlabel</code> <code>str</code> <p>The x label. Defaults to None.</p> <code>None</code> <code>stat</code> <code>str</code> <p>The stat. Defaults to \u201ccount\u201d.</p> <code>'count'</code> <code>use_kde</code> <code>bool</code> <p>Whether to use kde. Defaults to True.</p> <code>True</code> <code>columns</code> <code>int</code> <p>The number of columns. Defaults to 2.</p> <code>2</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_gradient_distributions(\n    net,\n    fun_control,\n    batch_size,\n    device=\"cpu\",\n    color=\"C0\",\n    xlabel=None,\n    stat=\"count\",\n    use_kde=True,\n    columns=2,\n    normalize=True,\n) -&gt; None:\n    \"\"\"\n    Plot the gradients distributions of a neural network.\n\n    Args:\n        net (object):\n            A neural network.\n        fun_control (dict):\n            A dictionary with the function control.\n        batch_size (int, optional):\n            The batch size.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        color (str, optional):\n            The color to use. Defaults to \"C0\".\n        xlabel (str, optional):\n            The x label. Defaults to None.\n        stat (str, optional):\n            The stat. Defaults to \"count\".\n        use_kde (bool, optional):\n            Whether to use kde. Defaults to True.\n        columns (int, optional):\n            The number of columns. Defaults to 2.\n        normalize (bool, optional):\n            Whether to normalize the input data. Defaults to True.\n\n    Returns:\n        None\n\n    \"\"\"\n    grads, _ = get_gradients(net, fun_control, batch_size, device, normalize=normalize)\n    plot_nn_values_hist(grads, net, nn_values_names=\"Gradients\", color=color, columns=columns)\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_gradients","title":"<code>visualize_gradients(net, fun_control, batch_size, absolute=True, cmap='gray', figsize=(6, 6), device='cpu', normalize=True)</code>","text":"<p>Scatter plots the gradients of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>absolute</code> <code>bool</code> <p>Whether to use the absolute values. Defaults to True.</p> <code>True</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cgray\u201d.</p> <code>'gray'</code> <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (6, 6).</p> <code>(6, 6)</code> <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_gradients(net, fun_control, batch_size, absolute=True, cmap=\"gray\", figsize=(6, 6), device=\"cpu\", normalize=True) -&gt; None:\n    \"\"\"\n    Scatter plots the gradients of a neural network.\n\n    Args:\n        net (object):\n            A neural network.\n        fun_control (dict):\n            A dictionary with the function control.\n        batch_size (int, optional):\n            The batch size.\n        absolute (bool, optional):\n            Whether to use the absolute values. Defaults to True.\n        cmap (str, optional):\n            The colormap to use. Defaults to \"gray\".\n        figsize (tuple, optional):\n            The figure size. Defaults to (6, 6).\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        normalize (bool, optional):\n            Whether to normalize the input data. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    grads, layer_sizes = get_gradients(\n        net=net,\n        fun_control=fun_control,\n        batch_size=batch_size,\n        device=device,\n        normalize=normalize,\n    )\n    plot_nn_values_scatter(\n        nn_values=grads,\n        layer_sizes=layer_sizes,\n        nn_values_names=\"Gradients\",\n        absolute=absolute,\n        cmap=cmap,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_mean_activations","title":"<code>visualize_mean_activations(mean_activations, layer_sizes, absolute=True, cmap='gray', figsize=(6, 6))</code>","text":"<p>Scatter plots the mean activations of a neural network for each layer. means_activations is a dictionary with the mean activations of the neural network computed via the get_activations function.</p> <p>Parameters:</p> Name Type Description Default <code>mean_activations</code> <code>dict</code> <p>A dictionary with the mean activations of the neural network.</p> required <code>layer_sizes</code> <code>dict</code> <p>A dictionary with layer names as keys and their sizes as entries in NumPy array format.</p> required <code>absolute</code> <code>bool</code> <p>Whether to use the absolute values. Defaults to True.</p> <code>True</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cgray\u201d.</p> <code>'gray'</code> <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (6, 6).</p> <code>(6, 6)</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.xai import get_activations\n    activations, mean_activations, layer_sizes = get_activations(net, fun_control)\n    visualize_mean_activations(mean_activations, layer_sizes)\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_mean_activations(mean_activations, layer_sizes, absolute=True, cmap=\"gray\", figsize=(6, 6)) -&gt; None:\n    \"\"\"\n    Scatter plots the mean activations of a neural network for each layer.\n    means_activations is a dictionary with the mean activations of the neural network computed via\n    the get_activations function.\n\n    Args:\n        mean_activations (dict):\n            A dictionary with the mean activations of the neural network.\n        layer_sizes (dict):\n            A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n        absolute (bool, optional):\n            Whether to use the absolute values. Defaults to True.\n        cmap (str, optional):\n            The colormap to use. Defaults to \"gray\".\n        figsize (tuple, optional):\n            The figure size. Defaults to (6, 6).\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.xai import get_activations\n            activations, mean_activations, layer_sizes = get_activations(net, fun_control)\n            visualize_mean_activations(mean_activations, layer_sizes)\n\n    \"\"\"\n    plot_nn_values_scatter(\n        nn_values=mean_activations,\n        layer_sizes=layer_sizes,\n        nn_values_names=\"Average Activations\",\n        absolute=absolute,\n        cmap=cmap,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_weights","title":"<code>visualize_weights(net, absolute=True, cmap='gray', figsize=(6, 6))</code>","text":"<p>Scatter plots the weights of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>absolute</code> <code>bool</code> <p>Whether to use the absolute values. Defaults to True.</p> <code>True</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cgray\u201d.</p> <code>'gray'</code> <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (6, 6).</p> <code>(6, 6)</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (\n            get_default_hyperparameters_as_array, get_one_config_from_X)\n    from spotpython.plot.xai import visualize_weights\n    fun_control = fun_control_init(\n        _L_in=10, # 10: diabetes\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\",\n        data_set=Diabetes(),\n        core_model=NNLinearRegressor,\n        hyperdict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    batch_size = 16\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n    visualize_weights(net=model, absolute=True, cmap=\"gray\", figsize=(6, 6))\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_weights(net, absolute=True, cmap=\"gray\", figsize=(6, 6)) -&gt; None:\n    \"\"\"\n    Scatter plots the weights of a neural network.\n\n    Args:\n        net (object):\n            A neural network.\n        absolute (bool, optional):\n            Whether to use the absolute values. Defaults to True.\n        cmap (str, optional):\n            The colormap to use. Defaults to \"gray\".\n        figsize (tuple, optional):\n            The figure size. Defaults to (6, 6).\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (\n                    get_default_hyperparameters_as_array, get_one_config_from_X)\n            from spotpython.plot.xai import visualize_weights\n            fun_control = fun_control_init(\n                _L_in=10, # 10: diabetes\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\",\n                data_set=Diabetes(),\n                core_model=NNLinearRegressor,\n                hyperdict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            config = get_one_config_from_X(X, fun_control)\n            _L_in = fun_control[\"_L_in\"]\n            _L_out = fun_control[\"_L_out\"]\n            _torchmetric = fun_control[\"_torchmetric\"]\n            batch_size = 16\n            model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n            visualize_weights(net=model, absolute=True, cmap=\"gray\", figsize=(6, 6))\n    \"\"\"\n    weights, layer_sizes = get_weights(net)\n    plot_nn_values_scatter(\n        nn_values=weights,\n        layer_sizes=layer_sizes,\n        nn_values_names=\"Weights\",\n        absolute=absolute,\n        cmap=cmap,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_weights_distributions","title":"<code>visualize_weights_distributions(net, color='C0', columns=2)</code>","text":"<p>Plot the weights distributions of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>color</code> <code>str</code> <p>The color to use. Defaults to \u201cC0\u201d.</p> <code>'C0'</code> <code>columns</code> <code>int</code> <p>The number of columns. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_weights_distributions(net, color=\"C0\", columns=2) -&gt; None:\n    \"\"\"\n    Plot the weights distributions of a neural network.\n\n    Args:\n        net (object):\n            A neural network.\n        color (str, optional):\n            The color to use. Defaults to \"C0\".\n        columns (int, optional):\n            The number of columns. Defaults to 2.\n\n    Returns:\n        None\n\n    \"\"\"\n    weights, _ = get_weights(net)\n    plot_nn_values_hist(weights, net, nn_values_names=\"Weights\", color=color, columns=columns)\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.viz_net","title":"<code>viz_net(net, device='cpu', show_attrs=False, show_saved=False, max_attr_chars=50, filename='model_architecture', format='png')</code>","text":"<p>Visualize the architecture of a linear neural network. Produces Graphviz representation of PyTorch autograd graph. If a node represents a backward function, it is gray. Otherwise, the node represents a tensor and is either blue, orange, or green: - Blue: reachable leaf tensors that requires grad (tensors whose .grad fields will be populated during .backward()) - Orange: saved tensors of custom autograd functions as well as those saved by built-in backward nodes - Green: tensor passed in as outputs - Dark green: if any output is a view, we represent its base tensor with a dark green node. If <code>show_attrs</code>=True and <code>show_saved</code>=True it is shown what autograd saves for the backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>The neural network model.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>show_attrs</code> <code>bool</code> <p>whether to display non-tensor attributes of backward nodes (Requires PyTorch version &gt;= 1.9)</p> <code>False</code> <code>show_saved</code> <code>bool</code> <p>whether to display saved tensor nodes that are not by custom autograd functions. Saved tensor nodes for custom functions, if present, are always displayed. (Requires PyTorch version &gt;= 1.9)</p> <code>False</code> <code>max_attr_chars</code> <code>int</code> <p>if show_attrs is True, sets max number of characters to display for any given attribute. Defaults to 50.</p> <code>50</code> <code>filename</code> <code>str</code> <p>The filename. Defaults to \u201cmodel_architecture\u201d.</p> <code>'model_architecture'</code> <code>format</code> <code>str</code> <p>The output format. Defaults to \u201cpng\u201d.</p> <code>'png'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model does not have a linear layer.</p> <code>TypeError</code> <p>If the network structure or parameters are invalid.</p> <code>RuntimeError</code> <p>If an unexpected error occurs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.xai import viz_net\n    from spotpython.utils.init import fun_control_init\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (\n            get_default_hyperparameters_as_array, get_one_config_from_X)\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    _L_in=10\n    _L_out=1\n    _torchmetric=\"mean_squared_error\"\n    fun_control = fun_control_init(\n        _L_in=_L_in,\n        _L_out=_L_out,\n        _torchmetric=_torchmetric,\n        data_set=Diabetes(),\n        core_model=NNLinearRegressor,\n        hyperdict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n    viz_net(net=model, device=\"cpu\", show_attrs=True, show_saved=True, filename=\"model_architecture3\", format=\"png\")\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def viz_net(\n    net,\n    device=\"cpu\",\n    show_attrs=False,\n    show_saved=False,\n    max_attr_chars=50,\n    filename=\"model_architecture\",\n    format=\"png\",\n) -&gt; None:\n    \"\"\"\n    Visualize the architecture of a linear neural network.\n    Produces Graphviz representation of PyTorch autograd graph.\n    If a node represents a backward function, it is gray. Otherwise, the node represents a tensor and is either blue, orange, or green:\n    - Blue: reachable leaf tensors that requires grad (tensors whose .grad fields will be populated during .backward())\n    - Orange: saved tensors of custom autograd functions as well as those saved by built-in backward nodes\n    - Green: tensor passed in as outputs\n    - Dark green: if any output is a view, we represent its base tensor with a dark green node.\n    If `show_attrs`=True and `show_saved`=True it is shown what autograd saves for the backward pass.\n\n    Args:\n        net (nn.Module):\n            The neural network model.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        show_attrs (bool, optional):\n            whether to display non-tensor attributes of backward nodes (Requires PyTorch version &gt;= 1.9)\n        show_saved (bool, optional):\n            whether to display saved tensor nodes that are not by custom autograd functions. Saved tensor nodes for custom functions, if present, are always displayed. (Requires PyTorch version &gt;= 1.9)\n        max_attr_chars (int, optional):\n            if show_attrs is True, sets max number of characters to display for any given attribute. Defaults to 50.\n        filename (str, optional):\n            The filename. Defaults to \"model_architecture\".\n        format (str, optional):\n            The output format. Defaults to \"png\".\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the model does not have a linear layer.\n        TypeError: If the network structure or parameters are invalid.\n        RuntimeError: If an unexpected error occurs.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.xai import viz_net\n            from spotpython.utils.init import fun_control_init\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (\n                    get_default_hyperparameters_as_array, get_one_config_from_X)\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            _L_in=10\n            _L_out=1\n            _torchmetric=\"mean_squared_error\"\n            fun_control = fun_control_init(\n                _L_in=_L_in,\n                _L_out=_L_out,\n                _torchmetric=_torchmetric,\n                data_set=Diabetes(),\n                core_model=NNLinearRegressor,\n                hyperdict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            config = get_one_config_from_X(X, fun_control)\n            model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n            viz_net(net=model, device=\"cpu\", show_attrs=True, show_saved=True, filename=\"model_architecture3\", format=\"png\")\n\n    \"\"\"\n    try:\n        dim = extract_linear_dims(net)\n    except ValueError as ve:\n        error_message = \"The model does not have a linear layer: \" + str(ve)\n        raise ValueError(error_message)\n    except TypeError as te:\n        error_message = \"Invalid network structure or parameters: \" + str(te)\n        raise TypeError(error_message)\n    except Exception as e:\n        # Catch any other unforeseen exceptions and log them for debugging purposes\n        error_message = \"An unexpected error occurred: \" + str(e)\n        raise RuntimeError(error_message)\n\n    # Proceed with the rest of the logic if dimensions were extracted successfully\n    x = torch.randn(1, dim[0]).requires_grad_(True)\n    x = x.to(device)\n    output = net(x)\n    dot = make_dot(\n        output,\n        params=dict(net.named_parameters()),\n        show_attrs=show_attrs,\n        show_saved=show_saved,\n        max_attr_chars=max_attr_chars,\n    )\n    dot.render(filename, format=format)\n</code></pre>"},{"location":"reference/spotpython/plot/xy/","title":"xy","text":""},{"location":"reference/spotpython/plot/xy/#spotpython.plot.xy.plot_y_vs_X","title":"<code>plot_y_vs_X(X, y, nrows=5, ncols=2, figsize=(30, 20), ylabel='y', feature_names=None)</code>","text":"<p>Plots y versus each feature in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D array of input features.</p> required <code>y</code> <code>ndarray</code> <p>1D array of target values.</p> required <code>nrows</code> <code>int</code> <p>Number of rows in the subplot grid. Defaults to 5.</p> <code>5</code> <code>ncols</code> <code>int</code> <p>Number of columns in the subplot grid. Defaults to 2.</p> <code>2</code> <code>figsize</code> <code>tuple</code> <p>Size of the entire figure. Defaults to (30, 20).</p> <code>(30, 20)</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis. Defaults to \u2018y\u2019.</p> <code>'y'</code> <code>feature_names</code> <code>list of str</code> <p>List of feature names. Defaults to None. If None, generates feature names as x0, x1, etc.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from spotpython.plot.xy import plot_y_vs_X\n&gt;&gt;&gt; data = load_diabetes()\n&gt;&gt;&gt; X, y = data.data, data.target\n&gt;&gt;&gt; plot_y_vs_X(X, y, nrows=5, ncols=2, figsize=(20, 15))\n</code></pre> Source code in <code>spotpython/plot/xy.py</code> <pre><code>def plot_y_vs_X(X, y, nrows=5, ncols=2, figsize=(30, 20), ylabel=\"y\", feature_names=None):\n    \"\"\"\n    Plots y versus each feature in X.\n\n    Args:\n        X (ndarray):\n            2D array of input features.\n        y (ndarray):\n            1D array of target values.\n        nrows (int, optional):\n            Number of rows in the subplot grid. Defaults to 5.\n        ncols (int, optional):\n            Number of columns in the subplot grid. Defaults to 2.\n        figsize (tuple, optional):\n            Size of the entire figure. Defaults to (30, 20).\n        ylabel (str, optional):\n            Label for the y-axis. Defaults to 'y'.\n        feature_names (list of str, optional):\n            List of feature names. Defaults to None. If None, generates feature names as x0, x1, etc.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from spotpython.plot.xy import plot_y_vs_X\n        &gt;&gt;&gt; data = load_diabetes()\n        &gt;&gt;&gt; X, y = data.data, data.target\n        &gt;&gt;&gt; plot_y_vs_X(X, y, nrows=5, ncols=2, figsize=(20, 15))\n    \"\"\"\n    if feature_names is None:\n        feature_names = [f\"x{i}\" for i in range(X.shape[1])]\n\n    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n\n    for i, (ax, col) in enumerate(zip(axs.flat, feature_names)):\n        x = X[:, i]\n        pf = np.polyfit(x, y, 1)\n        p = np.poly1d(pf)\n\n        ax.plot(x, y, \"o\")\n        ax.plot(x, p(x), \"r--\")\n\n        ax.set_title(col + \" \" + ylabel)\n        ax.set_xlabel(col)\n        ax.set_ylabel(ylabel)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/spotpython/sklearn/traintest/","title":"traintest","text":""},{"location":"reference/spotpython/sklearn/traintest/#spotpython.sklearn.traintest.evaluate_hold_out","title":"<code>evaluate_hold_out(model, fun_control)</code>","text":"<p>Evaluate a model using hold-out validation. A validation set is created from the training set. The test set is not used in this evaluation.</p> <p>Note: In contrast to <code>evaluate_model()</code>, this function creates a validation set as a subset of the training set. It can be selected by setting <code>fun_control[\"eval\"] = \"evaluate_hold_out\"</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>sklearn model</code> <p>sklearn model.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing evaluation results.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if call to train_test_split() or fit() or predict() fails.</p> Source code in <code>spotpython/sklearn/traintest.py</code> <pre><code>def evaluate_hold_out(model, fun_control) -&gt; np.ndarray:\n    \"\"\"Evaluate a model using hold-out validation.\n    A validation set is created from the training set.\n    The test set is not used in this evaluation.\n\n    Note:\n    In contrast to `evaluate_model()`, this function creates a validation set as\n    a subset of the training set.\n    It can be selected by setting `fun_control[\"eval\"] = \"evaluate_hold_out\"`.\n\n    Args:\n        model (sklearn model):\n            sklearn model.\n        fun_control (dict):\n            dictionary containing control parameters for the function.\n\n    Returns:\n        (np.ndarray): array containing evaluation results.\n\n    Raises:\n        Exception: if call to train_test_split() or fit() or predict() fails.\n    \"\"\"\n    train_df = fun_control[\"train\"]\n    target_column = fun_control[\"target_column\"]\n    try:\n        X_train, X_val, y_train, y_val = train_test_split(\n            train_df.drop(target_column, axis=1),\n            train_df[target_column],\n            random_state=42,\n            test_size=fun_control[\"test_size\"],\n            # stratify=train_df[target_column],\n        )\n    except Exception as err:\n        print(f\"Error in evaluate_hold_out(). Call to train_test_split() failed. {err=}, {type(err)=}\")\n    try:\n        if fun_control[\"scaler\"] is not None:\n            scaler = fun_control[\"scaler\"]()\n            X_train = scaler.fit_transform(X_train)\n            X_train = pd.DataFrame(X_train, columns=train_df.drop(target_column, axis=1).columns)  # Maintain column names\n        model.fit(X_train, y_train)\n    except Exception as err:\n        print(f\"Error in evaluate_hold_out(). Call to fit() failed. {err=}, {type(err)=}\")\n    try:\n        if fun_control[\"scaler\"] is not None:\n            X_val = scaler.transform(X_val)\n            X_val = pd.DataFrame(X_val, columns=train_df.drop(target_column, axis=1).columns)  # Maintain column names\n        y_val = np.array(y_val)\n        if fun_control[\"predict_proba\"] or fun_control[\"task\"] == \"classification\":\n            df_preds = model.predict_proba(X_val)\n        else:\n            df_preds = model.predict(X_val)\n        df_eval = fun_control[\"metric_sklearn\"](y_val, df_preds, **fun_control[\"metric_params\"])\n    except Exception as err:\n        print(f\"Error in evaluate_hold_out(). Call to predict() failed. {err=}, {type(err)=}\")\n        df_eval = np.nan\n    return df_eval, df_preds\n</code></pre>"},{"location":"reference/spotpython/sklearn/traintest/#spotpython.sklearn.traintest.evaluate_model","title":"<code>evaluate_model(model, fun_control)</code>","text":"<p>Evaluate a model using the test set. First, the model is trained on the training set. If a scaler is provided, the data is transformed using the scaler and <code>fit_transform(X_train)</code>. Then, the model is evaluated using the test set from <code>fun_control</code>, the scaler with <code>transform(X_test)</code>, the model.predict() method and the <code>metric_params</code> specified in <code>fun_control</code>.</p> <p>Note: In contrast to <code>evaluate_hold_out()</code>, this function uses the test set. It can be selected by setting <code>fun_control[\"eval\"] = \"eval_test\"</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>sklearn model</code> <p>sklearn model.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing evaluation results.</p> Source code in <code>spotpython/sklearn/traintest.py</code> <pre><code>def evaluate_model(model, fun_control) -&gt; np.ndarray:\n    \"\"\"Evaluate a model using the test set.\n    First, the model is trained on the training set. If a scaler\n    is provided, the data is transformed using the scaler and `fit_transform(X_train)`.\n    Then, the model is evaluated using the test set from `fun_control`,\n    the scaler with `transform(X_test)`,\n    the model.predict() method and the\n    `metric_params` specified in `fun_control`.\n\n    Note:\n    In contrast to `evaluate_hold_out()`, this function uses the test set.\n    It can be selected by setting `fun_control[\"eval\"] = \"eval_test\"`.\n\n    Args:\n        model (sklearn model):\n            sklearn model.\n        fun_control (dict):\n            dictionary containing control parameters for the function.\n\n    Returns:\n        (np.ndarray): array containing evaluation results.\n    \"\"\"\n    try:\n        X_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n        X_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n        if fun_control[\"scaler\"] is not None:\n            X_train = fun_control[\"scaler\"]().fit_transform(X_train)\n            X_test = fun_control[\"scaler\"]().transform(X_test)\n        model.fit(X_train, y_train)\n        if fun_control[\"predict_proba\"]:\n            df_preds = model.predict_proba(X_test)\n        else:\n            df_preds = model.predict(X_test)\n        df_eval = fun_control[\"metric_sklearn\"](y_test, df_preds, **fun_control[\"metric_params\"])\n    except Exception as err:\n        print(f\"Error in fun_sklearn(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n        df_eval = np.nan\n        df_eval = np.nan\n    return df_eval, df_preds\n</code></pre>"},{"location":"reference/spotpython/sklearn/traintest/#spotpython.sklearn.traintest.evaluate_model_oob","title":"<code>evaluate_model_oob(model, fun_control)</code>","text":"<p>Out-of-bag evaluation (Only for RandomForestClassifier). If fun_control[\u201ceval\u201d] == \u201ceval_oob_score\u201d.</p> Source code in <code>spotpython/sklearn/traintest.py</code> <pre><code>def evaluate_model_oob(model, fun_control):\n    \"\"\"Out-of-bag evaluation (Only for RandomForestClassifier).\n    If fun_control[\"eval\"] == \"eval_oob_score\".\n    \"\"\"\n    try:\n        X, y = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n        model.fit(X, y)\n        df_preds = model.oob_decision_function_\n        df_eval = fun_control[\"metric_sklearn\"](y, df_preds, **fun_control[\"metric_params\"])\n    except Exception as err:\n        print(f\"Error in fun_sklearn(). Call to evaluate_model_oob failed. {err=}, {type(err)=}\")\n        df_eval = np.nan\n        df_eval = np.nan\n    return df_eval, df_preds\n</code></pre>"},{"location":"reference/spotpython/spot/spot/","title":"spot","text":""},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot","title":"<code>Spot</code>","text":"<p>Spot base class to handle the following tasks in a uniform manner:</p> <ul> <li>Getting and setting parameters. This is done via the <code>Spot</code> initialization.</li> <li>Running surrogate based hyperparameter optimization. After the class is initialized, hyperparameter tuning runs can be performed via the <code>run</code> method.</li> <li>Displaying information. The <code>plot</code> method can be used for visualizing results. The <code>print</code> methods summarizes information about the tuning run.</li> </ul> <p>The <code>Spot</code> class is built in a modular manner. It combines the following components:</p> <pre><code>1. Fun (objective function)\n2. Design (experimental design)\n3. Optimizer to be used on the surrogate model\n4. Surrogate (model)\n</code></pre> <p>For each of the components different implementations can be selected and combined. Internal components are selected as default. These can be replaced by components from other packages, e.g., scikit-learn or scikit-optimize.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>objective function</p> <code>None</code> <code>fun_control</code> <code>Dict[str, Union[int, float]]</code> <p>objective function information stored as a dictionary. Default value is <code>fun_control_init()</code>.</p> <code>fun_control_init()</code> <code>design</code> <code>object</code> <p>experimental design. If <code>None</code>, spotpython\u2019s <code>SpaceFilling</code> is used. Default value is <code>None</code>.</p> <code>None</code> <code>design_control</code> <code>Dict[str, Union[int, float]]</code> <p>experimental design information stored as a dictionary. Default value is <code>design_control_init()</code>.</p> <code>design_control_init()</code> <code>optimizer</code> <code>object</code> <p>optimizer on the surrogate. If <code>None</code>, <code>scipy.optimize</code>\u2019s <code>differential_evolution</code> is used. Default value is <code>None</code>.</p> <code>None</code> <code>optimizer_control</code> <code>Dict[str, Union[int, float]]</code> <p>information about the optimizer stored as a dictionary. Default value is <code>optimizer_control_init()</code>.</p> <code>optimizer_control_init()</code> <code>surrogate</code> <code>object</code> <p>surrogate model. If <code>None</code>, spotpython\u2019s <code>kriging</code> is used. Default value is <code>None</code>.</p> <code>None</code> <code>surrogate_control</code> <code>Dict[str, Union[int, float]]</code> <p>surrogate model information stored as a dictionary. Default value is <code>surrogate_control_init()</code>.</p> <code>surrogate_control_init()</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Note <p>Description in the source code refers to [bart21i]: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches. In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67\u2013114.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from math import inf\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init,\n        design_control_init,\n        surrogate_control_init,\n        optimizer_control_init)\n    def objective_function(X, fun_control=None):\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        if X.shape[1] != 2:\n            raise Exception\n        x0 = X[:, 0]\n        x1 = X[:, 1]\n        y = x0**2 + 10*x1**2\n        return y\n    fun_control = fun_control_init(\n                lower = np.array([0, 0]),\n                upper = np.array([10, 10]),\n                fun_evals=8,\n                fun_repeats=1,\n                max_time=inf,\n                noise=False,\n                tolerance_x=0,\n                ocba_delta=0,\n                var_type=[\"num\", \"num\"],\n                infill_criterion=\"ei\",\n                n_points=1,\n                seed=123,\n                log_level=20,\n                show_models=False,\n                show_progress=True)\n    design_control = design_control_init(\n                init_size=5,\n                repeats=1)\n    surrogate_control = surrogate_control_init(\n                model_optimizer=differential_evolution,\n                model_fun_evals=10000,\n                min_theta=-3,\n                max_theta=3,\n                n_theta=2,\n                theta_init_zero=True,\n                n_p=1,\n                optim_p=False,\n                var_type=[\"num\", \"num\"],\n                metric_factorial=\"canberra\",\n                seed=124)\n    optimizer_control = optimizer_control_init(\n                max_iter=1000,\n                seed=125)\n    spot = spot.Spot(fun=objective_function,\n                fun_control=fun_control,\n                design_control=design_control,\n                surrogate_control=surrogate_control,\n                optimizer_control=optimizer_control)\n    spot.run()\n    spot.plot_progress()\n    spot.plot_contour(i=0, j=1)\n    spot.plot_importance()\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>class Spot:\n    \"\"\"\n    Spot base class to handle the following tasks in a uniform manner:\n\n    * Getting and setting parameters. This is done via the `Spot` initialization.\n    * Running surrogate based hyperparameter optimization. After the class is initialized, hyperparameter tuning\n    runs can be performed via the `run` method.\n    * Displaying information. The `plot` method can be used for visualizing results. The `print` methods summarizes\n    information about the tuning run.\n\n    The `Spot` class is built in a modular manner. It combines the following components:\n\n        1. Fun (objective function)\n        2. Design (experimental design)\n        3. Optimizer to be used on the surrogate model\n        4. Surrogate (model)\n\n    For each of the components different implementations can be selected and combined.\n    Internal components are selected as default.\n    These can be replaced by components from other packages, e.g., scikit-learn or scikit-optimize.\n\n    Args:\n        fun (Callable):\n            objective function\n        fun_control (Dict[str, Union[int, float]]):\n            objective function information stored as a dictionary.\n            Default value is `fun_control_init()`.\n        design (object):\n            experimental design. If `None`, spotpython's `SpaceFilling` is used.\n            Default value is `None`.\n        design_control (Dict[str, Union[int, float]]):\n            experimental design information stored as a dictionary.\n            Default value is `design_control_init()`.\n        optimizer (object):\n            optimizer on the surrogate. If `None`, `scipy.optimize`'s `differential_evolution` is used.\n            Default value is `None`.\n        optimizer_control (Dict[str, Union[int, float]]):\n            information about the optimizer stored as a dictionary.\n            Default value is `optimizer_control_init()`.\n        surrogate (object):\n            surrogate model. If `None`, spotpython's `kriging` is used. Default value is `None`.\n        surrogate_control (Dict[str, Union[int, float]]):\n            surrogate model information stored as a dictionary.\n            Default value is `surrogate_control_init()`.\n\n    Returns:\n        (NoneType): None\n\n    Note:\n        Description in the source code refers to [bart21i]:\n        Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches.\n        In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide,\n        E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67\u2013114.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from math import inf\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init,\n                design_control_init,\n                surrogate_control_init,\n                optimizer_control_init)\n            def objective_function(X, fun_control=None):\n                if not isinstance(X, np.ndarray):\n                    X = np.array(X)\n                if X.shape[1] != 2:\n                    raise Exception\n                x0 = X[:, 0]\n                x1 = X[:, 1]\n                y = x0**2 + 10*x1**2\n                return y\n            fun_control = fun_control_init(\n                        lower = np.array([0, 0]),\n                        upper = np.array([10, 10]),\n                        fun_evals=8,\n                        fun_repeats=1,\n                        max_time=inf,\n                        noise=False,\n                        tolerance_x=0,\n                        ocba_delta=0,\n                        var_type=[\"num\", \"num\"],\n                        infill_criterion=\"ei\",\n                        n_points=1,\n                        seed=123,\n                        log_level=20,\n                        show_models=False,\n                        show_progress=True)\n            design_control = design_control_init(\n                        init_size=5,\n                        repeats=1)\n            surrogate_control = surrogate_control_init(\n                        model_optimizer=differential_evolution,\n                        model_fun_evals=10000,\n                        min_theta=-3,\n                        max_theta=3,\n                        n_theta=2,\n                        theta_init_zero=True,\n                        n_p=1,\n                        optim_p=False,\n                        var_type=[\"num\", \"num\"],\n                        metric_factorial=\"canberra\",\n                        seed=124)\n            optimizer_control = optimizer_control_init(\n                        max_iter=1000,\n                        seed=125)\n            spot = spot.Spot(fun=objective_function,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control,\n                        optimizer_control=optimizer_control)\n            spot.run()\n            spot.plot_progress()\n            spot.plot_contour(i=0, j=1)\n            spot.plot_importance()\n    \"\"\"\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __init__(\n        self,\n        design: object = None,\n        design_control: dict = design_control_init(),\n        fun: Callable = None,\n        fun_control: dict = fun_control_init(),\n        optimizer: object = None,\n        optimizer_control: dict = optimizer_control_init(),\n        surrogate: object = None,\n        surrogate_control: dict = surrogate_control_init(),\n    ):\n        self.fun_control = fun_control\n        self.design_control = design_control\n        self.optimizer_control = optimizer_control\n        self.surrogate_control = surrogate_control\n\n        # small value:\n        self.eps = sqrt(spacing(1))\n\n        self.fun = fun\n        if self.fun is None:\n            raise Exception(\"No objective function specified.\")\n        if not callable(self.fun):\n            raise Exception(\"Objective function is not callable.\")\n\n        # 1. fun_control updates:\n        # -----------------------\n        # Random number generator:\n        self.rng = default_rng(self.fun_control[\"seed\"])\n\n        # 2. lower attribute updates:\n        # -----------------------\n        # if lower is in the fun_control dictionary, use the value of the key \"lower\" as the lower bound\n        if get_control_key_value(control_dict=self.fun_control, key=\"lower\") is not None:\n            self.lower = get_control_key_value(control_dict=self.fun_control, key=\"lower\")\n        # Number of dimensions is based on lower\n        self.k = self.lower.size\n\n        # 3. upper attribute updates:\n        # -----------------------\n        # if upper is in fun_control dictionary, use the value of the key \"upper\" as the upper bound\n        if get_control_key_value(control_dict=self.fun_control, key=\"upper\") is not None:\n            self.upper = get_control_key_value(control_dict=self.fun_control, key=\"upper\")\n\n        # 4. var_type attribute updates:\n        # -----------------------\n        self.var_type = self.fun_control[\"var_type\"]\n        # Force numeric type as default in every dim:\n        # assume all variable types are \"num\" if \"num\" is\n        # specified less than k times\n        if len(self.var_type) &lt; self.k:\n            self.var_type = self.var_type * self.k\n            logger.warning(\"All variable types forced to 'num'.\")\n\n        # 5. var_name attribute updates:\n        # -----------------------\n        self.var_name = self.fun_control[\"var_name\"]\n        # use x0, x1, ... as default variable names:\n        if self.var_name is None:\n            self.var_name = [\"x\" + str(i) for i in range(len(self.lower))]\n\n        # Reduce dim based on lower == upper logic:\n        # modifies lower, upper, var_type, and var_name\n        self.to_red_dim()\n\n        # 6. Additional self attributes updates:\n        # -----------------------\n        self.fun_evals = self.fun_control[\"fun_evals\"]\n        self.fun_repeats = self.fun_control[\"fun_repeats\"]\n        self.max_time = self.fun_control[\"max_time\"]\n        self.noise = self.fun_control[\"noise\"]\n        self.tolerance_x = self.fun_control[\"tolerance_x\"]\n        self.ocba_delta = self.fun_control[\"ocba_delta\"]\n        self.log_level = self.fun_control[\"log_level\"]\n        self.show_models = self.fun_control[\"show_models\"]\n        self.show_progress = self.fun_control[\"show_progress\"]\n        self.infill_criterion = self.fun_control[\"infill_criterion\"]\n        self.n_points = self.fun_control[\"n_points\"]\n        self.max_surrogate_points = self.fun_control[\"max_surrogate_points\"]\n        self.progress_file = self.fun_control[\"progress_file\"]\n        self.tkagg = self.fun_control[\"tkagg\"]\n        if self.tkagg:\n            matplotlib.use(\"TkAgg\")\n\n        # Tensorboard:\n        self.init_spot_writer()\n\n        # Bounds are internal, because they are functions of self.lower and self.upper\n        # and used by the optimizer:\n        de_bounds = []\n        for j in range(self.lower.size):\n            de_bounds.append([self.lower[j], self.upper[j]])\n        self.de_bounds = de_bounds\n\n        # Design related information:\n        self.design = design\n        if design is None:\n            self.design = SpaceFilling(k=self.lower.size, seed=self.fun_control[\"seed\"])\n        # self.design_control = {\"init_size\": 10, \"repeats\": 1}\n        # self.design_control.update(design_control)\n\n        # Optimizer related information:\n        self.optimizer = optimizer\n        # self.optimizer_control = {\"max_iter\": 1000, \"seed\": 125}\n        # self.optimizer_control.update(optimizer_control)\n        if self.optimizer is None:\n            self.optimizer = optimize.differential_evolution\n\n        # Surrogate related information:\n        self.surrogate = surrogate\n        self.surrogate_control.update({\"var_type\": self.var_type})\n        # Surrogate control updates:\n        # The default value for `noise` from the surrogate_control dictionary\n        # based on surrogate_control.init() is None. This value is updated\n        # to the value of the key \"noise\" from the fun_control dictionary.\n        # If the value is set (i.e., not None), it is not updated.\n        if self.surrogate_control[\"noise\"] is None:\n            self.surrogate_control.update({\"noise\": self.fun_control.noise})\n        if self.surrogate_control[\"model_fun_evals\"] is None:\n            self.surrogate_control.update({\"model_fun_evals\": self.optimizer_control[\"max_iter\"]})\n        # self.optimizer is not None here. If 1) the key \"model_optimizer\"\n        # is still None or 2) a user specified optimizer is provided, update the value of\n        # the key \"model_optimizer\" to the value of self.optimizer.\n        if self.surrogate_control[\"model_optimizer\"] is None or optimizer is not None:\n            self.surrogate_control.update({\"model_optimizer\": self.optimizer})\n\n        # if self.surrogate_control[\"n_theta\"] is a string and == isotropic, use 1 theta value:\n        if isinstance(self.surrogate_control[\"n_theta\"], str):\n            if self.surrogate_control[\"n_theta\"] == \"anisotropic\":\n                surrogate_control.update({\"n_theta\": self.k})\n            else:\n                # case \"isotropic\":\n                surrogate_control.update({\"n_theta\": 1})\n        if isinstance(self.surrogate_control[\"n_theta\"], int):\n            if self.surrogate_control[\"n_theta\"] &gt; 1:\n                surrogate_control.update({\"n_theta\": self.k})\n\n        # If no surrogate model is specified, use the internal\n        # spotpython kriging surrogate:\n        if self.surrogate is None:\n            # Call kriging with surrogate_control parameters:\n            self.surrogate = Kriging(\n                name=\"kriging\",\n                noise=self.surrogate_control[\"noise\"],\n                model_optimizer=self.surrogate_control[\"model_optimizer\"],\n                model_fun_evals=self.surrogate_control[\"model_fun_evals\"],\n                seed=self.surrogate_control[\"seed\"],\n                log_level=self.log_level,\n                min_theta=self.surrogate_control[\"min_theta\"],\n                max_theta=self.surrogate_control[\"max_theta\"],\n                metric_factorial=self.surrogate_control[\"metric_factorial\"],\n                n_theta=self.surrogate_control[\"n_theta\"],\n                theta_init_zero=self.surrogate_control[\"theta_init_zero\"],\n                p_val=self.surrogate_control[\"p_val\"],\n                n_p=self.surrogate_control[\"n_p\"],\n                optim_p=self.surrogate_control[\"optim_p\"],\n                min_Lambda=self.surrogate_control[\"min_Lambda\"],\n                max_Lambda=self.surrogate_control[\"max_Lambda\"],\n                var_type=self.surrogate_control[\"var_type\"],\n                spot_writer=self.spot_writer,\n                counter=self.design_control[\"init_size\"] * self.design_control[\"repeats\"] - 1,\n            )\n\n        # Internal attributes:\n        self.X = None\n        self.y = None\n        # Logging information:\n        self.counter = 0\n        self.min_y = None\n        self.min_X = None\n        self.min_mean_X = None\n        self.min_mean_y = None\n        self.mean_X = None\n        self.mean_y = None\n        self.var_y = None\n\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n        logger.debug(\"In Spot() init(): fun_control: %s\", self.fun_control)\n        logger.debug(\"In Spot() init(): design_control: %s\", self.design_control)\n        logger.debug(\"In Spot() init(): optimizer_control: %s\", self.optimizer_control)\n        logger.debug(\"In Spot() init(): surrogate_control: %s\", self.surrogate_control)\n        logger.debug(\"In Spot() init(): self.get_spot_attributes_as_df(): %s\", self.get_spot_attributes_as_df())\n\n    def get_spot_attributes_as_df(self) -&gt; pd.DataFrame:\n        \"\"\"Get all attributes of the spot object as a pandas dataframe.\n\n        Returns:\n            (pandas.DataFrame): dataframe with all attributes of the spot object.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from math import inf\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 7\n                # number of points\n                n = 10\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1]),\n                    upper = np.array([1])\n                    fun_evals=n)\n                design_control=design_control_init(init_size=ni)\n                spot_1 = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                spot_1.run()\n                df = spot_1.get_spot_attributes_as_df()\n                df\n                    Attribute Name                                    Attribute Value\n                0                   X  [[-0.3378148180708981], [0.698908280342222], [...\n                1           all_lower                                               [-1]\n                2           all_upper                                                [1]\n                3        all_var_name                                               [x0]\n                4        all_var_type                                              [num]\n                5             counter                                                 10\n                6           de_bounds                                          [[-1, 1]]\n                7              design  &lt;spotpython.design.spacefilling.SpaceFilling o...\n                8      design_control                     {'init_size': 7, 'repeats': 1}\n                9                 eps                                                0.0\n                10        fun_control                         {'sigma': 0, 'seed': None}\n                11          fun_evals                                                 10\n                12        fun_repeats                                                  1\n                13              ident                                            [False]\n                14   infill_criterion                                                  y\n                15                  k                                                  1\n                16          log_level                                                 50\n                17              lower                                               [-1]\n                18           max_time                                                inf\n                19             mean_X                                               None\n                20             mean_y                                               None\n                21              min_X                           [1.5392206722432657e-05]\n                22         min_mean_X                                               None\n                23         min_mean_y                                               None\n                24              min_y                                                0.0\n                25           n_points                                                  1\n                26              noise                                              False\n                27         ocba_delta                                                  0\n                28  optimizer_control                    {'max_iter': 1000, 'seed': 125}\n                29            red_dim                                              False\n                30                rng                                   Generator(PCG64)\n                31               seed                                                123\n                32        show_models                                              False\n                33      show_progress                                               True\n                34        spot_writer                                               None\n                35          surrogate  &lt;spotpython.build.kriging.Kriging object at 0x...\n                36  surrogate_control  {'noise': False, 'model_optimizer': &lt;function ...\n                37        tolerance_x                                                  0\n                38              upper                                                [1]\n                39           var_name                                               [x0]\n                40           var_type                                              [num]\n                41              var_y                                               None\n                42                  y  [0.11411885130827397, 0.48847278433092195, 0.0...\n\n        \"\"\"\n\n        attributes = [attr for attr in dir(self) if not callable(getattr(self, attr)) and not attr.startswith(\"__\")]\n        values = [getattr(self, attr) for attr in attributes]\n        df = pd.DataFrame({\"Attribute Name\": attributes, \"Attribute Value\": values})\n        return df\n\n    def to_red_dim(self) -&gt; None:\n        \"\"\"\n        Reduce dimension if lower == upper.\n        This is done by removing the corresponding entries from\n        lower, upper, var_type, and var_name.\n        k is modified accordingly.\n\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (NoneType): None\n\n        Attributes:\n            self.lower (numpy.ndarray): lower bound\n            self.upper (numpy.ndarray): upper bound\n            self.var_type (List[str]): list of variable types\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 3\n                # number of points\n                n = 10\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]),\n                    fun_evals = n)\n                design_control=design_control_init(init_size=ni)\n                spot_1 = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                spot_1.run()\n                assert spot_1.lower.size == 2\n                assert spot_1.upper.size == 2\n                assert len(spot_1.var_type) == 2\n                assert spot_1.red_dim == False\n                spot_1.lower = np.array([-1, -1])\n                spot_1.upper = np.array([-1, -1])\n                spot_1.to_red_dim()\n                assert spot_1.lower.size == 0\n                assert spot_1.upper.size == 0\n                assert len(spot_1.var_type) == 0\n                assert spot_1.red_dim == True\n\n        \"\"\"\n        # Backup of the original values:\n        self.all_lower = self.lower\n        self.all_upper = self.upper\n        # Select only lower != upper:\n        self.ident = (self.upper - self.lower) == 0\n        # Determine if dimension is reduced:\n        self.red_dim = self.ident.any()\n        # Modifications:\n        # Modify lower and upper:\n        self.lower = self.lower[~self.ident]\n        self.upper = self.upper[~self.ident]\n        # Modify k (dim):\n        self.k = self.lower.size\n        # Modify var_type:\n        if self.var_type is not None:\n            self.all_var_type = self.var_type\n            self.var_type = [x for x, y in zip(self.all_var_type, self.ident) if not y]\n        # Modify var_name:\n        if self.var_name is not None:\n            self.all_var_name = self.var_name\n            self.var_name = [x for x, y in zip(self.all_var_name, self.ident) if not y]\n\n    def to_all_dim(self, X0) -&gt; np.array:\n        n = X0.shape[0]\n        k = len(self.ident)\n        X = np.zeros((n, k))\n        j = 0\n        for i in range(k):\n            if self.ident[i]:\n                X[:, i] = self.all_lower[i]\n                j += 1\n            else:\n                X[:, i] = X0[:, i - j]\n        return X\n\n    def to_all_dim_if_needed(self, X) -&gt; np.array:\n        if self.red_dim:\n            return self.to_all_dim(X)\n        else:\n            return X\n\n    def get_new_X0(self) -&gt; np.array:\n        \"\"\"\n        Get new design points.\n        Calls `suggest_new_X()` and repairs the new design points, e.g.,\n        by `repair_non_numeric()` and `selectNew()`.\n\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (numpy.ndarray): new design points\n\n        Notes:\n            * self.design (object): an experimental design is used to generate new design points\n            if no new design points are found, a new experimental design is generated.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                               from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init\n                # number of initial points:\n                ni = 3\n                X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                            n_points=10,\n                            ocba_delta=0,\n                            lower = np.array([-1, -1]),\n                            upper = np.array([1, 1])\n                )\n                design_control=design_control_init(init_size=ni)\n                S = spot.Spot(fun=fun,\n                                fun_control=fun_control\n                                design_control=design_control,\n                )\n                S.initialize_design(X_start=X_start)\n                S.update_stats()\n                S.fit_surrogate()\n                X_ocba = None\n                X0 = S.get_new_X0()\n                assert X0.shape[0] == S.n_points\n                assert X0.shape[1] == S.lower.size\n                # assert new points are in the interval [lower, upper]\n                assert np.all(X0 &gt;= S.lower)\n                assert np.all(X0 &lt;= S.upper)\n                # print using 20 digits precision\n                np.set_printoptions(precision=20)\n                print(f\"X0: {X0}\")\n\n        \"\"\"\n        # Try to generate self.fun_repeats new X0 points:\n        X0 = self.suggest_new_X()\n        X0 = repair_non_numeric(X0, self.var_type)\n        # (S-16) Duplicate Handling:\n        # Condition: select only X= that have min distance\n        # to existing solutions\n        X0, X0_ind = selectNew(A=X0, X=self.X, tolerance=self.tolerance_x)\n        if X0.shape[0] &gt; 0:\n            # 1. There are X0 that fullfil the condition.\n            # Note: The number of new X0 can be smaller than self.n_points!\n            logger.debug(\"XO values are new: %s %s\", X0_ind, X0)\n            return repeat(X0, self.fun_repeats, axis=0)\n            return X0\n        # 2. No X0 found. Then generate self.n_points new solutions:\n        else:\n            self.design = SpaceFilling(k=self.k, seed=self.fun_control[\"seed\"] + self.counter)\n            X0 = self.generate_design(size=self.n_points, repeats=self.design_control[\"repeats\"], lower=self.lower, upper=self.upper)\n            X0 = repair_non_numeric(X0, self.var_type)\n            logger.warning(\"No new XO found on surrogate. Generate new solution %s\", X0)\n            return X0\n\n    def de_serialize_dicts(self) -&gt; tuple:\n        \"\"\"\n        Deserialize the spot object and return the dictionaries.\n\n        Args:\n            self (object):\n                Spot object\n\n        Returns:\n            (tuple):\n                tuple containing dictionaries of spot object:\n                fun_control (dict): function control dictionary,\n                design_control (dict): design control dictionary,\n                optimizer_control (dict): optimizer control dictionary,\n                spot_tuner_control (dict): spot tuner control dictionary, and\n                surrogate_control (dict): surrogate control dictionary\n        \"\"\"\n        spot_tuner = copy.deepcopy(self)\n        spot_tuner_control = vars(spot_tuner)\n\n        fun_control = copy.deepcopy(spot_tuner_control[\"fun_control\"])\n        design_control = copy.deepcopy(spot_tuner_control[\"design_control\"])\n        optimizer_control = copy.deepcopy(spot_tuner_control[\"optimizer_control\"])\n        surrogate_control = copy.deepcopy(spot_tuner_control[\"surrogate_control\"])\n\n        # remove keys from the dictionaries:\n        spot_tuner_control.pop(\"fun_control\", None)\n        spot_tuner_control.pop(\"design_control\", None)\n        spot_tuner_control.pop(\"optimizer_control\", None)\n        spot_tuner_control.pop(\"surrogate_control\", None)\n        spot_tuner_control.pop(\"spot_writer\", None)\n        spot_tuner_control.pop(\"design\", None)\n        spot_tuner_control.pop(\"fun\", None)\n        spot_tuner_control.pop(\"optimizer\", None)\n        spot_tuner_control.pop(\"rng\", None)\n        spot_tuner_control.pop(\"surrogate\", None)\n\n        fun_control.pop(\"core_model\", None)\n        fun_control.pop(\"metric_river\", None)\n        fun_control.pop(\"metric_sklearn\", None)\n        fun_control.pop(\"metric_torch\", None)\n        fun_control.pop(\"prep_model\", None)\n        fun_control.pop(\"spot_writer\", None)\n        fun_control.pop(\"test\", None)\n        fun_control.pop(\"train\", None)\n\n        surrogate_control.pop(\"model_optimizer\", None)\n        surrogate_control.pop(\"surrogate\", None)\n\n        return (fun_control, design_control, optimizer_control, spot_tuner_control, surrogate_control)\n\n    def write_db_dict(self) -&gt; None:\n        \"\"\"Writes a dictionary with the experiment parameters to the json file spotpython_db.json.\n\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (NoneType): None\n\n        \"\"\"\n        # get the time in seconds from 1.1.1970 and convert the time to a string\n        t_str = str(time.time())\n        ident = str(self.fun_control[\"PREFIX\"]) + \"_\" + t_str\n\n        (\n            fun_control,\n            design_control,\n            optimizer_control,\n            spot_tuner_control,\n            surrogate_control,\n        ) = self.de_serialize_dicts()\n        print(\"\\n**\")\n        print(\"The following dictionaries are written to the json file spotpython_db.json:\")\n        print(\"fun_control:\")\n        pprint.pprint(fun_control)\n\n        # Iterate over a list of the keys to avoid modifying the dictionary during iteration\n        for key in list(fun_control.keys()):\n            if not isinstance(fun_control[key], (int, float, str, list, dict)):\n                # remove the key from the dictionary\n                print(f\"Removing non-serializable key: {key}\")\n                fun_control.pop(key)\n\n        print(\"fun_control after removing non-serializabel keys:\")\n        pprint.pprint(fun_control)\n        pprint.pprint(fun_control)\n        print(\"design_control:\")\n        pprint.pprint(design_control)\n        print(\"optimizer_control:\")\n        pprint.pprint(optimizer_control)\n        print(\"spot_tuner_control:\")\n        pprint.pprint(spot_tuner_control)\n        print(\"surrogate_control:\")\n        pprint.pprint(surrogate_control)\n        #\n        # Generate a description of the results:\n        # if spot_tuner_control['min_y'] exists:\n        try:\n            result = f\"\"\"\n                      Results for {ident}: Finally, the best value is {spot_tuner_control['min_y']}\n                      at {spot_tuner_control['min_X']}.\"\"\"\n            #\n            db_dict = {\n                \"data\": {\n                    \"id\": str(ident),\n                    \"result\": result,\n                    \"fun_control\": fun_control,\n                    \"design_control\": design_control,\n                    \"surrogate_control\": surrogate_control,\n                    \"optimizer_control\": optimizer_control,\n                    \"spot_tuner_control\": spot_tuner_control,\n                }\n            }\n            # Check if the directory \"db_dicts\" exists.\n            if not os.path.exists(\"db_dicts\"):\n                try:\n                    os.makedirs(\"db_dicts\")\n                except OSError as e:\n                    raise Exception(f\"Error creating directory: {e}\")\n\n            if os.path.exists(\"db_dicts\"):\n                try:\n                    # Open the file in append mode to add each new dict as a new line\n                    with open(\"db_dicts/\" + self.fun_control[\"db_dict_name\"], \"a\") as f:\n                        # Using json.dumps to convert the dict to a JSON formatted string\n                        # We then write this string to the file followed by a newline character\n                        # This ensures that each dict is on its own line, conforming to the JSON Lines format\n                        f.write(json.dumps(db_dict, cls=NumpyEncoder) + \"\\n\")\n                except OSError as e:\n                    raise Exception(f\"Error writing to file: {e}\")\n        except KeyError:\n            print(\"No results to write.\")\n\n    def run(self, X_start=None) -&gt; Spot:\n        \"\"\"\n        Run the surrogate based optimization.\n        The optimization process is controlled by the following steps:\n            1. Initialize design\n            2. Update stats\n            3. Fit surrogate\n            4. Update design\n            5. Update stats\n            6. Update writer\n            7. Fit surrogate\n            8. Show progress if needed\n\n        Args:\n            self (object): Spot object\n            X_start (numpy.ndarray, optional): initial design. Defaults to None.\n\n        Returns:\n            (object): Spot object\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 7\n                # start point X_0\n                X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]))\n                design_control=design_control_init(init_size=ni)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                S.run(X_start=X_start)\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n                Seed set to 123\n                Seed set to 123\n                spotpython tuning: 0.0 [########--] 80.00%\n                spotpython tuning: 0.0 [#########-] 86.67%\n                spotpython tuning: 0.0 [#########-] 93.33%\n                spotpython tuning: 0.0 [##########] 100.00% Done...\n\n                S.X: [[ 0.00000000e+00  0.00000000e+00]\n                [ 0.00000000e+00  1.00000000e+00]\n                [ 1.00000000e+00  0.00000000e+00]\n                [ 1.00000000e+00  1.00000000e+00]\n                [-9.09243389e-01 -1.58234577e-01]\n                [-2.05817107e-01 -4.81249089e-01]\n                [ 9.49741171e-01 -9.46312716e-01]\n                [-1.20955714e-01  6.38358863e-02]\n                [-6.62787018e-01  1.74316373e-01]\n                [ 2.82008441e-01  9.30010114e-01]\n                [ 4.78788115e-01  6.53210582e-01]\n                [ 2.64764215e-04  4.00803185e-03]\n                [-1.66363820e-05  4.65001027e-03]\n                [-2.60995680e-04  5.46114194e-03]\n                [ 3.74504308e-03  1.86731890e-02]]\n                S.y: [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n                8.51761723e-01 2.73961367e-01 1.79751605e+00 1.87053051e-02\n                4.69672829e-01 9.44447573e-01 6.55922124e-01 1.61344194e-05\n                2.16228723e-05 2.98921900e-05 3.62713334e-04]\n\n        \"\"\"\n        self.initialize_design(X_start)\n        self.update_stats()\n        self.fit_surrogate()\n        timeout_start = time.time()\n        while self.should_continue(timeout_start):\n            self.update_design()\n            self.update_stats()\n            self.update_writer()\n            self.fit_surrogate()\n            self.show_progress_if_needed(timeout_start)\n        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n            self.spot_writer.flush()\n            self.spot_writer.close()\n        if self.fun_control[\"db_dict_name\"] is not None:\n            self.write_db_dict()\n        if self.fun_control[\"save_experiment\"]:\n            self.save_experiment()\n        return self\n\n    def initialize_design(self, X_start=None) -&gt; None:\n        \"\"\"\n        Initialize design. Generate and evaluate initial design.\n        If `X_start` is not `None`, append it to the initial design.\n        Therefore, the design size is `init_size` + `X_start.shape[0]`.\n\n        Args:\n            self (object): Spot object\n            X_start (numpy.ndarray, optional): initial design. Defaults to None.\n\n        Returns:\n            (NoneType): None\n\n        Attributes:\n            self.X (numpy.ndarray): initial design\n            self.y (numpy.ndarray): initial design values\n\n        Note:\n            * If `X_start` is has the wrong shape, it is ignored.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 7\n                # start point X_0\n                X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]))\n                design_control=design_control_init(init_size=ni)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                S.initialize_design(X_start=X_start)\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n        \"\"\"\n        if self.design_control[\"init_size\"] &gt; 0:\n            X0 = self.generate_design(\n                size=self.design_control[\"init_size\"],\n                repeats=self.design_control[\"repeats\"],\n                lower=self.lower,\n                upper=self.upper,\n            )\n        if X_start is not None:\n            if not isinstance(X_start, np.ndarray):\n                X_start = np.array(X_start)\n            X_start = np.atleast_2d(X_start)\n            try:\n                if self.design_control[\"init_size\"] &gt; 0:\n                    X0 = append(X_start, X0, axis=0)\n                else:\n                    X0 = X_start\n            except ValueError:\n                logger.warning(\"X_start has wrong shape. Ignoring it.\")\n        if X0.shape[0] == 0:\n            raise Exception(\"X0 has zero rows. Check design_control['init_size'] or X_start.\")\n        X0 = repair_non_numeric(X0, self.var_type)\n        self.X = X0\n        # (S-3): Eval initial design:\n        X_all = self.to_all_dim_if_needed(X0)\n        logger.debug(\"In Spot() initialize_design(), before calling self.fun: X_all: %s\", X_all)\n        logger.debug(\"In Spot() initialize_design(), before calling self.fun: fun_control: %s\", self.fun_control)\n        self.y = self.fun(X=X_all, fun_control=self.fun_control)\n        logger.debug(\"In Spot() initialize_design(), after calling self.fun: self.y: %s\", self.y)\n        # TODO: Error if only nan values are returned\n        logger.debug(\"New y value: %s\", self.y)\n        #\n        self.counter = self.y.size\n        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n            # range goes to init_size -1 because the last value is added by update_stats(),\n            # which always adds the last value.\n            # Changed in 0.5.9:\n            for j in range(len(self.y)):\n                X_j = self.X[j].copy()\n                y_j = self.y[j].copy()\n                config = {self.var_name[i]: X_j[i] for i in range(self.k)}\n                # var_dict = assign_values(X, get_var_name(fun_control))\n                # config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n                # see: https://github.com/pytorch/pytorch/issues/32651\n                # self.spot_writer.add_hparams(config, {\"spot_y\": y_j}, run_name=self.spot_tensorboard_path)\n                self.spot_writer.add_hparams(config, {\"hp_metric\": y_j})\n                self.spot_writer.flush()\n        #\n        self.X, self.y = remove_nan(self.X, self.y, stop_on_zero_return=True)\n        logger.debug(\"In Spot() initialize_design(), final X val, after remove nan: self.X: %s\", self.X)\n        logger.debug(\"In Spot() initialize_design(), final y val, after remove nan: self.y: %s\", self.y)\n\n    def should_continue(self, timeout_start) -&gt; bool:\n        return (self.counter &lt; self.fun_evals) and (time.time() &lt; timeout_start + self.max_time * 60)\n\n    def generate_random_point(self):\n        \"\"\"Generate a random point in the design space.\n\n        Returns:\n            (tuple): tuple containing:\n                X0 (numpy.ndarray): random point in the design space\n                y0 (numpy.ndarray): function value at X\n\n        Notes:\n            If the evaluation fails, the function returns arrays of shape[0] == 0.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1])\n                    )\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            )\n                X0, y0 = S.generate_random_point()\n                print(f\"X0: {X0}\")\n                print(f\"y0: {y0}\")\n                assert X0.size == 2\n                assert y0.size == 1\n                assert np.all(X0 &gt;= S.lower)\n                assert np.all(X0 &lt;= S.upper)\n                assert y0 &gt;= 0\n        \"\"\"\n        X0 = self.generate_design(\n            size=1,\n            repeats=1,\n            lower=self.lower,\n            upper=self.upper,\n        )\n        X0 = repair_non_numeric(X0, self.var_type)\n        X_all = self.to_all_dim_if_needed(X0)\n        logger.debug(\"In Spot() generate_random_point(), before calling self.fun: X_all: %s\", X_all)\n        logger.debug(\"In Spot() generate_random_point(), before calling self.fun: fun_control: %s\", self.fun_control)\n        y0 = self.fun(X=X_all, fun_control=self.fun_control)\n        X0, y0 = remove_nan(X0, y0, stop_on_zero_return=False)\n        return X0, y0\n\n    def update_design(self) -&gt; None:\n        \"\"\"\n        Update design. Generate and evaluate new design points.\n        It is basically a call to the method `get_new_X0()`.\n        If `noise` is `True`, additionally the following steps\n        (from `get_X_ocba()`) are performed:\n        1. Compute OCBA points.\n        2. Evaluate OCBA points.\n        3. Append OCBA points to the new design points.\n\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (NoneType): None\n\n        Attributes:\n            self.X (numpy.ndarray): updated design\n            self.y (numpy.ndarray): updated design values\n\n        Examples:\n            &gt;&gt;&gt; # 1. Without OCBA points:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                from spotpython.spot import spot\n                # number of initial points:\n                ni = 0\n                X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1])\n                    )\n                design_control=design_control_init(init_size=ni)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                S.initialize_design(X_start=X_start)\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n                X_shape_before = S.X.shape\n                print(f\"X_shape_before: {X_shape_before}\")\n                print(f\"y_size_before: {S.y.size}\")\n                y_size_before = S.y.size\n                S.update_stats()\n                S.fit_surrogate()\n                S.update_design()\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n                print(f\"S.n_points: {S.n_points}\")\n                print(f\"X_shape_after: {S.X.shape}\")\n                print(f\"y_size_after: {S.y.size}\")\n            &gt;&gt;&gt; #\n            &gt;&gt;&gt; # 2. Using the OCBA points:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init\n                # number of initial points:\n                ni = 3\n                X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                        sigma=0.02,\n                        lower = np.array([-1, -1]),\n                        upper = np.array([1, 1]),\n                        noise=True,\n                        ocba_delta=1,\n                    )\n                design_control=design_control_init(init_size=ni, repeats=2)\n\n                S = spot.Spot(fun=fun,\n                            design_control=design_control,\n                            fun_control=fun_control\n                )\n                S.initialize_design(X_start=X_start)\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n                X_shape_before = S.X.shape\n                print(f\"X_shape_before: {X_shape_before}\")\n                print(f\"y_size_before: {S.y.size}\")\n                y_size_before = S.y.size\n                S.update_stats()\n                S.fit_surrogate()\n                S.update_design()\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n                print(f\"S.n_points: {S.n_points}\")\n                print(f\"S.ocba_delta: {S.ocba_delta}\")\n                print(f\"X_shape_after: {S.X.shape}\")\n                print(f\"y_size_after: {S.y.size}\")\n                # compare the shapes of the X and y values before and after the update_design method\n                assert X_shape_before[0] + S.n_points * S.fun_repeats + S.ocba_delta == S.X.shape[0]\n                assert X_shape_before[1] == S.X.shape[1]\n                assert y_size_before + S.n_points * S.fun_repeats + S.ocba_delta == S.y.size\n\n        \"\"\"\n        # OCBA (only if noise). Determination of the OCBA points depends on the\n        # old X and y values.\n        if self.noise and self.ocba_delta &gt; 0 and not np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &lt;= 2):\n            logger.warning(\"self.var_y &lt;= 0. OCBA points are not generated:\")\n            logger.warning(\"There are less than 3 points or points with no variance information.\")\n            logger.debug(\"In update_design(): self.mean_X: %s\", self.mean_X)\n            logger.debug(\"In update_design(): self.var_y: %s\", self.var_y)\n        if self.noise and self.ocba_delta &gt; 0 and np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &gt; 2):\n            X_ocba = get_ocba_X(self.mean_X, self.mean_y, self.var_y, self.ocba_delta)\n        else:\n            X_ocba = None\n        # Determine the new X0 values based on the old X and y values:\n        X0 = self.get_new_X0()\n        # Append OCBA points to the new design points:\n        if self.noise and self.ocba_delta &gt; 0 and np.all(self.var_y &gt; 0):\n            X0 = append(X_ocba, X0, axis=0)\n        X_all = self.to_all_dim_if_needed(X0)\n        logger.debug(\n            \"In update_design(): self.fun_control sigma and seed passed to fun(): %s %s\",\n            self.fun_control[\"sigma\"],\n            self.fun_control[\"seed\"],\n        )\n        # (S-18): Evaluating New Solutions:\n        y0 = self.fun(X=X_all, fun_control=self.fun_control)\n        X0, y0 = remove_nan(X0, y0, stop_on_zero_return=False)\n        # Append New Solutions (only if they are not nan):\n        if y0.shape[0] &gt; 0:\n            self.X = np.append(self.X, X0, axis=0)\n            self.y = np.append(self.y, y0)\n        else:\n            # otherwise, generate a random point and append it to the design\n            Xr, yr = self.generate_random_point()\n            self.X = np.append(self.X, Xr, axis=0)\n            self.y = np.append(self.y, yr)\n\n    def fit_surrogate(self) -&gt; None:\n        \"\"\"\n        Fit surrogate model. The surrogate model\n        is fitted to the data stored in `self.X` and `self.y`.\n        It uses the generic `fit()` method of the\n        surrogate model `surrogate`. The default surrogate model is\n        an instance from spotpython's `Kriging` class.\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (NoneType): None\n\n        Attributes:\n            self.surrogate (object): surrogate model\n\n        Note:\n            * As shown in https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/\n            other surrogate models can be used as well.\n\n        Examples:\n                &gt;&gt;&gt; import numpy as np\n                    from spotpython.fun.objectivefunctions import analytical\n                    from spotpython.spot import spot\n                    from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                    # number of initial points:\n                    ni = 0\n                    X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n                    fun = analytical().fun_sphere\n                    fun_control = fun_control_init(\n                        lower = np.array([-1, -1]),\n                        upper = np.array([1, 1])\n                        )\n                    design_control=design_control_init(init_size=ni)\n                    S = spot.Spot(fun=fun,\n                                fun_control=fun_control,\n                                design_control=design_control,)\n                    S.initialize_design(X_start=X_start)\n                    S.update_stats()\n                    S.fit_surrogate()\n\n        \"\"\"\n        logger.debug(\"In fit_surrogate(): self.X: %s\", self.X)\n        logger.debug(\"In fit_surrogate(): self.y: %s\", self.y)\n        logger.debug(\"In fit_surrogate(): self.X.shape: %s\", self.X.shape)\n        logger.debug(\"In fit_surrogate(): self.y.shape: %s\", self.y.shape)\n        X_points = self.X.shape[0]\n        y_points = self.y.shape[0]\n        if X_points == y_points:\n            if X_points &gt; self.max_surrogate_points:\n                X_S, y_S = select_distant_points(X=self.X, y=self.y, k=self.max_surrogate_points)\n            else:\n                X_S = self.X\n                y_S = self.y\n            self.surrogate.fit(X_S, y_S)\n        else:\n            logger.warning(\"X and y have different sizes. Surrogate not fitted.\")\n        if self.show_models:\n            self.plot_model()\n\n    def show_progress_if_needed(self, timeout_start) -&gt; None:\n        \"\"\"Show progress bar if `show_progress` is `True`. If\n        self.progress_file is not `None`, the progress bar is saved\n        in the file with the name `self.progress_file`.\n\n        Args:\n            self (object): Spot object\n            timeout_start (float): start time\n\n        Returns:\n            (NoneType): None\n        \"\"\"\n        if not self.show_progress:\n            return\n        if isfinite(self.fun_evals):\n            progress_bar(progress=self.counter / self.fun_evals, y=self.min_y, filename=self.progress_file)\n        else:\n            progress_bar(progress=(time.time() - timeout_start) / (self.max_time * 60), y=self.min_y, filename=self.progress_file)\n\n    def generate_design(self, size, repeats, lower, upper) -&gt; np.array:\n        \"\"\"Generate a design with `size` points in the interval [lower, upper].\n\n        Args:\n            size (int): number of points\n            repeats (int): number of repeats\n            lower (numpy.ndarray): lower bound of the design space\n            upper (numpy.ndarray): upper bound of the design space\n\n        Returns:\n            (numpy.ndarray): design points\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.spot import spot\n                from spotpython.utils.init import design_control_init\n                from spotpython.fun.objectivefunctions import analytical\n                design_control = design_control_init(init_size=3)\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]),\n                    fun_evals=fun_evals,\n                    tolerance_x = np.sqrt(np.spacing(1))\n                    )\n                S = spot.Spot(fun = analytical().fun_sphere,\n                            fun_control = fun_control,\n                            design_control = design_control)\n                X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n                assert X.shape[0] == 3\n                assert X.shape[1] == 2\n                print(X)\n                    array([[77.25493789,  0.31539299],\n                    [59.32133757,  0.93854273],\n                    [27.4698033 ,  0.3959685 ]])\n        \"\"\"\n        return self.design.scipy_lhd(n=size, repeats=repeats, lower=lower, upper=upper)\n\n    def update_stats(self) -&gt; None:\n        \"\"\"\n        Update the following stats: 1. `min_y` 2. `min_X` 3. `counter`\n        If `noise` is `True`, additionally the following stats are computed: 1. `mean_X`\n        2. `mean_y` 3. `min_mean_y` 4. `min_mean_X`.\n\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (NoneType): None\n\n        Attributes:\n            self.min_y (float): minimum y value\n            self.min_X (numpy.ndarray): X value of the minimum y value\n            self.counter (int): number of function evaluations\n            self.mean_X (numpy.ndarray): mean X values\n            self.mean_y (numpy.ndarray): mean y values\n            self.var_y (numpy.ndarray): variance of y values\n            self.min_mean_y (float): minimum mean y value\n            self.min_mean_X (numpy.ndarray): X value of the minimum mean y value\n\n        \"\"\"\n        self.min_y = min(self.y)\n        self.min_X = self.X[argmin(self.y)]\n        self.counter = self.y.size\n        self.fun_control.update({\"counter\": self.counter})\n        # Update aggregated x and y values (if noise):\n        if self.noise:\n            Z = aggregate_mean_var(X=self.X, y=self.y)\n            self.mean_X = Z[0]\n            self.mean_y = Z[1]\n            self.var_y = Z[2]\n            # X value of the best mean y value so far:\n            self.min_mean_X = self.mean_X[argmin(self.mean_y)]\n            # variance of the best mean y value so far:\n            self.min_var_y = self.var_y[argmin(self.mean_y)]\n            # best mean y value so far:\n            self.min_mean_y = self.mean_y[argmin(self.mean_y)]\n\n    def update_writer(self) -&gt; None:\n        print(\"In update_writer().\")\n        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n            # get the last y value:\n            y_last = self.y[-1].copy()\n            if self.noise is False:\n                y_min = self.min_y.copy()\n                X_min = self.min_X.copy()\n                # y_min: best y value so far\n                # y_last: last y value, can be worse than y_min\n                self.spot_writer.add_scalars(\"spot_y\", {\"min\": y_min, \"last\": y_last}, self.counter)\n                # X_min: X value of the best y value so far\n                self.spot_writer.add_scalars(\"spot_X\", {f\"X_{i}\": X_min[i] for i in range(self.k)}, self.counter)\n            else:\n                # get the last n y values:\n                y_last_n = self.y[-self.fun_repeats :].copy()\n                # y_min_mean: best mean y value so far\n                y_min_mean = self.min_mean_y.copy()\n                # X_min_mean: X value of the best mean y value so far\n                X_min_mean = self.min_mean_X.copy()\n                # y_min_var: variance of the min y value so far\n                y_min_var = self.min_var_y.copy()\n                self.spot_writer.add_scalar(\"spot_y_min_var\", y_min_var, self.counter)\n                # y_min_mean: best mean y value so far (see above)\n                self.spot_writer.add_scalar(\"spot_y\", y_min_mean, self.counter)\n                # last n y values (noisy):\n                self.spot_writer.add_scalars(\"spot_y\", {f\"y_last_n{i}\": y_last_n[i] for i in range(self.fun_repeats)}, self.counter)\n                # X_min_mean: X value of the best mean y value so far (see above)\n                self.spot_writer.add_scalars(\"spot_X_noise\", {f\"X_min_mean{i}\": X_min_mean[i] for i in range(self.k)}, self.counter)\n            # get last value of self.X and convert to dict. take the values from self.var_name as keys:\n            X_last = self.X[-1].copy()\n            config = {self.var_name[i]: X_last[i] for i in range(self.k)}\n            # var_dict = assign_values(X, get_var_name(fun_control))\n            # config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n            # hyperparameters X and value y of the last configuration:\n            # see: https://github.com/pytorch/pytorch/issues/32651\n            # self.spot_writer.add_hparams(config, {\"spot_y\": y_last}, run_name=self.spot_tensorboard_path)\n            self.spot_writer.add_hparams(config, {\"hp_metric\": y_last})\n            self.spot_writer.flush()\n            print(\"update_writer(): Done.\")\n        else:\n            print(\"No spot_writer available.\")\n\n    def suggest_new_X(self) -&gt; np.array:\n        \"\"\"\n        Compute `n_points` new infill points in natural units.\n        These diffrent points are computed by the optimizer using increasing seed.\n        The optimizer searches in the ranges from `lower_j` to `upper_j`.\n        The method `infill()` is used as the objective function.\n\n        Returns:\n            (numpy.ndarray): `n_points` infill points in natural units, each of dim k\n\n        Note:\n            This is step (S-14a) in [bart21i].\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.spot import spot\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                nn = 3\n                fun_sphere = analytical().fun_sphere\n                fun_control = fun_control_init(\n                        lower = np.array([-1, -1]),\n                        upper = np.array([1, 1]),\n                        n_points=nn,\n                        )\n                spot_1 = spot.Spot(\n                    fun=fun_sphere,\n                    fun_control=fun_control,\n                    )\n                # (S-2) Initial Design:\n                spot_1.X = spot_1.design.scipy_lhd(\n                    spot_1.design_control[\"init_size\"], lower=spot_1.lower, upper=spot_1.upper\n                )\n                print(f\"spot_1.X: {spot_1.X}\")\n                # (S-3): Eval initial design:\n                spot_1.y = spot_1.fun(spot_1.X)\n                print(f\"spot_1.y: {spot_1.y}\")\n                spot_1.fit_surrogate()\n                X0 = spot_1.suggest_new_X()\n                print(f\"X0: {X0}\")\n                assert X0.size == spot_1.n_points * spot_1.k\n                assert X0.ndim == 2\n                assert X0.shape[0] == nn\n                assert X0.shape[1] == 2\n                spot_1.X: [[ 0.86352963  0.7892358 ]\n                            [-0.24407197 -0.83687436]\n                            [ 0.36481882  0.8375811 ]\n                            [ 0.415331    0.54468512]\n                            [-0.56395091 -0.77797854]\n                            [-0.90259409 -0.04899292]\n                            [-0.16484832  0.35724741]\n                            [ 0.05170659  0.07401196]\n                            [-0.78548145 -0.44638164]\n                            [ 0.64017497 -0.30363301]]\n                spot_1.y: [1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764\n                0.15480068 0.00815134 0.81623768 0.502017  ]\n                X0: [[0.00154544 0.003962  ]\n                    [0.00165526 0.00410847]\n                    [0.00165685 0.0039177 ]]\n        \"\"\"\n        # (S-14a) Optimization on the surrogate:\n        new_X = np.zeros([self.n_points, self.k], dtype=float)\n        optimizer_name = self.optimizer.__name__\n        optimizers = {\n            \"dual_annealing\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n            \"differential_evolution\": lambda: self.optimizer(\n                func=self.infill,\n                bounds=self.de_bounds,\n                maxiter=self.optimizer_control[\"max_iter\"],\n                seed=self.optimizer_control[\"seed\"],\n            ),\n            \"direct\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds, eps=1e-2),\n            \"shgo\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n            \"basinhopping\": lambda: self.optimizer(func=self.infill, x0=self.min_X),\n            \"default\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n        }\n        for i in range(self.n_points):\n            self.optimizer_control[\"seed\"] = self.optimizer_control[\"seed\"] + i\n            result = optimizers.get(optimizer_name, optimizers[\"default\"])()\n            new_X[i][:] = result.x\n        return np.unique(new_X, axis=0)\n\n    def infill(self, x) -&gt; float:\n        \"\"\"\n        Infill (acquisition) function. Evaluates one point on the surrogate via `surrogate.predict(x.reshape(1,-1))`,\n        if `sklearn` surrogates are used or `surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)`\n        if the internal surrogate `kriging` is selected.\n        This method is passed to the optimizer in `suggest_new_X`, i.e., the optimizer is called via\n        `self.optimizer(func=self.infill)`.\n\n        Args:\n            x (array): point in natural units with shape `(1, dim)`.\n\n        Returns:\n            (numpy.ndarray): value based on infill criterion, e.g., `\"ei\"`. Shape `(1,)`.\n                The objective function value `y` that is used as a base value for the\n                infill criterion is calculated in natural units.\n\n        Note:\n            This is step (S-12) in [bart21i].\n        \"\"\"\n        # Reshape x to have shape (1, -1) because the predict method expects a 2D array\n        X = x.reshape(1, -1)\n        if isinstance(self.surrogate, Kriging):\n            return self.surrogate.predict(X, return_val=self.infill_criterion)\n        else:\n            return self.surrogate.predict(X)\n\n    def plot_progress(self, show=True, log_x=False, log_y=False, filename=\"plot.png\", style=[\"ko\", \"k\", \"ro-\"], dpi=300, tkagg=False) -&gt; None:\n        \"\"\"Plot the progress of the hyperparameter tuning (optimization).\n\n        Args:\n            show (bool):\n                Show the plot.\n            log_x (bool):\n                Use logarithmic scale for x-axis.\n            log_y (bool):\n                Use logarithmic scale for y-axis.\n            filename (str):\n                Filename to save the plot.\n            style (list):\n                Style of the plot. Default: ['k', 'ro-'], i.e., the initial points are plotted as a black line\n                and the subsequent points as red dots connected by a line.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 7\n                # number of points\n                fun_evals = 10\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1])\n                    fun_evals=fun_evals,\n                    tolerance_x = np.sqrt(np.spacing(1))\n                    )\n                design_control=design_control_init(init_size=ni)\n                surrogate_control=surrogate_control_init(n_theta=3)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control\n                            design_control=design_control,\n                            surrogate_control=surrogate_control,)\n                S.run()\n                S.plot_progress(log_y=True)\n\n        \"\"\"\n        if tkagg:\n            matplotlib.use(\"TkAgg\")\n        fig = pylab.figure(figsize=(9, 6))\n        s_y = pd.Series(self.y)\n        s_c = s_y.cummin()\n        n_init = self.design_control[\"init_size\"] * self.design_control[\"repeats\"]\n\n        ax = fig.add_subplot(211)\n        if n_init &lt;= len(s_y):\n            ax.plot(\n                range(1, n_init + 1),\n                s_y[:n_init],\n                style[0],\n                range(1, n_init + 2),\n                [s_c[:n_init].min()] * (n_init + 1),\n                style[1],\n                range(n_init + 1, len(s_c) + 1),\n                s_c[n_init:],\n                style[2],\n            )\n        else:\n            # plot only s_y values:\n            ax.plot(range(1, len(s_y) + 1), s_y, style[0])\n            logger.warning(\"Less evaluations ({len(s_y)}) than initial design points ({n_init}).\")\n        ax.set_xlabel(\"Iteration\")\n        if log_x:\n            ax.set_xscale(\"log\")\n        if log_y:\n            ax.set_yscale(\"log\")\n        if filename is not None:\n            pylab.savefig(filename, dpi=dpi, bbox_inches=\"tight\")\n        if show:\n            pylab.show()\n\n    def plot_model(self, y_min=None, y_max=None) -&gt; None:\n        \"\"\"\n        Plot the model fit for 1-dim objective functions.\n\n        Args:\n            self (object):\n                spot object\n            y_min (float, optional):\n                y range, lower bound.\n            y_max (float, optional):\n                y range, upper bound.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                # number of initial points:\n                ni = 3\n                # number of points\n                fun_evals = 7\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1]),\n                    upper = np.array([1]),\n                    fun_evals=fun_evals,\n                    tolerance_x = np.sqrt(np.spacing(1))\n                    )\n                design_control=design_control_init(init_size=ni)\n\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control\n                S.run()\n                S.plot_model()\n        \"\"\"\n        if self.k == 1:\n            X_test = np.linspace(self.lower[0], self.upper[0], 100)\n            y_test = self.fun(X=X_test.reshape(-1, 1), fun_control=self.fun_control)\n            if isinstance(self.surrogate, Kriging):\n                y_hat = self.surrogate.predict(X_test[:, np.newaxis], return_val=\"y\")\n            else:\n                y_hat = self.surrogate.predict(X_test[:, np.newaxis])\n            plt.plot(X_test, y_hat, label=\"Model\")\n            plt.plot(X_test, y_test, label=\"True function\")\n            plt.scatter(self.X, self.y, edgecolor=\"b\", s=20, label=\"Samples\")\n            plt.scatter(self.X[-1], self.y[-1], edgecolor=\"r\", s=30, label=\"Last Sample\")\n            if self.noise:\n                plt.scatter(self.min_mean_X, self.min_mean_y, edgecolor=\"g\", s=30, label=\"Best Sample (mean)\")\n            else:\n                plt.scatter(self.min_X, self.min_y, edgecolor=\"g\", s=30, label=\"Best Sample\")\n            plt.xlabel(\"x\")\n            plt.ylabel(\"y\")\n            plt.xlim((self.lower[0], self.upper[0]))\n            if y_min is None:\n                y_min = min([min(self.y), min(y_test)])\n            if y_max is None:\n                y_max = max([max(self.y), max(y_test)])\n            plt.ylim((y_min, y_max))\n            plt.legend(loc=\"best\")\n            # plt.title(self.surrogate.__class__.__name__ + \". \" + str(self.counter) + \": \" + str(self.min_y))\n            if self.noise:\n                plt.title(\"fun_evals: \" + str(self.counter) + \". min_y (noise): \" + str(np.round(self.min_y, 6)) + \" min_mean_y: \" + str(np.round(self.min_mean_y, 6)))\n            else:\n                plt.title(\"fun_evals: \" + str(self.counter) + \". min_y: \" + str(np.round(self.min_y, 6)))\n            plt.show()\n\n    def print_results(self, print_screen=True, dict=None) -&gt; list[str]:\n        \"\"\"Print results from the run:\n            1. min y\n            2. min X\n            If `noise == True`, additionally the following values are printed:\n            3. min mean y\n            4. min mean X\n\n        Args:\n            print_screen (bool, optional):\n                print results to screen\n\n        Returns:\n            output (list):\n                list of results\n        \"\"\"\n        output = []\n        if print_screen:\n            print(f\"min y: {self.min_y}\")\n            if self.noise:\n                print(f\"min mean y: {self.min_mean_y}\")\n        if self.noise:\n            res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n        else:\n            res = self.to_all_dim(self.min_X.reshape(1, -1))\n        for i in range(res.shape[1]):\n            if self.all_var_name is None:\n                var_name = \"x\" + str(i)\n            else:\n                var_name = self.all_var_name[i]\n                var_type = self.all_var_type[i]\n                if var_type == \"factor\" and dict is not None:\n                    val = get_ith_hyperparameter_name_from_fun_control(fun_control=dict, key=var_name, i=int(res[0][i]))\n                else:\n                    val = res[0][i]\n            if print_screen:\n                print(var_name + \":\", val)\n            output.append([var_name, val])\n        return output\n\n    def print_results_old(self, print_screen=True, dict=None) -&gt; list[str]:\n        \"\"\"Print results from the run:\n            1. min y\n            2. min X\n            If `noise == True`, additionally the following values are printed:\n            3. min mean y\n            4. min mean X\n\n        Args:\n            print_screen (bool, optional):\n                print results to screen\n\n        Returns:\n            output (list):\n                list of results\n        \"\"\"\n        output = []\n        if print_screen:\n            print(f\"min y: {self.min_y}\")\n        res = self.to_all_dim(self.min_X.reshape(1, -1))\n        for i in range(res.shape[1]):\n            if self.all_var_name is None:\n                var_name = \"x\" + str(i)\n            else:\n                var_name = self.all_var_name[i]\n                var_type = self.all_var_type[i]\n                if var_type == \"factor\" and dict is not None:\n                    val = get_ith_hyperparameter_name_from_fun_control(fun_control=dict, key=var_name, i=int(res[0][i]))\n                else:\n                    val = res[0][i]\n            if print_screen:\n                print(var_name + \":\", val)\n            output.append([var_name, val])\n        if self.noise:\n            res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n            if print_screen:\n                print(f\"min mean y: {self.min_mean_y}\")\n            for i in range(res.shape[1]):\n                var_name = \"x\" + str(i) if self.all_var_name is None else self.all_var_name[i]\n                if print_screen:\n                    print(var_name + \":\", res[0][i])\n                output.append([var_name, res[0][i]])\n        return output\n\n    def get_tuned_hyperparameters(self, fun_control=None) -&gt; dict:\n        \"\"\"Return the tuned hyperparameter values from the run.\n        If `noise == True`, the mean values are returned.\n\n        Args:\n            fun_control (dict, optional):\n                fun_control dictionary\n\n        Returns:\n            (dict): dictionary of tuned hyperparameters.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.utils.device import getDevice\n                from math import inf\n                from spotpython.utils.init import fun_control_init\n                import numpy as np\n                from spotpython.hyperparameters.values import set_control_key_value\n                from spotpython.data.diabetes import Diabetes\n                MAX_TIME = 1\n                FUN_EVALS = 10\n                INIT_SIZE = 5\n                WORKERS = 0\n                PREFIX=\"037\"\n                DEVICE = getDevice()\n                DEVICES = 1\n                TEST_SIZE = 0.4\n                TORCH_METRIC = \"mean_squared_error\"\n                dataset = Diabetes()\n                fun_control = fun_control_init(\n                    _L_in=10,\n                    _L_out=1,\n                    _torchmetric=TORCH_METRIC,\n                    PREFIX=PREFIX,\n                    TENSORBOARD_CLEAN=True,\n                    data_set=dataset,\n                    device=DEVICE,\n                    enable_progress_bar=False,\n                    fun_evals=FUN_EVALS,\n                    log_level=50,\n                    max_time=MAX_TIME,\n                    num_workers=WORKERS,\n                    show_progress=True,\n                    test_size=TEST_SIZE,\n                    tolerance_x=np.sqrt(np.spacing(1)),\n                    )\n                from spotpython.light.regression.netlightregression import NetLightRegression\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.hyperparameters.values import add_core_model_to_fun_control\n                add_core_model_to_fun_control(fun_control=fun_control,\n                                            core_model=NetLightRegression,\n                                            hyper_dict=LightHyperDict)\n                from spotpython.hyperparameters.values import set_control_hyperparameter_value\n                set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n                set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n                set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n                set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                                \"Adam\",\n                                \"RAdam\",\n                            ])\n                set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n                set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n                set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n                set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                                \"ReLU\",\n                                \"LeakyReLU\"\n                            ] )\n                from spotpython.utils.init import design_control_init, surrogate_control_init\n                design_control = design_control_init(init_size=INIT_SIZE)\n                surrogate_control = surrogate_control_init(noise=True,\n                                                            n_theta=2)\n                from spotpython.fun.hyperlight import HyperLight\n                fun = HyperLight(log_level=50).fun\n                from spotpython.spot import spot\n                spot_tuner = spot.Spot(fun=fun,\n                                    fun_control=fun_control,\n                                    design_control=design_control,\n                                    surrogate_control=surrogate_control)\n                spot_tuner.run()\n                spot_tuner.get_tuned_hyperparameters()\n                    {'l1': 7.0,\n                    'epochs': 5.0,\n                    'batch_size': 4.0,\n                    'act_fn': 0.0,\n                    'optimizer': 0.0,\n                    'dropout_prob': 0.01,\n                    'lr_mult': 5.0,\n                    'patience': 3.0,\n                    'initialization': 1.0}\n\n        \"\"\"\n        output = []\n        if self.noise:\n            res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n        else:\n            res = self.to_all_dim(self.min_X.reshape(1, -1))\n        for i in range(res.shape[1]):\n            if self.all_var_name is None:\n                var_name = \"x\" + str(i)\n            else:\n                var_name = self.all_var_name[i]\n                var_type = self.all_var_type[i]\n                if var_type == \"factor\" and fun_control is not None:\n                    val = get_ith_hyperparameter_name_from_fun_control(fun_control=fun_control, key=var_name, i=int(res[0][i]))\n                else:\n                    val = res[0][i]\n            output.append([var_name, val])\n        # convert list to a dictionary\n        output = dict(output)\n        return output\n\n    def chg(self, x, y, z0, i, j) -&gt; list:\n        \"\"\"\n        Change the values of elements at indices `i` and `j` in the array `z0` to `x` and `y`, respectively.\n\n        Args:\n            x (int or float):\n                The new value for the element at index `i`.\n            y (int or float):\n                The new value for the element at index `j`.\n            z0 (list or numpy.ndarray):\n                The array to be modified.\n            i (int):\n                The index of the element to be changed to `x`.\n            j (int):\n                The index of the element to be changed to `y`.\n\n        Returns:\n            (list) or (numpy.ndarray): The modified array.\n\n        Examples:\n                &gt;&gt;&gt; import numpy as np\n                    from spotpython.fun.objectivefunctions import analytical\n                    from spotpython.spot import spot\n                    from spotpython.utils.init import (\n                        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                    fun = analytical().fun_sphere\n                    fun_control = fun_control_init(\n                        lower = np.array([-1]),\n                        upper = np.array([1]),\n                    )\n                    S = spot.Spot(fun=fun,\n                                func_control=fun_control)\n                    z0 = [1, 2, 3]\n                    print(f\"Before: {z0}\")\n                    new_val_1 = 4\n                    new_val_2 = 5\n                    index_1 = 0\n                    index_2 = 2\n                    S.chg(x=new_val_1, y=new_val_2, z0=z0, i=index_1, j=index_2)\n                    print(f\"After: {z0}\")\n                    Before: [1, 2, 3]\n                    After: [4, 2, 5]\n        \"\"\"\n        z0[i] = x\n        z0[j] = y\n        return z0\n\n    def process_z00(self, z00, use_min=True) -&gt; list:\n        \"\"\"Process each entry in the `z00` array according to the corresponding type\n        in the `self.var_type` list.\n        Specifically, if the type is \"float\", the function will calculate the mean of the two `z00` values.\n        If the type is not \"float\", the function will retrun the maximum of the two `z00` values.\n\n        Args:\n            z00 (numpy.ndarray):\n                Array of values to process.\n            use_min (bool):\n                If `True`, the minimum value is returned. If `False`, the maximum value is returned.\n\n        Returns:\n            (list): Processed values.\n\n        Examples:\n            from spotpython.spot import spot\n            import numpy as np\n            import random\n            z00 = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n            spot.var_type = [\"float\", \"int\", \"int\", \"float\"]\n            spot.process_z00(z00)\n            [3, 6, 7, 6]\n\n        \"\"\"\n        result = []\n        for i in range(len(self.var_type)):\n            if self.var_type[i] == \"float\":\n                mean_value = np.mean(z00[:, i])\n                result.append(mean_value)\n            else:  # var_type[i] == 'int'\n                if use_min:\n                    min_value = min(z00[:, i])\n                    result.append(min_value)\n                else:\n                    max_value = max(z00[:, i])\n                    result.append(max_value)\n        return result\n\n    def plot_contour(\n        self,\n        i=0,\n        j=1,\n        min_z=None,\n        max_z=None,\n        show=True,\n        filename=None,\n        n_grid=50,\n        contour_levels=10,\n        dpi=200,\n        title=\"\",\n        figsize=(12, 6),\n        use_min=False,\n        use_max=True,\n        tkagg=False,\n    ) -&gt; None:\n        \"\"\"Plot the contour of any dimension.\"\"\"\n\n        def generate_mesh_grid(lower, upper, grid_points):\n            \"\"\"Generate a mesh grid for the given range.\"\"\"\n            x = np.linspace(lower[i], upper[i], num=grid_points)\n            y = np.linspace(lower[j], upper[j], num=grid_points)\n            return np.meshgrid(x, y), x, y\n\n        def validate_types(var_type, lower, upper):\n            \"\"\"Validate if the dimensions of var_type, lower, and upper are the same.\"\"\"\n            if var_type is not None:\n                if len(var_type) != len(lower) or len(var_type) != len(upper):\n                    raise ValueError(\"The dimensions of var_type, lower, and upper must be the same.\")\n\n        def setup_plot():\n            \"\"\"Setup the plot with specified figure size.\"\"\"\n            fig = pylab.figure(figsize=figsize)\n            return fig\n\n        def predict_contour_values(X, Y, z0):\n            \"\"\"Predict contour values based on the surrogate model.\"\"\"\n            grid_points = np.c_[np.ravel(X), np.ravel(Y)]\n            predictions = []\n\n            for x, y in grid_points:\n                adjusted_z0 = self.chg(x, y, z0.copy(), i, j)\n                prediction = self.surrogate.predict(np.array([adjusted_z0]))\n                predictions.append(prediction[0])\n\n            Z = np.array(predictions).reshape(X.shape)\n            return Z\n\n        def plot_contour_subplots(X, Y, Z, ax, min_z, max_z, contour_levels):\n            \"\"\"Plot the contour and 3D surface subplots.\"\"\"\n            contour = ax.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\", vmin=min_z, vmax=max_z)\n            pylab.colorbar(contour, ax=ax)\n\n        if tkagg:\n            matplotlib.use(\"TkAgg\")\n        fig = setup_plot()\n\n        (X, Y), x, y = generate_mesh_grid(self.lower, self.upper, n_grid)\n        validate_types(self.var_type, self.lower, self.upper)\n\n        z00 = np.array([self.lower, self.upper])\n        Z_list, X_list, Y_list = [], [], []\n\n        if use_min:\n            z0_min = self.process_z00(z00, use_min=True)\n            Z_min = predict_contour_values(X, Y, z0_min)\n            Z_list.append(Z_min)\n            X_list.append(X)\n            Y_list.append(Y)\n\n        if use_max:\n            z0_max = self.process_z00(z00, use_min=False)\n            Z_max = predict_contour_values(X, Y, z0_max)\n            Z_list.append(Z_max)\n            X_list.append(X)\n            Y_list.append(Y)\n\n        if Z_list:  # Ensure that there is at least one Z to stack\n            Z_combined = np.vstack(Z_list)\n            X_combined = np.vstack(X_list)\n            Y_combined = np.vstack(Y_list)\n\n        if min_z is None:\n            min_z = np.min(Z_combined)\n        if max_z is None:\n            max_z = np.max(Z_combined)\n\n        ax_contour = fig.add_subplot(221)\n        plot_contour_subplots(X_combined, Y_combined, Z_combined, ax_contour, min_z, max_z, contour_levels)\n\n        if self.var_name is None:\n            ax_contour.set_xlabel(f\"x{i}\")\n            ax_contour.set_ylabel(f\"x{j}\")\n        else:\n            ax_contour.set_xlabel(f\"x{i}: {self.var_name[i]}\")\n            ax_contour.set_ylabel(f\"x{j}: {self.var_name[j]}\")\n\n        ax_3d = fig.add_subplot(222, projection=\"3d\")\n        ax_3d.plot_surface(X_combined, Y_combined, Z_combined, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\", vmin=min_z, vmax=max_z)\n\n        if self.var_name is None:\n            ax_3d.set_xlabel(f\"x{i}\")\n            ax_3d.set_ylabel(f\"x{j}\")\n        else:\n            ax_3d.set_xlabel(f\"x{i}: {self.var_name[i]}\")\n            ax_3d.set_ylabel(f\"x{j}: {self.var_name[j]}\")\n\n        plt.title(title)\n\n        if filename:\n            pylab.savefig(filename, bbox_inches=\"tight\", dpi=dpi, pad_inches=0)\n\n        if show:\n            pylab.show()\n\n    def plot_important_hyperparameter_contour(\n        self,\n        threshold=0.0,\n        filename=None,\n        show=True,\n        max_imp=None,\n        title=\"\",\n        scale_global=False,\n        n_grid=50,\n        contour_levels=10,\n        dpi=200,\n        use_min=False,\n        use_max=True,\n        tkagg=False,\n    ) -&gt; None:\n        \"\"\"\n        Plot the contour of important hyperparameters.\n        Calls `plot_contour` for each pair of important hyperparameters.\n        Importance can be specified by the threshold.\n\n        Args:\n            threshold (float):\n                threshold for the importance. Not used any more in spotpython &gt;= 0.13.2.\n            filename (str):\n                filename of the plot\n            show (bool):\n                show the plot. Default is `True`.\n            max_imp (int):\n                maximum number of important hyperparameters. If there are more important hyperparameters\n                than `max_imp`, only the max_imp important ones are selected.\n            title (str):\n                title of the plots\n            scale_global (bool):\n                scale the z-axis globally. Default is `False`.\n            n_grid (int):\n                number of grid points. Default is 50.\n            contour_levels (int):\n                number of contour levels. Default is 10.\n            dpi (int):\n                dpi of the plot. Default is 200.\n            use_min (bool):\n                Use the minimum value for determing the hidden dimensions in the plot for categorical and\n                integer parameters.\n                In 3d-plots, only two variables can be independent. The remaining input variables are set\n                to their minimum value.\n                Default is `False`.\n                If use_min and use_max are both `True`, both values are used.\n            use_max (bool):\n                Use the minimum value for determing the hidden dimensions in the plot for categorical and\n                integer parameters.\n                In 3d-plots, only two variables can be independent. The remaining input variables are set\n                to their minimum value.\n                Default is `True`.\n                If use_min and use_max are both `True`, both values are used.\n\n        Returns:\n            None.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 5\n                # number of points\n                fun_evals = 10\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1, -1]),\n                    upper = np.array([1, 1, 1]),\n                    fun_evals=fun_evals,\n                    tolerance_x = np.sqrt(np.spacing(1))\n                    )\n                design_control=design_control_init(init_size=ni)\n                surrogate_control=surrogate_control_init(n_theta=3)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,\n                            surrogate_control=surrogate_control,)\n                S.run()\n                S.plot_important_hyperparameter_contour()\n\n        \"\"\"\n        impo = self.print_importance(threshold=threshold, print_screen=True)\n        indices = sort_by_kth_and_return_indices(array=impo, k=1)\n        # take the first max_imp values from the indices array\n        if max_imp is not None:\n            indices = indices[:max_imp]\n        if scale_global:\n            min_z = min(self.y)\n            max_z = max(self.y)\n        else:\n            min_z = None\n            max_z = None\n        for i in indices:\n            for j in indices:\n                if j &gt; i:\n                    if filename is not None:\n                        filename_full = filename + \"_contour_\" + str(i) + \"_\" + str(j) + \".png\"\n                    else:\n                        filename_full = None\n                    self.plot_contour(\n                        i=i,\n                        j=j,\n                        min_z=min_z,\n                        max_z=max_z,\n                        filename=filename_full,\n                        show=show,\n                        title=title,\n                        n_grid=n_grid,\n                        contour_levels=contour_levels,\n                        dpi=dpi,\n                        use_max=use_max,\n                        use_min=use_min,\n                        tkagg=tkagg,\n                    )\n\n    def get_importance(self) -&gt; list:\n        \"\"\"Get importance of each variable and return the results as a list.\n\n        Returns:\n            output (list):\n                list of results. If the surrogate has more than one theta values,\n                the importance is calculated. Otherwise, a list of zeros is returned.\n\n        \"\"\"\n        if self.surrogate.n_theta &gt; 1 and self.var_name is not None:\n            output = [0] * len(self.all_var_name)\n            theta = np.power(10, self.surrogate.theta)\n            imp = 100 * theta / np.max(theta)\n            ind = find_indices(A=self.var_name, B=self.all_var_name)\n            j = 0\n            for i in ind:\n                output[i] = imp[j]\n                j = j + 1\n            return output\n        else:\n            print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n            # return a list of zeros of length len(all_var_name)\n            return [0] * len(self.all_var_name)\n\n    def print_importance(self, threshold=0.1, print_screen=True) -&gt; list:\n        \"\"\"Print importance of each variable and return the results as a list.\n\n        Args:\n            threshold (float):\n                threshold for printing\n            print_screen (boolean):\n                if `True`, values are also printed on the screen. Default is `True`.\n\n        Returns:\n            output (list):\n                list of results\n        \"\"\"\n        output = []\n        if self.surrogate.n_theta &gt; 1:\n            theta = np.power(10, self.surrogate.theta)\n            imp = 100 * theta / np.max(theta)\n            # imp = imp[imp &gt;= threshold]\n            if self.var_name is None:\n                for i in range(len(imp)):\n                    if imp[i] &gt;= threshold:\n                        if print_screen:\n                            print(\"x\", i, \": \", imp[i])\n                        output.append(\"x\" + str(i) + \": \" + str(imp[i]))\n            else:\n                var_name = [self.var_name[i] for i in range(len(imp))]\n                for i in range(len(imp)):\n                    if imp[i] &gt;= threshold:\n                        if print_screen:\n                            print(var_name[i] + \": \", imp[i])\n                    output.append([var_name[i], imp[i]])\n        else:\n            print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n        return output\n\n    def plot_importance(self, threshold=0.1, filename=None, dpi=300, show=True, tkagg=False) -&gt; None:\n        \"\"\"Plot the importance of each variable.\n\n        Args:\n            threshold (float):\n                The threshold of the importance.\n            filename (str):\n                The filename of the plot.\n            dpi (int):\n                The dpi of the plot.\n            show (bool):\n                Show the plot. Default is `True`.\n\n        Returns:\n            None\n        \"\"\"\n        if self.surrogate.n_theta &gt; 1:\n            if tkagg:\n                matplotlib.use(\"TkAgg\")\n            theta = np.power(10, self.surrogate.theta)\n            imp = 100 * theta / np.max(theta)\n            idx = np.where(imp &gt; threshold)[0]\n            if self.var_name is None:\n                plt.bar(range(len(imp[idx])), imp[idx])\n                plt.xticks(range(len(imp[idx])), [\"x\" + str(i) for i in idx])\n            else:\n                var_name = [self.var_name[i] for i in idx]\n                plt.bar(range(len(imp[idx])), imp[idx])\n                plt.xticks(range(len(imp[idx])), var_name)\n            if filename is not None:\n                plt.savefig(filename, bbox_inches=\"tight\", dpi=dpi)\n            if show:\n                plt.show()\n\n    def parallel_plot(self, show=False) -&gt; go.Figure:\n        \"\"\"\n        Parallel plot.\n\n        Args:\n            self (object):\n                Spot object\n            show (bool):\n                show the plot. Default is `False`.\n\n        Returns:\n                fig (plotly.graph_objects.Figure): figure object\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 5\n                # number of points\n                fun_evals = 10\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1, -1]),\n                    upper = np.array([1, 1, 1]),\n                    fun_evals=fun_evals,\n                    tolerance_x = np.sqrt(np.spacing(1))\n                    )\n                design_control=design_control_init(init_size=ni)\n                surrogate_control=surrogate_control_init(n_theta=3)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,\n                            surrogate_control=surrogate_control,)\n                S.run()\n                S.parallel_plot()\n\n        \"\"\"\n        X = self.X\n        y = self.y\n        df = pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=self.var_name + [\"y\"])\n\n        fig = go.Figure(\n            data=go.Parcoords(\n                line=dict(color=df[\"y\"], colorscale=\"Jet\", showscale=True, cmin=min(df[\"y\"]), cmax=max(df[\"y\"])),\n                dimensions=list([dict(range=[min(df.iloc[:, i]), max(df.iloc[:, i])], label=df.columns[i], values=df.iloc[:, i]) for i in range(len(df.columns) - 1)]),\n            )\n        )\n        if show:\n            fig.show()\n        return fig\n\n    def save_experiment(self, filename=None, path=None, overwrite=True) -&gt; None:\n        \"\"\"\n        Save the experiment to a file.\n\n        Args:\n            filename (str):\n                The filename of the experiment file.\n            path (str):\n                The path to the experiment file.\n            overwrite (bool):\n                If `True`, the file will be overwritten if it already exists. Default is `True`.\n\n        Returns:\n            None\n        \"\"\"\n        # Remove or close any unpickleable objects, e.g., the spot_writer\n        self.close_and_del_spot_writer()\n\n        # Remove the logger handler before pickling\n        self.remove_logger_handlers()\n\n        # Create deep copies of control dictionaries\n        fun_control = copy.deepcopy(self.fun_control)\n        optimizer_control = copy.deepcopy(self.optimizer_control)\n        surrogate_control = copy.deepcopy(self.surrogate_control)\n        design_control = copy.deepcopy(self.design_control)\n\n        # Deep copy the spot object itself (except unpickleable components)\n        try:\n            spot_tuner = copy.deepcopy(self)\n        except Exception as e:\n            print(\"Warning: Could not copy spot_tuner object!\")\n            print(f\"Error: {e}\")\n            spot_tuner = self\n\n        # Prepare the experiment dictionary\n        experiment = {\n            \"design_control\": design_control,\n            \"fun_control\": fun_control,\n            \"optimizer_control\": optimizer_control,\n            \"spot_tuner\": spot_tuner,\n            \"surrogate_control\": surrogate_control,\n        }\n\n        # Determine the filename based on PREFIX if not provided\n        PREFIX = fun_control.get(\"PREFIX\")\n        if filename is None and PREFIX is not None:\n            filename = get_experiment_filename(PREFIX)\n\n        if path is not None:\n            filename = os.path.join(path, filename)\n            if not os.path.exists(path):\n                os.makedirs(path)\n\n        # Check if the file already exists\n        if filename is not None and os.path.exists(filename) and not overwrite:\n            print(f\"Error: File {filename} already exists. Use overwrite=True to overwrite the file.\")\n            return\n\n        # Serialize the experiment dictionary to the pickle file\n        if filename is not None:\n            with open(filename, \"wb\") as handle:\n                try:\n                    pickle.dump(experiment, handle, protocol=pickle.HIGHEST_PROTOCOL)\n                except Exception as e:\n                    print(f\"Error: {e}\")\n                    raise e\n            print(f\"Experiment saved to {filename}\")\n\n    def remove_logger_handlers(self) -&gt; None:\n        \"\"\"\n        Remove handlers from the logger to avoid pickling issues.\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        for handler in logger.handlers[:]:  # Copy the list to avoid modification during iteration\n            logger.removeHandler(handler)\n\n    def reattach_logger_handlers(self) -&gt; None:\n        \"\"\"\n        Reattach handlers to the logger after unpickling.\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        # configure the handler and formatter as needed\n        py_handler = logging.FileHandler(f\"{__name__}.log\", mode=\"w\")\n        py_formatter = logging.Formatter(\"%(name)s %(asctime)s %(levelname)s %(message)s\")\n        # add formatter to the handler\n        py_handler.setFormatter(py_formatter)\n        # add handler to the logger\n        logger.addHandler(py_handler)\n\n    def init_spot_writer(self) -&gt; None:\n        \"\"\"\n        Initialize the spot_writer for the current experiment if in fun_control\n        the tensorboard_log is set to True\n        and the spot_tensorboard_path is not None.\n        Otherwise, the spot_writer is set to None.\n        \"\"\"\n        if self.fun_control[\"tensorboard_log\"] and self.fun_control[\"spot_tensorboard_path\"] is not None:\n            self.spot_writer = SummaryWriter(log_dir=self.fun_control[\"spot_tensorboard_path\"])\n        else:\n            self.spot_writer = None\n\n    def close_and_del_spot_writer(self) -&gt; None:\n        \"\"\"\n        Delete the spot_writer attribute from the object\n        if it exists and close the writer.\n        \"\"\"\n        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n            self.spot_writer.flush()\n            self.spot_writer.close()\n            del self.spot_writer\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.chg","title":"<code>chg(x, y, z0, i, j)</code>","text":"<p>Change the values of elements at indices <code>i</code> and <code>j</code> in the array <code>z0</code> to <code>x</code> and <code>y</code>, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int or float</code> <p>The new value for the element at index <code>i</code>.</p> required <code>y</code> <code>int or float</code> <p>The new value for the element at index <code>j</code>.</p> required <code>z0</code> <code>list or ndarray</code> <p>The array to be modified.</p> required <code>i</code> <code>int</code> <p>The index of the element to be changed to <code>x</code>.</p> required <code>j</code> <code>int</code> <p>The index of the element to be changed to <code>y</code>.</p> required <p>Returns:</p> Type Description <code>list) or (numpy.ndarray</code> <p>The modified array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n    )\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1]),\n        upper = np.array([1]),\n    )\n    S = spot.Spot(fun=fun,\n                func_control=fun_control)\n    z0 = [1, 2, 3]\n    print(f\"Before: {z0}\")\n    new_val_1 = 4\n    new_val_2 = 5\n    index_1 = 0\n    index_2 = 2\n    S.chg(x=new_val_1, y=new_val_2, z0=z0, i=index_1, j=index_2)\n    print(f\"After: {z0}\")\n    Before: [1, 2, 3]\n    After: [4, 2, 5]\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def chg(self, x, y, z0, i, j) -&gt; list:\n    \"\"\"\n    Change the values of elements at indices `i` and `j` in the array `z0` to `x` and `y`, respectively.\n\n    Args:\n        x (int or float):\n            The new value for the element at index `i`.\n        y (int or float):\n            The new value for the element at index `j`.\n        z0 (list or numpy.ndarray):\n            The array to be modified.\n        i (int):\n            The index of the element to be changed to `x`.\n        j (int):\n            The index of the element to be changed to `y`.\n\n    Returns:\n        (list) or (numpy.ndarray): The modified array.\n\n    Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1]),\n                    upper = np.array([1]),\n                )\n                S = spot.Spot(fun=fun,\n                            func_control=fun_control)\n                z0 = [1, 2, 3]\n                print(f\"Before: {z0}\")\n                new_val_1 = 4\n                new_val_2 = 5\n                index_1 = 0\n                index_2 = 2\n                S.chg(x=new_val_1, y=new_val_2, z0=z0, i=index_1, j=index_2)\n                print(f\"After: {z0}\")\n                Before: [1, 2, 3]\n                After: [4, 2, 5]\n    \"\"\"\n    z0[i] = x\n    z0[j] = y\n    return z0\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.close_and_del_spot_writer","title":"<code>close_and_del_spot_writer()</code>","text":"<p>Delete the spot_writer attribute from the object if it exists and close the writer.</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def close_and_del_spot_writer(self) -&gt; None:\n    \"\"\"\n    Delete the spot_writer attribute from the object\n    if it exists and close the writer.\n    \"\"\"\n    if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n        self.spot_writer.flush()\n        self.spot_writer.close()\n        del self.spot_writer\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.de_serialize_dicts","title":"<code>de_serialize_dicts()</code>","text":"<p>Deserialize the spot object and return the dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>tuple containing dictionaries of spot object: fun_control (dict): function control dictionary, design_control (dict): design control dictionary, optimizer_control (dict): optimizer control dictionary, spot_tuner_control (dict): spot tuner control dictionary, and surrogate_control (dict): surrogate control dictionary</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def de_serialize_dicts(self) -&gt; tuple:\n    \"\"\"\n    Deserialize the spot object and return the dictionaries.\n\n    Args:\n        self (object):\n            Spot object\n\n    Returns:\n        (tuple):\n            tuple containing dictionaries of spot object:\n            fun_control (dict): function control dictionary,\n            design_control (dict): design control dictionary,\n            optimizer_control (dict): optimizer control dictionary,\n            spot_tuner_control (dict): spot tuner control dictionary, and\n            surrogate_control (dict): surrogate control dictionary\n    \"\"\"\n    spot_tuner = copy.deepcopy(self)\n    spot_tuner_control = vars(spot_tuner)\n\n    fun_control = copy.deepcopy(spot_tuner_control[\"fun_control\"])\n    design_control = copy.deepcopy(spot_tuner_control[\"design_control\"])\n    optimizer_control = copy.deepcopy(spot_tuner_control[\"optimizer_control\"])\n    surrogate_control = copy.deepcopy(spot_tuner_control[\"surrogate_control\"])\n\n    # remove keys from the dictionaries:\n    spot_tuner_control.pop(\"fun_control\", None)\n    spot_tuner_control.pop(\"design_control\", None)\n    spot_tuner_control.pop(\"optimizer_control\", None)\n    spot_tuner_control.pop(\"surrogate_control\", None)\n    spot_tuner_control.pop(\"spot_writer\", None)\n    spot_tuner_control.pop(\"design\", None)\n    spot_tuner_control.pop(\"fun\", None)\n    spot_tuner_control.pop(\"optimizer\", None)\n    spot_tuner_control.pop(\"rng\", None)\n    spot_tuner_control.pop(\"surrogate\", None)\n\n    fun_control.pop(\"core_model\", None)\n    fun_control.pop(\"metric_river\", None)\n    fun_control.pop(\"metric_sklearn\", None)\n    fun_control.pop(\"metric_torch\", None)\n    fun_control.pop(\"prep_model\", None)\n    fun_control.pop(\"spot_writer\", None)\n    fun_control.pop(\"test\", None)\n    fun_control.pop(\"train\", None)\n\n    surrogate_control.pop(\"model_optimizer\", None)\n    surrogate_control.pop(\"surrogate\", None)\n\n    return (fun_control, design_control, optimizer_control, spot_tuner_control, surrogate_control)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.fit_surrogate","title":"<code>fit_surrogate()</code>","text":"<p>Fit surrogate model. The surrogate model is fitted to the data stored in <code>self.X</code> and <code>self.y</code>. It uses the generic <code>fit()</code> method of the surrogate model <code>surrogate</code>. The default surrogate model is an instance from spotpython\u2019s <code>Kriging</code> class. Args:     self (object): Spot object</p> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Attributes:</p> Name Type Description <code>self.surrogate</code> <code>object</code> <p>surrogate model</p> Note <ul> <li>As shown in https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/ other surrogate models can be used as well.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n    )\n    # number of initial points:\n    ni = 0\n    X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1])\n        )\n    design_control=design_control_init(init_size=ni)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    S.initialize_design(X_start=X_start)\n    S.update_stats()\n    S.fit_surrogate()\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def fit_surrogate(self) -&gt; None:\n    \"\"\"\n    Fit surrogate model. The surrogate model\n    is fitted to the data stored in `self.X` and `self.y`.\n    It uses the generic `fit()` method of the\n    surrogate model `surrogate`. The default surrogate model is\n    an instance from spotpython's `Kriging` class.\n    Args:\n        self (object): Spot object\n\n    Returns:\n        (NoneType): None\n\n    Attributes:\n        self.surrogate (object): surrogate model\n\n    Note:\n        * As shown in https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/\n        other surrogate models can be used as well.\n\n    Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n                # number of initial points:\n                ni = 0\n                X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1])\n                    )\n                design_control=design_control_init(init_size=ni)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                S.initialize_design(X_start=X_start)\n                S.update_stats()\n                S.fit_surrogate()\n\n    \"\"\"\n    logger.debug(\"In fit_surrogate(): self.X: %s\", self.X)\n    logger.debug(\"In fit_surrogate(): self.y: %s\", self.y)\n    logger.debug(\"In fit_surrogate(): self.X.shape: %s\", self.X.shape)\n    logger.debug(\"In fit_surrogate(): self.y.shape: %s\", self.y.shape)\n    X_points = self.X.shape[0]\n    y_points = self.y.shape[0]\n    if X_points == y_points:\n        if X_points &gt; self.max_surrogate_points:\n            X_S, y_S = select_distant_points(X=self.X, y=self.y, k=self.max_surrogate_points)\n        else:\n            X_S = self.X\n            y_S = self.y\n        self.surrogate.fit(X_S, y_S)\n    else:\n        logger.warning(\"X and y have different sizes. Surrogate not fitted.\")\n    if self.show_models:\n        self.plot_model()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.generate_design","title":"<code>generate_design(size, repeats, lower, upper)</code>","text":"<p>Generate a design with <code>size</code> points in the interval [lower, upper].</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>number of points</p> required <code>repeats</code> <code>int</code> <p>number of repeats</p> required <code>lower</code> <code>ndarray</code> <p>lower bound of the design space</p> required <code>upper</code> <code>ndarray</code> <p>upper bound of the design space</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>design points</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.spot import spot\n    from spotpython.utils.init import design_control_init\n    from spotpython.fun.objectivefunctions import analytical\n    design_control = design_control_init(init_size=3)\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1]),\n        fun_evals=fun_evals,\n        tolerance_x = np.sqrt(np.spacing(1))\n        )\n    S = spot.Spot(fun = analytical().fun_sphere,\n                fun_control = fun_control,\n                design_control = design_control)\n    X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n    assert X.shape[0] == 3\n    assert X.shape[1] == 2\n    print(X)\n        array([[77.25493789,  0.31539299],\n        [59.32133757,  0.93854273],\n        [27.4698033 ,  0.3959685 ]])\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def generate_design(self, size, repeats, lower, upper) -&gt; np.array:\n    \"\"\"Generate a design with `size` points in the interval [lower, upper].\n\n    Args:\n        size (int): number of points\n        repeats (int): number of repeats\n        lower (numpy.ndarray): lower bound of the design space\n        upper (numpy.ndarray): upper bound of the design space\n\n    Returns:\n        (numpy.ndarray): design points\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.spot import spot\n            from spotpython.utils.init import design_control_init\n            from spotpython.fun.objectivefunctions import analytical\n            design_control = design_control_init(init_size=3)\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1]),\n                fun_evals=fun_evals,\n                tolerance_x = np.sqrt(np.spacing(1))\n                )\n            S = spot.Spot(fun = analytical().fun_sphere,\n                        fun_control = fun_control,\n                        design_control = design_control)\n            X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n            assert X.shape[0] == 3\n            assert X.shape[1] == 2\n            print(X)\n                array([[77.25493789,  0.31539299],\n                [59.32133757,  0.93854273],\n                [27.4698033 ,  0.3959685 ]])\n    \"\"\"\n    return self.design.scipy_lhd(n=size, repeats=repeats, lower=lower, upper=upper)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.generate_random_point","title":"<code>generate_random_point()</code>","text":"<p>Generate a random point in the design space.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>tuple containing: X0 (numpy.ndarray): random point in the design space y0 (numpy.ndarray): function value at X</p> Notes <p>If the evaluation fails, the function returns arrays of shape[0] == 0.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1])\n        )\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                )\n    X0, y0 = S.generate_random_point()\n    print(f\"X0: {X0}\")\n    print(f\"y0: {y0}\")\n    assert X0.size == 2\n    assert y0.size == 1\n    assert np.all(X0 &gt;= S.lower)\n    assert np.all(X0 &lt;= S.upper)\n    assert y0 &gt;= 0\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def generate_random_point(self):\n    \"\"\"Generate a random point in the design space.\n\n    Returns:\n        (tuple): tuple containing:\n            X0 (numpy.ndarray): random point in the design space\n            y0 (numpy.ndarray): function value at X\n\n    Notes:\n        If the evaluation fails, the function returns arrays of shape[0] == 0.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1])\n                )\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        )\n            X0, y0 = S.generate_random_point()\n            print(f\"X0: {X0}\")\n            print(f\"y0: {y0}\")\n            assert X0.size == 2\n            assert y0.size == 1\n            assert np.all(X0 &gt;= S.lower)\n            assert np.all(X0 &lt;= S.upper)\n            assert y0 &gt;= 0\n    \"\"\"\n    X0 = self.generate_design(\n        size=1,\n        repeats=1,\n        lower=self.lower,\n        upper=self.upper,\n    )\n    X0 = repair_non_numeric(X0, self.var_type)\n    X_all = self.to_all_dim_if_needed(X0)\n    logger.debug(\"In Spot() generate_random_point(), before calling self.fun: X_all: %s\", X_all)\n    logger.debug(\"In Spot() generate_random_point(), before calling self.fun: fun_control: %s\", self.fun_control)\n    y0 = self.fun(X=X_all, fun_control=self.fun_control)\n    X0, y0 = remove_nan(X0, y0, stop_on_zero_return=False)\n    return X0, y0\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.get_importance","title":"<code>get_importance()</code>","text":"<p>Get importance of each variable and return the results as a list.</p> <p>Returns:</p> Name Type Description <code>output</code> <code>list</code> <p>list of results. If the surrogate has more than one theta values, the importance is calculated. Otherwise, a list of zeros is returned.</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def get_importance(self) -&gt; list:\n    \"\"\"Get importance of each variable and return the results as a list.\n\n    Returns:\n        output (list):\n            list of results. If the surrogate has more than one theta values,\n            the importance is calculated. Otherwise, a list of zeros is returned.\n\n    \"\"\"\n    if self.surrogate.n_theta &gt; 1 and self.var_name is not None:\n        output = [0] * len(self.all_var_name)\n        theta = np.power(10, self.surrogate.theta)\n        imp = 100 * theta / np.max(theta)\n        ind = find_indices(A=self.var_name, B=self.all_var_name)\n        j = 0\n        for i in ind:\n            output[i] = imp[j]\n            j = j + 1\n        return output\n    else:\n        print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n        # return a list of zeros of length len(all_var_name)\n        return [0] * len(self.all_var_name)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.get_new_X0","title":"<code>get_new_X0()</code>","text":"<p>Get new design points. Calls <code>suggest_new_X()</code> and repairs the new design points, e.g., by <code>repair_non_numeric()</code> and <code>selectNew()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>new design points</p> Notes <ul> <li>self.design (object): an experimental design is used to generate new design points if no new design points are found, a new experimental design is generated.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n                   from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init\n    # number of initial points:\n    ni = 3\n    X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n                n_points=10,\n                ocba_delta=0,\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1])\n    )\n    design_control=design_control_init(init_size=ni)\n    S = spot.Spot(fun=fun,\n                    fun_control=fun_control\n                    design_control=design_control,\n    )\n    S.initialize_design(X_start=X_start)\n    S.update_stats()\n    S.fit_surrogate()\n    X_ocba = None\n    X0 = S.get_new_X0()\n    assert X0.shape[0] == S.n_points\n    assert X0.shape[1] == S.lower.size\n    # assert new points are in the interval [lower, upper]\n    assert np.all(X0 &gt;= S.lower)\n    assert np.all(X0 &lt;= S.upper)\n    # print using 20 digits precision\n    np.set_printoptions(precision=20)\n    print(f\"X0: {X0}\")\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def get_new_X0(self) -&gt; np.array:\n    \"\"\"\n    Get new design points.\n    Calls `suggest_new_X()` and repairs the new design points, e.g.,\n    by `repair_non_numeric()` and `selectNew()`.\n\n    Args:\n        self (object): Spot object\n\n    Returns:\n        (numpy.ndarray): new design points\n\n    Notes:\n        * self.design (object): an experimental design is used to generate new design points\n        if no new design points are found, a new experimental design is generated.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n                           from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init\n            # number of initial points:\n            ni = 3\n            X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                        n_points=10,\n                        ocba_delta=0,\n                        lower = np.array([-1, -1]),\n                        upper = np.array([1, 1])\n            )\n            design_control=design_control_init(init_size=ni)\n            S = spot.Spot(fun=fun,\n                            fun_control=fun_control\n                            design_control=design_control,\n            )\n            S.initialize_design(X_start=X_start)\n            S.update_stats()\n            S.fit_surrogate()\n            X_ocba = None\n            X0 = S.get_new_X0()\n            assert X0.shape[0] == S.n_points\n            assert X0.shape[1] == S.lower.size\n            # assert new points are in the interval [lower, upper]\n            assert np.all(X0 &gt;= S.lower)\n            assert np.all(X0 &lt;= S.upper)\n            # print using 20 digits precision\n            np.set_printoptions(precision=20)\n            print(f\"X0: {X0}\")\n\n    \"\"\"\n    # Try to generate self.fun_repeats new X0 points:\n    X0 = self.suggest_new_X()\n    X0 = repair_non_numeric(X0, self.var_type)\n    # (S-16) Duplicate Handling:\n    # Condition: select only X= that have min distance\n    # to existing solutions\n    X0, X0_ind = selectNew(A=X0, X=self.X, tolerance=self.tolerance_x)\n    if X0.shape[0] &gt; 0:\n        # 1. There are X0 that fullfil the condition.\n        # Note: The number of new X0 can be smaller than self.n_points!\n        logger.debug(\"XO values are new: %s %s\", X0_ind, X0)\n        return repeat(X0, self.fun_repeats, axis=0)\n        return X0\n    # 2. No X0 found. Then generate self.n_points new solutions:\n    else:\n        self.design = SpaceFilling(k=self.k, seed=self.fun_control[\"seed\"] + self.counter)\n        X0 = self.generate_design(size=self.n_points, repeats=self.design_control[\"repeats\"], lower=self.lower, upper=self.upper)\n        X0 = repair_non_numeric(X0, self.var_type)\n        logger.warning(\"No new XO found on surrogate. Generate new solution %s\", X0)\n        return X0\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.get_spot_attributes_as_df","title":"<code>get_spot_attributes_as_df()</code>","text":"<p>Get all attributes of the spot object as a pandas dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dataframe with all attributes of the spot object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from math import inf\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n                    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 7\n    # number of points\n    n = 10\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1]),\n        upper = np.array([1])\n        fun_evals=n)\n    design_control=design_control_init(init_size=ni)\n    spot_1 = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    spot_1.run()\n    df = spot_1.get_spot_attributes_as_df()\n    df\n        Attribute Name                                    Attribute Value\n    0                   X  [[-0.3378148180708981], [0.698908280342222], [...\n    1           all_lower                                               [-1]\n    2           all_upper                                                [1]\n    3        all_var_name                                               [x0]\n    4        all_var_type                                              [num]\n    5             counter                                                 10\n    6           de_bounds                                          [[-1, 1]]\n    7              design  &lt;spotpython.design.spacefilling.SpaceFilling o...\n    8      design_control                     {'init_size': 7, 'repeats': 1}\n    9                 eps                                                0.0\n    10        fun_control                         {'sigma': 0, 'seed': None}\n    11          fun_evals                                                 10\n    12        fun_repeats                                                  1\n    13              ident                                            [False]\n    14   infill_criterion                                                  y\n    15                  k                                                  1\n    16          log_level                                                 50\n    17              lower                                               [-1]\n    18           max_time                                                inf\n    19             mean_X                                               None\n    20             mean_y                                               None\n    21              min_X                           [1.5392206722432657e-05]\n    22         min_mean_X                                               None\n    23         min_mean_y                                               None\n    24              min_y                                                0.0\n    25           n_points                                                  1\n    26              noise                                              False\n    27         ocba_delta                                                  0\n    28  optimizer_control                    {'max_iter': 1000, 'seed': 125}\n    29            red_dim                                              False\n    30                rng                                   Generator(PCG64)\n    31               seed                                                123\n    32        show_models                                              False\n    33      show_progress                                               True\n    34        spot_writer                                               None\n    35          surrogate  &lt;spotpython.build.kriging.Kriging object at 0x...\n    36  surrogate_control  {'noise': False, 'model_optimizer': &lt;function ...\n    37        tolerance_x                                                  0\n    38              upper                                                [1]\n    39           var_name                                               [x0]\n    40           var_type                                              [num]\n    41              var_y                                               None\n    42                  y  [0.11411885130827397, 0.48847278433092195, 0.0...\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def get_spot_attributes_as_df(self) -&gt; pd.DataFrame:\n    \"\"\"Get all attributes of the spot object as a pandas dataframe.\n\n    Returns:\n        (pandas.DataFrame): dataframe with all attributes of the spot object.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from math import inf\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n                            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 7\n            # number of points\n            n = 10\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1]),\n                upper = np.array([1])\n                fun_evals=n)\n            design_control=design_control_init(init_size=ni)\n            spot_1 = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,)\n            spot_1.run()\n            df = spot_1.get_spot_attributes_as_df()\n            df\n                Attribute Name                                    Attribute Value\n            0                   X  [[-0.3378148180708981], [0.698908280342222], [...\n            1           all_lower                                               [-1]\n            2           all_upper                                                [1]\n            3        all_var_name                                               [x0]\n            4        all_var_type                                              [num]\n            5             counter                                                 10\n            6           de_bounds                                          [[-1, 1]]\n            7              design  &lt;spotpython.design.spacefilling.SpaceFilling o...\n            8      design_control                     {'init_size': 7, 'repeats': 1}\n            9                 eps                                                0.0\n            10        fun_control                         {'sigma': 0, 'seed': None}\n            11          fun_evals                                                 10\n            12        fun_repeats                                                  1\n            13              ident                                            [False]\n            14   infill_criterion                                                  y\n            15                  k                                                  1\n            16          log_level                                                 50\n            17              lower                                               [-1]\n            18           max_time                                                inf\n            19             mean_X                                               None\n            20             mean_y                                               None\n            21              min_X                           [1.5392206722432657e-05]\n            22         min_mean_X                                               None\n            23         min_mean_y                                               None\n            24              min_y                                                0.0\n            25           n_points                                                  1\n            26              noise                                              False\n            27         ocba_delta                                                  0\n            28  optimizer_control                    {'max_iter': 1000, 'seed': 125}\n            29            red_dim                                              False\n            30                rng                                   Generator(PCG64)\n            31               seed                                                123\n            32        show_models                                              False\n            33      show_progress                                               True\n            34        spot_writer                                               None\n            35          surrogate  &lt;spotpython.build.kriging.Kriging object at 0x...\n            36  surrogate_control  {'noise': False, 'model_optimizer': &lt;function ...\n            37        tolerance_x                                                  0\n            38              upper                                                [1]\n            39           var_name                                               [x0]\n            40           var_type                                              [num]\n            41              var_y                                               None\n            42                  y  [0.11411885130827397, 0.48847278433092195, 0.0...\n\n    \"\"\"\n\n    attributes = [attr for attr in dir(self) if not callable(getattr(self, attr)) and not attr.startswith(\"__\")]\n    values = [getattr(self, attr) for attr in attributes]\n    df = pd.DataFrame({\"Attribute Name\": attributes, \"Attribute Value\": values})\n    return df\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.get_tuned_hyperparameters","title":"<code>get_tuned_hyperparameters(fun_control=None)</code>","text":"<p>Return the tuned hyperparameter values from the run. If <code>noise == True</code>, the mean values are returned.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary of tuned hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.device import getDevice\n    from math import inf\n    from spotpython.utils.init import fun_control_init\n    import numpy as np\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.data.diabetes import Diabetes\n    MAX_TIME = 1\n    FUN_EVALS = 10\n    INIT_SIZE = 5\n    WORKERS = 0\n    PREFIX=\"037\"\n    DEVICE = getDevice()\n    DEVICES = 1\n    TEST_SIZE = 0.4\n    TORCH_METRIC = \"mean_squared_error\"\n    dataset = Diabetes()\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        _torchmetric=TORCH_METRIC,\n        PREFIX=PREFIX,\n        TENSORBOARD_CLEAN=True,\n        data_set=dataset,\n        device=DEVICE,\n        enable_progress_bar=False,\n        fun_evals=FUN_EVALS,\n        log_level=50,\n        max_time=MAX_TIME,\n        num_workers=WORKERS,\n        show_progress=True,\n        test_size=TEST_SIZE,\n        tolerance_x=np.sqrt(np.spacing(1)),\n        )\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                hyper_dict=LightHyperDict)\n    from spotpython.hyperparameters.values import set_control_hyperparameter_value\n    set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n    set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n    set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n    set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                    \"Adam\",\n                    \"RAdam\",\n                ])\n    set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n    set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n    set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n    set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                    \"ReLU\",\n                    \"LeakyReLU\"\n                ] )\n    from spotpython.utils.init import design_control_init, surrogate_control_init\n    design_control = design_control_init(init_size=INIT_SIZE)\n    surrogate_control = surrogate_control_init(noise=True,\n                                                n_theta=2)\n    from spotpython.fun.hyperlight import HyperLight\n    fun = HyperLight(log_level=50).fun\n    from spotpython.spot import spot\n    spot_tuner = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control)\n    spot_tuner.run()\n    spot_tuner.get_tuned_hyperparameters()\n        {'l1': 7.0,\n        'epochs': 5.0,\n        'batch_size': 4.0,\n        'act_fn': 0.0,\n        'optimizer': 0.0,\n        'dropout_prob': 0.01,\n        'lr_mult': 5.0,\n        'patience': 3.0,\n        'initialization': 1.0}\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def get_tuned_hyperparameters(self, fun_control=None) -&gt; dict:\n    \"\"\"Return the tuned hyperparameter values from the run.\n    If `noise == True`, the mean values are returned.\n\n    Args:\n        fun_control (dict, optional):\n            fun_control dictionary\n\n    Returns:\n        (dict): dictionary of tuned hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.device import getDevice\n            from math import inf\n            from spotpython.utils.init import fun_control_init\n            import numpy as np\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.data.diabetes import Diabetes\n            MAX_TIME = 1\n            FUN_EVALS = 10\n            INIT_SIZE = 5\n            WORKERS = 0\n            PREFIX=\"037\"\n            DEVICE = getDevice()\n            DEVICES = 1\n            TEST_SIZE = 0.4\n            TORCH_METRIC = \"mean_squared_error\"\n            dataset = Diabetes()\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                _torchmetric=TORCH_METRIC,\n                PREFIX=PREFIX,\n                TENSORBOARD_CLEAN=True,\n                data_set=dataset,\n                device=DEVICE,\n                enable_progress_bar=False,\n                fun_evals=FUN_EVALS,\n                log_level=50,\n                max_time=MAX_TIME,\n                num_workers=WORKERS,\n                show_progress=True,\n                test_size=TEST_SIZE,\n                tolerance_x=np.sqrt(np.spacing(1)),\n                )\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        hyper_dict=LightHyperDict)\n            from spotpython.hyperparameters.values import set_control_hyperparameter_value\n            set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n            set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n            set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n            set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                            \"Adam\",\n                            \"RAdam\",\n                        ])\n            set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n            set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n            set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n            set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                            \"ReLU\",\n                            \"LeakyReLU\"\n                        ] )\n            from spotpython.utils.init import design_control_init, surrogate_control_init\n            design_control = design_control_init(init_size=INIT_SIZE)\n            surrogate_control = surrogate_control_init(noise=True,\n                                                        n_theta=2)\n            from spotpython.fun.hyperlight import HyperLight\n            fun = HyperLight(log_level=50).fun\n            from spotpython.spot import spot\n            spot_tuner = spot.Spot(fun=fun,\n                                fun_control=fun_control,\n                                design_control=design_control,\n                                surrogate_control=surrogate_control)\n            spot_tuner.run()\n            spot_tuner.get_tuned_hyperparameters()\n                {'l1': 7.0,\n                'epochs': 5.0,\n                'batch_size': 4.0,\n                'act_fn': 0.0,\n                'optimizer': 0.0,\n                'dropout_prob': 0.01,\n                'lr_mult': 5.0,\n                'patience': 3.0,\n                'initialization': 1.0}\n\n    \"\"\"\n    output = []\n    if self.noise:\n        res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n    else:\n        res = self.to_all_dim(self.min_X.reshape(1, -1))\n    for i in range(res.shape[1]):\n        if self.all_var_name is None:\n            var_name = \"x\" + str(i)\n        else:\n            var_name = self.all_var_name[i]\n            var_type = self.all_var_type[i]\n            if var_type == \"factor\" and fun_control is not None:\n                val = get_ith_hyperparameter_name_from_fun_control(fun_control=fun_control, key=var_name, i=int(res[0][i]))\n            else:\n                val = res[0][i]\n        output.append([var_name, val])\n    # convert list to a dictionary\n    output = dict(output)\n    return output\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.infill","title":"<code>infill(x)</code>","text":"<p>Infill (acquisition) function. Evaluates one point on the surrogate via <code>surrogate.predict(x.reshape(1,-1))</code>, if <code>sklearn</code> surrogates are used or <code>surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)</code> if the internal surrogate <code>kriging</code> is selected. This method is passed to the optimizer in <code>suggest_new_X</code>, i.e., the optimizer is called via <code>self.optimizer(func=self.infill)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>point in natural units with shape <code>(1, dim)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>value based on infill criterion, e.g., <code>\"ei\"</code>. Shape <code>(1,)</code>. The objective function value <code>y</code> that is used as a base value for the infill criterion is calculated in natural units.</p> Note <p>This is step (S-12) in [bart21i].</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def infill(self, x) -&gt; float:\n    \"\"\"\n    Infill (acquisition) function. Evaluates one point on the surrogate via `surrogate.predict(x.reshape(1,-1))`,\n    if `sklearn` surrogates are used or `surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)`\n    if the internal surrogate `kriging` is selected.\n    This method is passed to the optimizer in `suggest_new_X`, i.e., the optimizer is called via\n    `self.optimizer(func=self.infill)`.\n\n    Args:\n        x (array): point in natural units with shape `(1, dim)`.\n\n    Returns:\n        (numpy.ndarray): value based on infill criterion, e.g., `\"ei\"`. Shape `(1,)`.\n            The objective function value `y` that is used as a base value for the\n            infill criterion is calculated in natural units.\n\n    Note:\n        This is step (S-12) in [bart21i].\n    \"\"\"\n    # Reshape x to have shape (1, -1) because the predict method expects a 2D array\n    X = x.reshape(1, -1)\n    if isinstance(self.surrogate, Kriging):\n        return self.surrogate.predict(X, return_val=self.infill_criterion)\n    else:\n        return self.surrogate.predict(X)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.init_spot_writer","title":"<code>init_spot_writer()</code>","text":"<p>Initialize the spot_writer for the current experiment if in fun_control the tensorboard_log is set to True and the spot_tensorboard_path is not None. Otherwise, the spot_writer is set to None.</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def init_spot_writer(self) -&gt; None:\n    \"\"\"\n    Initialize the spot_writer for the current experiment if in fun_control\n    the tensorboard_log is set to True\n    and the spot_tensorboard_path is not None.\n    Otherwise, the spot_writer is set to None.\n    \"\"\"\n    if self.fun_control[\"tensorboard_log\"] and self.fun_control[\"spot_tensorboard_path\"] is not None:\n        self.spot_writer = SummaryWriter(log_dir=self.fun_control[\"spot_tensorboard_path\"])\n    else:\n        self.spot_writer = None\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.initialize_design","title":"<code>initialize_design(X_start=None)</code>","text":"<p>Initialize design. Generate and evaluate initial design. If <code>X_start</code> is not <code>None</code>, append it to the initial design. Therefore, the design size is <code>init_size</code> + <code>X_start.shape[0]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <code>X_start</code> <code>ndarray</code> <p>initial design. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Attributes:</p> Name Type Description <code>self.X</code> <code>ndarray</code> <p>initial design</p> <code>self.y</code> <code>ndarray</code> <p>initial design values</p> Note <ul> <li>If <code>X_start</code> is has the wrong shape, it is ignored.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 7\n    # start point X_0\n    X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1]))\n    design_control=design_control_init(init_size=ni)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    S.initialize_design(X_start=X_start)\n    print(f\"S.X: {S.X}\")\n    print(f\"S.y: {S.y}\")\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def initialize_design(self, X_start=None) -&gt; None:\n    \"\"\"\n    Initialize design. Generate and evaluate initial design.\n    If `X_start` is not `None`, append it to the initial design.\n    Therefore, the design size is `init_size` + `X_start.shape[0]`.\n\n    Args:\n        self (object): Spot object\n        X_start (numpy.ndarray, optional): initial design. Defaults to None.\n\n    Returns:\n        (NoneType): None\n\n    Attributes:\n        self.X (numpy.ndarray): initial design\n        self.y (numpy.ndarray): initial design values\n\n    Note:\n        * If `X_start` is has the wrong shape, it is ignored.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 7\n            # start point X_0\n            X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1]))\n            design_control=design_control_init(init_size=ni)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,)\n            S.initialize_design(X_start=X_start)\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n    \"\"\"\n    if self.design_control[\"init_size\"] &gt; 0:\n        X0 = self.generate_design(\n            size=self.design_control[\"init_size\"],\n            repeats=self.design_control[\"repeats\"],\n            lower=self.lower,\n            upper=self.upper,\n        )\n    if X_start is not None:\n        if not isinstance(X_start, np.ndarray):\n            X_start = np.array(X_start)\n        X_start = np.atleast_2d(X_start)\n        try:\n            if self.design_control[\"init_size\"] &gt; 0:\n                X0 = append(X_start, X0, axis=0)\n            else:\n                X0 = X_start\n        except ValueError:\n            logger.warning(\"X_start has wrong shape. Ignoring it.\")\n    if X0.shape[0] == 0:\n        raise Exception(\"X0 has zero rows. Check design_control['init_size'] or X_start.\")\n    X0 = repair_non_numeric(X0, self.var_type)\n    self.X = X0\n    # (S-3): Eval initial design:\n    X_all = self.to_all_dim_if_needed(X0)\n    logger.debug(\"In Spot() initialize_design(), before calling self.fun: X_all: %s\", X_all)\n    logger.debug(\"In Spot() initialize_design(), before calling self.fun: fun_control: %s\", self.fun_control)\n    self.y = self.fun(X=X_all, fun_control=self.fun_control)\n    logger.debug(\"In Spot() initialize_design(), after calling self.fun: self.y: %s\", self.y)\n    # TODO: Error if only nan values are returned\n    logger.debug(\"New y value: %s\", self.y)\n    #\n    self.counter = self.y.size\n    if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n        # range goes to init_size -1 because the last value is added by update_stats(),\n        # which always adds the last value.\n        # Changed in 0.5.9:\n        for j in range(len(self.y)):\n            X_j = self.X[j].copy()\n            y_j = self.y[j].copy()\n            config = {self.var_name[i]: X_j[i] for i in range(self.k)}\n            # var_dict = assign_values(X, get_var_name(fun_control))\n            # config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n            # see: https://github.com/pytorch/pytorch/issues/32651\n            # self.spot_writer.add_hparams(config, {\"spot_y\": y_j}, run_name=self.spot_tensorboard_path)\n            self.spot_writer.add_hparams(config, {\"hp_metric\": y_j})\n            self.spot_writer.flush()\n    #\n    self.X, self.y = remove_nan(self.X, self.y, stop_on_zero_return=True)\n    logger.debug(\"In Spot() initialize_design(), final X val, after remove nan: self.X: %s\", self.X)\n    logger.debug(\"In Spot() initialize_design(), final y val, after remove nan: self.y: %s\", self.y)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.parallel_plot","title":"<code>parallel_plot(show=False)</code>","text":"<p>Parallel plot.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <code>show</code> <code>bool</code> <p>show the plot. Default is <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>figure object</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 5\n    # number of points\n    fun_evals = 10\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1, -1]),\n        upper = np.array([1, 1, 1]),\n        fun_evals=fun_evals,\n        tolerance_x = np.sqrt(np.spacing(1))\n        )\n    design_control=design_control_init(init_size=ni)\n    surrogate_control=surrogate_control_init(n_theta=3)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,\n                surrogate_control=surrogate_control,)\n    S.run()\n    S.parallel_plot()\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def parallel_plot(self, show=False) -&gt; go.Figure:\n    \"\"\"\n    Parallel plot.\n\n    Args:\n        self (object):\n            Spot object\n        show (bool):\n            show the plot. Default is `False`.\n\n    Returns:\n            fig (plotly.graph_objects.Figure): figure object\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 5\n            # number of points\n            fun_evals = 10\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1, -1]),\n                upper = np.array([1, 1, 1]),\n                fun_evals=fun_evals,\n                tolerance_x = np.sqrt(np.spacing(1))\n                )\n            design_control=design_control_init(init_size=ni)\n            surrogate_control=surrogate_control_init(n_theta=3)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control,)\n            S.run()\n            S.parallel_plot()\n\n    \"\"\"\n    X = self.X\n    y = self.y\n    df = pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=self.var_name + [\"y\"])\n\n    fig = go.Figure(\n        data=go.Parcoords(\n            line=dict(color=df[\"y\"], colorscale=\"Jet\", showscale=True, cmin=min(df[\"y\"]), cmax=max(df[\"y\"])),\n            dimensions=list([dict(range=[min(df.iloc[:, i]), max(df.iloc[:, i])], label=df.columns[i], values=df.iloc[:, i]) for i in range(len(df.columns) - 1)]),\n        )\n    )\n    if show:\n        fig.show()\n    return fig\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.plot_contour","title":"<code>plot_contour(i=0, j=1, min_z=None, max_z=None, show=True, filename=None, n_grid=50, contour_levels=10, dpi=200, title='', figsize=(12, 6), use_min=False, use_max=True, tkagg=False)</code>","text":"<p>Plot the contour of any dimension.</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def plot_contour(\n    self,\n    i=0,\n    j=1,\n    min_z=None,\n    max_z=None,\n    show=True,\n    filename=None,\n    n_grid=50,\n    contour_levels=10,\n    dpi=200,\n    title=\"\",\n    figsize=(12, 6),\n    use_min=False,\n    use_max=True,\n    tkagg=False,\n) -&gt; None:\n    \"\"\"Plot the contour of any dimension.\"\"\"\n\n    def generate_mesh_grid(lower, upper, grid_points):\n        \"\"\"Generate a mesh grid for the given range.\"\"\"\n        x = np.linspace(lower[i], upper[i], num=grid_points)\n        y = np.linspace(lower[j], upper[j], num=grid_points)\n        return np.meshgrid(x, y), x, y\n\n    def validate_types(var_type, lower, upper):\n        \"\"\"Validate if the dimensions of var_type, lower, and upper are the same.\"\"\"\n        if var_type is not None:\n            if len(var_type) != len(lower) or len(var_type) != len(upper):\n                raise ValueError(\"The dimensions of var_type, lower, and upper must be the same.\")\n\n    def setup_plot():\n        \"\"\"Setup the plot with specified figure size.\"\"\"\n        fig = pylab.figure(figsize=figsize)\n        return fig\n\n    def predict_contour_values(X, Y, z0):\n        \"\"\"Predict contour values based on the surrogate model.\"\"\"\n        grid_points = np.c_[np.ravel(X), np.ravel(Y)]\n        predictions = []\n\n        for x, y in grid_points:\n            adjusted_z0 = self.chg(x, y, z0.copy(), i, j)\n            prediction = self.surrogate.predict(np.array([adjusted_z0]))\n            predictions.append(prediction[0])\n\n        Z = np.array(predictions).reshape(X.shape)\n        return Z\n\n    def plot_contour_subplots(X, Y, Z, ax, min_z, max_z, contour_levels):\n        \"\"\"Plot the contour and 3D surface subplots.\"\"\"\n        contour = ax.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\", vmin=min_z, vmax=max_z)\n        pylab.colorbar(contour, ax=ax)\n\n    if tkagg:\n        matplotlib.use(\"TkAgg\")\n    fig = setup_plot()\n\n    (X, Y), x, y = generate_mesh_grid(self.lower, self.upper, n_grid)\n    validate_types(self.var_type, self.lower, self.upper)\n\n    z00 = np.array([self.lower, self.upper])\n    Z_list, X_list, Y_list = [], [], []\n\n    if use_min:\n        z0_min = self.process_z00(z00, use_min=True)\n        Z_min = predict_contour_values(X, Y, z0_min)\n        Z_list.append(Z_min)\n        X_list.append(X)\n        Y_list.append(Y)\n\n    if use_max:\n        z0_max = self.process_z00(z00, use_min=False)\n        Z_max = predict_contour_values(X, Y, z0_max)\n        Z_list.append(Z_max)\n        X_list.append(X)\n        Y_list.append(Y)\n\n    if Z_list:  # Ensure that there is at least one Z to stack\n        Z_combined = np.vstack(Z_list)\n        X_combined = np.vstack(X_list)\n        Y_combined = np.vstack(Y_list)\n\n    if min_z is None:\n        min_z = np.min(Z_combined)\n    if max_z is None:\n        max_z = np.max(Z_combined)\n\n    ax_contour = fig.add_subplot(221)\n    plot_contour_subplots(X_combined, Y_combined, Z_combined, ax_contour, min_z, max_z, contour_levels)\n\n    if self.var_name is None:\n        ax_contour.set_xlabel(f\"x{i}\")\n        ax_contour.set_ylabel(f\"x{j}\")\n    else:\n        ax_contour.set_xlabel(f\"x{i}: {self.var_name[i]}\")\n        ax_contour.set_ylabel(f\"x{j}: {self.var_name[j]}\")\n\n    ax_3d = fig.add_subplot(222, projection=\"3d\")\n    ax_3d.plot_surface(X_combined, Y_combined, Z_combined, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\", vmin=min_z, vmax=max_z)\n\n    if self.var_name is None:\n        ax_3d.set_xlabel(f\"x{i}\")\n        ax_3d.set_ylabel(f\"x{j}\")\n    else:\n        ax_3d.set_xlabel(f\"x{i}: {self.var_name[i]}\")\n        ax_3d.set_ylabel(f\"x{j}: {self.var_name[j]}\")\n\n    plt.title(title)\n\n    if filename:\n        pylab.savefig(filename, bbox_inches=\"tight\", dpi=dpi, pad_inches=0)\n\n    if show:\n        pylab.show()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.plot_importance","title":"<code>plot_importance(threshold=0.1, filename=None, dpi=300, show=True, tkagg=False)</code>","text":"<p>Plot the importance of each variable.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold of the importance.</p> <code>0.1</code> <code>filename</code> <code>str</code> <p>The filename of the plot.</p> <code>None</code> <code>dpi</code> <code>int</code> <p>The dpi of the plot.</p> <code>300</code> <code>show</code> <code>bool</code> <p>Show the plot. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def plot_importance(self, threshold=0.1, filename=None, dpi=300, show=True, tkagg=False) -&gt; None:\n    \"\"\"Plot the importance of each variable.\n\n    Args:\n        threshold (float):\n            The threshold of the importance.\n        filename (str):\n            The filename of the plot.\n        dpi (int):\n            The dpi of the plot.\n        show (bool):\n            Show the plot. Default is `True`.\n\n    Returns:\n        None\n    \"\"\"\n    if self.surrogate.n_theta &gt; 1:\n        if tkagg:\n            matplotlib.use(\"TkAgg\")\n        theta = np.power(10, self.surrogate.theta)\n        imp = 100 * theta / np.max(theta)\n        idx = np.where(imp &gt; threshold)[0]\n        if self.var_name is None:\n            plt.bar(range(len(imp[idx])), imp[idx])\n            plt.xticks(range(len(imp[idx])), [\"x\" + str(i) for i in idx])\n        else:\n            var_name = [self.var_name[i] for i in idx]\n            plt.bar(range(len(imp[idx])), imp[idx])\n            plt.xticks(range(len(imp[idx])), var_name)\n        if filename is not None:\n            plt.savefig(filename, bbox_inches=\"tight\", dpi=dpi)\n        if show:\n            plt.show()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.plot_important_hyperparameter_contour","title":"<code>plot_important_hyperparameter_contour(threshold=0.0, filename=None, show=True, max_imp=None, title='', scale_global=False, n_grid=50, contour_levels=10, dpi=200, use_min=False, use_max=True, tkagg=False)</code>","text":"<p>Plot the contour of important hyperparameters. Calls <code>plot_contour</code> for each pair of important hyperparameters. Importance can be specified by the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>threshold for the importance. Not used any more in spotpython &gt;= 0.13.2.</p> <code>0.0</code> <code>filename</code> <code>str</code> <p>filename of the plot</p> <code>None</code> <code>show</code> <code>bool</code> <p>show the plot. Default is <code>True</code>.</p> <code>True</code> <code>max_imp</code> <code>int</code> <p>maximum number of important hyperparameters. If there are more important hyperparameters than <code>max_imp</code>, only the max_imp important ones are selected.</p> <code>None</code> <code>title</code> <code>str</code> <p>title of the plots</p> <code>''</code> <code>scale_global</code> <code>bool</code> <p>scale the z-axis globally. Default is <code>False</code>.</p> <code>False</code> <code>n_grid</code> <code>int</code> <p>number of grid points. Default is 50.</p> <code>50</code> <code>contour_levels</code> <code>int</code> <p>number of contour levels. Default is 10.</p> <code>10</code> <code>dpi</code> <code>int</code> <p>dpi of the plot. Default is 200.</p> <code>200</code> <code>use_min</code> <code>bool</code> <p>Use the minimum value for determing the hidden dimensions in the plot for categorical and integer parameters. In 3d-plots, only two variables can be independent. The remaining input variables are set to their minimum value. Default is <code>False</code>. If use_min and use_max are both <code>True</code>, both values are used.</p> <code>False</code> <code>use_max</code> <code>bool</code> <p>Use the minimum value for determing the hidden dimensions in the plot for categorical and integer parameters. In 3d-plots, only two variables can be independent. The remaining input variables are set to their minimum value. Default is <code>True</code>. If use_min and use_max are both <code>True</code>, both values are used.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 5\n    # number of points\n    fun_evals = 10\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1, -1]),\n        upper = np.array([1, 1, 1]),\n        fun_evals=fun_evals,\n        tolerance_x = np.sqrt(np.spacing(1))\n        )\n    design_control=design_control_init(init_size=ni)\n    surrogate_control=surrogate_control_init(n_theta=3)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,\n                surrogate_control=surrogate_control,)\n    S.run()\n    S.plot_important_hyperparameter_contour()\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def plot_important_hyperparameter_contour(\n    self,\n    threshold=0.0,\n    filename=None,\n    show=True,\n    max_imp=None,\n    title=\"\",\n    scale_global=False,\n    n_grid=50,\n    contour_levels=10,\n    dpi=200,\n    use_min=False,\n    use_max=True,\n    tkagg=False,\n) -&gt; None:\n    \"\"\"\n    Plot the contour of important hyperparameters.\n    Calls `plot_contour` for each pair of important hyperparameters.\n    Importance can be specified by the threshold.\n\n    Args:\n        threshold (float):\n            threshold for the importance. Not used any more in spotpython &gt;= 0.13.2.\n        filename (str):\n            filename of the plot\n        show (bool):\n            show the plot. Default is `True`.\n        max_imp (int):\n            maximum number of important hyperparameters. If there are more important hyperparameters\n            than `max_imp`, only the max_imp important ones are selected.\n        title (str):\n            title of the plots\n        scale_global (bool):\n            scale the z-axis globally. Default is `False`.\n        n_grid (int):\n            number of grid points. Default is 50.\n        contour_levels (int):\n            number of contour levels. Default is 10.\n        dpi (int):\n            dpi of the plot. Default is 200.\n        use_min (bool):\n            Use the minimum value for determing the hidden dimensions in the plot for categorical and\n            integer parameters.\n            In 3d-plots, only two variables can be independent. The remaining input variables are set\n            to their minimum value.\n            Default is `False`.\n            If use_min and use_max are both `True`, both values are used.\n        use_max (bool):\n            Use the minimum value for determing the hidden dimensions in the plot for categorical and\n            integer parameters.\n            In 3d-plots, only two variables can be independent. The remaining input variables are set\n            to their minimum value.\n            Default is `True`.\n            If use_min and use_max are both `True`, both values are used.\n\n    Returns:\n        None.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 5\n            # number of points\n            fun_evals = 10\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1, -1]),\n                upper = np.array([1, 1, 1]),\n                fun_evals=fun_evals,\n                tolerance_x = np.sqrt(np.spacing(1))\n                )\n            design_control=design_control_init(init_size=ni)\n            surrogate_control=surrogate_control_init(n_theta=3)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control,)\n            S.run()\n            S.plot_important_hyperparameter_contour()\n\n    \"\"\"\n    impo = self.print_importance(threshold=threshold, print_screen=True)\n    indices = sort_by_kth_and_return_indices(array=impo, k=1)\n    # take the first max_imp values from the indices array\n    if max_imp is not None:\n        indices = indices[:max_imp]\n    if scale_global:\n        min_z = min(self.y)\n        max_z = max(self.y)\n    else:\n        min_z = None\n        max_z = None\n    for i in indices:\n        for j in indices:\n            if j &gt; i:\n                if filename is not None:\n                    filename_full = filename + \"_contour_\" + str(i) + \"_\" + str(j) + \".png\"\n                else:\n                    filename_full = None\n                self.plot_contour(\n                    i=i,\n                    j=j,\n                    min_z=min_z,\n                    max_z=max_z,\n                    filename=filename_full,\n                    show=show,\n                    title=title,\n                    n_grid=n_grid,\n                    contour_levels=contour_levels,\n                    dpi=dpi,\n                    use_max=use_max,\n                    use_min=use_min,\n                    tkagg=tkagg,\n                )\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.plot_model","title":"<code>plot_model(y_min=None, y_max=None)</code>","text":"<p>Plot the model fit for 1-dim objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>spot object</p> required <code>y_min</code> <code>float</code> <p>y range, lower bound.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>y range, upper bound.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    # number of initial points:\n    ni = 3\n    # number of points\n    fun_evals = 7\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1]),\n        upper = np.array([1]),\n        fun_evals=fun_evals,\n        tolerance_x = np.sqrt(np.spacing(1))\n        )\n    design_control=design_control_init(init_size=ni)\n</code></pre> <pre><code>S = spot.Spot(fun=fun,\n            fun_control=fun_control,\n            design_control=design_control\nS.run()\nS.plot_model()\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def plot_model(self, y_min=None, y_max=None) -&gt; None:\n    \"\"\"\n    Plot the model fit for 1-dim objective functions.\n\n    Args:\n        self (object):\n            spot object\n        y_min (float, optional):\n            y range, lower bound.\n        y_max (float, optional):\n            y range, upper bound.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n            # number of initial points:\n            ni = 3\n            # number of points\n            fun_evals = 7\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1]),\n                upper = np.array([1]),\n                fun_evals=fun_evals,\n                tolerance_x = np.sqrt(np.spacing(1))\n                )\n            design_control=design_control_init(init_size=ni)\n\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control\n            S.run()\n            S.plot_model()\n    \"\"\"\n    if self.k == 1:\n        X_test = np.linspace(self.lower[0], self.upper[0], 100)\n        y_test = self.fun(X=X_test.reshape(-1, 1), fun_control=self.fun_control)\n        if isinstance(self.surrogate, Kriging):\n            y_hat = self.surrogate.predict(X_test[:, np.newaxis], return_val=\"y\")\n        else:\n            y_hat = self.surrogate.predict(X_test[:, np.newaxis])\n        plt.plot(X_test, y_hat, label=\"Model\")\n        plt.plot(X_test, y_test, label=\"True function\")\n        plt.scatter(self.X, self.y, edgecolor=\"b\", s=20, label=\"Samples\")\n        plt.scatter(self.X[-1], self.y[-1], edgecolor=\"r\", s=30, label=\"Last Sample\")\n        if self.noise:\n            plt.scatter(self.min_mean_X, self.min_mean_y, edgecolor=\"g\", s=30, label=\"Best Sample (mean)\")\n        else:\n            plt.scatter(self.min_X, self.min_y, edgecolor=\"g\", s=30, label=\"Best Sample\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.xlim((self.lower[0], self.upper[0]))\n        if y_min is None:\n            y_min = min([min(self.y), min(y_test)])\n        if y_max is None:\n            y_max = max([max(self.y), max(y_test)])\n        plt.ylim((y_min, y_max))\n        plt.legend(loc=\"best\")\n        # plt.title(self.surrogate.__class__.__name__ + \". \" + str(self.counter) + \": \" + str(self.min_y))\n        if self.noise:\n            plt.title(\"fun_evals: \" + str(self.counter) + \". min_y (noise): \" + str(np.round(self.min_y, 6)) + \" min_mean_y: \" + str(np.round(self.min_mean_y, 6)))\n        else:\n            plt.title(\"fun_evals: \" + str(self.counter) + \". min_y: \" + str(np.round(self.min_y, 6)))\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.plot_progress","title":"<code>plot_progress(show=True, log_x=False, log_y=False, filename='plot.png', style=['ko', 'k', 'ro-'], dpi=300, tkagg=False)</code>","text":"<p>Plot the progress of the hyperparameter tuning (optimization).</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>Show the plot.</p> <code>True</code> <code>log_x</code> <code>bool</code> <p>Use logarithmic scale for x-axis.</p> <code>False</code> <code>log_y</code> <code>bool</code> <p>Use logarithmic scale for y-axis.</p> <code>False</code> <code>filename</code> <code>str</code> <p>Filename to save the plot.</p> <code>'plot.png'</code> <code>style</code> <code>list</code> <p>Style of the plot. Default: [\u2018k\u2019, \u2018ro-\u2018], i.e., the initial points are plotted as a black line and the subsequent points as red dots connected by a line.</p> <code>['ko', 'k', 'ro-']</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 7\n    # number of points\n    fun_evals = 10\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1])\n        fun_evals=fun_evals,\n        tolerance_x = np.sqrt(np.spacing(1))\n        )\n    design_control=design_control_init(init_size=ni)\n    surrogate_control=surrogate_control_init(n_theta=3)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control\n                design_control=design_control,\n                surrogate_control=surrogate_control,)\n    S.run()\n    S.plot_progress(log_y=True)\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def plot_progress(self, show=True, log_x=False, log_y=False, filename=\"plot.png\", style=[\"ko\", \"k\", \"ro-\"], dpi=300, tkagg=False) -&gt; None:\n    \"\"\"Plot the progress of the hyperparameter tuning (optimization).\n\n    Args:\n        show (bool):\n            Show the plot.\n        log_x (bool):\n            Use logarithmic scale for x-axis.\n        log_y (bool):\n            Use logarithmic scale for y-axis.\n        filename (str):\n            Filename to save the plot.\n        style (list):\n            Style of the plot. Default: ['k', 'ro-'], i.e., the initial points are plotted as a black line\n            and the subsequent points as red dots connected by a line.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 7\n            # number of points\n            fun_evals = 10\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1])\n                fun_evals=fun_evals,\n                tolerance_x = np.sqrt(np.spacing(1))\n                )\n            design_control=design_control_init(init_size=ni)\n            surrogate_control=surrogate_control_init(n_theta=3)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control\n                        design_control=design_control,\n                        surrogate_control=surrogate_control,)\n            S.run()\n            S.plot_progress(log_y=True)\n\n    \"\"\"\n    if tkagg:\n        matplotlib.use(\"TkAgg\")\n    fig = pylab.figure(figsize=(9, 6))\n    s_y = pd.Series(self.y)\n    s_c = s_y.cummin()\n    n_init = self.design_control[\"init_size\"] * self.design_control[\"repeats\"]\n\n    ax = fig.add_subplot(211)\n    if n_init &lt;= len(s_y):\n        ax.plot(\n            range(1, n_init + 1),\n            s_y[:n_init],\n            style[0],\n            range(1, n_init + 2),\n            [s_c[:n_init].min()] * (n_init + 1),\n            style[1],\n            range(n_init + 1, len(s_c) + 1),\n            s_c[n_init:],\n            style[2],\n        )\n    else:\n        # plot only s_y values:\n        ax.plot(range(1, len(s_y) + 1), s_y, style[0])\n        logger.warning(\"Less evaluations ({len(s_y)}) than initial design points ({n_init}).\")\n    ax.set_xlabel(\"Iteration\")\n    if log_x:\n        ax.set_xscale(\"log\")\n    if log_y:\n        ax.set_yscale(\"log\")\n    if filename is not None:\n        pylab.savefig(filename, dpi=dpi, bbox_inches=\"tight\")\n    if show:\n        pylab.show()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.print_importance","title":"<code>print_importance(threshold=0.1, print_screen=True)</code>","text":"<p>Print importance of each variable and return the results as a list.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>threshold for printing</p> <code>0.1</code> <code>print_screen</code> <code>boolean</code> <p>if <code>True</code>, values are also printed on the screen. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>output</code> <code>list</code> <p>list of results</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def print_importance(self, threshold=0.1, print_screen=True) -&gt; list:\n    \"\"\"Print importance of each variable and return the results as a list.\n\n    Args:\n        threshold (float):\n            threshold for printing\n        print_screen (boolean):\n            if `True`, values are also printed on the screen. Default is `True`.\n\n    Returns:\n        output (list):\n            list of results\n    \"\"\"\n    output = []\n    if self.surrogate.n_theta &gt; 1:\n        theta = np.power(10, self.surrogate.theta)\n        imp = 100 * theta / np.max(theta)\n        # imp = imp[imp &gt;= threshold]\n        if self.var_name is None:\n            for i in range(len(imp)):\n                if imp[i] &gt;= threshold:\n                    if print_screen:\n                        print(\"x\", i, \": \", imp[i])\n                    output.append(\"x\" + str(i) + \": \" + str(imp[i]))\n        else:\n            var_name = [self.var_name[i] for i in range(len(imp))]\n            for i in range(len(imp)):\n                if imp[i] &gt;= threshold:\n                    if print_screen:\n                        print(var_name[i] + \": \", imp[i])\n                output.append([var_name[i], imp[i]])\n    else:\n        print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n    return output\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.print_results","title":"<code>print_results(print_screen=True, dict=None)</code>","text":"Print results from the run <ol> <li>min y</li> <li>min X If <code>noise == True</code>, additionally the following values are printed:</li> <li>min mean y</li> <li>min mean X</li> </ol> <p>Parameters:</p> Name Type Description Default <code>print_screen</code> <code>bool</code> <p>print results to screen</p> <code>True</code> <p>Returns:</p> Name Type Description <code>output</code> <code>list</code> <p>list of results</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def print_results(self, print_screen=True, dict=None) -&gt; list[str]:\n    \"\"\"Print results from the run:\n        1. min y\n        2. min X\n        If `noise == True`, additionally the following values are printed:\n        3. min mean y\n        4. min mean X\n\n    Args:\n        print_screen (bool, optional):\n            print results to screen\n\n    Returns:\n        output (list):\n            list of results\n    \"\"\"\n    output = []\n    if print_screen:\n        print(f\"min y: {self.min_y}\")\n        if self.noise:\n            print(f\"min mean y: {self.min_mean_y}\")\n    if self.noise:\n        res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n    else:\n        res = self.to_all_dim(self.min_X.reshape(1, -1))\n    for i in range(res.shape[1]):\n        if self.all_var_name is None:\n            var_name = \"x\" + str(i)\n        else:\n            var_name = self.all_var_name[i]\n            var_type = self.all_var_type[i]\n            if var_type == \"factor\" and dict is not None:\n                val = get_ith_hyperparameter_name_from_fun_control(fun_control=dict, key=var_name, i=int(res[0][i]))\n            else:\n                val = res[0][i]\n        if print_screen:\n            print(var_name + \":\", val)\n        output.append([var_name, val])\n    return output\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.print_results_old","title":"<code>print_results_old(print_screen=True, dict=None)</code>","text":"Print results from the run <ol> <li>min y</li> <li>min X If <code>noise == True</code>, additionally the following values are printed:</li> <li>min mean y</li> <li>min mean X</li> </ol> <p>Parameters:</p> Name Type Description Default <code>print_screen</code> <code>bool</code> <p>print results to screen</p> <code>True</code> <p>Returns:</p> Name Type Description <code>output</code> <code>list</code> <p>list of results</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def print_results_old(self, print_screen=True, dict=None) -&gt; list[str]:\n    \"\"\"Print results from the run:\n        1. min y\n        2. min X\n        If `noise == True`, additionally the following values are printed:\n        3. min mean y\n        4. min mean X\n\n    Args:\n        print_screen (bool, optional):\n            print results to screen\n\n    Returns:\n        output (list):\n            list of results\n    \"\"\"\n    output = []\n    if print_screen:\n        print(f\"min y: {self.min_y}\")\n    res = self.to_all_dim(self.min_X.reshape(1, -1))\n    for i in range(res.shape[1]):\n        if self.all_var_name is None:\n            var_name = \"x\" + str(i)\n        else:\n            var_name = self.all_var_name[i]\n            var_type = self.all_var_type[i]\n            if var_type == \"factor\" and dict is not None:\n                val = get_ith_hyperparameter_name_from_fun_control(fun_control=dict, key=var_name, i=int(res[0][i]))\n            else:\n                val = res[0][i]\n        if print_screen:\n            print(var_name + \":\", val)\n        output.append([var_name, val])\n    if self.noise:\n        res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n        if print_screen:\n            print(f\"min mean y: {self.min_mean_y}\")\n        for i in range(res.shape[1]):\n            var_name = \"x\" + str(i) if self.all_var_name is None else self.all_var_name[i]\n            if print_screen:\n                print(var_name + \":\", res[0][i])\n            output.append([var_name, res[0][i]])\n    return output\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.process_z00","title":"<code>process_z00(z00, use_min=True)</code>","text":"<p>Process each entry in the <code>z00</code> array according to the corresponding type in the <code>self.var_type</code> list. Specifically, if the type is \u201cfloat\u201d, the function will calculate the mean of the two <code>z00</code> values. If the type is not \u201cfloat\u201d, the function will retrun the maximum of the two <code>z00</code> values.</p> <p>Parameters:</p> Name Type Description Default <code>z00</code> <code>ndarray</code> <p>Array of values to process.</p> required <code>use_min</code> <code>bool</code> <p>If <code>True</code>, the minimum value is returned. If <code>False</code>, the maximum value is returned.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> <p>Processed values.</p> <p>Examples:</p> <p>from spotpython.spot import spot import numpy as np import random z00 = np.array([[1, 2, 3, 4], [5, 6, 7, 8]]) spot.var_type = [\u201cfloat\u201d, \u201cint\u201d, \u201cint\u201d, \u201cfloat\u201d] spot.process_z00(z00) [3, 6, 7, 6]</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def process_z00(self, z00, use_min=True) -&gt; list:\n    \"\"\"Process each entry in the `z00` array according to the corresponding type\n    in the `self.var_type` list.\n    Specifically, if the type is \"float\", the function will calculate the mean of the two `z00` values.\n    If the type is not \"float\", the function will retrun the maximum of the two `z00` values.\n\n    Args:\n        z00 (numpy.ndarray):\n            Array of values to process.\n        use_min (bool):\n            If `True`, the minimum value is returned. If `False`, the maximum value is returned.\n\n    Returns:\n        (list): Processed values.\n\n    Examples:\n        from spotpython.spot import spot\n        import numpy as np\n        import random\n        z00 = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n        spot.var_type = [\"float\", \"int\", \"int\", \"float\"]\n        spot.process_z00(z00)\n        [3, 6, 7, 6]\n\n    \"\"\"\n    result = []\n    for i in range(len(self.var_type)):\n        if self.var_type[i] == \"float\":\n            mean_value = np.mean(z00[:, i])\n            result.append(mean_value)\n        else:  # var_type[i] == 'int'\n            if use_min:\n                min_value = min(z00[:, i])\n                result.append(min_value)\n            else:\n                max_value = max(z00[:, i])\n                result.append(max_value)\n    return result\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.reattach_logger_handlers","title":"<code>reattach_logger_handlers()</code>","text":"<p>Reattach handlers to the logger after unpickling.</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def reattach_logger_handlers(self) -&gt; None:\n    \"\"\"\n    Reattach handlers to the logger after unpickling.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    # configure the handler and formatter as needed\n    py_handler = logging.FileHandler(f\"{__name__}.log\", mode=\"w\")\n    py_formatter = logging.Formatter(\"%(name)s %(asctime)s %(levelname)s %(message)s\")\n    # add formatter to the handler\n    py_handler.setFormatter(py_formatter)\n    # add handler to the logger\n    logger.addHandler(py_handler)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.remove_logger_handlers","title":"<code>remove_logger_handlers()</code>","text":"<p>Remove handlers from the logger to avoid pickling issues.</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def remove_logger_handlers(self) -&gt; None:\n    \"\"\"\n    Remove handlers from the logger to avoid pickling issues.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    for handler in logger.handlers[:]:  # Copy the list to avoid modification during iteration\n        logger.removeHandler(handler)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.run","title":"<code>run(X_start=None)</code>","text":"<p>Run the surrogate based optimization. The optimization process is controlled by the following steps:     1. Initialize design     2. Update stats     3. Fit surrogate     4. Update design     5. Update stats     6. Update writer     7. Fit surrogate     8. Show progress if needed</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <code>X_start</code> <code>ndarray</code> <p>initial design. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>object</code> <p>Spot object</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 7\n    # start point X_0\n    X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1]))\n    design_control=design_control_init(init_size=ni)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    S.run(X_start=X_start)\n    print(f\"S.X: {S.X}\")\n    print(f\"S.y: {S.y}\")\n    Seed set to 123\n    Seed set to 123\n    spotpython tuning: 0.0 [########--] 80.00%\n    spotpython tuning: 0.0 [#########-] 86.67%\n    spotpython tuning: 0.0 [#########-] 93.33%\n    spotpython tuning: 0.0 [##########] 100.00% Done...\n</code></pre> <pre><code>S.X: [[ 0.00000000e+00  0.00000000e+00]\n[ 0.00000000e+00  1.00000000e+00]\n[ 1.00000000e+00  0.00000000e+00]\n[ 1.00000000e+00  1.00000000e+00]\n[-9.09243389e-01 -1.58234577e-01]\n[-2.05817107e-01 -4.81249089e-01]\n[ 9.49741171e-01 -9.46312716e-01]\n[-1.20955714e-01  6.38358863e-02]\n[-6.62787018e-01  1.74316373e-01]\n[ 2.82008441e-01  9.30010114e-01]\n[ 4.78788115e-01  6.53210582e-01]\n[ 2.64764215e-04  4.00803185e-03]\n[-1.66363820e-05  4.65001027e-03]\n[-2.60995680e-04  5.46114194e-03]\n[ 3.74504308e-03  1.86731890e-02]]\nS.y: [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n8.51761723e-01 2.73961367e-01 1.79751605e+00 1.87053051e-02\n4.69672829e-01 9.44447573e-01 6.55922124e-01 1.61344194e-05\n2.16228723e-05 2.98921900e-05 3.62713334e-04]\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def run(self, X_start=None) -&gt; Spot:\n    \"\"\"\n    Run the surrogate based optimization.\n    The optimization process is controlled by the following steps:\n        1. Initialize design\n        2. Update stats\n        3. Fit surrogate\n        4. Update design\n        5. Update stats\n        6. Update writer\n        7. Fit surrogate\n        8. Show progress if needed\n\n    Args:\n        self (object): Spot object\n        X_start (numpy.ndarray, optional): initial design. Defaults to None.\n\n    Returns:\n        (object): Spot object\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 7\n            # start point X_0\n            X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1]))\n            design_control=design_control_init(init_size=ni)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,)\n            S.run(X_start=X_start)\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n            Seed set to 123\n            Seed set to 123\n            spotpython tuning: 0.0 [########--] 80.00%\n            spotpython tuning: 0.0 [#########-] 86.67%\n            spotpython tuning: 0.0 [#########-] 93.33%\n            spotpython tuning: 0.0 [##########] 100.00% Done...\n\n            S.X: [[ 0.00000000e+00  0.00000000e+00]\n            [ 0.00000000e+00  1.00000000e+00]\n            [ 1.00000000e+00  0.00000000e+00]\n            [ 1.00000000e+00  1.00000000e+00]\n            [-9.09243389e-01 -1.58234577e-01]\n            [-2.05817107e-01 -4.81249089e-01]\n            [ 9.49741171e-01 -9.46312716e-01]\n            [-1.20955714e-01  6.38358863e-02]\n            [-6.62787018e-01  1.74316373e-01]\n            [ 2.82008441e-01  9.30010114e-01]\n            [ 4.78788115e-01  6.53210582e-01]\n            [ 2.64764215e-04  4.00803185e-03]\n            [-1.66363820e-05  4.65001027e-03]\n            [-2.60995680e-04  5.46114194e-03]\n            [ 3.74504308e-03  1.86731890e-02]]\n            S.y: [0.00000000e+00 1.00000000e+00 1.00000000e+00 2.00000000e+00\n            8.51761723e-01 2.73961367e-01 1.79751605e+00 1.87053051e-02\n            4.69672829e-01 9.44447573e-01 6.55922124e-01 1.61344194e-05\n            2.16228723e-05 2.98921900e-05 3.62713334e-04]\n\n    \"\"\"\n    self.initialize_design(X_start)\n    self.update_stats()\n    self.fit_surrogate()\n    timeout_start = time.time()\n    while self.should_continue(timeout_start):\n        self.update_design()\n        self.update_stats()\n        self.update_writer()\n        self.fit_surrogate()\n        self.show_progress_if_needed(timeout_start)\n    if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n        self.spot_writer.flush()\n        self.spot_writer.close()\n    if self.fun_control[\"db_dict_name\"] is not None:\n        self.write_db_dict()\n    if self.fun_control[\"save_experiment\"]:\n        self.save_experiment()\n    return self\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.save_experiment","title":"<code>save_experiment(filename=None, path=None, overwrite=True)</code>","text":"<p>Save the experiment to a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename of the experiment file.</p> <code>None</code> <code>path</code> <code>str</code> <p>The path to the experiment file.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If <code>True</code>, the file will be overwritten if it already exists. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def save_experiment(self, filename=None, path=None, overwrite=True) -&gt; None:\n    \"\"\"\n    Save the experiment to a file.\n\n    Args:\n        filename (str):\n            The filename of the experiment file.\n        path (str):\n            The path to the experiment file.\n        overwrite (bool):\n            If `True`, the file will be overwritten if it already exists. Default is `True`.\n\n    Returns:\n        None\n    \"\"\"\n    # Remove or close any unpickleable objects, e.g., the spot_writer\n    self.close_and_del_spot_writer()\n\n    # Remove the logger handler before pickling\n    self.remove_logger_handlers()\n\n    # Create deep copies of control dictionaries\n    fun_control = copy.deepcopy(self.fun_control)\n    optimizer_control = copy.deepcopy(self.optimizer_control)\n    surrogate_control = copy.deepcopy(self.surrogate_control)\n    design_control = copy.deepcopy(self.design_control)\n\n    # Deep copy the spot object itself (except unpickleable components)\n    try:\n        spot_tuner = copy.deepcopy(self)\n    except Exception as e:\n        print(\"Warning: Could not copy spot_tuner object!\")\n        print(f\"Error: {e}\")\n        spot_tuner = self\n\n    # Prepare the experiment dictionary\n    experiment = {\n        \"design_control\": design_control,\n        \"fun_control\": fun_control,\n        \"optimizer_control\": optimizer_control,\n        \"spot_tuner\": spot_tuner,\n        \"surrogate_control\": surrogate_control,\n    }\n\n    # Determine the filename based on PREFIX if not provided\n    PREFIX = fun_control.get(\"PREFIX\")\n    if filename is None and PREFIX is not None:\n        filename = get_experiment_filename(PREFIX)\n\n    if path is not None:\n        filename = os.path.join(path, filename)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n    # Check if the file already exists\n    if filename is not None and os.path.exists(filename) and not overwrite:\n        print(f\"Error: File {filename} already exists. Use overwrite=True to overwrite the file.\")\n        return\n\n    # Serialize the experiment dictionary to the pickle file\n    if filename is not None:\n        with open(filename, \"wb\") as handle:\n            try:\n                pickle.dump(experiment, handle, protocol=pickle.HIGHEST_PROTOCOL)\n            except Exception as e:\n                print(f\"Error: {e}\")\n                raise e\n        print(f\"Experiment saved to {filename}\")\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.show_progress_if_needed","title":"<code>show_progress_if_needed(timeout_start)</code>","text":"<p>Show progress bar if <code>show_progress</code> is <code>True</code>. If self.progress_file is not <code>None</code>, the progress bar is saved in the file with the name <code>self.progress_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <code>timeout_start</code> <code>float</code> <p>start time</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def show_progress_if_needed(self, timeout_start) -&gt; None:\n    \"\"\"Show progress bar if `show_progress` is `True`. If\n    self.progress_file is not `None`, the progress bar is saved\n    in the file with the name `self.progress_file`.\n\n    Args:\n        self (object): Spot object\n        timeout_start (float): start time\n\n    Returns:\n        (NoneType): None\n    \"\"\"\n    if not self.show_progress:\n        return\n    if isfinite(self.fun_evals):\n        progress_bar(progress=self.counter / self.fun_evals, y=self.min_y, filename=self.progress_file)\n    else:\n        progress_bar(progress=(time.time() - timeout_start) / (self.max_time * 60), y=self.min_y, filename=self.progress_file)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.suggest_new_X","title":"<code>suggest_new_X()</code>","text":"<p>Compute <code>n_points</code> new infill points in natural units. These diffrent points are computed by the optimizer using increasing seed. The optimizer searches in the ranges from <code>lower_j</code> to <code>upper_j</code>. The method <code>infill()</code> is used as the objective function.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>n_points</code> infill points in natural units, each of dim k</p> Note <p>This is step (S-14a) in [bart21i].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.spot import spot\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    nn = 3\n    fun_sphere = analytical().fun_sphere\n    fun_control = fun_control_init(\n            lower = np.array([-1, -1]),\n            upper = np.array([1, 1]),\n            n_points=nn,\n            )\n    spot_1 = spot.Spot(\n        fun=fun_sphere,\n        fun_control=fun_control,\n        )\n    # (S-2) Initial Design:\n    spot_1.X = spot_1.design.scipy_lhd(\n        spot_1.design_control[\"init_size\"], lower=spot_1.lower, upper=spot_1.upper\n    )\n    print(f\"spot_1.X: {spot_1.X}\")\n    # (S-3): Eval initial design:\n    spot_1.y = spot_1.fun(spot_1.X)\n    print(f\"spot_1.y: {spot_1.y}\")\n    spot_1.fit_surrogate()\n    X0 = spot_1.suggest_new_X()\n    print(f\"X0: {X0}\")\n    assert X0.size == spot_1.n_points * spot_1.k\n    assert X0.ndim == 2\n    assert X0.shape[0] == nn\n    assert X0.shape[1] == 2\n    spot_1.X: [[ 0.86352963  0.7892358 ]\n                [-0.24407197 -0.83687436]\n                [ 0.36481882  0.8375811 ]\n                [ 0.415331    0.54468512]\n                [-0.56395091 -0.77797854]\n                [-0.90259409 -0.04899292]\n                [-0.16484832  0.35724741]\n                [ 0.05170659  0.07401196]\n                [-0.78548145 -0.44638164]\n                [ 0.64017497 -0.30363301]]\n    spot_1.y: [1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764\n    0.15480068 0.00815134 0.81623768 0.502017  ]\n    X0: [[0.00154544 0.003962  ]\n        [0.00165526 0.00410847]\n        [0.00165685 0.0039177 ]]\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def suggest_new_X(self) -&gt; np.array:\n    \"\"\"\n    Compute `n_points` new infill points in natural units.\n    These diffrent points are computed by the optimizer using increasing seed.\n    The optimizer searches in the ranges from `lower_j` to `upper_j`.\n    The method `infill()` is used as the objective function.\n\n    Returns:\n        (numpy.ndarray): `n_points` infill points in natural units, each of dim k\n\n    Note:\n        This is step (S-14a) in [bart21i].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.spot import spot\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            nn = 3\n            fun_sphere = analytical().fun_sphere\n            fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]),\n                    n_points=nn,\n                    )\n            spot_1 = spot.Spot(\n                fun=fun_sphere,\n                fun_control=fun_control,\n                )\n            # (S-2) Initial Design:\n            spot_1.X = spot_1.design.scipy_lhd(\n                spot_1.design_control[\"init_size\"], lower=spot_1.lower, upper=spot_1.upper\n            )\n            print(f\"spot_1.X: {spot_1.X}\")\n            # (S-3): Eval initial design:\n            spot_1.y = spot_1.fun(spot_1.X)\n            print(f\"spot_1.y: {spot_1.y}\")\n            spot_1.fit_surrogate()\n            X0 = spot_1.suggest_new_X()\n            print(f\"X0: {X0}\")\n            assert X0.size == spot_1.n_points * spot_1.k\n            assert X0.ndim == 2\n            assert X0.shape[0] == nn\n            assert X0.shape[1] == 2\n            spot_1.X: [[ 0.86352963  0.7892358 ]\n                        [-0.24407197 -0.83687436]\n                        [ 0.36481882  0.8375811 ]\n                        [ 0.415331    0.54468512]\n                        [-0.56395091 -0.77797854]\n                        [-0.90259409 -0.04899292]\n                        [-0.16484832  0.35724741]\n                        [ 0.05170659  0.07401196]\n                        [-0.78548145 -0.44638164]\n                        [ 0.64017497 -0.30363301]]\n            spot_1.y: [1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764\n            0.15480068 0.00815134 0.81623768 0.502017  ]\n            X0: [[0.00154544 0.003962  ]\n                [0.00165526 0.00410847]\n                [0.00165685 0.0039177 ]]\n    \"\"\"\n    # (S-14a) Optimization on the surrogate:\n    new_X = np.zeros([self.n_points, self.k], dtype=float)\n    optimizer_name = self.optimizer.__name__\n    optimizers = {\n        \"dual_annealing\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n        \"differential_evolution\": lambda: self.optimizer(\n            func=self.infill,\n            bounds=self.de_bounds,\n            maxiter=self.optimizer_control[\"max_iter\"],\n            seed=self.optimizer_control[\"seed\"],\n        ),\n        \"direct\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds, eps=1e-2),\n        \"shgo\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n        \"basinhopping\": lambda: self.optimizer(func=self.infill, x0=self.min_X),\n        \"default\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n    }\n    for i in range(self.n_points):\n        self.optimizer_control[\"seed\"] = self.optimizer_control[\"seed\"] + i\n        result = optimizers.get(optimizer_name, optimizers[\"default\"])()\n        new_X[i][:] = result.x\n    return np.unique(new_X, axis=0)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.to_red_dim","title":"<code>to_red_dim()</code>","text":"<p>Reduce dimension if lower == upper. This is done by removing the corresponding entries from lower, upper, var_type, and var_name. k is modified accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Attributes:</p> Name Type Description <code>self.lower</code> <code>ndarray</code> <p>lower bound</p> <code>self.upper</code> <code>ndarray</code> <p>upper bound</p> <code>self.var_type</code> <code>List[str]</code> <p>list of variable types</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n                    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 3\n    # number of points\n    n = 10\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1]),\n        fun_evals = n)\n    design_control=design_control_init(init_size=ni)\n    spot_1 = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    spot_1.run()\n    assert spot_1.lower.size == 2\n    assert spot_1.upper.size == 2\n    assert len(spot_1.var_type) == 2\n    assert spot_1.red_dim == False\n    spot_1.lower = np.array([-1, -1])\n    spot_1.upper = np.array([-1, -1])\n    spot_1.to_red_dim()\n    assert spot_1.lower.size == 0\n    assert spot_1.upper.size == 0\n    assert len(spot_1.var_type) == 0\n    assert spot_1.red_dim == True\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def to_red_dim(self) -&gt; None:\n    \"\"\"\n    Reduce dimension if lower == upper.\n    This is done by removing the corresponding entries from\n    lower, upper, var_type, and var_name.\n    k is modified accordingly.\n\n    Args:\n        self (object): Spot object\n\n    Returns:\n        (NoneType): None\n\n    Attributes:\n        self.lower (numpy.ndarray): lower bound\n        self.upper (numpy.ndarray): upper bound\n        self.var_type (List[str]): list of variable types\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n                            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 3\n            # number of points\n            n = 10\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1]),\n                fun_evals = n)\n            design_control=design_control_init(init_size=ni)\n            spot_1 = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,)\n            spot_1.run()\n            assert spot_1.lower.size == 2\n            assert spot_1.upper.size == 2\n            assert len(spot_1.var_type) == 2\n            assert spot_1.red_dim == False\n            spot_1.lower = np.array([-1, -1])\n            spot_1.upper = np.array([-1, -1])\n            spot_1.to_red_dim()\n            assert spot_1.lower.size == 0\n            assert spot_1.upper.size == 0\n            assert len(spot_1.var_type) == 0\n            assert spot_1.red_dim == True\n\n    \"\"\"\n    # Backup of the original values:\n    self.all_lower = self.lower\n    self.all_upper = self.upper\n    # Select only lower != upper:\n    self.ident = (self.upper - self.lower) == 0\n    # Determine if dimension is reduced:\n    self.red_dim = self.ident.any()\n    # Modifications:\n    # Modify lower and upper:\n    self.lower = self.lower[~self.ident]\n    self.upper = self.upper[~self.ident]\n    # Modify k (dim):\n    self.k = self.lower.size\n    # Modify var_type:\n    if self.var_type is not None:\n        self.all_var_type = self.var_type\n        self.var_type = [x for x, y in zip(self.all_var_type, self.ident) if not y]\n    # Modify var_name:\n    if self.var_name is not None:\n        self.all_var_name = self.var_name\n        self.var_name = [x for x, y in zip(self.all_var_name, self.ident) if not y]\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.update_design","title":"<code>update_design()</code>","text":"<p>Update design. Generate and evaluate new design points. It is basically a call to the method <code>get_new_X0()</code>. If <code>noise</code> is <code>True</code>, additionally the following steps (from <code>get_X_ocba()</code>) are performed: 1. Compute OCBA points. 2. Evaluate OCBA points. 3. Append OCBA points to the new design points.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Attributes:</p> Name Type Description <code>self.X</code> <code>ndarray</code> <p>updated design</p> <code>self.y</code> <code>ndarray</code> <p>updated design values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # 1. Without OCBA points:\n&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    from spotpython.spot import spot\n    # number of initial points:\n    ni = 0\n    X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1])\n        )\n    design_control=design_control_init(init_size=ni)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    S.initialize_design(X_start=X_start)\n    print(f\"S.X: {S.X}\")\n    print(f\"S.y: {S.y}\")\n    X_shape_before = S.X.shape\n    print(f\"X_shape_before: {X_shape_before}\")\n    print(f\"y_size_before: {S.y.size}\")\n    y_size_before = S.y.size\n    S.update_stats()\n    S.fit_surrogate()\n    S.update_design()\n    print(f\"S.X: {S.X}\")\n    print(f\"S.y: {S.y}\")\n    print(f\"S.n_points: {S.n_points}\")\n    print(f\"X_shape_after: {S.X.shape}\")\n    print(f\"y_size_after: {S.y.size}\")\n&gt;&gt;&gt; #\n&gt;&gt;&gt; # 2. Using the OCBA points:\n&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init\n    # number of initial points:\n    ni = 3\n    X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n            sigma=0.02,\n            lower = np.array([-1, -1]),\n            upper = np.array([1, 1]),\n            noise=True,\n            ocba_delta=1,\n        )\n    design_control=design_control_init(init_size=ni, repeats=2)\n</code></pre> <pre><code>S = spot.Spot(fun=fun,\n            design_control=design_control,\n            fun_control=fun_control\n)\nS.initialize_design(X_start=X_start)\nprint(f\"S.X: {S.X}\")\nprint(f\"S.y: {S.y}\")\nX_shape_before = S.X.shape\nprint(f\"X_shape_before: {X_shape_before}\")\nprint(f\"y_size_before: {S.y.size}\")\ny_size_before = S.y.size\nS.update_stats()\nS.fit_surrogate()\nS.update_design()\nprint(f\"S.X: {S.X}\")\nprint(f\"S.y: {S.y}\")\nprint(f\"S.n_points: {S.n_points}\")\nprint(f\"S.ocba_delta: {S.ocba_delta}\")\nprint(f\"X_shape_after: {S.X.shape}\")\nprint(f\"y_size_after: {S.y.size}\")\n# compare the shapes of the X and y values before and after the update_design method\nassert X_shape_before[0] + S.n_points * S.fun_repeats + S.ocba_delta == S.X.shape[0]\nassert X_shape_before[1] == S.X.shape[1]\nassert y_size_before + S.n_points * S.fun_repeats + S.ocba_delta == S.y.size\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def update_design(self) -&gt; None:\n    \"\"\"\n    Update design. Generate and evaluate new design points.\n    It is basically a call to the method `get_new_X0()`.\n    If `noise` is `True`, additionally the following steps\n    (from `get_X_ocba()`) are performed:\n    1. Compute OCBA points.\n    2. Evaluate OCBA points.\n    3. Append OCBA points to the new design points.\n\n    Args:\n        self (object): Spot object\n\n    Returns:\n        (NoneType): None\n\n    Attributes:\n        self.X (numpy.ndarray): updated design\n        self.y (numpy.ndarray): updated design values\n\n    Examples:\n        &gt;&gt;&gt; # 1. Without OCBA points:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            from spotpython.spot import spot\n            # number of initial points:\n            ni = 0\n            X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1])\n                )\n            design_control=design_control_init(init_size=ni)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,)\n            S.initialize_design(X_start=X_start)\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n            X_shape_before = S.X.shape\n            print(f\"X_shape_before: {X_shape_before}\")\n            print(f\"y_size_before: {S.y.size}\")\n            y_size_before = S.y.size\n            S.update_stats()\n            S.fit_surrogate()\n            S.update_design()\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n            print(f\"S.n_points: {S.n_points}\")\n            print(f\"X_shape_after: {S.X.shape}\")\n            print(f\"y_size_after: {S.y.size}\")\n        &gt;&gt;&gt; #\n        &gt;&gt;&gt; # 2. Using the OCBA points:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init\n            # number of initial points:\n            ni = 3\n            X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                    sigma=0.02,\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]),\n                    noise=True,\n                    ocba_delta=1,\n                )\n            design_control=design_control_init(init_size=ni, repeats=2)\n\n            S = spot.Spot(fun=fun,\n                        design_control=design_control,\n                        fun_control=fun_control\n            )\n            S.initialize_design(X_start=X_start)\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n            X_shape_before = S.X.shape\n            print(f\"X_shape_before: {X_shape_before}\")\n            print(f\"y_size_before: {S.y.size}\")\n            y_size_before = S.y.size\n            S.update_stats()\n            S.fit_surrogate()\n            S.update_design()\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n            print(f\"S.n_points: {S.n_points}\")\n            print(f\"S.ocba_delta: {S.ocba_delta}\")\n            print(f\"X_shape_after: {S.X.shape}\")\n            print(f\"y_size_after: {S.y.size}\")\n            # compare the shapes of the X and y values before and after the update_design method\n            assert X_shape_before[0] + S.n_points * S.fun_repeats + S.ocba_delta == S.X.shape[0]\n            assert X_shape_before[1] == S.X.shape[1]\n            assert y_size_before + S.n_points * S.fun_repeats + S.ocba_delta == S.y.size\n\n    \"\"\"\n    # OCBA (only if noise). Determination of the OCBA points depends on the\n    # old X and y values.\n    if self.noise and self.ocba_delta &gt; 0 and not np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &lt;= 2):\n        logger.warning(\"self.var_y &lt;= 0. OCBA points are not generated:\")\n        logger.warning(\"There are less than 3 points or points with no variance information.\")\n        logger.debug(\"In update_design(): self.mean_X: %s\", self.mean_X)\n        logger.debug(\"In update_design(): self.var_y: %s\", self.var_y)\n    if self.noise and self.ocba_delta &gt; 0 and np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &gt; 2):\n        X_ocba = get_ocba_X(self.mean_X, self.mean_y, self.var_y, self.ocba_delta)\n    else:\n        X_ocba = None\n    # Determine the new X0 values based on the old X and y values:\n    X0 = self.get_new_X0()\n    # Append OCBA points to the new design points:\n    if self.noise and self.ocba_delta &gt; 0 and np.all(self.var_y &gt; 0):\n        X0 = append(X_ocba, X0, axis=0)\n    X_all = self.to_all_dim_if_needed(X0)\n    logger.debug(\n        \"In update_design(): self.fun_control sigma and seed passed to fun(): %s %s\",\n        self.fun_control[\"sigma\"],\n        self.fun_control[\"seed\"],\n    )\n    # (S-18): Evaluating New Solutions:\n    y0 = self.fun(X=X_all, fun_control=self.fun_control)\n    X0, y0 = remove_nan(X0, y0, stop_on_zero_return=False)\n    # Append New Solutions (only if they are not nan):\n    if y0.shape[0] &gt; 0:\n        self.X = np.append(self.X, X0, axis=0)\n        self.y = np.append(self.y, y0)\n    else:\n        # otherwise, generate a random point and append it to the design\n        Xr, yr = self.generate_random_point()\n        self.X = np.append(self.X, Xr, axis=0)\n        self.y = np.append(self.y, yr)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.update_stats","title":"<code>update_stats()</code>","text":"<p>Update the following stats: 1. <code>min_y</code> 2. <code>min_X</code> 3. <code>counter</code> If <code>noise</code> is <code>True</code>, additionally the following stats are computed: 1. <code>mean_X</code> 2. <code>mean_y</code> 3. <code>min_mean_y</code> 4. <code>min_mean_X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Attributes:</p> Name Type Description <code>self.min_y</code> <code>float</code> <p>minimum y value</p> <code>self.min_X</code> <code>ndarray</code> <p>X value of the minimum y value</p> <code>self.counter</code> <code>int</code> <p>number of function evaluations</p> <code>self.mean_X</code> <code>ndarray</code> <p>mean X values</p> <code>self.mean_y</code> <code>ndarray</code> <p>mean y values</p> <code>self.var_y</code> <code>ndarray</code> <p>variance of y values</p> <code>self.min_mean_y</code> <code>float</code> <p>minimum mean y value</p> <code>self.min_mean_X</code> <code>ndarray</code> <p>X value of the minimum mean y value</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def update_stats(self) -&gt; None:\n    \"\"\"\n    Update the following stats: 1. `min_y` 2. `min_X` 3. `counter`\n    If `noise` is `True`, additionally the following stats are computed: 1. `mean_X`\n    2. `mean_y` 3. `min_mean_y` 4. `min_mean_X`.\n\n    Args:\n        self (object): Spot object\n\n    Returns:\n        (NoneType): None\n\n    Attributes:\n        self.min_y (float): minimum y value\n        self.min_X (numpy.ndarray): X value of the minimum y value\n        self.counter (int): number of function evaluations\n        self.mean_X (numpy.ndarray): mean X values\n        self.mean_y (numpy.ndarray): mean y values\n        self.var_y (numpy.ndarray): variance of y values\n        self.min_mean_y (float): minimum mean y value\n        self.min_mean_X (numpy.ndarray): X value of the minimum mean y value\n\n    \"\"\"\n    self.min_y = min(self.y)\n    self.min_X = self.X[argmin(self.y)]\n    self.counter = self.y.size\n    self.fun_control.update({\"counter\": self.counter})\n    # Update aggregated x and y values (if noise):\n    if self.noise:\n        Z = aggregate_mean_var(X=self.X, y=self.y)\n        self.mean_X = Z[0]\n        self.mean_y = Z[1]\n        self.var_y = Z[2]\n        # X value of the best mean y value so far:\n        self.min_mean_X = self.mean_X[argmin(self.mean_y)]\n        # variance of the best mean y value so far:\n        self.min_var_y = self.var_y[argmin(self.mean_y)]\n        # best mean y value so far:\n        self.min_mean_y = self.mean_y[argmin(self.mean_y)]\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.write_db_dict","title":"<code>write_db_dict()</code>","text":"<p>Writes a dictionary with the experiment parameters to the json file spotpython_db.json.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def write_db_dict(self) -&gt; None:\n    \"\"\"Writes a dictionary with the experiment parameters to the json file spotpython_db.json.\n\n    Args:\n        self (object): Spot object\n\n    Returns:\n        (NoneType): None\n\n    \"\"\"\n    # get the time in seconds from 1.1.1970 and convert the time to a string\n    t_str = str(time.time())\n    ident = str(self.fun_control[\"PREFIX\"]) + \"_\" + t_str\n\n    (\n        fun_control,\n        design_control,\n        optimizer_control,\n        spot_tuner_control,\n        surrogate_control,\n    ) = self.de_serialize_dicts()\n    print(\"\\n**\")\n    print(\"The following dictionaries are written to the json file spotpython_db.json:\")\n    print(\"fun_control:\")\n    pprint.pprint(fun_control)\n\n    # Iterate over a list of the keys to avoid modifying the dictionary during iteration\n    for key in list(fun_control.keys()):\n        if not isinstance(fun_control[key], (int, float, str, list, dict)):\n            # remove the key from the dictionary\n            print(f\"Removing non-serializable key: {key}\")\n            fun_control.pop(key)\n\n    print(\"fun_control after removing non-serializabel keys:\")\n    pprint.pprint(fun_control)\n    pprint.pprint(fun_control)\n    print(\"design_control:\")\n    pprint.pprint(design_control)\n    print(\"optimizer_control:\")\n    pprint.pprint(optimizer_control)\n    print(\"spot_tuner_control:\")\n    pprint.pprint(spot_tuner_control)\n    print(\"surrogate_control:\")\n    pprint.pprint(surrogate_control)\n    #\n    # Generate a description of the results:\n    # if spot_tuner_control['min_y'] exists:\n    try:\n        result = f\"\"\"\n                  Results for {ident}: Finally, the best value is {spot_tuner_control['min_y']}\n                  at {spot_tuner_control['min_X']}.\"\"\"\n        #\n        db_dict = {\n            \"data\": {\n                \"id\": str(ident),\n                \"result\": result,\n                \"fun_control\": fun_control,\n                \"design_control\": design_control,\n                \"surrogate_control\": surrogate_control,\n                \"optimizer_control\": optimizer_control,\n                \"spot_tuner_control\": spot_tuner_control,\n            }\n        }\n        # Check if the directory \"db_dicts\" exists.\n        if not os.path.exists(\"db_dicts\"):\n            try:\n                os.makedirs(\"db_dicts\")\n            except OSError as e:\n                raise Exception(f\"Error creating directory: {e}\")\n\n        if os.path.exists(\"db_dicts\"):\n            try:\n                # Open the file in append mode to add each new dict as a new line\n                with open(\"db_dicts/\" + self.fun_control[\"db_dict_name\"], \"a\") as f:\n                    # Using json.dumps to convert the dict to a JSON formatted string\n                    # We then write this string to the file followed by a newline character\n                    # This ensures that each dict is on its own line, conforming to the JSON Lines format\n                    f.write(json.dumps(db_dict, cls=NumpyEncoder) + \"\\n\")\n            except OSError as e:\n                raise Exception(f\"Error writing to file: {e}\")\n    except KeyError:\n        print(\"No results to write.\")\n</code></pre>"},{"location":"reference/spotpython/torch/activation/","title":"activation","text":""},{"location":"reference/spotpython/torch/cosinewarmupcheduler/","title":"cosinewarmupcheduler","text":""},{"location":"reference/spotpython/torch/cosinewarmupcheduler/#spotpython.torch.cosinewarmupcheduler.CosineWarmupScheduler","title":"<code>CosineWarmupScheduler</code>","text":"<p>               Bases: <code>_LRScheduler</code></p> <p>Cosine annealing with warmup learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to use during training.</p> required <code>warmup</code> <code>int</code> <p>The number of warmup steps.</p> required <code>max_iters</code> <code>int</code> <p>The number of maximum iterations the model is trained for.</p> required Example <p>optimizer = torch.optim.SGD(model.parameters(), lr=0.1) scheduler = CosineWarmupScheduler(optimizer, warmup=10, max_iters=100) for epoch in range(100):     scheduler.step()     train(\u2026)</p> Source code in <code>spotpython/torch/cosinewarmupcheduler.py</code> <pre><code>class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n    \"\"\"Cosine annealing with warmup learning rate scheduler.\n\n    Args:\n        optimizer (torch.optim.Optimizer): The optimizer to use during training.\n        warmup (int): The number of warmup steps.\n        max_iters (int): The number of maximum iterations the model is trained for.\n\n    Example:\n        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n        &gt;&gt;&gt; scheduler = CosineWarmupScheduler(optimizer, warmup=10, max_iters=100)\n        &gt;&gt;&gt; for epoch in range(100):\n        &gt;&gt;&gt;     scheduler.step()\n        &gt;&gt;&gt;     train(...)\n    \"\"\"\n\n    def __init__(self, optimizer, warmup, max_iters):\n        self.warmup = warmup\n        self.max_num_iters = max_iters\n        super().__init__(optimizer)\n\n    def get_lr(self):\n        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n        return [base_lr * lr_factor for base_lr in self.base_lrs]\n\n    def get_lr_factor(self, epoch):\n        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n        if epoch &lt;= self.warmup:\n            lr_factor *= epoch * 1.0 / self.warmup\n        return lr_factor\n</code></pre>"},{"location":"reference/spotpython/torch/dataframedataset/","title":"dataframedataset","text":""},{"location":"reference/spotpython/torch/dimensions/","title":"dimensions","text":""},{"location":"reference/spotpython/torch/dimensions/#spotpython.torch.dimensions.extract_linear_dims","title":"<code>extract_linear_dims(model)</code>","text":"<p>Extracts the input and output dimensions of the Linear layers in a PyTorch model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model.</p> required <p>Returns:</p> Type Description <code>array</code> <p>np.array: Array with the input and output dimensions of the Linear layers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.torch.dimensions import extract_linear_dims\n&gt;&gt;&gt; net = NNLinearRegressor()\n&gt;&gt;&gt; result = extract_linear_dims(net)\n</code></pre> Source code in <code>spotpython/torch/dimensions.py</code> <pre><code>def extract_linear_dims(model) -&gt; np.array:\n    \"\"\"Extracts the input and output dimensions of the Linear layers in a PyTorch model.\n\n    Args:\n        model (nn.Module): PyTorch model.\n\n    Returns:\n        np.array: Array with the input and output dimensions of the Linear layers.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.torch.dimensions import extract_linear_dims\n        &gt;&gt;&gt; net = NNLinearRegressor()\n        &gt;&gt;&gt; result = extract_linear_dims(net)\n\n    \"\"\"\n    dims = []\n    for layer in model.layers:\n        if isinstance(layer, nn.Linear):\n            # Append input and output features of the Linear layer\n            dims.append(layer.in_features)\n            dims.append(layer.out_features)\n    return np.array(dims)\n</code></pre>"},{"location":"reference/spotpython/torch/mapk/","title":"mapk","text":""},{"location":"reference/spotpython/torch/mapk/#spotpython.torch.mapk.MAPK","title":"<code>MAPK</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Mean Average Precision at K (MAPK) metric.</p> <p>This class inherits from the <code>Metric</code> class of the <code>torchmetrics</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of top predictions to consider when calculating the metric.</p> <code>10</code> <code>dist_sync_on_step</code> <code>bool</code> <p>Whether to synchronize the metric states across processes during the forward pass.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>total</code> <code>Tensor</code> <p>The cumulative sum of the metric scores across all batches.</p> <code>count</code> <code>Tensor</code> <p>The number of batches processed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.torch.mapk import MAPK\n    import torch\n    mapk = MAPK(k=2)\n    target = torch.tensor([0, 1, 2, 2])\n    preds = torch.tensor(\n        [\n            [0.5, 0.2, 0.2],  # 0 is in top 2\n            [0.3, 0.4, 0.2],  # 1 is in top 2\n            [0.2, 0.4, 0.3],  # 2 is in top 2\n            [0.7, 0.2, 0.1],  # 2 isn't in top 2\n        ]\n    )\n    mapk.update(preds, target)\n    print(mapk.compute()) # tensor(0.6250)\n</code></pre> Source code in <code>spotpython/torch/mapk.py</code> <pre><code>class MAPK(torchmetrics.Metric):\n    \"\"\"\n    Mean Average Precision at K (MAPK) metric.\n\n    This class inherits from the `Metric` class of the `torchmetrics` library.\n\n    Args:\n        k (int):\n            The number of top predictions to consider when calculating the metric.\n        dist_sync_on_step (bool):\n            Whether to synchronize the metric states across processes during the forward pass.\n\n    Attributes:\n        total (torch.Tensor):\n            The cumulative sum of the metric scores across all batches.\n        count (torch.Tensor):\n            The number of batches processed.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.torch.mapk import MAPK\n            import torch\n            mapk = MAPK(k=2)\n            target = torch.tensor([0, 1, 2, 2])\n            preds = torch.tensor(\n                [\n                    [0.5, 0.2, 0.2],  # 0 is in top 2\n                    [0.3, 0.4, 0.2],  # 1 is in top 2\n                    [0.2, 0.4, 0.3],  # 2 is in top 2\n                    [0.7, 0.2, 0.1],  # 2 isn't in top 2\n                ]\n            )\n            mapk.update(preds, target)\n            print(mapk.compute()) # tensor(0.6250)\n    \"\"\"\n\n    def __init__(self, k=10, dist_sync_on_step=False):\n        super().__init__(dist_sync_on_step=dist_sync_on_step)\n        self.k = k\n        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"count\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, predicted: torch.Tensor, actual: torch.Tensor):\n        \"\"\"\n        Update the state variables with a new batch of data.\n\n        Args:\n            predicted (torch.Tensor):\n                A 2D tensor containing the predicted scores for each class.\n            actual (torch.Tensor):\n                A 1D tensor containing the ground truth labels.\n        Returns:\n            (NoneType): None\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.torch.mapk import MAPK\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; mapk = MAPK(k=2)\n            &gt;&gt;&gt; target = torch.tensor([0, 1, 2, 2])\n            &gt;&gt;&gt; preds = torch.tensor(\n            ...     [\n            ...         [0.5, 0.2, 0.2],  # 0 is in top 2\n            ...         [0.3, 0.4, 0.2],  # 1 is in top 2\n            ...         [0.2, 0.4, 0.3],  # 2 is in top 2\n            ...         [0.7, 0.2, 0.1],  # 2 isn't in top 2\n            ...     ]\n            ... )\n            &gt;&gt;&gt; mapk.update(preds, target)\n            &gt;&gt;&gt; print(mapk.compute()) # tensor(0.6250)\n\n        Raises:\n            AssertionError: If the actual tensor is not 1D or the predicted tensor is not 2D.\n            AssertionError: If the number of elements in the actual and predicted tensors are not equal.\n\n        \"\"\"\n        assert len(actual.shape) == 1, \"actual must be a 1D tensor\"\n        assert len(predicted.shape) == 2, \"predicted must be a 2D tensor\"\n        assert actual.shape[0] == predicted.shape[0], \"actual and predicted must have the same number of elements\"\n\n        # Convert actual to list of lists\n        actual = actual.tolist()\n        actual = [[a] for a in actual]\n\n        # Convert predicted to list of lists of indices sorted by confidence score\n        _, predicted = predicted.topk(k=self.k, dim=1)\n        predicted = predicted.tolist()\n        # Code modified according to: \"Inplace update to inference tensor outside InferenceMode\n        # is not allowed. You can make a clone to get a normal tensor before doing inplace update.\"\n        score = np.mean([self.apk(p, a, self.k) for p, a in zip(predicted, actual)])\n        self.total = self.total + score\n        self.count = self.count + 1\n\n    def compute(self) -&gt; float:\n        \"\"\"\n        Compute the mean average precision at k.\n\n        Args:\n            self (MAPK):\n                The current instance of the class.\n\n        Returns:\n            (float):\n                The mean average precision at k.\n\n        Examples:\n            &gt;&gt;&gt; evaluator = Evaluator()\n            &gt;&gt;&gt; evaluator.total = 3.0\n            &gt;&gt;&gt; evaluator.count = 2\n            &gt;&gt;&gt; evaluator.compute()\n            1.5\n        \"\"\"\n        return self.total / self.count\n\n    @staticmethod\n    def apk(predicted: List[int], actual: List[int], k: int = 10) -&gt; float:\n        \"\"\"\n        Calculate the average precision at k for a single pair of actual and predicted labels.\n\n        Args:\n            predicted (list): A list of predicted labels.\n            actual (list): A list of ground truth labels.\n            k (int): The number of top predictions to consider.\n\n        Returns:\n            float: The average precision at k.\n\n        Examples:\n            &gt;&gt;&gt; Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3)\n            0.8888888888888888\n        \"\"\"\n        if not actual:\n            return 0.0\n\n        if len(predicted) &gt; k:\n            predicted = predicted[:k]\n\n        score = 0.0\n        num_hits = 0.0\n\n        for i, p in enumerate(predicted):\n            if p in actual and p not in predicted[:i]:\n                num_hits += 1.0\n                score += num_hits / (i + 1.0)\n\n        return score / min(len(actual), k)\n</code></pre>"},{"location":"reference/spotpython/torch/mapk/#spotpython.torch.mapk.MAPK.apk","title":"<code>apk(predicted, actual, k=10)</code>  <code>staticmethod</code>","text":"<p>Calculate the average precision at k for a single pair of actual and predicted labels.</p> <p>Parameters:</p> Name Type Description Default <code>predicted</code> <code>list</code> <p>A list of predicted labels.</p> required <code>actual</code> <code>list</code> <p>A list of ground truth labels.</p> required <code>k</code> <code>int</code> <p>The number of top predictions to consider.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The average precision at k.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3)\n0.8888888888888888\n</code></pre> Source code in <code>spotpython/torch/mapk.py</code> <pre><code>@staticmethod\ndef apk(predicted: List[int], actual: List[int], k: int = 10) -&gt; float:\n    \"\"\"\n    Calculate the average precision at k for a single pair of actual and predicted labels.\n\n    Args:\n        predicted (list): A list of predicted labels.\n        actual (list): A list of ground truth labels.\n        k (int): The number of top predictions to consider.\n\n    Returns:\n        float: The average precision at k.\n\n    Examples:\n        &gt;&gt;&gt; Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3)\n        0.8888888888888888\n    \"\"\"\n    if not actual:\n        return 0.0\n\n    if len(predicted) &gt; k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i, p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i + 1.0)\n\n    return score / min(len(actual), k)\n</code></pre>"},{"location":"reference/spotpython/torch/mapk/#spotpython.torch.mapk.MAPK.compute","title":"<code>compute()</code>","text":"<p>Compute the mean average precision at k.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>MAPK</code> <p>The current instance of the class.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean average precision at k.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; evaluator = Evaluator()\n&gt;&gt;&gt; evaluator.total = 3.0\n&gt;&gt;&gt; evaluator.count = 2\n&gt;&gt;&gt; evaluator.compute()\n1.5\n</code></pre> Source code in <code>spotpython/torch/mapk.py</code> <pre><code>def compute(self) -&gt; float:\n    \"\"\"\n    Compute the mean average precision at k.\n\n    Args:\n        self (MAPK):\n            The current instance of the class.\n\n    Returns:\n        (float):\n            The mean average precision at k.\n\n    Examples:\n        &gt;&gt;&gt; evaluator = Evaluator()\n        &gt;&gt;&gt; evaluator.total = 3.0\n        &gt;&gt;&gt; evaluator.count = 2\n        &gt;&gt;&gt; evaluator.compute()\n        1.5\n    \"\"\"\n    return self.total / self.count\n</code></pre>"},{"location":"reference/spotpython/torch/mapk/#spotpython.torch.mapk.MAPK.update","title":"<code>update(predicted, actual)</code>","text":"<p>Update the state variables with a new batch of data.</p> <p>Parameters:</p> Name Type Description Default <code>predicted</code> <code>Tensor</code> <p>A 2D tensor containing the predicted scores for each class.</p> required <code>actual</code> <code>Tensor</code> <p>A 1D tensor containing the ground truth labels.</p> required <p>Returns:     (NoneType): None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.torch.mapk import MAPK\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; mapk = MAPK(k=2)\n&gt;&gt;&gt; target = torch.tensor([0, 1, 2, 2])\n&gt;&gt;&gt; preds = torch.tensor(\n...     [\n...         [0.5, 0.2, 0.2],  # 0 is in top 2\n...         [0.3, 0.4, 0.2],  # 1 is in top 2\n...         [0.2, 0.4, 0.3],  # 2 is in top 2\n...         [0.7, 0.2, 0.1],  # 2 isn't in top 2\n...     ]\n... )\n&gt;&gt;&gt; mapk.update(preds, target)\n&gt;&gt;&gt; print(mapk.compute()) # tensor(0.6250)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the actual tensor is not 1D or the predicted tensor is not 2D.</p> <code>AssertionError</code> <p>If the number of elements in the actual and predicted tensors are not equal.</p> Source code in <code>spotpython/torch/mapk.py</code> <pre><code>def update(self, predicted: torch.Tensor, actual: torch.Tensor):\n    \"\"\"\n    Update the state variables with a new batch of data.\n\n    Args:\n        predicted (torch.Tensor):\n            A 2D tensor containing the predicted scores for each class.\n        actual (torch.Tensor):\n            A 1D tensor containing the ground truth labels.\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.torch.mapk import MAPK\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; mapk = MAPK(k=2)\n        &gt;&gt;&gt; target = torch.tensor([0, 1, 2, 2])\n        &gt;&gt;&gt; preds = torch.tensor(\n        ...     [\n        ...         [0.5, 0.2, 0.2],  # 0 is in top 2\n        ...         [0.3, 0.4, 0.2],  # 1 is in top 2\n        ...         [0.2, 0.4, 0.3],  # 2 is in top 2\n        ...         [0.7, 0.2, 0.1],  # 2 isn't in top 2\n        ...     ]\n        ... )\n        &gt;&gt;&gt; mapk.update(preds, target)\n        &gt;&gt;&gt; print(mapk.compute()) # tensor(0.6250)\n\n    Raises:\n        AssertionError: If the actual tensor is not 1D or the predicted tensor is not 2D.\n        AssertionError: If the number of elements in the actual and predicted tensors are not equal.\n\n    \"\"\"\n    assert len(actual.shape) == 1, \"actual must be a 1D tensor\"\n    assert len(predicted.shape) == 2, \"predicted must be a 2D tensor\"\n    assert actual.shape[0] == predicted.shape[0], \"actual and predicted must have the same number of elements\"\n\n    # Convert actual to list of lists\n    actual = actual.tolist()\n    actual = [[a] for a in actual]\n\n    # Convert predicted to list of lists of indices sorted by confidence score\n    _, predicted = predicted.topk(k=self.k, dim=1)\n    predicted = predicted.tolist()\n    # Code modified according to: \"Inplace update to inference tensor outside InferenceMode\n    # is not allowed. You can make a clone to get a normal tensor before doing inplace update.\"\n    score = np.mean([self.apk(p, a, self.k) for p, a in zip(predicted, actual)])\n    self.total = self.total + score\n    self.count = self.count + 1\n</code></pre>"},{"location":"reference/spotpython/torch/netcifar10/","title":"netcifar10","text":""},{"location":"reference/spotpython/torch/netcore/","title":"netcore","text":""},{"location":"reference/spotpython/torch/netfashionMNIST/","title":"netfashionMNIST","text":""},{"location":"reference/spotpython/torch/netregression/","title":"netregression","text":""},{"location":"reference/spotpython/torch/netvbdp/","title":"netvbdp","text":""},{"location":"reference/spotpython/torch/traintest/","title":"traintest","text":""},{"location":"reference/spotpython/utils/aggregate/","title":"aggregate","text":""},{"location":"reference/spotpython/utils/aggregate/#spotpython.utils.aggregate.aggregate_mean_var","title":"<code>aggregate_mean_var(X, y, sort=False)</code>","text":"<p>Aggregate array to mean.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array, shape <code>(n, k)</code>.</p> required <code>y</code> <code>ndarray</code> <p>values, shape <code>(n,)</code>.</p> required <code>sort</code> <code>bool</code> <p>Whether to sort the resulting DataFrame by the group keys.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>aggregated <code>X</code> values, shape <code>(n-m, k)</code>, if <code>m</code> duplicates in <code>X</code>.</p> <code>ndarray</code> <p>aggregated (mean per group) <code>y</code> values, shape <code>(1,)</code>, if <code>m</code> duplicates in <code>X</code>.</p> <code>ndarray</code> <p>aggregated (variance per group) <code>y</code> values, shape <code>(1,)</code>, if <code>m</code> duplicates in <code>X</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.aggregate import aggregate_mean_var\n    X = np.array([[1, 2], [3, 4], [1, 2]])\n    y = np.array([1, 2, 3])\n    X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n    print(X_agg)\n    [[1. 2.]\n    [3. 4.]]\n    print(y_mean)\n    [2. 2.]\n    print(y_var)\n    [1. 0.]\n</code></pre> Source code in <code>spotpython/utils/aggregate.py</code> <pre><code>def aggregate_mean_var(X, y, sort=False) -&gt; (np.ndarray, np.ndarray, np.ndarray):\n    \"\"\"\n    Aggregate array to mean.\n\n    Args:\n        X (numpy.ndarray): X array, shape `(n, k)`.\n        y (numpy.ndarray): values, shape `(n,)`.\n        sort (bool): Whether to sort the resulting DataFrame by the group keys.\n\n    Returns:\n        (numpy.ndarray):\n            aggregated `X` values, shape `(n-m, k)`, if `m` duplicates in `X`.\n        (numpy.ndarray):\n            aggregated (mean per group) `y` values, shape `(1,)`, if `m` duplicates in `X`.\n        (numpy.ndarray):\n            aggregated (variance per group) `y` values, shape `(1,)`, if `m` duplicates in `X`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.aggregate import aggregate_mean_var\n            X = np.array([[1, 2], [3, 4], [1, 2]])\n            y = np.array([1, 2, 3])\n            X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n            print(X_agg)\n            [[1. 2.]\n            [3. 4.]]\n            print(y_mean)\n            [2. 2.]\n            print(y_var)\n            [1. 0.]\n    \"\"\"\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"X and y must be numpy arrays.\")\n\n    if X.ndim != 2 or y.ndim != 1:\n        raise ValueError(\"X must be a 2D array and y must be a 1D array.\")\n\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"The number of rows in X must match the length of y.\")\n\n    # Create a DataFrame from X with y as the group target\n    df = pd.DataFrame(X)\n    df[\"y\"] = y\n\n    # Group by all X columns, calculating the mean and variance of y for each group\n    grouped = df.groupby(list(df.columns[:-1]), as_index=False, sort=sort).agg({\"y\": [\"mean\", \"var\"]})\n\n    # Extract mean and variance results from the multi-index DataFrame columns\n    y_mean = grouped[(\"y\", \"mean\")].to_numpy()\n    y_var = grouped[(\"y\", \"var\")].to_numpy()\n\n    # Extract the unique X values\n    X_agg = grouped.iloc[:, :-2].to_numpy()\n\n    return X_agg, y_mean, y_var\n</code></pre>"},{"location":"reference/spotpython/utils/aggregate/#spotpython.utils.aggregate.get_ranks","title":"<code>get_ranks(x)</code>","text":"<p>Returns a numpy array containing ranks of numbers within an input numpy array x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>numpy array</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ranks</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_ranks([2, 1])\n    [1, 0]\n&gt;&gt;&gt; get_ranks([20, 10, 100])\n    [1, 0, 2]\n</code></pre> Source code in <code>spotpython/utils/aggregate.py</code> <pre><code>def get_ranks(x):\n    \"\"\"\n    Returns a numpy array containing ranks of numbers within an input numpy array x.\n\n    Args:\n        x (numpy.ndarray): numpy array\n\n    Returns:\n        (numpy.ndarray): ranks\n\n    Examples:\n        &gt;&gt;&gt; get_ranks([2, 1])\n            [1, 0]\n        &gt;&gt;&gt; get_ranks([20, 10, 100])\n            [1, 0, 2]\n    \"\"\"\n    ts = x.argsort()\n    ranks = np.empty_like(ts)\n    ranks[ts] = np.arange(len(x))\n    return ranks\n</code></pre>"},{"location":"reference/spotpython/utils/aggregate/#spotpython.utils.aggregate.select_distant_points","title":"<code>select_distant_points(X, y, k)</code>","text":"<p>Selects k points that are distant from each other using a clustering approach.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array, shape <code>(n, k)</code>.</p> required <code>y</code> <code>ndarray</code> <p>values, shape <code>(n,)</code>.</p> required <code>k</code> <code>int</code> <p>number of points to select.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>selected <code>X</code> values, shape <code>(k, k)</code>.</p> <code>ndarray</code> <p>selected <code>y</code> values, shape <code>(k,)</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.aggregate import select_distant_points\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n    selected_points, selected_y = select_distant_points(X, y, 3)\n    print(selected_points)\n    [[1 2]\n    [7 8]\n    [9 10]]\n    print(selected_y)\n    [1 4 5]\n</code></pre> Source code in <code>spotpython/utils/aggregate.py</code> <pre><code>def select_distant_points(X, y, k):\n    \"\"\"\n    Selects k points that are distant from each other using a clustering approach.\n\n    Args:\n        X (numpy.ndarray): X array, shape `(n, k)`.\n        y (numpy.ndarray): values, shape `(n,)`.\n        k (int): number of points to select.\n\n    Returns:\n        (numpy.ndarray):\n            selected `X` values, shape `(k, k)`.\n        (numpy.ndarray):\n            selected `y` values, shape `(k,)`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.aggregate import select_distant_points\n            X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n            y = np.array([1, 2, 3, 4, 5])\n            selected_points, selected_y = select_distant_points(X, y, 3)\n            print(selected_points)\n            [[1 2]\n            [7 8]\n            [9 10]]\n            print(selected_y)\n            [1 4 5]\n\n    \"\"\"\n    # Perform k-means clustering to find k clusters\n    kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n    # Find the closest point in X to each cluster center\n    selected_points = np.array([X[np.argmin(np.linalg.norm(X - center, axis=1))] for center in kmeans.cluster_centers_])\n    # Find indices of the selected points in the original X array\n    indices = np.array([np.where(np.all(X == point, axis=1))[0][0] for point in selected_points])\n    # Select the corresponding y values\n    selected_y = y[indices]\n    return selected_points, selected_y\n</code></pre>"},{"location":"reference/spotpython/utils/classes/","title":"classes","text":""},{"location":"reference/spotpython/utils/compare/","title":"compare","text":""},{"location":"reference/spotpython/utils/compare/#spotpython.utils.compare.find_equal_in_lists","title":"<code>find_equal_in_lists(a, b)</code>","text":"<p>Find equal values in two lists.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>list</code> <p>list with a values</p> required <code>b</code> <code>list</code> <p>list with b values</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[int]</code> <p>list with 1 if equal, otherwise 0</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.compare import find_equal_in_lists\n    a = [1, 2, 3, 4, 5]\n    b = [1, 2, 3, 4, 5]\n    find_equal_in_lists(a, b)\n    [1, 1, 1, 1, 1]\n</code></pre> Source code in <code>spotpython/utils/compare.py</code> <pre><code>def find_equal_in_lists(a: List[int], b: List[int]) -&gt; List[int]:\n    \"\"\"Find equal values in two lists.\n\n    Args:\n        a (list): list with a values\n        b (list): list with b values\n\n    Returns:\n        list: list with 1 if equal, otherwise 0\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.compare import find_equal_in_lists\n            a = [1, 2, 3, 4, 5]\n            b = [1, 2, 3, 4, 5]\n            find_equal_in_lists(a, b)\n            [1, 1, 1, 1, 1]\n    \"\"\"\n    equal = [1 if a[i] == b[i] else 0 for i in range(len(a))]\n    return equal\n</code></pre>"},{"location":"reference/spotpython/utils/compare/#spotpython.utils.compare.selectNew","title":"<code>selectNew(A, X, tolerance=0)</code>","text":"<p>Select rows from A that are not in X.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>A array with new values</p> required <code>X</code> <code>ndarray</code> <p>X array with known values</p> required <code>tolerance</code> <code>float</code> <p>tolerance value for comparison</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array with unknown (new) values</p> <code>ndarray</code> <p>array with <code>True</code> if value is new, otherwise <code>False</code>.</p> <p>Examples:</p> <p>from spotpython.utils.compare import selectNew     import numpy as np     A = np.array([[1,2,3],[4,5,6]])     X = np.array([[1,2,3],[4,5,6]])     B, ind  = selectNew(A, X)     assert B.shape[0] == 0     assert np.equal(ind, np.array([False, False])).all() from spotpython.utils.compare import selectNew     A = np.array([[1,2,3],[4,5,7]])     X = np.array([[1,2,3],[4,5,6]])     B, ind  = selectNew(A, X)     assert B.shape[0] == 1     assert np.equal(ind, np.array([False, True])).all()</p> Source code in <code>spotpython/utils/compare.py</code> <pre><code>def selectNew(A: np.ndarray, X: np.ndarray, tolerance: float = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Select rows from A that are not in X.\n\n    Args:\n        A (numpy.ndarray): A array with new values\n        X (numpy.ndarray): X array with known values\n        tolerance (float): tolerance value for comparison\n\n    Returns:\n        (numpy.ndarray): array with unknown (new) values\n        (numpy.ndarray): array with `True` if value is new, otherwise `False`.\n\n    Examples:\n    &gt;&gt;&gt; from spotpython.utils.compare import selectNew\n        import numpy as np\n        A = np.array([[1,2,3],[4,5,6]])\n        X = np.array([[1,2,3],[4,5,6]])\n        B, ind  = selectNew(A, X)\n        assert B.shape[0] == 0\n        assert np.equal(ind, np.array([False, False])).all()\n    &gt;&gt;&gt; from spotpython.utils.compare import selectNew\n        A = np.array([[1,2,3],[4,5,7]])\n        X = np.array([[1,2,3],[4,5,6]])\n        B, ind  = selectNew(A, X)\n        assert B.shape[0] == 1\n        assert np.equal(ind, np.array([False, True])).all()\n    \"\"\"\n    B = np.abs(A[:, None] - X)\n    ind = np.any(np.all(B &lt;= tolerance, axis=2), axis=1)\n    return A[~ind], ~ind\n</code></pre>"},{"location":"reference/spotpython/utils/convert/","title":"convert","text":""},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.add_logical_columns","title":"<code>add_logical_columns(X, arity=2, operations=['and', 'or', 'xor'])</code>","text":"<p>Combines all features in a dataframe with each other using bitwise operations</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>dataframe with features</p> required <code>arity</code> <code>int</code> <p>the number of columns to combine at once</p> <code>2</code> <code>operations</code> <code>list of str</code> <p>the operations to apply. Possible values are \u2018and\u2019, \u2018or\u2019 and \u2018xor\u2019</p> <code>['and', 'or', 'xor']</code> <p>Returns:</p> Name Type Description <code>X</code> <code>DataFrame</code> <p>dataframe with new features</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n&gt;&gt;&gt; add_logical_columns(X)\n    a      b      c  a_and_b  a_and_c  b_and_c  a_or_b  a_or_c  b_or_c  a_xor_b  a_xor_c  b_xor_c\n0  True   True  False     True    False    False    True    True    True    False     True     True\n1 False   True  False    False    False    False    True   False    True     True     True    False\n2  True  False   True    False     True    False    True    True    True     True    False     True\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def add_logical_columns(X, arity=2, operations=[\"and\", \"or\", \"xor\"]):\n    \"\"\"Combines all features in a dataframe with each other using bitwise operations\n\n    Args:\n        X (pd.DataFrame): dataframe with features\n        arity (int): the number of columns to combine at once\n        operations (list of str): the operations to apply. Possible values are 'and', 'or' and 'xor'\n\n    Returns:\n        X (pd.DataFrame): dataframe with new features\n\n    Examples:\n        &gt;&gt;&gt; X = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n        &gt;&gt;&gt; add_logical_columns(X)\n            a      b      c  a_and_b  a_and_c  b_and_c  a_or_b  a_or_c  b_or_c  a_xor_b  a_xor_c  b_xor_c\n        0  True   True  False     True    False    False    True    True    True    False     True     True\n        1 False   True  False    False    False    False    True   False    True     True     True    False\n        2  True  False   True    False     True    False    True    True    True     True    False     True\n\n    \"\"\"\n    new_cols = []\n    # Iterate over all combinations of columns of the given arity\n    for cols in combinations(X.columns, arity):\n        # Create new columns for the specified operations\n        if \"and\" in operations:\n            and_col = X[list(cols)].apply(lambda x: x.all(), axis=1)\n            new_cols.append(and_col)\n        if \"or\" in operations:\n            or_col = X[list(cols)].apply(lambda x: x.any(), axis=1)\n            new_cols.append(or_col)\n        if \"xor\" in operations:\n            xor_col = X[list(cols)].apply(lambda x: x.sum() % 2 == 1, axis=1)\n            new_cols.append(xor_col)\n    # Join all the new columns at once\n    X = pd.concat([X] + new_cols, axis=1)\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.check_type","title":"<code>check_type(value)</code>","text":"<p>Check the type of the input value and return the type as a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>object</code> <p>The input value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The type of the input value as a string. Possible values are \u201cint\u201d, \u201cfloat\u201d, \u201cstr\u201d, \u201cbool\u201d, or None. Checks for numpy types as well, i.e., np.integer, np.floating, np.str_, np.bool_.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.convert import check_type\n&gt;&gt;&gt; check_type(5)\n\"int\"\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def check_type(value) -&gt; str:\n    \"\"\"Check the type of the input value and return the type as a string.\n\n    Args:\n        value (object): The input value.\n\n    Returns:\n        str:\n            The type of the input value as a string.\n            Possible values are \"int\", \"float\", \"str\", \"bool\", or None.\n            Checks for numpy types as well, i.e., np.integer, np.floating, np.str_, np.bool_.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.convert import check_type\n        &gt;&gt;&gt; check_type(5)\n        \"int\"\n\n    \"\"\"\n    if isinstance(value, (int, np.integer)):\n        return \"int\"\n    elif isinstance(value, (float, np.floating)):\n        return \"float\"\n    elif isinstance(value, (str, np.str_)):\n        return \"str\"\n    elif isinstance(value, (bool, np.bool_)):\n        return \"bool\"\n    else:\n        return None\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.class_for_name","title":"<code>class_for_name(module_name, class_name)</code>","text":"<p>Returns a class for a given module and class name.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>The name of the module.</p> required <code>class_name</code> <code>str</code> <p>The name of the class.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>The class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.convert import class_for_name\n    from scipy.optimize import rosen\n    bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n    shgo_class = class_for_name(\"scipy.optimize\", \"shgo\")\n    result = shgo_class(rosen, bounds)\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def class_for_name(module_name, class_name) -&gt; object:\n    \"\"\"Returns a class for a given module and class name.\n\n    Args:\n        module_name (str): The name of the module.\n        class_name (str): The name of the class.\n\n    Returns:\n        object: The class.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.convert import class_for_name\n            from scipy.optimize import rosen\n            bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n            shgo_class = class_for_name(\"scipy.optimize\", \"shgo\")\n            result = shgo_class(rosen, bounds)\n    \"\"\"\n    m = importlib.import_module(module_name)\n    c = getattr(m, class_name)\n    return c\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.get_Xy_from_df","title":"<code>get_Xy_from_df(df, target_column)</code>","text":"<p>Get X and y from a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataframe.</p> required <code>target_column</code> <code>str</code> <p>The name of the target column.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>The tuple (X, y).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.convert import get_Xy_from_df\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n&gt;&gt;&gt; X, y = get_Xy_from_df(df, \"c\")\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def get_Xy_from_df(df, target_column) -&gt; tuple:\n    \"\"\"Get X and y from a dataframe.\n\n    Args:\n        df (pandas.DataFrame): The input dataframe.\n        target_column (str): The name of the target column.\n\n    Returns:\n        tuple: The tuple (X, y).\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.convert import get_Xy_from_df\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n        &gt;&gt;&gt; X, y = get_Xy_from_df(df, \"c\")\n    \"\"\"\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    # convert to numpy arrays\n    X = X.to_numpy()\n    y = y.to_numpy()\n    return X, y\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.map_to_True_False","title":"<code>map_to_True_False(value)</code>","text":"<p>Map the string value to a boolean value. If the value is \u201cTrue\u201d or \u201ctrue\u201d, return True. Otherwise, return False.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string to be mapped to a boolean value.</p> required <p>Returns:     bool:         True if the value is \u201cTrue\u201d or \u201ctrue\u201d, False otherwise.</p> <p>Examples:</p> <p>from spotpython.utils.convert import map_to_True_False     map_to_True_False(\u201cTrue\u201d)     True</p> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def map_to_True_False(value):\n    \"\"\"\n    Map the string value to a boolean value.\n    If the value is \"True\" or \"true\", return True.\n    Otherwise, return False.\n\n    Args:\n        value (str):\n            The string to be mapped to a boolean value.\n    Returns:\n        bool:\n            True if the value is \"True\" or \"true\", False otherwise.\n\n    Examples:\n    &gt;&gt;&gt; from spotpython.utils.convert import map_to_True_False\n        map_to_True_False(\"True\")\n        True\n    \"\"\"\n    if value.lower() == \"true\":\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.series_to_array","title":"<code>series_to_array(series)</code>","text":"<p>Converts a pandas series to a numpy array. Args:     series (pandas.Series): The input series.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The output array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.convert import series_to_array\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; series = pd.Series([1, 2, 3])\n&gt;&gt;&gt; series_to_array(series)\narray([1, 2, 3])\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def series_to_array(series):\n    \"\"\"Converts a pandas series to a numpy array.\n    Args:\n        series (pandas.Series): The input series.\n\n    Returns:\n        (numpy.ndarray): The output array.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.convert import series_to_array\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; series = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; series_to_array(series)\n        array([1, 2, 3])\n    \"\"\"\n    if isinstance(series, np.ndarray):\n        return series\n    else:\n        return series.to_numpy()\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.set_dataset_target_type","title":"<code>set_dataset_target_type(dataset, target='y')</code>","text":"<p>Set the target column to 0 and 1 for boolean and string values.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>target</code> <code>str</code> <p>The name of the target column. Default is \u201cy\u201d.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The dataset with boolean and string target column values set to 0 and 1.</p> <p>Examples:</p> <p>from spotpython.utils.convert import set_dataset_target_type     import pandas as pd     dataset = pd.DataFrame({\u201ca\u201d: [1, 2, 3], \u201cb\u201d: [4, 5, 6], \u201cc\u201d: [7, 8, 9], \u201cy\u201d: [True, False, True]})     print(dataset)     dataset = set_dataset_target_type(dataset)     print(dataset)         a  b  c      y         0  1  4  7   True         1  2  5  8  False         2  3  6  9   True         a  b  c  y         0  1  4  7  1         1  2  5  8  0         2  3  6  9  1</p> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def set_dataset_target_type(dataset, target=\"y\") -&gt; pd.DataFrame:\n    \"\"\"Set the target column to 0 and 1 for boolean and string values.\n\n    Args:\n        dataset (pd.DataFrame): The input dataset.\n        target (str): The name of the target column. Default is \"y\".\n\n    Returns:\n        pd.DataFrame:\n            The dataset with boolean and string target column values set to 0 and 1.\n\n    Examples:\n    &gt;&gt;&gt; from spotpython.utils.convert import set_dataset_target_type\n        import pandas as pd\n        dataset = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9], \"y\": [True, False, True]})\n        print(dataset)\n        dataset = set_dataset_target_type(dataset)\n        print(dataset)\n            a  b  c      y\n            0  1  4  7   True\n            1  2  5  8  False\n            2  3  6  9   True\n            a  b  c  y\n            0  1  4  7  1\n            1  2  5  8  0\n            2  3  6  9  1\n\n\n    \"\"\"\n    val = copy.deepcopy(dataset.iloc[0, -1])\n    target_type = check_type(val)\n    if target_type == \"bool\" or target_type == \"str\":\n        # convert the target column to 0 and 1\n        dataset[target] = dataset[target].astype(int)\n    return dataset\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.sort_by_kth_and_return_indices","title":"<code>sort_by_kth_and_return_indices(array, k)</code>","text":"<p>Sorts an array of arrays based on the k-th values in descending order and returns the indices of the original array entries.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>list of lists</code> <p>The array to be sorted. Each sub-array should have at least <code>k+1</code> elements.</p> required <code>k</code> <code>int</code> <p>The index (zero-based) of the element within each sub-array to sort by.</p> required <p>Returns:</p> Type Description <code>list</code> <p>list of int: Indices of the original array entries after sorting by the k-th value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input array is empty, None, or any sub-array does not have at least <code>k+1</code> elements, or if k is out of bounds for any sub-array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.convert import sort_by_kth_and_return_indices\n    try:\n        array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n        k = 1  # Sort by the second element in each sub-array\n        indices = sort_by_kth_and_return_indices(array, k)\n        print(\"Indices of the sorted elements using the k-th value:\", indices)\n    except ValueError as error:\n        print(f\"Sorting failed due to: {error}\")\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def sort_by_kth_and_return_indices(array, k) -&gt; list:\n    \"\"\"Sorts an array of arrays based on the k-th values in descending order and returns\n    the indices of the original array entries.\n\n    Args:\n        array (list of lists): The array to be sorted. Each sub-array should have at least\n            `k+1` elements.\n        k (int): The index (zero-based) of the element within each sub-array to sort by.\n\n    Returns:\n        list of int: Indices of the original array entries after sorting by the k-th value.\n\n    Raises:\n        ValueError: If the input array is empty, None, or any sub-array does not have at least\n            `k+1` elements, or if k is out of bounds for any sub-array.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.convert import sort_by_kth_and_return_indices\n            try:\n                array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n                k = 1  # Sort by the second element in each sub-array\n                indices = sort_by_kth_and_return_indices(array, k)\n                print(\"Indices of the sorted elements using the k-th value:\", indices)\n            except ValueError as error:\n                print(f\"Sorting failed due to: {error}\")\n    \"\"\"\n    if not array:\n        return []\n\n    # Check for improperly structured sub-arrays and that k is within bounds\n    for item in array:\n        if not isinstance(item, list) or len(item) &lt;= k:\n            raise ValueError(\"All sub-arrays must be lists with at least k+1 elements.\")\n\n    # Enumerate the array to keep track of original indices, then sort by the k-th item\n    sorted_indices = [index for index, value in sorted(enumerate(array), key=lambda x: x[1][k], reverse=True)]\n\n    return sorted_indices\n</code></pre>"},{"location":"reference/spotpython/utils/device/","title":"device","text":""},{"location":"reference/spotpython/utils/device/#spotpython.utils.device.getDevice","title":"<code>getDevice(device=None)</code>","text":"<p>Get CPU, GPU (CUDA), or MPS device for training.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>Device for training. If None or \u201cauto\u201d, the device is selected automatically based on availability.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>device</code> <code>str</code> <p>Device for training.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested device is not recognized or available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.device import getDevice\n    getDevice()\n        'cuda:0'\n</code></pre> Source code in <code>spotpython/utils/device.py</code> <pre><code>def getDevice(device=None):\n    \"\"\"Get CPU, GPU (CUDA), or MPS device for training.\n\n    Args:\n        device (str):\n            Device for training. If None or \"auto\", the device is selected automatically based on availability.\n\n    Returns:\n        device (str):\n            Device for training.\n\n    Raises:\n        ValueError: If the requested device is not recognized or available.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.device import getDevice\n            getDevice()\n                'cuda:0'\n    \"\"\"\n    if device is None or device == \"auto\":\n        # Automatically select device\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        elif torch.backends.mps.is_available():\n            device = \"mps\"\n        return device\n\n    # Check the explicit device request\n    if device.startswith(\"cuda\"):\n        if not torch.cuda.is_available():\n            raise ValueError(\"CUDA device requested but no CUDA device is available.\")\n    elif device == \"mps\":\n        if not torch.backends.mps.is_available():\n            raise ValueError(\"MPS device requested but MPS is not available.\")\n    elif device == \"cpu\":\n        return \"cpu\"\n    else:\n        raise ValueError(f\"Unrecognized device: {device}. Valid options are 'cpu', 'cuda:x', or 'mps'.\")\n\n    return device\n</code></pre>"},{"location":"reference/spotpython/utils/eda/","title":"eda","text":""},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.compare_two_tree_models","title":"<code>compare_two_tree_models(model1, model2, headers=['Parameter', 'Default', 'Spot'])</code>","text":"<p>Compares two tree models. Args:     model1 (object):         A tree model.     model2 (object):         A tree model.     headers (list):         A list with the headers of the table.</p> <p>Returns:</p> Type Description <code>str</code> <p>A table with the comparison of the two models.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.eda import compare_two_tree_models\n&gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n&gt;&gt;&gt; fun_control = {\n...     \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n... }\n&gt;&gt;&gt; default_values = get_default_values(fun_control)\n&gt;&gt;&gt; model1 = spot_tuner.get_model(\"rf\", default_values)\n&gt;&gt;&gt; model2 = spot_tuner.get_model(\"rf\", default_values)\n&gt;&gt;&gt; compare_two_tree_models(model1, model2)\n</code></pre> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def compare_two_tree_models(model1, model2, headers=[\"Parameter\", \"Default\", \"Spot\"]):\n    \"\"\"Compares two tree models.\n    Args:\n        model1 (object):\n            A tree model.\n        model2 (object):\n            A tree model.\n        headers (list):\n            A list with the headers of the table.\n\n    Returns:\n        (str):\n            A table with the comparison of the two models.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.eda import compare_two_tree_models\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n        &gt;&gt;&gt; fun_control = {\n        ...     \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ... }\n        &gt;&gt;&gt; default_values = get_default_values(fun_control)\n        &gt;&gt;&gt; model1 = spot_tuner.get_model(\"rf\", default_values)\n        &gt;&gt;&gt; model2 = spot_tuner.get_model(\"rf\", default_values)\n        &gt;&gt;&gt; compare_two_tree_models(model1, model2)\n    \"\"\"\n    keys = model1.summary.keys()\n    values1 = model1.summary.values()\n    values2 = model2.summary.values()\n    tbl = []\n    for key, value1, value2 in zip(keys, values1, values2):\n        tbl.append([key, value1, value2])\n    return tabulate(tbl, headers=headers, numalign=\"right\", tablefmt=\"github\")\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.count_missing_data","title":"<code>count_missing_data(df)</code>","text":"<p>Counts the number of missing values in each column of the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data to be counted.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the number of missing values in each column.</p> Example <p>import pandas as pd df = pd.DataFrame({\u2018A\u2019: [1, 2, None], \u2018B\u2019: [4, None, 6], \u2018C\u2019: [7, 8, 9]}) count_missing_data(df)    column_name  missing_count 0           A              1 1           B              1</p> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def count_missing_data(df) -&gt; pd.DataFrame:\n    \"\"\"\n    Counts the number of missing values in each column of the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data to be counted.\n\n    Returns:\n        (pd.DataFrame): DataFrame containing the number of missing values in each column.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, None], 'B': [4, None, 6], 'C': [7, 8, 9]})\n        &gt;&gt;&gt; count_missing_data(df)\n           column_name  missing_count\n        0           A              1\n        1           B              1\n    \"\"\"\n    missing_df = df.isnull().sum(axis=0).reset_index()\n    missing_df.columns = [\"column_name\", \"missing_count\"]\n    missing_df = missing_df.loc[missing_df[\"missing_count\"] &gt; 0]\n    missing_df = missing_df.sort_values(by=\"missing_count\")\n\n    return missing_df\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.filter_highly_correlated","title":"<code>filter_highly_correlated(df, sorted=True, threshold=1 - 1e-05)</code>","text":"<p>Return a new DataFrame with only those columns that are highly correlated.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>threshold</code> <code>float</code> <p>The correlation threshold.</p> <code>1 - 1e-05</code> <code>sorted</code> <code>bool</code> <p>If True, the columns are sorted by name.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with only highly correlated columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    df = filter_highly_correlated(df, sorted=True, threshold=0.99)\n</code></pre> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def filter_highly_correlated(df: pd.DataFrame, sorted: bool = True, threshold: float = 1 - 1e-5) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new DataFrame with only those columns that are highly correlated.\n\n    Args:\n        df (DataFrame): The input DataFrame.\n        threshold (float): The correlation threshold.\n        sorted (bool): If True, the columns are sorted by name.\n\n    Returns:\n        DataFrame: A new DataFrame with only highly correlated columns.\n\n    Examples:\n        &gt;&gt;&gt; df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n            df = filter_highly_correlated(df, sorted=True, threshold=0.99)\n\n    \"\"\"\n    corr_matrix = df.corr()\n    # Find pairs of columns with correlation greater than threshold\n    corr_pairs = corr_matrix.abs().unstack()\n    corr_pairs = corr_pairs[corr_pairs &lt; 1]  # Remove self-correlations\n    high_corr = corr_pairs[corr_pairs &gt; threshold]\n    high_corr = high_corr[high_corr &lt; 1]  # Remove self-correlations\n\n    # Get the column names of highly correlated columns\n    high_corr_cols = list(set([col[0] for col in high_corr.index]))\n\n    # Create new DataFrame with only highly correlated columns\n    new_df = df[high_corr_cols]\n    # sort the columns by name\n    if sorted:\n        new_df = new_df.sort_index(axis=1)\n\n    return new_df\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.gen_design_table","title":"<code>gen_design_table(fun_control, spot=None, tablefmt='github')</code>","text":"<p>Generates a table with the design variables and their bounds. Args:     fun_control (dict):         A dictionary with function design variables.     spot (object):         A spot object. Defaults to None. Returns:     (str):         a table with the design variables, their default values, and their bounds.         If a spot object is provided,         the table will also include the value and the importance of each hyperparameter.         Use the <code>print</code> function to display the table.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.eda import gen_design_table\n&gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n&gt;&gt;&gt; fun_control = {\n...     \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n... }\n</code></pre> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def gen_design_table(fun_control: dict, spot: object = None, tablefmt=\"github\") -&gt; str:\n    \"\"\"Generates a table with the design variables and their bounds.\n    Args:\n        fun_control (dict):\n            A dictionary with function design variables.\n        spot (object):\n            A spot object. Defaults to None.\n    Returns:\n        (str):\n            a table with the design variables, their default values, and their bounds.\n            If a spot object is provided,\n            the table will also include the value and the importance of each hyperparameter.\n            Use the `print` function to display the table.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.eda import gen_design_table\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n        &gt;&gt;&gt; fun_control = {\n        ...     \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ... }\n    \"\"\"\n    default_values = get_default_values(fun_control)\n    defaults = list(default_values.values())\n    if spot is None:\n        tab = tabulate(\n            {\n                \"name\": get_var_name(fun_control),\n                \"type\": get_var_type(fun_control),\n                \"default\": defaults,\n                \"lower\": get_bound_values(fun_control, \"lower\", as_list=True),\n                \"upper\": get_bound_values(fun_control, \"upper\", as_list=True),\n                \"transform\": get_transform(fun_control),\n            },\n            headers=\"keys\",\n            tablefmt=tablefmt,\n        )\n    else:\n        res = spot.print_results(print_screen=False, dict=fun_control)\n        tuned = [item[1] for item in res]\n        # imp = spot.print_importance(threshold=0.0, print_screen=False)\n        # importance = [item[1] for item in imp]\n        importance = spot.get_importance()\n        stars = get_stars(importance)\n        tab = tabulate(\n            {\n                \"name\": get_var_name(fun_control),\n                \"type\": get_var_type(fun_control),\n                \"default\": defaults,\n                \"lower\": get_bound_values(fun_control, \"lower\", as_list=True),\n                \"upper\": get_bound_values(fun_control, \"upper\", as_list=True),\n                \"tuned\": tuned,\n                \"transform\": get_transform(fun_control),\n                \"importance\": importance,\n                \"stars\": stars,\n            },\n            headers=\"keys\",\n            numalign=\"right\",\n            floatfmt=(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \".2f\"),\n            tablefmt=tablefmt,\n        )\n    return tab\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.generate_config_id","title":"<code>generate_config_id(config, hash=False, timestamp=False)</code>","text":"<p>Generates a unique id for a configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary with the configuration.</p> required <code>hash</code> <code>bool</code> <p>If True, the id is hashed.</p> <code>False</code> <code>timestamp</code> <code>bool</code> <p>If True, the id is appended with a timestamp. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>A unique id for the configuration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_one_config_from_X\n&gt;&gt;&gt; X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n&gt;&gt;&gt; config = get_one_config_from_X(X, fun_control)\n&gt;&gt;&gt; generate_config_id(config)\n</code></pre> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def generate_config_id(config, hash=False, timestamp=False):\n    \"\"\"Generates a unique id for a configuration.\n\n    Args:\n        config (dict):\n            A dictionary with the configuration.\n        hash (bool):\n            If True, the id is hashed.\n        timestamp (bool):\n            If True, the id is appended with a timestamp. Defaults to False.\n\n    Returns:\n        (str):\n            A unique id for the configuration.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_one_config_from_X\n        &gt;&gt;&gt; X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n        &gt;&gt;&gt; config = get_one_config_from_X(X, fun_control)\n        &gt;&gt;&gt; generate_config_id(config)\n    \"\"\"\n    config_id = \"\"\n    for key in config:\n        # if config[key] is a number, round it to 4 digits after the decimal point\n        if isinstance(config[key], float):\n            config_id += str(round(config[key], 4)) + \"_\"\n        else:\n            config_id += str(config[key]) + \"_\"\n    # hash the config_id to make it shorter and unique\n    if hash:\n        config_id = str(hash(config_id)) + \"_\"\n    # remove () and , from the string\n    config_id = config_id.replace(\"(\", \"\")\n    config_id = config_id.replace(\")\", \"\")\n    config_id = config_id.replace(\",\", \"\")\n    config_id = config_id.replace(\" \", \"\")\n    config_id = config_id.replace(\":\", \"\")\n    if timestamp:\n        config_id = get_timestamp(only_int=True) + \"_\" + config_id\n    return config_id[:-1]\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.get_stars","title":"<code>get_stars(input_list)</code>","text":"<p>Converts a list of values to a list of stars, which can be used to     visualize the importance of a variable.</p> <p>Parameters:</p> Name Type Description Default <code>input_list</code> <code>list</code> <p>A list of values.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of strings.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.eda import convert_list\n&gt;&gt;&gt; get_stars([100, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n[***, '', '', '', '', '', '', '', '']\n</code></pre> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def get_stars(input_list) -&gt; list:\n    \"\"\"Converts a list of values to a list of stars, which can be used to\n        visualize the importance of a variable.\n\n    Args:\n        input_list (list): A list of values.\n\n    Returns:\n        (list):\n            A list of strings.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.eda import convert_list\n        &gt;&gt;&gt; get_stars([100, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n        [***, '', '', '', '', '', '', '', '']\n    \"\"\"\n    output_list = []\n    for value in input_list:\n        if value &gt; 95:\n            output_list.append(\"***\")\n        elif value &gt; 50:\n            output_list.append(\"**\")\n        elif value &gt; 1:\n            output_list.append(\"*\")\n        elif value &gt; 0.1:\n            output_list.append(\".\")\n        else:\n            output_list.append(\"\")\n    return output_list\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.plot_missing_data","title":"<code>plot_missing_data(df, relative=False, figsize=(7, 5), color='grey', xlabel='Missing Data', title='Missing Data')</code>","text":"<p>Plots a horizontal bar chart of the number of missing values in each column of the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data to be plotted.</p> required <code>relative</code> <code>bool</code> <p>Whether to plot relative values (percentage) or absolute values.</p> <code>False</code> <code>figsize</code> <code>tuple</code> <p>Size of the figure to be plotted.</p> <code>(7, 5)</code> <code>color</code> <code>str</code> <p>Color of the bars in the bar chart.</p> <code>'grey'</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis.</p> <code>'Missing Data'</code> <code>title</code> <code>str</code> <p>Title for the plot.</p> <code>'Missing Data'</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Example <p>import pandas as pd df = pd.DataFrame({\u2018A\u2019: [1, 2, np.nan], \u2018B\u2019: [4, np.nan, 6], \u2018C\u2019: [7, 8, 9]}) plot_missing_data(df)</p> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def plot_missing_data(df, relative=False, figsize=(7, 5), color=\"grey\", xlabel=\"Missing Data\", title=\"Missing Data\") -&gt; None:\n    \"\"\"\n    Plots a horizontal bar chart of the number of missing values in each column of the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data to be plotted.\n        relative (bool): Whether to plot relative values (percentage) or absolute values.\n        figsize (tuple): Size of the figure to be plotted.\n        color (str): Color of the bars in the bar chart.\n        xlabel (str): Label for the x-axis.\n        title (str): Title for the plot.\n\n    Returns:\n        (NoneType): None\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, np.nan], 'B': [4, np.nan, 6], 'C': [7, 8, 9]})\n        &gt;&gt;&gt; plot_missing_data(df)\n    \"\"\"\n    missing_df = count_missing_data(df)\n\n    if relative:\n        missing_df[\"missing_count\"] = missing_df[\"missing_count\"] / df.shape[0]\n        xlabel = \"Percentage of \" + xlabel\n        title = \"Percentage of \" + title\n\n    ind = np.arange(missing_df.shape[0])\n    _, ax = plt.subplots(figsize=figsize)\n    _ = ax.barh(ind, missing_df.missing_count.values, color=color)\n    ax.set_yticks(ind)\n    ax.set_yticklabels(missing_df.column_name.values, rotation=\"horizontal\")\n    ax.set_xlabel(xlabel)\n    ax.set_title(title)\n    plt.vlines(1, 0, missing_df.shape[0])\n    plt.vlines(0.97, 0, missing_df.shape[0])\n    plt.vlines(0.5, 0, missing_df.shape[0])\n    plt.show()\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.plot_sns_heatmap","title":"<code>plot_sns_heatmap(df_heat, figsize=(16, 12), cmap='vlag', vmin=-1, vmax=1, annot=True, fmt='.5f', linewidths=0.5, annot_kws={'size': 8})</code>","text":"<p>Plots a heatmap of the correlation matrix of the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df_heat</code> <code>DataFrame</code> <p>DataFrame containing the data to be plotted.</p> required <code>figsize</code> <code>tuple</code> <p>Size of the figure to be plotted.</p> <code>(16, 12)</code> <code>cmap</code> <code>str</code> <p>Color map to be used for the heatmap.</p> <code>'vlag'</code> <code>vmin</code> <code>int</code> <p>Minimum value for the color scale.</p> <code>-1</code> <code>vmax</code> <code>int</code> <p>Maximum value for the color scale.</p> <code>1</code> <code>annot</code> <code>bool</code> <p>Whether to display annotations on the heatmap.</p> <code>True</code> <code>fmt</code> <code>str</code> <p>Format string for annotations.</p> <code>'.5f'</code> <code>linewidths</code> <code>float</code> <p>Width of lines separating cells in the heatmap.</p> <code>0.5</code> <code>annot_kws</code> <code>dict</code> <p>Keyword arguments for annotations.</p> <code>{'size': 8}</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Example <p>import pandas as pd df = pd.DataFrame({\u2018A\u2019: [1, 2, 3], \u2018B\u2019: [4, 5, 6], \u2018C\u2019: [7, 8, 9]}) plot_heatmap(df)</p> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def plot_sns_heatmap(\n    df_heat,\n    figsize=(16, 12),\n    cmap=\"vlag\",\n    vmin=-1,\n    vmax=1,\n    annot=True,\n    fmt=\".5f\",\n    linewidths=0.5,\n    annot_kws={\"size\": 8},\n) -&gt; None:\n    \"\"\"\n    Plots a heatmap of the correlation matrix of the given DataFrame.\n\n    Args:\n        df_heat (pd.DataFrame): DataFrame containing the data to be plotted.\n        figsize (tuple): Size of the figure to be plotted.\n        cmap (str): Color map to be used for the heatmap.\n        vmin (int): Minimum value for the color scale.\n        vmax (int): Maximum value for the color scale.\n        annot (bool): Whether to display annotations on the heatmap.\n        fmt (str): Format string for annotations.\n        linewidths (float): Width of lines separating cells in the heatmap.\n        annot_kws (dict): Keyword arguments for annotations.\n\n    Returns:\n        (NoneType): None\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n        &gt;&gt;&gt; plot_heatmap(df)\n    \"\"\"\n    plt.figure(figsize=figsize)\n    matrix = np.triu(np.ones_like(df_heat.corr()))\n    sns.heatmap(\n        data=df_heat.corr(),\n        cmap=cmap,\n        vmin=vmin,\n        vmax=vmax,\n        annot=annot,\n        fmt=fmt,\n        linewidths=linewidths,\n        annot_kws=annot_kws,\n        mask=matrix,\n    )\n    plt.show()\n    plt.gcf().clear()\n</code></pre>"},{"location":"reference/spotpython/utils/file/","title":"file","text":""},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.get_experiment_filename","title":"<code>get_experiment_filename(PREFIX)</code>","text":"<p>Returns the name of the experiment.</p> <p>Parameters:</p> Name Type Description Default <code>PREFIX</code> <code>str</code> <p>Prefix of the experiment.</p> required <p>Returns:</p> Name Type Description <code>filename</code> <code>str</code> <p>Name of the experiment.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import get_experiment_name\n&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n&gt;&gt;&gt; fun_control = fun_control_init(PREFIX=\"branin\")\n&gt;&gt;&gt; PREFIX = fun_control[\"PREFIX\"]\n&gt;&gt;&gt; filename = get_experiment_filename(PREFIX)\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def get_experiment_filename(PREFIX):\n    \"\"\"Returns the name of the experiment.\n\n    Args:\n        PREFIX (str): Prefix of the experiment.\n\n    Returns:\n        filename (str): Name of the experiment.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import get_experiment_name\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n        &gt;&gt;&gt; fun_control = fun_control_init(PREFIX=\"branin\")\n        &gt;&gt;&gt; PREFIX = fun_control[\"PREFIX\"]\n        &gt;&gt;&gt; filename = get_experiment_filename(PREFIX)\n    \"\"\"\n    if PREFIX is None:\n        return None\n    else:\n        filename = \"spot_\" + PREFIX + \"_experiment.pickle\"\n    return filename\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.get_experiment_from_PREFIX","title":"<code>get_experiment_from_PREFIX(PREFIX, return_dict=True)</code>","text":"<p>Setup the experiment based on the PREFIX provided and return the relevant configuration and control objects.</p> <p>Parameters:</p> Name Type Description Default <code>PREFIX</code> <code>str</code> <p>The prefix for the experiment filename.</p> required <code>return_dict</code> <code>bool</code> <p>Whether to return the configuration and control objects as a dictionary. If False, a tuple is returned: \u201c(config, fun_control, design_control, surrogate_control, optimizer_control).\u201d Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the configuration and control objects.</p> Example <p>from spotpython.utils.file import get_experiment_from_PREFIX config = get_experiment_from_PREFIX(\u201c100\u201d)[\u201cconfig\u201d]</p> Source code in <code>spotpython/utils/file.py</code> <pre><code>def get_experiment_from_PREFIX(PREFIX, return_dict=True) -&gt; dict:\n    \"\"\"\n    Setup the experiment based on the PREFIX provided and return the relevant configuration\n    and control objects.\n\n    Args:\n        PREFIX (str):\n            The prefix for the experiment filename.\n        return_dict (bool, optional):\n            Whether to return the configuration and control objects as a dictionary.\n            If False, a tuple is returned:\n            \"(config, fun_control, design_control, surrogate_control, optimizer_control).\"\n            Defaults to True.\n\n    Returns:\n        dict: Dictionary containing the configuration and control objects.\n\n    Example:\n        &gt;&gt;&gt; from spotpython.utils.file import get_experiment_from_PREFIX\n        &gt;&gt;&gt; config = get_experiment_from_PREFIX(\"100\")[\"config\"]\n\n    \"\"\"\n    experiment_name = get_experiment_filename(PREFIX)\n    spot_tuner, fun_control, design_control, surrogate_control, optimizer_control = load_experiment(experiment_name)\n    config = get_tuned_architecture(spot_tuner, fun_control)\n    if return_dict:\n        return {\n            \"config\": config,\n            \"fun_control\": fun_control,\n            \"design_control\": design_control,\n            \"surrogate_control\": surrogate_control,\n            \"optimizer_control\": optimizer_control,\n        }\n    else:\n        return config, fun_control, design_control, surrogate_control, optimizer_control\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_and_run_spot_python_experiment","title":"<code>load_and_run_spot_python_experiment(spot_pkl_name)</code>","text":"<p>Loads and runs a spot experiment.</p> <p>Parameters:</p> Name Type Description Default <code>spot_pkl_name</code> <code>str</code> <p>The name of the spot experiment file.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the spot tuner, fun control,    design control, surrogate control, optimizer control,    and the tensorboard process object (p_popen).</p> Notes <p>p_open is deprecated and should be removed in future versions. It returns None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import load_and_run_spot_python_experiment\n&gt;&gt;&gt; spot_tuner = load_and_run_spot_python_experiment(\"spot_branin_experiment.pickle\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_and_run_spot_python_experiment(spot_pkl_name) -&gt; tuple:\n    \"\"\"Loads and runs a spot experiment.\n\n    Args:\n        spot_pkl_name (str):\n            The name of the spot experiment file.\n\n    Returns:\n        tuple: A tuple containing the spot tuner, fun control,\n               design control, surrogate control, optimizer control,\n               and the tensorboard process object (p_popen).\n\n    Notes:\n        p_open is deprecated and should be removed in future versions.\n        It returns None.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import load_and_run_spot_python_experiment\n        &gt;&gt;&gt; spot_tuner = load_and_run_spot_python_experiment(\"spot_branin_experiment.pickle\")\n\n    \"\"\"\n    p_open = None\n    (spot_tuner, fun_control, design_control, surrogate_control, optimizer_control) = load_experiment(spot_pkl_name)\n    print(\"\\nLoaded fun_control in spotRun():\")\n    # pprint.pprint(fun_control)\n    print(gen_design_table(fun_control))\n    setup_paths(fun_control[\"TENSORBOARD_CLEAN\"])\n    spot_tuner.init_spot_writer()\n    # if fun_control[\"tensorboard_start\"]:\n    #     p_open = start_tensorboard()\n    # else:\n    #     p_open = None\n    spot_tuner.run()\n    # # tensorboard --logdir=\"runs/\"\n    # stop_tensorboard(p_open)\n    print(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n    return spot_tuner, fun_control, design_control, surrogate_control, optimizer_control, p_open\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_cifar10_data","title":"<code>load_cifar10_data(data_dir='./data')</code>","text":"<p>Loads the CIFAR10 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Directory to save the data. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <p>Returns:</p> Name Type Description <code>trainset</code> <code>CIFAR10</code> <p>Training dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import load_cifar10_data\n&gt;&gt;&gt; trainset = load_cifar10_data(data_dir=\"./data\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_cifar10_data(data_dir=\"./data\"):\n    \"\"\"Loads the CIFAR10 dataset.\n\n    Args:\n        data_dir (str, optional): Directory to save the data. Defaults to \"./data\".\n\n    Returns:\n        trainset (torchvision.datasets.CIFAR10): Training dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import load_cifar10_data\n        &gt;&gt;&gt; trainset = load_cifar10_data(data_dir=\"./data\")\n\n    \"\"\"\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n    testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n    return trainset, testset\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_core_model_from_file","title":"<code>load_core_model_from_file(coremodel, dirname='userModel')</code>","text":"<p>Loads a core model from a python file.</p> <p>Parameters:</p> Name Type Description Default <code>coremodel</code> <code>str</code> <p>Name of the core model.</p> required <code>dirname</code> <code>str</code> <p>Directory name. Defaults to \u201cuserModel\u201d.</p> <code>'userModel'</code> <p>Returns:</p> Name Type Description <code>coremodel</code> <code>object</code> <p>Core model.</p> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_core_model_from_file(coremodel, dirname=\"userModel\"):\n    \"\"\"Loads a core model from a python file.\n\n    Args:\n        coremodel (str): Name of the core model.\n        dirname (str, optional): Directory name. Defaults to \"userModel\".\n\n    Returns:\n        coremodel (object): Core model.\n\n    \"\"\"\n    sys.path.insert(0, \"./\" + dirname)\n    module = importlib.import_module(coremodel)\n    core_model = getattr(module, coremodel)\n    return core_model\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_dict_from_file","title":"<code>load_dict_from_file(coremodel, dirname='userModel')</code>","text":"<p>Loads a dictionary from a json file.</p> <p>Parameters:</p> Name Type Description Default <code>coremodel</code> <code>str</code> <p>Name of the core model.</p> required <code>dirname</code> <code>str</code> <p>Directory name. Defaults to \u201cuserModel\u201d.</p> <code>'userModel'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with the core model.</p> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_dict_from_file(coremodel, dirname=\"userModel\"):\n    \"\"\"Loads a dictionary from a json file.\n\n    Args:\n        coremodel (str): Name of the core model.\n        dirname (str, optional): Directory name. Defaults to \"userModel\".\n\n    Returns:\n        dict (dict): Dictionary with the core model.\n\n    \"\"\"\n    file_path = os.path.join(dirname, f\"{coremodel}.json\")\n    if os.path.isfile(file_path):\n        with open(file_path, \"r\") as f:\n            dict_tmp = json.load(f)\n            dict = dict_tmp[coremodel]\n    else:\n        print(f\"The file {file_path} does not exist.\")\n        dict = None\n    return dict\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_experiment","title":"<code>load_experiment(PKL_NAME=None, PREFIX=None)</code>","text":"<p>Loads the experiment from a pickle file. If PKL_NAME is None and PREFIX is not None, the experiment is loaded based on the PREFIX using the get_experiment_filename function. If the spot tuner object and the fun control dictionary do not exist, an error is thrown. If the design control, surrogate control, and optimizer control dictionaries do not exist, a warning is issued and <code>None</code> is assigned to the corresponding variables.</p> <p>Parameters:</p> Name Type Description Default <code>PKL_NAME</code> <code>str</code> <p>Name of the pickle file. Defaults to None.</p> <code>None</code> <code>PREFIX</code> <code>str</code> <p>Prefix of the experiment. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>spot_tuner</code> <code>object</code> <p>The spot tuner object.</p> <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>design_control</code> <code>dict</code> <p>The design control dictionary.</p> <code>surrogate_control</code> <code>dict</code> <p>The surrogate control dictionary.</p> <code>optimizer_control</code> <code>dict</code> <p>The optimizer control dictionary.</p> Notes <p>The corresponding save_experiment function is part of the class spot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import load_experiment\n&gt;&gt;&gt; spot_tuner, fun_control, design_control, _, _ = load_experiment(\"spot_0_experiment.pickle\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_experiment(PKL_NAME=None, PREFIX=None):\n    \"\"\"\n    Loads the experiment from a pickle file.\n    If PKL_NAME is None and PREFIX is not None, the experiment is loaded based on the PREFIX\n    using the get_experiment_filename function.\n    If the spot tuner object and the fun control dictionary do not exist, an error is thrown.\n    If the design control, surrogate control, and optimizer control dictionaries do not exist, a warning is issued\n    and `None` is assigned to the corresponding variables.\n\n    Args:\n        PKL_NAME (str): Name of the pickle file. Defaults to None.\n        PREFIX (str, optional): Prefix of the experiment. Defaults to None.\n\n    Returns:\n        spot_tuner (object): The spot tuner object.\n        fun_control (dict): The function control dictionary.\n        design_control (dict): The design control dictionary.\n        surrogate_control (dict): The surrogate control dictionary.\n        optimizer_control (dict): The optimizer control dictionary.\n\n    Notes:\n        The corresponding save_experiment function is part of the class spot.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import load_experiment\n        &gt;&gt;&gt; spot_tuner, fun_control, design_control, _, _ = load_experiment(\"spot_0_experiment.pickle\")\n\n    \"\"\"\n    if PKL_NAME is None and PREFIX is not None:\n        PKL_NAME = get_experiment_filename(PREFIX)\n    with open(PKL_NAME, \"rb\") as handle:\n        experiment = pickle.load(handle)\n        print(f\"Loaded experiment from {PKL_NAME}\")\n    # assign spot_tuner and fun_control only if they exist otherwise throw an error\n    if \"spot_tuner\" not in experiment:\n        raise ValueError(\"The spot tuner object does not exist in the pickle file.\")\n    if \"fun_control\" not in experiment:\n        raise ValueError(\"The fun control dictionary does not exist in the pickle file.\")\n    spot_tuner = experiment[\"spot_tuner\"]\n    fun_control = experiment[\"fun_control\"]\n    # assign the rest of the dictionaries if they exist otherwise assign None\n    if \"design_control\" not in experiment:\n        design_control = None\n        # issue a warning\n        print(\"The design control dictionary does not exist in the pickle file. Returning None.\")\n    else:\n        design_control = experiment[\"design_control\"]\n    if \"surrogate_control\" not in experiment:\n        surrogate_control = None\n        # issue a warning\n        print(\"The surrogate control dictionary does not exist in the pickle file. Returning None.\")\n    else:\n        surrogate_control = experiment[\"surrogate_control\"]\n    if \"optimizer_control\" not in experiment:\n        # issue a warning\n        print(\"The optimizer control dictionary does not exist in the pickle file. Returning None.\")\n        optimizer_control = None\n    else:\n        optimizer_control = experiment[\"optimizer_control\"]\n    return spot_tuner, fun_control, design_control, surrogate_control, optimizer_control\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_pickle","title":"<code>load_pickle(filename)</code>","text":"<p>Loads a pickle file.     Add .pkl to the filename.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the pickle file.</p> required <p>Returns:</p> Type Description <code>object</code> <p>Loaded object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import load_pickle\n&gt;&gt;&gt; obj = load_pickle(filename=\"obj.pkl\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_pickle(filename: str):\n    \"\"\"Loads a pickle file.\n        Add .pkl to the filename.\n\n    Args:\n        filename (str): Name of the pickle file.\n\n    Returns:\n        (object): Loaded object.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import load_pickle\n        &gt;&gt;&gt; obj = load_pickle(filename=\"obj.pkl\")\n    \"\"\"\n    filename = filename + \".pkl\"\n    with open(filename, \"rb\") as f:\n        obj = pickle.load(f)\n    return obj\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.save_pickle","title":"<code>save_pickle(obj, filename)</code>","text":"<p>Saves an object as a pickle file.     Add .pkl to the filename.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>Object to be saved.</p> required <code>filename</code> <code>str</code> <p>Name of the pickle file.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import save_pickle\n&gt;&gt;&gt; save_pickle(obj, filename=\"obj.pkl\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def save_pickle(obj, filename: str):\n    \"\"\"Saves an object as a pickle file.\n        Add .pkl to the filename.\n\n    Args:\n        obj (object): Object to be saved.\n        filename (str): Name of the pickle file.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import save_pickle\n        &gt;&gt;&gt; save_pickle(obj, filename=\"obj.pkl\")\n    \"\"\"\n    filename = filename + \".pkl\"\n    with open(filename, \"wb\") as f:\n        pickle.dump(obj, f)\n</code></pre>"},{"location":"reference/spotpython/utils/init/","title":"init","text":""},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.X_reshape","title":"<code>X_reshape(X)</code>","text":"<p>Reshape X to 2D array.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The input array.</p> required <p>Returns:</p> Name Type Description <code>X</code> <code>array</code> <p>The reshaped input array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPy.utils.init import X_reshape\n&gt;&gt;&gt; X = np.array([1,2,3])\n&gt;&gt;&gt; X_reshape(X)\narray([[1, 2, 3]])\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def X_reshape(X) -&gt; np.array:\n    \"\"\"Reshape X to 2D array.\n\n    Args:\n        X (np.array):\n            The input array.\n\n    Returns:\n        X (np.array):\n            The reshaped input array.\n\n    Examples:\n        &gt;&gt;&gt; from spotPy.utils.init import X_reshape\n        &gt;&gt;&gt; X = np.array([1,2,3])\n        &gt;&gt;&gt; X_reshape(X)\n        array([[1, 2, 3]])\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    X = np.atleast_2d(X)\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.check_and_create_dir","title":"<code>check_and_create_dir(path)</code>","text":"<p>Check if the path exists and create it if it does not.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the directory.</p> required <p>Returns:</p> Type Description <code>noneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fromspotPy.utils.init import check_and_create_dir\n&gt;&gt;&gt; check_and_create_dir(\"data/\")\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def check_and_create_dir(path) -&gt; None:\n    \"\"\"Check if the path exists and create it if it does not.\n\n    Args:\n        path (str): Path to the directory.\n\n    Returns:\n        (noneType): None\n\n    Examples:\n        &gt;&gt;&gt; fromspotPy.utils.init import check_and_create_dir\n        &gt;&gt;&gt; check_and_create_dir(\"data/\")\n    \"\"\"\n    if not isinstance(path, str):\n        raise Exception(\"path must be a string\")\n    if not os.path.exists(path):\n        os.makedirs(path)\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.create_spot_tensorboard_path","title":"<code>create_spot_tensorboard_path(tensorboard_log, prefix)</code>","text":"<p>Creates the spot_tensorboard_path and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>tensorboard_log</code> <code>bool</code> <p>If True, the path to the folder where the tensorboard files are saved is created.</p> required <code>prefix</code> <code>str</code> <p>The prefix for the experiment name.</p> required <p>Returns:</p> Name Type Description <code>spot_tensorboard_path</code> <code>str</code> <p>The path to the folder where the tensorboard files are saved.</p> Source code in <code>spotpython/utils/init.py</code> <pre><code>def create_spot_tensorboard_path(tensorboard_log, prefix) -&gt; str:\n    \"\"\"Creates the spot_tensorboard_path and returns it.\n\n    Args:\n        tensorboard_log (bool):\n            If True, the path to the folder where the tensorboard files are saved is created.\n        prefix (str):\n            The prefix for the experiment name.\n\n    Returns:\n        spot_tensorboard_path (str):\n            The path to the folder where the tensorboard files are saved.\n    \"\"\"\n    if tensorboard_log:\n        experiment_name = get_experiment_name(prefix=prefix)\n        spot_tensorboard_path = get_spot_tensorboard_path(experiment_name)\n        os.makedirs(spot_tensorboard_path, exist_ok=True)\n        print(f\"Created spot_tensorboard_path: {spot_tensorboard_path} for SummaryWriter()\")\n    else:\n        spot_tensorboard_path = None\n    return spot_tensorboard_path\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.design_control_init","title":"<code>design_control_init(init_size=10, repeats=1)</code>","text":"<p>Initialize design_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>init_size</code> <code>int</code> <p>The initial size of the experimental design.</p> <code>10</code> <code>repeats</code> <code>int</code> <p>The number of repeats of the design.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>design_control</code> <code>dict</code> <p>A dictionary containing the information about the design of experiments.</p> Source code in <code>spotpython/utils/init.py</code> <pre><code>def design_control_init(init_size=10, repeats=1) -&gt; dict:\n    \"\"\"Initialize design_control dictionary.\n\n    Args:\n        init_size (int): The initial size of the experimental design.\n        repeats (int): The number of repeats of the design.\n\n    Returns:\n        design_control (dict):\n            A dictionary containing the information about the design of experiments.\n\n    \"\"\"\n    design_control = {\"init_size\": init_size, \"repeats\": repeats}\n    return design_control\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.fun_control_init","title":"<code>fun_control_init(_L_in=None, _L_out=None, _L_cond=None, _torchmetric=None, PREFIX='00', TENSORBOARD_CLEAN=False, accelerator='auto', converters=None, core_model=None, core_model_name=None, data=None, data_dir='./data', data_module=None, data_set=None, data_set_name=None, db_dict_name=None, design=None, device=None, devices='auto', enable_progress_bar=False, EXPERIMENT_NAME=None, eval=None, fun_evals=15, fun_repeats=1, horizon=None, hyperdict=None, infill_criterion='y', log_every_n_steps=50, log_level=50, lower=None, max_time=1, max_surrogate_points=30, metric_sklearn=None, metric_sklearn_name=None, noise=False, n_points=1, n_samples=None, num_sanity_val_steps=2, n_total=None, num_workers=0, num_nodes=1, ocba_delta=0, oml_grace_period=None, optimizer=None, precision='32', prep_model=None, prep_model_name=None, progress_file=None, save_experiment=False, scaler=None, scaler_name=None, scenario=None, seed=123, show_config=False, show_models=False, show_progress=True, shuffle=None, sigma=0.0, strategy='auto', surrogate=None, target_column=None, target_type=None, task=None, tensorboard_log=False, tensorboard_start=False, tensorboard_stop=False, test=None, test_seed=1234, test_size=0.4, tkagg=False, train=None, tolerance_x=0, upper=None, var_name=None, var_type=['num'], verbosity=0, weights=1.0, weight_coeff=0.0, weights_entry=None)</code>","text":"<p>Initialize fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>None</code> <code>_L_out</code> <code>int</code> <p>The number of output features.</p> <code>None</code> <code>_L_cond</code> <code>int</code> <p>The number of conditional features.</p> <code>None</code> <code>_torchmetric</code> <code>str</code> <p>The metric to be used by the Lighting Trainer. For example \u201cmean_squared_error\u201d, see https://lightning.ai/docs/torchmetrics/stable/regression/mean_squared_error.html</p> <code>None</code> <code>accelerator</code> <code>str</code> <p>The accelerator to be used by the Lighting Trainer. It can be either \u201cauto\u201d, \u201cdp\u201d, \u201cddp\u201d, \u201cddp2\u201d, \u201cddp_spawn\u201d, \u201cddp_cpu\u201d, \u201cgpu\u201d, \u201ctpu\u201d. Default is \u201cauto\u201d.</p> <code>'auto'</code> <code>converters</code> <code>dict</code> <p>A dictionary containing the converters. Default is None.</p> <code>None</code> <code>core_model</code> <code>object</code> <p>The core model object. Default is None.</p> <code>None</code> <code>core_model_name</code> <code>str</code> <p>The name of the core model. Default is None.</p> <code>None</code> <code>data</code> <code>object</code> <p>The data object. Default is None.</p> <code>None</code> <code>data_dir</code> <code>str</code> <p>The directory to save the data. Default is \u201c./data\u201d.</p> <code>'./data'</code> <code>data_module</code> <code>object</code> <p>The data module object. Default is None.</p> <code>None</code> <code>data_set</code> <code>object</code> <p>The data set object. Default is None.</p> <code>None</code> <code>data_set_name</code> <code>str</code> <p>The name of the data set. Default is None.</p> <code>None</code> <code>db_dict_name</code> <code>str</code> <p>The name of the database dictionary. Default is None.</p> <code>None</code> <code>device</code> <code>str</code> <p>The device to use for the training. It can be either \u201ccpu\u201d, \u201cmps\u201d, or \u201ccuda\u201d.</p> <code>None</code> <code>devices</code> <code>str or int</code> <p>The number of devices to use for the training/validation/testing. Default is 1. Can be \u201cauto\u201d or an integer.</p> <code>'auto'</code> <code>design</code> <code>object</code> <p>The experimental design object. Default is None.</p> <code>None</code> <code>enable_progress_bar</code> <code>bool</code> <p>Whether to enable the progress bar or not.</p> <code>False</code> <code>eval</code> <code>str</code> <p>evaluation method used in sklearn taintest.py. Can be \u201ceval_test\u201d, \u201ceval_oon_score\u201d, \u201ctrain_cv\u201d or None. Default is None.</p> <code>None</code> <code>EXPERIMENT_NAME</code> <code>str</code> <p>The name of the experiment. Default is None. If None, the experiment name is generated based on the current date and time.</p> <code>None</code> <code>fun_evals</code> <code>int</code> <p>The number of function evaluations.</p> <code>15</code> <code>fun_repeats</code> <code>int</code> <p>The number of function repeats during the optimization. this value does not affect the number of the repeats in the initial design (this value can be set in the design_control). Default is 1.</p> <code>1</code> <code>horizon</code> <code>int</code> <p>The horizon of the time series data. Default is None.</p> <code>None</code> <code>hyperdict</code> <code>dict</code> <p>A dictionary containing the hyperparameters. Default is None. For example: <code>spotriver.hyperdict.river_hyper_dict import RiverHyperDict</code></p> <code>None</code> <code>infill_criterion</code> <code>str</code> <p>Can be <code>\"y\"</code>, <code>\"s\"</code>, <code>\"ei\"</code> (negative expected improvement), or <code>\"all\"</code>. Default is \u201cy\u201d.</p> <code>'y'</code> <code>log_every_n_steps</code> <code>int</code> <p>Lightning: How often to log within steps. Default: 50.</p> <code>50</code> <code>log_level</code> <code>int</code> <p>log level with the following settings: <code>NOTSET</code> (<code>0</code>), <code>DEBUG</code> (<code>10</code>: Detailed information, typically of interest only when diagnosing problems.), <code>INFO</code> (<code>20</code>: Confirmation that things are working as expected.), <code>WARNING</code> (<code>30</code>: An indication that something unexpected happened, or indicative of some problem in the near     future (e.g. \u2018disk space low\u2019). The software is still working as expected.), <code>ERROR</code> (<code>40</code>: Due to a more serious problem, the software has not been able to perform some function.), and <code>CRITICAL</code> (<code>50</code>: A serious error, indicating that the program itself may be unable to continue running.)</p> <code>50</code> <code>lower</code> <code>array</code> <p>lower bound</p> <code>None</code> <code>max_time</code> <code>int</code> <p>The maximum time in minutes.</p> <code>1</code> <code>max_surrogate_points</code> <code>int</code> <p>The maximum number of points in the surrogate model. Default is inf.</p> <code>30</code> <code>metric_sklearn</code> <code>object</code> <p>The metric object from the scikit-learn library. Default is None.</p> <code>None</code> <code>metric_sklearn_name</code> <code>str</code> <p>The name of the metric object from the scikit-learn library. Default is None.</p> <code>None</code> <code>noise</code> <code>bool</code> <p>Whether the objective function is noiy or not. Default is False. Affects the repeat of the function evaluations.</p> <code>False</code> <code>n_points</code> <code>int</code> <p>The number of infill points to be generated by the surrogate in each iteration.</p> <code>1</code> <code>num_sanity_val_steps</code> <code>int</code> <pre><code>Lightning: Sanity check runs n validation batches before starting the training routine.\nSet it to -1 to run all batches in all validation dataloaders.\nDefault: 2.\n</code></pre> <code>2</code> <code>n_samples</code> <code>int</code> <p>The number of samples in the dataset. Default is None.</p> <code>None</code> <code>n_total</code> <code>int</code> <p>The total number of samples in the dataset. Default is None.</p> <code>None</code> <code>num_nodes</code> <code>int</code> <p>The number of GPU nodes to use for the training/validation/testing. Default is 1.</p> <code>1</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for the data loading. Default is 0.</p> <code>0</code> <code>ocba_delta</code> <code>int</code> <p>The number of additional, new points (only used if noise==True) generated by the OCBA infill criterion. Default is 0.</p> <code>0</code> <code>oml_grace_period</code> <code>int</code> <p>The grace period for the OML algorithm. Default is None.</p> <code>None</code> <code>optimizer</code> <code>object</code> <p>The optimizer object used for the search on surrogate. Default is None.</p> <code>None</code> <code>precision</code> <code>str</code> <p>The precision of the data. Default is \u201c32\u201d. Can be e.g., \u201c16-mixed\u201d or \u201c16-true\u201d.</p> <code>'32'</code> <code>PREFIX</code> <code>str</code> <p>The prefix of the experiment name. If the PREFIX is not None, a spotWriter that us an instance of a SummaryWriter(), is created. Default is \u201c00\u201d.</p> <code>'00'</code> <code>prep_model</code> <code>object</code> <p>The preprocessing model object. Used for river. Default is None.</p> <code>None</code> <code>prep_model_name</code> <code>str</code> <p>The name of the preprocessing model. Default is None.</p> <code>None</code> <code>progress_file</code> <code>str</code> <p>The name of the progress file. Default is None.</p> <code>None</code> <code>save_experiment</code> <code>bool</code> <p>Whether to save the experiment or not. Default is False.</p> <code>False</code> <code>scaler</code> <code>object</code> <p>The scaler object, e.g., the TorchStandard scaler from spot.utils.scaler.py. Default is None.</p> <code>None</code> <code>scaler_name</code> <code>str</code> <p>The name of the scaler object. Default is None.</p> <code>None</code> <code>scenario</code> <code>str</code> <p>The scenario to use. Default is None. Can be \u201criver\u201d, \u201csklearn\u201d, or \u201clightning\u201d.</p> <code>None</code> <code>seed</code> <code>int</code> <p>The seed to use for the random number generator. Default is 123.</p> <code>123</code> <code>sigma</code> <code>float</code> <p>The standard deviation of the noise of the objective function.</p> <code>0.0</code> <code>show_progress</code> <code>bool</code> <p>Whether to show the progress or not. Default is <code>True</code>.</p> <code>True</code> <code>show_models</code> <code>bool</code> <p>Plot model each generation. Currently only 1-dim functions are supported. Default is <code>False</code>.</p> <code>False</code> <code>show_config</code> <code>bool</code> <p>Whether to show the configuration or not. Default is <code>False</code>.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>Whether the data were shuffled or not. Default is None.</p> <code>None</code> <code>surrogate</code> <code>object</code> <p>The surrogate model object. Default is None.</p> <code>None</code> <code>strategy</code> <code>str</code> <p>The strategy to use. Default is \u201cauto\u201d.</p> <code>'auto'</code> <code>target_column</code> <code>str</code> <p>The name of the target column. Default is None.</p> <code>None</code> <code>target_type</code> <code>str</code> <p>The type of the target column. Default is None.</p> <code>None</code> <code>task</code> <code>str</code> <p>The task to perform. It can be either \u201cclassification\u201d or \u201cregression\u201d. Default is None.</p> <code>None</code> <code>TENSORBOARD_CLEAN</code> <code>bool</code> <p>Whether to clean (delete) the tensorboard folder or not. Default is False.</p> <code>False</code> <code>tensorboard_log</code> <code>bool</code> <p>Whether to log the tensorboard or not. Starts the SummaryWriter. Default is False.</p> <code>False</code> <code>tensorboard_start</code> <code>bool</code> <p>Whether to start the tensorboard or not. Default is False.</p> <code>False</code> <code>tensorboard_stop</code> <code>bool</code> <p>Whether to stop the tensorboard or not. Default is False.</p> <code>False</code> <code>test</code> <code>object</code> <p>The test data set for spotriver. Default is None.</p> <code>None</code> <code>test_seed</code> <code>int</code> <p>The seed to use for the test set. Default is 1234.</p> <code>1234</code> <code>test_size</code> <code>float</code> <p>The size of the test set. Default is 0.4, i.e., 60% of the data is used for training and 40% for testing.</p> <code>0.4</code> <code>tkagg</code> <code>bool</code> <p>Whether to use matplotlib TkAgg or not. Default is False.</p> <code>False</code> <code>tolerance_x</code> <code>float</code> <p>tolerance for new x solutions. Minimum distance of new solutions, generated by <code>suggest_new_X</code>, to already existing solutions. If zero (which is the default), every new solution is accepted.</p> <code>0</code> <code>train</code> <code>object</code> <p>The training data set for spotriver. Default is None.</p> <code>None</code> <code>upper</code> <code>array</code> <p>upper bound</p> <code>None</code> <code>var_name</code> <code>list</code> <p>A list containing the name of the variables, e.g., [\u201cx1\u201d, \u201cx2\u201d]. Default is None.</p> <code>None</code> <code>var_type</code> <code>List[str]</code> <p>list of type information, can be either \u201cint\u201d, \u201cnum\u201d or \u201cfactor\u201d. Default is [\u201cnum\u201d].</p> <code>['num']</code> <code>verbosity</code> <code>int</code> <p>The verbosity level. Determines print output to console. Higher values result in more output. Default is 0.</p> <code>0</code> <code>weights</code> <code>float</code> <p>The weight coefficient of the objective function. Positive values mean minimization. If set to -1, scores that are better when maximized will be minimized, e.g, accuracy. Can be an array, so that different weights can be used for different (multiple) objectives. Default is 1.0.</p> <code>1.0</code> <code>weight_coeff</code> <code>float</code> <p>Determines how to weight older measures. Default is 1.0. Used in the OML algorithm eval_oml.py. Default is 0.0.</p> <code>0.0</code> <code>weights_entry</code> <code>str</code> <p>The weights entry used in the GUI. Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fun_control</code> <code>dict</code> <p>A dictionary containing the information about the core model, loss function, metrics, and the hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n    fun_control\n    {'CHECKPOINT_PATH': 'saved_models/',\n        'DATASET_PATH': 'data/',\n        'RESULTS_PATH': 'results/',\n        'TENSORBOARD_PATH': 'runs/',\n        '_L_in': 64,\n        '_L_out': 11,\n        '_L_cond': None,\n        'accelerator': \"auto\",\n        'core_model': None,\n        'core_model_name': None,\n        'data': None,\n        'data_dir': './data',\n        'db_dict_name': None,\n        'device': None,\n        'devices': \"auto\",\n        'enable_progress_bar': False,\n        'eval': None,\n        'horizon': 7,\n        'infill_criterion': 'y',\n        'k_folds': None,\n        'loss_function': None,\n        'lower': None,\n        'max_surrogate_points': 100,\n        'metric_river': None,\n        'metric_sklearn': None,\n        'metric_sklearn_name': None,\n        'metric_torch': None,\n        'metric_params': {},\n        'model_dict': {},\n        'noise': False,\n        'n_points': 1,\n        'n_samples': None,\n        'num_workers': 0,\n        'ocba_delta': 0,\n        'oml_grace_period': None,\n        'optimizer': None,\n        'path': None,\n        'prep_model': None,\n        'prep_model_name': None,\n        'save_model': False,\n        'scenario': \"lightning\",\n        'seed': 1234,\n        'show_batch_interval': 1000000,\n        'shuffle': None,\n        'sigma': 0.0,\n        'target_column': None,\n        'target_type': None,\n        'train': None,\n        'test': None,\n        'task': 'classification',\n        'tensorboard_path': None,\n        'upper': None,\n        'weights': 1.0,\n        'writer': None}\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def fun_control_init(\n    _L_in=None,\n    _L_out=None,\n    _L_cond=None,\n    _torchmetric=None,\n    PREFIX=\"00\",\n    TENSORBOARD_CLEAN=False,\n    accelerator=\"auto\",\n    converters=None,\n    core_model=None,\n    core_model_name=None,\n    data=None,\n    data_dir=\"./data\",\n    data_module=None,\n    data_set=None,\n    data_set_name=None,\n    db_dict_name=None,\n    design=None,\n    device=None,\n    devices=\"auto\",\n    enable_progress_bar=False,\n    EXPERIMENT_NAME=None,\n    eval=None,\n    fun_evals=15,\n    fun_repeats=1,\n    horizon=None,\n    hyperdict=None,\n    infill_criterion=\"y\",\n    log_every_n_steps=50,\n    log_level=50,\n    lower=None,\n    max_time=1,\n    max_surrogate_points=30,\n    metric_sklearn=None,\n    metric_sklearn_name=None,\n    noise=False,\n    n_points=1,\n    n_samples=None,\n    num_sanity_val_steps=2,\n    n_total=None,\n    num_workers=0,\n    num_nodes=1,\n    ocba_delta=0,\n    oml_grace_period=None,\n    optimizer=None,\n    precision=\"32\",\n    prep_model=None,\n    prep_model_name=None,\n    progress_file=None,\n    save_experiment=False,\n    scaler=None,\n    scaler_name=None,\n    scenario=None,\n    seed=123,\n    show_config=False,\n    show_models=False,\n    show_progress=True,\n    shuffle=None,\n    sigma=0.0,\n    strategy=\"auto\",\n    surrogate=None,\n    target_column=None,\n    target_type=None,\n    task=None,\n    tensorboard_log=False,\n    tensorboard_start=False,\n    tensorboard_stop=False,\n    test=None,\n    test_seed=1234,\n    test_size=0.4,\n    tkagg=False,\n    train=None,\n    tolerance_x=0,\n    upper=None,\n    var_name=None,\n    var_type=[\"num\"],\n    verbosity=0,\n    weights=1.0,\n    weight_coeff=0.0,\n    weights_entry=None,\n):\n    \"\"\"Initialize fun_control dictionary.\n\n    Args:\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output features.\n        _L_cond (int):\n            The number of conditional features.\n        _torchmetric (str):\n            The metric to be used by the Lighting Trainer.\n            For example \"mean_squared_error\",\n            see https://lightning.ai/docs/torchmetrics/stable/regression/mean_squared_error.html\n        accelerator (str):\n            The accelerator to be used by the Lighting Trainer.\n            It can be either \"auto\", \"dp\", \"ddp\", \"ddp2\", \"ddp_spawn\", \"ddp_cpu\", \"gpu\", \"tpu\".\n            Default is \"auto\".\n        converters (dict):\n            A dictionary containing the converters. Default is None.\n        core_model (object):\n            The core model object. Default is None.\n        core_model_name (str):\n            The name of the core model. Default is None.\n        data (object):\n            The data object. Default is None.\n        data_dir (str):\n            The directory to save the data. Default is \"./data\".\n        data_module (object):\n            The data module object. Default is None.\n        data_set (object):\n            The data set object. Default is None.\n        data_set_name (str):\n            The name of the data set. Default is None.\n        db_dict_name (str):\n            The name of the database dictionary. Default is None.\n        device (str):\n            The device to use for the training. It can be either \"cpu\", \"mps\", or \"cuda\".\n        devices (str or int):\n            The number of devices to use for the training/validation/testing.\n            Default is 1. Can be \"auto\" or an integer.\n        design (object):\n            The experimental design object. Default is None.\n        enable_progress_bar (bool):\n            Whether to enable the progress bar or not.\n        eval (str):\n            evaluation method used in sklearn taintest.py.\n            Can be \"eval_test\", \"eval_oon_score\", \"train_cv\" or None. Default is None.\n        EXPERIMENT_NAME (str):\n            The name of the experiment.\n            Default is None. If None, the experiment name is generated based on the\n            current date and time.\n        fun_evals (int):\n            The number of function evaluations.\n        fun_repeats (int):\n            The number of function repeats during the optimization. this value does not affect\n            the number of the repeats in the initial design (this value can be set in the\n            design_control). Default is 1.\n        horizon (int):\n            The horizon of the time series data. Default is None.\n        hyperdict (dict):\n            A dictionary containing the hyperparameters. Default is None.\n            For example: `spotriver.hyperdict.river_hyper_dict import RiverHyperDict`\n        infill_criterion (str):\n            Can be `\"y\"`, `\"s\"`, `\"ei\"` (negative expected improvement), or `\"all\"`. Default is \"y\".\n        log_every_n_steps (int):\n            Lightning: How often to log within steps. Default: 50.\n        log_level (int):\n            log level with the following settings:\n            `NOTSET` (`0`),\n            `DEBUG` (`10`: Detailed information, typically of interest only when diagnosing problems.),\n            `INFO` (`20`: Confirmation that things are working as expected.),\n            `WARNING` (`30`: An indication that something unexpected happened, or indicative of some problem in the near\n                future (e.g. \u2018disk space low\u2019). The software is still working as expected.),\n            `ERROR` (`40`: Due to a more serious problem, the software has not been able to perform some function.), and\n            `CRITICAL` (`50`: A serious error, indicating that the program itself may be unable to continue running.)\n        lower (np.array):\n            lower bound\n        max_time (int):\n            The maximum time in minutes.\n        max_surrogate_points (int):\n            The maximum number of points in the surrogate model. Default is inf.\n        metric_sklearn (object):\n            The metric object from the scikit-learn library. Default is None.\n        metric_sklearn_name (str):\n            The name of the metric object from the scikit-learn library. Default is None.\n        noise (bool):\n            Whether the objective function is noiy or not. Default is False.\n            Affects the repeat of the function evaluations.\n        n_points (int):\n            The number of infill points to be generated by the surrogate in each iteration.\n        num_sanity_val_steps (int):\n                Lightning: Sanity check runs n validation batches before starting the training routine.\n                Set it to -1 to run all batches in all validation dataloaders.\n                Default: 2.\n        n_samples (int):\n            The number of samples in the dataset. Default is None.\n        n_total (int):\n            The total number of samples in the dataset. Default is None.\n        num_nodes (int):\n            The number of GPU nodes to use for the training/validation/testing. Default is 1.\n        num_workers (int):\n            The number of workers to use for the data loading. Default is 0.\n        ocba_delta (int):\n            The number of additional, new points (only used if noise==True) generated by\n            the OCBA infill criterion. Default is 0.\n        oml_grace_period (int):\n            The grace period for the OML algorithm. Default is None.\n        optimizer (object):\n            The optimizer object used for the search on surrogate. Default is None.\n        precision (str):\n            The precision of the data. Default is \"32\". Can be e.g., \"16-mixed\" or \"16-true\".\n        PREFIX (str):\n            The prefix of the experiment name. If the PREFIX is not None, a spotWriter\n            that us an instance of a SummaryWriter(), is created. Default is \"00\".\n        prep_model (object):\n            The preprocessing model object. Used for river. Default is None.\n        prep_model_name (str):\n            The name of the preprocessing model. Default is None.\n        progress_file (str):\n            The name of the progress file. Default is None.\n        save_experiment (bool):\n            Whether to save the experiment or not. Default is False.\n        scaler (object):\n            The scaler object, e.g., the TorchStandard scaler from spot.utils.scaler.py.\n            Default is None.\n        scaler_name (str):\n            The name of the scaler object. Default is None.\n        scenario (str):\n            The scenario to use. Default is None. Can be \"river\", \"sklearn\", or \"lightning\".\n        seed (int):\n            The seed to use for the random number generator. Default is 123.\n        sigma (float):\n            The standard deviation of the noise of the objective function.\n        show_progress (bool):\n            Whether to show the progress or not. Default is `True`.\n        show_models (bool):\n            Plot model each generation.\n            Currently only 1-dim functions are supported. Default is `False`.\n        show_config (bool):\n            Whether to show the configuration or not. Default is `False`.\n        shuffle (bool):\n            Whether the data were shuffled or not. Default is None.\n        surrogate (object):\n            The surrogate model object. Default is None.\n        strategy (str):\n            The strategy to use. Default is \"auto\".\n        target_column (str):\n            The name of the target column. Default is None.\n        target_type (str):\n            The type of the target column. Default is None.\n        task (str):\n            The task to perform. It can be either \"classification\" or \"regression\".\n            Default is None.\n        TENSORBOARD_CLEAN (bool):\n            Whether to clean (delete) the tensorboard folder or not. Default is False.\n        tensorboard_log (bool):\n            Whether to log the tensorboard or not. Starts the SummaryWriter.\n            Default is False.\n        tensorboard_start (bool):\n            Whether to start the tensorboard or not. Default is False.\n        tensorboard_stop (bool):\n            Whether to stop the tensorboard or not. Default is False.\n        test (object):\n            The test data set for spotriver. Default is None.\n        test_seed (int):\n            The seed to use for the test set. Default is 1234.\n        test_size (float):\n            The size of the test set. Default is 0.4, i.e.,\n            60% of the data is used for training and 40% for testing.\n        tkagg (bool):\n            Whether to use matplotlib TkAgg or not. Default is False.\n        tolerance_x (float):\n            tolerance for new x solutions. Minimum distance of new solutions,\n            generated by `suggest_new_X`, to already existing solutions.\n            If zero (which is the default), every new solution is accepted.\n        train (object):\n            The training data set for spotriver. Default is None.\n        upper (np.array):\n            upper bound\n        var_name (list):\n            A list containing the name of the variables, e.g., [\"x1\", \"x2\"]. Default is None.\n        var_type (List[str]):\n            list of type information, can be either \"int\", \"num\" or \"factor\".\n            Default is [\"num\"].\n        verbosity (int):\n            The verbosity level. Determines print output to console. Higher values\n            result in more output. Default is 0.\n        weights (float):\n            The weight coefficient of the objective function. Positive values mean minimization.\n            If set to -1, scores that are better when maximized will be minimized, e.g, accuracy.\n            Can be an array, so that different weights can be used for different (multiple) objectives.\n            Default is 1.0.\n        weight_coeff (float):\n            Determines how to weight older measures. Default is 1.0. Used in the OML algorithm eval_oml.py.\n            Default is 0.0.\n        weights_entry (str):\n            The weights entry used in the GUI. Default is None.\n\n    Returns:\n        fun_control (dict):\n            A dictionary containing the information about the core model,\n            loss function, metrics, and the hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n            fun_control\n            {'CHECKPOINT_PATH': 'saved_models/',\n                'DATASET_PATH': 'data/',\n                'RESULTS_PATH': 'results/',\n                'TENSORBOARD_PATH': 'runs/',\n                '_L_in': 64,\n                '_L_out': 11,\n                '_L_cond': None,\n                'accelerator': \"auto\",\n                'core_model': None,\n                'core_model_name': None,\n                'data': None,\n                'data_dir': './data',\n                'db_dict_name': None,\n                'device': None,\n                'devices': \"auto\",\n                'enable_progress_bar': False,\n                'eval': None,\n                'horizon': 7,\n                'infill_criterion': 'y',\n                'k_folds': None,\n                'loss_function': None,\n                'lower': None,\n                'max_surrogate_points': 100,\n                'metric_river': None,\n                'metric_sklearn': None,\n                'metric_sklearn_name': None,\n                'metric_torch': None,\n                'metric_params': {},\n                'model_dict': {},\n                'noise': False,\n                'n_points': 1,\n                'n_samples': None,\n                'num_workers': 0,\n                'ocba_delta': 0,\n                'oml_grace_period': None,\n                'optimizer': None,\n                'path': None,\n                'prep_model': None,\n                'prep_model_name': None,\n                'save_model': False,\n                'scenario': \"lightning\",\n                'seed': 1234,\n                'show_batch_interval': 1000000,\n                'shuffle': None,\n                'sigma': 0.0,\n                'target_column': None,\n                'target_type': None,\n                'train': None,\n                'test': None,\n                'task': 'classification',\n                'tensorboard_path': None,\n                'upper': None,\n                'weights': 1.0,\n                'writer': None}\n    \"\"\"\n    # Setting the seed\n    L.seed_everything(seed)\n\n    CHECKPOINT_PATH, DATASET_PATH, RESULTS_PATH, TENSORBOARD_PATH = setup_paths(TENSORBOARD_CLEAN)\n    spot_tensorboard_path = create_spot_tensorboard_path(tensorboard_log, PREFIX)\n\n    if metric_sklearn is None and metric_sklearn_name is not None:\n        metric_sklearn = get_metric_sklearn(metric_sklearn_name)\n\n    fun_control = {\n        \"PREFIX\": PREFIX,\n        \"CHECKPOINT_PATH\": CHECKPOINT_PATH,\n        \"DATASET_PATH\": DATASET_PATH,\n        \"RESULTS_PATH\": RESULTS_PATH,\n        \"TENSORBOARD_PATH\": TENSORBOARD_PATH,\n        \"TENSORBOARD_CLEAN\": TENSORBOARD_CLEAN,\n        \"_L_in\": _L_in,\n        \"_L_out\": _L_out,\n        \"_L_cond\": _L_cond,\n        \"_torchmetric\": _torchmetric,\n        \"accelerator\": accelerator,\n        \"converters\": converters,\n        \"core_model\": core_model,\n        \"core_model_name\": core_model_name,\n        \"counter\": 0,\n        \"data\": data,\n        \"data_dir\": data_dir,\n        \"data_module\": data_module,\n        \"data_set\": data_set,\n        \"data_set_name\": data_set_name,\n        \"db_dict_name\": db_dict_name,\n        \"design\": design,\n        \"device\": device,\n        \"devices\": devices,\n        \"enable_progress_bar\": enable_progress_bar,\n        \"eval\": eval,\n        \"fun_evals\": fun_evals,\n        \"fun_repeats\": fun_repeats,\n        \"horizon\": horizon,\n        \"hyperdict\": hyperdict,\n        \"infill_criterion\": infill_criterion,\n        \"k_folds\": 3,\n        \"log_every_n_steps\": log_every_n_steps,\n        \"log_graph\": False,\n        \"log_level\": log_level,\n        \"loss_function\": None,\n        \"lower\": lower,\n        \"max_time\": max_time,\n        \"max_surrogate_points\": max_surrogate_points,\n        \"metric_river\": None,\n        \"metric_sklearn\": metric_sklearn,\n        \"metric_sklearn_name\": metric_sklearn_name,\n        \"metric_torch\": None,\n        \"metric_params\": {},\n        \"model_dict\": {},\n        \"noise\": noise,\n        \"n_points\": n_points,\n        \"n_samples\": n_samples,\n        \"n_total\": n_total,\n        \"num_nodes\": num_nodes,\n        \"num_sanity_val_steps\": num_sanity_val_steps,\n        \"num_workers\": num_workers,\n        \"ocba_delta\": ocba_delta,\n        \"oml_grace_period\": oml_grace_period,\n        \"optimizer\": optimizer,\n        \"path\": None,\n        \"precision\": precision,\n        \"prep_model\": prep_model,\n        \"prep_model_name\": prep_model_name,\n        \"progress_file\": progress_file,\n        \"save_experiment\": save_experiment,\n        \"save_model\": False,\n        \"scaler\": scaler,\n        \"scaler_name\": scaler_name,\n        \"scenario\": scenario,\n        \"seed\": seed,\n        \"show_batch_interval\": 1_000_000,\n        \"show_config\": show_config,\n        \"show_models\": show_models,\n        \"show_progress\": show_progress,\n        \"shuffle\": shuffle,\n        \"sigma\": sigma,\n        \"spot_tensorboard_path\": spot_tensorboard_path,\n        \"strategy\": strategy,\n        \"target_column\": target_column,\n        \"target_type\": target_type,\n        \"task\": task,\n        \"tensorboard_log\": tensorboard_log,\n        \"tensorboard_start\": tensorboard_start,\n        \"tensorboard_stop\": tensorboard_stop,\n        \"test\": test,\n        \"test_seed\": test_seed,\n        \"test_size\": test_size,\n        \"tkagg\": tkagg,\n        \"tolerance_x\": tolerance_x,\n        \"train\": train,\n        \"upper\": upper,\n        \"var_name\": var_name,\n        \"var_type\": var_type,\n        \"verbosity\": verbosity,\n        \"weights\": weights,\n        \"weight_coeff\": weight_coeff,\n        \"weights_entry\": weights_entry,\n    }\n    if hyperdict is not None and core_model_name is not None:\n        # check if hyperdict implements the methods get_scenario:\n        if hasattr(hyperdict, \"get_scenario\"):\n            scenario = hyperdict().get_scenario()\n        else:\n            scenario = None\n        if fun_control[\"hyperdict\"].__name__ == RiverHyperDict.__name__ or scenario == \"river\":\n            coremodel, core_model_instance = get_river_core_model_from_name(core_model_name)\n            if prep_model is None and prep_model_name is not None:\n                prep_model = get_river_prep_model(prep_model_name)\n        else:\n            coremodel, core_model_instance = get_core_model_from_name(core_model_name)\n            if prep_model is None and prep_model_name is not None:\n                prep_model = get_prep_model(prep_model_name)\n        fun_control.update({\"prep_model\": prep_model})\n        add_core_model_to_fun_control(\n            core_model=core_model_instance,\n            fun_control=fun_control,\n            hyper_dict=hyperdict,\n            filename=None,\n        )\n    if hyperdict is not None and core_model is not None:\n        add_core_model_to_fun_control(\n            core_model=core_model,\n            fun_control=fun_control,\n            hyper_dict=hyperdict,\n            filename=None,\n        )\n    return fun_control\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.get_experiment_name","title":"<code>get_experiment_name(prefix='00')</code>","text":"<p>Returns a unique experiment name with a given prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Prefix for the experiment name. Defaults to \u201c00\u201d.</p> <code>'00'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique experiment name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import get_experiment_name\n&gt;&gt;&gt; get_experiment_name(prefix=\"00\")\n00_ubuntu_2021-08-31_14-30-00\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def get_experiment_name(prefix: str = \"00\") -&gt; str:\n    \"\"\"Returns a unique experiment name with a given prefix.\n\n    Args:\n        prefix (str, optional): Prefix for the experiment name. Defaults to \"00\".\n\n    Returns:\n        str: Unique experiment name.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import get_experiment_name\n        &gt;&gt;&gt; get_experiment_name(prefix=\"00\")\n        00_ubuntu_2021-08-31_14-30-00\n    \"\"\"\n    start_time = datetime.datetime.now(tzlocal())\n    HOSTNAME = socket.gethostname().split(\".\")[0]\n    experiment_name = prefix + \"_\" + HOSTNAME + \"_\" + str(start_time).split(\".\", 1)[0].replace(\" \", \"_\")\n    experiment_name = experiment_name.replace(\":\", \"-\")\n    return experiment_name\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.get_feature_names","title":"<code>get_feature_names(fun_control)</code>","text":"<p>Get the feature names from the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The function control dictionary. Must contain a \u201cdata_set\u201d key.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of feature names.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If \u201cdata_set\u201d is not in fun_control.</p> <code>ValueError</code> <p>If \u201cdata_set\u201d is None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import get_feature_names\n    get_feature_names(fun_control)\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def get_feature_names(fun_control: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"\n    Get the feature names from the fun_control dictionary.\n\n    Args:\n        fun_control (dict): The function control dictionary. Must contain a \"data_set\" key.\n\n    Returns:\n        List[str]: List of feature names.\n\n    Raises:\n        ValueError: If \"data_set\" is not in fun_control.\n        ValueError: If \"data_set\" is None.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import get_feature_names\n            get_feature_names(fun_control)\n    \"\"\"\n    data_set = fun_control.get(\"data_set\")\n\n    if data_set is None:\n        raise ValueError(\"'data_set' key not found or is None in 'fun_control'\")\n\n    return data_set.names\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.get_spot_tensorboard_path","title":"<code>get_spot_tensorboard_path(experiment_name)</code>","text":"<p>Get the path to the spot tensorboard files.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>The name of the experiment.</p> required <p>Returns:</p> Name Type Description <code>spot_tensorboard_path</code> <code>str</code> <p>The path to the folder where the spot tensorboard files are saved.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import get_spot_tensorboard_path\n&gt;&gt;&gt; get_spot_tensorboard_path(\"00_ubuntu_2021-08-31_14-30-00\")\nruns/spot_logs/00_ubuntu_2021-08-31_14-30-00\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def get_spot_tensorboard_path(experiment_name) -&gt; str:\n    \"\"\"Get the path to the spot tensorboard files.\n\n    Args:\n        experiment_name (str): The name of the experiment.\n\n    Returns:\n        spot_tensorboard_path (str): The path to the folder where the spot tensorboard files are saved.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import get_spot_tensorboard_path\n        &gt;&gt;&gt; get_spot_tensorboard_path(\"00_ubuntu_2021-08-31_14-30-00\")\n        runs/spot_logs/00_ubuntu_2021-08-31_14-30-00\n\n    \"\"\"\n    spot_tensorboard_path = os.environ.get(\"PATH_TENSORBOARD\", \"runs/spot_logs/\")\n    spot_tensorboard_path = os.path.join(spot_tensorboard_path, experiment_name)\n    return spot_tensorboard_path\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.get_tensorboard_path","title":"<code>get_tensorboard_path(fun_control)</code>","text":"<p>Get the path to the tensorboard files.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> required <p>Returns:</p> Name Type Description <code>tensorboard_path</code> <code>str</code> <p>The path to the folder where the tensorboard files are saved.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import get_tensorboard_path\n&gt;&gt;&gt; get_tensorboard_path(fun_control)\nruns/\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def get_tensorboard_path(fun_control) -&gt; str:\n    \"\"\"Get the path to the tensorboard files.\n\n    Args:\n        fun_control (dict): The function control dictionary.\n\n    Returns:\n        tensorboard_path (str): The path to the folder where the tensorboard files are saved.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import get_tensorboard_path\n        &gt;&gt;&gt; get_tensorboard_path(fun_control)\n        runs/\n    \"\"\"\n    return fun_control[\"TENSORBOARD_PATH\"]\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.optimizer_control_init","title":"<code>optimizer_control_init(max_iter=1000, seed=125)</code>","text":"<p>Initialize optimizer_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>max_iter</code> <code>int</code> <p>The maximum number of iterations. This will be used for the optimization of the surrogate model. Default is 1000.</p> <code>1000</code> <code>seed</code> <code>int</code> <p>The seed to use for the random number generator. Default is 125.</p> <code>125</code> Notes <ul> <li>Differential evaluation uses <code>maxiter = 1000</code> and sets the number of function evaluations to   (maxiter + 1) * popsize * N, which results in 1000 * 15 * k,   because the default popsize is 15 and N is the number of parameters. This is already sufficient   for many situations. For example, for k=2 these are 30 000 iterations.   Therefore we set this value to 1000.</li> <li>This value will be passed to the surrogate model in the <code>Spot</code> class.</li> </ul> <p>Returns:</p> Name Type Description <code>optimizer_control</code> <code>dict</code> <p>A dictionary containing the information about the optimizer.</p> Source code in <code>spotpython/utils/init.py</code> <pre><code>def optimizer_control_init(\n    max_iter=1000,\n    seed=125,\n) -&gt; dict:\n    \"\"\"Initialize optimizer_control dictionary.\n\n    Args:\n        max_iter (int):\n            The maximum number of iterations. This will be used for the\n            optimization of the surrogate model. Default is 1000.\n        seed (int):\n            The seed to use for the random number generator.\n            Default is 125.\n\n    Notes:\n        * Differential evaluation uses `maxiter = 1000` and sets the number of function evaluations to\n          (maxiter + 1) * popsize * N, which results in 1000 * 15 * k,\n          because the default popsize is 15 and N is the number of parameters. This is already sufficient\n          for many situations. For example, for k=2 these are 30 000 iterations.\n          Therefore we set this value to 1000.\n        * This value will be passed to the surrogate model in the `Spot` class.\n\n    Returns:\n        optimizer_control (dict):\n            A dictionary containing the information about the optimizer.\n\n    \"\"\"\n    optimizer_control = {\"max_iter\": max_iter, \"seed\": seed}\n    return optimizer_control\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.setup_paths","title":"<code>setup_paths(tensorboard_clean)</code>","text":"<p>Setup paths for checkpoints, datasets, results, and tensorboard files. This function also handles cleaning the tensorboard path if specified.</p> <p>Parameters:</p> Name Type Description Default <code>tensorboard_clean</code> <code>bool</code> <p>If True, move the existing tensorboard folder to a timestamped backup folder to avoid overwriting old tensorboard files.</p> required <p>Returns:</p> Name Type Description <code>CHECKPOINT_PATH</code> <code>str</code> <p>The path to the folder where the pretrained models are saved.</p> <code>DATASET_PATH</code> <code>str</code> <p>The path to the folder where the datasets are/should be downloaded.</p> <code>RESULTS_PATH</code> <code>str</code> <p>The path to the folder where the results (plots, csv, etc.) are saved.</p> <code>TENSORBOARD_PATH</code> <code>str</code> <p>The path to the folder where the tensorboard files are saved.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import setup_paths\n&gt;&gt;&gt; setup_paths(tensorboard_clean=True)\n('runs/saved_models/', 'data/', 'results/', 'runs/')\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def setup_paths(tensorboard_clean) -&gt; tuple:\n    \"\"\"\n    Setup paths for checkpoints, datasets, results, and tensorboard files.\n    This function also handles cleaning the tensorboard path if specified.\n\n    Args:\n        tensorboard_clean (bool):\n            If True, move the existing tensorboard folder to a timestamped backup\n            folder to avoid overwriting old tensorboard files.\n\n    Returns:\n        CHECKPOINT_PATH (str):\n            The path to the folder where the pretrained models are saved.\n        DATASET_PATH (str):\n            The path to the folder where the datasets are/should be downloaded.\n        RESULTS_PATH (str):\n            The path to the folder where the results (plots, csv, etc.) are saved.\n        TENSORBOARD_PATH (str):\n            The path to the folder where the tensorboard files are saved.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import setup_paths\n        &gt;&gt;&gt; setup_paths(tensorboard_clean=True)\n        ('runs/saved_models/', 'data/', 'results/', 'runs/')\n\n    \"\"\"\n    # Path to the folder where the pretrained models are saved\n    CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"runs/saved_models/\")\n    os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n\n    # Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n    DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n    os.makedirs(DATASET_PATH, exist_ok=True)\n\n    # Path to the folder where the results (plots, csv, etc.) are saved\n    RESULTS_PATH = os.environ.get(\"PATH_RESULTS\", \"results/\")\n    os.makedirs(RESULTS_PATH, exist_ok=True)\n\n    # Path to the folder where the tensorboard files are saved\n    TENSORBOARD_PATH = os.environ.get(\"PATH_TENSORBOARD\", \"runs/\")\n    if tensorboard_clean:\n        # if the folder \"runs\" exists, move it to \"runs_Y_M_D_H_M_S\" to avoid overwriting old tensorboard files\n        if os.path.exists(TENSORBOARD_PATH):\n            now = datetime.datetime.now()\n            os.makedirs(\"runs_OLD\", exist_ok=True)\n            # use [:-1] to remove \"/\" from the end of the path\n            TENSORBOARD_PATH_OLD = \"runs_OLD/\" + TENSORBOARD_PATH[:-1] + \"_\" + now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n            print(f\"Moving TENSORBOARD_PATH: {TENSORBOARD_PATH} to TENSORBOARD_PATH_OLD: {TENSORBOARD_PATH_OLD}\")\n            os.rename(TENSORBOARD_PATH[:-1], TENSORBOARD_PATH_OLD)\n\n    os.makedirs(TENSORBOARD_PATH, exist_ok=True)\n\n    # Ensure the figures folder exists\n    if not os.path.exists(\"./figures\"):\n        os.makedirs(\"./figures\")\n    return CHECKPOINT_PATH, DATASET_PATH, RESULTS_PATH, TENSORBOARD_PATH\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.surrogate_control_init","title":"<code>surrogate_control_init(log_level=50, noise=False, model_optimizer=differential_evolution, model_fun_evals=10000, min_theta=-3.0, max_theta=2.0, n_theta='anisotropic', p_val=2.0, n_p=1, optim_p=False, min_Lambda=1e-09, max_Lambda=1, seed=124, theta_init_zero=True, var_type=None, metric_factorial='canberra')</code>","text":"<p>Initialize surrogate_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>model_optimizer</code> <code>object</code> <p>The optimizer object used for the search on surrogate. Default is differential_evolution.</p> <code>differential_evolution</code> <code>model_fun_evals</code> <code>int</code> <p>The number of function evaluations. This will be used for the optimization of the surrogate model. Default is 1000.</p> <code>10000</code> <code>min_theta</code> <code>float</code> <p>The minimum value of theta. Note that the base10-logarithm is used.  Default is -3.</p> <code>-3.0</code> <code>max_theta</code> <code>float</code> <p>The maximum value of theta. Note that the base10-logarithm is used. Default is 3.</p> <code>2.0</code> <code>noise</code> <code>bool</code> <p>Whether the objective function is noisy or not. If Kriging, then a nugget is added. Default is False. Note: Will be set in the Spot class.</p> <code>False</code> <code>n_theta</code> <code>int</code> <p>The number of theta values. If larger than 1 or set to the string \u201canisotropic\u201d, then the k theta values are used, where k is the problem dimension. This is handled in spot.py. Default is \u201canisotropic\u201d.</p> <code>'anisotropic'</code> <code>p_val</code> <code>float</code> <pre><code>p value. Used as an initial value if optim_p = True. Otherwise as a constant. Defaults to 2.0.\n</code></pre> <code>2.0</code> <code>n_p</code> <code>int</code> <p>The number of p values. Number of p values to be used. Default is 1.</p> <code>1</code> <code>optim_p</code> <code>bool</code> <p>Whether to optimize p or not.</p> <code>False</code> <code>min_Lambda</code> <code>float</code> <p>The minimum value of lambda. Default is 1e-9.</p> <code>1e-09</code> <code>max_Lambda</code> <code>float</code> <p>The maximum value of lambda. Default is 1.</p> <code>1</code> <code>seed</code> <code>int</code> <p>The seed to use for the random number generator.</p> <code>124</code> <code>theta_init_zero</code> <code>bool</code> <p>Whether to initialize theta with zero or not. Default is True.</p> <code>True</code> <code>var_type</code> <code>list</code> <p>A list containing the type of the variables. Default is None. Note: Will be set in the Spot class.</p> <code>None</code> <code>metric_factorial</code> <code>str</code> <p>The metric to be used for the factorial design. Default is \u201ccanberra\u201d.</p> <code>'canberra'</code> <p>Returns:</p> Name Type Description <code>surrogate_control</code> <code>dict</code> <p>A dictionary containing the information about the surrogate model.</p> Note <ul> <li>The surrogate_control dictionary is used in the Spot class. The following values   are updated in the Spot class if they are None in the surrogate_control dictionary:<ul> <li><code>noise</code>: If the surrogate model dictionary is passed to the Spot class,   and the <code>noise</code> value is <code>None</code>, then the noise value is set in the   Spot class based on the value of <code>noise</code> in the Spot class fun_control dictionary.</li> <li><code>var_type</code>: The <code>var_type</code> value is set in the Spot class based on the value    of <code>var_type</code> in the Spot class fun_control dictionary and the dimension of the problem.    If the Kriging model is used as a surrogate in the Spot class, the setting from     surrogate_control_init() is overwritten.</li> <li><code>n_theta</code>: If self.surrogate_control[\u201cn_theta\u201d] &gt; 1,    use k theta values, where k is the problem dimension specified in the Spot class.    The problem dimension is set in the Spot class based on the    length of the lower bounds.</li> </ul> </li> <li>This value <code>model_fun_evals</code> will used for the optimization of the surrogate model, e.g., theta values.   Differential evaluation uses <code>maxiter = 1000</code> and sets the number of function evaluations to   (maxiter + 1) * popsize * N, which results in 1000 * 15 * k,   because the default popsize is 15 and N is the number of parameters. This is already sufficient   for many situations. For example, for k=2 these are 30 000 iterations.   Therefore we set this value to 1000.</li> </ul> Source code in <code>spotpython/utils/init.py</code> <pre><code>def surrogate_control_init(\n    log_level: int = 50,\n    noise=False,\n    model_optimizer=differential_evolution,\n    model_fun_evals=10000,\n    min_theta=-3.0,\n    max_theta=2.0,\n    n_theta=\"anisotropic\",\n    p_val=2.0,\n    n_p=1,\n    optim_p=False,\n    min_Lambda=1e-9,\n    max_Lambda=1,\n    seed=124,\n    theta_init_zero=True,\n    var_type=None,\n    metric_factorial=\"canberra\",\n) -&gt; dict:\n    \"\"\"Initialize surrogate_control dictionary.\n\n    Args:\n        model_optimizer (object):\n            The optimizer object used for the search on surrogate.\n            Default is differential_evolution.\n        model_fun_evals (int):\n            The number of function evaluations. This will be used for the\n            optimization of the surrogate model. Default is 1000.\n        min_theta (float):\n            The minimum value of theta. Note that the base10-logarithm is used.\n             Default is -3.\n        max_theta (float): The maximum value of theta. Note that the base10-logarithm is used.\n            Default is 3.\n        noise (bool):\n            Whether the objective function is noisy or not. If Kriging, then a nugget is added.\n            Default is False. Note: Will be set in the Spot class.\n        n_theta (int):\n            The number of theta values. If larger than 1 or set to the string \"anisotropic\",\n            then the k theta values are used, where k is the problem dimension.\n            This is handled in spot.py. Default is \"anisotropic\".\n        p_val (float):\n                p value. Used as an initial value if optim_p = True. Otherwise as a constant. Defaults to 2.0.\n        n_p (int):\n            The number of p values. Number of p values to be used. Default is 1.\n        optim_p (bool):\n            Whether to optimize p or not.\n        min_Lambda (float):\n            The minimum value of lambda. Default is 1e-9.\n        max_Lambda (float):\n            The maximum value of lambda. Default is 1.\n        seed (int):\n            The seed to use for the random number generator.\n        theta_init_zero (bool):\n            Whether to initialize theta with zero or not. Default is True.\n        var_type (list):\n            A list containing the type of the variables. Default is None.\n            Note: Will be set in the Spot class.\n        metric_factorial (str):\n            The metric to be used for the factorial design. Default is \"canberra\".\n\n    Returns:\n        surrogate_control (dict):\n            A dictionary containing the information about the surrogate model.\n\n    Note:\n        * The surrogate_control dictionary is used in the Spot class. The following values\n          are updated in the Spot class if they are None in the surrogate_control dictionary:\n            * `noise`: If the surrogate model dictionary is passed to the Spot class,\n              and the `noise` value is `None`, then the noise value is set in the\n              Spot class based on the value of `noise` in the Spot class fun_control dictionary.\n            * `var_type`: The `var_type` value is set in the Spot class based on the value\n               of `var_type` in the Spot class fun_control dictionary and the dimension of the problem.\n               If the Kriging model is used as a surrogate in the Spot class, the setting from\n                surrogate_control_init() is overwritten.\n            * `n_theta`: If self.surrogate_control[\"n_theta\"] &gt; 1,\n               use k theta values, where k is the problem dimension specified in the Spot class.\n               The problem dimension is set in the Spot class based on the\n               length of the lower bounds.\n        * This value `model_fun_evals` will used for the optimization of the surrogate model, e.g., theta values.\n          Differential evaluation uses `maxiter = 1000` and sets the number of function evaluations to\n          (maxiter + 1) * popsize * N, which results in 1000 * 15 * k,\n          because the default popsize is 15 and N is the number of parameters. This is already sufficient\n          for many situations. For example, for k=2 these are 30 000 iterations.\n          Therefore we set this value to 1000.\n\n    \"\"\"\n    surrogate_control = {\n        \"log_level\": log_level,\n        \"noise\": noise,\n        \"model_optimizer\": model_optimizer,\n        \"model_fun_evals\": model_fun_evals,\n        \"min_theta\": min_theta,\n        \"max_theta\": max_theta,\n        \"n_theta\": n_theta,\n        \"p_val\": p_val,\n        \"n_p\": n_p,\n        \"optim_p\": optim_p,\n        \"min_Lambda\": min_Lambda,\n        \"max_Lambda\": max_Lambda,\n        \"seed\": seed,\n        \"theta_init_zero\": theta_init_zero,\n        \"var_type\": var_type,\n        \"metric_factorial\": metric_factorial,\n    }\n    return surrogate_control\n</code></pre>"},{"location":"reference/spotpython/utils/math/","title":"math","text":""},{"location":"reference/spotpython/utils/math/#spotpython.utils.math.generate_div2_list","title":"<code>generate_div2_list(n, n_min)</code>","text":"<p>Generate a list of numbers from n to n_min (inclusive) by dividing n by 2 until the result is less than n_min. This function starts with n and keeps dividing it by 2 until n_min is reached. The number of times each value is added to the list is determined by n // current.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number to start with.</p> required <code>n_min</code> <code>int</code> <p>The minimum number to stop at.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of numbers from n to n_min (inclusive).</p> <p>Examples:</p> <p>from spotpython.utils.math import generate_div2_list generate_div2_list(10, 1) [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] generate_div2_list(10, 2) [10, 5, 5, 2, 2, 2, 2, 2]</p> Source code in <code>spotpython/utils/math.py</code> <pre><code>def generate_div2_list(n, n_min) -&gt; list:\n    \"\"\"\n    Generate a list of numbers from n to n_min (inclusive) by dividing n by 2\n    until the result is less than n_min.\n    This function starts with n and keeps dividing it by 2 until n_min is reached.\n    The number of times each value is added to the list is determined by n // current.\n\n    Args:\n        n (int): The number to start with.\n        n_min (int): The minimum number to stop at.\n\n    Returns:\n        list: A list of numbers from n to n_min (inclusive).\n\n    Examples:\n        from spotpython.utils.math import generate_div2_list\n        generate_div2_list(10, 1)\n        [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n        generate_div2_list(10, 2)\n        [10, 5, 5, 2, 2, 2, 2, 2]\n    \"\"\"\n    result = []\n    current = n\n    while current &gt;= n_min:\n        result.extend([current] * (n // current))\n        current = current // 2\n    return result\n</code></pre>"},{"location":"reference/spotpython/utils/metrics/","title":"metrics","text":""},{"location":"reference/spotpython/utils/metrics/#spotpython.utils.metrics.apk","title":"<code>apk(actual, predicted, k=10)</code>","text":"<p>Computes the average precision at k. This function computes the average precision at k between two lists of items.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>list</code> <p>A list of elements that are to be predicted (order doesn\u2019t matter)</p> required <code>predicted</code> <code>list</code> <p>A list of predicted elements (order does matter)</p> required <code>k</code> <code>int</code> <p>The maximum number of predicted elements</p> <code>10</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average precision at k over the input lists</p> Source code in <code>spotpython/utils/metrics.py</code> <pre><code>def apk(actual, predicted, k=10):\n    \"\"\"\n    Computes the average precision at k.\n    This function computes the average precision at k between two lists of\n    items.\n\n    Args:\n        actual (list): A list of elements that are to be predicted (order doesn't matter)\n        predicted (list): A list of predicted elements (order does matter)\n        k (int): The maximum number of predicted elements\n\n    Returns:\n        score (float): The average precision at k over the input lists\n    \"\"\"\n    if len(predicted) &gt; k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i, p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i + 1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n</code></pre>"},{"location":"reference/spotpython/utils/metrics/#spotpython.utils.metrics.get_metric_sign","title":"<code>get_metric_sign(metric_name)</code>","text":"<p>Returns the sign of a metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The name of the metric. Can be one of the following:     - \u201caccuracy_score\u201d     - \u201ccohen_kappa_score\u201d     - \u201cf1_score\u201d     - \u201chamming_loss\u201d     - \u201chinge_loss\u201d     -\u201cjaccard_score\u201d     - \u201cmatthews_corrcoef\u201d     - \u201cprecision_score\u201d     - \u201crecall_score\u201d     - \u201croc_auc_score\u201d     - \u201czero_one_loss\u201d</p> required <p>Returns:</p> Name Type Description <code>sign</code> <code>float</code> <p>The sign of the metric. -1 for max, +1 for min.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the metric is not found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.metrics import get_metric_sign\n&gt;&gt;&gt; get_metric_sign(\"accuracy_score\")\n-1\n&gt;&gt;&gt; get_metric_sign(\"hamming_loss\")\n+1\n</code></pre> Source code in <code>spotpython/utils/metrics.py</code> <pre><code>def get_metric_sign(metric_name):\n    \"\"\"Returns the sign of a metric.\n\n    Args:\n        metric_name (str):\n            The name of the metric. Can be one of the following:\n                - \"accuracy_score\"\n                - \"cohen_kappa_score\"\n                - \"f1_score\"\n                - \"hamming_loss\"\n                - \"hinge_loss\"\n                -\"jaccard_score\"\n                - \"matthews_corrcoef\"\n                - \"precision_score\"\n                - \"recall_score\"\n                - \"roc_auc_score\"\n                - \"zero_one_loss\"\n\n    Returns:\n        sign (float): The sign of the metric. -1 for max, +1 for min.\n\n    Raises:\n        ValueError: If the metric is not found.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.metrics import get_metric_sign\n        &gt;&gt;&gt; get_metric_sign(\"accuracy_score\")\n        -1\n        &gt;&gt;&gt; get_metric_sign(\"hamming_loss\")\n        +1\n\n    \"\"\"\n    if metric_name in [\n        \"accuracy_score\",\n        \"cohen_kappa_score\",\n        \"f1_score\",\n        \"jaccard_score\",\n        \"matthews_corrcoef\",\n        \"precision_score\",\n        \"recall_score\",\n        \"roc_auc_score\",\n        \"explained_variance_score\",\n        \"r2_score\",\n        \"d2_absolute_error_score\",\n        \"d2_pinball_score\",\n        \"d2_tweedie_score\",\n    ]:\n        return -1\n    elif metric_name in [\n        \"hamming_loss\",\n        \"hinge_loss\",\n        \"zero_one_loss\",\n        \"max_error\",\n        \"mean_absolute_error\",\n        \"mean_squared_error\",\n        \"root_mean_squared_error\",\n        \"mean_squared_log_error\",\n        \"root_mean_squared_log_error\",\n        \"median_absolute_error\",\n        \"mean_poisson_deviance\",\n        \"mean_gamma_deviance\",\n        \"mean_absolute_percentage_error\",\n    ]:\n        return +1\n    else:\n        raise ValueError(f\"Metric '{metric_name}' not found.\")\n</code></pre>"},{"location":"reference/spotpython/utils/metrics/#spotpython.utils.metrics.mapk","title":"<code>mapk(actual, predicted, k=10)</code>","text":"<p>Computes the mean average precision at k. This function computes the mean average precision at k between two lists of lists of items.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>list</code> <p>A list of lists of elements that are to be predicted (order doesn\u2019t matter in the lists)</p> required <code>predicted</code> <code>list</code> <p>A list of lists of predicted elements (order matters in the lists)</p> required <code>k</code> <code>int</code> <p>The maximum number of predicted elements</p> <code>10</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The mean average precision at k over the input lists</p> Source code in <code>spotpython/utils/metrics.py</code> <pre><code>def mapk(actual, predicted, k=10):\n    \"\"\"\n    Computes the mean average precision at k.\n    This function computes the mean average precision at k between two lists\n    of lists of items.\n\n    Args:\n        actual (list): A list of lists of elements that are to be predicted\n            (order doesn't matter in the lists)\n        predicted (list): A list of lists of predicted elements\n            (order matters in the lists)\n        k (int): The maximum number of predicted elements\n\n    Returns:\n        score (float): The mean average precision at k over the input lists\n    \"\"\"\n    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n</code></pre>"},{"location":"reference/spotpython/utils/metrics/#spotpython.utils.metrics.mapk_score","title":"<code>mapk_score(y_true, y_pred, k=3)</code>","text":"<p>Wrapper for mapk func using numpy arrays</p> <p>Args:         y_true (np.array): array of true values         y_pred (np.array): array of predicted values         k (int): number of predictions</p> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>mean average precision at k</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y_true = np.array([0, 1, 2, 2])\n&gt;&gt;&gt; y_pred = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n         [0.3, 0.4, 0.2],  # 1 is in top 2\n         [0.2, 0.4, 0.3],  # 2 is in top 2\n         [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n&gt;&gt;&gt; mapk_score(y_true, y_pred, k=1)\n0.25\n&gt;&gt;&gt; mapk_score(y_true, y_pred, k=2)\n0.375\n&gt;&gt;&gt; mapk_score(y_true, y_pred, k=3)\n0.4583333333333333\n&gt;&gt;&gt; mapk_score(y_true, y_pred, k=4)\n0.4583333333333333\n</code></pre> Source code in <code>spotpython/utils/metrics.py</code> <pre><code>def mapk_score(y_true, y_pred, k=3):\n    \"\"\"Wrapper for mapk func using numpy arrays\n\n     Args:\n            y_true (np.array): array of true values\n            y_pred (np.array): array of predicted values\n            k (int): number of predictions\n\n    Returns:\n            score (float): mean average precision at k\n\n    Examples:\n            &gt;&gt;&gt; y_true = np.array([0, 1, 2, 2])\n            &gt;&gt;&gt; y_pred = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n                     [0.3, 0.4, 0.2],  # 1 is in top 2\n                     [0.2, 0.4, 0.3],  # 2 is in top 2\n                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=1)\n            0.25\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=2)\n            0.375\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=3)\n            0.4583333333333333\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=4)\n            0.4583333333333333\n    \"\"\"\n    y_true = series_to_array(y_true)\n    sorted_prediction_ids = np.argsort(-y_pred, axis=1)\n    top_k_prediction_ids = sorted_prediction_ids[:, :k]\n    score = mapk(y_true.reshape(-1, 1), top_k_prediction_ids, k=k)\n    return score\n</code></pre>"},{"location":"reference/spotpython/utils/metrics/#spotpython.utils.metrics.mapk_scorer","title":"<code>mapk_scorer(estimator, X, y)</code>","text":"<p>Scorer for mean average precision at k. This function computes the mean average precision at k between two lists of lists of items.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>sklearn estimator</code> <p>The estimator to be used for prediction.</p> required <code>X</code> <code>array-like of shape (n_samples, n_features</code> <p>The input samples.</p> required <code>y</code> <code>array-like of shape (n_samples,</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The mean average precision at k over the input lists</p> Source code in <code>spotpython/utils/metrics.py</code> <pre><code>def mapk_scorer(estimator, X, y):\n    \"\"\"\n    Scorer for mean average precision at k.\n    This function computes the mean average precision at k between two lists\n    of lists of items.\n\n    Args:\n        estimator (sklearn estimator): The estimator to be used for prediction.\n        X (array-like of shape (n_samples, n_features)): The input samples.\n        y (array-like of shape (n_samples,)): The target values.\n\n    Returns:\n        score (float): The mean average precision at k over the input lists\n    \"\"\"\n    y_pred = estimator.predict_proba(X)\n    score = mapk_score(y, y_pred, k=3)\n    return score\n</code></pre>"},{"location":"reference/spotpython/utils/numpy2json/","title":"numpy2json","text":""},{"location":"reference/spotpython/utils/numpy2json/#spotpython.utils.numpy2json.NumpyEncoder","title":"<code>NumpyEncoder</code>","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>JSONEncoder subclass that knows how to encode numpy arrays.</p> Note <p>Taken from: https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable</p> Source code in <code>spotpython/utils/numpy2json.py</code> <pre><code>class NumpyEncoder(json.JSONEncoder):\n    \"\"\"\n    JSONEncoder subclass that knows how to encode numpy arrays.\n\n    Note:\n        Taken from:\n        https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable\n\n    \"\"\"\n\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.bool_):\n            return int(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, np.float64):\n            if np.isnan(obj):\n                return \"NaN\"\n            elif np.isinf(obj):\n                return \"Inf\"\n            else:\n                return float(obj)\n        return json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"reference/spotpython/utils/progress/","title":"progress","text":""},{"location":"reference/spotpython/utils/progress/#spotpython.utils.progress.progress_bar","title":"<code>progress_bar(progress, bar_length=10, message='spotpython tuning:', y=None, filename=None)</code>","text":"<p>Displays or updates a console progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>progress</code> <code>float</code> <p>a float between 0 and 1. Any int will be converted to a float. A value under 0 represents a halt. A value at 1 or bigger represents 100%.</p> required <code>bar_length</code> <code>int</code> <p>length of the progress bar</p> <code>10</code> <code>message</code> <code>str</code> <p>message text to display</p> <code>'spotpython tuning:'</code> <code>filename</code> <code>str</code> <p>If not None, write the progress bar to filename.</p> <code>None</code> Source code in <code>spotpython/utils/progress.py</code> <pre><code>def progress_bar(progress: float, bar_length: int = 10, message: str = \"spotpython tuning:\", y=None, filename=None) -&gt; None:\n    \"\"\"\n    Displays or updates a console progress bar.\n\n    Args:\n        progress (float):\n            a float between 0 and 1. Any int will be converted to a float.\n            A value under 0 represents a halt.\n            A value at 1 or bigger represents 100%.\n        bar_length (int):\n            length of the progress bar\n        message (str):\n            message text to display\n        filename (str):\n            If not None, write the progress bar to filename.\n    \"\"\"\n    if filename is not None:\n        # open the file in append mode\n        file = open(filename, \"a\")\n    status = \"\"\n    if y is not None:\n        message = f\"{message} {y}\"\n    if progress &lt; 0:\n        progress = 0\n        status = \"Halt...\\r\\n\"\n    elif progress &gt;= 1:\n        progress = 1\n        status = \"Done...\\r\\n\"\n    block = int(round(bar_length * progress))\n    text = f\"{message} [{'#' * block + '-' * (bar_length - block)}] {progress * 100:.2f}% {status}\\r\\n\"\n    if filename is not None:\n        file.write(text)\n        file.flush()\n    stdout.write(text)\n    stdout.flush()\n    if filename is not None:\n        file.close()\n</code></pre>"},{"location":"reference/spotpython/utils/repair/","title":"repair","text":""},{"location":"reference/spotpython/utils/repair/#spotpython.utils.repair.remove_nan","title":"<code>remove_nan(X, y, stop_on_zero_return=False)</code>","text":"<p>Remove rows from X and y where y contains NaN values and issue a warning     if the dimension of the returned y array is smaller than the dimension of the original y array.     Issues a ValueError if the dimension of the returned y array is less than 21 and     stop_on_zero_return is True.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array</p> required <code>y</code> <code>ndarray</code> <p>y array</p> required <code>stop_on_zero_return</code> <code>bool</code> <p>whether to stop if the returned dimension is less than 1. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[numpy.ndarray, np.ndarray]: X and y arrays with rows containing NaN values in y removed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.repair import remove_nan\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, np.nan, 2])\n    X_cleaned, y_cleaned = remove_nan(X, y)\n    print(X_cleaned, y_cleaned)\n    [[1 2]\n     [5 6]] [1. 2.]\n</code></pre> Source code in <code>spotpython/utils/repair.py</code> <pre><code>def remove_nan(X: np.ndarray, y: np.ndarray, stop_on_zero_return: bool = False) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Remove rows from X and y where y contains NaN values and issue a warning\n        if the dimension of the returned y array is smaller than the dimension of the original y array.\n        Issues a ValueError if the dimension of the returned y array is less than 21 and\n        stop_on_zero_return is True.\n\n    Args:\n        X (numpy.ndarray):\n            X array\n        y (numpy.ndarray):\n            y array\n        stop_on_zero_return (bool):\n            whether to stop if the returned dimension is less than 1.\n            Default is False.\n\n    Returns:\n        Tuple[numpy.ndarray, np.ndarray]:\n            X and y arrays with rows containing NaN values in y removed.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.repair import remove_nan\n            X = np.array([[1, 2], [3, 4], [5, 6]])\n            y = np.array([1, np.nan, 2])\n            X_cleaned, y_cleaned = remove_nan(X, y)\n            print(X_cleaned, y_cleaned)\n            [[1 2]\n             [5 6]] [1. 2.]\n    \"\"\"\n    # Get the original dimension of the y array\n    original_dim = y.shape[0]\n\n    # Identify indices where y is not NaN\n    ind = np.isfinite(y)\n\n    # Update X and y by removing rows with NaN in y\n    X_cleaned = X[ind, :]\n    y_cleaned = y[ind]\n\n    # Check if dimensions have been reduced\n    returned_dim = y_cleaned.shape[0]\n    if returned_dim &lt; original_dim:\n        warnings.warn(f\"\\n!!! The dimension of the returned y array is {y_cleaned.shape[0]}, \" f\"which is smaller than the original dimension {original_dim}.\")\n        warnings.warn(\"\\n!!! Check whether to continue with the reduced dimension is useful.\")\n    # throw an error if the returned dimension is smaller than one\n    if returned_dim &lt; 1 and stop_on_zero_return:\n        raise ValueError(\"!!!! The dimension of the returned y array is less than 1. Check the input data.\")\n\n    return X_cleaned, y_cleaned\n</code></pre>"},{"location":"reference/spotpython/utils/repair/#spotpython.utils.repair.repair_non_numeric","title":"<code>repair_non_numeric(X, var_type)</code>","text":"<p>Round non-numeric values to integers. This applies to all variables except for \u201cnum\u201d and \u201cfloat\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array</p> required <code>var_type</code> <code>list</code> <p>list with type information</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: X array with non-numeric values rounded to integers</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.array([[1.2, 2.3], [3.4, 4.5]])\n&gt;&gt;&gt; var_type = [\"num\", \"factor\"]\n&gt;&gt;&gt; repair_non_numeric(X, var_type)\narray([[1., 2.],\n       [3., 4.]])\n</code></pre> Source code in <code>spotpython/utils/repair.py</code> <pre><code>def repair_non_numeric(X: np.ndarray, var_type: List[str]) -&gt; np.ndarray:\n    \"\"\"\n    Round non-numeric values to integers.\n    This applies to all variables except for \"num\" and \"float\".\n\n    Args:\n        X (numpy.ndarray): X array\n        var_type (list): list with type information\n\n    Returns:\n        numpy.ndarray: X array with non-numeric values rounded to integers\n\n    Examples:\n        &gt;&gt;&gt; X = np.array([[1.2, 2.3], [3.4, 4.5]])\n        &gt;&gt;&gt; var_type = [\"num\", \"factor\"]\n        &gt;&gt;&gt; repair_non_numeric(X, var_type)\n        array([[1., 2.],\n               [3., 4.]])\n    \"\"\"\n    mask = np.isin(var_type, [\"num\", \"float\"], invert=True)\n    X[:, mask] = np.around(X[:, mask])\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/","title":"scaler","text":""},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchMinMaxScaler","title":"<code>TorchMinMaxScaler</code>","text":"<p>A class for scaling data using min-max normalization with PyTorch tensors. This scaler calculates the minimum and maximum values in the dataset to scale the data within a given range.</p> <p>Attributes:</p> Name Type Description <code>min</code> <code>Tensor</code> <p>The minimum values computed over the fitted data.</p> <code>max</code> <code>Tensor</code> <p>The maximum values computed over the fitted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotpython.utils.scaler import TorchMinMaxScaler\n&gt;&gt;&gt; scaler = TorchMinMaxScaler()\n# Given a tensor\n&gt;&gt;&gt; tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n# Fit and transform the tensor using the scaler\n&gt;&gt;&gt; scaled_tensor = scaler.fit_transform(tensor)\n&gt;&gt;&gt; print(scaled_tensor)\n# The output will be a tensor with values scaled between 0 and 1.\n</code></pre> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>class TorchMinMaxScaler:\n    \"\"\"\n    A class for scaling data using min-max normalization with PyTorch tensors.\n    This scaler calculates the minimum and maximum values in the dataset to scale the data within a given range.\n\n    Attributes:\n        min (torch.Tensor): The minimum values computed over the fitted data.\n        max (torch.Tensor): The maximum values computed over the fitted data.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from spotpython.utils.scaler import TorchMinMaxScaler\n        &gt;&gt;&gt; scaler = TorchMinMaxScaler()\n        # Given a tensor\n        &gt;&gt;&gt; tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n        # Fit and transform the tensor using the scaler\n        &gt;&gt;&gt; scaled_tensor = scaler.fit_transform(tensor)\n        &gt;&gt;&gt; print(scaled_tensor)\n        # The output will be a tensor with values scaled between 0 and 1.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the TorchMinMaxScaler class without any predefined min and max.\n        \"\"\"\n        self.min = None\n        self.max = None\n\n    def fit(self, x: torch.Tensor) -&gt; None:\n        \"\"\"\n        Compute the minimum and maximum value of the input tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise TypeError(\"Input should be a torch tensor\")\n        self.min = x.min(dim=0, keepdim=True).values\n        self.max = x.max(dim=0, keepdim=True).values\n\n    def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Scale the input tensor using the computed minimum and maximum values.\n\n        Args:\n            x (torch.Tensor): The input tensor to be scaled.\n\n        Returns:\n            torch.Tensor: The scaled tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n            RuntimeError: If the scaler has not been fitted before transforming data.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise TypeError(\"Input should be a torch tensor\")\n        if self.min is None or self.max is None:\n            raise RuntimeError(\"Must fit scaler before transforming data\")\n        x = (x - self.min) / (self.max - self.min + 1e-7)\n        return x\n\n    def fit_transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Fit the scaler to the input tensor and then scale the tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The scaled tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n        \"\"\"\n        self.fit(x)\n        return self.transform(x)\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchMinMaxScaler.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the TorchMinMaxScaler class without any predefined min and max.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the TorchMinMaxScaler class without any predefined min and max.\n    \"\"\"\n    self.min = None\n    self.max = None\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchMinMaxScaler.fit","title":"<code>fit(x)</code>","text":"<p>Compute the minimum and maximum value of the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def fit(self, x: torch.Tensor) -&gt; None:\n    \"\"\"\n    Compute the minimum and maximum value of the input tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n    \"\"\"\n    if not torch.is_tensor(x):\n        raise TypeError(\"Input should be a torch tensor\")\n    self.min = x.min(dim=0, keepdim=True).values\n    self.max = x.max(dim=0, keepdim=True).values\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchMinMaxScaler.fit_transform","title":"<code>fit_transform(x)</code>","text":"<p>Fit the scaler to the input tensor and then scale the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The scaled tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def fit_transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Fit the scaler to the input tensor and then scale the tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The scaled tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n    \"\"\"\n    self.fit(x)\n    return self.transform(x)\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchMinMaxScaler.transform","title":"<code>transform(x)</code>","text":"<p>Scale the input tensor using the computed minimum and maximum values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to be scaled.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The scaled tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> <code>RuntimeError</code> <p>If the scaler has not been fitted before transforming data.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Scale the input tensor using the computed minimum and maximum values.\n\n    Args:\n        x (torch.Tensor): The input tensor to be scaled.\n\n    Returns:\n        torch.Tensor: The scaled tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n        RuntimeError: If the scaler has not been fitted before transforming data.\n    \"\"\"\n    if not torch.is_tensor(x):\n        raise TypeError(\"Input should be a torch tensor\")\n    if self.min is None or self.max is None:\n        raise RuntimeError(\"Must fit scaler before transforming data\")\n    x = (x - self.min) / (self.max - self.min + 1e-7)\n    return x\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchStandardScaler","title":"<code>TorchStandardScaler</code>","text":"<p>A class for scaling data using standardization with torch tensors. This scaler computes the mean and standard deviation on a dataset so that it can later be used to scale the data using the computed mean and standard deviation.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>Tensor</code> <p>The mean value computed over the fitted data.</p> <code>std</code> <code>Tensor</code> <p>The standard deviation computed over the fitted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotpython.utils.scaler import TorchStandardScaler\n# Create a sample tensor\n&gt;&gt;&gt; tensor = torch.rand((10, 3))  # Random tensor with shape (10, 3)\n&gt;&gt;&gt; scaler = TorchStandardScaler()\n# Fit the scaler to the data\n&gt;&gt;&gt; scaler.fit(tensor)\n# Transform the data using the fitted scaler\n&gt;&gt;&gt; transformed_tensor = scaler.transform(tensor)\n&gt;&gt;&gt; print(transformed_tensor)\n# Using fit_transform method to fit and transform in one step\n&gt;&gt;&gt; another_tensor = torch.rand((10, 3))\n&gt;&gt;&gt; scaled_tensor = scaler.fit_transform(another_tensor)\n&gt;&gt;&gt; print(scaled_tensor)\n</code></pre> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>class TorchStandardScaler:\n    \"\"\"\n    A class for scaling data using standardization with torch tensors.\n    This scaler computes the mean and standard deviation on a dataset so that\n    it can later be used to scale the data using the computed mean and standard deviation.\n\n    Attributes:\n        mean (torch.Tensor): The mean value computed over the fitted data.\n        std (torch.Tensor): The standard deviation computed over the fitted data.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from spotpython.utils.scaler import TorchStandardScaler\n        # Create a sample tensor\n        &gt;&gt;&gt; tensor = torch.rand((10, 3))  # Random tensor with shape (10, 3)\n        &gt;&gt;&gt; scaler = TorchStandardScaler()\n        # Fit the scaler to the data\n        &gt;&gt;&gt; scaler.fit(tensor)\n        # Transform the data using the fitted scaler\n        &gt;&gt;&gt; transformed_tensor = scaler.transform(tensor)\n        &gt;&gt;&gt; print(transformed_tensor)\n        # Using fit_transform method to fit and transform in one step\n        &gt;&gt;&gt; another_tensor = torch.rand((10, 3))\n        &gt;&gt;&gt; scaled_tensor = scaler.fit_transform(another_tensor)\n        &gt;&gt;&gt; print(scaled_tensor)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the TorchStandardScaler class without any pre-defined mean and std.\n        \"\"\"\n        self.mean = None\n        self.std = None\n\n    def fit(self, x: torch.Tensor) -&gt; None:\n        \"\"\"\n        Compute the mean and standard deviation of the input tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise TypeError(\"Input should be a torch tensor\")\n        self.mean = x.mean(dim=0, keepdim=True)\n        self.std = x.std(dim=0, unbiased=False, keepdim=True)\n\n    def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Scale the input tensor using the computed mean and standard deviation.\n\n        Args:\n            x (torch.Tensor): The input tensor to be transformed, expected shape [n_samples, n_features]\n\n        Returns:\n            torch.Tensor: The scaled tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n            RuntimeError: If the scaler has not been fitted before transforming data.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise TypeError(\"Input should be a torch tensor\")\n        if self.mean is None or self.std is None:\n            raise RuntimeError(\"Must fit scaler before transforming data\")\n        x = (x - self.mean) / (self.std + 1e-7)\n        return x\n\n    def fit_transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Fit the scaler to the input tensor and then scale the tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n        Returns:\n            torch.Tensor: The scaled tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n        \"\"\"\n        self.fit(x)\n        return self.transform(x)\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchStandardScaler.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the TorchStandardScaler class without any pre-defined mean and std.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the TorchStandardScaler class without any pre-defined mean and std.\n    \"\"\"\n    self.mean = None\n    self.std = None\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchStandardScaler.fit","title":"<code>fit(x)</code>","text":"<p>Compute the mean and standard deviation of the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor, expected shape [n_samples, n_features]</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def fit(self, x: torch.Tensor) -&gt; None:\n    \"\"\"\n    Compute the mean and standard deviation of the input tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n    \"\"\"\n    if not torch.is_tensor(x):\n        raise TypeError(\"Input should be a torch tensor\")\n    self.mean = x.mean(dim=0, keepdim=True)\n    self.std = x.std(dim=0, unbiased=False, keepdim=True)\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchStandardScaler.fit_transform","title":"<code>fit_transform(x)</code>","text":"<p>Fit the scaler to the input tensor and then scale the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor, expected shape [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The scaled tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def fit_transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Fit the scaler to the input tensor and then scale the tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n    Returns:\n        torch.Tensor: The scaled tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n    \"\"\"\n    self.fit(x)\n    return self.transform(x)\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchStandardScaler.transform","title":"<code>transform(x)</code>","text":"<p>Scale the input tensor using the computed mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to be transformed, expected shape [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The scaled tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> <code>RuntimeError</code> <p>If the scaler has not been fitted before transforming data.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Scale the input tensor using the computed mean and standard deviation.\n\n    Args:\n        x (torch.Tensor): The input tensor to be transformed, expected shape [n_samples, n_features]\n\n    Returns:\n        torch.Tensor: The scaled tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n        RuntimeError: If the scaler has not been fitted before transforming data.\n    \"\"\"\n    if not torch.is_tensor(x):\n        raise TypeError(\"Input should be a torch tensor\")\n    if self.mean is None or self.std is None:\n        raise RuntimeError(\"Must fit scaler before transforming data\")\n    x = (x - self.mean) / (self.std + 1e-7)\n    return x\n</code></pre>"},{"location":"reference/spotpython/utils/split/","title":"split","text":""},{"location":"reference/spotpython/utils/split/#spotpython.utils.split.calculate_data_split","title":"<code>calculate_data_split(test_size, full_size, verbosity=0, stage=None)</code>","text":"<p>Calculates the split sizes for training, validation, and test datasets.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>float or int</code> <p>The size of the test set. Can be a float for proportion or an int for absolute number of test samples.</p> required <code>full_size</code> <code>int</code> <p>The size of the full dataset.</p> required <code>verbosity</code> <code>int</code> <p>The level of verbosity for debug output. Defaults to 0.</p> <code>0</code> <code>stage</code> <code>str</code> <p>The stage of setup, for debug output if needed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the sizes (full_train_size, val_size, train_size, test_size).</p> Source code in <code>spotpython/utils/split.py</code> <pre><code>def calculate_data_split(test_size, full_size, verbosity=0, stage=None) -&gt; tuple:\n    \"\"\"\n    Calculates the split sizes for training, validation, and test datasets.\n\n    Args:\n        test_size (float or int):\n            The size of the test set.\n            Can be a float for proportion or an int for absolute number of test samples.\n        full_size (int):\n            The size of the full dataset.\n        verbosity (int, optional):\n            The level of verbosity for debug output. Defaults to 0.\n        stage (str, optional):\n            The stage of setup, for debug output if needed.\n\n    Returns:\n        tuple: A tuple containing the sizes (full_train_size, val_size, train_size, test_size).\n    \"\"\"\n    if isinstance(test_size, float):\n        full_train_size = round(1.0 - test_size, 2)\n        val_size = round(full_train_size * test_size, 2)\n        train_size = round(full_train_size - val_size, 2)\n    else:\n        # test_size is considered an int, training size calculation directly based on it\n        full_train_size = full_size - test_size\n        val_size = int(full_train_size * test_size / full_size)\n        train_size = full_train_size - val_size\n\n    if verbosity &gt; 0:\n        print(f\"stage: {stage}\")\n    if verbosity &gt; 1:\n        print(f\"full_sizefull_train_size: {full_train_size}\")\n        print(f\"full_sizeval_size: {val_size}\")\n        print(f\"full_sizetrain_size: {train_size}\")\n        print(f\"full_sizetest_size: {test_size}\")\n\n    return full_train_size, val_size, train_size, test_size\n</code></pre>"},{"location":"reference/spotpython/utils/tensorboard/","title":"tensorboard","text":""},{"location":"reference/spotpython/utils/tensorboard/#spotpython.utils.tensorboard.start_tensorboard","title":"<code>start_tensorboard()</code>","text":"<p>Starts a tensorboard server in the background.</p> <p>Returns:</p> Name Type Description <code>process</code> <code>Popen</code> <p>The process of the tensorboard server.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.tensorboard import start_tensorboard\n&gt;&gt;&gt; process = start_tensorboard()\n</code></pre> Source code in <code>spotpython/utils/tensorboard.py</code> <pre><code>def start_tensorboard() -&gt; subprocess.Popen:\n    \"\"\"Starts a tensorboard server in the background.\n\n    Returns:\n        process: The process of the tensorboard server.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.tensorboard import start_tensorboard\n        &gt;&gt;&gt; process = start_tensorboard()\n\n    \"\"\"\n    cmd = [\"tensorboard\", \"--logdir=./runs\"]\n    process = subprocess.Popen(cmd)\n    return process\n</code></pre>"},{"location":"reference/spotpython/utils/tensorboard/#spotpython.utils.tensorboard.stop_tensorboard","title":"<code>stop_tensorboard(process)</code>","text":"<p>Stops a tensorboard server if the process exists.</p> <p>Parameters:</p> Name Type Description Default <code>process</code> <code>Popen</code> <p>The process of the tensorboard server.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.tensorboard import start_tensorboard, stop_tensorboard\n&gt;&gt;&gt; process = start_tensorboard()\n&gt;&gt;&gt; stop_tensorboard(process)\n</code></pre> Source code in <code>spotpython/utils/tensorboard.py</code> <pre><code>def stop_tensorboard(process) -&gt; None:\n    \"\"\"\n    Stops a tensorboard server if the process exists.\n\n    Args:\n        process (subprocess.Popen): The process of the tensorboard server.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.tensorboard import start_tensorboard, stop_tensorboard\n        &gt;&gt;&gt; process = start_tensorboard()\n        &gt;&gt;&gt; stop_tensorboard(process)\n    \"\"\"\n    if process is not None and process.poll() is None:\n        process.terminate()\n        process.wait()  # Ensure the process has terminated\n    else:\n        print(\"No active tensorboard process found or the process is already terminated.\")\n</code></pre>"},{"location":"reference/spotpython/utils/time/","title":"time","text":""},{"location":"reference/spotpython/utils/time/#spotpython.utils.time.get_timestamp","title":"<code>get_timestamp(only_int=True)</code>","text":"<p>Returns a timestamp as a string.</p> <p>Parameters:</p> Name Type Description Default <code>only_int</code> <code>bool</code> <p>if True, the timestamp is returned as an integer.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the timestamp as a string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.time import get_timestamp\n&gt;&gt;&gt; get_timestamp()\n'2021-06-28 14:51:54.500000'\n&gt;&gt;&gt; get_timestamp(only_int=True)\n'20210628145154500000'\n</code></pre> Source code in <code>spotpython/utils/time.py</code> <pre><code>def get_timestamp(only_int=True) -&gt; str:\n    \"\"\"\n    Returns a timestamp as a string.\n\n    Args:\n        only_int (bool): if True, the timestamp is returned as an integer.\n\n    Returns:\n        str: the timestamp as a string.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.time import get_timestamp\n        &gt;&gt;&gt; get_timestamp()\n        '2021-06-28 14:51:54.500000'\n        &gt;&gt;&gt; get_timestamp(only_int=True)\n        '20210628145154500000'\n    \"\"\"\n    dt = datetime.datetime.now().isoformat(sep=\" \", timespec=\"microseconds\")\n    if only_int:\n        # remove - . : and space\n        dt = dt.replace(\"-\", \"\")\n        dt = dt.replace(\".\", \"\")\n        dt = dt.replace(\":\", \"\")\n        dt = dt.replace(\" \", \"\")\n    return dt\n</code></pre>"},{"location":"reference/spotpython/utils/transform/","title":"transform","text":""},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.cod_to_nat_X","title":"<code>cod_to_nat_X(cod_X, cod_type, min_X=None, max_X=None, mean_X=None, std_X=None)</code>","text":"<p>Compute natural X-values from coded units based on the setting of the <code>cod_type</code> attribute. If <code>cod_type</code> is \u201cnorm\u201d, the values are de-normalized from [0,1]. If <code>cod_type</code> is \u201cstd\u201d, the values are de-standardized. Otherwise, the values are not modified.</p> <p>Parameters:</p> Name Type Description Default <code>cod_X</code> <code>array</code> <p>The coded X-values.</p> required <code>cod_type</code> <code>str</code> <p>The type of coding (\u201cnorm\u201d, \u201cstd\u201d, or other).</p> required <code>min_X</code> <code>array</code> <p>The minimum values of X. Defaults to None.</p> <code>None</code> <code>max_X</code> <code>array</code> <p>The maximum values of X. Defaults to None.</p> <code>None</code> <code>mean_X</code> <code>array</code> <p>The mean values of X. Defaults to None.</p> <code>None</code> <code>std_X</code> <code>array</code> <p>The standard deviation of X. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X</code> <code>array</code> <p>The natural (physical or real world) X-values.</p> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def cod_to_nat_X(cod_X, cod_type, min_X=None, max_X=None, mean_X=None, std_X=None) -&gt; np.ndarray:\n    \"\"\"\n    Compute natural X-values from coded units based on the\n    setting of the `cod_type` attribute. If `cod_type` is \"norm\", the values are\n    de-normalized from [0,1]. If `cod_type` is \"std\", the values are de-standardized.\n    Otherwise, the values are not modified.\n\n    Args:\n        cod_X (np.array):\n            The coded X-values.\n        cod_type (str):\n            The type of coding (\"norm\", \"std\", or other).\n        min_X (np.array):\n            The minimum values of X. Defaults to None.\n        max_X (np.array):\n            The maximum values of X. Defaults to None.\n        mean_X (np.array):\n            The mean values of X. Defaults to None.\n        std_X (np.array):\n            The standard deviation of X. Defaults to None.\n\n    Returns:\n        X (np.array): The natural (physical or real world) X-values.\n    \"\"\"\n    X_copy = copy.deepcopy(cod_X)\n    # k is the number of columns in X, i.e., the dimension of the input space.\n    k = cod_X.shape[1]\n    if cod_type == \"norm\":\n        # De-normalize X from [0,1] column-wise.\n        for i in range(k):\n            X_copy[:, i] = X_copy[:, i] * (max_X[i] - min_X[i]) + min_X[i]\n        X = X_copy\n    elif cod_type == \"std\":\n        # De-standardize X column-wise.\n        for i in range(k):\n            X_copy[:, i] = X_copy[:, i] * std_X[i] + mean_X[i]\n        X = X_copy\n    else:\n        X = X_copy\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.cod_to_nat_y","title":"<code>cod_to_nat_y(cod_y, cod_type, min_y=None, max_y=None, mean_y=None, std_y=None)</code>","text":"<p>Compute natural y-values from coded units based on the setting of the <code>cod_type</code> attribute. If <code>cod_type</code> is \u201cnorm\u201d, the values are de-normalized from [0,1]. If <code>cod_type</code> is \u201cstd\u201d, the values are de-standardized. Otherwise, the values are not modified.</p> <p>Parameters:</p> Name Type Description Default <code>cod_y</code> <code>array</code> <p>The coded y-values.</p> required <code>cod_type</code> <code>str</code> <p>The type of coding (\u201cnorm\u201d, \u201cstd\u201d, or other).</p> required <code>min_y</code> <code>array</code> <p>The minimum values of y. Defaults to None.</p> <code>None</code> <code>max_y</code> <code>array</code> <p>The maximum values of y. Defaults to None.</p> <code>None</code> <code>mean_y</code> <code>array</code> <p>The mean values of y. Defaults to None.</p> <code>None</code> <code>std_y</code> <code>array</code> <p>The standard deviation of y. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y</code> <code>array</code> <p>The natural (physical or real world) y-values.</p> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def cod_to_nat_y(cod_y, cod_type, min_y=None, max_y=None, mean_y=None, std_y=None) -&gt; np.ndarray:\n    \"\"\"\n    Compute natural y-values from coded units based on the\n    setting of the `cod_type` attribute. If `cod_type` is \"norm\", the values are\n    de-normalized from [0,1]. If `cod_type` is \"std\", the values are de-standardized.\n    Otherwise, the values are not modified.\n\n    Args:\n        cod_y (np.array):\n            The coded y-values.\n        cod_type (str):\n            The type of coding (\"norm\", \"std\", or other).\n        min_y (np.array):\n            The minimum values of y. Defaults to None.\n        max_y (np.array):\n            The maximum values of y. Defaults to None.\n        mean_y (np.array):\n            The mean values of y. Defaults to None.\n        std_y (np.array):\n            The standard deviation of y. Defaults to None.\n\n    Returns:\n        y (np.array): The natural (physical or real world) y-values.\n    \"\"\"\n    y_copy = copy.deepcopy(cod_y)\n    if cod_type == \"norm\":\n        y = y_copy * (max_y - min_y) + min_y\n    elif cod_type == \"std\":\n        y = y_copy * std_y + mean_y\n    else:\n        y = y_copy\n    return y\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.nat_to_cod_X","title":"<code>nat_to_cod_X(X, cod_type)</code>","text":"<p>Compute coded X-values from natural (physical or real world) units based on the setting of the <code>cod_type</code> attribute. If <code>cod_type</code> is \u201cnorm\u201d, the values are normalized to [0,1]. If <code>cod_type</code> is \u201cstd\u201d, the values are standardized. Otherwise, the values are not modified.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The input array.</p> required <code>cod_type</code> <code>str</code> <p>The type of coding (\u201cnorm\u201d, \u201cstd\u201d, or other).</p> required <p>Returns:</p> Name Type Description <code>cod_X</code> <code>array</code> <p>The coded X-values.</p> <code>min_X</code> <code>array</code> <p>The minimum values of X.</p> <code>max_X</code> <code>array</code> <p>The maximum values of X.</p> <code>mean_X</code> <code>array</code> <p>The mean values of X.</p> <code>std_X</code> <code>array</code> <p>The standard deviation of X.</p> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def nat_to_cod_X(X, cod_type):\n    \"\"\"\n    Compute coded X-values from natural (physical or real world) units based on the\n    setting of the `cod_type` attribute. If `cod_type` is \"norm\", the values are\n    normalized to [0,1]. If `cod_type` is \"std\", the values are standardized.\n    Otherwise, the values are not modified.\n\n    Args:\n        X (np.array): The input array.\n        cod_type (str): The type of coding (\"norm\", \"std\", or other).\n\n    Returns:\n        cod_X (np.array): The coded X-values.\n        min_X (np.array): The minimum values of X.\n        max_X (np.array): The maximum values of X.\n        mean_X (np.array): The mean values of X.\n        std_X (np.array): The standard deviation of X.\n    \"\"\"\n    min_X = np.min(X, axis=0)\n    max_X = np.max(X, axis=0)\n    mean_X = np.mean(X, axis=0)\n    # make std_X array similar to mean_X array\n    std_X = np.zeros_like(mean_X)\n    X_copy = copy.deepcopy(X)\n    # k is the number of columns in X, i.e., the dimension of the input space.\n    k = X.shape[1]\n    if cod_type == \"norm\":\n        # Normalize X to [0,1] column-wise. If the range is zero, set the value to 0.5.\n        for i in range(k):\n            if max_X[i] - min_X[i] == 0:\n                X_copy[:, i] = 0.5\n            else:\n                X_copy[:, i] = (X_copy[:, i] - min_X[i]) / (max_X[i] - min_X[i])\n        cod_X = X_copy\n    elif cod_type == \"std\":\n        # Standardize X column-wise. If the standard deviation is zero, do not divide.\n        for i in range(k):\n            if max_X[i] - min_X[i] == 0:\n                X_copy[:, i] = 0\n            else:\n                std_X[i] = np.std(X_copy[:, i], ddof=1)\n                X_copy[:, i] = (X_copy[:, i] - mean_X[i]) / std_X[i]\n        cod_X = X_copy\n    else:\n        cod_X = X_copy\n    return cod_X, min_X, max_X, mean_X, std_X\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.nat_to_cod_y","title":"<code>nat_to_cod_y(y, cod_type)</code>","text":"<p>Compute coded y-values from natural (physical or real world) units based on the setting of the <code>cod_type</code> attribute. If <code>cod_type</code> is \u201cnorm\u201d, the values are normalized to [0,1]. If <code>cod_type</code> is \u201cstd\u201d, the values are standardized. Otherwise, the values are not modified.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array</code> <p>The input array.</p> required <code>cod_type</code> <code>str</code> <p>The type of coding (\u201cnorm\u201d, \u201cstd\u201d, or other).</p> required <p>Returns:</p> Name Type Description <code>cod_y</code> <code>array</code> <p>The coded y-values.</p> <code>min_y</code> <code>array</code> <p>The minimum values of y.</p> <code>max_y</code> <code>array</code> <p>The maximum values of y.</p> <code>mean_y</code> <code>array</code> <p>The mean values of y.</p> <code>std_y</code> <code>array</code> <p>The standard deviation of y.</p> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def nat_to_cod_y(y, cod_type) -&gt; np.ndarray:\n    \"\"\"\n    Compute coded y-values from natural (physical or real world) units based on the\n    setting of the `cod_type` attribute. If `cod_type` is \"norm\", the values are\n    normalized to [0,1]. If `cod_type` is \"std\", the values are standardized.\n    Otherwise, the values are not modified.\n\n    Args:\n        y (np.array): The input array.\n        cod_type (str): The type of coding (\"norm\", \"std\", or other).\n\n    Returns:\n        cod_y (np.array):\n            The coded y-values.\n        min_y (np.array):\n            The minimum values of y.\n        max_y (np.array):\n            The maximum values of y.\n        mean_y (np.array):\n            The mean values of y.\n        std_y (np.array):\n            The standard deviation of y.\n    \"\"\"\n    mean_y = np.mean(y)\n    std_y = None\n    min_y = min(y)\n    max_y = max(y)\n    y_copy = copy.deepcopy(y)\n    if cod_type == \"norm\":\n        if (max_y - min_y) != 0:\n            cod_y = (y_copy - min_y) / (max_y - min_y)\n        else:\n            cod_y = 0.5 * np.ones_like(y_copy)\n    elif cod_type == \"std\":\n        if (max_y - min_y) != 0:\n            std_y = np.std(y, ddof=1)\n            cod_y = (y_copy - mean_y) / std_y\n        else:\n            cod_y = np.zeros_like(y_copy)\n    else:\n        cod_y = y_copy\n    return cod_y, min_y, max_y, mean_y, std_y\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.scale","title":"<code>scale(X, lower, upper)</code>","text":"<p>Sample scaling from unit hypercube to different bounds. Converts a sample from <code>[0, 1)</code> to <code>[a, b)</code>. The following transformation is used: <code>(b - a) * X + a</code></p> Note <p>equal lower and upper bounds are feasible.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>Sample to scale.</p> required <code>lower</code> <code>array</code> <p>lower bound of transformed data.</p> required <code>upper</code> <code>array</code> <p>upper bounds of transformed data.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Scaled sample.</p> <p>Examples:</p> <p>Transform three samples in the unit hypercube to (lower, upper) bounds:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.stats import qmc\n&gt;&gt;&gt; from spotpython.utils.transform import scale\n&gt;&gt;&gt; lower = np.array([6, 0])\n&gt;&gt;&gt; upper = np.array([6, 5])\n&gt;&gt;&gt; sample = np.array([[0.5 , 0.75],\n&gt;&gt;&gt;             [0.5 , 0.5],\n&gt;&gt;&gt;             [0.75, 0.25]])\n&gt;&gt;&gt; scale(sample, lower, upper)\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def scale(X: np.ndarray, lower: np.ndarray, upper: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Sample scaling from unit hypercube to different bounds. Converts a sample from `[0, 1)` to `[a, b)`.\n    The following transformation is used:\n    `(b - a) * X + a`\n\n    Note:\n        equal lower and upper bounds are feasible.\n\n    Args:\n        X (array):\n            Sample to scale.\n        lower (array):\n            lower bound of transformed data.\n        upper (array):\n            upper bounds of transformed data.\n\n    Returns:\n        (array):\n            Scaled sample.\n\n    Examples:\n        Transform three samples in the unit hypercube to (lower, upper) bounds:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from scipy.stats import qmc\n        &gt;&gt;&gt; from spotpython.utils.transform import scale\n        &gt;&gt;&gt; lower = np.array([6, 0])\n        &gt;&gt;&gt; upper = np.array([6, 5])\n        &gt;&gt;&gt; sample = np.array([[0.5 , 0.75],\n        &gt;&gt;&gt;             [0.5 , 0.5],\n        &gt;&gt;&gt;             [0.75, 0.25]])\n        &gt;&gt;&gt; scale(sample, lower, upper)\n\n    \"\"\"\n    # Checking that X is within (0,1) interval\n    if (X.max() &gt; 1.0) or (X.min() &lt; 0.0):\n        raise ValueError(\"Sample is not in unit hypercube\")\n    # Vectorized scaling operation\n    X = (upper - lower) * X + lower\n    # Handling case where lower == upper\n    X[:, lower == upper] = lower[lower == upper]\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_hyper_parameter_values","title":"<code>transform_hyper_parameter_values(fun_control, hyper_parameter_values)</code>","text":"<p>Transform the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \u201cint\u201d, or \u201cfloat\u201d or \u201cnum\u201d. Let fun_control = {\u201ccore_model_hyper_dict\u201d:{ \u201cleaf_prediction\u201d: { \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d], \u201ctype\u201d: \u201cfactor\u201d, \u201cdefault\u201d: \u201cmean\u201d, \u201ccore_model_parameter_type\u201d: \u201cstr\u201d}, \u201cmax_depth\u201d: { \u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 20, \u201ctransform\u201d: \u201ctransform_power_2\u201d, \u201clower\u201d: 2, \u201cupper\u201d: 20}}} and v = {\u2018max_depth\u2019: 20,\u2019leaf_prediction\u2019: \u2018mean\u2019} and def transform_power_2(x): return 2**x. The function takes fun_control and v as input and returns a dictionary with the same structure as v. The function transforms the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \u201cint\u201d, or \u201cfloat\u201d or \u201cnum\u201d. For example, transform_hyper_parameter_values(fun_control, v) returns  {\u2018max_depth\u2019: 1048576, \u2018leaf_prediction\u2019: \u2018mean\u2019}.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>A dictionary containing the information about the core model and the hyperparameters.</p> required <code>hyper_parameter_values</code> <code>dict</code> <p>A dictionary containing the values of the hyperparameters.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the values of the hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import copy\n    from spotpython.utils.prepare import transform_hyper_parameter_values\n    fun_control = {\n    \"core_model_hyper_dict\": {\n        \"leaf_prediction\": {\n            \"levels\": [\"mean\", \"model\", \"adaptive\"],\n            \"type\": \"factor\",\n            \"default\": \"mean\",\n            \"core_model_parameter_type\": \"str\"},\n        \"max_depth\": {\"type\": \"int\",\n                      \"default\": 20\n                      \"transform\": \"transform_power_2\",\n                      \"lower\": 2,\n                      \"upper\": 20}}}\n    hyper_parameter_values = {'max_depth': 20,\n                              'leaf_prediction': 'mean'}\n    transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n    {'max_depth': 1048576,\n     'leaf_prediction': 'mean'}\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_hyper_parameter_values(fun_control, hyper_parameter_values):\n    \"\"\"\n    Transform the values of the hyperparameters according to the transform function specified in fun_control\n    if the hyperparameter is of type \"int\", or \"float\" or \"num\".\n    Let fun_control = {\"core_model_hyper_dict\":{ \"leaf_prediction\":\n    { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"},\n    \"max_depth\": { \"type\": \"int\", \"default\": 20, \"transform\": \"transform_power_2\", \"lower\": 2, \"upper\": 20}}}\n    and v = {'max_depth': 20,'leaf_prediction': 'mean'} and def transform_power_2(x): return 2**x.\n    The function takes fun_control and v as input and returns a dictionary with the same structure as v.\n    The function transforms the values of the hyperparameters according to the transform function\n    specified in fun_control if the hyperparameter is of type \"int\", or \"float\" or \"num\".\n    For example, transform_hyper_parameter_values(fun_control, v) returns\n     {'max_depth': 1048576, 'leaf_prediction': 'mean'}.\n\n    Args:\n        fun_control (dict):\n            A dictionary containing the information about the core model and the hyperparameters.\n        hyper_parameter_values (dict):\n            A dictionary containing the values of the hyperparameters.\n\n    Returns:\n        (dict):\n            A dictionary containing the values of the hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; import copy\n            from spotpython.utils.prepare import transform_hyper_parameter_values\n            fun_control = {\n            \"core_model_hyper_dict\": {\n                \"leaf_prediction\": {\n                    \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                    \"type\": \"factor\",\n                    \"default\": \"mean\",\n                    \"core_model_parameter_type\": \"str\"},\n                \"max_depth\": {\"type\": \"int\",\n                              \"default\": 20\n                              \"transform\": \"transform_power_2\",\n                              \"lower\": 2,\n                              \"upper\": 20}}}\n            hyper_parameter_values = {'max_depth': 20,\n                                      'leaf_prediction': 'mean'}\n            transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n            {'max_depth': 1048576,\n             'leaf_prediction': 'mean'}\n    \"\"\"\n    hyper_parameter_values = copy.deepcopy(hyper_parameter_values)\n    for key, value in hyper_parameter_values.items():\n        if fun_control[\"core_model_hyper_dict\"][key][\"type\"] in [\"int\", \"float\", \"num\", \"factor\"] and fun_control[\"core_model_hyper_dict\"][key][\"transform\"] != \"None\":\n            hyper_parameter_values[key] = eval(fun_control[\"core_model_hyper_dict\"][key][\"transform\"])(value)\n    return hyper_parameter_values\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_multby2_int","title":"<code>transform_multby2_int(x)</code>","text":"<p>Transformations for hyperparameters of type int.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>input, will be multiplied by 2</p> required <p>Returns:</p> Type Description <code>int</code> <p>The result of multiplying x by 2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_multby2_int\n&gt;&gt;&gt; transform_multby2_int(3)\n6\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_multby2_int(x: int) -&gt; int:\n    \"\"\"Transformations for hyperparameters of type int.\n\n    Args:\n        x (int):\n            input, will be multiplied by 2\n\n    Returns:\n        (int):\n            The result of multiplying x by 2.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_multby2_int\n        &gt;&gt;&gt; transform_multby2_int(3)\n        6\n    \"\"\"\n    return int(2 * x)\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_none_to_None","title":"<code>transform_none_to_None(x)</code>","text":"<p>Transformations for hyperparameters of type None.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>The string to transform.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The transformed string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_none_to_None\n&gt;&gt;&gt; transform_none_to_None(\"none\")\nNone\n</code></pre> Note <p>Needed for sklearn.linear_model.LogisticRegression</p> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_none_to_None(x):\n    \"\"\"\n    Transformations for hyperparameters of type None.\n\n    Args:\n        x (str): The string to transform.\n\n    Returns:\n        (str): The transformed string.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_none_to_None\n        &gt;&gt;&gt; transform_none_to_None(\"none\")\n        None\n\n    Note:\n        Needed for sklearn.linear_model.LogisticRegression\n    \"\"\"\n    if x == \"none\":\n        return None\n    else:\n        return x\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_power","title":"<code>transform_power(base, x, as_int=False)</code>","text":"<p>Raises a given base to the power of x.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>int</code> <p>The base to raise to the power of x.</p> required <code>x</code> <code>int</code> <p>The exponent.</p> required <code>as_int</code> <code>bool</code> <p>If True, returns the result as an integer.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The result of raising the base to the power of x.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_power\n&gt;&gt;&gt; transform_power(2, 3)\n8\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_power(base: int, x: int, as_int: bool = False) -&gt; float:\n    \"\"\"\n    Raises a given base to the power of x.\n\n    Args:\n        base (int):\n            The base to raise to the power of x.\n        x (int):\n            The exponent.\n        as_int (bool):\n            If True, returns the result as an integer.\n\n    Returns:\n        (float):\n            The result of raising the base to the power of x.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_power\n        &gt;&gt;&gt; transform_power(2, 3)\n        8\n    \"\"\"\n    result = base**x\n    if as_int:\n        result = int(result)\n    return result\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_power_10","title":"<code>transform_power_10(x)</code>","text":"<p>Transformations for hyperparameters of type float.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The exponent.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The result of raising 10 to the power of x.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_power_10\n&gt;&gt;&gt; transform_power_10(3)\n1000\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_power_10(x):\n    \"\"\"Transformations for hyperparameters of type float.\n\n    Args:\n        x (float): The exponent.\n\n    Returns:\n        (float): The result of raising 10 to the power of x.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_power_10\n        &gt;&gt;&gt; transform_power_10(3)\n        1000\n    \"\"\"\n    return 10**x\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_power_10_int","title":"<code>transform_power_10_int(x)</code>","text":"<p>Transformations for hyperparameters of type int. Args:     x (int): The exponent.</p> <p>Returns:</p> Type Description <code>int</code> <p>The result of raising 10 to the power of x.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_power_10_int\n&gt;&gt;&gt; transform_power_10_int(3)\n1000\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_power_10_int(x: int) -&gt; int:\n    \"\"\"Transformations for hyperparameters of type int.\n    Args:\n        x (int): The exponent.\n\n    Returns:\n        (int): The result of raising 10 to the power of x.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_power_10_int\n        &gt;&gt;&gt; transform_power_10_int(3)\n        1000\n    \"\"\"\n    return int(10**x)\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_power_2","title":"<code>transform_power_2(x)</code>","text":"<p>Transformations for hyperparameters of type float.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The exponent.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The result of raising 2 to the power of x.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_power_2\n&gt;&gt;&gt; transform_power_2(3)\n8\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_power_2(x):\n    \"\"\"Transformations for hyperparameters of type float.\n\n    Args:\n        x (float): The exponent.\n\n    Returns:\n        (float): The result of raising 2 to the power of x.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_power_2\n        &gt;&gt;&gt; transform_power_2(3)\n        8\n    \"\"\"\n    return 2**x\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_power_2_int","title":"<code>transform_power_2_int(x)</code>","text":"<p>Transformations for hyperparameters of type int.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>The exponent.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The result of raising 2 to the power of x.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_power_2_int\n&gt;&gt;&gt; transform_power_2_int(3)\n8\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_power_2_int(x: int) -&gt; int:\n    \"\"\"Transformations for hyperparameters of type int.\n\n    Args:\n        x (int): The exponent.\n\n    Returns:\n        (int): The result of raising 2 to the power of x.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_power_2_int\n        &gt;&gt;&gt; transform_power_2_int(3)\n        8\n    \"\"\"\n    return int(2**x)\n</code></pre>"}]}