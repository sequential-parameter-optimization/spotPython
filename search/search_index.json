{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"spotpython","text":""},{"location":"#surrogate-model-based-optimization-and-hyperparameter-tuning-in-python","title":"Surrogate Model Based Optimization and Hyperparameter Tuning in Python","text":"<ul> <li>Documentation for spotpython see Hyperparameter Tuning Cookbook, a guide for scikit-learn, PyTorch, river, and spotpython.</li> <li>News and updates related to spotpython see SPOTSeven</li> </ul>"},{"location":"about/","title":"Contact/Privacy Policy","text":""},{"location":"about/#address","title":"Address","text":"<p>Prof. Dr. Thomas Bartz-Beielstein TH K\u00f6ln Raum 1.519 Steinm\u00fcllerallee 6 51643 Gummersbach +49 (0)2261 8196 6391 thomas.bartz-beielstein [at] th-koeln.de www.spotseven.de</p>"},{"location":"about/#privacy-policy","title":"Privacy Policy","text":"<p>We are very delighted that you have shown interest in our enterprise. Data protection is of a particularly high priority for the management of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. The use of the Internet pages of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is possible without any indication of personal data; however, if a data subject wants to use special enterprise services via our website, processing of personal data could become necessary. If the processing of personal data is necessary and there is no statutory basis for such processing, we generally obtain consent from the data subject.</p> <p>The processing of personal data, such as the name, address, e-mail address, or telephone number of a data subject shall always be in line with the General Data Protection Regulation (GDPR), and in accordance with the country-specific data protection regulations applicable to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. By means of this data protection declaration, our enterprise would like to inform the general public of the nature, scope, and purpose of the personal data we collect, use and process. Furthermore, data subjects are informed, by means of this data protection declaration, of the rights to which they are entitled.</p> <p>As the controller, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab has implemented numerous technical and organizational measures to ensure the most complete protection of personal data processed through this website. However, Internet-based data transmissions may in principle have security gaps, so absolute protection may not be guaranteed. For this reason, every data subject is free to transfer personal data to us via alternative means, e.g. by telephone.</p> <ol> <li>Definitions</li> </ol> <p>The data protection declaration of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is based on the terms used by the European legislator for the adoption of the General Data Protection Regulation (GDPR). Our data protection declaration should be legible and understandable for the general public, as well as our customers and business partners. To ensure this, we would like to first explain the terminology used.</p> <p>In this data protection declaration, we use, inter alia, the following terms:</p> <p>a)    Personal data</p> <p>Personal data means any information relating to an identified or identifiable natural person (\u201cdata subject\u201d). An identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.</p> <p>b) Data subject</p> <p>Data subject is any identified or identifiable natural person, whose personal data is processed by the controller responsible for the processing.</p> <p>c)    Processing</p> <p>Processing is any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.</p> <p>d)    Restriction of processing</p> <p>Restriction of processing is the marking of stored personal data with the aim of limiting their processing in the future.</p> <p>e)    Profiling</p> <p>Profiling means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person\u2019s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements.</p> <p>f)     Pseudonymisation</p> <p>Pseudonymisation is the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person.</p> <p>g)    Controller or controller responsible for the processing</p> <p>Controller or controller responsible for the processing is the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law.</p> <p>h)    Processor</p> <p>Processor is a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller.</p> <p>i)      Recipient</p> <p>Recipient is a natural or legal person, public authority, agency or another body, to which the personal data are disclosed, whether a third party or not. However, public authorities which may receive personal data in the framework of a particular inquiry in accordance with Union or Member State law shall not be regarded as recipients; the processing of those data by those public authorities shall be in compliance with the applicable data protection rules according to the purposes of the processing.</p> <p>j)      Third party</p> <p>Third party is a natural or legal person, public authority, agency or body other than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data.</p> <p>k)    Consent</p> <p>Consent of the data subject is any freely given, specific, informed and unambiguous indication of the data subject\u2019s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.</p> <ol> <li>Name and Address of the controller</li> </ol> <p>Controller for the purposes of the General Data Protection Regulation (GDPR), other data protection laws applicable in Member states of the European Union and other provisions related to data protection is:</p> <p>TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab</p> <p>Steinm\u00fcllerallee 1</p> <p>51643 Gummersbach</p> <p>Deutschland</p> <p>Phone: +49 2261 81966391</p> <p>Email: thomas.bartz-beielstein@th-koeln.de</p> <p>Website: www.spotseven.de</p> <ol> <li>Collection of general data and information</li> </ol> <p>The website of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab collects a series of general data and information when a data subject or automated system calls up the website. This general data and information are stored in the server log files. Collected may be (1) the browser types and versions used, (2) the operating system used by the accessing system, (3) the website from which an accessing system reaches our website (so-called referrers), (4) the sub-websites, (5) the date and time of access to the Internet site, (6) an Internet protocol address (IP address), (7) the Internet service provider of the accessing system, and (8) any other similar data and information that may be used in the event of attacks on our information technology systems.</p> <p>When using these general data and information, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab does not draw any conclusions about the data subject. Rather, this information is needed to (1) deliver the content of our website correctly, (2) optimize the content of our website as well as its advertisement, (3) ensure the long-term viability of our information technology systems and website technology, and (4) provide law enforcement authorities with the information necessary for criminal prosecution in case of a cyber-attack. Therefore, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab analyzes anonymously collected data and information statistically, with the aim of increasing the data protection and data security of our enterprise, and to ensure an optimal level of protection for the personal data we process. The anonymous data of the server log files are stored separately from all personal data provided by a data subject.</p> <ol> <li>Comments function in the blog on the website</li> </ol> <p>The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab offers users the possibility to leave individual comments on individual blog contributions on a blog, which is on the website of the controller. A blog is a web-based, publicly-accessible portal, through which one or more people called bloggers or web-bloggers may post articles or write down thoughts in so-called blogposts. Blogposts may usually be commented by third parties.</p> <p>If a data subject leaves a comment on the blog published on this website, the comments made by the data subject are also stored and published, as well as information on the date of the commentary and on the user\u2019s (pseudonym) chosen by the data subject. In addition, the IP address assigned by the Internet service provider (ISP) to the data subject is also logged. This storage of the IP address takes place for security reasons, and in case the data subject violates the rights of third parties, or posts illegal content through a given comment. The storage of these personal data is, therefore, in the own interest of the data controller, so that he can exculpate in the event of an infringement. This collected personal data will not be passed to third parties, unless such a transfer is required by law or serves the aim of the defense of the data controller.</p> <ol> <li>Routine erasure and blocking of personal data</li> </ol> <p>The data controller shall process and store the personal data of the data subject only for the period necessary to achieve the purpose of storage, or as far as this is granted by the European legislator or other legislators in laws or regulations to which the controller is subject to.</p> <p>If the storage purpose is not applicable, or if a storage period prescribed by the European legislator or another competent legislator expires, the personal data are routinely blocked or erased in accordance with legal requirements.</p> <ol> <li>Rights of the data subject</li> </ol> <p>a) Right of confirmation</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller the confirmation as to whether or not personal data concerning him or her are being processed. If a data subject wishes to avail himself of this right of confirmation, he or she may, at any time, contact our Data Protection Officer or another employee of the controller.</p> <p>b) Right of access</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller free information about his or her personal data stored at any time and a copy of this information. Furthermore, the European directives and regulations grant the data subject access to the following information:</p> <p>the purposes of the processing; the categories of personal data concerned; the recipients or categories of recipients to whom the personal data have been or will be disclosed, in particular recipients in third countries or international organisations; where possible, the envisaged period for which the personal data will be stored, or, if not possible, the criteria used to determine that period; the existence of the right to request from the controller rectification or erasure of personal data, or restriction of processing of personal data concerning the data subject, or to object to such processing; the existence of the right to lodge a complaint with a supervisory authority; where the personal data are not collected from the data subject, any available information as to their source; the existence of automated decision-making, including profiling, referred to in Article 22(1) and (4) of the GDPR and, at least in those cases, meaningful information about the logic involved, as well as the significance and envisaged consequences of such processing for the data subject. Furthermore, the data subject shall have a right to obtain information as to whether personal data are transferred to a third country or to an international organisation. Where this is the case, the data subject shall have the right to be informed of the appropriate safeguards relating to the transfer.</p> <p>If a data subject wishes to avail himself of this right of access, he or she may at any time contact our Data Protection Officer or another employee of the controller.</p> <p>c) Right to rectification</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller without undue delay the rectification of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement.</p> <p>If a data subject wishes to exercise this right to rectification, he or she may, at any time, contact our Data Protection Officer or another employee of the controller.</p> <p>d) Right to erasure (Right to be forgotten)</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies, as long as the processing is not necessary:</p> <p>The personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed. The data subject withdraws consent to which the processing is based according to point (a) of Article 6(1) of the GDPR, or point (a) of Article 9(2) of the GDPR, and where there is no other legal ground for the processing. The data subject objects to the processing pursuant to Article 21(1) of the GDPR and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2) of the GDPR. The personal data have been unlawfully processed. The personal data must be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject. The personal data have been collected in relation to the offer of information society services referred to in Article 8(1) of the GDPR. If one of the aforementioned reasons applies, and a data subject wishes to request the erasure of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee shall promptly ensure that the erasure request is complied with immediately.</p> <p>Where the controller has made personal data public and is obliged pursuant to Article 17(1) to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform other controllers processing the personal data that the data subject has requested erasure by such controllers of any links to, or copy or replication of, those personal data, as far as processing is not required. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the necessary measures in individual cases.</p> <p>e) Right of restriction of processing</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller restriction of processing where one of the following applies:</p> <p>The accuracy of the personal data is contested by the data subject, for a period enabling the controller to verify the accuracy of the personal data. The processing is unlawful and the data subject opposes the erasure of the personal data and requests instead the restriction of their use instead. The controller no longer needs the personal data for the purposes of the processing, but they are required by the data subject for the establishment, exercise or defence of legal claims. The data subject has objected to processing pursuant to Article 21(1) of the GDPR pending the verification whether the legitimate grounds of the controller override those of the data subject. If one of the aforementioned conditions is met, and a data subject wishes to request the restriction of the processing of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the restriction of the processing.</p> <p>f) Right to data portability</p> <p>Each data subject shall have the right granted by the European legislator, to receive the personal data concerning him or her, which was provided to a controller, in a structured, commonly used and machine-readable format. He or she shall have the right to transmit those data to another controller without hindrance from the controller to which the personal data have been provided, as long as the processing is based on consent pursuant to point (a) of Article 6(1) of the GDPR or point (a) of Article 9(2) of the GDPR, or on a contract pursuant to point (b) of Article 6(1) of the GDPR, and the processing is carried out by automated means, as long as the processing is not necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller.</p> <p>Furthermore, in exercising his or her right to data portability pursuant to Article 20(1) of the GDPR, the data subject shall have the right to have personal data transmitted directly from one controller to another, where technically feasible and when doing so does not adversely affect the rights and freedoms of others.</p> <p>In order to assert the right to data portability, the data subject may at any time contact the Data Protection Officer designated by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee.</p> <p>g) Right to object</p> <p>Each data subject shall have the right granted by the European legislator to object, on grounds relating to his or her particular situation, at any time, to processing of personal data concerning him or her, which is based on point (e) or (f) of Article 6(1) of the GDPR. This also applies to profiling based on these provisions.</p> <p>The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall no longer process the personal data in the event of the objection, unless we can demonstrate compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject, or for the establishment, exercise or defence of legal claims.</p> <p>If the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab processes personal data for direct marketing purposes, the data subject shall have the right to object at any time to processing of personal data concerning him or her for such marketing. This applies to profiling to the extent that it is related to such direct marketing. If the data subject objects to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab to the processing for direct marketing purposes, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab will no longer process the personal data for these purposes.</p> <p>In addition, the data subject has the right, on grounds relating to his or her particular situation, to object to processing of personal data concerning him or her by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab for scientific or historical research purposes, or for statistical purposes pursuant to Article 89(1) of the GDPR, unless the processing is necessary for the performance of a task carried out for reasons of public interest.</p> <p>In order to exercise the right to object, the data subject may directly contact the Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee. In addition, the data subject is free in the context of the use of information society services, and notwithstanding Directive 2002/58/EC, to use his or her right to object by automated means using technical specifications.</p> <p>h) Automated individual decision-making, including profiling</p> <p>Each data subject shall have the right granted by the European legislator not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her, or similarly significantly affects him or her, as long as the decision (1) is not is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) is not authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, or (3) is not based on the data subject\u2019s explicit consent.</p> <p>If the decision (1) is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) it is based on the data subject\u2019s explicit consent, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall implement suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and contest the decision.</p> <p>If the data subject wishes to exercise the rights concerning automated individual decision-making, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller.</p> <p>i) Right to withdraw data protection consent</p> <p>Each data subject shall have the right granted by the European legislator to withdraw his or her consent to processing of his or her personal data at any time.</p> <p>f the data subject wishes to exercise the right to withdraw the consent, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller.</p> <ol> <li>Data protection provisions about the application and use of Facebook</li> </ol> <p>On this website, the controller has integrated components of the enterprise Facebook. Facebook is a social network.</p> <p>A social network is a place for social meetings on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Facebook allows social network users to include the creation of private profiles, upload photos, and network through friend requests.</p> <p>The operating company of Facebook is Facebook, Inc., 1 Hacker Way, Menlo Park, CA 94025, United States. If a person lives outside of the United States or Canada, the controller is the Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Ireland.</p> <p>With each call-up to one of the individual pages of this Internet website, which is operated by the controller and into which a Facebook component (Facebook plug-ins) was integrated, the web browser on the information technology system of the data subject is automatically prompted to download display of the corresponding Facebook component from Facebook through the Facebook component. An overview of all the Facebook Plug-ins may be accessed under https://developers.facebook.com/docs/plugins/. During the course of this technical procedure, Facebook is made aware of what specific sub-site of our website was visited by the data subject.</p> <p>If the data subject is logged in at the same time on Facebook, Facebook detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-site of our Internet page was visited by the data subject. This information is collected through the Facebook component and associated with the respective Facebook account of the data subject. If the data subject clicks on one of the Facebook buttons integrated into our website, e.g. the \u201cLike\u201d button, or if the data subject submits a comment, then Facebook matches this information with the personal Facebook user account of the data subject and stores the personal data.</p> <p>Facebook always receives, through the Facebook component, information about a visit to our website by the data subject, whenever the data subject is logged in at the same time on Facebook during the time of the call-up to our website. This occurs regardless of whether the data subject clicks on the Facebook component or not. If such a transmission of information to Facebook is not desirable for the data subject, then he or she may prevent this by logging off from their Facebook account before a call-up to our website is made.</p> <p>The data protection guideline published by Facebook, which is available at https://facebook.com/about/privacy/, provides information about the collection, processing and use of personal data by Facebook. In addition, it is explained there what setting options Facebook offers to protect the privacy of the data subject. In addition, different configuration options are made available to allow the elimination of data transmission to Facebook, e.g. the Facebook blocker of the provider Webgraph, which may be obtained under http://webgraph.com/resources/facebookblocker/. These applications may be used by the data subject to eliminate a data transmission to Facebook.</p> <ol> <li>Data protection provisions about the application and use of Google+</li> </ol> <p>On this website, the controller has integrated the Google+ button as a component. Google+ is a so-called social network. A social network is a social meeting place on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Google+ allows users of the social network to include the creation of private profiles, upload photos and network through friend requests.</p> <p>The operating company of Google+ is Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this website, which is operated by the controller and on which a Google+ button has been integrated, the Internet browser on the information technology system of the data subject automatically downloads a display of the corresponding Google+ button of Google through the respective Google+ button component. During the course of this technical procedure, Google is made aware of what specific sub-page of our website was visited by the data subject. More detailed information about Google+ is available under https://developers.google.com/+/.</p> <p>If the data subject is logged in at the same time to Google+, Google recognizes with each call-up to our website by the data subject and for the entire duration of his or her stay on our Internet site, which specific sub-pages of our Internet page were visited by the data subject. This information is collected through the Google+ button and Google matches this with the respective Google+ account associated with the data subject.</p> <p>If the data subject clicks on the Google+ button integrated on our website and thus gives a Google+ 1 recommendation, then Google assigns this information to the personal Google+ user account of the data subject and stores the personal data. Google stores the Google+ 1 recommendation of the data subject, making it publicly available in accordance with the terms and conditions accepted by the data subject in this regard. Subsequently, a Google+ 1 recommendation given by the data subject on this website together with other personal data, such as the Google+ account name used by the data subject and the stored photo, is stored and processed on other Google services, such as search-engine results of the Google search engine, the Google account of the data subject or in other places, e.g. on Internet pages, or in relation to advertisements. Google is also able to link the visit to this website with other personal data stored on Google. Google further records this personal information with the purpose of improving or optimizing the various Google services.</p> <p>Through the Google+ button, Google receives information that the data subject visited our website, if the data subject at the time of the call-up to our website is logged in to Google+. This occurs regardless of whether the data subject clicks or doesn\u2019t click on the Google+ button.</p> <p>If the data subject does not wish to transmit personal data to Google, he or she may prevent such transmission by logging out of his Google+ account before calling up our website.</p> <p>Further information and the data protection provisions of Google may be retrieved under https://www.google.com/intl/en/policies/privacy/. More references from Google about the Google+ 1 button may be obtained under https://developers.google.com/+/web/buttons-policy.</p> <ol> <li>Data protection provisions about the application and use of Jetpack for WordPress</li> </ol> <p>On this website, the controller has integrated Jetpack. Jetpack is a WordPress plug-in, which provides additional features to the operator of a website based on WordPress. Jetpack allows the Internet site operator, inter alia, an overview of the visitors of the site. By displaying related posts and publications, or the ability to share content on the page, it is also possible to increase visitor numbers. In addition, security features are integrated into Jetpack, so a Jetpack-using site is better protected against brute-force attacks. Jetpack also optimizes and accelerates the loading of images on the website.</p> <p>The operating company of Jetpack Plug-Ins for WordPress is the Automattic Inc., 132 Hawthorne Street, San Francisco, CA 94107, UNITED STATES. The operating enterprise uses the tracking technology created by Quantcast Inc., 201 Third Street, San Francisco, CA 94103, UNITED STATES.</p> <p>Jetpack sets a cookie on the information technology system used by the data subject. The definition of cookies is explained above. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Jetpack component was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to submit data through the Jetpack component for analysis purposes to Automattic. During the course of this technical procedure Automattic receives data that is used to create an overview of website visits. The data obtained in this way serves the analysis of the behaviour of the data subject, which has access to the Internet page of the controller and is analyzed with the aim to optimize the website. The data collected through the Jetpack component is not used to identify the data subject without a prior obtaining of a separate express consent of the data subject. The data comes also to the notice of Quantcast. Quantcast uses the data for the same purposes as Automattic.</p> <p>The data subject can, as stated above, prevent the setting of cookies through our website at any time by means of a corresponding adjustment of the web browser used and thus permanently deny the setting of cookies. Such an adjustment to the Internet browser used would also prevent Automattic/Quantcast from setting a cookie on the information technology system of the data subject. In addition, cookies already in use by Automattic/Quantcast may be deleted at any time via a web browser or other software programs.</p> <p>In addition, the data subject has the possibility of objecting to a collection of data relating to a use of this Internet site that are generated by the Jetpack cookie as well as the processing of these data by Automattic/Quantcast and the chance to preclude any such. For this purpose, the data subject must press the \u2018opt-out\u2019 button under the link https://www.quantcast.com/opt-out/ which sets an opt-out cookie. The opt-out cookie set with this purpose is placed on the information technology system used by the data subject. If the cookies are deleted on the system of the data subject, then the data subject must call up the link again and set a new opt-out cookie.</p> <p>With the setting of the opt-out cookie, however, the possibility exists that the websites of the controller are not fully usable anymore by the data subject.</p> <p>The applicable data protection provisions of Automattic may be accessed under https://automattic.com/privacy/. The applicable data protection provisions of Quantcast can be accessed under https://www.quantcast.com/privacy/.</p> <ol> <li>Data protection provisions about the application and use of LinkedIn</li> </ol> <p>The controller has integrated components of the LinkedIn Corporation on this website. LinkedIn is a web-based social network that enables users with existing business contacts to connect and to make new business contacts. Over 400 million registered people in more than 200 countries use LinkedIn. Thus, LinkedIn is currently the largest platform for business contacts and one of the most visited websites in the world.</p> <p>The operating company of LinkedIn is LinkedIn Corporation, 2029 Stierlin Court Mountain View, CA 94043, UNITED STATES. For privacy matters outside of the UNITED STATES LinkedIn Ireland, Privacy Policy Issues, Wilton Plaza, Wilton Place, Dublin 2, Ireland, is responsible.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a LinkedIn component (LinkedIn plug-in) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to the download of a display of the corresponding LinkedIn component of LinkedIn. Further information about the LinkedIn plug-in may be accessed under https://developer.linkedin.com/plugins. During the course of this technical procedure, LinkedIn gains knowledge of what specific sub-page of our website was visited by the data subject.</p> <p>If the data subject is logged in at the same time on LinkedIn, LinkedIn detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-page of our Internet page was visited by the data subject. This information is collected through the LinkedIn component and associated with the respective LinkedIn account of the data subject. If the data subject clicks on one of the LinkedIn buttons integrated on our website, then LinkedIn assigns this information to the personal LinkedIn user account of the data subject and stores the personal data.</p> <p>LinkedIn receives information via the LinkedIn component that the data subject has visited our website, provided that the data subject is logged in at LinkedIn at the time of the call-up to our website. This occurs regardless of whether the person clicks on the LinkedIn button or not. If such a transmission of information to LinkedIn is not desirable for the data subject, then he or she may prevent this by logging off from their LinkedIn account before a call-up to our website is made.</p> <p>LinkedIn provides under https://www.linkedin.com/psettings/guest-controls the possibility to unsubscribe from e-mail messages, SMS messages and targeted ads, as well as the ability to manage ad settings. LinkedIn also uses affiliates such as Eire, Google Analytics, BlueKai, DoubleClick, Nielsen, Comscore, Eloqua, and Lotame. The setting of such cookies may be denied under https://www.linkedin.com/legal/cookie-policy. The applicable privacy policy for LinkedIn is available under https://www.linkedin.com/legal/privacy-policy. The LinkedIn Cookie Policy is available under https://www.linkedin.com/legal/cookie-policy.</p> <ol> <li>Data protection provisions about the application and use of Twitter</li> </ol> <p>On this website, the controller has integrated components of Twitter. Twitter is a multilingual, publicly-accessible microblogging service on which users may publish and spread so-called \u2018tweets,\u2019 e.g. short messages, which are limited to 140 characters. These short messages are available for everyone, including those who are not logged on to Twitter. The tweets are also displayed to so-called followers of the respective user. Followers are other Twitter users who follow a user\u2019s tweets. Furthermore, Twitter allows you to address a wide audience via hashtags, links or retweets.</p> <p>The operating company of Twitter is Twitter, Inc., 1355 Market Street, Suite 900, San Francisco, CA 94103, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Twitter component (Twitter button) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding Twitter component of Twitter. Further information about the Twitter buttons is available under https://about.twitter.com/de/resources/buttons. During the course of this technical procedure, Twitter gains knowledge of what specific sub-page of our website was visited by the data subject. The purpose of the integration of the Twitter component is a retransmission of the contents of this website to allow our users to introduce this web page to the digital world and increase our visitor numbers.</p> <p>If the data subject is logged in at the same time on Twitter, Twitter detects with every call-up to our website by the data subject and for the entire duration of their stay on our Internet site which specific sub-page of our Internet page was visited by the data subject. This information is collected through the Twitter component and associated with the respective Twitter account of the data subject. If the data subject clicks on one of the Twitter buttons integrated on our website, then Twitter assigns this information to the personal Twitter user account of the data subject and stores the personal data.</p> <p>Twitter receives information via the Twitter component that the data subject has visited our website, provided that the data subject is logged in on Twitter at the time of the call-up to our website. This occurs regardless of whether the person clicks on the Twitter component or not. If such a transmission of information to Twitter is not desirable for the data subject, then he or she may prevent this by logging off from their Twitter account before a call-up to our website is made.</p> <p>The applicable data protection provisions of Twitter may be accessed under https://twitter.com/privacy?lang=en.</p> <ol> <li>Data protection provisions about the application and use of YouTube</li> </ol> <p>On this website, the controller has integrated components of YouTube. YouTube is an Internet video portal that enables video publishers to set video clips and other users free of charge, which also provides free viewing, review and commenting on them. YouTube allows you to publish all kinds of videos, so you can access both full movies and TV broadcasts, as well as music videos, trailers, and videos made by users via the Internet portal.</p> <p>The operating company of YouTube is YouTube, LLC, 901 Cherry Ave., San Bruno, CA 94066, UNITED STATES. The YouTube, LLC is a subsidiary of Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a YouTube component (YouTube video) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding YouTube component. Further information about YouTube may be obtained under https://www.youtube.com/yt/about/en/. During the course of this technical procedure, YouTube and Google gain knowledge of what specific sub-page of our website was visited by the data subject.</p> <p>If the data subject is logged in on YouTube, YouTube recognizes with each call-up to a sub-page that contains a YouTube video, which specific sub-page of our Internet site was visited by the data subject. This information is collected by YouTube and Google and assigned to the respective YouTube account of the data subject.</p> <p>YouTube and Google will receive information through the YouTube component that the data subject has visited our website, if the data subject at the time of the call to our website is logged in on YouTube; this occurs regardless of whether the person clicks on a YouTube video or not. If such a transmission of this information to YouTube and Google is not desirable for the data subject, the delivery may be prevented if the data subject logs off from their own YouTube account before a call-up to our website is made.</p> <p>YouTube\u2019s data protection provisions, available at https://www.google.com/intl/en/policies/privacy/, provide information about the collection, processing and use of personal data by YouTube and Google.</p> <ol> <li>Legal basis for the processing</li> </ol> <p>Art. 6(1) lit. a GDPR serves as the legal basis for processing operations for which we obtain consent for a specific processing purpose. If the processing of personal data is necessary for the performance of a contract to which the data subject is party, as is the case, for example, when processing operations are necessary for the supply of goods or to provide any other service, the processing is based on Article 6(1) lit. b GDPR. The same applies to such processing operations which are necessary for carrying out pre-contractual measures, for example in the case of inquiries concerning our products or services. Is our company subject to a legal obligation by which processing of personal data is required, such as for the fulfillment of tax obligations, the processing is based on Art. 6(1) lit. c GDPR. In rare cases, the processing of personal data may be necessary to protect the vital interests of the data subject or of another natural person. This would be the case, for example, if a visitor were injured in our company and his name, age, health insurance data or other vital information would have to be passed on to a doctor, hospital or other third party. Then the processing would be based on Art. 6(1) lit. d GDPR. Finally, processing operations could be based on Article 6(1) lit. f GDPR. This legal basis is used for processing operations which are not covered by any of the abovementioned legal grounds, if processing is necessary for the purposes of the legitimate interests pursued by our company or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data. Such processing operations are particularly permissible because they have been specifically mentioned by the European legislator. He considered that a legitimate interest could be assumed if the data subject is a client of the controller (Recital 47 Sentence 2 GDPR).</p> <ol> <li>The legitimate interests pursued by the controller or by a third party</li> </ol> <p>Where the processing of personal data is based on Article 6(1) lit. f GDPR our legitimate interest is to carry out our business in favor of the well-being of all our employees and the shareholders.</p> <ol> <li>Period for which the personal data will be stored</li> </ol> <p>The criteria used to determine the period of storage of personal data is the respective statutory retention period. After expiration of that period, the corresponding data is routinely deleted, as long as it is no longer necessary for the fulfillment of the contract or the initiation of a contract.</p> <ol> <li>Provision of personal data as statutory or contractual requirement; Requirement necessary to enter into a contract; Obligation of the data subject to provide the personal data; possible consequences of failure to provide such data</li> </ol> <p>We clarify that the provision of personal data is partly required by law (e.g. tax regulations) or can also result from contractual provisions (e.g. information on the contractual partner). Sometimes it may be necessary to conclude a contract that the data subject provides us with personal data, which must subsequently be processed by us. The data subject is, for example, obliged to provide us with personal data when our company signs a contract with him or her. The non-provision of the personal data would have the consequence that the contract with the data subject could not be concluded. Before personal data is provided by the data subject, the data subject must contact our Data Protection Officer. Our Data Protection Officer clarifies to the data subject whether the provision of the personal data is required by law or contract or is necessary for the conclusion of the contract, whether there is an obligation to provide the personal data and the consequences of non-provision of the personal data.</p> <ol> <li>Existence of automated decision-making</li> </ol> <p>As a responsible company, we do not use automatic decision-making or profiling.</p> <p>This Privacy Policy has been generated by the Privacy Policy Generator of the External Data Protection Officers that was developed in cooperation with RC GmbH, which sells used notebooks and the Media Law Lawyers from WBS-LAW.</p>"},{"location":"download/","title":"Install spotpython","text":"<pre><code>pip install spotpython\n</code></pre>"},{"location":"examples/","title":"SPOT Examples","text":""},{"location":"examples/#simple-spotpython-run","title":"Simple spotpython run","text":"<pre><code>import numpy as np\nfrom spotpython.spot import spot\nfrom spotpython.fun.objectivefunctions import analytical\nfrom spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n\nfun = analytical().fun_branin\nfun_control = fun_control_init(lower = np.array([-5, 0]),\n                               upper = np.array([10, 15]),\n                               fun_evals=20)\ndesign_control = design_control_init(init_size=10)\nsurrogate_control = surrogate_control_init(n_theta=2)\nS = spot.Spot(fun=fun, fun_control=fun_control, design_control=design_control)\nS.run()\n</code></pre> <pre><code>spotpython tuning: 3.146824136952164 [######----] 55.00% \nspotpython tuning: 3.146824136952164 [######----] 60.00% \nspotpython tuning: 3.146824136952164 [######----] 65.00% \nspotpython tuning: 3.146824136952164 [#######---] 70.00% \nspotpython tuning: 1.1487233101571483 [########--] 75.00% \nspotpython tuning: 1.0236891516766402 [########--] 80.00% \nspotpython tuning: 0.41994270072214057 [########--] 85.00% \nspotpython tuning: 0.40193544341108023 [#########-] 90.00% \nspotpython tuning: 0.3991519598268951 [##########] 95.00% \nspotpython tuning: 0.3991519598268951 [##########] 100.00% Done...\n</code></pre> <pre><code>S.print_results()\n</code></pre> <pre><code>min y: 0.3991519598268951\nx0: 3.1546575195040987\nx1: 2.285931113926263\n</code></pre> <pre><code>S.plot_progress(log_y=True)\n</code></pre> <pre><code>S.surrogate.plot()\n</code></pre>"},{"location":"examples/#further-examples","title":"Further Examples","text":"<p>Examples can be found in the Hyperparameter Tuning Cookbook, e.g., Documentation of the Sequential Parameter Optimization.</p>"},{"location":"hyperparameter-tuning-cookbook/","title":"Hyperparameter Tuning Cookbook","text":"<p>The following is a cookbook of hyperparameter tuning recipes. It is not meant to be exhaustive, but instead act as a place to capture a number of the common patterns used in hyperparameter tuning.</p> <p>Hyperparameter Tuning Cookbook</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>spotpython<ul> <li>budget<ul> <li>ocba</li> </ul> </li> <li>build<ul> <li>kriging</li> <li>surrogates</li> </ul> </li> <li>data<ul> <li>base</li> <li>california</li> <li>california_housing</li> <li>csvdataset</li> <li>diabetes</li> <li>friedman</li> <li>lightcrossvalidationdatamodule</li> <li>lightdatamodule</li> <li>manydataset</li> <li>mnistdatamodule</li> <li>pkldataset</li> <li>torchdata</li> <li>vbdp</li> </ul> </li> <li>design<ul> <li>designs</li> <li>factorial</li> <li>spacefilling</li> <li>utils</li> </ul> </li> <li>fun<ul> <li>hyperlight</li> <li>hypersklearn</li> <li>hypertorch</li> <li>mohyperlight</li> <li>multiobjectivefunctions</li> <li>objectivefunctions</li> <li>xai_hyperlight</li> </ul> </li> <li>gp<ul> <li>covar</li> <li>distances</li> <li>functions</li> <li>gp</li> <li>gp_sep</li> <li>likelihood</li> <li>linalg</li> <li>lite</li> <li>matrix</li> <li>regressor</li> <li>util</li> </ul> </li> <li>hyperdict<ul> <li>light_hyper_dict</li> <li>sklearn_hyper_dict</li> <li>torch_hyper_dict</li> </ul> </li> <li>hyperparameters<ul> <li>architecture</li> <li>categorical</li> <li>listgenerator</li> <li>optimizer</li> <li>values</li> </ul> </li> <li>light<ul> <li>classification<ul> <li>netlightbasemapk</li> </ul> </li> <li>cnn<ul> <li>googlenet</li> <li>inceptionblock</li> <li>netcnnbase</li> </ul> </li> <li>cvmodel</li> <li>litmodel</li> <li>loadmodel</li> <li>predictmodel</li> <li>regression<ul> <li>netlightregression</li> <li>nn_condnet_regressor</li> <li>nn_funnel_regressor</li> <li>nn_linear_regressor</li> <li>nn_many_to_many_gru_regressor</li> <li>nn_many_to_many_lstm_regressor</li> <li>nn_many_to_many_rnn_regressor</li> <li>nn_resnet_regressor</li> <li>nn_transformer_regressor</li> <li>pos_enc</li> <li>rnnlightregression</li> <li>transformerlightregression</li> </ul> </li> <li>testmodel</li> <li>trainmodel</li> <li>transformer<ul> <li>attention</li> <li>encoder</li> <li>encoderblock</li> <li>multiheadattention</li> <li>positionalEncoding</li> <li>positionalEncodingBasic</li> <li>skiplinear</li> <li>transformerlightpredictor</li> </ul> </li> </ul> </li> <li>mo<ul> <li>functions</li> <li>pareto</li> <li>plot</li> </ul> </li> <li>pinns<ul> <li>nn<ul> <li>fcn</li> </ul> </li> <li>plot<ul> <li>result</li> </ul> </li> <li>solvers</li> </ul> </li> <li>plot<ul> <li>contour</li> <li>importance</li> <li>ts</li> <li>utils</li> <li>validation</li> <li>xai</li> <li>xy</li> </ul> </li> <li>sklearn<ul> <li>traintest</li> </ul> </li> <li>spot<ul> <li>spot</li> </ul> </li> <li>surrogate<ul> <li>functions<ul> <li>forr08a</li> </ul> </li> <li>kriging</li> </ul> </li> <li>torch<ul> <li>activation</li> <li>cosinewarmupcheduler</li> <li>dataframedataset</li> <li>dimensions</li> <li>mapk</li> <li>netcifar10</li> <li>netcore</li> <li>netfashionMNIST</li> <li>netregression</li> <li>netvbdp</li> <li>traintest</li> </ul> </li> <li>uc<ul> <li>plot</li> </ul> </li> <li>utils<ul> <li>aggregate</li> <li>classes</li> <li>compare</li> <li>convert</li> <li>device</li> <li>eda</li> <li>effects</li> <li>file</li> <li>init</li> <li>linalg</li> <li>math</li> <li>metrics</li> <li>misc</li> <li>numpy2json</li> <li>optimize</li> <li>parallel</li> <li>pca</li> <li>preprocess</li> <li>progress</li> <li>repair</li> <li>sampling</li> <li>scaler</li> <li>seed</li> <li>split</li> <li>stats</li> <li>tensorboard</li> <li>time</li> <li>transform</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/spotpython/budget/ocba/","title":"ocba","text":"<p>OCBA: Optimal Computing Budget Allocation</p>"},{"location":"reference/spotpython/budget/ocba/#spotpython.budget.ocba.get_ocba","title":"<code>get_ocba(means, vars, delta, verbose=False)</code>","text":"<p>Optimal Computer Budget Allocation (OCBA)</p> <p>This function calculates the budget recommendations for a given set of means, variances, and incremental budget using the OCBA algorithm.</p> References <p>[1]: Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization: An Optimal Computer Budget Allocation, pp. 49 and pp. 215 [2]: C.S.M Currie and T. Monks: How to choose the best setup for a system. A tutorial for the Simulation Workshop 2021, see: https://colab.research.google.com/github/TomMonks/sim-tools/blob/master/examples/sw21_tutorial.ipynb and https://github.com/TomMonks/sim-tools</p> <p>Parameters:</p> Name Type Description Default <code>means</code> <code>array</code> <p>An array of means.</p> required <code>vars</code> <code>array</code> <p>An array of variances.</p> required <code>delta</code> <code>int</code> <p>The incremental budget.</p> required <code>verbose</code> <code>bool</code> <p>If True, print the results.</p> <code>False</code> <p>Returns:</p> Type Description <code>array</code> <p>An array of budget recommendations.</p> Note <p>The implementation is based on the pseudo-code in the Chen et al. (p. 49), see [1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import copy\n    import numpy as np\n    from spotpython.fun import Analytical\n    from spotpython.spot import Spot\n    from spotpython.budget.ocba import get_ocba\n    from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n    # Example is based on the example from the book:\n    # Chun-Hung Chen and Loo Hay Lee:\n    #     Stochastic Simulation Optimization: An Optimal Computer Budget Allocation,\n    #     pp. 49 and pp. 215\n    #     p. 49:\n    #     mean_y = np.array([1,2,3,4,5])\n    #     var_y = np.array([1,1,9,9,4])\n    #     get_ocba(mean_y, var_y, 50)\n    #     [11  9 19  9  2]\n    fun = Analytical().fun_linear\n    fun_control = fun_control_init(\n                    lower = np.array([-1]),\n                    upper = np.array([1]),\n                    fun_evals = 20,\n                    fun_repeats = 2,\n                    noise = True,\n                    ocba_delta=1,\n                    seed=123,\n                    show_models=False,\n                    sigma=0.001,\n                    )\n    design_control = design_control_init(init_size=3, repeats=2)\n    surrogate_control = surrogate_control_init(noise=True)\n    spot_1_noisy = Spot(fun=fun,\n                    fun_control = fun_control,\n                    design_control=design_control,\n                    surrogate_control=surrogate_control)\n    spot_1_noisy.run()\n    spot_2 = copy.deepcopy(spot_1_noisy)\n    spot_2.mean_y = np.array([1,2,3,4,5])\n    spot_2.var_y = np.array([1,1,9,9,4])\n    n = 50\n    o = get_ocba(spot_2.mean_y, spot_2.var_y, n)\n    assert sum(o) == 50\n    assert (o == np.array([[11, 9, 19, 9, 2]])).all()\n    o\n        spotpython tuning: -1.000367786651468 [####------] 45.00%\n        spotpython tuning: -1.000989121350348 [######----] 60.00%\n        spotpython tuning: -1.000989121350348 [########--] 75.00%\n        spotpython tuning: -1.000989121350348 [#########-] 90.00%\n        spotpython tuning: -1.000989121350348 [##########] 100.00% Done...\n        array([11,  9, 19,  9,  2])\n</code></pre> Source code in <code>spotpython/budget/ocba.py</code> <pre><code>def get_ocba(means, vars, delta, verbose=False) -&gt; array:\n    \"\"\"\n    Optimal Computer Budget Allocation (OCBA)\n\n    This function calculates the budget recommendations for a given set of means,\n    variances, and incremental budget using the OCBA algorithm.\n\n    References:\n        [1]: Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization: An Optimal Computer Budget Allocation,\n        pp. 49 and pp. 215\n        [2]: C.S.M Currie and T. Monks: How to choose the best setup for a system.\n        A tutorial for the Simulation Workshop 2021, see:\n        https://colab.research.google.com/github/TomMonks/sim-tools/blob/master/examples/sw21_tutorial.ipynb\n        and\n        https://github.com/TomMonks/sim-tools\n\n    Args:\n        means (numpy.array):\n            An array of means.\n        vars (numpy.array):\n            An array of variances.\n        delta (int):\n            The incremental budget.\n        verbose (bool):\n            If True, print the results.\n\n    Returns:\n        (numpy.array): An array of budget recommendations.\n\n    Note:\n        The implementation is based on the pseudo-code in the Chen et al. (p. 49), see [1].\n\n    Examples:\n        &gt;&gt;&gt; import copy\n            import numpy as np\n            from spotpython.fun import Analytical\n            from spotpython.spot import Spot\n            from spotpython.budget.ocba import get_ocba\n            from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n            # Example is based on the example from the book:\n            # Chun-Hung Chen and Loo Hay Lee:\n            #     Stochastic Simulation Optimization: An Optimal Computer Budget Allocation,\n            #     pp. 49 and pp. 215\n            #     p. 49:\n            #     mean_y = np.array([1,2,3,4,5])\n            #     var_y = np.array([1,1,9,9,4])\n            #     get_ocba(mean_y, var_y, 50)\n            #     [11  9 19  9  2]\n            fun = Analytical().fun_linear\n            fun_control = fun_control_init(\n                            lower = np.array([-1]),\n                            upper = np.array([1]),\n                            fun_evals = 20,\n                            fun_repeats = 2,\n                            noise = True,\n                            ocba_delta=1,\n                            seed=123,\n                            show_models=False,\n                            sigma=0.001,\n                            )\n            design_control = design_control_init(init_size=3, repeats=2)\n            surrogate_control = surrogate_control_init(noise=True)\n            spot_1_noisy = Spot(fun=fun,\n                            fun_control = fun_control,\n                            design_control=design_control,\n                            surrogate_control=surrogate_control)\n            spot_1_noisy.run()\n            spot_2 = copy.deepcopy(spot_1_noisy)\n            spot_2.mean_y = np.array([1,2,3,4,5])\n            spot_2.var_y = np.array([1,1,9,9,4])\n            n = 50\n            o = get_ocba(spot_2.mean_y, spot_2.var_y, n)\n            assert sum(o) == 50\n            assert (o == np.array([[11, 9, 19, 9, 2]])).all()\n            o\n                spotpython tuning: -1.000367786651468 [####------] 45.00%\n                spotpython tuning: -1.000989121350348 [######----] 60.00%\n                spotpython tuning: -1.000989121350348 [########--] 75.00%\n                spotpython tuning: -1.000989121350348 [#########-] 90.00%\n                spotpython tuning: -1.000989121350348 [##########] 100.00% Done...\n                array([11,  9, 19,  9,  2])\n    \"\"\"\n    if np.all(vars &gt; 0) and (means.shape[0] &gt; 2):\n        n_designs = means.shape[0]\n        allocations = zeros(n_designs, int32)\n        ratios = zeros(n_designs, float64)\n        budget = delta\n        ranks = get_ranks(means)\n        best, second_best = argpartition(ranks, 2)[:2]\n        ratios[second_best] = 1.0\n        select = [i for i in range(n_designs) if i not in [best, second_best]]\n        temp = (means[best] - means[second_best]) / (means[best] - means[select])\n        ratios[select] = square(temp) * (vars[select] / vars[second_best])\n        select = [i for i in range(n_designs) if i not in [best]]\n        temp = (square(ratios[select]) / vars[select]).sum()\n        ratios[best] = sqrt(vars[best] * temp)\n        more_runs = full(n_designs, True, dtype=bool)\n        add_budget = zeros(n_designs, dtype=float)\n        more_alloc = True\n        if verbose:\n            print(\"\\nIn get_ocba():\")\n            print(f\"means: {means}\")\n            print(f\"vars: {vars}\")\n            print(f\"delta: {delta}\")\n            print(f\"n_designs: {n_designs}\")\n            print(f\"Allocations: {allocations}\")\n            print(f\"Ratios: {ratios}\")\n            print(f\"Budget: {budget}\")\n            print(f\"Ranks: {ranks}\")\n            print(f\"Best: {best}\")\n            print(f\"Second best: {second_best}\")\n            print(f\"Select: {select}\")\n            print(f\"Temp: {temp}\")\n            print(f\"More runs: {more_runs}\")\n            print(f\"Add budget: {add_budget}\")\n            print(f\"More allocations: {more_alloc}\")\n        while more_alloc:\n            more_alloc = False\n            ratio_s = (more_runs * ratios).sum()\n            add_budget[more_runs] = (budget / ratio_s) * ratios[more_runs]\n            add_budget = around(add_budget).astype(int)\n            mask = add_budget &lt; allocations\n            add_budget[mask] = allocations[mask]\n            more_runs[mask] = 0\n            if verbose:\n                print(\"\\nIn more_alloc:\")\n                print(f\"ratio_s: {ratio_s}\")\n                print(f\"more_runs: {more_runs}\")\n                print(f\"add_budget: {add_budget}\")\n            if mask.sum() &gt; 0:\n                more_alloc = True\n            if more_alloc:\n                budget = allocations.sum() + delta\n                budget -= (add_budget * ~more_runs).sum()\n        t_budget = add_budget.sum()\n        add_budget[best] += allocations.sum() + delta - t_budget\n        return add_budget - allocations\n    else:\n        return None\n</code></pre>"},{"location":"reference/spotpython/budget/ocba/#spotpython.budget.ocba.get_ocba_X","title":"<code>get_ocba_X(X, means, vars, delta, verbose=False)</code>","text":"<p>This function calculates the OCBA allocation and repeats the input array X along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input array to be repeated.</p> required <code>means</code> <code>list</code> <p>List of means for each alternative.</p> required <code>vars</code> <code>list</code> <p>List of variances for each alternative.</p> required <code>delta</code> <code>float</code> <p>Indifference zone parameter.</p> required <code>verbose</code> <code>bool</code> <p>If True, print the results.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Repeated array of X along the specified axis based on the OCBA allocation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.budget.ocba import get_ocba_X\n    from spotpython.utils.aggregate import aggregate_mean_var\n    import numpy as np\n    X = np.array([[1,2,3],\n                [1,2,3],\n                [4,5,6],\n                [4,5,6],\n                [4,5,6],\n                [7,8,9],\n                [7,8,9],])\n    y = np.array([1,2,30,40, 40, 500, 600  ])\n    Z = aggregate_mean_var(X=X, y=y)\n    mean_X = Z[0]\n    mean_y = Z[1]\n    var_y = Z[2]\n    print(f\"X: {X}\")\n    print(f\"y: {y}\")\n    print(f\"mean_X: {mean_X}\")\n    print(f\"mean_y: {mean_y}\")\n    print(f\"var_y: {var_y}\")\n    delta = 5\n    X_new = get_ocba_X(X=mean_X, means=mean_y, vars=var_y, delta=delta,verbose=True)\n    X_new\n    array([[4., 5., 6.],\n           [4., 5., 6.],\n           [4., 5., 6.],\n           [7., 8., 9.],\n           [7., 8., 9.]])\n</code></pre> Source code in <code>spotpython/budget/ocba.py</code> <pre><code>def get_ocba_X(X, means, vars, delta, verbose=False) -&gt; float64:\n    \"\"\"\n    This function calculates the OCBA allocation and repeats the input array X along the specified axis.\n\n    Args:\n        X (numpy.ndarray): Input array to be repeated.\n        means (list): List of means for each alternative.\n        vars (list): List of variances for each alternative.\n        delta (float): Indifference zone parameter.\n        verbose (bool): If True, print the results.\n\n    Returns:\n        (numpy.ndarray): Repeated array of X along the specified axis based on the OCBA allocation.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.budget.ocba import get_ocba_X\n            from spotpython.utils.aggregate import aggregate_mean_var\n            import numpy as np\n            X = np.array([[1,2,3],\n                        [1,2,3],\n                        [4,5,6],\n                        [4,5,6],\n                        [4,5,6],\n                        [7,8,9],\n                        [7,8,9],])\n            y = np.array([1,2,30,40, 40, 500, 600  ])\n            Z = aggregate_mean_var(X=X, y=y)\n            mean_X = Z[0]\n            mean_y = Z[1]\n            var_y = Z[2]\n            print(f\"X: {X}\")\n            print(f\"y: {y}\")\n            print(f\"mean_X: {mean_X}\")\n            print(f\"mean_y: {mean_y}\")\n            print(f\"var_y: {var_y}\")\n            delta = 5\n            X_new = get_ocba_X(X=mean_X, means=mean_y, vars=var_y, delta=delta,verbose=True)\n            X_new\n            array([[4., 5., 6.],\n                   [4., 5., 6.],\n                   [4., 5., 6.],\n                   [7., 8., 9.],\n                   [7., 8., 9.]])\n\n    \"\"\"\n    if np.all(vars &gt; 0) and (means.shape[0] &gt; 2):\n        o = get_ocba(means=means, vars=vars, delta=delta, verbose=verbose)\n        return repeat(X, o, axis=0)\n    else:\n        return None\n</code></pre>"},{"location":"reference/spotpython/build/kriging/","title":"kriging","text":""},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging","title":"<code>Kriging</code>","text":"<p>               Bases: <code>surrogates</code></p> <p>Kriging surrogate.</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>class Kriging(surrogates):\n    \"\"\"Kriging surrogate.\n    \"\"\"\n    def __init__(\n            self: object,\n            noise: bool = False,\n            var_type: List[str] = [\"num\"],\n            name: str = \"kriging\",\n            seed: int = 124,\n            model_optimizer=None,\n            model_fun_evals: Optional[int] = None,\n            min_theta: float = -3.0,\n            max_theta: float = 2.0,\n            n_theta: int = 1,\n            theta_init_zero: bool = False,\n            p_val: float = 2.0,\n            n_p: int = 1,\n            optim_p: bool = False,\n            min_Lambda: float = 1e-9,\n            max_Lambda: float = 1.,\n            log_level: int = 50,\n            spot_writer=None,\n            counter=None,\n            metric_factorial=\"canberra\",\n            **kwargs\n    ):\n        \"\"\"\n        Initialize the Kriging surrogate.\n\n        Args:\n            noise (bool): Use regression instead of interpolation kriging. Defaults to False.\n            var_type (List[str]):\n                Variable type. Can be either \"num\" (numerical) or \"factor\" (factor).\n                Defaults to [\"num\"].\n            name (str):\n                Surrogate name. Defaults to \"kriging\".\n            seed (int):\n                Random seed. Defaults to 124.\n            model_optimizer (Optional[object]):\n                Optimizer on the surrogate. If None, differential_evolution is selected.\n            model_fun_evals (Optional[int]):\n                Number of iterations used by the optimizer on the surrogate.\n            min_theta (float):\n                Min log10 theta value. Defaults to -3.\n            max_theta (float):\n                Max log10 theta value. Defaults to 2.\n            n_theta (int):\n                Number of theta values. Defaults to 1.\n            theta_init_zero (bool):\n                Initialize theta with zero. Defaults to True.\n            p_val (float):\n                p value. Used as an initial value if optim_p = True. Otherwise as a constant. Defaults to 2.\n            n_p (int):\n                Number of p values. Defaults to 1.\n            optim_p (bool):\n                Determines whether p should be optimized. Deafults to False.\n            min_Lambda (float):\n                Min Lambda value. Defaults to 1e-9.\n            max_Lambda (float):\n                Max Lambda value. Defaults to 1.\n            log_level (int):\n                Logging level, e.g., 20 is \"INFO\". Defaults to 50 (\"CRITICAL\").\n            spot_writer (Optional[object]):\n                Spot writer. Defaults to None.\n            counter (Optional[int]):\n                Counter. Defaults to None.\n            metric_factorial (str):\n                Metric for factorial. Defaults to \"canberra\". Can be \"euclidean\",\n                \"cityblock\", seuclidean\", \"sqeuclidean\", \"cosine\",\n                \"correlation\", \"hamming\", \"jaccard\", \"jensenshannon\",\n                \"chebyshev\", \"canberra\", \"braycurtis\", \"mahalanobis\", \"matching\".\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build import Kriging\n                import numpy as np\n                import matplotlib.pyplot as plt\n                from numpy import linspace, arange\n                rng = np.random.RandomState(1)\n                X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n                y = np.squeeze(X * np.sin(X))\n                training_indices = rng.choice(arange(y.size), size=6, replace=False)\n                X_train, y_train = X[training_indices], y[training_indices]\n                S = Kriging(name='kriging', seed=124)\n                S.fit(X_train, y_train)\n                mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n                plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n                plt.scatter(X_train, y_train, label=\"Observations\")\n                plt.plot(X, mean_prediction, label=\"Mean prediction\")\n                plt.fill_between(\n                    X.ravel(),\n                    mean_prediction - 1.96 * std_prediction,\n                    mean_prediction + 1.96 * std_prediction,\n                    alpha=0.5,\n                    label=r\"95% confidence interval\",\n                    )\n                plt.legend()\n                plt.xlabel(\"$x$\")\n                plt.ylabel(\"$f(x)$\")\n                _ = plt.title(\"Gaussian process regression on noise-free dataset\")\n                plt.show()\n\n        References:\n            https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html\n            [[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)]\n            scikit-learn: Gaussian Processes regression: basic introductory example\n\n        \"\"\"\n        super().__init__(name, seed, log_level)\n        self._set_internal_attributes()\n\n        self.noise = noise\n        self.var_type = var_type\n        self.name = name\n        self.seed = seed\n        self.log_level = log_level\n        self.spot_writer = spot_writer\n        self.counter = counter\n        self.metric_factorial = metric_factorial\n        self.min_theta = min_theta\n        self.max_theta = max_theta\n        self.min_Lambda = min_Lambda\n        self.max_Lambda = max_Lambda\n        self.n_theta = n_theta\n        self.p_val = p_val\n        self.n_p = n_p\n        self.optim_p = optim_p\n        self.theta_init_zero = theta_init_zero\n        self.model_optimizer = model_optimizer\n        if self.model_optimizer is None:\n            self.model_optimizer = differential_evolution\n        self.model_fun_evals = model_fun_evals\n        # differential evolution uses maxiter = 1000\n        # and sets the number of function evaluations to\n        # (maxiter + 1) * popsize * N, which results in\n        # 1000 * 15 * k, because the default popsize is 15 and\n        # N is the number of parameters. This seems to be quite large:\n        # for k=2 these are 30 000 iterations. Therefore we set this value to\n        # 100\n        if self.model_fun_evals is None:\n            self.model_fun_evals = 100\n\n        # Logging information\n        self.log[\"negLnLike\"] = []\n        self.log[\"theta\"] = []\n        self.log[\"p\"] = []\n        self.log[\"Lambda\"] = []\n        # Logger\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def _set_internal_attributes(self) -&gt; None:\n        \"\"\" Set attributes that are not using external arguments that are passed\n            to the class constructor.\n        \"\"\"\n        self.sigma = 0\n        self.eps = sqrt(spacing(1))\n        self.min_p = 1\n        self.max_p = 2\n        # Psi matrix condition:\n        self.cnd_Psi = 0\n        self.inf_Psi = False\n\n    def _initialize_variables(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; None:\n        \"\"\"\n        Initialize variables for the class instance.\n        This method takes in the independent and dependent variable data as input\n        and initializes the class instance variables.\n        It creates deep copies of the input data and stores them in the\n        instance variables `nat_X` and `nat_y`.\n        It also calculates the number of observations `n` and\n        the number of independent variables `k` from the shape of `nat_X`.\n        Finally, it creates empty arrays with the same shape as `nat_X`\n        and `nat_y` and stores them in the instance variables `cod_X` and `cod_y`.\n\n        Args:\n            self (object): The Kriging object.\n            nat_X (np.ndarray): The independent variable data.\n            nat_y (np.ndarray): The dependent variable data.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4], [1,2]])\n                nat_y = np.array([1, 2, 11])\n                S = Kriging()\n                S._initialize_variables(nat_X, nat_y)\n                print(f\"S.nat_X: {S.nat_X}\")\n                print(f\"S.nat_y: {S.nat_y}\")\n                print(f\"S.aggregated_mean_y: {S.aggregated_mean_y}\")\n                print(f\"S.min_X: {S.min_X}\")\n                print(f\"S.max_X: {S.max_X}\")\n                print(f\"S.n: {S.n}\")\n                print(f\"S.k: {S.k}\")\n                   S.nat_X: [[1 2]\n                    [3 4]\n                    [1 2]]\n                    S.nat_y: [ 1  2 11]\n                    S.aggregated_mean_y: [6. 2.]\n                    S.min_X: [1 2]\n                    S.max_X: [3 4]\n                    S.n: 3\n                    S.k: 2\n        \"\"\"\n        # Validate input dimensions\n        if nat_X.ndim != 2 or nat_y.ndim != 1:\n            raise ValueError(\"nat_X must be a 2D array and nat_y must be a 1D array.\")\n        if nat_X.shape[0] != nat_y.shape[0]:\n            raise ValueError(\"The number of samples in nat_X and nat_y must be equal.\")\n\n        # Initialize instance variables\n        self.nat_X = copy.deepcopy(nat_X)\n        self.nat_y = copy.deepcopy(nat_y)\n        self.n, self.k = self.nat_X.shape\n\n        # Calculate and store min and max of X\n        self.min_X = np.min(self.nat_X, axis=0)\n        self.max_X = np.max(self.nat_X, axis=0)\n\n        # Calculate the aggregated mean of y\n        _, aggregated_mean_y, _ = aggregate_mean_var(X=self.nat_X, y=self.nat_y)\n        self.aggregated_mean_y = np.copy(aggregated_mean_y)\n\n        # Logging the initialized variables\n        logger.debug(\"In _initialize_variables(): self.nat_X: %s\", self.nat_X)\n        logger.debug(\"In _initialize_variables(): self.nat_y: %s\", self.nat_y)\n        logger.debug(\"In _initialize_variables(): self.aggregated_mean_y: %s\", self.aggregated_mean_y)\n        logger.debug(\"In _initialize_variables(): self.min_X: %s\", self.min_X)\n        logger.debug(\"In _initialize_variables(): self.max_X: %s\", self.max_X)\n        logger.debug(\"In _initialize_variables(): self.n: %d\", self.n)\n        logger.debug(\"In _initialize_variables(): self.k: %d\", self.k)\n\n    def _set_variable_types(self) -&gt; None:\n        \"\"\"\n        Set the variable types for the class instance.\n        This method sets the variable types for the class instance based\n        on the `var_type` attribute. If the length of `var_type` is less\n        than `k`, all variable types are forced to 'num' and a warning is logged.\n        The method then creates Boolean masks for each variable\n        type ('num', 'factor', 'int', 'ordered') using numpy arrays, e.g.,\n        `num_mask = array([ True,  True])` if two numerical variables are present.\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n                nat_y = np.array([1, 2, 3])\n                var_type = [\"num\", \"int\", \"float\"]\n                n_theta=2\n                n_p=2\n                S=Kriging(var_type=var_type, seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                assert S.var_type == [\"num\", \"int\", \"float\"]\n                assert S.num_mask.all() == False\n                assert S.factor_mask.all() == False\n                assert S.int_mask.all() == False\n                assert S.ordered_mask.all() == True\n                assert np.all(S.num_mask == np.array([True, False, False]))\n                assert np.all(S.int_mask == np.array([False, True, False]))\n                assert np.all(S.ordered_mask == np.array([True, True, True]))\n\n        Returns:\n            None\n        \"\"\"\n        logger.debug(\"In _set_variable_types(): self.k: %s\", self.k)\n        logger.debug(\"In _set_variable_types(): self.var_type: %s\", self.var_type)\n\n        # Ensure var_type has appropriate length by defaulting to 'num'\n        if len(self.var_type) &lt; self.k:\n            self.var_type = ['num'] * self.k  # Corrected to fill with 'num' instead of duplicating\n            logger.warning(\"In _set_variable_types(): All variable types forced to 'num'.\")\n            logger.debug(\"In _set_variable_types(): self.var_type: %s\", self.var_type)\n        # Create masks for each type using numpy vectorized operations\n        var_type_array = np.array(self.var_type)\n        self.num_mask = (var_type_array == \"num\")\n        self.factor_mask = (var_type_array == \"factor\")\n        self.int_mask = (var_type_array == \"int\")\n        self.ordered_mask = np.isin(var_type_array, [\"int\", \"num\", \"float\"])\n        logger.debug(\"In _set_variable_types(): self.num_mask: %s\", self.num_mask)\n        logger.debug(\"In _set_variable_types(): self.factor_mask: %s\", self.factor_mask)\n        logger.debug(\"In _set_variable_types(): self.int_mask: %s\", self.int_mask)\n        logger.debug(\"In _set_variable_types(): self.ordered_mask: %s\", self.ordered_mask)\n\n    def _set_theta_values(self) -&gt; None:\n        \"\"\"\n        Set the theta values for the class instance.\n        This method sets the theta values for the class instance based\n        on the `n_theta` and `k` attributes. If `n_theta` is greater than\n        `k`, `n_theta` is set to `k` and a warning is logged.\n        The method then initializes the `theta` attribute as a list\n        of zeros with length `n_theta`.\n        The `x0_theta` attribute is also initialized as a list of ones\n        with length `n_theta`, multiplied by `n / (100 * k)`.\n\n        Args:\n            self (object): The Kriging object.\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build import Kriging\n                import numpy as np\n                from numpy import array\n                nat_X = np.array([[1, 2], [3, 4]])\n                n = nat_X.shape[0]\n                k = nat_X.shape[1]\n                nat_y = np.array([1, 2])\n                n_theta=2\n                n_p=2\n                S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=True)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                assert S.theta.all() == array([0., 0.]).all()\n                S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=False)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                t = np.ones(n_theta, dtype=float) * n / (100 * k)\n                assert S.theta.all() == t.all()\n                nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n                n = nat_X.shape[0]\n                k = nat_X.shape[1]\n                nat_y = np.array([1, 2, 3])\n                n_theta=2\n                n_p=2\n                S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=True)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                assert S.theta.all() == array([0., 0.]).all()\n                S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=False)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                t = np.ones(n_theta, dtype=float) * n / (100 * k)\n                assert S.theta.all() == t.all()\n        \"\"\"\n        logger.debug(\"In set_theta_values(): self.k: %s\", self.k)\n        logger.debug(\"In set_theta_values(): self.n_theta: %s\", self.n_theta)\n\n        # Adjust `n_theta` if it exceeds `k`\n        if self.n_theta &gt; self.k:\n            self.n_theta = self.k\n            logger.warning(\"Too few theta values or more theta values than dimensions. `n_theta` set to `k`.\")\n            logger.debug(\"In set_theta_values(): self.n_theta reset to: %s\", self.n_theta)\n\n        # Initialize theta values\n        if hasattr(self, \"theta_init_zero\") and self.theta_init_zero:\n            self.theta = np.zeros(self.n_theta, dtype=float)\n            logger.debug(\"Theta initialized to zeros: %s\", self.theta)\n        else:\n            logger.debug(\"In set_theta_values(): self.n: %s\", self.n)\n            self.theta = np.ones(self.n_theta, dtype=float) * self.n / (100 * self.k)\n            logger.debug(\"Theta initialized based on n and k: %s\", self.theta)\n\n    def _initialize_matrices(self) -&gt; None:\n        \"\"\"\n        Initialize the matrices for the class instance.\n        This method initializes several matrices and attributes for the class instance.\n        The `p` attribute is initialized as a list of ones with length `n_p`, multiplied by 2.0.\n        The `pen_val` attribute is initialized as the natural logarithm of the\n        variance of `nat_y`, multiplied by `n`, plus 1e4.\n        The `negLnLike`, `LnDetPsi`, `mu`, `U`, `SigmaSqr`, and `Lambda` attributes are all set to None.\n        The `Psi` attribute is initialized as a zero matrix with shape `(n, n)` and dtype `float64`.\n        The `psi` attribute is initialized as a zero matrix with shape `(n, 1)`.\n        The `one` attribute is initialized as a list of ones with length `n`.\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build import Kriging\n                import numpy as np\n                from numpy import log, var\n                nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n                nat_y = np.array([1, 2, 3])\n                n = nat_X.shape[0]\n                k = nat_X.shape[1]\n                n_theta=2\n                n_p=2\n                S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                S._initialize_matrices()\n                assert np.all(S.p == 2.0 * np.ones(n_p))\n                # if var(self.nat_y) is &gt; 0, then self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n                # else self.pen_val = self.n * var(self.nat_y) + 1e4\n                assert S.pen_val == nat_X.shape[0] * log(var(S.nat_y)) + 1e4\n                assert S.Psi.shape == (n, n)\n                assert S.psi.shape == (n, 1)\n                assert S.one.shape == (n,)\n\n        Returns:\n            None\n        \"\"\"\n        logger.debug(\"In _initialize_matrices(): self.n_p: %s\", self.n_p)\n\n        # Adjust `n_p` if it exceeds `k`\n        if self.n_p &gt; self.k:\n            self.n_p = self.k\n            logger.warning(\"More p values than dimensions. `n_p` set to `k`.\")\n            logger.debug(\"In _initialize_matrices(): self.n_p reset to: %s\", self.n_p)\n\n        # Initialize p\n        self.p = np.ones(self.n_p) * self.p_val\n        logger.debug(\"In _initialize_matrices(): self.p: %s\", self.p)\n\n        # Calculate variance of nat_y\n        y_variance = var(self.nat_y)\n        logger.debug(\"In _initialize_matrices(): var(self.nat_y): %s\", y_variance)\n\n        # Set penalty value based on variance\n        if y_variance &gt; 0:\n            self.pen_val = self.n * log(y_variance) + 1e4\n        else:\n            self.pen_val = self.n * y_variance + 1e4\n        logger.debug(\"In _initialize_matrices(): self.pen_val: %s\", self.pen_val)\n\n        # Initialize other attributes\n        self.negLnLike = None\n        self.LnDetPsi = None\n        self.mu = None\n        self.U = None\n        self.SigmaSqr = None\n        self.Lambda = None\n\n        # Initialize matrix Psi and vector psi\n        self.Psi = np.zeros((self.n, self.n), dtype=np.float64)\n        logger.debug(\"In _initialize_matrices(): self.Psi shape: %s\", self.Psi.shape)\n\n        self.psi = np.zeros((self.n, 1), dtype=np.float64)\n        logger.debug(\"In _initialize_matrices(): self.psi shape: %s\", self.psi.shape)\n\n        # Initialize one\n        self.one = np.ones(self.n, dtype=np.float64)\n        logger.debug(\"In _initialize_matrices(): self.one: %s\", self.one)\n\n    def _set_de_bounds(self) -&gt; None:\n        \"\"\"\n        Determine search bounds for model_optimizer, e.g., differential evolution.\n        This method sets the attribute `de_bounds` of the object to a list of lists,\n        where each inner list represents the lower and upper bounds for a parameter\n        being optimized. The number of inner lists is determined by the number of\n        parameters being optimized (`n_theta` and `n_p`), as well as whether noise is\n        being considered (`noise`).\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build import Kriging\n                S = Kriging()\n                S._set_de_bounds()\n                print(S.de_bounds)\n                    [[-3.0, 2.0]]\n                S = Kriging(n_theta=2, n_p=2, optim_p=True)\n                S._set_de_bounds()\n                print(S.de_bounds)\n                    [[-3.0, 2.0], [-3.0, 2.0], [1, 2], [1, 2]]\n                S = Kriging(n_theta=2, n_p=2, optim_p=True, noise=True)\n                S._set_de_bounds()\n                print(S.de_bounds)\n                    [[-3.0, 2.0], [-3.0, 2.0], [1, 2], [1, 2], [1e-09, 1.0]]\n                S = Kriging(n_theta=2, n_p=2, noise=True)\n                S._set_de_bounds()\n                print(S.de_bounds)\n                    [[-3.0, 2.0], [-3.0, 2.0], [1e-09, 1.0]]\n\n        Returns:\n            None\n        \"\"\"\n        logger.debug(\"In _set_de_bounds(): self.min_theta: %s\", self.min_theta)\n        logger.debug(\"In _set_de_bounds(): self.max_theta: %s\", self.max_theta)\n        logger.debug(\"In _set_de_bounds(): self.n_theta: %s\", self.n_theta)\n        logger.debug(\"In _set_de_bounds(): self.optim_p: %s\", self.optim_p)\n        logger.debug(\"In _set_de_bounds(): self.min_p: %s\", self.min_p)\n        logger.debug(\"In _set_de_bounds(): self.max_p: %s\", self.max_p)\n        logger.debug(\"In _set_de_bounds(): self.n_p: %s\", self.n_p)\n        logger.debug(\"In _set_de_bounds(): self.noise: %s\", self.noise)\n        logger.debug(\"In _set_de_bounds(): self.min_Lambda: %s\", self.min_Lambda)\n        logger.debug(\"In _set_de_bounds(): self.max_Lambda: %s\", self.max_Lambda)\n\n        de_bounds = [[self.min_theta, self.max_theta] for _ in range(self.n_theta)]\n        if self.optim_p:\n            de_bounds += [[self.min_p, self.max_p] for _ in range(self.n_p)]\n            if self.noise:\n                de_bounds.append([self.min_Lambda, self.max_Lambda])\n        else:\n            if self.noise:\n                de_bounds.append([self.min_Lambda, self.max_Lambda])\n        self.de_bounds = de_bounds\n        logger.debug(\"In _set_de_bounds(): self.de_bounds: %s\", self.de_bounds)\n\n    def _optimize_model(self) -&gt; Union[List[float], Tuple[float]]:\n        \"\"\"\n        Optimize the model using the specified model_optimizer.\n\n        This method uses the specified model_optimizer to optimize the\n        likelihood function (`fun_likelihood`) with respect to the model parameters.\n        The optimization is performed within the bounds specified by the attribute\n        `de_bounds`.\n        The result of the optimization is returned as a list or tuple of optimized parameter values.\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                n=2\n                p=2\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                S._initialize_matrices()\n                S._set_de_bounds()\n                new_theta_p_Lambda = S._optimize_model()\n                print(new_theta_p_Lambda)\n                    [0.12167915 1.49467909 1.82808259 1.69648798 0.79564346]\n            &gt;&gt;&gt; from spotpython.build import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                n_theta=2\n                n_p=2\n                S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                S._initialize_matrices()\n                S._set_de_bounds()\n                new_theta_p_Lambda = S._optimize_model()\n                assert  len(new_theta_p_Lambda) == n_theta + n_p + 1\n            &gt;&gt;&gt; from spotpython.build import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                n_theta=2\n                n_p=2\n                S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=False)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                S._initialize_matrices()\n                S._set_de_bounds()\n                new_theta_p_Lambda = S._optimize_model()\n                assert len(new_theta_p_Lambda) == n_theta + n_p\n            &gt;&gt;&gt; from spotpython.build import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                n_theta=2\n                n_p=1\n                S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=False)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                S._initialize_matrices()\n                S._set_de_bounds()\n                new_theta_p_Lambda = S._optimize_model()\n                assert  len(new_theta_p_Lambda) == n_theta + n_p\n            &gt;&gt;&gt; from spotpython.build import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                n_theta=1\n                n_p=1\n                S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=False, noise=False)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                S._initialize_matrices()\n                S._set_de_bounds()\n                new_theta_p_Lambda = S._optimize_model()\n                assert  len(new_theta_p_Lambda) == 1\n\n        Returns:\n            result[\"x\"] (Union[List[float], Tuple[float]]):\n                A list or tuple of optimized parameter values.\n        \"\"\"\n        logger.debug(\"Entering _optimize_model.\")\n        if not callable(self.model_optimizer):\n            logger.error(\"model_optimizer is not callable.\")\n            raise ValueError(\"model_optimizer must be a callable function or method.\")\n\n        optimizer_strategies: Dict[str, Dict] = {\n            'dual_annealing': {'func': self.fun_likelihood, 'bounds': self.de_bounds},\n            'differential_evolution': {\n                'func': self.fun_likelihood,\n                'bounds': self.de_bounds,\n                'maxiter': self.model_fun_evals,\n                'seed': self.seed\n            },\n            'direct': {\n                'func': self.fun_likelihood,\n                'bounds': self.de_bounds,\n                'eps': 1e-2\n            },\n            'shgo': {'func': self.fun_likelihood, 'bounds': self.de_bounds},\n            'basinhopping': {'func': self.fun_likelihood, 'x0': np.mean(self.de_bounds, axis=1)}\n        }\n\n        optimizer_name = self.model_optimizer.__name__\n        logger.debug(\"Optimizer selected: %s\", optimizer_name)\n\n        if optimizer_name not in optimizer_strategies:\n            logger.info(\"Using default options for optimizer: %s\", optimizer_name)\n            optimizer_args = {'func': self.fun_likelihood, 'bounds': self.de_bounds}\n        else:\n            optimizer_args = optimizer_strategies[optimizer_name]\n\n        logger.debug(\"Parameters for optimization: %s\", optimizer_args)\n\n        try:\n            result = self.model_optimizer(**optimizer_args)\n        except Exception as e:\n            logger.error(\"Optimization failed due to error: %s\", str(e))\n            raise\n\n        if \"x\" not in result:\n            logger.error(\"Optimization result does not contain 'x'. Result: %s\", result)\n            raise ValueError(\"The optimization result does not contain the expected 'x' key.\")\n        logger.debug(\"Optimization result: %s\", result)\n        optimized_parameters = list(result[\"x\"])\n        logger.debug(\"Extracted optimized parameters: %s\", optimized_parameters)\n        return optimized_parameters\n\n    def _extract_from_bounds(self, new_theta_p_Lambda: np.ndarray) -&gt; None:\n        \"\"\"\n        Extract `theta`, `p`, and `Lambda` from bounds. The kriging object stores\n        `theta` as an array,  `p` as an array, and `Lambda` as a float.\n\n        Args:\n            self (object): The Kriging object.\n            new_theta_p_Lambda (np.ndarray):\n                1d-array with theta, p, and Lambda values. Order is important.\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.build import Kriging\n                num_theta = 2\n                num_p = 3\n                S = Kriging(\n                    seed=124,\n                    n_theta=num_theta,\n                    n_p=num_p,\n                    optim_p=True,\n                    noise=True\n                )\n                bounds_array = np.array([1, 2, 3, 4, 5, 6])\n                S._extract_from_bounds(new_theta_p_Lambda=bounds_array)\n                assert np.array_equal(S.theta,\n                    [1, 2]), f\"Expected theta to be [1, 2] but got {S.theta}\"\n                assert np.array_equal(S.p,\n                    [3, 4, 5]), f\"Expected p to be [3, 4, 5] but got {S.p}\"\n                assert S.Lambda == 6, f\"Expected Lambda to be 6 but got {S.Lambda}\"\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.build import Kriging\n                num_theta = 1\n                num_p = 1\n                S = Kriging(\n                    seed=124,\n                    n_theta=num_theta,\n                    n_p=num_p,\n                    optim_p=False,\n                    noise=False\n                )\n                bounds_array = np.array([1])\n                S._extract_from_bounds(new_theta_p_Lambda=bounds_array)\n                assert np.array_equal(S.theta,\n                    [1]), f\"Expected theta to be [1] but got {S.theta}\"\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.build import Kriging\n                num_theta = 1\n                num_p = 2\n                S = Kriging(\n                    seed=124,\n                    n_theta=num_theta,\n                    n_p=num_p,\n                    optim_p=True,\n                    noise=True\n                )\n                bounds_array = np.array([1, 2, 3, 4])\n                S._extract_from_bounds(new_theta_p_Lambda=bounds_array)\n                assert np.array_equal(S.theta,\n                    [1]), f\"Expected theta to be [1, 2] but got {S.theta}\"\n                assert np.array_equal(S.p,\n                    [2, 3]), f\"Expected p to be [3, 4, 5] but got {S.p}\"\n                assert S.Lambda == 4, f\"Expected Lambda to be 6 but got {S.Lambda}\"\n\n        \"\"\"\n        logger.debug(\"Extracting parameters from: %s\", new_theta_p_Lambda)\n\n        # Validate array length\n        expected_length = self.n_theta\n        if self.optim_p:\n            expected_length += self.n_p\n        if self.noise:\n            expected_length += 1\n\n        if len(new_theta_p_Lambda) &lt; expected_length:\n            logger.error(\"Input array is too short. Expected at least %d elements, got %d.\",\n                         expected_length, len(new_theta_p_Lambda))\n            raise ValueError(f\"Input array must have at least {expected_length} elements.\")\n\n        # Extract theta\n        self.theta = new_theta_p_Lambda[:self.n_theta]\n        logger.debug(\"Extracted theta: %s\", self.theta)\n\n        if self.optim_p:\n            # Extract p if optim_p is True\n            self.p = new_theta_p_Lambda[self.n_theta:self.n_theta + self.n_p]\n            logger.debug(\"Extracted p: %s\", self.p)\n\n        if self.noise:\n            # Extract Lambda\n            lambda_index = self.n_theta + (self.n_p if self.optim_p else 0)\n            self.Lambda = new_theta_p_Lambda[lambda_index]\n            logger.debug(\"Extracted Lambda: %s\", self.Lambda)\n\n    def build_Psi(self) -&gt; None:\n        \"\"\"\n        Constructs a new (n x n) correlation matrix Psi to reflect new data\n        or a change in hyperparameters.\n        This method uses `theta`, `p`, and coded `X` values to construct the\n        correlation matrix as described in [Forr08a, p.57].\n\n        Attributes:\n            Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n            cnd_Psi (float): Condition number of Psi.\n            inf_Psi (bool): True if Psi is infinite, False otherwise.\n\n        Raises:\n            LinAlgError: If building Psi fails.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[0], [1]])\n                nat_y = np.array([0, 1])\n                n=1\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                print(S.nat_X)\n                print(S.nat_y)\n                S._set_theta_values()\n                print(f\"S.theta: {S.theta}\")\n                S._initialize_matrices()\n                S._set_de_bounds()\n                new_theta_p_Lambda = S._optimize_model()\n                S._extract_from_bounds(new_theta_p_Lambda)\n                print(f\"S.theta: {S.theta}\")\n                S.build_Psi()\n                print(f\"S.Psi: {S.Psi}\")\n                    [[0]\n                    [1]]\n                    [0 1]\n                    S.theta: [0.]\n                    S.theta: [1.60036366]\n                    S.Psi: [[1.00000001e+00 4.96525625e-18]\n                    [4.96525625e-18 1.00000001e+00]]\n        \"\"\"\n        try:\n            n = self.n\n            k = self.k\n            theta10 = np.power(10.0, self.theta)\n\n            # Ensure theta has the correct length\n            if self.n_theta == 1:\n                theta10 = theta10 * np.ones(k)\n\n            # Initialize the Psi matrix\n            self.Psi = np.zeros((n, n), dtype=np.float64)\n\n            # Calculate the distance matrix using ordered variables\n            if self.ordered_mask.any():\n                X_ordered = self.nat_X[:, self.ordered_mask]\n                D_ordered = squareform(\n                    pdist(X_ordered, metric='sqeuclidean', w=theta10[self.ordered_mask])\n                )\n                self.Psi += D_ordered\n\n            # Add the contribution of factor variables to the distance matrix\n            if self.factor_mask.any():\n                X_factor = self.nat_X[:, self.factor_mask]\n                D_factor = squareform(\n                    pdist(X_factor, metric=self.metric_factorial, w=theta10[self.factor_mask])\n                )\n                self.Psi += D_factor\n\n            # Calculate correlation from distance\n            self.Psi = np.exp(-self.Psi)\n\n            # Adjust diagonal elements for noise or minimum epsilon\n            diag_indices = np.diag_indices_from(self.Psi)\n            if self.noise:\n                self.Psi[diag_indices] += self.Lambda\n                logger.debug(\"Noise level Lambda applied to diagonal: %s\", self.Lambda)\n            else:\n                self.Psi[diag_indices] += self.eps\n\n            # Check for infinite values\n            self.inf_Psi = np.isinf(self.Psi).any()\n\n            # Calculate condition number\n            self.cnd_Psi = cond(self.Psi)\n            logger.debug(\"Condition number of Psi: %f\", self.cnd_Psi)\n\n        except LinAlgError as err:\n            logger.error(\"Building Psi failed. Error: %s, Type: %s\", err, type(err))\n            raise\n\n    def build_U(self, scipy: bool = True) -&gt; None:\n        \"\"\"\n        Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57].\n        This method uses either `scipy_cholesky` or numpy's `cholesky` to perform the Cholesky factorization of Psi.\n\n        Args:\n            self (object):\n                The Kriging object.\n            scipy (bool):\n                If True, use `scipy_cholesky`.\n                If False, use numpy's `cholesky`.\n                Defaults to True.\n\n        Returns:\n            None\n\n        Raises:\n            LinAlgError:\n                If Cholesky factorization fails for Psi.\n\n        Attributes:\n            U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[0], [1]])\n                nat_y = np.array([0, 1])\n                n=1\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                print(S.nat_X)\n                print(S.nat_y)\n                S._set_theta_values()\n                print(f\"S.theta: {S.theta}\")\n                S._initialize_matrices()\n                S._set_de_bounds()\n                new_theta_p_Lambda = S._optimize_model()\n                S._extract_from_bounds(new_theta_p_Lambda)\n                print(f\"S.theta: {S.theta}\")\n                S.build_Psi()\n                print(f\"S.Psi: {S.Psi}\")\n                S.build_U()\n                print(f\"S.U:{S.U}\")\n                    [[0]\n                    [1]]\n                    [0 1]\n                    S.theta: [0.]\n                    S.theta: [1.60036366]\n                    S.Psi: [[1.00000001e+00 4.96525625e-18]\n                    [4.96525625e-18 1.00000001e+00]]\n                    S.U:[[1.00000001e+00 4.96525622e-18]\n                    [0.00000000e+00 1.00000001e+00]]\n        \"\"\"\n        try:\n            self.U = scipy_cholesky(self.Psi, lower=True) if scipy else cholesky(self.Psi)\n            self.U = self.U.T\n        except LinAlgError as err:\n            print(f\"build_U() Cholesky failed for Psi:\\n {self.Psi}. {err=}, {type(err)=}\")\n\n    def fit(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; object:\n        \"\"\"\n        Fits the hyperparameters (`theta`, `p`, `Lambda`) of the Kriging model.\n        The function computes the following internal values:\n        1. `theta`, `p`, and `Lambda` values via optimization of the function `fun_likelihood()`.\n        2. Correlation matrix `Psi` via `buildPsi()`.\n        3. U matrix via `buildU()`.\n\n        Args:\n            self (object): The Kriging object.\n            nat_X (np.ndarray): Sample points.\n            nat_y (np.ndarray): Function values.\n\n        Returns:\n            object: Fitted estimator.\n\n        Attributes:\n            theta (np.ndarray): Kriging theta values. Shape (k,).\n            p (np.ndarray): Kriging p values. Shape (k,).\n            LnDetPsi (np.float64): Determinant Psi matrix.\n            Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n            psi (np.ndarray): psi vector. Shape (n,).\n            one (np.ndarray): vector of ones. Shape (n,).\n            mu (np.float64): Kriging expected mean value mu.\n            U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n            SigmaSqr (np.float64): Sigma squared value.\n            Lambda (float): lambda noise value.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 0], [1, 0]])\n                nat_y = np.array([1, 2])\n                S = Kriging()\n                S.fit(nat_X, nat_y)\n                print(S.Psi)\n                [[1.00000001 1.        ]\n                [1.         1.00000001]]\n\n        \"\"\"\n        logger.debug(\"In fit(): nat_X: %s\", nat_X)\n        logger.debug(\"In fit(): nat_y: %s\", nat_y)\n        self._initialize_variables(nat_X, nat_y)\n        self._set_variable_types()\n        self._set_theta_values()\n        self._initialize_matrices()\n        # build_Psi() and build_U() are called in fun_likelihood\n        self._set_de_bounds()\n        # Finally, set new theta and p values and update the surrogate again\n        # for new_theta_p_Lambda in de_results[\"x\"]:\n        new_theta_p_Lambda = self._optimize_model()\n        self._extract_from_bounds(new_theta_p_Lambda)\n        self.build_Psi()\n        self.build_U()\n        # TODO: check if the following line is necessary!\n        self.likelihood()\n        self.update_log()\n\n    def predict(self, nat_X: ndarray, return_val: str = \"y\") -&gt; Union[float, Tuple[float, float]]:\n        \"\"\"\n        This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X.\n\n        Args:\n            self (object): The Kriging object.\n            nat_X (ndarray): Design variable to evaluate in natural units.\n            return_val (str): Specifies which prediction values to return. It can be \"y\", \"s\", \"ei\", or \"all\".\n\n        Returns:\n            Union[float, Tuple[float, float, float]]: Depending on `return_val`, returns the predicted value,\n            predicted error, expected improvement, or all.\n\n        Raises:\n            TypeError: If `nat_X` is not an ndarray or doesn't match expected dimensions.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                from numpy import linspace, arange\n                rng = np.random.RandomState(1)\n                X = linspace(start=0, stop=10, num=1_0).reshape(-1, 1)\n                y = np.squeeze(X * np.sin(X))\n                training_indices = rng.choice(arange(y.size), size=6, replace=False)\n                X_train, y_train = X[training_indices], y[training_indices]\n                S = Kriging(name='kriging', seed=124)\n                S.fit(X_train, y_train)\n                mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n                print(f\"mean_prediction: {mean_prediction}\")\n                print(f\"std_prediction: {std_prediction}\")\n                print(f\"s_ei: {s_ei}\")\n        \"\"\"\n        if not isinstance(nat_X, ndarray):\n            raise TypeError(f\"Expected an ndarray, got {type(nat_X)} instead.\")\n\n        try:\n            X = nat_X.reshape(-1, self.nat_X.shape[1])\n            X = repair_non_numeric(X, self.var_type)\n        except Exception as e:\n            raise TypeError(\"Input to predict was not convertible to the size of X\") from e\n\n        y, s, ei = self.predict_coded_batch(X)\n\n        if return_val == \"y\":\n            return y\n        elif return_val == \"s\":\n            return s\n        elif return_val == \"ei\":\n            return -ei\n        elif return_val == \"all\":\n            return y, s, -ei\n        else:\n            raise ValueError(f\"Invalid return_val: {return_val}. Supported values are 'y', 's', 'ei', 'all'.\")\n\n    def predict_coded(self, cod_x: np.ndarray) -&gt; Tuple[float, float, float]:\n        \"\"\"\n        Kriging prediction of one point in coded units as described in (2.20) in [Forr08a].\n        The error is returned as well. The method is used in `predict`.\n\n        Args:\n            self (object): The Kriging object.\n            cod_x (np.ndarray): Point in coded units to make prediction at.\n\n        Returns:\n            Tuple[float, float, float]: Predicted value, predicted error, and expected improvement.\n\n        Note:\n            Uses attributes such as `self.mu` and `self.SigmaSqr` that are expected\n            to be calculated by `likelihood`.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                from numpy import linspace, arange, empty\n                rng = np.random.RandomState(1)\n                X = linspace(start=0, stop=10, num=10).reshape(-1, 1)\n                y = np.squeeze(X * np.sin(X))\n                training_indices = rng.choice(arange(y.size), size=6, replace=False)\n                X_train, y_train = X[training_indices], y[training_indices]\n                S = Kriging(name='kriging', seed=124)\n                S.fit(X_train, y_train)\n                n = X.shape[0]\n                y = empty(n, dtype=float)\n                s = empty(n, dtype=float)\n                ei = empty(n, dtype=float)\n                for i in range(n):\n                    y_coded, s_coded, ei_coded = S.predict_coded(X[i, :])\n                    y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n                    s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n                    ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n                print(f\"y: {y}\")\n                print(f\"s: {s}\")\n                print(f\"ei: {-1.0*ei}\")\n        \"\"\"\n        self.build_psi_vec(cod_x)\n        mu_adj = self.mu\n        psi = self.psi\n\n        # Calculate the prediction\n        U_T_inv = solve(self.U.T, self.nat_y - self.one.dot(mu_adj))\n        f = mu_adj + psi.T.dot(solve(self.U, U_T_inv))[0]\n\n        Lambda = self.Lambda if self.noise else 0.0\n\n        # Calculate the estimated error\n        SSqr = self.SigmaSqr * (1 + Lambda - psi.T.dot(solve(self.U, solve(self.U.T, psi))))\n        SSqr = power(abs(SSqr), 0.5)[0]\n\n        # Calculate expected improvement\n        EI = self.exp_imp(y0=f, s0=SSqr)\n\n        return f, SSqr, EI\n\n    def predict_coded_batch(self, X: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Vectorized prediction for batch input using coded units.\n\n        Args:\n            X (np.ndarray): Input array of coded points.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]:\n                Arrays of predicted values, predicted errors, and expected improvements.\n        \"\"\"\n        n = X.shape[0]\n        y = np.empty(n, dtype=float)\n        s = np.empty(n, dtype=float)\n        ei = np.empty(n, dtype=float)\n\n        for i in range(n):\n            y_coded, s_coded, ei_coded = self.predict_coded(X[i, :])\n            y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n            s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n            ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n\n        return y, s, ei\n\n    def exp_imp(self, y0: float, s0: float) -&gt; float:\n        \"\"\"\n        Calculates the expected improvement for a given function value and error in coded units.\n\n        Args:\n            self (object): The Kriging object.\n            y0 (float): The function value in coded units.\n            s0 (float): The error value.\n\n        Returns:\n            float: The expected improvement value.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                S = Kriging(name='kriging', seed=124)\n                S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n                S.exp_imp(1.0, 0.0)\n                0.0\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                S = Kriging(name='kriging', seed=124)\n                S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n                # assert S.exp_imp(0.0, 1.0) == 1/np.sqrt(2*np.pi)\n                # which is approx. 0.3989422804014327\n                S.exp_imp(0.0, 1.0)\n                0.3989422804014327\n        \"\"\"\n        # We do not use the min y values, but the aggregated mean values\n        # y_min = min(self.nat_y)\n        y_min = min(self.aggregated_mean_y)\n        if s0 &lt;= 0.0:\n            EI = 0.0\n        elif s0 &gt; 0.0:\n            # Ensure (y_min - y0) / s0 is a scalar\n            diff_scaled = (y_min - y0) / s0\n            # Calculate expected improvement components\n            EI_one = (y_min - y0) * (0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * diff_scaled))\n            EI_two = (s0 * (1.0 / sqrt(2.0 * pi))) * exp(-(1.0 / 2.0) * diff_scaled ** 2)\n\n            EI = EI_one + EI_two\n\n        return EI\n\n    def update_log(self) -&gt; None:\n        \"\"\"\n        Update the log with the current values of negLnLike, theta, p, and Lambda.\n        This method appends the current values of negLnLike, theta, p (if optim_p is True),\n        and Lambda (if noise is True)\n        to their respective lists in the log dictionary.\n        It also updates the log_length attribute with the current length\n        of the negLnLike list in the log.\n        If spot_writer is not None, this method also writes the current values of\n        negLnLike, theta, p (if optim_p is True),\n        and Lambda (if noise is True) to the spot_writer object.\n\n        Args:\n            self (object): The Kriging object.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[1, 2], [3, 4]])\n                nat_y = np.array([1, 2])\n                n=2\n                p=2\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                S._initialize_matrices()\n                S._set_de_bounds()\n                new_theta_p_Lambda = S._optimize_model()\n                S.update_log()\n                print(S.log)\n                {'negLnLike': array([-1.38629436]),\n                 'theta': array([-1.14525993,  1.6123372 ]),\n                  'p': array([1.84444406, 1.74590865]),\n                  'Lambda': array([0.44268472])}\n\n        \"\"\"\n        self.log[\"negLnLike\"] = append(self.log[\"negLnLike\"], self.negLnLike)\n        self.log[\"theta\"] = append(self.log[\"theta\"], self.theta)\n        if self.optim_p:\n            self.log[\"p\"] = append(self.log[\"p\"], self.p)\n        if self.noise:\n            self.log[\"Lambda\"] = append(self.log[\"Lambda\"], self.Lambda)\n        # get the length of the log\n        self.log_length = len(self.log[\"negLnLike\"])\n        if self.spot_writer is not None:\n            negLnLike = self.negLnLike.copy()\n            self.spot_writer.add_scalar(\"spot_negLnLike\", negLnLike, self.counter+self.log_length)\n            # add the self.n_theta theta values to the writer with one key \"theta\",\n            # i.e, the same key for all theta values\n            theta = self.theta.copy()\n            self.spot_writer.add_scalars(\"spot_theta\", {f\"theta_{i}\": theta[i] for i in range(self.n_theta)},\n                                         self.counter+self.log_length)\n            if self.noise:\n                Lambda = self.Lambda.copy()\n                self.spot_writer.add_scalar(\"spot_Lambda\", Lambda, self.counter+self.log_length)\n            if self.optim_p:\n                p = self.p.copy()\n                self.spot_writer.add_scalars(\"spot_p\",\n                                             {f\"p_{i}\": p[i] for i in range(self.n_p)}, self.counter+self.log_length)\n            self.spot_writer.flush()\n\n    def fun_likelihood(self, new_theta_p_Lambda: np.ndarray) -&gt; float:\n        \"\"\"\n        Compute log likelihood for a set of hyperparameters (theta, p, Lambda).\n\n        This method computes the log likelihood for a set of hyperparameters\n        (theta, p, Lambda) using several internal methods for matrix construction\n        and likelihood evaluation. It handles potential errors by returning a\n        penalty value for non-computable states.\n\n        Args:\n            new_theta_p_Lambda (np.ndarray): An array containing `theta`, `p`, and `Lambda` values.\n\n        Returns:\n            float: The negative log likelihood or the penalty value if computation fails.\n\n        Attributes:\n            theta (np.ndarray): Kriging theta values. Shape (k,).\n            p (np.ndarray): Kriging p values. Shape (k,).\n            Lambda (float): lambda noise value.\n            Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n            U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n            negLnLike (float): Negative log likelihood of the surface at the specified hyperparameters.\n            pen_val (float): Penalty value.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[0], [1]])\n                nat_y = np.array([0, 1])\n                n=1\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                print(S.nat_X)\n                print(S.nat_y)\n                S._set_theta_values()\n                print(f\"S.theta: {S.theta}\")\n                S._initialize_matrices()\n                S._set_de_bounds()\n                new_theta_p_Lambda = S._optimize_model()\n                S._extract_from_bounds(new_theta_p_Lambda)\n                print(f\"S.theta: {S.theta}\")\n                S.build_Psi()\n                print(f\"S.Psi: {S.Psi}\")\n                S.build_U()\n                print(f\"S.U:{S.U}\")\n                S.likelihood()\n                S.negLnLike\n                    [[0]\n                    [1]]\n                    [0 1]\n                    S.theta: [0.]\n                    S.theta: [1.60036366]\n                    S.Psi: [[1.00000001e+00 4.96525625e-18]\n                    [4.96525625e-18 1.00000001e+00]]\n                    S.U:[[1.00000001e+00 4.96525622e-18]\n                    [0.00000000e+00 1.00000001e+00]]\n                    -1.3862943611198906\n        \"\"\"\n        # Extract hyperparameters\n        self._extract_from_bounds(new_theta_p_Lambda)\n        # Check transformed theta values\n        theta10 = np.power(10.0, self.theta)\n        if self.__is_any__(theta10, 0):\n            logger.warning(\"Failure in fun_likelihood: 10^theta == 0. Setting negLnLike to %s\", self.pen_val)\n            return self.pen_val\n        # Build Psi matrix and check its condition\n        self.build_Psi()\n        if getattr(self, 'inf_Psi', False) or getattr(self, 'cnd_Psi', float('inf')) &gt; 1e9:\n            logger.warning(\"Failure in fun_likelihood: Psi is ill-conditioned: %s\", getattr(self, 'cnd_Psi', 'unknown'))\n            logger.warning(\"Setting negLnLike to: %s\", self.pen_val)\n            return self.pen_val\n        # Build U matrix and handle exceptions\n        try:\n            self.build_U()\n        except Exception as error:\n            logger.error(\"Error in fun_likelihood(). Call to build_U() failed: %s\", error)\n            logger.error(\"Setting negLnLike to %.2f.\", self.pen_val)\n            return self.pen_val\n\n        # Calculate likelihood\n        self.likelihood()\n        return self.negLnLike\n\n    def __is_any__(self, x: Union[np.ndarray, Any], v: Any) -&gt; bool:\n        \"\"\"\n        Check if any element in `x` is equal to `v`.\n\n        This method checks if any element in the input array-like `x`\n        is equal to the given value `v`. Converts inputs to numpy arrays as necessary.\n\n        Args:\n            x (Union[np.ndarray, Any]): The input array-like object to check.\n            v (Any): The value to check for in `x`.\n\n        Returns:\n            bool: True if any element in `x` is equal to `v`, False otherwise.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                from numpy import power\n                import numpy as np\n                nat_X = np.array([[0], [1]])\n                nat_y = np.array([0, 1])\n                n=1\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                print(f\"S.theta: {S.theta}\")\n                print(S.__is_any__(power(10.0, S.theta), 0))\n                print(S.__is_any__(S.theta, 0))\n                    S.theta: [0.]\n                    False\n                    True\n        \"\"\"\n\n        if not isinstance(x, np.ndarray):\n            x = np.array([x])  # Wrap scalar x in an array\n        return np.any(x == v)\n\n    def likelihood(self) -&gt; None:\n        \"\"\"\n        Calculate the negative concentrated log-likelihood.\n        Implements equation (2.32) from [Forr08a] to compute the negative of the\n        concentrated log-likelihood. Updates `mu`, `SigmaSqr`, `LnDetPsi`, and `negLnLike`.\n\n        Note:\n            Requires prior calls to `build_Psi` and `build_U`.\n\n        Attributes:\n            mu (np.float64): Kriging expected mean value mu.\n            SigmaSqr (np.float64): Sigma squared value.\n            LnDetPsi (np.float64): Logarithm of the determinant of Psi matrix.\n            negLnLike (float): Negative log likelihood of the surface at the specified hyperparameters.\n\n        Raises:\n            LinAlgError: If matrix operations fail.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n                import numpy as np\n                nat_X = np.array([[1], [2]])\n                nat_y = np.array([5, 10])\n                n=2\n                p=1\n                S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n                S._initialize_variables(nat_X, nat_y)\n                S._set_variable_types()\n                S._set_theta_values()\n                S._initialize_matrices()\n                S.build_Psi()\n                S.build_U()\n                S.likelihood()\n                assert np.allclose(S.mu, 7.5, atol=1e-6)\n                E = np.exp(1)\n                sigma2 = E / (E**2 - 1) * (25/4 + 25/4*E)\n                assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)\n                print(f\"S.LnDetPsi:{S.LnDetPsi}\")\n                print(f\"S.negLnLike:{S.negLnLike}\")\n                    S.LnDetPsi:-0.1454134234019476\n                    S.negLnLike:2.2185498738611282\n        \"\"\"\n        try:\n            # Solving linear equations for needed components\n            U_T_inv_one = solve(self.U.T, self.one)\n            U_T_inv_nat_y = solve(self.U.T, self.nat_y)\n            # Mean calculation: (2.20) in [Forr08a]\n            self.mu = (self.one.T @ solve(self.U, U_T_inv_nat_y)) / (self.one.T @ solve(self.U, U_T_inv_one))\n            # Residuals\n            cod_y_minus_mu = self.nat_y - self.one * self.mu\n            # Sigma squared calculation: (2.31) in [Forr08a]\n            self.SigmaSqr = (cod_y_minus_mu.T @ solve(self.U, solve(self.U.T, cod_y_minus_mu))) / self.n\n            # Log determinant of Psi: (2.32) in [Forr08a]\n            self.LnDetPsi = 2.0 * np.sum(np.log(np.abs(np.diag(self.U))))\n            # Negative log-likelihood calculation: simplified from (2.32)\n            self.negLnLike = 0.5 * (self.n * np.log(self.SigmaSqr) + self.LnDetPsi)\n            logger.debug(\"Likelihood calculated: mu=%s, SigmaSqr=%s, LnDetPsi=%s, negLnLike=%s\",\n                         self.mu, self.SigmaSqr, self.LnDetPsi, self.negLnLike)\n        except LinAlgError as error:\n            logger.error(\"LinAlgError in likelihood calculation: %s\", error)\n            raise\n\n    def plot(self, show: Optional[bool] = True) -&gt; None:\n        \"\"\"\n        This function plots 1D and 2D surrogates.\n\n        Args:\n            self (object):\n                The Kriging object.\n            show (bool):\n                If `True`, the plots are displayed.\n                If `False`, `plt.show()` should be called outside this function.\n\n        Returns:\n            None\n\n        Note:\n            * This method provides only a basic plot. For more advanced plots,\n                use the `plot_contour()` method of the `Spot` class.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init, design_control_init\n                # 1-dimensional example\n                fun = analytical().fun_sphere\n                fun_control=fun_control_init(lower = np.array([-1]),\n                                            upper = np.array([1]),\n                                            noise=False)\n                design_control=design_control_init(init_size=10)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control)\n                S.initialize_design()\n                S.update_stats()\n                S.fit_surrogate()\n                S.surrogate.plot()\n                # 2-dimensional example\n                fun = analytical().fun_sphere\n                fun_control=fun_control_init(lower = np.array([-1, -1]),\n                                            upper = np.array([1, 1]),\n                                            noise=False)\n                design_control=design_control_init(init_size=10)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control)\n                S.initialize_design()\n                S.update_stats()\n                S.fit_surrogate()\n                S.surrogate.plot()\n        \"\"\"\n        if self.k == 1:\n            # TODO: Improve plot (add conf. interval etc.)\n            fig = pylab.figure(figsize=(9, 6))\n            n_grid = 100\n            x = linspace(\n                self.min_X[0], self.max_X[0], num=n_grid\n            )\n            y = self.predict(x)\n            plt.figure()\n            plt.plot(x, y, \"k\")\n            if show:\n                plt.show()\n\n        if self.k == 2:\n            fig = pylab.figure(figsize=(9, 6))\n            n_grid = 100\n            x = linspace(\n                self.min_X[0], self.max_X[0], num=n_grid\n            )\n            y = linspace(\n                self.min_X[1], self.max_X[1], num=n_grid\n            )\n            X, Y = meshgrid(x, y)\n            # Predict based on the optimized results\n            zz = array(\n                [self.predict(array([x, y]), return_val=\"all\") for x, y in zip(ravel(X), ravel(Y))]\n            )\n            zs = zz[:, 0, :]\n            zse = zz[:, 1, :]\n            Z = zs.reshape(X.shape)\n            Ze = zse.reshape(X.shape)\n\n            nat_point_X = self.nat_X[:, 0]\n            nat_point_Y = self.nat_X[:, 1]\n            contour_levels = 30\n            ax = fig.add_subplot(224)\n            # plot predicted values:\n            pylab.contourf(X, Y, Ze, contour_levels, cmap=\"jet\")\n            pylab.title(\"Error\")\n            pylab.colorbar()\n            # plot observed points:\n            pylab.plot(nat_point_X, nat_point_Y, \"ow\")\n            #\n            ax = fig.add_subplot(223)\n            # plot predicted values:\n            plt.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\")\n            plt.title(\"Surrogate\")\n            # plot observed points:\n            pylab.plot(nat_point_X, nat_point_Y, \"ow\", zorder=3)\n            pylab.colorbar()\n            #\n            ax = fig.add_subplot(221, projection=\"3d\")\n            ax.plot_surface(X, Y, Z, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n            #\n            ax = fig.add_subplot(222, projection=\"3d\")\n            ax.plot_surface(X, Y, Ze, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n            #\n            pylab.show()\n\n    def build_psi_vec(self, cod_x: np.ndarray) -&gt; None:\n        \"\"\"\n        Build the psi vector required for predictive methods.\n\n        Args:\n            cod_x (ndarray): Point to calculate the psi vector for.\n\n        Returns:\n            None\n\n        Modifies:\n            self.psi (np.ndarray): Updates the psi vector.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.build.kriging import Kriging\n                X_train = np.array([[1., 2.],\n                                    [2., 4.],\n                                    [3., 6.]])\n                y_train = np.array([1., 2., 3.])\n                S = Kriging(name='kriging',\n                            seed=123,\n                            log_level=50,\n                            n_theta=1,\n                            noise=False,\n                            cod_type=\"norm\")\n                S.fit(X_train, y_train)\n                # force theta to simple values:\n                S.theta = np.array([0.0])\n                nat_X = np.array([1., 0.])\n                S.psi = np.zeros((S.n, 1))\n                S.build_psi_vec(nat_X)\n                res = np.array([[np.exp(-4)],\n                    [np.exp(-17)],\n                    [np.exp(-40)]])\n                assert np.array_equal(S.psi, res)\n                print(f\"S.psi: {S.psi}\")\n                print(f\"Control value res: {res}\")\n        \"\"\"\n        logger.debug(\"Building psi vector for point: %s\", cod_x)\n        try:\n            self.psi = np.zeros((self.n, 1))\n            theta10 = np.power(10.0, self.theta)\n            if self.n_theta == 1:\n                theta10 = theta10 * np.ones(self.k)\n\n            D = np.zeros(self.n)\n\n            # Compute ordered distance contributions\n            if self.ordered_mask.any():\n                X_ordered = self.nat_X[:, self.ordered_mask]\n                x_ordered = cod_x[self.ordered_mask]\n                D += cdist(x_ordered.reshape(1, -1),\n                           X_ordered,\n                           metric='sqeuclidean',\n                           w=theta10[self.ordered_mask]).ravel()\n            logger.debug(\"Distance D after ordered mask: %s\", D)\n            # Compute factor distance contributions\n            if self.factor_mask.any():\n                X_factor = self.nat_X[:, self.factor_mask]\n                x_factor = cod_x[self.factor_mask]\n                D += cdist(x_factor.reshape(1, -1),\n                           X_factor,\n                           metric=self.metric_factorial,\n                           w=theta10[self.factor_mask]).ravel()\n            logger.debug(\"Distance D after factor mask: %s\", D)\n\n            self.psi = np.exp(-D).reshape(-1, 1)\n\n        except np.linalg.LinAlgError as err:\n            logger.error(\"Building psi failed due to a linear algebra error: %s. Error type: %s\", err, type(err))\n\n    def weighted_exp_imp(self, cod_x: np.ndarray, w: float) -&gt; float:\n        \"\"\"\n        Weighted expected improvement. Currently not used in `spotpython`\n\n        Args:\n            self (object): The Kriging object.\n            cod_x (np.ndarray): A coded design vector.\n            w (float): Weight.\n\n        Returns:\n            EI (float): Weighted expected improvement.\n\n        References:\n            [Sobester et al. 2005].\n        \"\"\"\n        y0, s0 = self.predict_coded(cod_x)\n        y_min = min(self.nat_y)\n        if s0 &lt;= 0.0:\n            EI = 0.0\n        else:\n            y_min_y0 = y_min - y0\n            EI_one = w * (\n                    y_min_y0\n                    * (0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * (y_min_y0 / s0)))\n            )\n            EI_two = (\n                    (1.0 - w)\n                    * (s0 * (1.0 / sqrt(2.0 * pi)))\n                    * (exp(-(1.0 / 2.0) * ((y_min_y0) ** 2.0 / s0 ** 2.0)))\n            )\n            EI = EI_one + EI_two\n        return EI\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.__init__","title":"<code>__init__(noise=False, var_type=['num'], name='kriging', seed=124, model_optimizer=None, model_fun_evals=None, min_theta=-3.0, max_theta=2.0, n_theta=1, theta_init_zero=False, p_val=2.0, n_p=1, optim_p=False, min_Lambda=1e-09, max_Lambda=1.0, log_level=50, spot_writer=None, counter=None, metric_factorial='canberra', **kwargs)</code>","text":"<p>Initialize the Kriging surrogate.</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>bool</code> <p>Use regression instead of interpolation kriging. Defaults to False.</p> <code>False</code> <code>var_type</code> <code>List[str]</code> <p>Variable type. Can be either \u201cnum\u201d (numerical) or \u201cfactor\u201d (factor). Defaults to [\u201cnum\u201d].</p> <code>['num']</code> <code>name</code> <code>str</code> <p>Surrogate name. Defaults to \u201ckriging\u201d.</p> <code>'kriging'</code> <code>seed</code> <code>int</code> <p>Random seed. Defaults to 124.</p> <code>124</code> <code>model_optimizer</code> <code>Optional[object]</code> <p>Optimizer on the surrogate. If None, differential_evolution is selected.</p> <code>None</code> <code>model_fun_evals</code> <code>Optional[int]</code> <p>Number of iterations used by the optimizer on the surrogate.</p> <code>None</code> <code>min_theta</code> <code>float</code> <p>Min log10 theta value. Defaults to -3.</p> <code>-3.0</code> <code>max_theta</code> <code>float</code> <p>Max log10 theta value. Defaults to 2.</p> <code>2.0</code> <code>n_theta</code> <code>int</code> <p>Number of theta values. Defaults to 1.</p> <code>1</code> <code>theta_init_zero</code> <code>bool</code> <p>Initialize theta with zero. Defaults to True.</p> <code>False</code> <code>p_val</code> <code>float</code> <p>p value. Used as an initial value if optim_p = True. Otherwise as a constant. Defaults to 2.</p> <code>2.0</code> <code>n_p</code> <code>int</code> <p>Number of p values. Defaults to 1.</p> <code>1</code> <code>optim_p</code> <code>bool</code> <p>Determines whether p should be optimized. Deafults to False.</p> <code>False</code> <code>min_Lambda</code> <code>float</code> <p>Min Lambda value. Defaults to 1e-9.</p> <code>1e-09</code> <code>max_Lambda</code> <code>float</code> <p>Max Lambda value. Defaults to 1.</p> <code>1.0</code> <code>log_level</code> <code>int</code> <p>Logging level, e.g., 20 is \u201cINFO\u201d. Defaults to 50 (\u201cCRITICAL\u201d).</p> <code>50</code> <code>spot_writer</code> <code>Optional[object]</code> <p>Spot writer. Defaults to None.</p> <code>None</code> <code>counter</code> <code>Optional[int]</code> <p>Counter. Defaults to None.</p> <code>None</code> <code>metric_factorial</code> <code>str</code> <p>Metric for factorial. Defaults to \u201ccanberra\u201d. Can be \u201ceuclidean\u201d, \u201ccityblock\u201d, seuclidean\u201d, \u201csqeuclidean\u201d, \u201ccosine\u201d, \u201ccorrelation\u201d, \u201chamming\u201d, \u201cjaccard\u201d, \u201cjensenshannon\u201d, \u201cchebyshev\u201d, \u201ccanberra\u201d, \u201cbraycurtis\u201d, \u201cmahalanobis\u201d, \u201cmatching\u201d.</p> <code>'canberra'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build import Kriging\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from numpy import linspace, arange\n    rng = np.random.RandomState(1)\n    X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n    y = np.squeeze(X * np.sin(X))\n    training_indices = rng.choice(arange(y.size), size=6, replace=False)\n    X_train, y_train = X[training_indices], y[training_indices]\n    S = Kriging(name='kriging', seed=124)\n    S.fit(X_train, y_train)\n    mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n    plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n    plt.scatter(X_train, y_train, label=\"Observations\")\n    plt.plot(X, mean_prediction, label=\"Mean prediction\")\n    plt.fill_between(\n        X.ravel(),\n        mean_prediction - 1.96 * std_prediction,\n        mean_prediction + 1.96 * std_prediction,\n        alpha=0.5,\n        label=r\"95% confidence interval\",\n        )\n    plt.legend()\n    plt.xlabel(\"$x$\")\n    plt.ylabel(\"$f(x)$\")\n    _ = plt.title(\"Gaussian process regression on noise-free dataset\")\n    plt.show()\n</code></pre> References <p>https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html [1] scikit-learn: Gaussian Processes regression: basic introductory example</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def __init__(\n        self: object,\n        noise: bool = False,\n        var_type: List[str] = [\"num\"],\n        name: str = \"kriging\",\n        seed: int = 124,\n        model_optimizer=None,\n        model_fun_evals: Optional[int] = None,\n        min_theta: float = -3.0,\n        max_theta: float = 2.0,\n        n_theta: int = 1,\n        theta_init_zero: bool = False,\n        p_val: float = 2.0,\n        n_p: int = 1,\n        optim_p: bool = False,\n        min_Lambda: float = 1e-9,\n        max_Lambda: float = 1.,\n        log_level: int = 50,\n        spot_writer=None,\n        counter=None,\n        metric_factorial=\"canberra\",\n        **kwargs\n):\n    \"\"\"\n    Initialize the Kriging surrogate.\n\n    Args:\n        noise (bool): Use regression instead of interpolation kriging. Defaults to False.\n        var_type (List[str]):\n            Variable type. Can be either \"num\" (numerical) or \"factor\" (factor).\n            Defaults to [\"num\"].\n        name (str):\n            Surrogate name. Defaults to \"kriging\".\n        seed (int):\n            Random seed. Defaults to 124.\n        model_optimizer (Optional[object]):\n            Optimizer on the surrogate. If None, differential_evolution is selected.\n        model_fun_evals (Optional[int]):\n            Number of iterations used by the optimizer on the surrogate.\n        min_theta (float):\n            Min log10 theta value. Defaults to -3.\n        max_theta (float):\n            Max log10 theta value. Defaults to 2.\n        n_theta (int):\n            Number of theta values. Defaults to 1.\n        theta_init_zero (bool):\n            Initialize theta with zero. Defaults to True.\n        p_val (float):\n            p value. Used as an initial value if optim_p = True. Otherwise as a constant. Defaults to 2.\n        n_p (int):\n            Number of p values. Defaults to 1.\n        optim_p (bool):\n            Determines whether p should be optimized. Deafults to False.\n        min_Lambda (float):\n            Min Lambda value. Defaults to 1e-9.\n        max_Lambda (float):\n            Max Lambda value. Defaults to 1.\n        log_level (int):\n            Logging level, e.g., 20 is \"INFO\". Defaults to 50 (\"CRITICAL\").\n        spot_writer (Optional[object]):\n            Spot writer. Defaults to None.\n        counter (Optional[int]):\n            Counter. Defaults to None.\n        metric_factorial (str):\n            Metric for factorial. Defaults to \"canberra\". Can be \"euclidean\",\n            \"cityblock\", seuclidean\", \"sqeuclidean\", \"cosine\",\n            \"correlation\", \"hamming\", \"jaccard\", \"jensenshannon\",\n            \"chebyshev\", \"canberra\", \"braycurtis\", \"mahalanobis\", \"matching\".\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build import Kriging\n            import numpy as np\n            import matplotlib.pyplot as plt\n            from numpy import linspace, arange\n            rng = np.random.RandomState(1)\n            X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n            y = np.squeeze(X * np.sin(X))\n            training_indices = rng.choice(arange(y.size), size=6, replace=False)\n            X_train, y_train = X[training_indices], y[training_indices]\n            S = Kriging(name='kriging', seed=124)\n            S.fit(X_train, y_train)\n            mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n            plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n            plt.scatter(X_train, y_train, label=\"Observations\")\n            plt.plot(X, mean_prediction, label=\"Mean prediction\")\n            plt.fill_between(\n                X.ravel(),\n                mean_prediction - 1.96 * std_prediction,\n                mean_prediction + 1.96 * std_prediction,\n                alpha=0.5,\n                label=r\"95% confidence interval\",\n                )\n            plt.legend()\n            plt.xlabel(\"$x$\")\n            plt.ylabel(\"$f(x)$\")\n            _ = plt.title(\"Gaussian process regression on noise-free dataset\")\n            plt.show()\n\n    References:\n        https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html\n        [[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)]\n        scikit-learn: Gaussian Processes regression: basic introductory example\n\n    \"\"\"\n    super().__init__(name, seed, log_level)\n    self._set_internal_attributes()\n\n    self.noise = noise\n    self.var_type = var_type\n    self.name = name\n    self.seed = seed\n    self.log_level = log_level\n    self.spot_writer = spot_writer\n    self.counter = counter\n    self.metric_factorial = metric_factorial\n    self.min_theta = min_theta\n    self.max_theta = max_theta\n    self.min_Lambda = min_Lambda\n    self.max_Lambda = max_Lambda\n    self.n_theta = n_theta\n    self.p_val = p_val\n    self.n_p = n_p\n    self.optim_p = optim_p\n    self.theta_init_zero = theta_init_zero\n    self.model_optimizer = model_optimizer\n    if self.model_optimizer is None:\n        self.model_optimizer = differential_evolution\n    self.model_fun_evals = model_fun_evals\n    # differential evolution uses maxiter = 1000\n    # and sets the number of function evaluations to\n    # (maxiter + 1) * popsize * N, which results in\n    # 1000 * 15 * k, because the default popsize is 15 and\n    # N is the number of parameters. This seems to be quite large:\n    # for k=2 these are 30 000 iterations. Therefore we set this value to\n    # 100\n    if self.model_fun_evals is None:\n        self.model_fun_evals = 100\n\n    # Logging information\n    self.log[\"negLnLike\"] = []\n    self.log[\"theta\"] = []\n    self.log[\"p\"] = []\n    self.log[\"Lambda\"] = []\n    # Logger\n    logger.setLevel(self.log_level)\n    logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.__is_any__","title":"<code>__is_any__(x, v)</code>","text":"<p>Check if any element in <code>x</code> is equal to <code>v</code>.</p> <p>This method checks if any element in the input array-like <code>x</code> is equal to the given value <code>v</code>. Converts inputs to numpy arrays as necessary.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, Any]</code> <p>The input array-like object to check.</p> required <code>v</code> <code>Any</code> <p>The value to check for in <code>x</code>.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if any element in <code>x</code> is equal to <code>v</code>, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    from numpy import power\n    import numpy as np\n    nat_X = np.array([[0], [1]])\n    nat_y = np.array([0, 1])\n    n=1\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n    S._initialize_variables(nat_X, nat_y)\n    S._set_variable_types()\n    S._set_theta_values()\n    print(f\"S.theta: {S.theta}\")\n    print(S.__is_any__(power(10.0, S.theta), 0))\n    print(S.__is_any__(S.theta, 0))\n        S.theta: [0.]\n        False\n        True\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def __is_any__(self, x: Union[np.ndarray, Any], v: Any) -&gt; bool:\n    \"\"\"\n    Check if any element in `x` is equal to `v`.\n\n    This method checks if any element in the input array-like `x`\n    is equal to the given value `v`. Converts inputs to numpy arrays as necessary.\n\n    Args:\n        x (Union[np.ndarray, Any]): The input array-like object to check.\n        v (Any): The value to check for in `x`.\n\n    Returns:\n        bool: True if any element in `x` is equal to `v`, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            from numpy import power\n            import numpy as np\n            nat_X = np.array([[0], [1]])\n            nat_y = np.array([0, 1])\n            n=1\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n            S._initialize_variables(nat_X, nat_y)\n            S._set_variable_types()\n            S._set_theta_values()\n            print(f\"S.theta: {S.theta}\")\n            print(S.__is_any__(power(10.0, S.theta), 0))\n            print(S.__is_any__(S.theta, 0))\n                S.theta: [0.]\n                False\n                True\n    \"\"\"\n\n    if not isinstance(x, np.ndarray):\n        x = np.array([x])  # Wrap scalar x in an array\n    return np.any(x == v)\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.build_Psi","title":"<code>build_Psi()</code>","text":"<p>Constructs a new (n x n) correlation matrix Psi to reflect new data or a change in hyperparameters. This method uses <code>theta</code>, <code>p</code>, and coded <code>X</code> values to construct the correlation matrix as described in [Forr08a, p.57].</p> <p>Attributes:</p> Name Type Description <code>Psi</code> <code>matrix</code> <p>Correlation matrix Psi. Shape (n,n).</p> <code>cnd_Psi</code> <code>float</code> <p>Condition number of Psi.</p> <code>inf_Psi</code> <code>bool</code> <p>True if Psi is infinite, False otherwise.</p> <p>Raises:</p> Type Description <code>LinAlgError</code> <p>If building Psi fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[0], [1]])\n    nat_y = np.array([0, 1])\n    n=1\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n    S._initialize_variables(nat_X, nat_y)\n    S._set_variable_types()\n    print(S.nat_X)\n    print(S.nat_y)\n    S._set_theta_values()\n    print(f\"S.theta: {S.theta}\")\n    S._initialize_matrices()\n    S._set_de_bounds()\n    new_theta_p_Lambda = S._optimize_model()\n    S._extract_from_bounds(new_theta_p_Lambda)\n    print(f\"S.theta: {S.theta}\")\n    S.build_Psi()\n    print(f\"S.Psi: {S.Psi}\")\n        [[0]\n        [1]]\n        [0 1]\n        S.theta: [0.]\n        S.theta: [1.60036366]\n        S.Psi: [[1.00000001e+00 4.96525625e-18]\n        [4.96525625e-18 1.00000001e+00]]\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def build_Psi(self) -&gt; None:\n    \"\"\"\n    Constructs a new (n x n) correlation matrix Psi to reflect new data\n    or a change in hyperparameters.\n    This method uses `theta`, `p`, and coded `X` values to construct the\n    correlation matrix as described in [Forr08a, p.57].\n\n    Attributes:\n        Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n        cnd_Psi (float): Condition number of Psi.\n        inf_Psi (bool): True if Psi is infinite, False otherwise.\n\n    Raises:\n        LinAlgError: If building Psi fails.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[0], [1]])\n            nat_y = np.array([0, 1])\n            n=1\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n            S._initialize_variables(nat_X, nat_y)\n            S._set_variable_types()\n            print(S.nat_X)\n            print(S.nat_y)\n            S._set_theta_values()\n            print(f\"S.theta: {S.theta}\")\n            S._initialize_matrices()\n            S._set_de_bounds()\n            new_theta_p_Lambda = S._optimize_model()\n            S._extract_from_bounds(new_theta_p_Lambda)\n            print(f\"S.theta: {S.theta}\")\n            S.build_Psi()\n            print(f\"S.Psi: {S.Psi}\")\n                [[0]\n                [1]]\n                [0 1]\n                S.theta: [0.]\n                S.theta: [1.60036366]\n                S.Psi: [[1.00000001e+00 4.96525625e-18]\n                [4.96525625e-18 1.00000001e+00]]\n    \"\"\"\n    try:\n        n = self.n\n        k = self.k\n        theta10 = np.power(10.0, self.theta)\n\n        # Ensure theta has the correct length\n        if self.n_theta == 1:\n            theta10 = theta10 * np.ones(k)\n\n        # Initialize the Psi matrix\n        self.Psi = np.zeros((n, n), dtype=np.float64)\n\n        # Calculate the distance matrix using ordered variables\n        if self.ordered_mask.any():\n            X_ordered = self.nat_X[:, self.ordered_mask]\n            D_ordered = squareform(\n                pdist(X_ordered, metric='sqeuclidean', w=theta10[self.ordered_mask])\n            )\n            self.Psi += D_ordered\n\n        # Add the contribution of factor variables to the distance matrix\n        if self.factor_mask.any():\n            X_factor = self.nat_X[:, self.factor_mask]\n            D_factor = squareform(\n                pdist(X_factor, metric=self.metric_factorial, w=theta10[self.factor_mask])\n            )\n            self.Psi += D_factor\n\n        # Calculate correlation from distance\n        self.Psi = np.exp(-self.Psi)\n\n        # Adjust diagonal elements for noise or minimum epsilon\n        diag_indices = np.diag_indices_from(self.Psi)\n        if self.noise:\n            self.Psi[diag_indices] += self.Lambda\n            logger.debug(\"Noise level Lambda applied to diagonal: %s\", self.Lambda)\n        else:\n            self.Psi[diag_indices] += self.eps\n\n        # Check for infinite values\n        self.inf_Psi = np.isinf(self.Psi).any()\n\n        # Calculate condition number\n        self.cnd_Psi = cond(self.Psi)\n        logger.debug(\"Condition number of Psi: %f\", self.cnd_Psi)\n\n    except LinAlgError as err:\n        logger.error(\"Building Psi failed. Error: %s, Type: %s\", err, type(err))\n        raise\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.build_U","title":"<code>build_U(scipy=True)</code>","text":"<p>Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57]. This method uses either <code>scipy_cholesky</code> or numpy\u2019s <code>cholesky</code> to perform the Cholesky factorization of Psi.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>scipy</code> <code>bool</code> <p>If True, use <code>scipy_cholesky</code>. If False, use numpy\u2019s <code>cholesky</code>. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>LinAlgError</code> <p>If Cholesky factorization fails for Psi.</p> <p>Attributes:</p> Name Type Description <code>U</code> <code>matrix</code> <p>Kriging U matrix, Cholesky decomposition. Shape (n,n).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[0], [1]])\n    nat_y = np.array([0, 1])\n    n=1\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n    S._initialize_variables(nat_X, nat_y)\n    S._set_variable_types()\n    print(S.nat_X)\n    print(S.nat_y)\n    S._set_theta_values()\n    print(f\"S.theta: {S.theta}\")\n    S._initialize_matrices()\n    S._set_de_bounds()\n    new_theta_p_Lambda = S._optimize_model()\n    S._extract_from_bounds(new_theta_p_Lambda)\n    print(f\"S.theta: {S.theta}\")\n    S.build_Psi()\n    print(f\"S.Psi: {S.Psi}\")\n    S.build_U()\n    print(f\"S.U:{S.U}\")\n        [[0]\n        [1]]\n        [0 1]\n        S.theta: [0.]\n        S.theta: [1.60036366]\n        S.Psi: [[1.00000001e+00 4.96525625e-18]\n        [4.96525625e-18 1.00000001e+00]]\n        S.U:[[1.00000001e+00 4.96525622e-18]\n        [0.00000000e+00 1.00000001e+00]]\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def build_U(self, scipy: bool = True) -&gt; None:\n    \"\"\"\n    Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57].\n    This method uses either `scipy_cholesky` or numpy's `cholesky` to perform the Cholesky factorization of Psi.\n\n    Args:\n        self (object):\n            The Kriging object.\n        scipy (bool):\n            If True, use `scipy_cholesky`.\n            If False, use numpy's `cholesky`.\n            Defaults to True.\n\n    Returns:\n        None\n\n    Raises:\n        LinAlgError:\n            If Cholesky factorization fails for Psi.\n\n    Attributes:\n        U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[0], [1]])\n            nat_y = np.array([0, 1])\n            n=1\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n            S._initialize_variables(nat_X, nat_y)\n            S._set_variable_types()\n            print(S.nat_X)\n            print(S.nat_y)\n            S._set_theta_values()\n            print(f\"S.theta: {S.theta}\")\n            S._initialize_matrices()\n            S._set_de_bounds()\n            new_theta_p_Lambda = S._optimize_model()\n            S._extract_from_bounds(new_theta_p_Lambda)\n            print(f\"S.theta: {S.theta}\")\n            S.build_Psi()\n            print(f\"S.Psi: {S.Psi}\")\n            S.build_U()\n            print(f\"S.U:{S.U}\")\n                [[0]\n                [1]]\n                [0 1]\n                S.theta: [0.]\n                S.theta: [1.60036366]\n                S.Psi: [[1.00000001e+00 4.96525625e-18]\n                [4.96525625e-18 1.00000001e+00]]\n                S.U:[[1.00000001e+00 4.96525622e-18]\n                [0.00000000e+00 1.00000001e+00]]\n    \"\"\"\n    try:\n        self.U = scipy_cholesky(self.Psi, lower=True) if scipy else cholesky(self.Psi)\n        self.U = self.U.T\n    except LinAlgError as err:\n        print(f\"build_U() Cholesky failed for Psi:\\n {self.Psi}. {err=}, {type(err)=}\")\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.build_psi_vec","title":"<code>build_psi_vec(cod_x)</code>","text":"<p>Build the psi vector required for predictive methods.</p> <p>Parameters:</p> Name Type Description Default <code>cod_x</code> <code>ndarray</code> <p>Point to calculate the psi vector for.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Modifies <p>self.psi (np.ndarray): Updates the psi vector.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.build.kriging import Kriging\n    X_train = np.array([[1., 2.],\n                        [2., 4.],\n                        [3., 6.]])\n    y_train = np.array([1., 2., 3.])\n    S = Kriging(name='kriging',\n                seed=123,\n                log_level=50,\n                n_theta=1,\n                noise=False,\n                cod_type=\"norm\")\n    S.fit(X_train, y_train)\n    # force theta to simple values:\n    S.theta = np.array([0.0])\n    nat_X = np.array([1., 0.])\n    S.psi = np.zeros((S.n, 1))\n    S.build_psi_vec(nat_X)\n    res = np.array([[np.exp(-4)],\n        [np.exp(-17)],\n        [np.exp(-40)]])\n    assert np.array_equal(S.psi, res)\n    print(f\"S.psi: {S.psi}\")\n    print(f\"Control value res: {res}\")\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def build_psi_vec(self, cod_x: np.ndarray) -&gt; None:\n    \"\"\"\n    Build the psi vector required for predictive methods.\n\n    Args:\n        cod_x (ndarray): Point to calculate the psi vector for.\n\n    Returns:\n        None\n\n    Modifies:\n        self.psi (np.ndarray): Updates the psi vector.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.build.kriging import Kriging\n            X_train = np.array([[1., 2.],\n                                [2., 4.],\n                                [3., 6.]])\n            y_train = np.array([1., 2., 3.])\n            S = Kriging(name='kriging',\n                        seed=123,\n                        log_level=50,\n                        n_theta=1,\n                        noise=False,\n                        cod_type=\"norm\")\n            S.fit(X_train, y_train)\n            # force theta to simple values:\n            S.theta = np.array([0.0])\n            nat_X = np.array([1., 0.])\n            S.psi = np.zeros((S.n, 1))\n            S.build_psi_vec(nat_X)\n            res = np.array([[np.exp(-4)],\n                [np.exp(-17)],\n                [np.exp(-40)]])\n            assert np.array_equal(S.psi, res)\n            print(f\"S.psi: {S.psi}\")\n            print(f\"Control value res: {res}\")\n    \"\"\"\n    logger.debug(\"Building psi vector for point: %s\", cod_x)\n    try:\n        self.psi = np.zeros((self.n, 1))\n        theta10 = np.power(10.0, self.theta)\n        if self.n_theta == 1:\n            theta10 = theta10 * np.ones(self.k)\n\n        D = np.zeros(self.n)\n\n        # Compute ordered distance contributions\n        if self.ordered_mask.any():\n            X_ordered = self.nat_X[:, self.ordered_mask]\n            x_ordered = cod_x[self.ordered_mask]\n            D += cdist(x_ordered.reshape(1, -1),\n                       X_ordered,\n                       metric='sqeuclidean',\n                       w=theta10[self.ordered_mask]).ravel()\n        logger.debug(\"Distance D after ordered mask: %s\", D)\n        # Compute factor distance contributions\n        if self.factor_mask.any():\n            X_factor = self.nat_X[:, self.factor_mask]\n            x_factor = cod_x[self.factor_mask]\n            D += cdist(x_factor.reshape(1, -1),\n                       X_factor,\n                       metric=self.metric_factorial,\n                       w=theta10[self.factor_mask]).ravel()\n        logger.debug(\"Distance D after factor mask: %s\", D)\n\n        self.psi = np.exp(-D).reshape(-1, 1)\n\n    except np.linalg.LinAlgError as err:\n        logger.error(\"Building psi failed due to a linear algebra error: %s. Error type: %s\", err, type(err))\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.exp_imp","title":"<code>exp_imp(y0, s0)</code>","text":"<p>Calculates the expected improvement for a given function value and error in coded units.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>y0</code> <code>float</code> <p>The function value in coded units.</p> required <code>s0</code> <code>float</code> <p>The error value.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The expected improvement value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    S = Kriging(name='kriging', seed=124)\n    S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n    S.exp_imp(1.0, 0.0)\n    0.0\n&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    S = Kriging(name='kriging', seed=124)\n    S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n    # assert S.exp_imp(0.0, 1.0) == 1/np.sqrt(2*np.pi)\n    # which is approx. 0.3989422804014327\n    S.exp_imp(0.0, 1.0)\n    0.3989422804014327\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def exp_imp(self, y0: float, s0: float) -&gt; float:\n    \"\"\"\n    Calculates the expected improvement for a given function value and error in coded units.\n\n    Args:\n        self (object): The Kriging object.\n        y0 (float): The function value in coded units.\n        s0 (float): The error value.\n\n    Returns:\n        float: The expected improvement value.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            S = Kriging(name='kriging', seed=124)\n            S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n            S.exp_imp(1.0, 0.0)\n            0.0\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            S = Kriging(name='kriging', seed=124)\n            S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n            # assert S.exp_imp(0.0, 1.0) == 1/np.sqrt(2*np.pi)\n            # which is approx. 0.3989422804014327\n            S.exp_imp(0.0, 1.0)\n            0.3989422804014327\n    \"\"\"\n    # We do not use the min y values, but the aggregated mean values\n    # y_min = min(self.nat_y)\n    y_min = min(self.aggregated_mean_y)\n    if s0 &lt;= 0.0:\n        EI = 0.0\n    elif s0 &gt; 0.0:\n        # Ensure (y_min - y0) / s0 is a scalar\n        diff_scaled = (y_min - y0) / s0\n        # Calculate expected improvement components\n        EI_one = (y_min - y0) * (0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * diff_scaled))\n        EI_two = (s0 * (1.0 / sqrt(2.0 * pi))) * exp(-(1.0 / 2.0) * diff_scaled ** 2)\n\n        EI = EI_one + EI_two\n\n    return EI\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.fit","title":"<code>fit(nat_X, nat_y)</code>","text":"<p>Fits the hyperparameters (<code>theta</code>, <code>p</code>, <code>Lambda</code>) of the Kriging model. The function computes the following internal values: 1. <code>theta</code>, <code>p</code>, and <code>Lambda</code> values via optimization of the function <code>fun_likelihood()</code>. 2. Correlation matrix <code>Psi</code> via <code>buildPsi()</code>. 3. U matrix via <code>buildU()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>nat_X</code> <code>ndarray</code> <p>Sample points.</p> required <code>nat_y</code> <code>ndarray</code> <p>Function values.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>Fitted estimator.</p> <p>Attributes:</p> Name Type Description <code>theta</code> <code>ndarray</code> <p>Kriging theta values. Shape (k,).</p> <code>p</code> <code>ndarray</code> <p>Kriging p values. Shape (k,).</p> <code>LnDetPsi</code> <code>float64</code> <p>Determinant Psi matrix.</p> <code>Psi</code> <code>matrix</code> <p>Correlation matrix Psi. Shape (n,n).</p> <code>psi</code> <code>ndarray</code> <p>psi vector. Shape (n,).</p> <code>one</code> <code>ndarray</code> <p>vector of ones. Shape (n,).</p> <code>mu</code> <code>float64</code> <p>Kriging expected mean value mu.</p> <code>U</code> <code>matrix</code> <p>Kriging U matrix, Cholesky decomposition. Shape (n,n).</p> <code>SigmaSqr</code> <code>float64</code> <p>Sigma squared value.</p> <code>Lambda</code> <code>float</code> <p>lambda noise value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build import Kriging\n    import numpy as np\n    nat_X = np.array([[1, 0], [1, 0]])\n    nat_y = np.array([1, 2])\n    S = Kriging()\n    S.fit(nat_X, nat_y)\n    print(S.Psi)\n    [[1.00000001 1.        ]\n    [1.         1.00000001]]\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def fit(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; object:\n    \"\"\"\n    Fits the hyperparameters (`theta`, `p`, `Lambda`) of the Kriging model.\n    The function computes the following internal values:\n    1. `theta`, `p`, and `Lambda` values via optimization of the function `fun_likelihood()`.\n    2. Correlation matrix `Psi` via `buildPsi()`.\n    3. U matrix via `buildU()`.\n\n    Args:\n        self (object): The Kriging object.\n        nat_X (np.ndarray): Sample points.\n        nat_y (np.ndarray): Function values.\n\n    Returns:\n        object: Fitted estimator.\n\n    Attributes:\n        theta (np.ndarray): Kriging theta values. Shape (k,).\n        p (np.ndarray): Kriging p values. Shape (k,).\n        LnDetPsi (np.float64): Determinant Psi matrix.\n        Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n        psi (np.ndarray): psi vector. Shape (n,).\n        one (np.ndarray): vector of ones. Shape (n,).\n        mu (np.float64): Kriging expected mean value mu.\n        U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n        SigmaSqr (np.float64): Sigma squared value.\n        Lambda (float): lambda noise value.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build import Kriging\n            import numpy as np\n            nat_X = np.array([[1, 0], [1, 0]])\n            nat_y = np.array([1, 2])\n            S = Kriging()\n            S.fit(nat_X, nat_y)\n            print(S.Psi)\n            [[1.00000001 1.        ]\n            [1.         1.00000001]]\n\n    \"\"\"\n    logger.debug(\"In fit(): nat_X: %s\", nat_X)\n    logger.debug(\"In fit(): nat_y: %s\", nat_y)\n    self._initialize_variables(nat_X, nat_y)\n    self._set_variable_types()\n    self._set_theta_values()\n    self._initialize_matrices()\n    # build_Psi() and build_U() are called in fun_likelihood\n    self._set_de_bounds()\n    # Finally, set new theta and p values and update the surrogate again\n    # for new_theta_p_Lambda in de_results[\"x\"]:\n    new_theta_p_Lambda = self._optimize_model()\n    self._extract_from_bounds(new_theta_p_Lambda)\n    self.build_Psi()\n    self.build_U()\n    # TODO: check if the following line is necessary!\n    self.likelihood()\n    self.update_log()\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.fun_likelihood","title":"<code>fun_likelihood(new_theta_p_Lambda)</code>","text":"<p>Compute log likelihood for a set of hyperparameters (theta, p, Lambda).</p> <p>This method computes the log likelihood for a set of hyperparameters (theta, p, Lambda) using several internal methods for matrix construction and likelihood evaluation. It handles potential errors by returning a penalty value for non-computable states.</p> <p>Parameters:</p> Name Type Description Default <code>new_theta_p_Lambda</code> <code>ndarray</code> <p>An array containing <code>theta</code>, <code>p</code>, and <code>Lambda</code> values.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The negative log likelihood or the penalty value if computation fails.</p> <p>Attributes:</p> Name Type Description <code>theta</code> <code>ndarray</code> <p>Kriging theta values. Shape (k,).</p> <code>p</code> <code>ndarray</code> <p>Kriging p values. Shape (k,).</p> <code>Lambda</code> <code>float</code> <p>lambda noise value.</p> <code>Psi</code> <code>matrix</code> <p>Correlation matrix Psi. Shape (n,n).</p> <code>U</code> <code>matrix</code> <p>Kriging U matrix, Cholesky decomposition. Shape (n,n).</p> <code>negLnLike</code> <code>float</code> <p>Negative log likelihood of the surface at the specified hyperparameters.</p> <code>pen_val</code> <code>float</code> <p>Penalty value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[0], [1]])\n    nat_y = np.array([0, 1])\n    n=1\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n    S._initialize_variables(nat_X, nat_y)\n    S._set_variable_types()\n    print(S.nat_X)\n    print(S.nat_y)\n    S._set_theta_values()\n    print(f\"S.theta: {S.theta}\")\n    S._initialize_matrices()\n    S._set_de_bounds()\n    new_theta_p_Lambda = S._optimize_model()\n    S._extract_from_bounds(new_theta_p_Lambda)\n    print(f\"S.theta: {S.theta}\")\n    S.build_Psi()\n    print(f\"S.Psi: {S.Psi}\")\n    S.build_U()\n    print(f\"S.U:{S.U}\")\n    S.likelihood()\n    S.negLnLike\n        [[0]\n        [1]]\n        [0 1]\n        S.theta: [0.]\n        S.theta: [1.60036366]\n        S.Psi: [[1.00000001e+00 4.96525625e-18]\n        [4.96525625e-18 1.00000001e+00]]\n        S.U:[[1.00000001e+00 4.96525622e-18]\n        [0.00000000e+00 1.00000001e+00]]\n        -1.3862943611198906\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def fun_likelihood(self, new_theta_p_Lambda: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute log likelihood for a set of hyperparameters (theta, p, Lambda).\n\n    This method computes the log likelihood for a set of hyperparameters\n    (theta, p, Lambda) using several internal methods for matrix construction\n    and likelihood evaluation. It handles potential errors by returning a\n    penalty value for non-computable states.\n\n    Args:\n        new_theta_p_Lambda (np.ndarray): An array containing `theta`, `p`, and `Lambda` values.\n\n    Returns:\n        float: The negative log likelihood or the penalty value if computation fails.\n\n    Attributes:\n        theta (np.ndarray): Kriging theta values. Shape (k,).\n        p (np.ndarray): Kriging p values. Shape (k,).\n        Lambda (float): lambda noise value.\n        Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n        U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n        negLnLike (float): Negative log likelihood of the surface at the specified hyperparameters.\n        pen_val (float): Penalty value.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[0], [1]])\n            nat_y = np.array([0, 1])\n            n=1\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n            S._initialize_variables(nat_X, nat_y)\n            S._set_variable_types()\n            print(S.nat_X)\n            print(S.nat_y)\n            S._set_theta_values()\n            print(f\"S.theta: {S.theta}\")\n            S._initialize_matrices()\n            S._set_de_bounds()\n            new_theta_p_Lambda = S._optimize_model()\n            S._extract_from_bounds(new_theta_p_Lambda)\n            print(f\"S.theta: {S.theta}\")\n            S.build_Psi()\n            print(f\"S.Psi: {S.Psi}\")\n            S.build_U()\n            print(f\"S.U:{S.U}\")\n            S.likelihood()\n            S.negLnLike\n                [[0]\n                [1]]\n                [0 1]\n                S.theta: [0.]\n                S.theta: [1.60036366]\n                S.Psi: [[1.00000001e+00 4.96525625e-18]\n                [4.96525625e-18 1.00000001e+00]]\n                S.U:[[1.00000001e+00 4.96525622e-18]\n                [0.00000000e+00 1.00000001e+00]]\n                -1.3862943611198906\n    \"\"\"\n    # Extract hyperparameters\n    self._extract_from_bounds(new_theta_p_Lambda)\n    # Check transformed theta values\n    theta10 = np.power(10.0, self.theta)\n    if self.__is_any__(theta10, 0):\n        logger.warning(\"Failure in fun_likelihood: 10^theta == 0. Setting negLnLike to %s\", self.pen_val)\n        return self.pen_val\n    # Build Psi matrix and check its condition\n    self.build_Psi()\n    if getattr(self, 'inf_Psi', False) or getattr(self, 'cnd_Psi', float('inf')) &gt; 1e9:\n        logger.warning(\"Failure in fun_likelihood: Psi is ill-conditioned: %s\", getattr(self, 'cnd_Psi', 'unknown'))\n        logger.warning(\"Setting negLnLike to: %s\", self.pen_val)\n        return self.pen_val\n    # Build U matrix and handle exceptions\n    try:\n        self.build_U()\n    except Exception as error:\n        logger.error(\"Error in fun_likelihood(). Call to build_U() failed: %s\", error)\n        logger.error(\"Setting negLnLike to %.2f.\", self.pen_val)\n        return self.pen_val\n\n    # Calculate likelihood\n    self.likelihood()\n    return self.negLnLike\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.likelihood","title":"<code>likelihood()</code>","text":"<p>Calculate the negative concentrated log-likelihood. Implements equation (2.32) from [Forr08a] to compute the negative of the concentrated log-likelihood. Updates <code>mu</code>, <code>SigmaSqr</code>, <code>LnDetPsi</code>, and <code>negLnLike</code>.</p> Note <p>Requires prior calls to <code>build_Psi</code> and <code>build_U</code>.</p> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float64</code> <p>Kriging expected mean value mu.</p> <code>SigmaSqr</code> <code>float64</code> <p>Sigma squared value.</p> <code>LnDetPsi</code> <code>float64</code> <p>Logarithm of the determinant of Psi matrix.</p> <code>negLnLike</code> <code>float</code> <p>Negative log likelihood of the surface at the specified hyperparameters.</p> <p>Raises:</p> Type Description <code>LinAlgError</code> <p>If matrix operations fail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[1], [2]])\n    nat_y = np.array([5, 10])\n    n=2\n    p=1\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n    S._initialize_variables(nat_X, nat_y)\n    S._set_variable_types()\n    S._set_theta_values()\n    S._initialize_matrices()\n    S.build_Psi()\n    S.build_U()\n    S.likelihood()\n    assert np.allclose(S.mu, 7.5, atol=1e-6)\n    E = np.exp(1)\n    sigma2 = E / (E**2 - 1) * (25/4 + 25/4*E)\n    assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)\n    print(f\"S.LnDetPsi:{S.LnDetPsi}\")\n    print(f\"S.negLnLike:{S.negLnLike}\")\n        S.LnDetPsi:-0.1454134234019476\n        S.negLnLike:2.2185498738611282\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def likelihood(self) -&gt; None:\n    \"\"\"\n    Calculate the negative concentrated log-likelihood.\n    Implements equation (2.32) from [Forr08a] to compute the negative of the\n    concentrated log-likelihood. Updates `mu`, `SigmaSqr`, `LnDetPsi`, and `negLnLike`.\n\n    Note:\n        Requires prior calls to `build_Psi` and `build_U`.\n\n    Attributes:\n        mu (np.float64): Kriging expected mean value mu.\n        SigmaSqr (np.float64): Sigma squared value.\n        LnDetPsi (np.float64): Logarithm of the determinant of Psi matrix.\n        negLnLike (float): Negative log likelihood of the surface at the specified hyperparameters.\n\n    Raises:\n        LinAlgError: If matrix operations fail.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[1], [2]])\n            nat_y = np.array([5, 10])\n            n=2\n            p=1\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n            S._initialize_variables(nat_X, nat_y)\n            S._set_variable_types()\n            S._set_theta_values()\n            S._initialize_matrices()\n            S.build_Psi()\n            S.build_U()\n            S.likelihood()\n            assert np.allclose(S.mu, 7.5, atol=1e-6)\n            E = np.exp(1)\n            sigma2 = E / (E**2 - 1) * (25/4 + 25/4*E)\n            assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)\n            print(f\"S.LnDetPsi:{S.LnDetPsi}\")\n            print(f\"S.negLnLike:{S.negLnLike}\")\n                S.LnDetPsi:-0.1454134234019476\n                S.negLnLike:2.2185498738611282\n    \"\"\"\n    try:\n        # Solving linear equations for needed components\n        U_T_inv_one = solve(self.U.T, self.one)\n        U_T_inv_nat_y = solve(self.U.T, self.nat_y)\n        # Mean calculation: (2.20) in [Forr08a]\n        self.mu = (self.one.T @ solve(self.U, U_T_inv_nat_y)) / (self.one.T @ solve(self.U, U_T_inv_one))\n        # Residuals\n        cod_y_minus_mu = self.nat_y - self.one * self.mu\n        # Sigma squared calculation: (2.31) in [Forr08a]\n        self.SigmaSqr = (cod_y_minus_mu.T @ solve(self.U, solve(self.U.T, cod_y_minus_mu))) / self.n\n        # Log determinant of Psi: (2.32) in [Forr08a]\n        self.LnDetPsi = 2.0 * np.sum(np.log(np.abs(np.diag(self.U))))\n        # Negative log-likelihood calculation: simplified from (2.32)\n        self.negLnLike = 0.5 * (self.n * np.log(self.SigmaSqr) + self.LnDetPsi)\n        logger.debug(\"Likelihood calculated: mu=%s, SigmaSqr=%s, LnDetPsi=%s, negLnLike=%s\",\n                     self.mu, self.SigmaSqr, self.LnDetPsi, self.negLnLike)\n    except LinAlgError as error:\n        logger.error(\"LinAlgError in likelihood calculation: %s\", error)\n        raise\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.plot","title":"<code>plot(show=True)</code>","text":"<p>This function plots 1D and 2D surrogates.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>show</code> <code>bool</code> <p>If <code>True</code>, the plots are displayed. If <code>False</code>, <code>plt.show()</code> should be called outside this function.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Note <ul> <li>This method provides only a basic plot. For more advanced plots,     use the <code>plot_contour()</code> method of the <code>Spot</code> class.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init, design_control_init\n    # 1-dimensional example\n    fun = analytical().fun_sphere\n    fun_control=fun_control_init(lower = np.array([-1]),\n                                upper = np.array([1]),\n                                noise=False)\n    design_control=design_control_init(init_size=10)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control)\n    S.initialize_design()\n    S.update_stats()\n    S.fit_surrogate()\n    S.surrogate.plot()\n    # 2-dimensional example\n    fun = analytical().fun_sphere\n    fun_control=fun_control_init(lower = np.array([-1, -1]),\n                                upper = np.array([1, 1]),\n                                noise=False)\n    design_control=design_control_init(init_size=10)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control)\n    S.initialize_design()\n    S.update_stats()\n    S.fit_surrogate()\n    S.surrogate.plot()\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def plot(self, show: Optional[bool] = True) -&gt; None:\n    \"\"\"\n    This function plots 1D and 2D surrogates.\n\n    Args:\n        self (object):\n            The Kriging object.\n        show (bool):\n            If `True`, the plots are displayed.\n            If `False`, `plt.show()` should be called outside this function.\n\n    Returns:\n        None\n\n    Note:\n        * This method provides only a basic plot. For more advanced plots,\n            use the `plot_contour()` method of the `Spot` class.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init, design_control_init\n            # 1-dimensional example\n            fun = analytical().fun_sphere\n            fun_control=fun_control_init(lower = np.array([-1]),\n                                        upper = np.array([1]),\n                                        noise=False)\n            design_control=design_control_init(init_size=10)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control)\n            S.initialize_design()\n            S.update_stats()\n            S.fit_surrogate()\n            S.surrogate.plot()\n            # 2-dimensional example\n            fun = analytical().fun_sphere\n            fun_control=fun_control_init(lower = np.array([-1, -1]),\n                                        upper = np.array([1, 1]),\n                                        noise=False)\n            design_control=design_control_init(init_size=10)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control)\n            S.initialize_design()\n            S.update_stats()\n            S.fit_surrogate()\n            S.surrogate.plot()\n    \"\"\"\n    if self.k == 1:\n        # TODO: Improve plot (add conf. interval etc.)\n        fig = pylab.figure(figsize=(9, 6))\n        n_grid = 100\n        x = linspace(\n            self.min_X[0], self.max_X[0], num=n_grid\n        )\n        y = self.predict(x)\n        plt.figure()\n        plt.plot(x, y, \"k\")\n        if show:\n            plt.show()\n\n    if self.k == 2:\n        fig = pylab.figure(figsize=(9, 6))\n        n_grid = 100\n        x = linspace(\n            self.min_X[0], self.max_X[0], num=n_grid\n        )\n        y = linspace(\n            self.min_X[1], self.max_X[1], num=n_grid\n        )\n        X, Y = meshgrid(x, y)\n        # Predict based on the optimized results\n        zz = array(\n            [self.predict(array([x, y]), return_val=\"all\") for x, y in zip(ravel(X), ravel(Y))]\n        )\n        zs = zz[:, 0, :]\n        zse = zz[:, 1, :]\n        Z = zs.reshape(X.shape)\n        Ze = zse.reshape(X.shape)\n\n        nat_point_X = self.nat_X[:, 0]\n        nat_point_Y = self.nat_X[:, 1]\n        contour_levels = 30\n        ax = fig.add_subplot(224)\n        # plot predicted values:\n        pylab.contourf(X, Y, Ze, contour_levels, cmap=\"jet\")\n        pylab.title(\"Error\")\n        pylab.colorbar()\n        # plot observed points:\n        pylab.plot(nat_point_X, nat_point_Y, \"ow\")\n        #\n        ax = fig.add_subplot(223)\n        # plot predicted values:\n        plt.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\")\n        plt.title(\"Surrogate\")\n        # plot observed points:\n        pylab.plot(nat_point_X, nat_point_Y, \"ow\", zorder=3)\n        pylab.colorbar()\n        #\n        ax = fig.add_subplot(221, projection=\"3d\")\n        ax.plot_surface(X, Y, Z, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n        #\n        ax = fig.add_subplot(222, projection=\"3d\")\n        ax.plot_surface(X, Y, Ze, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n        #\n        pylab.show()\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.predict","title":"<code>predict(nat_X, return_val='y')</code>","text":"<p>This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>nat_X</code> <code>ndarray</code> <p>Design variable to evaluate in natural units.</p> required <code>return_val</code> <code>str</code> <p>Specifies which prediction values to return. It can be \u201cy\u201d, \u201cs\u201d, \u201cei\u201d, or \u201call\u201d.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>Union[float, Tuple[float, float]]</code> <p>Union[float, Tuple[float, float, float]]: Depending on <code>return_val</code>, returns the predicted value,</p> <code>Union[float, Tuple[float, float]]</code> <p>predicted error, expected improvement, or all.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>nat_X</code> is not an ndarray or doesn\u2019t match expected dimensions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    from numpy import linspace, arange\n    rng = np.random.RandomState(1)\n    X = linspace(start=0, stop=10, num=1_0).reshape(-1, 1)\n    y = np.squeeze(X * np.sin(X))\n    training_indices = rng.choice(arange(y.size), size=6, replace=False)\n    X_train, y_train = X[training_indices], y[training_indices]\n    S = Kriging(name='kriging', seed=124)\n    S.fit(X_train, y_train)\n    mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n    print(f\"mean_prediction: {mean_prediction}\")\n    print(f\"std_prediction: {std_prediction}\")\n    print(f\"s_ei: {s_ei}\")\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def predict(self, nat_X: ndarray, return_val: str = \"y\") -&gt; Union[float, Tuple[float, float]]:\n    \"\"\"\n    This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X.\n\n    Args:\n        self (object): The Kriging object.\n        nat_X (ndarray): Design variable to evaluate in natural units.\n        return_val (str): Specifies which prediction values to return. It can be \"y\", \"s\", \"ei\", or \"all\".\n\n    Returns:\n        Union[float, Tuple[float, float, float]]: Depending on `return_val`, returns the predicted value,\n        predicted error, expected improvement, or all.\n\n    Raises:\n        TypeError: If `nat_X` is not an ndarray or doesn't match expected dimensions.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            from numpy import linspace, arange\n            rng = np.random.RandomState(1)\n            X = linspace(start=0, stop=10, num=1_0).reshape(-1, 1)\n            y = np.squeeze(X * np.sin(X))\n            training_indices = rng.choice(arange(y.size), size=6, replace=False)\n            X_train, y_train = X[training_indices], y[training_indices]\n            S = Kriging(name='kriging', seed=124)\n            S.fit(X_train, y_train)\n            mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n            print(f\"mean_prediction: {mean_prediction}\")\n            print(f\"std_prediction: {std_prediction}\")\n            print(f\"s_ei: {s_ei}\")\n    \"\"\"\n    if not isinstance(nat_X, ndarray):\n        raise TypeError(f\"Expected an ndarray, got {type(nat_X)} instead.\")\n\n    try:\n        X = nat_X.reshape(-1, self.nat_X.shape[1])\n        X = repair_non_numeric(X, self.var_type)\n    except Exception as e:\n        raise TypeError(\"Input to predict was not convertible to the size of X\") from e\n\n    y, s, ei = self.predict_coded_batch(X)\n\n    if return_val == \"y\":\n        return y\n    elif return_val == \"s\":\n        return s\n    elif return_val == \"ei\":\n        return -ei\n    elif return_val == \"all\":\n        return y, s, -ei\n    else:\n        raise ValueError(f\"Invalid return_val: {return_val}. Supported values are 'y', 's', 'ei', 'all'.\")\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.predict_coded","title":"<code>predict_coded(cod_x)</code>","text":"<p>Kriging prediction of one point in coded units as described in (2.20) in [Forr08a]. The error is returned as well. The method is used in <code>predict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>cod_x</code> <code>ndarray</code> <p>Point in coded units to make prediction at.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float, float]</code> <p>Tuple[float, float, float]: Predicted value, predicted error, and expected improvement.</p> Note <p>Uses attributes such as <code>self.mu</code> and <code>self.SigmaSqr</code> that are expected to be calculated by <code>likelihood</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    from numpy import linspace, arange, empty\n    rng = np.random.RandomState(1)\n    X = linspace(start=0, stop=10, num=10).reshape(-1, 1)\n    y = np.squeeze(X * np.sin(X))\n    training_indices = rng.choice(arange(y.size), size=6, replace=False)\n    X_train, y_train = X[training_indices], y[training_indices]\n    S = Kriging(name='kriging', seed=124)\n    S.fit(X_train, y_train)\n    n = X.shape[0]\n    y = empty(n, dtype=float)\n    s = empty(n, dtype=float)\n    ei = empty(n, dtype=float)\n    for i in range(n):\n        y_coded, s_coded, ei_coded = S.predict_coded(X[i, :])\n        y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n        s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n        ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n    print(f\"y: {y}\")\n    print(f\"s: {s}\")\n    print(f\"ei: {-1.0*ei}\")\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def predict_coded(self, cod_x: np.ndarray) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    Kriging prediction of one point in coded units as described in (2.20) in [Forr08a].\n    The error is returned as well. The method is used in `predict`.\n\n    Args:\n        self (object): The Kriging object.\n        cod_x (np.ndarray): Point in coded units to make prediction at.\n\n    Returns:\n        Tuple[float, float, float]: Predicted value, predicted error, and expected improvement.\n\n    Note:\n        Uses attributes such as `self.mu` and `self.SigmaSqr` that are expected\n        to be calculated by `likelihood`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            from numpy import linspace, arange, empty\n            rng = np.random.RandomState(1)\n            X = linspace(start=0, stop=10, num=10).reshape(-1, 1)\n            y = np.squeeze(X * np.sin(X))\n            training_indices = rng.choice(arange(y.size), size=6, replace=False)\n            X_train, y_train = X[training_indices], y[training_indices]\n            S = Kriging(name='kriging', seed=124)\n            S.fit(X_train, y_train)\n            n = X.shape[0]\n            y = empty(n, dtype=float)\n            s = empty(n, dtype=float)\n            ei = empty(n, dtype=float)\n            for i in range(n):\n                y_coded, s_coded, ei_coded = S.predict_coded(X[i, :])\n                y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n                s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n                ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n            print(f\"y: {y}\")\n            print(f\"s: {s}\")\n            print(f\"ei: {-1.0*ei}\")\n    \"\"\"\n    self.build_psi_vec(cod_x)\n    mu_adj = self.mu\n    psi = self.psi\n\n    # Calculate the prediction\n    U_T_inv = solve(self.U.T, self.nat_y - self.one.dot(mu_adj))\n    f = mu_adj + psi.T.dot(solve(self.U, U_T_inv))[0]\n\n    Lambda = self.Lambda if self.noise else 0.0\n\n    # Calculate the estimated error\n    SSqr = self.SigmaSqr * (1 + Lambda - psi.T.dot(solve(self.U, solve(self.U.T, psi))))\n    SSqr = power(abs(SSqr), 0.5)[0]\n\n    # Calculate expected improvement\n    EI = self.exp_imp(y0=f, s0=SSqr)\n\n    return f, SSqr, EI\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.predict_coded_batch","title":"<code>predict_coded_batch(X)</code>","text":"<p>Vectorized prediction for batch input using coded units.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input array of coded points.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: Arrays of predicted values, predicted errors, and expected improvements.</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def predict_coded_batch(self, X: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Vectorized prediction for batch input using coded units.\n\n    Args:\n        X (np.ndarray): Input array of coded points.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]:\n            Arrays of predicted values, predicted errors, and expected improvements.\n    \"\"\"\n    n = X.shape[0]\n    y = np.empty(n, dtype=float)\n    s = np.empty(n, dtype=float)\n    ei = np.empty(n, dtype=float)\n\n    for i in range(n):\n        y_coded, s_coded, ei_coded = self.predict_coded(X[i, :])\n        y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n        s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n        ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n\n    return y, s, ei\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.update_log","title":"<code>update_log()</code>","text":"<p>Update the log with the current values of negLnLike, theta, p, and Lambda. This method appends the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to their respective lists in the log dictionary. It also updates the log_length attribute with the current length of the negLnLike list in the log. If spot_writer is not None, this method also writes the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to the spot_writer object.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.build.kriging import Kriging\n    import numpy as np\n    nat_X = np.array([[1, 2], [3, 4]])\n    nat_y = np.array([1, 2])\n    n=2\n    p=2\n    S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n    S._initialize_variables(nat_X, nat_y)\n    S._set_variable_types()\n    S._set_theta_values()\n    S._initialize_matrices()\n    S._set_de_bounds()\n    new_theta_p_Lambda = S._optimize_model()\n    S.update_log()\n    print(S.log)\n    {'negLnLike': array([-1.38629436]),\n     'theta': array([-1.14525993,  1.6123372 ]),\n      'p': array([1.84444406, 1.74590865]),\n      'Lambda': array([0.44268472])}\n</code></pre> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def update_log(self) -&gt; None:\n    \"\"\"\n    Update the log with the current values of negLnLike, theta, p, and Lambda.\n    This method appends the current values of negLnLike, theta, p (if optim_p is True),\n    and Lambda (if noise is True)\n    to their respective lists in the log dictionary.\n    It also updates the log_length attribute with the current length\n    of the negLnLike list in the log.\n    If spot_writer is not None, this method also writes the current values of\n    negLnLike, theta, p (if optim_p is True),\n    and Lambda (if noise is True) to the spot_writer object.\n\n    Args:\n        self (object): The Kriging object.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.build.kriging import Kriging\n            import numpy as np\n            nat_X = np.array([[1, 2], [3, 4]])\n            nat_y = np.array([1, 2])\n            n=2\n            p=2\n            S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n            S._initialize_variables(nat_X, nat_y)\n            S._set_variable_types()\n            S._set_theta_values()\n            S._initialize_matrices()\n            S._set_de_bounds()\n            new_theta_p_Lambda = S._optimize_model()\n            S.update_log()\n            print(S.log)\n            {'negLnLike': array([-1.38629436]),\n             'theta': array([-1.14525993,  1.6123372 ]),\n              'p': array([1.84444406, 1.74590865]),\n              'Lambda': array([0.44268472])}\n\n    \"\"\"\n    self.log[\"negLnLike\"] = append(self.log[\"negLnLike\"], self.negLnLike)\n    self.log[\"theta\"] = append(self.log[\"theta\"], self.theta)\n    if self.optim_p:\n        self.log[\"p\"] = append(self.log[\"p\"], self.p)\n    if self.noise:\n        self.log[\"Lambda\"] = append(self.log[\"Lambda\"], self.Lambda)\n    # get the length of the log\n    self.log_length = len(self.log[\"negLnLike\"])\n    if self.spot_writer is not None:\n        negLnLike = self.negLnLike.copy()\n        self.spot_writer.add_scalar(\"spot_negLnLike\", negLnLike, self.counter+self.log_length)\n        # add the self.n_theta theta values to the writer with one key \"theta\",\n        # i.e, the same key for all theta values\n        theta = self.theta.copy()\n        self.spot_writer.add_scalars(\"spot_theta\", {f\"theta_{i}\": theta[i] for i in range(self.n_theta)},\n                                     self.counter+self.log_length)\n        if self.noise:\n            Lambda = self.Lambda.copy()\n            self.spot_writer.add_scalar(\"spot_Lambda\", Lambda, self.counter+self.log_length)\n        if self.optim_p:\n            p = self.p.copy()\n            self.spot_writer.add_scalars(\"spot_p\",\n                                         {f\"p_{i}\": p[i] for i in range(self.n_p)}, self.counter+self.log_length)\n        self.spot_writer.flush()\n</code></pre>"},{"location":"reference/spotpython/build/kriging/#spotpython.build.kriging.Kriging.weighted_exp_imp","title":"<code>weighted_exp_imp(cod_x, w)</code>","text":"<p>Weighted expected improvement. Currently not used in <code>spotpython</code></p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>cod_x</code> <code>ndarray</code> <p>A coded design vector.</p> required <code>w</code> <code>float</code> <p>Weight.</p> required <p>Returns:</p> Name Type Description <code>EI</code> <code>float</code> <p>Weighted expected improvement.</p> References <p>[Sobester et al. 2005].</p> Source code in <code>spotpython/build/kriging.py</code> <pre><code>def weighted_exp_imp(self, cod_x: np.ndarray, w: float) -&gt; float:\n    \"\"\"\n    Weighted expected improvement. Currently not used in `spotpython`\n\n    Args:\n        self (object): The Kriging object.\n        cod_x (np.ndarray): A coded design vector.\n        w (float): Weight.\n\n    Returns:\n        EI (float): Weighted expected improvement.\n\n    References:\n        [Sobester et al. 2005].\n    \"\"\"\n    y0, s0 = self.predict_coded(cod_x)\n    y_min = min(self.nat_y)\n    if s0 &lt;= 0.0:\n        EI = 0.0\n    else:\n        y_min_y0 = y_min - y0\n        EI_one = w * (\n                y_min_y0\n                * (0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * (y_min_y0 / s0)))\n        )\n        EI_two = (\n                (1.0 - w)\n                * (s0 * (1.0 / sqrt(2.0 * pi)))\n                * (exp(-(1.0 / 2.0) * ((y_min_y0) ** 2.0 / s0 ** 2.0)))\n        )\n        EI = EI_one + EI_two\n    return EI\n</code></pre>"},{"location":"reference/spotpython/build/surrogates/","title":"surrogates","text":""},{"location":"reference/spotpython/build/surrogates/#spotpython.build.surrogates.surrogates","title":"<code>surrogates</code>","text":"<p>Super class for all surrogate model classes (e.g., Kriging)</p> Source code in <code>spotpython/build/surrogates.py</code> <pre><code>class surrogates:\n    \"\"\"\n    Super class for all surrogate model classes (e.g., Kriging)\n    \"\"\"\n    def __init__(self, name=\"\", seed=123, verbosity=0):\n        self.name = name\n        self.seed = seed\n        self.rng = default_rng(self.seed)\n        self.log = {}\n        self.verbosity = verbosity\n</code></pre>"},{"location":"reference/spotpython/data/base/","title":"base","text":""},{"location":"reference/spotpython/data/base/#spotpython.data.base.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all configurations.</p> <p>All configurations inherit from this class, be they stored in a file or generated on the fly.</p> <p>Attributes:</p> Name Type Description <code>desc</code> <code>str</code> <p>The description from the docstring.</p> <code>_repr_content</code> <code>dict</code> <p>The items that are displayed in the repr method.</p> Source code in <code>spotpython/data/base.py</code> <pre><code>class Config(abc.ABC):\n    \"\"\"Base class for all configurations.\n\n    All configurations inherit from this class, be they stored in a file or generated on the fly.\n\n    Attributes:\n        desc (str): The description from the docstring.\n        _repr_content (dict): The items that are displayed in the __repr__ method.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize a Config object.\"\"\"\n        pass\n\n    @property\n    def desc(self) -&gt; str:\n        \"\"\"Return the description from the docstring.\n\n        Returns:\n            str: The description from the docstring.\n\n        Examples:\n            &gt;&gt;&gt; class MyConfig(Config):\n            ...     '''My configuration class.'''\n            ...     pass\n            &gt;&gt;&gt; MyConfig().desc\n            'My configuration class.'\n        \"\"\"\n        desc = re.split(pattern=r\"\\w+\\n\\s{4}\\-{3,}\", string=self.__doc__, maxsplit=0)[0]\n        return inspect.cleandoc(desc)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The items that are displayed in the __repr__ method.\n\n        This property can be overridden in order to modify the output of the __repr__ method.\n\n        Returns:\n            dict: A dictionary containing the items to be displayed in the __repr__ method.\n\n        Examples:\n            &gt;&gt;&gt; class MyConfig(Config):\n            ...     '''My configuration class.'''\n            ...     pass\n            &gt;&gt;&gt; MyConfig()._repr_content\n            {'Name': 'MyConfig'}\n        \"\"\"\n        content = {}\n        content[\"Name\"] = self.__class__.__name__\n        return content\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Config.desc","title":"<code>desc: str</code>  <code>property</code>","text":"<p>Return the description from the docstring.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The description from the docstring.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyConfig(Config):\n...     '''My configuration class.'''\n...     pass\n&gt;&gt;&gt; MyConfig().desc\n'My configuration class.'\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Config.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a Config object.</p> Source code in <code>spotpython/data/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize a Config object.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all datasets.</p> <p>All datasets inherit from this class, be they stored in a file or generated on the fly.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d</p> required <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> <code>None</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset, only applies to classification datasets.</p> <code>None</code> <code>n_outputs</code> <code>int</code> <p>Number of outputs the target is made of, only applies to multi-output datasets.</p> <code>None</code> <code>sparse</code> <code>bool</code> <p>Whether the dataset is sparse or not.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>desc</code> <code>str</code> <p>The description from the docstring.</p> <code>_repr_content</code> <code>dict</code> <p>The items that are displayed in the repr method.</p> Source code in <code>spotpython/data/base.py</code> <pre><code>class Dataset(abc.ABC):\n    \"\"\"Base class for all datasets.\n\n    All datasets inherit from this class, be they stored in a file or generated on the fly.\n\n    Args:\n        task (str): Type of task the dataset is meant for. Should be one of:\n            - \"Regression\"\n            - \"Binary classification\"\n            - \"Multi-class classification\"\n            - \"Multi-output binary classification\"\n            - \"Multi-output regression\"\n        n_features (int): Number of features in the dataset.\n        n_samples (int, optional): Number of samples in the dataset.\n        n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets.\n        n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets.\n        sparse (bool, optional): Whether the dataset is sparse or not.\n\n    Attributes:\n        desc (str): The description from the docstring.\n        _repr_content (dict): The items that are displayed in the __repr__ method.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        n_features: int,\n        n_samples: Optional[int] = None,\n        n_classes: Optional[int] = None,\n        n_outputs: Optional[int] = None,\n        sparse: bool = False,\n    ):\n        \"\"\"Initialize a Dataset object.\n\n        Args:\n            task (str): Type of task the dataset is meant for. Should be one of:\n                - \"Regression\"\n                - \"Binary classification\"\n                - \"Multi-class classification\"\n                - \"Multi-output binary classification\"\n                - \"Multi-output regression\"\n            n_features (int): Number of features in the dataset.\n            n_samples (int, optional): Number of samples in the dataset. Defaults to None.\n            n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets.\n                Defaults to None.\n            n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets.\n                Defaults to None.\n            sparse (bool, optional): Whether the dataset is sparse or not. Defaults to False.\n        \"\"\"\n        self.task = task\n        self.n_features = n_features\n        self.n_samples = n_samples\n        self.n_outputs = n_outputs\n        self.n_classes = n_classes\n        self.sparse = sparse\n\n    @abc.abstractmethod\n    def __iter__(self):\n        \"\"\"Abstract method for iterating over samples in the dataset.\"\"\"\n        raise NotImplementedError\n\n    def take(self, k: int) -&gt; itertools.islice:\n        \"\"\"Iterate over the k samples.\n\n        Args:\n            k (int): The number of samples to iterate over.\n\n        Returns:\n            itertools.islice: An iterator over the first k samples in the dataset.\n\n        Examples:\n            &gt;&gt;&gt; class MyDataset(Dataset):\n            ...     def __init__(self):\n            ...         super().__init__('Regression', 10)\n            ...     def __iter__(self):\n            ...         yield from range(10)\n            &gt;&gt;&gt; list(MyDataset().take(5))\n            [0, 1, 2, 3, 4]\n        \"\"\"\n        return itertools.islice(self, k)\n\n    @property\n    def desc(self) -&gt; str:\n        \"\"\"Return the description from the docstring.\n\n        Returns:\n            str: The description from the docstring.\n\n        Examples:\n            &gt;&gt;&gt; class MyDataset(Dataset):\n            ...     '''My dataset class.'''\n            ...     def __init__(self):\n            ...         super().__init__('Regression', 10)\n            ...     def __iter__(self):\n            ...         yield from range(10)\n            &gt;&gt;&gt; MyDataset().desc\n            'My dataset class.'\n        \"\"\"\n        desc = re.split(pattern=r\"\\w+\\n\\s{4}\\-{3,}\", string=self.__doc__, maxsplit=0)[0]\n        return inspect.cleandoc(desc)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The items that are displayed in the __repr__ method.\n\n        This property can be overridden in order to modify the output of the __repr__ method.\n\n        Returns:\n            dict: A dictionary containing the items to be displayed in the __repr__ method.\n        \"\"\"\n\n        content = {}\n        content[\"Name\"] = self.__class__.__name__\n        content[\"Task\"] = self.task\n        if isinstance(self, SyntheticDataset) and self.n_samples is None:\n            content[\"Samples\"] = \"\u221e\"\n        elif self.n_samples:\n            content[\"Samples\"] = f\"{self.n_samples:,}\"\n        if self.n_features:\n            content[\"Features\"] = f\"{self.n_features:,}\"\n        if self.n_outputs:\n            content[\"Outputs\"] = f\"{self.n_outputs:,}\"\n        if self.n_classes:\n            content[\"Classes\"] = f\"{self.n_classes:,}\"\n        content[\"Sparse\"] = str(self.sparse)\n\n        return content\n\n    def __repr__(self):\n        l_len = max(map(len, self._repr_content.keys()))\n        r_len = max(map(len, self._repr_content.values()))\n\n        out = f\"{self.desc}\\n\\n\" + \"\\n\".join(k.rjust(l_len) + \"  \" + v.ljust(r_len) for k, v in self._repr_content.items())\n\n        if \"Parameters\\n    ----------\" in self.__doc__:\n            params = re.split(\n                r\"\\w+\\n\\s{4}\\-{3,}\",\n                re.split(\"Parameters\\n    ----------\", self.__doc__)[1],\n            )[0].rstrip()\n            out += f\"\\n\\nParameters\\n----------{params}\"\n\n        return out\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Dataset.desc","title":"<code>desc: str</code>  <code>property</code>","text":"<p>Return the description from the docstring.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The description from the docstring.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyDataset(Dataset):\n...     '''My dataset class.'''\n...     def __init__(self):\n...         super().__init__('Regression', 10)\n...     def __iter__(self):\n...         yield from range(10)\n&gt;&gt;&gt; MyDataset().desc\n'My dataset class.'\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Dataset.__init__","title":"<code>__init__(task, n_features, n_samples=None, n_classes=None, n_outputs=None, sparse=False)</code>","text":"<p>Initialize a Dataset object.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d</p> required <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset. Defaults to None.</p> <code>None</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset, only applies to classification datasets. Defaults to None.</p> <code>None</code> <code>n_outputs</code> <code>int</code> <p>Number of outputs the target is made of, only applies to multi-output datasets. Defaults to None.</p> <code>None</code> <code>sparse</code> <code>bool</code> <p>Whether the dataset is sparse or not. Defaults to False.</p> <code>False</code> Source code in <code>spotpython/data/base.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    n_features: int,\n    n_samples: Optional[int] = None,\n    n_classes: Optional[int] = None,\n    n_outputs: Optional[int] = None,\n    sparse: bool = False,\n):\n    \"\"\"Initialize a Dataset object.\n\n    Args:\n        task (str): Type of task the dataset is meant for. Should be one of:\n            - \"Regression\"\n            - \"Binary classification\"\n            - \"Multi-class classification\"\n            - \"Multi-output binary classification\"\n            - \"Multi-output regression\"\n        n_features (int): Number of features in the dataset.\n        n_samples (int, optional): Number of samples in the dataset. Defaults to None.\n        n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets.\n            Defaults to None.\n        n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets.\n            Defaults to None.\n        sparse (bool, optional): Whether the dataset is sparse or not. Defaults to False.\n    \"\"\"\n    self.task = task\n    self.n_features = n_features\n    self.n_samples = n_samples\n    self.n_outputs = n_outputs\n    self.n_classes = n_classes\n    self.sparse = sparse\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Dataset.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Abstract method for iterating over samples in the dataset.</p> Source code in <code>spotpython/data/base.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n    \"\"\"Abstract method for iterating over samples in the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.Dataset.take","title":"<code>take(k)</code>","text":"<p>Iterate over the k samples.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of samples to iterate over.</p> required <p>Returns:</p> Type Description <code>islice</code> <p>itertools.islice: An iterator over the first k samples in the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyDataset(Dataset):\n...     def __init__(self):\n...         super().__init__('Regression', 10)\n...     def __iter__(self):\n...         yield from range(10)\n&gt;&gt;&gt; list(MyDataset().take(5))\n[0, 1, 2, 3, 4]\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>def take(self, k: int) -&gt; itertools.islice:\n    \"\"\"Iterate over the k samples.\n\n    Args:\n        k (int): The number of samples to iterate over.\n\n    Returns:\n        itertools.islice: An iterator over the first k samples in the dataset.\n\n    Examples:\n        &gt;&gt;&gt; class MyDataset(Dataset):\n        ...     def __init__(self):\n        ...         super().__init__('Regression', 10)\n        ...     def __iter__(self):\n        ...         yield from range(10)\n        &gt;&gt;&gt; list(MyDataset().take(5))\n        [0, 1, 2, 3, 4]\n    \"\"\"\n    return itertools.islice(self, k)\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.FileConfig","title":"<code>FileConfig</code>","text":"<p>               Bases: <code>Config</code></p> <p>Base class for configurations that are stored in a local file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The file\u2019s name.</p> required <code>directory</code> <code>Optional[str]</code> <p>The directory where the file is contained. Defaults to the location of the <code>datasets</code> module.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra config parameters to pass as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FileConfig</code> <p>A FileConfig object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>class FileConfig(Config):\n    \"\"\"Base class for configurations that are stored in a local file.\n\n    Args:\n        filename (str): The file's name.\n        directory (Optional[str]):\n            The directory where the file is contained.\n            Defaults to the location of the `datasets` module.\n        desc (dict): Extra config parameters to pass as keyword arguments.\n\n    Returns:\n        (FileConfig): A FileConfig object.\n\n    Examples:\n        &gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n    \"\"\"\n\n    def __init__(self, filename: str, directory: Optional[str] = None, **desc):\n        super().__init__(**desc)\n        self.filename = filename\n        self.directory = directory\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"The path to the configuration file.\n\n        Returns:\n            pathlib.Path: The path to the configuration file.\n\n        Examples:\n            &gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; config.path\n            PosixPath('/path/to/directory/config.json')\n        \"\"\"\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The content of the string representation of the FileConfig object.\n\n        Returns:\n            dict: A dictionary containing the content of the string representation of the FileConfig object.\n\n        Examples:\n            &gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; config._repr_content\n            {'Path': '/path/to/directory/config.json'}\n        \"\"\"\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.FileConfig.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>The path to the configuration file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the configuration file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n&gt;&gt;&gt; config.path\nPosixPath('/path/to/directory/config.json')\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.FileDataset","title":"<code>FileDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for datasets that are stored in a local file.</p> <p>Small datasets that are part of the spotriver package inherit from this class.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The file\u2019s name.</p> required <code>directory</code> <code>Optional[str]</code> <p>The directory where the file is contained. Defaults to the location of the <code>datasets</code> module.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra dataset parameters to pass as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FileDataset</code> <p>A FileDataset object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>class FileDataset(Dataset):\n    \"\"\"Base class for datasets that are stored in a local file.\n\n    Small datasets that are part of the spotriver package inherit from this class.\n\n    Args:\n        filename (str): The file's name.\n        directory (Optional[str]):\n            The directory where the file is contained.\n            Defaults to the location of the `datasets` module.\n        desc (dict): Extra dataset parameters to pass as keyword arguments.\n\n    Returns:\n        (FileDataset): A FileDataset object.\n\n    Examples:\n        &gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n    \"\"\"\n\n    def __init__(self, filename: str, directory: Optional[str] = None, **desc):\n        super().__init__(**desc)\n        self.filename = filename\n        self.directory = directory\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"The path to the dataset file.\n\n        Returns:\n            pathlib.Path: The path to the dataset file.\n\n        Examples:\n            &gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; dataset.path\n            PosixPath('/path/to/directory/dataset.csv')\n        \"\"\"\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The content of the string representation of the FileDataset object.\n\n        Returns:\n            dict: A dictionary containing the content of the string representation of the FileDataset object.\n\n        Examples:\n            &gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; dataset._repr_content\n            {'Path': '/path/to/directory/dataset.csv'}\n        \"\"\"\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.FileDataset.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>The path to the dataset file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the dataset file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n&gt;&gt;&gt; dataset.path\nPosixPath('/path/to/directory/dataset.csv')\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.GenericFileDataset","title":"<code>GenericFileDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for datasets that are stored in a local file.</p> <p>Small datasets that are part of the spotriver package inherit from this class.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The file\u2019s name.</p> required <code>target</code> <code>str</code> <p>The name of the target variable.</p> required <code>converters</code> <code>dict</code> <p>A dictionary specifying how to convert the columns of the dataset. Defaults to None.</p> <code>None</code> <code>parse_dates</code> <code>list</code> <p>A list of columns to parse as dates. Defaults to None.</p> <code>None</code> <code>directory</code> <code>str</code> <p>The directory where the file is contained. Defaults to the location of the <code>datasets</code> module.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra dataset parameters to pass as keyword arguments.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.datasets import Iris\n&gt;&gt;&gt; dataset = Iris()\n&gt;&gt;&gt; for x, y in dataset:\n...     print(x, y)\n...     break\n({'sepal_length': 5.1,\n  'sepal_width': 3.5,\n  'petal_length': 1.4,\n  'petal_width': 0.2},\n  'setosa')\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>class GenericFileDataset(Dataset):\n    \"\"\"Base class for datasets that are stored in a local file.\n\n    Small datasets that are part of the spotriver package inherit from this class.\n\n    Args:\n        filename (str): The file's name.\n        target (str): The name of the target variable.\n        converters (dict):\n            A dictionary specifying how to convert the columns of the dataset. Defaults to None.\n        parse_dates (list): A list of columns to parse as dates. Defaults to None.\n        directory (str):\n            The directory where the file is contained. Defaults to the location of the `datasets` module.\n        desc (dict): Extra dataset parameters to pass as keyword arguments.\n\n\n    Examples:\n\n        &gt;&gt;&gt; from river.datasets import Iris\n        &gt;&gt;&gt; dataset = Iris()\n        &gt;&gt;&gt; for x, y in dataset:\n        ...     print(x, y)\n        ...     break\n        ({'sepal_length': 5.1,\n          'sepal_width': 3.5,\n          'petal_length': 1.4,\n          'petal_width': 0.2},\n          'setosa')\n\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str,\n        target: str,\n        converters: dict = None,\n        parse_dates: list = None,\n        directory: str = None,\n        **desc: dict,\n    ):\n        super().__init__(**desc)\n        self.filename = filename\n        self.directory = directory\n        self.target = target\n        self.converters = converters\n        self.parse_dates = parse_dates\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"Returns the path where the dataset is stored.\"\"\"\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self):\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.GenericFileDataset.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>Returns the path where the dataset is stored.</p>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.RemoteDataset","title":"<code>RemoteDataset</code>","text":"<p>               Bases: <code>FileDataset</code></p> <p>Base class for datasets that are stored in a remote file.</p> <p>Medium and large datasets that are not part of the river package inherit from this class.</p> <p>The filename doesn\u2019t have to be provided if unpack is False. Indeed in the latter case the filename will be inferred from the URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL the dataset is located at.</p> required <code>size</code> <code>int</code> <p>The expected download size.</p> required <code>unpack</code> <code>bool</code> <p>Whether to unpack the download or not. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>An optional name to given to the file if the file is unpacked. Defaults to None.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra dataset parameters to pass as keyword arguments.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.datasets import AirlinePassengers\n&gt;&gt;&gt; dataset = AirlinePassengers()\n&gt;&gt;&gt; for x, y in dataset:\n...     print(x, y)\n...     break\n({'month': datetime.datetime(1949, 1, 1, 0, 0)}, 112)\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>class RemoteDataset(FileDataset):\n    \"\"\"Base class for datasets that are stored in a remote file.\n\n    Medium and large datasets that are not part of the river package inherit from this class.\n\n    The filename doesn't have to be provided if unpack is False. Indeed in the latter case the\n    filename will be inferred from the URL.\n\n    Args:\n        url (str): The URL the dataset is located at.\n        size (int): The expected download size.\n        unpack (bool): Whether to unpack the download or not. Defaults to True.\n        filename (str):\n            An optional name to given to the file if the file is unpacked. Defaults to None.\n        desc (dict): Extra dataset parameters to pass as keyword arguments.\n\n    Examples:\n\n        &gt;&gt;&gt; from river.datasets import AirlinePassengers\n        &gt;&gt;&gt; dataset = AirlinePassengers()\n        &gt;&gt;&gt; for x, y in dataset:\n        ...     print(x, y)\n        ...     break\n        ({'month': datetime.datetime(1949, 1, 1, 0, 0)}, 112)\n\n    \"\"\"\n\n    def __init__(self, url: str, size: int, unpack: bool = True, filename: str = None, **desc: dict):\n        if filename is None:\n            filename = path.basename(url)\n\n        super().__init__(filename=filename, **desc)\n        self.url = url\n        self.size = size\n        self.unpack = unpack\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"Returns the path where the dataset is stored.\"\"\"\n        return pathlib.Path(get_data_home(), self.__class__.__name__, self.filename)\n\n    def download(self, force: bool = False, verbose: bool = True) -&gt; None:\n        \"\"\"Downloads the dataset.\n\n        Args:\n            force (bool):\n                Whether to force the download even if the data is already downloaded.\n                Defaults to False.\n            verbose (bool):\n                Whether to display information about the download. Defaults to True.\n\n        \"\"\"\n        if not force and self.is_downloaded:\n            return\n\n        # Determine where to download the archive\n        directory = self.path.parent\n        directory.mkdir(parents=True, exist_ok=True)\n        archive_path = directory.joinpath(path.basename(self.url))\n\n        with request.urlopen(self.url) as r:\n            # Notify the user\n            if verbose:\n                meta = r.info()\n                try:\n                    n_bytes = int(meta[\"Content-Length\"])\n                    msg = f\"Downloading {self.url} ({n_bytes})\"\n                except KeyError:\n                    msg = f\"Downloading {self.url}\"\n                print(msg)\n\n            # Now dump the contents of the requests\n            with open(archive_path, \"wb\") as f:\n                shutil.copyfileobj(r, f)\n\n        if not self.unpack:\n            return\n\n        if verbose:\n            print(f\"Uncompressing into {directory}\")\n\n        if archive_path.suffix.endswith(\"zip\"):\n            with zipfile.ZipFile(archive_path, \"r\") as zf:\n                zf.extractall(directory)\n\n        elif archive_path.suffix.endswith((\"gz\", \"tar\")):\n            mode = \"r:\" if archive_path.suffix.endswith(\"tar\") else \"r:gz\"\n            tar = tarfile.open(archive_path, mode)\n            tar.extractall(directory)\n            tar.close()\n\n        else:\n            raise RuntimeError(f\"Unhandled extension type: {archive_path.suffix}\")\n\n        # Delete the archive file now that it has been uncompressed\n        archive_path.unlink()\n\n    @abc.abstractmethod\n    def _iter(self):\n        pass\n\n    @property\n    def is_downloaded(self) -&gt; bool:\n        \"\"\"Indicate whether or not the data has been correctly downloaded.\"\"\"\n        if self.path.exists():\n            if self.path.is_file():\n                return self.path.stat().st_size == self.size\n            return sum(f.stat().st_size for f in self.path.glob(\"**/*\") if f.is_file())\n\n        return False\n\n    def __iter__(self):\n        \"\"\"Iterates over the samples of a dataset.\"\"\"\n        if not self.is_downloaded:\n            self.download(verbose=True)\n        if not self.is_downloaded:\n            raise RuntimeError(\"Something went wrong during the download\")\n        yield from self._iter()\n\n    @property\n    def _repr_content(self):\n        content = super()._repr_content\n        content[\"URL\"] = self.url\n        content[\"Size\"] = self.size\n        content[\"Downloaded\"] = str(self.is_downloaded)\n        return content\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.RemoteDataset.is_downloaded","title":"<code>is_downloaded: bool</code>  <code>property</code>","text":"<p>Indicate whether or not the data has been correctly downloaded.</p>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.RemoteDataset.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>Returns the path where the dataset is stored.</p>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.RemoteDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over the samples of a dataset.</p> Source code in <code>spotpython/data/base.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterates over the samples of a dataset.\"\"\"\n    if not self.is_downloaded:\n        self.download(verbose=True)\n    if not self.is_downloaded:\n        raise RuntimeError(\"Something went wrong during the download\")\n    yield from self._iter()\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.RemoteDataset.download","title":"<code>download(force=False, verbose=True)</code>","text":"<p>Downloads the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>Whether to force the download even if the data is already downloaded. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to display information about the download. Defaults to True.</p> <code>True</code> Source code in <code>spotpython/data/base.py</code> <pre><code>def download(self, force: bool = False, verbose: bool = True) -&gt; None:\n    \"\"\"Downloads the dataset.\n\n    Args:\n        force (bool):\n            Whether to force the download even if the data is already downloaded.\n            Defaults to False.\n        verbose (bool):\n            Whether to display information about the download. Defaults to True.\n\n    \"\"\"\n    if not force and self.is_downloaded:\n        return\n\n    # Determine where to download the archive\n    directory = self.path.parent\n    directory.mkdir(parents=True, exist_ok=True)\n    archive_path = directory.joinpath(path.basename(self.url))\n\n    with request.urlopen(self.url) as r:\n        # Notify the user\n        if verbose:\n            meta = r.info()\n            try:\n                n_bytes = int(meta[\"Content-Length\"])\n                msg = f\"Downloading {self.url} ({n_bytes})\"\n            except KeyError:\n                msg = f\"Downloading {self.url}\"\n            print(msg)\n\n        # Now dump the contents of the requests\n        with open(archive_path, \"wb\") as f:\n            shutil.copyfileobj(r, f)\n\n    if not self.unpack:\n        return\n\n    if verbose:\n        print(f\"Uncompressing into {directory}\")\n\n    if archive_path.suffix.endswith(\"zip\"):\n        with zipfile.ZipFile(archive_path, \"r\") as zf:\n            zf.extractall(directory)\n\n    elif archive_path.suffix.endswith((\"gz\", \"tar\")):\n        mode = \"r:\" if archive_path.suffix.endswith(\"tar\") else \"r:gz\"\n        tar = tarfile.open(archive_path, mode)\n        tar.extractall(directory)\n        tar.close()\n\n    else:\n        raise RuntimeError(f\"Unhandled extension type: {archive_path.suffix}\")\n\n    # Delete the archive file now that it has been uncompressed\n    archive_path.unlink()\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.SyntheticDataset","title":"<code>SyntheticDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A synthetic dataset.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d</p> required <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset, only applies to classification datasets.</p> <code>None</code> <code>n_outputs</code> <code>int</code> <p>Number of outputs the target is made of, only applies to multi-output datasets.</p> <code>None</code> <code>sparse</code> <code>bool</code> <p>Whether the dataset is sparse or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>SyntheticDataset</code> <p>A synthetic dataset object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n&gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                    n_features=4,\n                                    n_samples=100,\n                                    n_classes=2,\n                                    n_outputs=1,\n                                    sparse=False)\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>class SyntheticDataset(Dataset):\n    \"\"\"A synthetic dataset.\n\n    Args:\n        task (str): Type of task the dataset is meant for. Should be one of:\n            - \"Regression\"\n            - \"Binary classification\"\n            - \"Multi-class classification\"\n            - \"Multi-output binary classification\"\n            - \"Multi-output regression\"\n        n_features (int): Number of features in the dataset.\n        n_samples (int): Number of samples in the dataset.\n        n_classes (int): Number of classes in the dataset, only applies to classification datasets.\n        n_outputs (int): Number of outputs the target is made of, only applies to multi-output datasets.\n        sparse (bool): Whether the dataset is sparse or not.\n\n    Returns:\n        (SyntheticDataset): A synthetic dataset object.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import make_classification\n        &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n        &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                            n_features=4,\n                                            n_samples=100,\n                                            n_classes=2,\n                                            n_outputs=1,\n                                            sparse=False)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        n_features: int,\n        n_samples: int,\n        n_classes: Union[int, None] = None,\n        n_outputs: Union[int, None] = None,\n        sparse: bool = False,\n    ):\n        pass\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the SyntheticDataset object.\n\n        Returns:\n            str: A string representation of the SyntheticDataset object.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.datasets import make_classification\n            &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n            &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                                n_features=4,\n                                                n_samples=100,\n                                                n_classes=2,\n                                                n_outputs=1,\n                                                sparse=False)\n            &gt;&gt;&gt; print(dataset)\n            Synthetic data generator\n\n            Configuration\n            -------------\n                task  Binary classification\n          n_features  4\n           n_samples  100\n           n_classes  2\n           n_outputs  1\n              sparse  False\n        \"\"\"\n        l_len_prop = max(map(len, self._repr_content.keys()))\n        r_len_prop = max(map(len, self._repr_content.values()))\n        params = self._get_params()\n        l_len_config = max(map(len, params.keys()))\n        r_len_config = max(map(len, map(str, params.values())))\n\n        out = (\n            \"Synthetic data generator\\n\\n\"\n            + \"\\n\".join(k.rjust(l_len_prop) + \"  \" + v.ljust(r_len_prop) for k, v in self._repr_content.items())\n            + \"\\n\\nConfiguration\\n-------------\\n\"\n            + \"\\n\".join(k.rjust(l_len_config) + \"  \" + str(v).ljust(r_len_config) for k, v in params.items())\n        )\n\n        return out\n\n    def _get_params(self) -&gt; typing.Dict[str, typing.Any]:\n        \"\"\"Return the parameters that were used during initialization.\n\n        Returns:\n            dict: A dictionary containing the parameters that were used during initialization.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.datasets import make_classification\n            &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n            &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                            n_features=4,\n                                            n_samples=100,\n                                            n_classes=2,\n                                            n_outputs=1,\n                                            sparse=False)\n            &gt;&gt;&gt; dataset._get_params()\n            {'task': 'Binary classification',\n             'n_features': 4,\n             'n_samples': 100,\n             'n_classes': 2,\n             'n_outputs': 1,\n             'sparse': False}\n        \"\"\"\n        return {name: getattr(self, name) for name, param in inspect.signature(self.__init__).parameters.items() if param.kind != param.VAR_KEYWORD}  # type: ignore\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.SyntheticDataset.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the SyntheticDataset object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the SyntheticDataset object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n&gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                    n_features=4,\n                                    n_samples=100,\n                                    n_classes=2,\n                                    n_outputs=1,\n                                    sparse=False)\n&gt;&gt;&gt; print(dataset)\nSynthetic data generator\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.SyntheticDataset.__repr__--configuration","title":"Configuration","text":"<pre><code>task  Binary classification\n</code></pre> <p>n_features  4    n_samples  100    n_classes  2    n_outputs  1       sparse  False</p> Source code in <code>spotpython/data/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the SyntheticDataset object.\n\n    Returns:\n        str: A string representation of the SyntheticDataset object.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import make_classification\n        &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n        &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                            n_features=4,\n                                            n_samples=100,\n                                            n_classes=2,\n                                            n_outputs=1,\n                                            sparse=False)\n        &gt;&gt;&gt; print(dataset)\n        Synthetic data generator\n\n        Configuration\n        -------------\n            task  Binary classification\n      n_features  4\n       n_samples  100\n       n_classes  2\n       n_outputs  1\n          sparse  False\n    \"\"\"\n    l_len_prop = max(map(len, self._repr_content.keys()))\n    r_len_prop = max(map(len, self._repr_content.values()))\n    params = self._get_params()\n    l_len_config = max(map(len, params.keys()))\n    r_len_config = max(map(len, map(str, params.values())))\n\n    out = (\n        \"Synthetic data generator\\n\\n\"\n        + \"\\n\".join(k.rjust(l_len_prop) + \"  \" + v.ljust(r_len_prop) for k, v in self._repr_content.items())\n        + \"\\n\\nConfiguration\\n-------------\\n\"\n        + \"\\n\".join(k.rjust(l_len_config) + \"  \" + str(v).ljust(r_len_config) for k, v in params.items())\n    )\n\n    return out\n</code></pre>"},{"location":"reference/spotpython/data/base/#spotpython.data.base.get_data_home","title":"<code>get_data_home(data_home=None)</code>","text":"<p>Return the location where remote datasets are to be stored.</p> <p>By default the data directory is set to a folder named \u2018spotriver_data\u2019 in the user home folder. Alternatively, it can be set by the \u2018SPOTRIVER_DATA\u2019 environment variable or programmatically by giving an explicit folder path. The \u2018~\u2019 symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotriver data directory. If <code>None</code>, the default path is <code>~/spotriver_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotriver data directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; get_data_home()\nPosixPath('/home/user/spotriver_data')\n&gt;&gt;&gt; get_data_home(Path('/tmp/spotriver_data'))\nPosixPath('/tmp/spotriver_data')\n</code></pre> Source code in <code>spotpython/data/base.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where remote datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotriver_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTRIVER_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotriver data directory. If `None`, the default path\n            is `~/spotriver_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotriver data directory.\n\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotriver_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotriver_data'))\n        PosixPath('/tmp/spotriver_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\"SPOTRIVER_DATA\", Path.home() / \"spotriver_data\")\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"reference/spotpython/data/california/","title":"california","text":""},{"location":"reference/spotpython/data/california/#spotpython.data.california.CaliforniaHousing","title":"<code>CaliforniaHousing</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for regression. A toy data set from scikit-learn. Features:     * MedInc median income in block group     * HouseAge median house age in block group     * AveRooms average number of rooms per household     * AveBedrms average number of bedrooms per household     * Population block group population     * AveOccup average number of household members     * Latitude block group latitude     * Longitude block group longitude The target variable is the median house value for California districts, expressed in hundreds of thousands of Dollars ($100,000). Samples total: 20640, Dimensionality: 8, Features: real, Target: real 0.15 - 5. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).</p> <p>Parameters:</p> Name Type Description Default <code>feature_type</code> <code>dtype</code> <p>The data type of the features. Defaults to torch.float.</p> <code>float</code> <code>target_type</code> <code>dtype</code> <p>The data type of the targets. Defaults to torch.long.</p> <code>float</code> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not. Defaults to True.</p> <code>True</code> <code>n_samples</code> <code>int</code> <p>The number of samples of the dataset. Defaults to None, which means the entire dataset is used.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>Tensor</code> <p>The data features.</p> <code>targets</code> <code>Tensor</code> <p>The data targets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    import torch\n    dataset = Diabetes(feature_type=torch.float32, target_type=torch.float32)\n    # Set batch size for DataLoader\n    batch_size = 5\n    # Create DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    # Iterate over the data in the DataLoader\n    for batch in dataloader:\n        inputs, targets = batch\n        print(f\"Batch Size: {inputs.size(0)}\")\n        print(\"---------------\")\n        print(f\"Inputs: {inputs}\")\n        print(f\"Targets: {targets}\")\n</code></pre> Source code in <code>spotpython/data/california.py</code> <pre><code>class CaliforniaHousing(Dataset):\n    \"\"\"\n    A PyTorch Dataset for regression. A toy data set from scikit-learn.\n    Features:\n        * MedInc median income in block group\n        * HouseAge median house age in block group\n        * AveRooms average number of rooms per household\n        * AveBedrms average number of bedrooms per household\n        * Population block group population\n        * AveOccup average number of household members\n        * Latitude block group latitude\n        * Longitude block group longitude\n    The target variable is the median house value for California districts,\n    expressed in hundreds of thousands of Dollars ($100,000).\n    Samples total: 20640, Dimensionality: 8, Features: real, Target: real 0.15 - 5.\n    This dataset was derived from the 1990 U.S. census, using one row per census block group.\n    A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data\n    (a block group typically has a population of 600 to 3,000 people).\n\n    Args:\n        feature_type (torch.dtype): The data type of the features. Defaults to torch.float.\n        target_type (torch.dtype): The data type of the targets. Defaults to torch.long.\n        train (bool): Whether the dataset is for training or not. Defaults to True.\n        n_samples (int): The number of samples of the dataset. Defaults to None, which means the entire dataset is used.\n\n    Attributes:\n        data (Tensor): The data features.\n        targets (Tensor): The data targets.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            import torch\n            dataset = Diabetes(feature_type=torch.float32, target_type=torch.float32)\n            # Set batch size for DataLoader\n            batch_size = 5\n            # Create DataLoader\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n            # Iterate over the data in the DataLoader\n            for batch in dataloader:\n                inputs, targets = batch\n                print(f\"Batch Size: {inputs.size(0)}\")\n                print(\"---------------\")\n                print(f\"Inputs: {inputs}\")\n                print(f\"Targets: {targets}\")\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_type: torch.dtype = torch.float,\n        target_type: torch.dtype = torch.float,\n        train: bool = True,\n        n_samples: int = None,\n    ) -&gt; None:\n        super().__init__()\n        self.feature_type = feature_type\n        self.target_type = target_type\n        self.train = train\n        self.n_samples = n_samples\n        self.data, self.targets = self._load_data()\n\n    def _load_data(self) -&gt; tuple:\n        \"\"\"Loads the data from scikit-learn and returns the features and targets.\n\n        Returns:\n            tuple: A tuple containing the features and targets.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n                dataset = Diabetes()\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([442, 10])\n                torch.Size([442])\n        \"\"\"\n        feature_df, target_df = fetch_california_housing(return_X_y=True, as_frame=True)\n        if self.n_samples is not None:\n            feature_df = feature_df[: self.n_samples]\n            target_df = target_df[: self.n_samples]\n        # Convert DataFrames to PyTorch tensors\n        feature_tensor = torch.tensor(feature_df.values, dtype=self.feature_type)\n        target_tensor = torch.tensor(target_df.values, dtype=self.target_type)\n\n        return feature_tensor, target_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n                dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([11, 65])\n                torch.Size([11])\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(len(dataset))\n            60000\n\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the dataset.\n\n        Returns:\n            str: A string representation of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(dataset)\n            Split: Train\n\n        \"\"\"\n        split = \"Train\" if self.train else \"Test\"\n        return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotpython/data/california/#spotpython.data.california.CaliforniaHousing.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n    dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n    print(dataset.data.shape)\n    print(dataset.targets.shape)\n    torch.Size([11, 65])\n    torch.Size([11])\n</code></pre> Source code in <code>spotpython/data/california.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n            dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n            print(dataset.data.shape)\n            print(dataset.targets.shape)\n            torch.Size([11, 65])\n            torch.Size([11])\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotpython/data/california/#spotpython.data.california.CaliforniaHousing.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(len(dataset))\n60000\n</code></pre> Source code in <code>spotpython/data/california.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(len(dataset))\n        60000\n\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/california/#spotpython.data.california.CaliforniaHousing.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string representation of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(dataset)\nSplit: Train\n</code></pre> Source code in <code>spotpython/data/california.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the dataset.\n\n    Returns:\n        str: A string representation of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(dataset)\n        Split: Train\n\n    \"\"\"\n    split = \"Train\" if self.train else \"Test\"\n    return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotpython/data/california_housing/","title":"california_housing","text":""},{"location":"reference/spotpython/data/california_housing/#spotpython.data.california_housing.CaliforniaHousing","title":"<code>CaliforniaHousing</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for regression. A toy data set from scikit-learn. Data Set Characteristics: * Number of Instances: 20640 * Number of Attributes: 8 numeric, predictive attributes and the target * Attribute Information:     - MedInc median income in block group     - HouseAge median house age in block group     - AveRooms average number of rooms per household     - AveBedrms average number of bedrooms per household     - Population block group population     - AveOccup average number of household members     - Latitude block group latitude     - Longitude block group longitude * Missing Attribute Values: None * Target: The target variable is the median house value for California districts,     expressed in hundreds of thousands of dollars ($100,000).     This dataset was obtained from the StatLib repository:     https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html     This dataset was derived from the 1990 U.S. census, using one row per census block group.     A block group is the smallest geographical unit for which the U.S. Census Bureau publishes     sample data (a block group typically has a population of 600 to 3,000 people).     A household is a group of people residing within a home. Since the average number of rooms     and bedrooms in this dataset are provided per household, these columns may take surprisingly     large values for block groups with few households and many empty houses, such as vacation resorts.</p> <p>Parameters:</p> Name Type Description Default <code>feature_type</code> <code>dtype</code> <p>The data type of the features. Defaults to torch.float.</p> <code>float</code> <code>target_type</code> <code>dtype</code> <p>The data type of the targets. Defaults to torch.long.</p> <code>float</code> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>Tensor</code> <p>The data features.</p> <code>targets</code> <code>Tensor</code> <p>The data targets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.california_housing import CaliforniaHousing\n    import torch\n    dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n    # Set batch size for DataLoader\n    batch_size = 5\n    # Create DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    # Iterate over the data in the DataLoader\n    for batch in dataloader:\n        inputs, targets = batch\n        print(f\"Batch Size: {inputs.size(0)}\")\n        print(\"---------------\")\n        print(f\"Inputs: {inputs}\")\n        print(f\"Targets: {targets}\")\n</code></pre> Source code in <code>spotpython/data/california_housing.py</code> <pre><code>class CaliforniaHousing(Dataset):\n    \"\"\"\n    A PyTorch Dataset for regression. A toy data set from scikit-learn.\n    Data Set Characteristics:\n    * Number of Instances: 20640\n    * Number of Attributes: 8 numeric, predictive attributes and the target\n    * Attribute Information:\n        - MedInc median income in block group\n        - HouseAge median house age in block group\n        - AveRooms average number of rooms per household\n        - AveBedrms average number of bedrooms per household\n        - Population block group population\n        - AveOccup average number of household members\n        - Latitude block group latitude\n        - Longitude block group longitude\n    * Missing Attribute Values: None\n    * Target: The target variable is the median house value for California districts,\n        expressed in hundreds of thousands of dollars ($100,000).\n        This dataset was obtained from the StatLib repository:\n        https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n        This dataset was derived from the 1990 U.S. census, using one row per census block group.\n        A block group is the smallest geographical unit for which the U.S. Census Bureau publishes\n        sample data (a block group typically has a population of 600 to 3,000 people).\n        A household is a group of people residing within a home. Since the average number of rooms\n        and bedrooms in this dataset are provided per household, these columns may take surprisingly\n        large values for block groups with few households and many empty houses, such as vacation resorts.\n\n    Args:\n        feature_type (torch.dtype): The data type of the features. Defaults to torch.float.\n        target_type (torch.dtype): The data type of the targets. Defaults to torch.long.\n        train (bool): Whether the dataset is for training or not. Defaults to True.\n\n    Attributes:\n        data (Tensor): The data features.\n        targets (Tensor): The data targets.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.california_housing import CaliforniaHousing\n            import torch\n            dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n            # Set batch size for DataLoader\n            batch_size = 5\n            # Create DataLoader\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n            # Iterate over the data in the DataLoader\n            for batch in dataloader:\n                inputs, targets = batch\n                print(f\"Batch Size: {inputs.size(0)}\")\n                print(\"---------------\")\n                print(f\"Inputs: {inputs}\")\n                print(f\"Targets: {targets}\")\n    \"\"\"\n\n    def __init__(self, feature_type: torch.dtype = torch.float, target_type: torch.dtype = torch.float, train: bool = True) -&gt; None:\n        super().__init__()\n        self.feature_type = feature_type\n        self.target_type = target_type\n        self.train = train\n        self.names = self.get_names()\n        self.data, self.targets = self._load_data()\n\n    def _load_data(self) -&gt; tuple:\n        \"\"\"Loads the data from scikit-learn and returns the features and targets.\n\n        Returns:\n            tuple: A tuple containing the features and targets.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n                dataset = CaliforniaHousing()\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([20640, 8])\n                torch.Size([20640])\n        \"\"\"\n        feature_df, target_df = fetch_california_housing(return_X_y=True, as_frame=True)\n        # Convert DataFrames to PyTorch tensors\n        feature_tensor = torch.tensor(feature_df.values, dtype=self.feature_type)\n        target_tensor = torch.tensor(target_df.values, dtype=self.target_type)\n\n        return feature_tensor, target_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n                dataset = CaliforniaHousing()\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([20640, 8])\n                torch.Size([20640])\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n                dataset = CaliforniaHousing()\n                print(len(dataset))\n                20640\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the dataset.\n\n        Returns:\n            str: A string representation of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(dataset)\n            Split: Train\n\n        \"\"\"\n        split = \"Train\" if self.train else \"Test\"\n        return f\"Split: {split}\"\n\n    def get_names(self) -&gt; list:\n        \"\"\"\n        Returns the names of the features.\n\n        Returns:\n            list: A list containing the names of the features.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n                dataset = CaliforniaHousing()\n                print(dataset.get_names())\n                ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n        \"\"\"\n        housing = fetch_california_housing()\n        return housing.feature_names\n</code></pre>"},{"location":"reference/spotpython/data/california_housing/#spotpython.data.california_housing.CaliforniaHousing.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n    dataset = CaliforniaHousing()\n    print(dataset.data.shape)\n    print(dataset.targets.shape)\n    torch.Size([20640, 8])\n    torch.Size([20640])\n</code></pre> Source code in <code>spotpython/data/california_housing.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n            dataset = CaliforniaHousing()\n            print(dataset.data.shape)\n            print(dataset.targets.shape)\n            torch.Size([20640, 8])\n            torch.Size([20640])\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotpython/data/california_housing/#spotpython.data.california_housing.CaliforniaHousing.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n    dataset = CaliforniaHousing()\n    print(len(dataset))\n    20640\n</code></pre> Source code in <code>spotpython/data/california_housing.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n            dataset = CaliforniaHousing()\n            print(len(dataset))\n            20640\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/california_housing/#spotpython.data.california_housing.CaliforniaHousing.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string representation of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(dataset)\nSplit: Train\n</code></pre> Source code in <code>spotpython/data/california_housing.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the dataset.\n\n    Returns:\n        str: A string representation of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(dataset)\n        Split: Train\n\n    \"\"\"\n    split = \"Train\" if self.train else \"Test\"\n    return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotpython/data/california_housing/#spotpython.data.california_housing.CaliforniaHousing.get_names","title":"<code>get_names()</code>","text":"<p>Returns the names of the features.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing the names of the features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n    dataset = CaliforniaHousing()\n    print(dataset.get_names())\n    ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n</code></pre> Source code in <code>spotpython/data/california_housing.py</code> <pre><code>def get_names(self) -&gt; list:\n    \"\"\"\n    Returns the names of the features.\n\n    Returns:\n        list: A list containing the names of the features.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.california_housing import CaliforniaHousing\n            dataset = CaliforniaHousing()\n            print(dataset.get_names())\n            ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n    \"\"\"\n    housing = fetch_california_housing()\n    return housing.feature_names\n</code></pre>"},{"location":"reference/spotpython/data/csvdataset/","title":"csvdataset","text":""},{"location":"reference/spotpython/data/csvdataset/#spotpython.data.csvdataset.CSVDataset","title":"<code>CSVDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for handling CSV data.</p> Source code in <code>spotpython/data/csvdataset.py</code> <pre><code>class CSVDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset for handling CSV data.\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"data.csv\",\n        directory: None = None,\n        feature_type: torch.dtype = torch.float,\n        target_column: str = \"y\",\n        target_type: torch.dtype = torch.float,\n        train: bool = True,\n        rmNA=True,\n        dropId=False,\n        oe=OrdinalEncoder(),\n        le=LabelEncoder(),\n        **desc,\n    ) -&gt; None:\n        super().__init__()\n        self.filename = filename\n        self.directory = directory\n        self.feature_type = feature_type\n        self.target_type = target_type\n        self.target_column = target_column\n        self.train = train\n        self.rmNA = rmNA\n        self.dropId = dropId\n        self.oe = oe\n        self.le = le\n        self.data, self.targets = self._load_data()\n\n    @property\n    def path(self):\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self):\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n\n    def _load_data(self) -&gt; tuple:\n        df = pd.read_csv(self.path, index_col=False)\n\n        # Remove rows with NA if specified\n        if self.rmNA:\n            df = df.dropna()\n\n        # Drop the id column if specified\n        if self.dropId and \"id\" in df.columns:\n            df = df.drop(columns=[\"id\"])\n\n        # Split DataFrame into feature and target DataFrames\n        feature_df = df.drop(columns=[self.target_column])\n\n        # Identify non-numerical columns in the feature DataFrame\n        non_numerical_columns = feature_df.select_dtypes(exclude=[\"number\"]).columns.tolist()\n\n        # Apply OrdinalEncoder to non-numerical feature columns\n        if non_numerical_columns:\n            if self.oe is None:\n                raise ValueError(f\"\\n!!! non_numerical_columns in data: {non_numerical_columns}\" \"\\nOrdinalEncoder object oe must be provided for encoding non-numerical columns\")\n            feature_df[non_numerical_columns] = self.oe.fit_transform(feature_df[non_numerical_columns])\n\n        target_df = df[self.target_column]\n\n        # Check if the target column is non-numerical using dtype\n        if not pd.api.types.is_numeric_dtype(target_df):\n            if self.le is None:\n                raise ValueError(f\"\\n!!! The target column '{self.target_column}' is non-numerical\" \"\\nLabelEncoder object le must be provided for encoding non-numerical target\")\n            target_df = self.le.fit_transform(target_df)\n\n        # Convert DataFrames to NumPy arrays and then to PyTorch tensors\n        feature_array = feature_df.to_numpy()\n        target_array = target_df\n\n        feature_tensor = torch.tensor(feature_array, dtype=self.feature_type)\n        target_tensor = torch.tensor(target_array, dtype=self.target_type)\n\n        return feature_tensor, target_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n                dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([11, 65])\n                torch.Size([11])\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(len(dataset))\n            60000\n\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the dataset.\n\n        Returns:\n            str: A string representation of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(dataset)\n            Split: Train\n\n        \"\"\"\n        split = \"Train\" if self.train else \"Test\"\n        return f\"Split: {split}\"\n\n    def __ncols__(self) -&gt; int:\n        \"\"\"\n        Returns the number of columns in the dataset.\n\n        Returns:\n            int: The number of columns in the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n                import torch\n                from torch.utils.data import DataLoader\n                dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n                print(dataset.__ncols__())\n                64\n        \"\"\"\n        return self.data.size(1)\n</code></pre>"},{"location":"reference/spotpython/data/csvdataset/#spotpython.data.csvdataset.CSVDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n    dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n    print(dataset.data.shape)\n    print(dataset.targets.shape)\n    torch.Size([11, 65])\n    torch.Size([11])\n</code></pre> Source code in <code>spotpython/data/csvdataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n            dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n            print(dataset.data.shape)\n            print(dataset.targets.shape)\n            torch.Size([11, 65])\n            torch.Size([11])\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotpython/data/csvdataset/#spotpython.data.csvdataset.CSVDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(len(dataset))\n60000\n</code></pre> Source code in <code>spotpython/data/csvdataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(len(dataset))\n        60000\n\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/csvdataset/#spotpython.data.csvdataset.CSVDataset.__ncols__","title":"<code>__ncols__()</code>","text":"<p>Returns the number of columns in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of columns in the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    print(dataset.__ncols__())\n    64\n</code></pre> Source code in <code>spotpython/data/csvdataset.py</code> <pre><code>def __ncols__(self) -&gt; int:\n    \"\"\"\n    Returns the number of columns in the dataset.\n\n    Returns:\n        int: The number of columns in the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            print(dataset.__ncols__())\n            64\n    \"\"\"\n    return self.data.size(1)\n</code></pre>"},{"location":"reference/spotpython/data/csvdataset/#spotpython.data.csvdataset.CSVDataset.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string representation of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(dataset)\nSplit: Train\n</code></pre> Source code in <code>spotpython/data/csvdataset.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the dataset.\n\n    Returns:\n        str: A string representation of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(dataset)\n        Split: Train\n\n    \"\"\"\n    split = \"Train\" if self.train else \"Test\"\n    return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotpython/data/diabetes/","title":"diabetes","text":""},{"location":"reference/spotpython/data/diabetes/#spotpython.data.diabetes.Diabetes","title":"<code>Diabetes</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for regression. A toy data set from scikit-learn. Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. Number of Instances: 442 Number of Attributes:First 10 columns are numeric predictive values. Target: Column 11 is a quantitative measure of disease progression one year after baseline. Attribute Information:     * age age in years     * sex     * bmi body mass index     * bp average blood pressure     * s1 tc, total serum cholesterol     * s2 ldl, low-density lipoproteins     * s3 hdl, high-density lipoproteins     * s4 tch, total cholesterol / HDL     * s5 ltg, possibly log of serum triglycerides level     * s6 glu, blood sugar level</p> <p>Parameters:</p> Name Type Description Default <code>feature_type</code> <code>dtype</code> <p>The data type of the features. Defaults to torch.float.</p> <code>float</code> <code>target_type</code> <code>dtype</code> <p>The data type of the targets. Defaults to torch.long.</p> <code>float</code> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>Tensor</code> <p>The data features.</p> <code>targets</code> <code>Tensor</code> <p>The data targets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    import torch\n    dataset = Diabetes(feature_type=torch.float32, target_type=torch.float32)\n    # Set batch size for DataLoader\n    batch_size = 5\n    # Create DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    # Iterate over the data in the DataLoader\n    for batch in dataloader:\n        inputs, targets = batch\n        print(f\"Batch Size: {inputs.size(0)}\")\n        print(\"---------------\")\n        print(f\"Inputs: {inputs}\")\n        print(f\"Targets: {targets}\")\n</code></pre> Source code in <code>spotpython/data/diabetes.py</code> <pre><code>class Diabetes(Dataset):\n    \"\"\"\n    A PyTorch Dataset for regression. A toy data set from scikit-learn.\n    Ten baseline variables, age, sex, body mass index, average blood pressure,\n    and six blood serum measurements were obtained for each of n = 442 diabetes patients,\n    as well as the response of interest,\n    a quantitative measure of disease progression one year after baseline.\n    Number of Instances: 442\n    Number of Attributes:First 10 columns are numeric predictive values.\n    Target: Column 11 is a quantitative measure of disease progression one year after baseline.\n    Attribute Information:\n        * age age in years\n        * sex\n        * bmi body mass index\n        * bp average blood pressure\n        * s1 tc, total serum cholesterol\n        * s2 ldl, low-density lipoproteins\n        * s3 hdl, high-density lipoproteins\n        * s4 tch, total cholesterol / HDL\n        * s5 ltg, possibly log of serum triglycerides level\n        * s6 glu, blood sugar level\n\n    Args:\n        feature_type (torch.dtype): The data type of the features. Defaults to torch.float.\n        target_type (torch.dtype): The data type of the targets. Defaults to torch.long.\n        train (bool): Whether the dataset is for training or not. Defaults to True.\n\n    Attributes:\n        data (Tensor): The data features.\n        targets (Tensor): The data targets.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            import torch\n            dataset = Diabetes(feature_type=torch.float32, target_type=torch.float32)\n            # Set batch size for DataLoader\n            batch_size = 5\n            # Create DataLoader\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n            # Iterate over the data in the DataLoader\n            for batch in dataloader:\n                inputs, targets = batch\n                print(f\"Batch Size: {inputs.size(0)}\")\n                print(\"---------------\")\n                print(f\"Inputs: {inputs}\")\n                print(f\"Targets: {targets}\")\n    \"\"\"\n\n    def __init__(self, feature_type: torch.dtype = torch.float, target_type: torch.dtype = torch.float, train: bool = True) -&gt; None:\n        super().__init__()\n        self.feature_type = feature_type\n        self.target_type = target_type\n        self.train = train\n        self.names = self.get_names()\n        self.data, self.targets = self._load_data()\n\n    def _load_data(self) -&gt; tuple:\n        \"\"\"Loads the data from scikit-learn and returns the features and targets.\n\n        Returns:\n            tuple: A tuple containing the features and targets.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n                dataset = Diabetes()\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([442, 10])\n                torch.Size([442])\n        \"\"\"\n        feature_df, target_df = load_diabetes(return_X_y=True, as_frame=True)\n        # Convert DataFrames to PyTorch tensors\n        feature_tensor = torch.tensor(feature_df.values, dtype=self.feature_type)\n        target_tensor = torch.tensor(target_df.values, dtype=self.target_type)\n\n        return feature_tensor, target_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n                dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([11, 65])\n                torch.Size([11])\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(len(dataset))\n            60000\n\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the dataset.\n\n        Returns:\n            str: A string representation of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(dataset)\n            Split: Train\n\n        \"\"\"\n        split = \"Train\" if self.train else \"Test\"\n        return f\"Split: {split}\"\n\n    def get_names(self) -&gt; list:\n        \"\"\"\n        Returns the names of the features.\n\n        Returns:\n            list: A list containing the names of the features.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n                dataset = Diabetes()\n                print(dataset.get_names())\n                [\"age\", \"sex\", \"bmi\", \"bp\", \"tc\", \"ldl\", \"hdl\", \"tch\", \"ltg\", \"glu\"]\n        \"\"\"\n        return [\"age\", \"sex\", \"bmi\", \"bp\", \"s1_tc\", \"s2_ldl\", \"s3_hdl\", \"s4_tch\", \"s5_ltg\", \"s6_glu\"]\n</code></pre>"},{"location":"reference/spotpython/data/diabetes/#spotpython.data.diabetes.Diabetes.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n    dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n    print(dataset.data.shape)\n    print(dataset.targets.shape)\n    torch.Size([11, 65])\n    torch.Size([11])\n</code></pre> Source code in <code>spotpython/data/diabetes.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.csvdataset import CSVDataset\n            dataset = CSVDataset(filename='./data/spotpython/data.csv', target_column='prognosis')\n            print(dataset.data.shape)\n            print(dataset.targets.shape)\n            torch.Size([11, 65])\n            torch.Size([11])\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotpython/data/diabetes/#spotpython.data.diabetes.Diabetes.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(len(dataset))\n60000\n</code></pre> Source code in <code>spotpython/data/diabetes.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(len(dataset))\n        60000\n\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/diabetes/#spotpython.data.diabetes.Diabetes.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string representation of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(dataset)\nSplit: Train\n</code></pre> Source code in <code>spotpython/data/diabetes.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the dataset.\n\n    Returns:\n        str: A string representation of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(dataset)\n        Split: Train\n\n    \"\"\"\n    split = \"Train\" if self.train else \"Test\"\n    return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotpython/data/diabetes/#spotpython.data.diabetes.Diabetes.get_names","title":"<code>get_names()</code>","text":"<p>Returns the names of the features.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing the names of the features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n    dataset = Diabetes()\n    print(dataset.get_names())\n    [\"age\", \"sex\", \"bmi\", \"bp\", \"tc\", \"ldl\", \"hdl\", \"tch\", \"ltg\", \"glu\"]\n</code></pre> Source code in <code>spotpython/data/diabetes.py</code> <pre><code>def get_names(self) -&gt; list:\n    \"\"\"\n    Returns the names of the features.\n\n    Returns:\n        list: A list containing the names of the features.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n            dataset = Diabetes()\n            print(dataset.get_names())\n            [\"age\", \"sex\", \"bmi\", \"bp\", \"tc\", \"ldl\", \"hdl\", \"tch\", \"ltg\", \"glu\"]\n    \"\"\"\n    return [\"age\", \"sex\", \"bmi\", \"bp\", \"s1_tc\", \"s2_ldl\", \"s3_hdl\", \"s4_tch\", \"s5_ltg\", \"s6_glu\"]\n</code></pre>"},{"location":"reference/spotpython/data/friedman/","title":"friedman","text":""},{"location":"reference/spotpython/data/friedman/#spotpython.data.friedman.FriedmanDriftDataset","title":"<code>FriedmanDriftDataset</code>","text":"<p>Friedman Drift Dataset.</p> Source code in <code>spotpython/data/friedman.py</code> <pre><code>class FriedmanDriftDataset:\n    \"\"\"Friedman Drift Dataset.\"\"\"\n\n    def __init__(self, n_samples=100, change_point1=50, change_point2=75, seed=None, constant=False) -&gt; None:\n        \"\"\"Constructor for the Friedman Drift Dataset.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            change_point1 (int): The index of the first change point.\n            change_point2 (int): The index of the second change point.\n            seed (int): The seed for the random number generator.\n            constant (bool): If True, only the first feature is set to 1 and all others are set to 0.\n\n        Returns:\n            None (None): None\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.friedman import FriedmanDriftDataset\n                data_generator = FriedmanDriftDataset(n_samples=100,\n                    seed=42, change_point1=50, change_point2=75, constant=False)\n                data = [data for data in data_generator]\n                indices = [i for _, _, i in data]\n                values = {f\"x{i}\": [] for i in range(5)}\n                values[\"y\"] = []\n                for x, y, _ in data:\n                    for i in range(5):\n                        values[f\"x{i}\"].append(x[i])\n                    values[\"y\"].append(y)\n                plt.figure(figsize=(10, 6))\n                for label, series in values.items():\n                    plt.plot(indices, series, label=label)\n                plt.xlabel('Index')\n                plt.ylabel('Value')\n                plt.title('')\n                plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n                plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n                plt.legend()\n                plt.grid(True)\n                plt.show()\n        \"\"\"\n        self.n_samples = n_samples\n        self._change_point1 = change_point1\n        self._change_point2 = change_point2\n        self.seed = seed\n        self.index = 0\n        self.rng = random.Random(self.seed)\n        self.constant = constant\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.index &gt;= self.n_samples:  # Specifying end of generation\n            raise StopIteration\n        if self.constant:\n            # x[0] is set to 1, all others to 0\n            x = {0: 1}\n            x.update({i: 0 for i in range(1, 10)})  # All x[i] are 0 for i &gt; 0\n        else:\n            x = {i: self.rng.uniform(a=0, b=1) for i in range(10)}\n        y = self._global_recurring_abrupt_gen(x, self.index) + self.rng.gauss(mu=0, sigma=1)\n        result = (x, y, self.index)\n        self.index += 1\n        return result\n\n    def _global_recurring_abrupt_gen(self, x, index):\n        if index &lt; self._change_point1 or index &gt;= self._change_point2:\n            return 10 * math.sin(math.pi * x[0] * x[1]) + 20 * (x[2] - 0.5) ** 2 + 10 * x[3] + 5 * x[4]\n        else:\n            return 10 * math.sin(math.pi * x[3] * x[5]) + 20 * (x[1] - 0.5) ** 2 + 10 * x[0] + 5 * x[2]\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n\n        \"\"\"\n        return self.n_samples\n</code></pre>"},{"location":"reference/spotpython/data/friedman/#spotpython.data.friedman.FriedmanDriftDataset.__init__","title":"<code>__init__(n_samples=100, change_point1=50, change_point2=75, seed=None, constant=False)</code>","text":"<p>Constructor for the Friedman Drift Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> <code>100</code> <code>change_point1</code> <code>int</code> <p>The index of the first change point.</p> <code>50</code> <code>change_point2</code> <code>int</code> <p>The index of the second change point.</p> <code>75</code> <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> <code>None</code> <code>constant</code> <code>bool</code> <p>If True, only the first feature is set to 1 and all others are set to 0.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.friedman import FriedmanDriftDataset\n    data_generator = FriedmanDriftDataset(n_samples=100,\n        seed=42, change_point1=50, change_point2=75, constant=False)\n    data = [data for data in data_generator]\n    indices = [i for _, _, i in data]\n    values = {f\"x{i}\": [] for i in range(5)}\n    values[\"y\"] = []\n    for x, y, _ in data:\n        for i in range(5):\n            values[f\"x{i}\"].append(x[i])\n        values[\"y\"].append(y)\n    plt.figure(figsize=(10, 6))\n    for label, series in values.items():\n        plt.plot(indices, series, label=label)\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.title('')\n    plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n    plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n</code></pre> Source code in <code>spotpython/data/friedman.py</code> <pre><code>def __init__(self, n_samples=100, change_point1=50, change_point2=75, seed=None, constant=False) -&gt; None:\n    \"\"\"Constructor for the Friedman Drift Dataset.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        change_point1 (int): The index of the first change point.\n        change_point2 (int): The index of the second change point.\n        seed (int): The seed for the random number generator.\n        constant (bool): If True, only the first feature is set to 1 and all others are set to 0.\n\n    Returns:\n        None (None): None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.friedman import FriedmanDriftDataset\n            data_generator = FriedmanDriftDataset(n_samples=100,\n                seed=42, change_point1=50, change_point2=75, constant=False)\n            data = [data for data in data_generator]\n            indices = [i for _, _, i in data]\n            values = {f\"x{i}\": [] for i in range(5)}\n            values[\"y\"] = []\n            for x, y, _ in data:\n                for i in range(5):\n                    values[f\"x{i}\"].append(x[i])\n                values[\"y\"].append(y)\n            plt.figure(figsize=(10, 6))\n            for label, series in values.items():\n                plt.plot(indices, series, label=label)\n            plt.xlabel('Index')\n            plt.ylabel('Value')\n            plt.title('')\n            plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n            plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n            plt.legend()\n            plt.grid(True)\n            plt.show()\n    \"\"\"\n    self.n_samples = n_samples\n    self._change_point1 = change_point1\n    self._change_point2 = change_point2\n    self.seed = seed\n    self.index = 0\n    self.rng = random.Random(self.seed)\n    self.constant = constant\n</code></pre>"},{"location":"reference/spotpython/data/friedman/#spotpython.data.friedman.FriedmanDriftDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> Source code in <code>spotpython/data/friedman.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n\n    \"\"\"\n    return self.n_samples\n</code></pre>"},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/","title":"lightcrossvalidationdatamodule","text":""},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/#spotpython.data.lightcrossvalidationdatamodule.LightCrossValidationDataModule","title":"<code>LightCrossValidationDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for handling cross-validation data splits.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The size of the batch. Defaults to 64.</p> <code>64</code> <code>dataset</code> <code>Dataset</code> <p>The dataset from the torch.utils.data Dataset class. It must implement three functions: init, len, and getitem.</p> <code>None</code> <code>data_full_train</code> <code>Dataset</code> <p>The full training dataset from which training and validation sets will be derived.</p> <code>None</code> <code>data_test</code> <code>Dataset</code> <p>The separate test dataset that will be used for testing.</p> <code>None</code> <code>data_val</code> <code>Dataset</code> <p>The separate validation dataset that will be used for validation.</p> <code>None</code> <code>k</code> <code>int</code> <p>The fold number. Defaults to 1.</p> <code>1</code> <code>split_seed</code> <code>int</code> <p>The random seed for splitting the data. Defaults to 42.</p> <code>42</code> <code>num_splits</code> <code>int</code> <p>The number of splits for cross-validation. Defaults to 10.</p> <code>10</code> <code>data_dir</code> <code>str</code> <p>The path to the dataset. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <code>num_workers</code> <code>int</code> <p>The number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory for data loading. Defaults to False.</p> <code>False</code> <code>verbosity</code> <code>int</code> <p>The verbosity level. Defaults to 0.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>data_train</code> <code>Optional[Dataset]</code> <p>The training dataset.</p> <code>data_val</code> <code>Optional[Dataset]</code> <p>The validation dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n&gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n&gt;&gt;&gt; data_module.setup()\n&gt;&gt;&gt; print(f\"Training set size: {len(data_module.data_train)}\")\nTraining set size: 45000\n&gt;&gt;&gt; print(f\"Validation set size: {len(data_module.data_val)}\")\nValidation set size: 5000\n&gt;&gt;&gt; print(f\"Test set size: {len(data_module.data_test)}\")\nTest set size: 10000\n</code></pre> Source code in <code>spotpython/data/lightcrossvalidationdatamodule.py</code> <pre><code>class LightCrossValidationDataModule(L.LightningDataModule):\n    \"\"\"\n    A LightningDataModule for handling cross-validation data splits.\n\n    Args:\n        batch_size (int): The size of the batch. Defaults to 64.\n        dataset (torch.utils.data.Dataset, optional):\n            The dataset from the torch.utils.data Dataset class.\n            It must implement three functions: __init__, __len__, and __getitem__.\n        data_full_train (torch.utils.data.Dataset, optional):\n            The full training dataset from which training and validation sets will be derived.\n        data_test (torch.utils.data.Dataset, optional):\n            The separate test dataset that will be used for testing.\n        data_val (torch.utils.data.Dataset, optional):\n            The separate validation dataset that will be used for validation.\n        k (int): The fold number. Defaults to 1.\n        split_seed (int): The random seed for splitting the data. Defaults to 42.\n        num_splits (int): The number of splits for cross-validation. Defaults to 10.\n        data_dir (str): The path to the dataset. Defaults to \"./data\".\n        num_workers (int): The number of workers for data loading. Defaults to 0.\n        pin_memory (bool): Whether to pin memory for data loading. Defaults to False.\n        verbosity (int): The verbosity level. Defaults to 0.\n\n    Attributes:\n        data_train (Optional[Dataset]): The training dataset.\n        data_val (Optional[Dataset]): The validation dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n        &gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; print(f\"Training set size: {len(data_module.data_train)}\")\n        Training set size: 45000\n        &gt;&gt;&gt; print(f\"Validation set size: {len(data_module.data_val)}\")\n        Validation set size: 5000\n        &gt;&gt;&gt; print(f\"Test set size: {len(data_module.data_test)}\")\n        Test set size: 10000\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size=64,\n        dataset: Optional[object] = None,\n        data_full_train: Optional[object] = None,\n        data_test: Optional[object] = None,\n        data_val: Optional[object] = None,\n        k: int = 1,\n        split_seed: int = 42,\n        num_splits: int = 10,\n        data_dir: str = \"./data\",\n        num_workers: int = 0,\n        pin_memory: bool = False,\n        scaler: Optional[object] = None,\n        collate_fn_name: Optional[str] = None,\n        shuffle_train: bool = True,\n        shuffle_val: bool = False,\n        shuffle_test: bool = False,\n        verbosity: int = 0,\n    ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.data_full = dataset\n        self.data_full_train = data_full_train\n        self.data_test = data_test\n        self.data_val = data_val\n        self.data_dir = data_dir\n        self.num_workers = num_workers\n        self.k = k\n        self.split_seed = split_seed\n        self.num_splits = num_splits\n        self.pin_memory = pin_memory\n        self.scaler = scaler\n        self.save_hyperparameters(logger=False)\n        assert 0 &lt;= self.k &lt; self.num_splits, \"incorrect fold number\"\n        self.collate_fn_name = collate_fn_name\n        self.shuffle_train = shuffle_train\n        self.shuffle_val = shuffle_val\n        self.shuffle_test = shuffle_test\n        self.verbosity = verbosity\n\n        # no data transformations\n        self.transforms = None\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Prepares the data for use.\"\"\"\n        # download\n        pass\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Sets up the data for use.\n\n        Args:\n            stage (Optional[str]): The current stage. Defaults to None.\n        \"\"\"\n        if not self.data_train and not self.data_val:\n            dataset_full = self.data_full\n            kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n            all_splits = [k for k in kf.split(dataset_full)]\n            train_indexes, val_indexes = all_splits[self.hparams.k]\n            train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n            self.data_train = Subset(dataset_full, train_indexes)\n            self.data_val = Subset(dataset_full, val_indexes)\n            if self.verbosity &gt; 0:\n                print(f\"Train Dataset Size: {len(self.data_train)}\")\n                print(f\"Val Dataset Size: {len(self.data_val)}\")\n\n        if self.scaler is not None:\n            # Fit the scaler on training data and transform both train and val data\n            scaler_train_data = torch.stack([self.data_train[i][0] for i in range(len(self.data_train))]).squeeze(1)\n            self.scaler.fit(scaler_train_data)\n            self.data_train = [(self.scaler.transform(data), target) for data, target in self.data_train]\n            data_tensors_train = [data.clone().detach() for data, target in self.data_train]\n            target_tensors_train = [target.clone().detach() for data, target in self.data_train]\n            self.data_train = TensorDataset(torch.stack(data_tensors_train).squeeze(1), torch.stack(target_tensors_train))\n            self.data_val = [(self.scaler.transform(data), target) for data, target in self.data_val]\n            data_tensors_val = [data.clone().detach() for data, target in self.data_val]\n            target_tensors_val = [target.clone().detach() for data, target in self.data_val]\n            self.data_val = TensorDataset(torch.stack(data_tensors_val).squeeze(1), torch.stack(target_tensors_val))\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the training dataloader.\n\n        Returns:\n            DataLoader: The training dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n            &gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n            &gt;&gt;&gt; data_module.setup()\n            &gt;&gt;&gt; train_dataloader = data_module.train_dataloader()\n            &gt;&gt;&gt; print(f\"Training set size: {len(train_dataloader.dataset)}\")\n            Training set size: 45000\n\n        \"\"\"\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the validation dataloader.\n\n        Returns:\n            DataLoader: The validation dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n            &gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n            &gt;&gt;&gt; data_module.setup()\n            &gt;&gt;&gt; val_dataloader = data_module.val_dataloader()\n            &gt;&gt;&gt; print(f\"Validation set size: {len(val_dataloader.dataset)}\")\n            Validation set size: 5000\n        \"\"\"\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n        )\n</code></pre>"},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/#spotpython.data.lightcrossvalidationdatamodule.LightCrossValidationDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepares the data for use.</p> Source code in <code>spotpython/data/lightcrossvalidationdatamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Prepares the data for use.\"\"\"\n    # download\n    pass\n</code></pre>"},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/#spotpython.data.lightcrossvalidationdatamodule.LightCrossValidationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Sets up the data for use.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>The current stage. Defaults to None.</p> <code>None</code> Source code in <code>spotpython/data/lightcrossvalidationdatamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Sets up the data for use.\n\n    Args:\n        stage (Optional[str]): The current stage. Defaults to None.\n    \"\"\"\n    if not self.data_train and not self.data_val:\n        dataset_full = self.data_full\n        kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n        all_splits = [k for k in kf.split(dataset_full)]\n        train_indexes, val_indexes = all_splits[self.hparams.k]\n        train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n        self.data_train = Subset(dataset_full, train_indexes)\n        self.data_val = Subset(dataset_full, val_indexes)\n        if self.verbosity &gt; 0:\n            print(f\"Train Dataset Size: {len(self.data_train)}\")\n            print(f\"Val Dataset Size: {len(self.data_val)}\")\n\n    if self.scaler is not None:\n        # Fit the scaler on training data and transform both train and val data\n        scaler_train_data = torch.stack([self.data_train[i][0] for i in range(len(self.data_train))]).squeeze(1)\n        self.scaler.fit(scaler_train_data)\n        self.data_train = [(self.scaler.transform(data), target) for data, target in self.data_train]\n        data_tensors_train = [data.clone().detach() for data, target in self.data_train]\n        target_tensors_train = [target.clone().detach() for data, target in self.data_train]\n        self.data_train = TensorDataset(torch.stack(data_tensors_train).squeeze(1), torch.stack(target_tensors_train))\n        self.data_val = [(self.scaler.transform(data), target) for data, target in self.data_val]\n        data_tensors_val = [data.clone().detach() for data, target in self.data_val]\n        target_tensors_val = [target.clone().detach() for data, target in self.data_val]\n        self.data_val = TensorDataset(torch.stack(data_tensors_val).squeeze(1), torch.stack(target_tensors_val))\n</code></pre>"},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/#spotpython.data.lightcrossvalidationdatamodule.LightCrossValidationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The training dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n&gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n&gt;&gt;&gt; data_module.setup()\n&gt;&gt;&gt; train_dataloader = data_module.train_dataloader()\n&gt;&gt;&gt; print(f\"Training set size: {len(train_dataloader.dataset)}\")\nTraining set size: 45000\n</code></pre> Source code in <code>spotpython/data/lightcrossvalidationdatamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the training dataloader.\n\n    Returns:\n        DataLoader: The training dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n        &gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; train_dataloader = data_module.train_dataloader()\n        &gt;&gt;&gt; print(f\"Training set size: {len(train_dataloader.dataset)}\")\n        Training set size: 45000\n\n    \"\"\"\n    return DataLoader(\n        dataset=self.data_train,\n        batch_size=self.hparams.batch_size,\n        num_workers=self.hparams.num_workers,\n        pin_memory=self.hparams.pin_memory,\n        shuffle=True,\n    )\n</code></pre>"},{"location":"reference/spotpython/data/lightcrossvalidationdatamodule/#spotpython.data.lightcrossvalidationdatamodule.LightCrossValidationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The validation dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n&gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n&gt;&gt;&gt; data_module.setup()\n&gt;&gt;&gt; val_dataloader = data_module.val_dataloader()\n&gt;&gt;&gt; print(f\"Validation set size: {len(val_dataloader.dataset)}\")\nValidation set size: 5000\n</code></pre> Source code in <code>spotpython/data/lightcrossvalidationdatamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the validation dataloader.\n\n    Returns:\n        DataLoader: The validation dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light import LightCrossValidationDataModule\n        &gt;&gt;&gt; data_module = LightCrossValidationDataModule()\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; val_dataloader = data_module.val_dataloader()\n        &gt;&gt;&gt; print(f\"Validation set size: {len(val_dataloader.dataset)}\")\n        Validation set size: 5000\n    \"\"\"\n    return DataLoader(\n        dataset=self.data_val,\n        batch_size=self.hparams.batch_size,\n        num_workers=self.hparams.num_workers,\n        pin_memory=self.hparams.pin_memory,\n    )\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/","title":"lightdatamodule","text":""},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule","title":"<code>LightDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for handling data.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size. Required.</p> required <code>dataset</code> <code>Dataset</code> <p>The dataset from the torch.utils.data Dataset class. It must implement three functions: init, len, and getitem.</p> <code>None</code> <code>data_full_train</code> <code>Dataset</code> <p>The full training dataset from which training and validation sets will be derived.</p> <code>None</code> <code>data_test</code> <code>Dataset</code> <p>The separate test dataset that will be used for testing.</p> <code>None</code> <code>test_size</code> <code>float</code> <p>The test size. If test_size is float, then train_size is 1 - test_size. If test_size is int, then train_size is len(data_full) - test_size.</p> <code>None</code> <code>test_seed</code> <code>int</code> <p>The test seed. Defaults to 42.</p> <code>42</code> <code>num_workers</code> <code>int</code> <p>The number of workers. Defaults to 0.</p> <code>0</code> <code>scaler</code> <code>object</code> <p>The spot scaler object (e.g. TorchStandardScaler). Defaults to None.</p> <code>None</code> <code>verbosity</code> <code>int</code> <p>The verbosity level. Defaults to 0.</p> <code>0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    from spotpython.utils.scaler import TorchStandardScaler\n    import torch\n    # data.csv is simple csv file with 11 samples\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    scaler = TorchStandardScaler()\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n    data_module.setup()\n    print(f\"Training set size: {len(data_module.data_train)}\")\n    print(f\"Validation set size: {len(data_module.data_val)}\")\n    print(f\"Test set size: {len(data_module.data_test)}\")\n    full_train_size: 0.5\n    val_size: 0.25\n    train_size: 0.25\n    test_size: 0.5\n    Training set size: 3\n    Validation set size: 3\n    Test set size: 6\n</code></pre> References <p>See https://lightning.ai/docs/pytorch/stable/data/datamodule.html</p> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>class LightDataModule(L.LightningDataModule):\n    \"\"\"\n    A LightningDataModule for handling data.\n\n    Args:\n        batch_size (int):\n            The batch size. Required.\n        dataset (torch.utils.data.Dataset, optional):\n            The dataset from the torch.utils.data Dataset class.\n            It must implement three functions: __init__, __len__, and __getitem__.\n        data_full_train (torch.utils.data.Dataset, optional):\n            The full training dataset from which training and validation sets will be derived.\n        data_test (torch.utils.data.Dataset, optional):\n            The separate test dataset that will be used for testing.\n        test_size (float, optional):\n            The test size. If test_size is float, then train_size is 1 - test_size.\n            If test_size is int, then train_size is len(data_full) - test_size.\n        test_seed (int):\n            The test seed. Defaults to 42.\n        num_workers (int):\n            The number of workers. Defaults to 0.\n        scaler (object, optional):\n            The spot scaler object (e.g. TorchStandardScaler). Defaults to None.\n        verbosity (int):\n            The verbosity level. Defaults to 0.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            from spotpython.utils.scaler import TorchStandardScaler\n            import torch\n            # data.csv is simple csv file with 11 samples\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            scaler = TorchStandardScaler()\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n            data_module.setup()\n            print(f\"Training set size: {len(data_module.data_train)}\")\n            print(f\"Validation set size: {len(data_module.data_val)}\")\n            print(f\"Test set size: {len(data_module.data_test)}\")\n            full_train_size: 0.5\n            val_size: 0.25\n            train_size: 0.25\n            test_size: 0.5\n            Training set size: 3\n            Validation set size: 3\n            Test set size: 6\n\n    References:\n        See https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        dataset: Optional[object] = None,\n        data_full_train: Optional[object] = None,\n        data_test: Optional[object] = None,\n        data_val: Optional[object] = None,\n        test_size: Optional[float] = None,\n        test_seed: int = 42,\n        collate_fn_name: Optional[str] = None,\n        shuffle_train: bool = True,\n        shuffle_val: bool = False,\n        shuffle_test: bool = False,\n        num_workers: int = 0,\n        scaler: Optional[object] = None,\n        verbosity: int = 0,\n    ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.data_full = dataset\n        self.data_full_train = data_full_train\n        self.data_test = data_test\n        self.data_val = data_val\n        self.test_size = test_size\n        self.test_seed = test_seed\n        self.collate_fn_name = collate_fn_name\n        self.shuffle_train = shuffle_train\n        self.shuffle_val = shuffle_val\n        self.shuffle_test = shuffle_test\n        self.num_workers = num_workers\n        self.scaler = scaler\n        self.verbosity = verbosity\n\n    def transform_dataset(self, dataset) -&gt; TensorDataset:\n        \"\"\"Applies the scaler transformation to the dataset.\n\n        Args:\n            dataset (List[Tuple[torch.Tensor, Any]]): The dataset to transform, consisting of data and target pairs.\n\n        Returns:\n            TensorDataset: A PyTorch TensorDataset containing the transformed and cloned data and targets.\n\n        Raises:\n            ValueError: If the input data is not correctly formatted for transformation.\n        \"\"\"\n        try:\n            # Perform transformations on the data in a single iteration\n            transformed_data = [(self.scaler.transform(data), target) for data, target in dataset]\n            # Clone and detach data tensors\n            data_tensors = [data.clone().detach() for data, _ in transformed_data]\n            target_tensors = [target.clone().detach() for _, target in transformed_data]\n            # Create a TensorDataset from the processed data\n            return TensorDataset(torch.stack(data_tensors).squeeze(1), torch.stack(target_tensors))\n        except Exception as e:\n            raise ValueError(f\"Error transforming dataset: {e}\")\n\n    def handle_scaling_and_transform(self) -&gt; None:\n        \"\"\"\n        Fits the scaler on the training data and transforms both training and validation datasets.\n        This function is only called when self.scaler is not None.\n        \"\"\"\n        # Ensure self.scaler is not None before proceeding\n        if self.scaler is None:\n            raise ValueError(\"Scaler object is required to perform scaling and transformation.\")\n        # Fit the scaler on training data\n        scaler_train_data = torch.stack([self.data_train[i][0] for i in range(len(self.data_train))]).squeeze(1)\n        if self.verbosity &gt; 0:\n            print(scaler_train_data.shape)\n        self.scaler.fit(scaler_train_data)\n        # Transform the training data\n        self.data_train = self.transform_dataset(self.data_train)\n        # Transform the validation data\n        self.data_val = self.transform_dataset(self.data_val)\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Prepares the data for use.\"\"\"\n        # download\n        pass\n\n    def _setup_full_data_provided(self, stage) -&gt; None:\n        full_size = len(self.data_full)\n        test_size = self.test_size\n\n        # consider the case when test_size is a float\n        if isinstance(self.test_size, float):\n            full_train_size = 1.0 - self.test_size\n            val_size = full_train_size * self.test_size\n            train_size = full_train_size - val_size\n        else:\n            # test_size is an int, training size calculation directly based on it\n            full_train_size = full_size - self.test_size\n            val_size = floor(full_train_size * self.test_size / full_size)\n            train_size = full_size - val_size - test_size\n\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            generator_fit = torch.Generator().manual_seed(self.test_seed)\n            self.data_train, self.data_val, _ = random_split(self.data_full, [train_size, val_size, test_size], generator=generator_fit)\n            if self.verbosity &gt; 0:\n                print(f\"train_size: {train_size}, val_size: {val_size}, test_sie: {test_size} for splitting train &amp; val data.\")\n                print(f\"train samples: {len(self.data_train)}, val samples: {len(self.data_val)} generated for train &amp; val data.\")\n            # Handle scaling and transformation if scaler is provided\n            if self.scaler is not None:\n                self.handle_scaling_and_transform()\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            generator_test = torch.Generator().manual_seed(self.test_seed)\n            self.data_test, _, _ = random_split(self.data_full, [test_size, train_size, val_size], generator=generator_test)\n            if self.verbosity &gt; 0:\n                print(f\"train_size: {train_size}, val_size: {val_size}, test_sie: {test_size} for splitting test data.\")\n                print(f\"test samples: {len(self.data_test)} generated for test data.\")\n            if self.scaler is not None:\n                # Transform the test data\n                self.data_test = self.transform_dataset(self.data_test)\n\n        # Assign pred dataset for use in dataloader(s)\n        if stage == \"predict\" or stage is None:\n            generator_predict = torch.Generator().manual_seed(self.test_seed)\n            self.data_predict, _, _ = random_split(self.data_full, [test_size, train_size, val_size], generator=generator_predict)\n            if self.verbosity &gt; 0:\n                print(f\"train_size: {train_size}, val_size: {val_size}, test_size (= predict_size): {test_size} for splitting predict data.\")\n                print(f\"predict samples: {len(self.data_predict)} generated for predict data.\")\n            if self.scaler is not None:\n                # Transform the predict data\n                self.data_predict = self.transform_dataset(self.data_predict)\n\n    def _setup_test_data_provided(self, stage) -&gt; None:\n        # New functionality with separate full_train and test datasets. Use these datasets directly.\n        full_train_size = len(self.data_full_train)\n        test_size = self.test_size\n        # consider the case when test_size is a float\n        if isinstance(self.test_size, float):\n            val_size = self.test_size\n            train_size = 1 - self.test_size\n        else:\n            # test_size is an int, training size calculation directly based on it\n            full_size = len(self.data_full_train) + len(self.data_test)\n            full_train_size = len(self.data_full_train)\n            val_size = floor(full_train_size * self.test_size / full_size)\n            train_size = full_train_size - val_size\n\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            if self.verbosity &gt; 0:\n                print(f\"train_size: {train_size}, val_size: {val_size} used for train &amp; val data.\")\n            generator_fit = torch.Generator().manual_seed(self.test_seed)\n            self.data_train, self.data_val = random_split(self.data_full_train, [train_size, val_size], generator=generator_fit)\n            # Handle scaling and transformation if scaler is provided\n            if self.scaler is not None:\n                self.handle_scaling_and_transform()\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            if self.verbosity &gt; 0:\n                print(f\"test_size: {test_size} used for test dataset.\")\n            self.data_test = self.data_test\n            if self.scaler is not None:\n                # Transform the test data\n                self.data_test = self.transform_dataset(self.data_test)\n\n        # Assign pred dataset for use in dataloader(s)\n        if stage == \"predict\" or stage is None:\n            if self.verbosity &gt; 0:\n                print(f\"test_size: {test_size} used for predict dataset.\")\n            self.data_predict = self.data_test\n            if self.scaler is not None:\n                # Transform the predict data\n                self.data_predict = self.transform_dataset(self.data_predict)\n\n    def _setup_val_data_provided(self, stage) -&gt; None:\n        # New functionality for predefined train, validation and test data in the fun_control\n        # Get the data set sizes\n        if self.data_full_train is None:\n            raise ValueError(\"If data_val is defined, data_full_train must also be defined.\")\n        train_size = len(self.data_full_train)\n        val_size = len(self.data_val)\n        test_size = len(self.data_test)\n        # Assign train and validation data sets\n        if stage == \"fit\" or stage is None:\n            if self.verbosity &gt; 0:\n                print(f\"train_size: {train_size}, val_size: {val_size} used for train &amp; val data.\")\n            # Use all data from data_full_train as training data\n            self.data_train = self.data_full_train\n            # Handle scaling and transformation if scaler is provided\n            if self.scaler is not None:\n                self.handle_scaling_and_transform()\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            if self.verbosity &gt; 0:\n                print(f\"test_size: {test_size} used for test dataset.\")\n            self.data_test = self.data_test\n            if self.scaler is not None:\n                # Transform the test data\n                self.data_test = self.transform_dataset(self.data_test)\n\n        # Assign pred dataset for use in dataloader(s)\n        if stage == \"predict\" or stage is None:\n            if self.verbosity &gt; 0:\n                print(f\"test_size: {test_size} used for predict dataset.\")\n            self.data_predict = self.data_test\n            if self.scaler is not None:\n                # Transform the predict data\n                self.data_predict = self.transform_dataset(self.data_predict)\n\n    def _get_collate_fn(self, collate_fn_name: str = None) -&gt; object:\n        \"\"\"\n        Returns the collate function based on the collate_fn_name.\n        \"\"\"\n        if collate_fn_name == \"PadSequenceManyToMany\":\n            collate_fn = PadSequenceManyToMany()\n        elif collate_fn_name == \"PadSequenceManyToOne\":\n            collate_fn = PadSequenceManyToOne()\n        else:\n            collate_fn = None\n        return collate_fn\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Splits the data for use in training, validation, and testing.\n        Uses torch.utils.data.random_split() to split the data.\n        Splitting is based on the test_size and test_seed.\n        The test_size can be a float or an int.\n        If a spotpython scaler object is defined, the data will be scaled.\n\n        Args:\n            stage (Optional[str]):\n                The current stage. Can be \"fit\" (for training and validation), \"test\" (testing),\n                or None (for all three stages). Defaults to None.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n                from spotpython.data.csvdataset import CSVDataset\n                import torch\n                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n                data_module.setup()\n                print(f\"Training set size: {len(data_module.data_train)}\")\n                Training set size: 3\n\n        \"\"\"\n        if self.data_full is not None:\n            self._setup_full_data_provided(stage)\n        elif self.data_val is not None:\n            self._setup_val_data_provided(stage)\n        else:\n            self._setup_test_data_provided(stage)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the training dataloader, i.e., a pytorch DataLoader instance\n        using the training dataset.\n\n        Returns:\n            DataLoader: The training dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n                from spotpython.data.csvdataset import CSVDataset\n                import torch\n                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n                data_module.setup()\n                print(f\"Training set size: {len(data_module.data_train)}\")\n                Training set size: 3\n\n        \"\"\"\n        if self.verbosity &gt; 0:\n            print(f\"LightDataModule.train_dataloader(). data_train size: {len(self.data_train)}\")\n        # print(f\"LightDataModule: train_dataloader(). batch_size: {self.batch_size}\")\n        # print(f\"LightDataModule: train_dataloader(). num_workers: {self.num_workers}\")\n        return DataLoader(self.data_train, batch_size=self.batch_size, shuffle=self.shuffle_train, collate_fn=self._get_collate_fn(collate_fn_name=self.collate_fn_name), num_workers=self.num_workers)\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the validation dataloader, i.e., a pytorch DataLoader instance\n        using the validation dataset.\n\n        Returns:\n            DataLoader: The validation dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n                from spotpython.data.csvdataset import CSVDataset\n                import torch\n                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n                data_module.setup()\n                print(f\"Training set size: {len(data_module.data_val)}\")\n                Training set size: 3\n        \"\"\"\n        if self.verbosity &gt; 0:\n            print(f\"LightDataModule.val_dataloader(). Val. set size: {len(self.data_val)}\")\n        # print(f\"LightDataModule: val_dataloader(). batch_size: {self.batch_size}\")\n        # print(f\"LightDataModule: val_dataloader(). num_workers: {self.num_workers}\")\n        return DataLoader(self.data_val, batch_size=self.batch_size, shuffle=self.shuffle_val, collate_fn=self._get_collate_fn(collate_fn_name=self.collate_fn_name), num_workers=self.num_workers)\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the test dataloader, i.e., a pytorch DataLoader instance\n        using the test dataset.\n\n        Returns:\n            DataLoader: The test dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n                from spotpython.data.csvdataset import CSVDataset\n                import torch\n                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n                data_module.setup()\n                print(f\"Test set size: {len(data_module.data_test)}\")\n                Test set size: 6\n\n        \"\"\"\n        if self.verbosity &gt; 0:\n            print(f\"LightDataModule.test_dataloader(). Test set size: {len(self.data_test)}\")\n        # print(f\"LightDataModule: test_dataloader(). batch_size: {self.batch_size}\")\n        # print(f\"LightDataModule: test_dataloader(). num_workers: {self.num_workers}\")\n        return DataLoader(self.data_test, batch_size=self.batch_size, shuffle=self.shuffle_test, collate_fn=self._get_collate_fn(collate_fn_name=self.collate_fn_name), num_workers=self.num_workers)\n\n    def predict_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the predict dataloader, i.e., a pytorch DataLoader instance\n        using the predict dataset.\n\n        Returns:\n            DataLoader: The predict dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n                from spotpython.data.csvdataset import CSVDataset\n                import torch\n                dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n                data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n                data_module.setup()\n                print(f\"Predict set size: {len(data_module.data_predict)}\")\n                Predict set size: 6\n\n        \"\"\"\n        if self.verbosity &gt; 0:\n            print(f\"LightDataModule.predict_dataloader(). Predict set size: {len(self.data_predict)}\")\n        # print(f\"LightDataModule: predict_dataloader(). batch_size: {self.batch_size}\")\n        # print(f\"LightDataModule: predict_dataloader(). num_workers: {self.num_workers}\")\n        return DataLoader(self.data_predict, batch_size=len(self.data_predict), shuffle=False, collate_fn=self._get_collate_fn(collate_fn_name=self.collate_fn_name), num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.handle_scaling_and_transform","title":"<code>handle_scaling_and_transform()</code>","text":"<p>Fits the scaler on the training data and transforms both training and validation datasets. This function is only called when self.scaler is not None.</p> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def handle_scaling_and_transform(self) -&gt; None:\n    \"\"\"\n    Fits the scaler on the training data and transforms both training and validation datasets.\n    This function is only called when self.scaler is not None.\n    \"\"\"\n    # Ensure self.scaler is not None before proceeding\n    if self.scaler is None:\n        raise ValueError(\"Scaler object is required to perform scaling and transformation.\")\n    # Fit the scaler on training data\n    scaler_train_data = torch.stack([self.data_train[i][0] for i in range(len(self.data_train))]).squeeze(1)\n    if self.verbosity &gt; 0:\n        print(scaler_train_data.shape)\n    self.scaler.fit(scaler_train_data)\n    # Transform the training data\n    self.data_train = self.transform_dataset(self.data_train)\n    # Transform the validation data\n    self.data_val = self.transform_dataset(self.data_val)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns the predict dataloader, i.e., a pytorch DataLoader instance using the predict dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The predict dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    import torch\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n    data_module.setup()\n    print(f\"Predict set size: {len(data_module.data_predict)}\")\n    Predict set size: 6\n</code></pre> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the predict dataloader, i.e., a pytorch DataLoader instance\n    using the predict dataset.\n\n    Returns:\n        DataLoader: The predict dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            import torch\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n            data_module.setup()\n            print(f\"Predict set size: {len(data_module.data_predict)}\")\n            Predict set size: 6\n\n    \"\"\"\n    if self.verbosity &gt; 0:\n        print(f\"LightDataModule.predict_dataloader(). Predict set size: {len(self.data_predict)}\")\n    # print(f\"LightDataModule: predict_dataloader(). batch_size: {self.batch_size}\")\n    # print(f\"LightDataModule: predict_dataloader(). num_workers: {self.num_workers}\")\n    return DataLoader(self.data_predict, batch_size=len(self.data_predict), shuffle=False, collate_fn=self._get_collate_fn(collate_fn_name=self.collate_fn_name), num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepares the data for use.</p> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Prepares the data for use.\"\"\"\n    # download\n    pass\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Splits the data for use in training, validation, and testing. Uses torch.utils.data.random_split() to split the data. Splitting is based on the test_size and test_seed. The test_size can be a float or an int. If a spotpython scaler object is defined, the data will be scaled.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>The current stage. Can be \u201cfit\u201d (for training and validation), \u201ctest\u201d (testing), or None (for all three stages). Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    import torch\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n    data_module.setup()\n    print(f\"Training set size: {len(data_module.data_train)}\")\n    Training set size: 3\n</code></pre> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Splits the data for use in training, validation, and testing.\n    Uses torch.utils.data.random_split() to split the data.\n    Splitting is based on the test_size and test_seed.\n    The test_size can be a float or an int.\n    If a spotpython scaler object is defined, the data will be scaled.\n\n    Args:\n        stage (Optional[str]):\n            The current stage. Can be \"fit\" (for training and validation), \"test\" (testing),\n            or None (for all three stages). Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            import torch\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n            data_module.setup()\n            print(f\"Training set size: {len(data_module.data_train)}\")\n            Training set size: 3\n\n    \"\"\"\n    if self.data_full is not None:\n        self._setup_full_data_provided(stage)\n    elif self.data_val is not None:\n        self._setup_val_data_provided(stage)\n    else:\n        self._setup_test_data_provided(stage)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader, i.e., a pytorch DataLoader instance using the test dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The test dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    import torch\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n    data_module.setup()\n    print(f\"Test set size: {len(data_module.data_test)}\")\n    Test set size: 6\n</code></pre> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the test dataloader, i.e., a pytorch DataLoader instance\n    using the test dataset.\n\n    Returns:\n        DataLoader: The test dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            import torch\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n            data_module.setup()\n            print(f\"Test set size: {len(data_module.data_test)}\")\n            Test set size: 6\n\n    \"\"\"\n    if self.verbosity &gt; 0:\n        print(f\"LightDataModule.test_dataloader(). Test set size: {len(self.data_test)}\")\n    # print(f\"LightDataModule: test_dataloader(). batch_size: {self.batch_size}\")\n    # print(f\"LightDataModule: test_dataloader(). num_workers: {self.num_workers}\")\n    return DataLoader(self.data_test, batch_size=self.batch_size, shuffle=self.shuffle_test, collate_fn=self._get_collate_fn(collate_fn_name=self.collate_fn_name), num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader, i.e., a pytorch DataLoader instance using the training dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The training dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    import torch\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n    data_module.setup()\n    print(f\"Training set size: {len(data_module.data_train)}\")\n    Training set size: 3\n</code></pre> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the training dataloader, i.e., a pytorch DataLoader instance\n    using the training dataset.\n\n    Returns:\n        DataLoader: The training dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            import torch\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n            data_module.setup()\n            print(f\"Training set size: {len(data_module.data_train)}\")\n            Training set size: 3\n\n    \"\"\"\n    if self.verbosity &gt; 0:\n        print(f\"LightDataModule.train_dataloader(). data_train size: {len(self.data_train)}\")\n    # print(f\"LightDataModule: train_dataloader(). batch_size: {self.batch_size}\")\n    # print(f\"LightDataModule: train_dataloader(). num_workers: {self.num_workers}\")\n    return DataLoader(self.data_train, batch_size=self.batch_size, shuffle=self.shuffle_train, collate_fn=self._get_collate_fn(collate_fn_name=self.collate_fn_name), num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.transform_dataset","title":"<code>transform_dataset(dataset)</code>","text":"<p>Applies the scaler transformation to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>List[Tuple[Tensor, Any]]</code> <p>The dataset to transform, consisting of data and target pairs.</p> required <p>Returns:</p> Name Type Description <code>TensorDataset</code> <code>TensorDataset</code> <p>A PyTorch TensorDataset containing the transformed and cloned data and targets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data is not correctly formatted for transformation.</p> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def transform_dataset(self, dataset) -&gt; TensorDataset:\n    \"\"\"Applies the scaler transformation to the dataset.\n\n    Args:\n        dataset (List[Tuple[torch.Tensor, Any]]): The dataset to transform, consisting of data and target pairs.\n\n    Returns:\n        TensorDataset: A PyTorch TensorDataset containing the transformed and cloned data and targets.\n\n    Raises:\n        ValueError: If the input data is not correctly formatted for transformation.\n    \"\"\"\n    try:\n        # Perform transformations on the data in a single iteration\n        transformed_data = [(self.scaler.transform(data), target) for data, target in dataset]\n        # Clone and detach data tensors\n        data_tensors = [data.clone().detach() for data, _ in transformed_data]\n        target_tensors = [target.clone().detach() for _, target in transformed_data]\n        # Create a TensorDataset from the processed data\n        return TensorDataset(torch.stack(data_tensors).squeeze(1), torch.stack(target_tensors))\n    except Exception as e:\n        raise ValueError(f\"Error transforming dataset: {e}\")\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.LightDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader, i.e., a pytorch DataLoader instance using the validation dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The validation dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.data.csvdataset import CSVDataset\n    import torch\n    dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n    data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n    data_module.setup()\n    print(f\"Training set size: {len(data_module.data_val)}\")\n    Training set size: 3\n</code></pre> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the validation dataloader, i.e., a pytorch DataLoader instance\n    using the validation dataset.\n\n    Returns:\n        DataLoader: The validation dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.data.csvdataset import CSVDataset\n            import torch\n            dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n            data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n            data_module.setup()\n            print(f\"Training set size: {len(data_module.data_val)}\")\n            Training set size: 3\n    \"\"\"\n    if self.verbosity &gt; 0:\n        print(f\"LightDataModule.val_dataloader(). Val. set size: {len(self.data_val)}\")\n    # print(f\"LightDataModule: val_dataloader(). batch_size: {self.batch_size}\")\n    # print(f\"LightDataModule: val_dataloader(). num_workers: {self.num_workers}\")\n    return DataLoader(self.data_val, batch_size=self.batch_size, shuffle=self.shuffle_val, collate_fn=self._get_collate_fn(collate_fn_name=self.collate_fn_name), num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.PadSequenceManyToMany","title":"<code>PadSequenceManyToMany</code>","text":"<p>A callable class for padding sequences in a many-to-many RNN model.</p> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>class PadSequenceManyToMany:\n    \"\"\"\n    A callable class for padding sequences in a many-to-many RNN model.\n    \"\"\"\n\n    def __call__(self, batch):\n        batch_x, batch_y = zip(*batch)\n        padded_batch_x = pad_sequence(batch_x, batch_first=True)\n        padded_batch_y = pad_sequence(batch_y, batch_first=True)\n        lengths = torch.tensor([len(x) for x in batch_x])\n\n        return padded_batch_x, lengths, padded_batch_y\n</code></pre>"},{"location":"reference/spotpython/data/lightdatamodule/#spotpython.data.lightdatamodule.PadSequenceManyToOne","title":"<code>PadSequenceManyToOne</code>","text":"<p>A callable class for padding sequences in a many-to-one RNN model.</p> Source code in <code>spotpython/data/lightdatamodule.py</code> <pre><code>class PadSequenceManyToOne:\n    \"\"\"\n    A callable class for padding sequences in a many-to-one RNN model.\n    \"\"\"\n\n    def __call__(self, batch):\n        batch_x, batch_y = zip(*batch)\n        padded_batch_x = pad_sequence(batch_x, batch_first=True)\n        lengths = torch.tensor([len(x) for x in batch_x])\n\n        return padded_batch_x, lengths, torch.tensor(batch_y)\n</code></pre>"},{"location":"reference/spotpython/data/manydataset/","title":"manydataset","text":""},{"location":"reference/spotpython/data/manydataset/#spotpython.data.manydataset.ManyToManyDataset","title":"<code>ManyToManyDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for many-to-many data.</p> <p>Parameters:</p> Name Type Description Default <code>df_list</code> <code>List[DataFrame]</code> <p>List of pandas DataFrames.</p> required <code>target</code> <code>str</code> <p>The target column name.</p> required <code>drop</code> <code>Optional[Union[str, List[str]]]</code> <p>Column(s) to drop from the DataFrames. Default is None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensors. Default is torch.float32.</p> <code>float32</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>List[DataFrame]</code> <p>List of pandas DataFrames with specified columns dropped.</p> <code>target</code> <code>List[Tensor]</code> <p>List of target tensors.</p> <code>features</code> <code>List[Tensor]</code> <p>List of feature tensors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotpython.data.manydataset import ManyToManyDataset\n&gt;&gt;&gt; df1 = pd.DataFrame({'feature1': [1, 2], 'feature2': [3, 4], 'target': [5, 6]})\n&gt;&gt;&gt; df2 = pd.DataFrame({'feature1': [7, 8], 'feature2': [9, 10], 'target': [11, 12]})\n&gt;&gt;&gt; dataset = ManyToManyDataset([df1, df2], target='target', drop='feature2')\n&gt;&gt;&gt; len(dataset)\n2\n&gt;&gt;&gt; dataset[0]\n(tensor([[1.],\n         [2.]]), tensor([5., 6.]))\n</code></pre> Source code in <code>spotpython/data/manydataset.py</code> <pre><code>class ManyToManyDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset for many-to-many data.\n\n    Args:\n        df_list (List[pd.DataFrame]): List of pandas DataFrames.\n        target (str): The target column name.\n        drop (Optional[Union[str, List[str]]]): Column(s) to drop from the DataFrames. Default is None.\n        dtype (torch.dtype): Data type for the tensors. Default is torch.float32.\n\n    Attributes:\n        data (List[pd.DataFrame]): List of pandas DataFrames with specified columns dropped.\n        target (List[torch.Tensor]): List of target tensors.\n        features (List[torch.Tensor]): List of feature tensors.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotpython.data.manydataset import ManyToManyDataset\n        &gt;&gt;&gt; df1 = pd.DataFrame({'feature1': [1, 2], 'feature2': [3, 4], 'target': [5, 6]})\n        &gt;&gt;&gt; df2 = pd.DataFrame({'feature1': [7, 8], 'feature2': [9, 10], 'target': [11, 12]})\n        &gt;&gt;&gt; dataset = ManyToManyDataset([df1, df2], target='target', drop='feature2')\n        &gt;&gt;&gt; len(dataset)\n        2\n        &gt;&gt;&gt; dataset[0]\n        (tensor([[1.],\n                 [2.]]), tensor([5., 6.]))\n    \"\"\"\n\n    def __init__(\n        self,\n        df_list: List[pd.DataFrame],\n        target: str,\n        drop: Optional[Union[str, List[str]]] = None,\n        dtype: torch.dtype = torch.float32,\n    ):\n        try:\n            self.data = [df.drop(drop, axis=1) for df in df_list]\n        except KeyError:\n            self.data = df_list\n        self.target = [torch.tensor(df[target].to_numpy(), dtype=dtype) for df in self.data]\n        self.features = [torch.tensor(df.drop([target], axis=1).to_numpy(), dtype=dtype) for df in self.data]\n\n    def __getitem__(self, index: int):\n        x = self.features[index]\n        y = self.target[index]\n        return x, y\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/manydataset/#spotpython.data.manydataset.ManyToOneDataset","title":"<code>ManyToOneDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for many-to-one data.</p> <p>Parameters:</p> Name Type Description Default <code>df_list</code> <code>List[DataFrame]</code> <p>List of pandas DataFrames.</p> required <code>target</code> <code>str</code> <p>The target column name.</p> required <code>drop</code> <code>Optional[Union[str, List[str]]]</code> <p>Column(s) to drop from the DataFrames. Default is None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type for the tensors. Default is torch.float32.</p> <code>float32</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>List[DataFrame]</code> <p>List of pandas DataFrames with specified columns dropped.</p> <code>target</code> <code>List[Tensor]</code> <p>List of target tensors.</p> <code>features</code> <code>List[Tensor]</code> <p>List of feature tensors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotpython.data.manydataset import ManyToOneDataset\n&gt;&gt;&gt; df1 = pd.DataFrame({'feature1': [1, 2], 'feature2': [3, 4], 'target': [5, 6]})\n&gt;&gt;&gt; df2 = pd.DataFrame({'feature1': [7, 8], 'feature2': [9, 10], 'target': [11, 12]})\n&gt;&gt;&gt; dataset = ManyToOneDataset([df1, df2], target='target', drop='feature2')\n&gt;&gt;&gt; len(dataset)\n2\n&gt;&gt;&gt; dataset[0]\n(tensor([[1.],\n         [2.]]), tensor(5.))\n</code></pre> Source code in <code>spotpython/data/manydataset.py</code> <pre><code>class ManyToOneDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset for many-to-one data.\n\n    Args:\n        df_list (List[pd.DataFrame]): List of pandas DataFrames.\n        target (str): The target column name.\n        drop (Optional[Union[str, List[str]]]): Column(s) to drop from the DataFrames. Default is None.\n        dtype (torch.dtype): Data type for the tensors. Default is torch.float32.\n\n    Attributes:\n        data (List[pd.DataFrame]): List of pandas DataFrames with specified columns dropped.\n        target (List[torch.Tensor]): List of target tensors.\n        features (List[torch.Tensor]): List of feature tensors.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotpython.data.manydataset import ManyToOneDataset\n        &gt;&gt;&gt; df1 = pd.DataFrame({'feature1': [1, 2], 'feature2': [3, 4], 'target': [5, 6]})\n        &gt;&gt;&gt; df2 = pd.DataFrame({'feature1': [7, 8], 'feature2': [9, 10], 'target': [11, 12]})\n        &gt;&gt;&gt; dataset = ManyToOneDataset([df1, df2], target='target', drop='feature2')\n        &gt;&gt;&gt; len(dataset)\n        2\n        &gt;&gt;&gt; dataset[0]\n        (tensor([[1.],\n                 [2.]]), tensor(5.))\n    \"\"\"\n\n    def __init__(\n        self,\n        df_list: List[pd.DataFrame],\n        target: str,\n        drop: Optional[Union[str, List[str]]] = None,\n        dtype: torch.dtype = torch.float32,\n    ):\n        try:\n            self.data = [df.drop(drop, axis=1) for df in df_list]\n        except KeyError:\n            self.data = df_list\n        self.target = [torch.tensor(df[target].to_numpy()[0], dtype=dtype) for df in self.data]\n        self.features = [torch.tensor(df.drop([target], axis=1).to_numpy(), dtype=dtype) for df in self.data]\n\n    def __getitem__(self, index: int):\n        x = self.features[index]\n        y = self.target[index]\n        return x, y\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/mnistdatamodule/","title":"mnistdatamodule","text":""},{"location":"reference/spotpython/data/pkldataset/","title":"pkldataset","text":""},{"location":"reference/spotpython/data/pkldataset/#spotpython.data.pkldataset.PKLDataset","title":"<code>PKLDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset Class for handling pickle (*.pkl) data.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename of the pkl file. Defaults to \u201cdata.pkl\u201d.</p> <code>'data.pkl'</code> <code>directory</code> <code>str</code> <p>The directory where the pkl file is located. Defaults to None.</p> <code>None</code> <code>feature_type</code> <code>dtype</code> <p>The data type of the features. Defaults to torch.float.</p> <code>float</code> <code>target_column</code> <code>str</code> <p>The name of the target column. Defaults to \u201cy\u201d.</p> <code>'y'</code> <code>target_type</code> <code>dtype</code> <p>The data type of the targets. Defaults to torch.long.</p> <code>float</code> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not. Defaults to True.</p> <code>True</code> <code>rmNA</code> <code>bool</code> <p>Whether to remove rows with NA values or not. Defaults to True.</p> <code>True</code> <code>**desc</code> <code>Any</code> <p>Additional arguments to be passed to the base class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The filename of the pkl file.</p> <code>directory</code> <code>str</code> <p>The directory where the pkl file is located.</p> <code>feature_type</code> <code>dtype</code> <p>The data type of the features. Defaults to torch.float.</p> <code>target_column</code> <code>str</code> <p>The name of the target column.</p> <code>target_type</code> <code>dtype</code> <p>The data type of the targets. Defaults to torch.float.</p> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not.</p> <code>rmNA</code> <code>bool</code> <p>Whether to remove rows with NA values or not.</p> <code>data</code> <code>Tensor</code> <p>The features.</p> <code>targets</code> <code>Tensor</code> <p>The targets.</p> Notes <ul> <li><code>spotpython</code> comes with a sample pkl file, which is located at <code>spotpython/data/pkldataset.pkl</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    # Set batch size for DataLoader\n    batch_size = 5\n    # Create DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    # Iterate over the data in the DataLoader\n    for batch in dataloader:\n        inputs, targets = batch\n        print(f\"Batch Size: {inputs.size(0)}\")\n        print(\"---------------\")\n        print(f\"Inputs: {inputs}\")\n        print(f\"Targets: {targets}\")\n        break\n    Batch Size: 5\n    ---------------\n    Inputs: tensor([[1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n            0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n            [1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n            1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n            0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n            [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n            1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n            0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n    Targets: tensor([ 0,  1,  6,  9, 10])\n&gt;&gt;&gt; # Load the data from a different directory:\n&gt;&gt;&gt; # Similar to the above example, but with a different target column, full path, and different data type\n&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\",\n                        filename=\"data_sensitive.pkl\",\n                        target_column='N',\n                        feature_type=torch.float32,\n                        target_type=torch.float32,\n                        rmNA=True)\n</code></pre> Source code in <code>spotpython/data/pkldataset.py</code> <pre><code>class PKLDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset Class for handling pickle (*.pkl) data.\n\n    Args:\n        filename (str):\n            The filename of the pkl file. Defaults to \"data.pkl\".\n        directory (str):\n            The directory where the pkl file is located. Defaults to None.\n        feature_type (torch.dtype):\n            The data type of the features. Defaults to torch.float.\n        target_column (str):\n            The name of the target column. Defaults to \"y\".\n        target_type (torch.dtype):\n            The data type of the targets. Defaults to torch.long.\n        train (bool):\n            Whether the dataset is for training or not. Defaults to True.\n        rmNA (bool):\n            Whether to remove rows with NA values or not. Defaults to True.\n        **desc (Any):\n            Additional arguments to be passed to the base class.\n\n    Attributes:\n        filename (str):\n            The filename of the pkl file.\n        directory (str):\n            The directory where the pkl file is located.\n        feature_type (torch.dtype):\n            The data type of the features.\n            Defaults to torch.float.\n        target_column (str):\n            The name of the target column.\n        target_type (torch.dtype):\n            The data type of the targets.\n            Defaults to torch.float.\n        train (bool):\n            Whether the dataset is for training or not.\n        rmNA (bool):\n            Whether to remove rows with NA values or not.\n        data (torch.Tensor):\n            The features.\n        targets (torch.Tensor):\n            The targets.\n\n    Notes:\n        * `spotpython` comes with a sample pkl file, which is located at `spotpython/data/pkldataset.pkl`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            # Set batch size for DataLoader\n            batch_size = 5\n            # Create DataLoader\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n            # Iterate over the data in the DataLoader\n            for batch in dataloader:\n                inputs, targets = batch\n                print(f\"Batch Size: {inputs.size(0)}\")\n                print(\"---------------\")\n                print(f\"Inputs: {inputs}\")\n                print(f\"Targets: {targets}\")\n                break\n            Batch Size: 5\n            ---------------\n            Inputs: tensor([[1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n                    0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n                    [1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n                    1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n                    0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n                    [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n                    1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n                    0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n            Targets: tensor([ 0,  1,  6,  9, 10])\n        &gt;&gt;&gt; # Load the data from a different directory:\n        &gt;&gt;&gt; # Similar to the above example, but with a different target column, full path, and different data type\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\",\n                                filename=\"data_sensitive.pkl\",\n                                target_column='N',\n                                feature_type=torch.float32,\n                                target_type=torch.float32,\n                                rmNA=True)\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"data.pkl\",\n        directory: None = None,\n        feature_type: torch.dtype = torch.float,\n        target_column: str = \"y\",\n        target_type: torch.dtype = torch.float,\n        train: bool = True,\n        rmNA=True,\n        oe=OrdinalEncoder(),\n        le=LabelEncoder(),\n        **desc,\n    ) -&gt; None:\n        super().__init__()\n        self.filename = filename\n        self.directory = directory\n        self.feature_type = feature_type\n        self.target_type = target_type\n        self.target_column = target_column\n        self.train = train\n        self.rmNA = rmNA\n        self.oe = oe\n        self.le = le\n        self.data, self.targets = self._load_data()\n\n    @property\n    def path(self):\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self):\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n\n    def _load_data(self) -&gt; tuple:\n        # ensure that self.target_type and self.feature_type are the same torch types\n        if self.target_type != self.feature_type:\n            raise ValueError(\"target_type and feature_type must be the same torch type\")\n        with open(self.path, \"rb\") as f:\n            df = pd.read_pickle(f)\n        # rm rows with NA\n        if self.rmNA:\n            df = df.dropna()\n\n        # Split DataFrame into feature and target DataFrames\n        feature_df = df.drop(columns=[self.target_column])\n\n        # Identify non-numerical columns in the feature DataFrame\n        non_numerical_columns = feature_df.select_dtypes(exclude=[\"number\"]).columns.tolist()\n\n        # Apply OrdinalEncoder to non-numerical feature columns\n        if non_numerical_columns:\n            feature_df[non_numerical_columns] = self.oe.fit_transform(feature_df[non_numerical_columns])\n\n        target_df = df[self.target_column]\n\n        # Check if the target column is non-numerical using dtype\n        if not pd.api.types.is_numeric_dtype(target_df):\n            target_df = self.le.fit_transform(target_df)\n\n        # Convert DataFrames to NumPy arrays and then to PyTorch tensors\n        feature_array = feature_df.to_numpy()\n        target_array = target_df\n\n        feature_tensor = torch.tensor(feature_array, dtype=self.feature_type)\n        target_tensor = torch.tensor(target_array, dtype=self.target_type)\n\n        return feature_tensor, target_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n                import torch\n                from torch.utils.data import DataLoader\n                dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n                print(dataset.data.shape)\n                print(dataset.targets.shape)\n                torch.Size([11, 64])\n                torch.Size([11])\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n                import torch\n                from torch.utils.data import DataLoader\n                dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n                print(len(dataset))\n                11\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string with the filename and directory of the dataset.\n\n        Returns:\n            str: A string with the filename and directory of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n                import torch\n                from torch.utils.data import DataLoader\n                dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n                print(dataset)\n        \"\"\"\n        return \"filename={}, directory={}\".format(self.filename, self.directory)\n\n    def __ncols__(self) -&gt; int:\n        \"\"\"\n        Returns the number of columns in the dataset.\n\n        Returns:\n            int: The number of columns in the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n                import torch\n                from torch.utils.data import DataLoader\n                dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n                print(dataset.__ncols__())\n                64\n        \"\"\"\n        return self.data.size(1)\n</code></pre>"},{"location":"reference/spotpython/data/pkldataset/#spotpython.data.pkldataset.PKLDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    print(dataset.data.shape)\n    print(dataset.targets.shape)\n    torch.Size([11, 64])\n    torch.Size([11])\n</code></pre> Source code in <code>spotpython/data/pkldataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            print(dataset.data.shape)\n            print(dataset.targets.shape)\n            torch.Size([11, 64])\n            torch.Size([11])\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotpython/data/pkldataset/#spotpython.data.pkldataset.PKLDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    print(len(dataset))\n    11\n</code></pre> Source code in <code>spotpython/data/pkldataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            print(len(dataset))\n            11\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotpython/data/pkldataset/#spotpython.data.pkldataset.PKLDataset.__ncols__","title":"<code>__ncols__()</code>","text":"<p>Returns the number of columns in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of columns in the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    print(dataset.__ncols__())\n    64\n</code></pre> Source code in <code>spotpython/data/pkldataset.py</code> <pre><code>def __ncols__(self) -&gt; int:\n    \"\"\"\n    Returns the number of columns in the dataset.\n\n    Returns:\n        int: The number of columns in the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            print(dataset.__ncols__())\n            64\n    \"\"\"\n    return self.data.size(1)\n</code></pre>"},{"location":"reference/spotpython/data/pkldataset/#spotpython.data.pkldataset.PKLDataset.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string with the filename and directory of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string with the filename and directory of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n    import torch\n    from torch.utils.data import DataLoader\n    dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n    print(dataset)\n</code></pre> Source code in <code>spotpython/data/pkldataset.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string with the filename and directory of the dataset.\n\n    Returns:\n        str: A string with the filename and directory of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.pkldataset import PKLDataset\n            import torch\n            from torch.utils.data import DataLoader\n            dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n            print(dataset)\n    \"\"\"\n    return \"filename={}, directory={}\".format(self.filename, self.directory)\n</code></pre>"},{"location":"reference/spotpython/data/torchdata/","title":"torchdata","text":""},{"location":"reference/spotpython/data/torchdata/#spotpython.data.torchdata.load_data_cifar10","title":"<code>load_data_cifar10(data_dir='./data')</code>","text":"<p>Load the CIFAR-10 dataset.     This function loads the CIFAR-10 dataset using the torchvision library.     The data is split into a training set and a test set.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>The directory where the data is stored. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <p>Returns:</p> Type Description <code>Tuple[CIFAR10, CIFAR10]</code> <p>Tuple[datasets.CIFAR10, datasets.CIFAR10]: A tuple containing the training set and the test set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; trainset, testset = load_data_cifar10()\n&gt;&gt;&gt; print(f\"Training set size: {len(trainset)}\")\nTraining set size: 50000\n&gt;&gt;&gt; print(f\"Test set size: {len(testset)}\")\nTest set size: 10000\n</code></pre> Source code in <code>spotpython/data/torchdata.py</code> <pre><code>def load_data_cifar10(data_dir: str = \"./data\") -&gt; Tuple[datasets.CIFAR10, datasets.CIFAR10]:\n    \"\"\"Load the CIFAR-10 dataset.\n        This function loads the CIFAR-10 dataset using the torchvision library.\n        The data is split into a training set and a test set.\n\n    Args:\n        data_dir (str):\n            The directory where the data is stored. Defaults to \"./data\".\n\n    Returns:\n        Tuple[datasets.CIFAR10, datasets.CIFAR10]:\n            A tuple containing the training set and the test set.\n\n    Examples:\n        &gt;&gt;&gt; trainset, testset = load_data_cifar10()\n        &gt;&gt;&gt; print(f\"Training set size: {len(trainset)}\")\n        Training set size: 50000\n        &gt;&gt;&gt; print(f\"Test set size: {len(testset)}\")\n        Test set size: 10000\n\n    \"\"\"\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n    trainset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n\n    testset = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n\n    return trainset, testset\n</code></pre>"},{"location":"reference/spotpython/data/vbdp/","title":"vbdp","text":""},{"location":"reference/spotpython/data/vbdp/#spotpython.data.vbdp.affinity_propagation_features","title":"<code>affinity_propagation_features(X)</code>","text":"<p>Clusters the features of a dataframe using Affinity Propagation.</p> <p>This function takes a dataframe with features and clusters them using the Affinity Propagation algorithm. The resulting dataframe contains the original features as well as a new feature representing the cluster labels.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>A dataframe with features.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the original features and a new cluster feature.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n&gt;&gt;&gt; df\n    a      b      c\n0  True   True   False\n1  False  True   False\n2  True   False  True\n&gt;&gt;&gt; affinity_propagation_features(df)\nEstimated number of clusters: 3\n    a      b      c  cluster\n0  True   True   False       0\n1  False  True   False       1\n2  True   False  True        2\n</code></pre> Source code in <code>spotpython/data/vbdp.py</code> <pre><code>def affinity_propagation_features(X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Clusters the features of a dataframe using Affinity Propagation.\n\n    This function takes a dataframe with features and clusters them using the\n    Affinity Propagation algorithm. The resulting dataframe contains the original\n    features as well as a new feature representing the cluster labels.\n\n    Args:\n        X (pd.DataFrame):\n            A dataframe with features.\n\n    Returns:\n        (pd.DataFrame):\n            A dataframe with the original features and a new cluster feature.\n\n    Examples:\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n        &gt;&gt;&gt; df\n            a      b      c\n        0  True   True   False\n        1  False  True   False\n        2  True   False  True\n        &gt;&gt;&gt; affinity_propagation_features(df)\n        Estimated number of clusters: 3\n            a      b      c  cluster\n        0  True   True   False       0\n        1  False  True   False       1\n        2  True   False  True        2\n    \"\"\"\n    D = manhattan_distances(X)\n    af = AffinityPropagation(random_state=0, affinity=\"precomputed\").fit(D)\n    cluster_centers_indices = af.cluster_centers_indices_\n    n_clusters_ = len(cluster_centers_indices)\n    print(\"Estimated number of clusters: %d\" % n_clusters_)\n    X[\"cluster\"] = af.labels_\n    return X\n</code></pre>"},{"location":"reference/spotpython/data/vbdp/#spotpython.data.vbdp.cluster_features","title":"<code>cluster_features(X)</code>","text":"<p>Clusters the features of a dataframe based on similarity.</p> <p>This function takes a dataframe with features and clusters them based on similarity. The resulting dataframe contains the original features as well as new features representing the clusters.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>A dataframe with features.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the original features and new cluster features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n&gt;&gt;&gt; df\n    a      b      c\n0  True   True  False\n1 False   True  False\n2  True  False   True\n&gt;&gt;&gt; cluster_features(df)\n    a      b      c  c_0  c_1  c_2  c_3\n0  True   True  False    0    0    0    0\n1 False   True  False    0    0    0    0\n2  True  False   True    0    0    0    0\n</code></pre> Source code in <code>spotpython/data/vbdp.py</code> <pre><code>def cluster_features(X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Clusters the features of a dataframe based on similarity.\n\n    This function takes a dataframe with features and clusters them based on similarity.\n    The resulting dataframe contains the original features as well as new features representing the clusters.\n\n    Args:\n        X (pd.DataFrame): A dataframe with features.\n\n    Returns:\n        (pd.DataFrame): A dataframe with the original features and new cluster features.\n\n    Examples:\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n        &gt;&gt;&gt; df\n            a      b      c\n        0  True   True  False\n        1 False   True  False\n        2  True  False   True\n        &gt;&gt;&gt; cluster_features(df)\n            a      b      c  c_0  c_1  c_2  c_3\n        0  True   True  False    0    0    0    0\n        1 False   True  False    0    0    0    0\n        2  True  False   True    0    0    0    0\n    \"\"\"\n    c_0 = X.columns[X.columns.str.contains(\"pain\")]\n    c_1 = X.columns[X.columns.str.contains(\"inflammation\")]\n    c_2 = X.columns[X.columns.str.contains(\"bleed\")]\n    c_3 = X.columns[X.columns.str.contains(\"skin\")]\n    X[\"c_0\"] = X[c_0].sum(axis=1)\n    X[\"c_1\"] = X[c_1].sum(axis=1)\n    X[\"c_2\"] = X[c_2].sum(axis=1)\n    X[\"c_3\"] = X[c_3].sum(axis=1)\n    return X\n</code></pre>"},{"location":"reference/spotpython/design/designs/","title":"designs","text":""},{"location":"reference/spotpython/design/designs/#spotpython.design.designs.Designs","title":"<code>Designs</code>","text":"<p>Super class for all design classes (factorial and space-filling).</p> <p>Attributes:</p> Name Type Description <code>designs</code> <code>List</code> <p>A list of design instances.</p> <code>k</code> <code>int</code> <p>The dimension of the design.</p> <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> <code>rng</code> <code>Generator</code> <p>A random number generator instance.</p> Source code in <code>spotpython/design/designs.py</code> <pre><code>class Designs:\n    \"\"\"\n    Super class for all design classes (factorial and space-filling).\n\n    Attributes:\n        designs (List): A list of design instances.\n        k (int): The dimension of the design.\n        seed (int): The seed for the random number generator.\n        rng (Generator): A random number generator instance.\n    \"\"\"\n\n    def __init__(self, k: int, seed: int = 123) -&gt; None:\n        \"\"\"\n        Initializes a Designs object with the given dimension and seed.\n\n        Args:\n            k (int): The dimension of the design.\n            seed (int): The seed for the random number generator. Defaults to 123.\n\n        Raises:\n            ValueError: If 'k' is not an integer.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.design.designs import Designs\n            &gt;&gt;&gt; designs = Designs(k=2, seed=123)\n            &gt;&gt;&gt; designs.get_dim()\n            2\n        \"\"\"\n        if not isinstance(k, int):\n            raise ValueError(\"The dimension of the design must be an integer.\")\n\n        self.k: int = k\n        self.seed: int = seed\n        self.rng = default_rng(self.seed)\n        self.designs: List = []\n\n    def get_dim(self) -&gt; int:\n        \"\"\"\n        Returns the dimension of the design.\n\n        Returns:\n            int: The dimension of the design.\n        \"\"\"\n        return self.k\n</code></pre>"},{"location":"reference/spotpython/design/designs/#spotpython.design.designs.Designs.__init__","title":"<code>__init__(k, seed=123)</code>","text":"<p>Initializes a Designs object with the given dimension and seed.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The dimension of the design.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 123.</p> <code>123</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If \u2018k\u2019 is not an integer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.design.designs import Designs\n&gt;&gt;&gt; designs = Designs(k=2, seed=123)\n&gt;&gt;&gt; designs.get_dim()\n2\n</code></pre> Source code in <code>spotpython/design/designs.py</code> <pre><code>def __init__(self, k: int, seed: int = 123) -&gt; None:\n    \"\"\"\n    Initializes a Designs object with the given dimension and seed.\n\n    Args:\n        k (int): The dimension of the design.\n        seed (int): The seed for the random number generator. Defaults to 123.\n\n    Raises:\n        ValueError: If 'k' is not an integer.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.design.designs import Designs\n        &gt;&gt;&gt; designs = Designs(k=2, seed=123)\n        &gt;&gt;&gt; designs.get_dim()\n        2\n    \"\"\"\n    if not isinstance(k, int):\n        raise ValueError(\"The dimension of the design must be an integer.\")\n\n    self.k: int = k\n    self.seed: int = seed\n    self.rng = default_rng(self.seed)\n    self.designs: List = []\n</code></pre>"},{"location":"reference/spotpython/design/designs/#spotpython.design.designs.Designs.get_dim","title":"<code>get_dim()</code>","text":"<p>Returns the dimension of the design.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The dimension of the design.</p> Source code in <code>spotpython/design/designs.py</code> <pre><code>def get_dim(self) -&gt; int:\n    \"\"\"\n    Returns the dimension of the design.\n\n    Returns:\n        int: The dimension of the design.\n    \"\"\"\n    return self.k\n</code></pre>"},{"location":"reference/spotpython/design/factorial/","title":"factorial","text":""},{"location":"reference/spotpython/design/factorial/#spotpython.design.factorial.factorial","title":"<code>factorial</code>","text":"<p>               Bases: <code>designs</code></p> <p>Super class for factorial designs.</p> <p>Attributes:</p> Name Type Description <code>k</code> <code>int</code> <p>The number of factors.</p> <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> Source code in <code>spotpython/design/factorial.py</code> <pre><code>class factorial(designs):\n    \"\"\"\n    Super class for factorial designs.\n\n    Attributes:\n        k (int): The number of factors.\n        seed (int): The seed for the random number generator.\n    \"\"\"\n\n    def __init__(self, k: int = 2, seed: int = 123) -&gt; None:\n        \"\"\"\n        Initializes a factorial design object.\n\n        Args:\n            k (int): The number of factors. Defaults to 2.\n            seed (int): The seed for the random number generator. Defaults to 123.\n        \"\"\"\n        super().__init__(k, seed)\n\n    def full_factorial(self, p: int) -&gt; \"np.ndarray\":\n        \"\"\"\n        Generates a full factorial design.\n\n        Args:\n            p (int): The number of levels for each factor.\n\n        Returns:\n            numpy.ndarray: A 2D array representing the full factorial design.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.design.factorial import factorial\n                factorial_design = factorial(k=2)\n                factorial_design.full_factorial(p=2)\n                array([[0., 0.],\n                    [0., 1.],\n                    [1., 0.],\n                    [1., 1.]])\n        \"\"\"\n        i = (slice(0, 1, p * 1j),) * self.k\n        return mgrid[i].reshape(self.k, p**self.k).T\n</code></pre>"},{"location":"reference/spotpython/design/factorial/#spotpython.design.factorial.factorial.__init__","title":"<code>__init__(k=2, seed=123)</code>","text":"<p>Initializes a factorial design object.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of factors. Defaults to 2.</p> <code>2</code> <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 123.</p> <code>123</code> Source code in <code>spotpython/design/factorial.py</code> <pre><code>def __init__(self, k: int = 2, seed: int = 123) -&gt; None:\n    \"\"\"\n    Initializes a factorial design object.\n\n    Args:\n        k (int): The number of factors. Defaults to 2.\n        seed (int): The seed for the random number generator. Defaults to 123.\n    \"\"\"\n    super().__init__(k, seed)\n</code></pre>"},{"location":"reference/spotpython/design/factorial/#spotpython.design.factorial.factorial.full_factorial","title":"<code>full_factorial(p)</code>","text":"<p>Generates a full factorial design.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>int</code> <p>The number of levels for each factor.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: A 2D array representing the full factorial design.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.design.factorial import factorial\n    factorial_design = factorial(k=2)\n    factorial_design.full_factorial(p=2)\n    array([[0., 0.],\n        [0., 1.],\n        [1., 0.],\n        [1., 1.]])\n</code></pre> Source code in <code>spotpython/design/factorial.py</code> <pre><code>def full_factorial(self, p: int) -&gt; \"np.ndarray\":\n    \"\"\"\n    Generates a full factorial design.\n\n    Args:\n        p (int): The number of levels for each factor.\n\n    Returns:\n        numpy.ndarray: A 2D array representing the full factorial design.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.design.factorial import factorial\n            factorial_design = factorial(k=2)\n            factorial_design.full_factorial(p=2)\n            array([[0., 0.],\n                [0., 1.],\n                [1., 0.],\n                [1., 1.]])\n    \"\"\"\n    i = (slice(0, 1, p * 1j),) * self.k\n    return mgrid[i].reshape(self.k, p**self.k).T\n</code></pre>"},{"location":"reference/spotpython/design/spacefilling/","title":"spacefilling","text":""},{"location":"reference/spotpython/design/spacefilling/#spotpython.design.spacefilling.SpaceFilling","title":"<code>SpaceFilling</code>","text":"<p>               Bases: <code>Designs</code></p> <p>A class for generating space-filling designs using Latin Hypercube Sampling.</p> Source code in <code>spotpython/design/spacefilling.py</code> <pre><code>class SpaceFilling(Designs):\n    \"\"\"\n    A class for generating space-filling designs using Latin Hypercube Sampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        k: int,\n        scramble: bool = True,\n        strength: int = 1,\n        optimization: Optional[Union[None, str]] = None,\n        seed: int = 123,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a SpaceFilling design class.\n        Based on scipy.stats.qmc's LatinHypercube method.\n\n        Args:\n            k (int):\n                Dimension of the parameter space.\n            scramble (bool, optional):\n                When False, center samples within cells of a multi-dimensional grid.\n                Otherwise, samples are randomly placed within cells of the grid.\n                Note:\n                    Setting `scramble=False` does not ensure deterministic output. For that, use the `seed` parameter.\n                Default is True.\n            optimization (Optional[Union[None, str]]):\n                Whether to use an optimization scheme to improve the quality after sampling.\n                Note that this is a post-processing step that does not guarantee that all\n                properties of the sample will be conserved.\n                Defaults to None.\n                Options:\n                    - \"random-cd\": Random permutations of coordinates to lower the centered discrepancy. The best sample based on the centered discrepancy is constantly updated.\n                    Centered discrepancy-based sampling shows better space-filling\n                    robustness toward 2D and 3D subprojections compared to using other discrepancy measures.\n                    - \"lloyd\": Perturb samples using a modified Lloyd-Max algorithm. The process converges to equally spaced samples.\n            strength (Optional[int]):\n                Strength of the LHS. `strength=1` produces a plain LHS while `strength=2` produces an orthogonal array based LHS of strength 2.\n                In that case, only `n=p**2` points can be sampled, with `p` a prime number.\n                It also constrains `d &lt;= p + 1`.\n                Defaults to 1.\n            seed (int, optional):\n                Seed for the random number generator. Defaults to 123.\n        \"\"\"\n        super().__init__(k=k, seed=seed)\n        self.sampler = LatinHypercube(d=self.k, scramble=scramble, strength=strength, optimization=optimization, seed=seed)\n\n    def scipy_lhd(\n        self,\n        n: int,\n        repeats: int = 1,\n        lower: Optional[Union[int, float, np.ndarray]] = None,\n        upper: Optional[Union[int, float, np.ndarray]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Latin hypercube sampling based on scipy.\n\n        Args:\n            n (int): Number of samples.\n            repeats (int): Number of repeats (replicates).\n            lower (int, float, or np.ndarray, optional): Lower bound. Defaults to a zero vector.\n            upper (int, float, or np.ndarray, optional): Upper bound. Defaults to a one vector.\n\n        Returns:\n            np.ndarray: Latin hypercube design with specified dimensions and boundaries.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.design.spacefilling import SpaceFilling\n            &gt;&gt;&gt; lhd = SpaceFilling(k=2, seed=123)\n            &gt;&gt;&gt; lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0, 0]), upper=np.array([1, 1]))\n            array([[0.66352963, 0.5892358 ],\n                   [0.66352963, 0.5892358 ],\n                   [0.55592803, 0.96312564],\n                   [0.55592803, 0.96312564],\n                   [0.16481882, 0.0375811 ],\n                   [0.16481882, 0.0375811 ],\n                   [0.215331  , 0.34468512],\n                   [0.215331  , 0.34468512],\n                   [0.83604909, 0.62202146],\n                   [0.83604909, 0.62202146]])\n        \"\"\"\n        if lower is None:\n            lower = np.zeros(self.k)\n        if upper is None:\n            upper = np.ones(self.k)\n\n        sample = self.sampler.random(n=n)\n        des = scale(sample, lower, upper)\n        return np.repeat(des, repeats, axis=0)\n</code></pre>"},{"location":"reference/spotpython/design/spacefilling/#spotpython.design.spacefilling.SpaceFilling.__init__","title":"<code>__init__(k, scramble=True, strength=1, optimization=None, seed=123)</code>","text":"<p>Initializes a SpaceFilling design class. Based on scipy.stats.qmc\u2019s LatinHypercube method.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Dimension of the parameter space.</p> required <code>scramble</code> <code>bool</code> <p>When False, center samples within cells of a multi-dimensional grid. Otherwise, samples are randomly placed within cells of the grid. Note:     Setting <code>scramble=False</code> does not ensure deterministic output. For that, use the <code>seed</code> parameter. Default is True.</p> <code>True</code> <code>optimization</code> <code>Optional[Union[None, str]]</code> <p>Whether to use an optimization scheme to improve the quality after sampling. Note that this is a post-processing step that does not guarantee that all properties of the sample will be conserved. Defaults to None. Options:     - \u201crandom-cd\u201d: Random permutations of coordinates to lower the centered discrepancy. The best sample based on the centered discrepancy is constantly updated.     Centered discrepancy-based sampling shows better space-filling     robustness toward 2D and 3D subprojections compared to using other discrepancy measures.     - \u201clloyd\u201d: Perturb samples using a modified Lloyd-Max algorithm. The process converges to equally spaced samples.</p> <code>None</code> <code>strength</code> <code>Optional[int]</code> <p>Strength of the LHS. <code>strength=1</code> produces a plain LHS while <code>strength=2</code> produces an orthogonal array based LHS of strength 2. In that case, only <code>n=p**2</code> points can be sampled, with <code>p</code> a prime number. It also constrains <code>d &lt;= p + 1</code>. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>int</code> <p>Seed for the random number generator. Defaults to 123.</p> <code>123</code> Source code in <code>spotpython/design/spacefilling.py</code> <pre><code>def __init__(\n    self,\n    k: int,\n    scramble: bool = True,\n    strength: int = 1,\n    optimization: Optional[Union[None, str]] = None,\n    seed: int = 123,\n) -&gt; None:\n    \"\"\"\n    Initializes a SpaceFilling design class.\n    Based on scipy.stats.qmc's LatinHypercube method.\n\n    Args:\n        k (int):\n            Dimension of the parameter space.\n        scramble (bool, optional):\n            When False, center samples within cells of a multi-dimensional grid.\n            Otherwise, samples are randomly placed within cells of the grid.\n            Note:\n                Setting `scramble=False` does not ensure deterministic output. For that, use the `seed` parameter.\n            Default is True.\n        optimization (Optional[Union[None, str]]):\n            Whether to use an optimization scheme to improve the quality after sampling.\n            Note that this is a post-processing step that does not guarantee that all\n            properties of the sample will be conserved.\n            Defaults to None.\n            Options:\n                - \"random-cd\": Random permutations of coordinates to lower the centered discrepancy. The best sample based on the centered discrepancy is constantly updated.\n                Centered discrepancy-based sampling shows better space-filling\n                robustness toward 2D and 3D subprojections compared to using other discrepancy measures.\n                - \"lloyd\": Perturb samples using a modified Lloyd-Max algorithm. The process converges to equally spaced samples.\n        strength (Optional[int]):\n            Strength of the LHS. `strength=1` produces a plain LHS while `strength=2` produces an orthogonal array based LHS of strength 2.\n            In that case, only `n=p**2` points can be sampled, with `p` a prime number.\n            It also constrains `d &lt;= p + 1`.\n            Defaults to 1.\n        seed (int, optional):\n            Seed for the random number generator. Defaults to 123.\n    \"\"\"\n    super().__init__(k=k, seed=seed)\n    self.sampler = LatinHypercube(d=self.k, scramble=scramble, strength=strength, optimization=optimization, seed=seed)\n</code></pre>"},{"location":"reference/spotpython/design/spacefilling/#spotpython.design.spacefilling.SpaceFilling.scipy_lhd","title":"<code>scipy_lhd(n, repeats=1, lower=None, upper=None)</code>","text":"<p>Latin hypercube sampling based on scipy.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples.</p> required <code>repeats</code> <code>int</code> <p>Number of repeats (replicates).</p> <code>1</code> <code>lower</code> <code>int, float, or np.ndarray</code> <p>Lower bound. Defaults to a zero vector.</p> <code>None</code> <code>upper</code> <code>int, float, or np.ndarray</code> <p>Upper bound. Defaults to a one vector.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Latin hypercube design with specified dimensions and boundaries.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.design.spacefilling import SpaceFilling\n&gt;&gt;&gt; lhd = SpaceFilling(k=2, seed=123)\n&gt;&gt;&gt; lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0, 0]), upper=np.array([1, 1]))\narray([[0.66352963, 0.5892358 ],\n       [0.66352963, 0.5892358 ],\n       [0.55592803, 0.96312564],\n       [0.55592803, 0.96312564],\n       [0.16481882, 0.0375811 ],\n       [0.16481882, 0.0375811 ],\n       [0.215331  , 0.34468512],\n       [0.215331  , 0.34468512],\n       [0.83604909, 0.62202146],\n       [0.83604909, 0.62202146]])\n</code></pre> Source code in <code>spotpython/design/spacefilling.py</code> <pre><code>def scipy_lhd(\n    self,\n    n: int,\n    repeats: int = 1,\n    lower: Optional[Union[int, float, np.ndarray]] = None,\n    upper: Optional[Union[int, float, np.ndarray]] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Latin hypercube sampling based on scipy.\n\n    Args:\n        n (int): Number of samples.\n        repeats (int): Number of repeats (replicates).\n        lower (int, float, or np.ndarray, optional): Lower bound. Defaults to a zero vector.\n        upper (int, float, or np.ndarray, optional): Upper bound. Defaults to a one vector.\n\n    Returns:\n        np.ndarray: Latin hypercube design with specified dimensions and boundaries.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.design.spacefilling import SpaceFilling\n        &gt;&gt;&gt; lhd = SpaceFilling(k=2, seed=123)\n        &gt;&gt;&gt; lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0, 0]), upper=np.array([1, 1]))\n        array([[0.66352963, 0.5892358 ],\n               [0.66352963, 0.5892358 ],\n               [0.55592803, 0.96312564],\n               [0.55592803, 0.96312564],\n               [0.16481882, 0.0375811 ],\n               [0.16481882, 0.0375811 ],\n               [0.215331  , 0.34468512],\n               [0.215331  , 0.34468512],\n               [0.83604909, 0.62202146],\n               [0.83604909, 0.62202146]])\n    \"\"\"\n    if lower is None:\n        lower = np.zeros(self.k)\n    if upper is None:\n        upper = np.ones(self.k)\n\n    sample = self.sampler.random(n=n)\n    des = scale(sample, lower, upper)\n    return np.repeat(des, repeats, axis=0)\n</code></pre>"},{"location":"reference/spotpython/design/utils/","title":"utils","text":""},{"location":"reference/spotpython/design/utils/#spotpython.design.utils.generate_search_grid","title":"<code>generate_search_grid(x_min, x_max, n_points=5, col_names=None)</code>","text":"<p>Generates a search grid based on the minimum and maximum values of each feature.</p> <p>Parameters:</p> Name Type Description Default <code>x_min</code> <code>ndarray</code> <p>A NumPy array containing the minimum values for each feature.</p> required <code>x_max</code> <code>ndarray</code> <p>A NumPy array containing the maximum values for each feature.</p> required <code>n_points</code> <code>int</code> <p>The number of points to generate for each feature. Defaults to 5.</p> <code>5</code> <code>col_names</code> <code>list</code> <p>A list of column names for the DataFrame. If None, a NumPy array is returned. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataFrame, ndarray]</code> <p>Union[pd.DataFrame, np.ndarray]: A Pandas DataFrame representing the search grid if col_names is provided, otherwise a NumPy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the length of x_min and x_max are different.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.design.utils import generate_search_grid\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x_min = np.array([0, 0, 0])\n&gt;&gt;&gt; x_max = np.array([1, 1, 1])\n&gt;&gt;&gt; search_grid = generate_search_grid(x_min, x_max, num_points=3)\n&gt;&gt;&gt; print(search_grid)\n[[0.  0.  0. ]\n [0.  0.  0.5]\n [0.  0.  1. ]\n ...\n [1.  1.  0.5]\n [1.  1.  1. ]]\n</code></pre> <pre><code>&gt;&gt;&gt; search_grid = generate_search_grid(x_min, x_max, num_points=3, col_names=['feature_0', 'feature_1', 'feature_2'])\n&gt;&gt;&gt; print(search_grid)\n   feature_0  feature_1  feature_2\n0        0.0      0.00      0.00\n1        0.0      0.00      0.50\n2        0.0      0.00      1.00\n3        0.0      0.50      0.00\n4        0.0      0.50      0.50\n..       ...      ...      ...\n22       1.0      1.00      0.50\n23       1.0      1.00      1.00\n</code></pre> <p>[27 rows x 3 columns]</p> Source code in <code>spotpython/design/utils.py</code> <pre><code>def generate_search_grid(x_min: np.ndarray, x_max: np.ndarray, n_points: int = 5, col_names: list = None) -&gt; Union[pd.DataFrame, np.ndarray]:\n    \"\"\"\n    Generates a search grid based on the minimum and maximum values of each feature.\n\n    Args:\n        x_min (np.ndarray): A NumPy array containing the minimum values for each feature.\n        x_max (np.ndarray): A NumPy array containing the maximum values for each feature.\n        n_points (int, optional): The number of points to generate for each feature. Defaults to 5.\n        col_names (list, optional): A list of column names for the DataFrame. If None, a NumPy array is returned. Defaults to None.\n\n    Returns:\n        Union[pd.DataFrame, np.ndarray]: A Pandas DataFrame representing the search grid if col_names is provided,\n            otherwise a NumPy array.\n\n    Raises:\n        ValueError: If the length of x_min and x_max are different.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.design.utils import generate_search_grid\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; x_min = np.array([0, 0, 0])\n        &gt;&gt;&gt; x_max = np.array([1, 1, 1])\n        &gt;&gt;&gt; search_grid = generate_search_grid(x_min, x_max, num_points=3)\n        &gt;&gt;&gt; print(search_grid)\n        [[0.  0.  0. ]\n         [0.  0.  0.5]\n         [0.  0.  1. ]\n         ...\n         [1.  1.  0.5]\n         [1.  1.  1. ]]\n\n        &gt;&gt;&gt; search_grid = generate_search_grid(x_min, x_max, num_points=3, col_names=['feature_0', 'feature_1', 'feature_2'])\n        &gt;&gt;&gt; print(search_grid)\n           feature_0  feature_1  feature_2\n        0        0.0      0.00      0.00\n        1        0.0      0.00      0.50\n        2        0.0      0.00      1.00\n        3        0.0      0.50      0.00\n        4        0.0      0.50      0.50\n        ..       ...      ...      ...\n        22       1.0      1.00      0.50\n        23       1.0      1.00      1.00\n\n        [27 rows x 3 columns]\n    \"\"\"\n    if len(x_min) != len(x_max):\n        raise ValueError(\"x_min and x_max must have the same length.\")\n\n    num_features = len(x_min)\n    # Create linspace for each dimension\n    ranges = [np.linspace(x_min[i], x_max[i], n_points) for i in range(num_features)]\n\n    # Use meshgrid to create all combinations\n    # The maximum number of inputs for np.broadcast is 32\n    if num_features &gt; 30:\n        raise ValueError(\"Too many features for meshgrid. Maximum 30 features are supported.\")\n    mesh = np.meshgrid(*ranges, indexing=\"ij\")\n\n    # Reshape the meshgrid output to a list of points\n    points = np.array([m.ravel() for m in mesh]).T\n\n    if col_names:\n        # Create a Pandas DataFrame from the points\n        if len(col_names) != num_features:\n            raise ValueError(\"The number of column names must match the number of features.\")\n        search_grid = pd.DataFrame(points, columns=col_names)\n        return search_grid\n    else:\n        return points\n</code></pre>"},{"location":"reference/spotpython/design/utils/#spotpython.design.utils.get_boundaries","title":"<code>get_boundaries(data)</code>","text":"<p>Calculates the minimum and maximum values for each column in a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A NumPy array of shape (n, k), where n is the number of rows and k is the number of columns.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>tuple[np.ndarray, np.ndarray]: A tuple containing two NumPy arrays: - The first array contains the minimum values for each column, with shape (k,). - The second array contains the maximum values for each column, with shape (k,).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input array has shape (1, 0) (empty array).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.design.utils import get_boundaries\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; min_values, max_values = get_boundaries(data)\n&gt;&gt;&gt; print(\"Minimum values:\", min_values)\nMinimum values: [1 2 3]\n&gt;&gt;&gt; print(\"Maximum values:\", max_values)\nMaximum values: [7 8 9]\n</code></pre> Source code in <code>spotpython/design/utils.py</code> <pre><code>def get_boundaries(data: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculates the minimum and maximum values for each column in a NumPy array.\n\n    Args:\n        data (np.ndarray): A NumPy array of shape (n, k), where n is the number of rows\n            and k is the number of columns.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing two NumPy arrays:\n            - The first array contains the minimum values for each column, with shape (k,).\n            - The second array contains the maximum values for each column, with shape (k,).\n\n    Raises:\n        ValueError: If the input array has shape (1, 0) (empty array).\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.design.utils import get_boundaries\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        &gt;&gt;&gt; min_values, max_values = get_boundaries(data)\n        &gt;&gt;&gt; print(\"Minimum values:\", min_values)\n        Minimum values: [1 2 3]\n        &gt;&gt;&gt; print(\"Maximum values:\", max_values)\n        Maximum values: [7 8 9]\n    \"\"\"\n    if data.size == 0:\n        raise ValueError(\"Input array cannot be empty.\")\n    min_values = np.min(data, axis=0)\n    max_values = np.max(data, axis=0)\n    return min_values, max_values\n</code></pre>"},{"location":"reference/spotpython/design/utils/#spotpython.design.utils.map_to_original_scale","title":"<code>map_to_original_scale(X_search, x_min, x_max)</code>","text":"<p>Maps the values in X_search from the range [0, 1] to the original scale defined by x_min and x_max.</p> <p>Parameters:</p> Name Type Description Default <code>X_search</code> <code>Union[DataFrame, ndarray]</code> <p>A Pandas DataFrame or NumPy array containing the search points in the range [0, 1].</p> required <code>x_min</code> <code>ndarray</code> <p>A NumPy array containing the minimum values for each feature in the original scale.</p> required <code>x_max</code> <code>ndarray</code> <p>A NumPy array containing the maximum values for each feature in the original scale.</p> required <p>Returns:</p> Type Description <code>Union[DataFrame, ndarray]</code> <p>Union[pd.DataFrame, np.ndarray]: A Pandas DataFrame or NumPy array with the values mapped to the original scale.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.design.utils import map_to_original_scale\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; X_search = pd.DataFrame([[0.5, 0.5], [0.25, 0.75]], columns=['x', 'y'])\n&gt;&gt;&gt; x_min = np.array([0, 0])\n&gt;&gt;&gt; x_max = np.array([10, 20])\n&gt;&gt;&gt; X_search_scaled = map_to_original_scale(X_search, x_min, x_max)\n&gt;&gt;&gt; print(X_search_scaled)\n      x     y\n0   5.0  10.0\n1   2.5  15.0\n</code></pre> Source code in <code>spotpython/design/utils.py</code> <pre><code>def map_to_original_scale(X_search: Union[pd.DataFrame, np.ndarray], x_min: np.ndarray, x_max: np.ndarray) -&gt; Union[pd.DataFrame, np.ndarray]:\n    \"\"\"\n    Maps the values in X_search from the range [0, 1] to the original scale defined by x_min and x_max.\n\n    Args:\n        X_search (Union[pd.DataFrame, np.ndarray]): A Pandas DataFrame or NumPy array containing the search points in the range [0, 1].\n        x_min (np.ndarray): A NumPy array containing the minimum values for each feature in the original scale.\n        x_max (np.ndarray): A NumPy array containing the maximum values for each feature in the original scale.\n\n    Returns:\n        Union[pd.DataFrame, np.ndarray]: A Pandas DataFrame or NumPy array with the values mapped to the original scale.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.design.utils import map_to_original_scale\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; X_search = pd.DataFrame([[0.5, 0.5], [0.25, 0.75]], columns=['x', 'y'])\n        &gt;&gt;&gt; x_min = np.array([0, 0])\n        &gt;&gt;&gt; x_max = np.array([10, 20])\n        &gt;&gt;&gt; X_search_scaled = map_to_original_scale(X_search, x_min, x_max)\n        &gt;&gt;&gt; print(X_search_scaled)\n              x     y\n        0   5.0  10.0\n        1   2.5  15.0\n    \"\"\"\n    if not isinstance(X_search, (pd.DataFrame, np.ndarray)):\n        raise TypeError(\"X_search must be a Pandas DataFrame or a NumPy array.\")\n\n    # if x_min or x_max are not numpy arrays, convert them to numpy arrays\n    if not isinstance(x_min, np.ndarray):\n        x_min = np.array(x_min)\n    if not isinstance(x_max, np.ndarray):\n        x_max = np.array(x_max)\n\n    if len(x_min) != X_search.shape[1]:\n        raise IndexError(f\"x_min and X_search must have the same number of columns. x_min has {len(x_min)} columns and X_search has {X_search.shape[1]} columns.\")\n    if len(x_max) != X_search.shape[1]:\n        raise IndexError(f\"x_max and X_search must have the same number of columns. x_max has {len(x_max)} columns and X_search has {X_search.shape[1]} columns.\")\n\n    if isinstance(X_search, pd.DataFrame):\n        X_search_scaled = X_search.copy()  # Create a copy to avoid modifying the original DataFrame\n        for i, col in enumerate(X_search.columns):\n            X_search_scaled.loc[:, col] = X_search[col] * (x_max[i] - x_min[i]) + x_min[i]\n        return X_search_scaled\n    elif isinstance(X_search, np.ndarray):\n        X_search_scaled = X_search.copy()  # Create a copy to avoid modifying the original array\n        for i in range(X_search.shape[1]):\n            X_search_scaled[:, i] = X_search[:, i] * (x_max[i] - x_min[i]) + x_min[i]\n        return X_search_scaled\n</code></pre>"},{"location":"reference/spotpython/fun/hyperlight/","title":"hyperlight","text":""},{"location":"reference/spotpython/fun/hyperlight/#spotpython.fun.hyperlight.HyperLight","title":"<code>HyperLight</code>","text":"<p>Hyperparameter Tuning for Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for the random number generator. See Numpy Random Sampling.</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for the random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n&gt;&gt;&gt; print(hyper_light.seed)\n    126\n</code></pre> Source code in <code>spotpython/fun/hyperlight.py</code> <pre><code>class HyperLight:\n    \"\"\"\n    Hyperparameter Tuning for Lightning.\n\n    Args:\n        seed (int): seed for the random number generator. See Numpy Random Sampling.\n        log_level (int): log level for the logger.\n\n    Attributes:\n        seed (int): seed for the random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the hyperparameter tuning.\n        log_level (int): log level for the logger.\n\n    Examples:\n        &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n        &gt;&gt;&gt; print(hyper_light.seed)\n            126\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50) -&gt; None:\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.log_level = log_level\n        logger.setLevel(log_level)\n        logger.info(f\"Starting the logger at level {log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray, fun_control: dict) -&gt; np.ndarray:\n        \"\"\"\n        Checks the shape of the input array X and raises an exception if it is not valid.\n\n        Args:\n            X (np.ndarray):\n                input array.\n            fun_control (dict):\n                dictionary containing control parameters for the hyperparameter tuning.\n\n        Returns:\n            np.ndarray:\n                input array with valid shape.\n\n        Raises:\n            Exception:\n                if the shape of the input array is not valid.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.utils.init import fun_control_init\n                from spotpython.light.regression.netlightregression import NetLightRegression\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.hyperparameters.values import add_core_model_to_fun_control\n                from spotpython.fun.hyperlight import HyperLight\n                from spotpython.hyperparameters.values import get_var_name\n                fun_control = fun_control_init()\n                add_core_model_to_fun_control(core_model=NetLightRegression,\n                                            fun_control=fun_control,\n                                            hyper_dict=LightHyperDict)\n                hyper_light = HyperLight(seed=126, log_level=50)\n                n_hyperparams = len(get_var_name(fun_control))\n                # generate a random np.array X with shape (2, n_hyperparams)\n                X = np.random.rand(2, n_hyperparams)\n                X == hyper_light.check_X_shape(X, fun_control)\n                array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n                [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(get_var_name(fun_control)):\n            raise Exception(\"Invalid shape of input array X.\")\n        return X\n\n    def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluates the function for the given input array X of shape (n,k)\n        and control parameters specified as a dict.\n        Calls the train_model function from spotpython.light.trainmodel\n        to train the model and evaluate the results.\n\n        Args:\n            X (np.ndarray):\n                input array of shape (n, k) where n is the number of configurations evaluated\n                and k is the number of hyperparameters.\n            fun_control (dict):\n                dictionary containing control parameters for the hyperparameter tuning.\n\n        Returns:\n            (np.ndarray):\n                (n,) array containing the `n` evaluation results.\n\n        Examples:\n            &gt;&gt;&gt; from math import inf\n                import numpy as np\n                from spotpython.data.diabetes import Diabetes\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.fun.hyperlight import HyperLight\n                from spotpython.utils.init import fun_control_init\n                from spotpython.utils.eda import print_exp_table\n                from spotpython.spot import spot\n                from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n                PREFIX=\"000\"\n                data_set = Diabetes()\n                fun_control = fun_control_init(\n                    PREFIX=PREFIX,\n                    save_experiment=True,\n                    fun_evals=inf,\n                    max_time=1,\n                    data_set = data_set,\n                    core_model_name=\"light.regression.NNLinearRegressor\",\n                    hyperdict=LightHyperDict,\n                    _L_in=10,\n                    _L_out=1,\n                    TENSORBOARD_CLEAN=True,\n                    tensorboard_log=True,\n                    seed=42,)\n                print_exp_table(fun_control)\n                X = get_default_hyperparameters_as_array(fun_control)\n                # set epochs to 2^8:\n                X[0, 1] = 8\n                # set patience to 2^10:\n                X[0, 7] = 10\n                print(f\"X: {X}\")\n                # combine X and X to a np.array with shape (2, n_hyperparams)\n                # so that two values are returned\n                X = np.vstack((X, X))\n                hyper_light = HyperLight(seed=125, log_level=50)\n                hyper_light.fun(X, fun_control)\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        self.check_X_shape(X=X, fun_control=fun_control)\n        var_dict = assign_values(X, get_var_name(fun_control))\n        # type information and transformations are considered in generate_one_config_from_var_dict:\n        for config in generate_one_config_from_var_dict(var_dict, fun_control):\n            if fun_control[\"show_config\"]:\n                print(\"\\nIn fun(): config:\")\n                pprint.pprint(config)\n            logger.debug(f\"\\nconfig: {config}\")\n            # extract parameters like epochs, batch_size, lr, etc. from config\n            # config_id = generate_config_id(config)\n            try:\n                logger.debug(\"fun: Calling train_model\")\n                df_eval = train_model(config, fun_control)\n                logger.debug(\"fun: train_model returned\")\n            except Exception as err:\n                if fun_control[\"verbosity\"] &gt; 0:\n                    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n                    if fun_control[\"verbosity\"] &gt; 1:\n                        pprint.pprint(fun_control)\n                    print(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                    print(\"Setting df_eval to np.nan\\n\")\n                logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                logger.error(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            # Multiply results by the weights. Positive weights mean that the result is to be minimized.\n            # Negative weights mean that the result is to be maximized, e.g., accuracy.\n            z_val = fun_control[\"weights\"] * df_eval\n            # Append, since several configurations can be evaluated at once.\n            z_res = np.append(z_res, z_val)\n        # Finally, z_res is a 1-dim array\n        # of shape (n,) where n is the number of configurations evaluated.\n        return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/hyperlight/#spotpython.fun.hyperlight.HyperLight.check_X_shape","title":"<code>check_X_shape(X, fun_control)</code>","text":"<p>Checks the shape of the input array X and raises an exception if it is not valid.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: input array with valid shape.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if the shape of the input array is not valid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.init import fun_control_init\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    from spotpython.fun.hyperlight import HyperLight\n    from spotpython.hyperparameters.values import get_var_name\n    fun_control = fun_control_init()\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n    hyper_light = HyperLight(seed=126, log_level=50)\n    n_hyperparams = len(get_var_name(fun_control))\n    # generate a random np.array X with shape (2, n_hyperparams)\n    X = np.random.rand(2, n_hyperparams)\n    X == hyper_light.check_X_shape(X, fun_control)\n    array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n    [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n</code></pre> Source code in <code>spotpython/fun/hyperlight.py</code> <pre><code>def check_X_shape(self, X: np.ndarray, fun_control: dict) -&gt; np.ndarray:\n    \"\"\"\n    Checks the shape of the input array X and raises an exception if it is not valid.\n\n    Args:\n        X (np.ndarray):\n            input array.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        np.ndarray:\n            input array with valid shape.\n\n    Raises:\n        Exception:\n            if the shape of the input array is not valid.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.init import fun_control_init\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            from spotpython.fun.hyperlight import HyperLight\n            from spotpython.hyperparameters.values import get_var_name\n            fun_control = fun_control_init()\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n            hyper_light = HyperLight(seed=126, log_level=50)\n            n_hyperparams = len(get_var_name(fun_control))\n            # generate a random np.array X with shape (2, n_hyperparams)\n            X = np.random.rand(2, n_hyperparams)\n            X == hyper_light.check_X_shape(X, fun_control)\n            array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n            [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(get_var_name(fun_control)):\n        raise Exception(\"Invalid shape of input array X.\")\n    return X\n</code></pre>"},{"location":"reference/spotpython/fun/hyperlight/#spotpython.fun.hyperlight.HyperLight.fun","title":"<code>fun(X, fun_control=None)</code>","text":"<p>Evaluates the function for the given input array X of shape (n,k) and control parameters specified as a dict. Calls the train_model function from spotpython.light.trainmodel to train the model and evaluate the results.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array of shape (n, k) where n is the number of configurations evaluated and k is the number of hyperparameters.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>(n,) array containing the <code>n</code> evaluation results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from math import inf\n    import numpy as np\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.fun.hyperlight import HyperLight\n    from spotpython.utils.init import fun_control_init\n    from spotpython.utils.eda import print_exp_table\n    from spotpython.spot import spot\n    from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n    PREFIX=\"000\"\n    data_set = Diabetes()\n    fun_control = fun_control_init(\n        PREFIX=PREFIX,\n        save_experiment=True,\n        fun_evals=inf,\n        max_time=1,\n        data_set = data_set,\n        core_model_name=\"light.regression.NNLinearRegressor\",\n        hyperdict=LightHyperDict,\n        _L_in=10,\n        _L_out=1,\n        TENSORBOARD_CLEAN=True,\n        tensorboard_log=True,\n        seed=42,)\n    print_exp_table(fun_control)\n    X = get_default_hyperparameters_as_array(fun_control)\n    # set epochs to 2^8:\n    X[0, 1] = 8\n    # set patience to 2^10:\n    X[0, 7] = 10\n    print(f\"X: {X}\")\n    # combine X and X to a np.array with shape (2, n_hyperparams)\n    # so that two values are returned\n    X = np.vstack((X, X))\n    hyper_light = HyperLight(seed=125, log_level=50)\n    hyper_light.fun(X, fun_control)\n</code></pre> Source code in <code>spotpython/fun/hyperlight.py</code> <pre><code>def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates the function for the given input array X of shape (n,k)\n    and control parameters specified as a dict.\n    Calls the train_model function from spotpython.light.trainmodel\n    to train the model and evaluate the results.\n\n    Args:\n        X (np.ndarray):\n            input array of shape (n, k) where n is the number of configurations evaluated\n            and k is the number of hyperparameters.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        (np.ndarray):\n            (n,) array containing the `n` evaluation results.\n\n    Examples:\n        &gt;&gt;&gt; from math import inf\n            import numpy as np\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.fun.hyperlight import HyperLight\n            from spotpython.utils.init import fun_control_init\n            from spotpython.utils.eda import print_exp_table\n            from spotpython.spot import spot\n            from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n            PREFIX=\"000\"\n            data_set = Diabetes()\n            fun_control = fun_control_init(\n                PREFIX=PREFIX,\n                save_experiment=True,\n                fun_evals=inf,\n                max_time=1,\n                data_set = data_set,\n                core_model_name=\"light.regression.NNLinearRegressor\",\n                hyperdict=LightHyperDict,\n                _L_in=10,\n                _L_out=1,\n                TENSORBOARD_CLEAN=True,\n                tensorboard_log=True,\n                seed=42,)\n            print_exp_table(fun_control)\n            X = get_default_hyperparameters_as_array(fun_control)\n            # set epochs to 2^8:\n            X[0, 1] = 8\n            # set patience to 2^10:\n            X[0, 7] = 10\n            print(f\"X: {X}\")\n            # combine X and X to a np.array with shape (2, n_hyperparams)\n            # so that two values are returned\n            X = np.vstack((X, X))\n            hyper_light = HyperLight(seed=125, log_level=50)\n            hyper_light.fun(X, fun_control)\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    self.check_X_shape(X=X, fun_control=fun_control)\n    var_dict = assign_values(X, get_var_name(fun_control))\n    # type information and transformations are considered in generate_one_config_from_var_dict:\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        if fun_control[\"show_config\"]:\n            print(\"\\nIn fun(): config:\")\n            pprint.pprint(config)\n        logger.debug(f\"\\nconfig: {config}\")\n        # extract parameters like epochs, batch_size, lr, etc. from config\n        # config_id = generate_config_id(config)\n        try:\n            logger.debug(\"fun: Calling train_model\")\n            df_eval = train_model(config, fun_control)\n            logger.debug(\"fun: train_model returned\")\n        except Exception as err:\n            if fun_control[\"verbosity\"] &gt; 0:\n                print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n                if fun_control[\"verbosity\"] &gt; 1:\n                    pprint.pprint(fun_control)\n                print(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                print(\"Setting df_eval to np.nan\\n\")\n            logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n            logger.error(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        # Multiply results by the weights. Positive weights mean that the result is to be minimized.\n        # Negative weights mean that the result is to be maximized, e.g., accuracy.\n        z_val = fun_control[\"weights\"] * df_eval\n        # Append, since several configurations can be evaluated at once.\n        z_res = np.append(z_res, z_val)\n    # Finally, z_res is a 1-dim array\n    # of shape (n,) where n is the number of configurations evaluated.\n    return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/hypersklearn/","title":"hypersklearn","text":""},{"location":"reference/spotpython/fun/hypersklearn/#spotpython.fun.hypersklearn.HyperSklearn","title":"<code>HyperSklearn</code>","text":"<p>Hyperparameter Tuning for Sklearn.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed. See Numpy Random Sampling</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for logger. Default is 50.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> <code>log_level</code> <code>int</code> <p>log level for logger.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.hypersklearn import HyperSklearn\n&gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n&gt;&gt;&gt; print(hyper_sklearn.seed)\n126\n</code></pre> Source code in <code>spotpython/fun/hypersklearn.py</code> <pre><code>class HyperSklearn:\n    \"\"\"\n    Hyperparameter Tuning for Sklearn.\n\n    Args:\n        seed (int): seed.\n            See Numpy Random Sampling\n        log_level (int): log level for logger. Default is 50.\n\n    Attributes:\n        seed (int): seed for random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the function.\n        log_level (int): log level for logger.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.hypersklearn import HyperSklearn\n        &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n        &gt;&gt;&gt; print(hyper_sklearn.seed)\n        126\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50):\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\n            \"seed\": None,\n            \"data\": None,\n            \"step\": 10_000,\n            \"horizon\": None,\n            \"grace_period\": None,\n            \"metric_river\": None,\n            \"metric_sklearn\": mean_absolute_error,\n            \"weights\": array([1, 0, 0]),\n            \"weight_coeff\": 0.0,\n            \"log_level\": log_level,\n            \"var_name\": [],\n            \"var_type\": [],\n            \"prep_model\": None,\n            \"predict_proba\": False,\n        }\n        self.log_level = self.fun_control[\"log_level\"]\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Check the shape of the input array X.\n\n        Args:\n            X (np.ndarray): input array.\n\n        Raises:\n            Exception: if the second dimension of X does not match the length of var_name in fun_control.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.hypersklearn import HyperSklearn\n            &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n            &gt;&gt;&gt; hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"]\n            &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]]))\n            &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2]]))\n            Traceback (most recent call last):\n            ...\n            Exception\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(self.fun_control[\"var_name\"]):\n            raise Exception\n\n    def get_sklearn_df_eval_preds(self, model) -&gt; tuple:\n        \"\"\"\n        Get evaluation and prediction dataframes for a given model.\n        Args:\n            model (sklearn model): sklearn model.\n\n        Returns:\n            (tuple): tuple containing evaluation and prediction dataframes.\n\n        Raises:\n            Exception: if call to evaluate_model fails.\n\n        \"\"\"\n        try:\n            df_eval, df_preds = self.evaluate_model(model, self.fun_control)\n        except Exception as err:\n            print(f\"Error in get_sklearn_df_eval_preds(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n            print(\"Setting df_eval and df.preds to np.nan\")\n            df_eval = np.nan\n            df_preds = np.nan\n        return df_eval, df_preds\n\n    def fun_sklearn(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluate a sklearn model using hyperparameters specified in X.\n\n        Args:\n            X (np.ndarray): input array containing hyperparameters.\n            fun_control (dict): dictionary containing control parameters for the function. Default is None.\n\n        Returns:\n            (np.ndarray): array containing evaluation results.\n\n        Raises:\n            Exception: if call to evaluate_model fails.\n\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        self.fun_control.update(fun_control)\n        self.check_X_shape(X)\n        var_dict = assign_values(X, self.fun_control[\"var_name\"])\n        for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n            if self.fun_control[\"prep_model\"] is not None:\n                model = make_pipeline(self.fun_control[\"prep_model\"](), self.fun_control[\"core_model\"](**config))\n            else:\n                model = self.fun_control[\"core_model\"](**config)\n            try:\n                eval_type = fun_control[\"eval\"]\n                if eval_type == \"eval_test\":\n                    df_eval, _ = evaluate_model(model, self.fun_control)\n                elif eval_type == \"eval_oob_score\":\n                    df_eval, _ = evaluate_model_oob(model, self.fun_control)\n                elif eval_type == \"train_cv\":\n                    df_eval, _ = evaluate_cv(model, self.fun_control)\n                else:  # None or \"evaluate_hold_out\":\n                    df_eval, _ = evaluate_hold_out(model, self.fun_control)\n            except Exception as err:\n                print(f\"Error in fun_sklearn(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n                print(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            z_res = np.append(z_res, fun_control[\"weights\"] * df_eval)\n        return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/hypersklearn/#spotpython.fun.hypersklearn.HyperSklearn.check_X_shape","title":"<code>check_X_shape(X)</code>","text":"<p>Check the shape of the input array X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>if the second dimension of X does not match the length of var_name in fun_control.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.hypersklearn import HyperSklearn\n&gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n&gt;&gt;&gt; hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"]\n&gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]]))\n&gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2]]))\nTraceback (most recent call last):\n...\nException\n</code></pre> Source code in <code>spotpython/fun/hypersklearn.py</code> <pre><code>def check_X_shape(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Check the shape of the input array X.\n\n    Args:\n        X (np.ndarray): input array.\n\n    Raises:\n        Exception: if the second dimension of X does not match the length of var_name in fun_control.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.hypersklearn import HyperSklearn\n        &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n        &gt;&gt;&gt; hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"]\n        &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]]))\n        &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2]]))\n        Traceback (most recent call last):\n        ...\n        Exception\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(self.fun_control[\"var_name\"]):\n        raise Exception\n</code></pre>"},{"location":"reference/spotpython/fun/hypersklearn/#spotpython.fun.hypersklearn.HyperSklearn.fun_sklearn","title":"<code>fun_sklearn(X, fun_control=None)</code>","text":"<p>Evaluate a sklearn model using hyperparameters specified in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array containing hyperparameters.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing evaluation results.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if call to evaluate_model fails.</p> Source code in <code>spotpython/fun/hypersklearn.py</code> <pre><code>def fun_sklearn(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluate a sklearn model using hyperparameters specified in X.\n\n    Args:\n        X (np.ndarray): input array containing hyperparameters.\n        fun_control (dict): dictionary containing control parameters for the function. Default is None.\n\n    Returns:\n        (np.ndarray): array containing evaluation results.\n\n    Raises:\n        Exception: if call to evaluate_model fails.\n\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    self.fun_control.update(fun_control)\n    self.check_X_shape(X)\n    var_dict = assign_values(X, self.fun_control[\"var_name\"])\n    for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n        if self.fun_control[\"prep_model\"] is not None:\n            model = make_pipeline(self.fun_control[\"prep_model\"](), self.fun_control[\"core_model\"](**config))\n        else:\n            model = self.fun_control[\"core_model\"](**config)\n        try:\n            eval_type = fun_control[\"eval\"]\n            if eval_type == \"eval_test\":\n                df_eval, _ = evaluate_model(model, self.fun_control)\n            elif eval_type == \"eval_oob_score\":\n                df_eval, _ = evaluate_model_oob(model, self.fun_control)\n            elif eval_type == \"train_cv\":\n                df_eval, _ = evaluate_cv(model, self.fun_control)\n            else:  # None or \"evaluate_hold_out\":\n                df_eval, _ = evaluate_hold_out(model, self.fun_control)\n        except Exception as err:\n            print(f\"Error in fun_sklearn(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n            print(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        z_res = np.append(z_res, fun_control[\"weights\"] * df_eval)\n    return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/hypersklearn/#spotpython.fun.hypersklearn.HyperSklearn.get_sklearn_df_eval_preds","title":"<code>get_sklearn_df_eval_preds(model)</code>","text":"<p>Get evaluation and prediction dataframes for a given model. Args:     model (sklearn model): sklearn model.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>tuple containing evaluation and prediction dataframes.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if call to evaluate_model fails.</p> Source code in <code>spotpython/fun/hypersklearn.py</code> <pre><code>def get_sklearn_df_eval_preds(self, model) -&gt; tuple:\n    \"\"\"\n    Get evaluation and prediction dataframes for a given model.\n    Args:\n        model (sklearn model): sklearn model.\n\n    Returns:\n        (tuple): tuple containing evaluation and prediction dataframes.\n\n    Raises:\n        Exception: if call to evaluate_model fails.\n\n    \"\"\"\n    try:\n        df_eval, df_preds = self.evaluate_model(model, self.fun_control)\n    except Exception as err:\n        print(f\"Error in get_sklearn_df_eval_preds(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n        print(\"Setting df_eval and df.preds to np.nan\")\n        df_eval = np.nan\n        df_preds = np.nan\n    return df_eval, df_preds\n</code></pre>"},{"location":"reference/spotpython/fun/hypertorch/","title":"hypertorch","text":""},{"location":"reference/spotpython/fun/hypertorch/#spotpython.fun.hypertorch.HyperTorch","title":"<code>HyperTorch</code>","text":"<p>Hyperparameter Tuning for Torch.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for random number generator. See Numpy Random Sampling</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for logger. Default is 50.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> <code>log_level</code> <code>int</code> <p>log level for logger.</p> Source code in <code>spotpython/fun/hypertorch.py</code> <pre><code>class HyperTorch:\n    \"\"\"\n    Hyperparameter Tuning for Torch.\n\n    Args:\n        seed (int): seed for random number generator.\n            See Numpy Random Sampling\n        log_level (int): log level for logger. Default is 50.\n\n    Attributes:\n        seed (int): seed for random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the function.\n        log_level (int): log level for logger.\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50):\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\n            \"seed\": None,\n            \"data\": None,\n            \"step\": 10_000,\n            \"horizon\": None,\n            \"grace_period\": None,\n            \"metric_river\": None,\n            \"metric_sklearn\": None,\n            \"weights\": array([1, 0, 0]),\n            \"weight_coeff\": 0.0,\n            \"log_level\": log_level,\n            \"var_name\": [],\n            \"var_type\": [],\n        }\n        self.log_level = self.fun_control[\"log_level\"]\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Check the shape of the input array X.\n\n        Args:\n            X (np.ndarray): input array.\n\n        Raises:\n            Exception: if the second dimension of X does not match the length of var_name in fun_control.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n            &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n            &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]]))\n            &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([1, 2]))\n            Traceback (most recent call last):\n            ...\n            Exception\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(self.fun_control[\"var_name\"]):\n            raise Exception\n\n    def fun_torch(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Function to be optimized.\n\n        Args:\n            X (np.ndarray): input array.\n            fun_control (dict): dictionary containing control parameters for the function.\n        Returns:\n            np.ndarray: output array.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n            &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n            &gt;&gt;&gt; hyper_torch.fun_torch(np.array([[1, 2], [3, 4]]))\n\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        self.fun_control.update(fun_control)\n        self.check_X_shape(X)\n        var_dict = assign_values(X, self.fun_control[\"var_name\"])\n        for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n            print(f\"\\nconfig: {config}\")\n            config_id = generate_config_id(config)\n            if self.fun_control[\"prep_model\"] is not None:\n                model = make_pipeline(self.fun_control[\"prep_model\"], self.fun_control[\"core_model\"](**config))\n            else:\n                model = self.fun_control[\"core_model\"](**config)\n            try:\n                if self.fun_control[\"eval\"] == \"train_cv\":\n                    df_eval, _ = evaluate_cv(\n                        model,\n                        dataset=fun_control[\"train\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n                elif self.fun_control[\"eval\"] == \"test_cv\":\n                    df_eval, _ = evaluate_cv(\n                        model,\n                        dataset=fun_control[\"test\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n                elif self.fun_control[\"eval\"] == \"test_hold_out\":\n                    df_eval, _ = evaluate_hold_out(\n                        model,\n                        train_dataset=fun_control[\"train\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        loss_function=self.fun_control[\"loss_function\"],\n                        metric=self.fun_control[\"metric_torch\"],\n                        test_dataset=fun_control[\"test\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        path=self.fun_control[\"path\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n                else:  # eval == \"train_hold_out\"\n                    df_eval, _ = evaluate_hold_out(\n                        model,\n                        train_dataset=fun_control[\"train\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        loss_function=self.fun_control[\"loss_function\"],\n                        metric=self.fun_control[\"metric_torch\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        path=self.fun_control[\"path\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n            except Exception as err:\n                print(f\"Error in fun_torch(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n                print(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            z_val = fun_control[\"weights\"] * df_eval\n            if self.fun_control[\"spot_writer\"] is not None:\n                writer = self.fun_control[\"spot_writer\"]\n                writer.add_hparams(config, {\"fun_torch: loss\": z_val})\n                writer.flush()\n            z_res = np.append(z_res, z_val)\n        return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/hypertorch/#spotpython.fun.hypertorch.HyperTorch.check_X_shape","title":"<code>check_X_shape(X)</code>","text":"<p>Check the shape of the input array X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>if the second dimension of X does not match the length of var_name in fun_control.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n&gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n&gt;&gt;&gt; hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]]))\n&gt;&gt;&gt; hyper_torch.check_X_shape(np.array([1, 2]))\nTraceback (most recent call last):\n...\nException\n</code></pre> Source code in <code>spotpython/fun/hypertorch.py</code> <pre><code>def check_X_shape(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Check the shape of the input array X.\n\n    Args:\n        X (np.ndarray): input array.\n\n    Raises:\n        Exception: if the second dimension of X does not match the length of var_name in fun_control.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n        &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n        &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]]))\n        &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([1, 2]))\n        Traceback (most recent call last):\n        ...\n        Exception\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(self.fun_control[\"var_name\"]):\n        raise Exception\n</code></pre>"},{"location":"reference/spotpython/fun/hypertorch/#spotpython.fun.hypertorch.HyperTorch.fun_torch","title":"<code>fun_torch(X, fun_control=None)</code>","text":"<p>Function to be optimized.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> <code>None</code> <p>Returns:     np.ndarray: output array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n&gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n&gt;&gt;&gt; hyper_torch.fun_torch(np.array([[1, 2], [3, 4]]))\n</code></pre> Source code in <code>spotpython/fun/hypertorch.py</code> <pre><code>def fun_torch(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Function to be optimized.\n\n    Args:\n        X (np.ndarray): input array.\n        fun_control (dict): dictionary containing control parameters for the function.\n    Returns:\n        np.ndarray: output array.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.hypertorch import HyperTorch\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n        &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n        &gt;&gt;&gt; hyper_torch.fun_torch(np.array([[1, 2], [3, 4]]))\n\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    self.fun_control.update(fun_control)\n    self.check_X_shape(X)\n    var_dict = assign_values(X, self.fun_control[\"var_name\"])\n    for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n        print(f\"\\nconfig: {config}\")\n        config_id = generate_config_id(config)\n        if self.fun_control[\"prep_model\"] is not None:\n            model = make_pipeline(self.fun_control[\"prep_model\"], self.fun_control[\"core_model\"](**config))\n        else:\n            model = self.fun_control[\"core_model\"](**config)\n        try:\n            if self.fun_control[\"eval\"] == \"train_cv\":\n                df_eval, _ = evaluate_cv(\n                    model,\n                    dataset=fun_control[\"train\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n            elif self.fun_control[\"eval\"] == \"test_cv\":\n                df_eval, _ = evaluate_cv(\n                    model,\n                    dataset=fun_control[\"test\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n            elif self.fun_control[\"eval\"] == \"test_hold_out\":\n                df_eval, _ = evaluate_hold_out(\n                    model,\n                    train_dataset=fun_control[\"train\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    loss_function=self.fun_control[\"loss_function\"],\n                    metric=self.fun_control[\"metric_torch\"],\n                    test_dataset=fun_control[\"test\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    path=self.fun_control[\"path\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n            else:  # eval == \"train_hold_out\"\n                df_eval, _ = evaluate_hold_out(\n                    model,\n                    train_dataset=fun_control[\"train\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    loss_function=self.fun_control[\"loss_function\"],\n                    metric=self.fun_control[\"metric_torch\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    path=self.fun_control[\"path\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n        except Exception as err:\n            print(f\"Error in fun_torch(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n            print(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        z_val = fun_control[\"weights\"] * df_eval\n        if self.fun_control[\"spot_writer\"] is not None:\n            writer = self.fun_control[\"spot_writer\"]\n            writer.add_hparams(config, {\"fun_torch: loss\": z_val})\n            writer.flush()\n        z_res = np.append(z_res, z_val)\n    return z_res\n</code></pre>"},{"location":"reference/spotpython/fun/mohyperlight/","title":"mohyperlight","text":""},{"location":"reference/spotpython/fun/mohyperlight/#spotpython.fun.mohyperlight.MoHyperLight","title":"<code>MoHyperLight</code>","text":"<p>Multi-Objective Hyperparameter Tuning for Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for the random number generator. See Numpy Random Sampling.</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for the random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_light = MoHyperLight(seed=126, log_level=50)\n&gt;&gt;&gt; print(hyper_light.seed)\n    126\n</code></pre> Source code in <code>spotpython/fun/mohyperlight.py</code> <pre><code>class MoHyperLight:\n    \"\"\"\n    Multi-Objective Hyperparameter Tuning for Lightning.\n\n    Args:\n        seed (int): seed for the random number generator. See Numpy Random Sampling.\n        log_level (int): log level for the logger.\n\n    Attributes:\n        seed (int): seed for the random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the hyperparameter tuning.\n        log_level (int): log level for the logger.\n\n    Examples:\n        &gt;&gt;&gt; hyper_light = MoHyperLight(seed=126, log_level=50)\n        &gt;&gt;&gt; print(hyper_light.seed)\n            126\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50) -&gt; None:\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.log_level = log_level\n        logger.setLevel(log_level)\n        logger.info(f\"Starting the logger at level {log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray, fun_control: dict) -&gt; np.ndarray:\n        \"\"\"\n        Checks the shape of the input array X and raises an exception if it is not valid.\n\n        Args:\n            X (np.ndarray):\n                input array.\n            fun_control (dict):\n                dictionary containing control parameters for the hyperparameter tuning.\n\n        Returns:\n            np.ndarray:\n                input array with valid shape.\n\n        Raises:\n            Exception:\n                if the shape of the input array is not valid.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.utils.init import fun_control_init\n                from spotpython.light.regression.netlightregression import NetLightRegression\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.hyperparameters.values import add_core_model_to_fun_control\n                from spotpython.fun.mohyperlight import MoHyperLight\n                from spotpython.hyperparameters.values import get_var_name\n                fun_control = fun_control_init()\n                add_core_model_to_fun_control(core_model=NetLightRegression,\n                                            fun_control=fun_control,\n                                            hyper_dict=LightHyperDict)\n                hyper_light = MoHyperLight(seed=126, log_level=50)\n                n_hyperparams = len(get_var_name(fun_control))\n                # generate a random np.array X with shape (2, n_hyperparams)\n                X = np.random.rand(2, n_hyperparams)\n                X == hyper_light.check_X_shape(X, fun_control)\n                array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n                [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(get_var_name(fun_control)):\n            raise Exception(\"Invalid shape of input array X.\")\n        return X\n\n    def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluates the function for the given input array X of shape (n, k)\n        and control parameters specified as a dict.\n        Calls the train_model function from spotpython.light.trainmodel\n        to train the model and evaluate the results.\n\n        Args:\n            X (np.ndarray):\n                input array of shape (n, k) where n is the number of configurations evaluated\n                and k is the number of hyperparameters.\n            fun_control (dict):\n                dictionary containing control parameters for the hyperparameter tuning.\n\n        Returns:\n            (np.ndarray):\n                (2, n) array where the first row contains the evaluation results (z_res)\n                and the second row contains the extracted \"epochs\" values (epochs_res).\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        epochs_res = np.array([], dtype=float)  # Array to store \"epochs\" values\n\n        self.check_X_shape(X=X, fun_control=fun_control)\n        var_dict = assign_values(X, get_var_name(fun_control))\n\n        # Type information and transformations are considered in generate_one_config_from_var_dict:\n        for config in generate_one_config_from_var_dict(var_dict, fun_control):\n            if fun_control[\"show_config\"]:\n                print(\"\\nIn fun(): config:\")\n                pprint.pprint(config)\n            logger.debug(f\"\\nconfig: {config}\")\n\n            try:\n                logger.debug(\"fun: Calling train_model\")\n                df_eval = train_model(config, fun_control)\n                logger.debug(\"fun: train_model returned\")\n            except Exception as err:\n                if fun_control[\"verbosity\"] &gt; 0:\n                    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n                    if fun_control[\"verbosity\"] &gt; 1:\n                        pprint.pprint(fun_control)\n                    print(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                    print(\"Setting df_eval to np.nan\\n\")\n                logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                logger.error(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n\n            # Multiply results by the weights. Positive weights mean that the result is to be minimized.\n            # Negative weights mean that the result is to be maximized, e.g., accuracy.\n            z_val = fun_control[\"weights\"] * df_eval\n            z_res = np.append(z_res, z_val)  # Append evaluation result\n\n            # Extract \"epochs\" from the config and append to epochs_res\n            epochs_val = config.get(\"epochs\", np.nan)  # Default to np.nan if \"epochs\" is not in config\n            epochs_res = np.append(epochs_res, epochs_val)\n\n        # Stack z_res and epochs_res into a (n, 2) array\n        result = np.column_stack((z_res, epochs_res))\n        print(f\"result.shape: {result.shape}\")\n        print(f\"result: {result}\")\n\n        return result\n</code></pre>"},{"location":"reference/spotpython/fun/mohyperlight/#spotpython.fun.mohyperlight.MoHyperLight.check_X_shape","title":"<code>check_X_shape(X, fun_control)</code>","text":"<p>Checks the shape of the input array X and raises an exception if it is not valid.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: input array with valid shape.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if the shape of the input array is not valid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.init import fun_control_init\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    from spotpython.fun.mohyperlight import MoHyperLight\n    from spotpython.hyperparameters.values import get_var_name\n    fun_control = fun_control_init()\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n    hyper_light = MoHyperLight(seed=126, log_level=50)\n    n_hyperparams = len(get_var_name(fun_control))\n    # generate a random np.array X with shape (2, n_hyperparams)\n    X = np.random.rand(2, n_hyperparams)\n    X == hyper_light.check_X_shape(X, fun_control)\n    array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n    [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n</code></pre> Source code in <code>spotpython/fun/mohyperlight.py</code> <pre><code>def check_X_shape(self, X: np.ndarray, fun_control: dict) -&gt; np.ndarray:\n    \"\"\"\n    Checks the shape of the input array X and raises an exception if it is not valid.\n\n    Args:\n        X (np.ndarray):\n            input array.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        np.ndarray:\n            input array with valid shape.\n\n    Raises:\n        Exception:\n            if the shape of the input array is not valid.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.init import fun_control_init\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            from spotpython.fun.mohyperlight import MoHyperLight\n            from spotpython.hyperparameters.values import get_var_name\n            fun_control = fun_control_init()\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n            hyper_light = MoHyperLight(seed=126, log_level=50)\n            n_hyperparams = len(get_var_name(fun_control))\n            # generate a random np.array X with shape (2, n_hyperparams)\n            X = np.random.rand(2, n_hyperparams)\n            X == hyper_light.check_X_shape(X, fun_control)\n            array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n            [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(get_var_name(fun_control)):\n        raise Exception(\"Invalid shape of input array X.\")\n    return X\n</code></pre>"},{"location":"reference/spotpython/fun/mohyperlight/#spotpython.fun.mohyperlight.MoHyperLight.fun","title":"<code>fun(X, fun_control=None)</code>","text":"<p>Evaluates the function for the given input array X of shape (n, k) and control parameters specified as a dict. Calls the train_model function from spotpython.light.trainmodel to train the model and evaluate the results.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array of shape (n, k) where n is the number of configurations evaluated and k is the number of hyperparameters.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>(2, n) array where the first row contains the evaluation results (z_res) and the second row contains the extracted \u201cepochs\u201d values (epochs_res).</p> Source code in <code>spotpython/fun/mohyperlight.py</code> <pre><code>def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates the function for the given input array X of shape (n, k)\n    and control parameters specified as a dict.\n    Calls the train_model function from spotpython.light.trainmodel\n    to train the model and evaluate the results.\n\n    Args:\n        X (np.ndarray):\n            input array of shape (n, k) where n is the number of configurations evaluated\n            and k is the number of hyperparameters.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        (np.ndarray):\n            (2, n) array where the first row contains the evaluation results (z_res)\n            and the second row contains the extracted \"epochs\" values (epochs_res).\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    epochs_res = np.array([], dtype=float)  # Array to store \"epochs\" values\n\n    self.check_X_shape(X=X, fun_control=fun_control)\n    var_dict = assign_values(X, get_var_name(fun_control))\n\n    # Type information and transformations are considered in generate_one_config_from_var_dict:\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        if fun_control[\"show_config\"]:\n            print(\"\\nIn fun(): config:\")\n            pprint.pprint(config)\n        logger.debug(f\"\\nconfig: {config}\")\n\n        try:\n            logger.debug(\"fun: Calling train_model\")\n            df_eval = train_model(config, fun_control)\n            logger.debug(\"fun: train_model returned\")\n        except Exception as err:\n            if fun_control[\"verbosity\"] &gt; 0:\n                print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n                if fun_control[\"verbosity\"] &gt; 1:\n                    pprint.pprint(fun_control)\n                print(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                print(\"Setting df_eval to np.nan\\n\")\n            logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n            logger.error(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n\n        # Multiply results by the weights. Positive weights mean that the result is to be minimized.\n        # Negative weights mean that the result is to be maximized, e.g., accuracy.\n        z_val = fun_control[\"weights\"] * df_eval\n        z_res = np.append(z_res, z_val)  # Append evaluation result\n\n        # Extract \"epochs\" from the config and append to epochs_res\n        epochs_val = config.get(\"epochs\", np.nan)  # Default to np.nan if \"epochs\" is not in config\n        epochs_res = np.append(epochs_res, epochs_val)\n\n    # Stack z_res and epochs_res into a (n, 2) array\n    result = np.column_stack((z_res, epochs_res))\n    print(f\"result.shape: {result.shape}\")\n    print(f\"result: {result}\")\n\n    return result\n</code></pre>"},{"location":"reference/spotpython/fun/multiobjectivefunctions/","title":"multiobjectivefunctions","text":""},{"location":"reference/spotpython/fun/multiobjectivefunctions/#spotpython.fun.multiobjectivefunctions.MultiAnalytical","title":"<code>MultiAnalytical</code>","text":"<p>Class for multiobjective analytical test functions.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>float</code> <p>Offset value. Defaults to 0.0.</p> <code>0.0</code> <code>seed</code> <code>int</code> <p>Seed value for random number generation. Defaults to 126.</p> <code>126</code> <code>fun_control</code> <code>dict</code> <p>Dictionary containing control parameters for the function. Defaults to None.</p> <code>None</code> Notes <p>See Numpy Random Sampling</p> <p>Attributes:</p> Name Type Description <code>offset</code> <code>float</code> <p>Offset value.</p> <code>sigma</code> <code>float</code> <p>Noise level.</p> <code>seed</code> <code>int</code> <p>Seed value for random number generation.</p> <code>rng</code> <code>Generator</code> <p>Numpy random number generator object.</p> <code>fun_control</code> <code>dict</code> <p>Dictionary containing control parameters for the function.</p> <code>m</code> <code>int</code> <p>Number of objectives.</p> Source code in <code>spotpython/fun/multiobjectivefunctions.py</code> <pre><code>class MultiAnalytical:\n    \"\"\"\n    Class for multiobjective analytical test functions.\n\n    Args:\n        offset (float):\n            Offset value. Defaults to 0.0.\n        seed (int):\n            Seed value for random number generation. Defaults to 126.\n        fun_control (dict):\n            Dictionary containing control parameters for the function. Defaults to None.\n\n    Notes:\n        See [Numpy Random Sampling](https://numpy.org/doc/stable/reference/random/index.html#random-quick-start)\n\n    Attributes:\n        offset (float):\n            Offset value.\n        sigma (float):\n            Noise level.\n        seed (int):\n            Seed value for random number generation.\n        rng (Generator):\n            Numpy random number generator object.\n        fun_control (dict):\n            Dictionary containing control parameters for the function.\n        m (int):\n            Number of objectives.\n    \"\"\"\n\n    def __init__(self, offset: float = 0.0, sigma=0.0, seed: int = 126, fun_control=None, m=1) -&gt; None:\n        self.offset = offset\n        self.sigma = sigma\n        self.m = m\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\"offset\": offset, \"sigma\": self.sigma, \"seed\": self.seed}\n        # overwrite fun_control with user input if provided\n        if fun_control is not None:\n            self.fun_control = fun_control\n        # check if fun_control contains offset, sigma and seed, if not, add them\n        if \"offset\" not in self.fun_control:\n            self.fun_control[\"offset\"] = self.offset\n        if \"sigma\" not in self.fun_control:\n            self.fun_control[\"sigma\"] = self.sigma\n        if \"seed\" not in self.fun_control:\n            self.fun_control[\"seed\"] = self.seed\n\n    def __repr__(self) -&gt; str:\n        return f\"analytical(offset={self.offset}, sigma={self.sigma}, seed={self.seed})\"\n\n    def _prepare_input_data(self, X, fun_control):\n        if fun_control is not None:\n            self.fun_control = fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        X = np.atleast_2d(X)\n        return X\n\n    def _add_noise(self, y: List[float]) -&gt; np.ndarray:\n        \"\"\"\n        Adds noise to the input data.\n        This method takes in a list of float values y as input and adds noise to\n        the data using a random number generator. The method returns a numpy array\n        containing the noisy data.\n\n        Args:\n            self (analytical): analytical class object.\n            y (List[float]): Input data.\n\n        Returns:\n            np.ndarray: Noisy data.\n\n        \"\"\"\n        if self.fun_control[\"sigma\"] &gt; 0:\n            # Use own rng:\n            if self.fun_control[\"seed\"] is not None:\n                rng = default_rng(seed=self.fun_control[\"seed\"])\n            # Use class rng:\n            else:\n                rng = self.rng\n            noise_y = np.array([], dtype=float)\n            for y_i in y:\n                noise_y = np.append(\n                    noise_y,\n                    y_i + rng.normal(loc=0, scale=self.fun_control[\"sigma\"], size=1),\n                )\n            return noise_y\n        else:\n            return y\n\n    def fun_mo_linear(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Linear function with multi-objective support.\n\n        Args:\n            X (np.ndarray): Input array of shape (n, k), where n is the number of samples and k is the number of features.\n            fun_control (dict): Dictionary with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 2D numpy array with shape (n, m), where n is the number of samples and m is the number of objectives.\n\n        Examples:\n        &gt;&gt;&gt; from spotpython.fun.multiobjectivefunctions import MultiAnalytical\n            import numpy as np\n            fun = MultiAnalytical(m=1)\n            # Input data\n            X = np.array([[0, 0, 0], [1, 1, 1]])\n            # Single objective\n            print(fun.fun_mo_linear(X))\n            # Output: [[0.]\n            #          [3.]]\n            # Two objectives\n            fun = MultiAnalytical(m=2)\n            print(fun.fun_mo_linear(X))\n            # Output: [[ 0. -0.]\n            #          [ 3. -3.]]\n            # Three objectives\n            fun = MultiAnalytical(m=3)\n            print(fun.fun_mo_linear(X))\n            # Output: [[ 0. -0.  0.]\n            #          [ 3. -3.  3.]]\n            # Four objectives\n            fun = MultiAnalytical(m=4)\n            print(fun.fun_mo_linear(X))\n            # Output: [[ 0. -0.  0. -0.]\n            #          [ 3. -3.  3. -3.]]\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        offset = np.ones(X.shape[1]) * self.offset\n\n        alpha = self.fun_control.get(\"alpha\", 0.0)\n        beta = self.fun_control.get(\"beta\", None)\n        if beta is not None:\n            # Check if beta is a numpy array\n            if not isinstance(beta, np.ndarray):\n                # Convert beta to numpy array of shape (n,), where n is the number of columns in X\n                beta = np.array(beta)\n            if beta.shape[0] != X.shape[1]:\n                raise Exception(\"beta must have the same number of elements as the number of columns in X\")\n\n        # Compute the linear response\n        if beta is not None:\n            # Weighted sum with intercept\n            y_0 = alpha + np.dot(X - offset, beta)\n        else:\n            # Original behavior: just sum the rows\n            y_0 = alpha + np.sum(X - offset, axis=1)\n\n        # Add noise to the primary objective\n        y_0 = self._add_noise(y_0)\n\n        # Generate multi-objective outputs\n        objectives = [y_0 if i % 2 == 0 else -y_0 for i in range(self.m)]\n        return np.column_stack(objectives)\n</code></pre>"},{"location":"reference/spotpython/fun/multiobjectivefunctions/#spotpython.fun.multiobjectivefunctions.MultiAnalytical.fun_mo_linear","title":"<code>fun_mo_linear(X, fun_control=None)</code>","text":"<p>Linear function with multi-objective support.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input array of shape (n, k), where n is the number of samples and k is the number of features.</p> required <code>fun_control</code> <code>dict</code> <p>Dictionary with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D numpy array with shape (n, m), where n is the number of samples and m is the number of objectives.</p> <p>Examples:</p> <p>from spotpython.fun.multiobjectivefunctions import MultiAnalytical     import numpy as np     fun = MultiAnalytical(m=1)     # Input data     X = np.array([[0, 0, 0], [1, 1, 1]])     # Single objective     print(fun.fun_mo_linear(X))     # Output: [[0.]     #          [3.]]     # Two objectives     fun = MultiAnalytical(m=2)     print(fun.fun_mo_linear(X))     # Output: [[ 0. -0.]     #          [ 3. -3.]]     # Three objectives     fun = MultiAnalytical(m=3)     print(fun.fun_mo_linear(X))     # Output: [[ 0. -0.  0.]     #          [ 3. -3.  3.]]     # Four objectives     fun = MultiAnalytical(m=4)     print(fun.fun_mo_linear(X))     # Output: [[ 0. -0.  0. -0.]     #          [ 3. -3.  3. -3.]]</p> Source code in <code>spotpython/fun/multiobjectivefunctions.py</code> <pre><code>def fun_mo_linear(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Linear function with multi-objective support.\n\n    Args:\n        X (np.ndarray): Input array of shape (n, k), where n is the number of samples and k is the number of features.\n        fun_control (dict): Dictionary with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 2D numpy array with shape (n, m), where n is the number of samples and m is the number of objectives.\n\n    Examples:\n    &gt;&gt;&gt; from spotpython.fun.multiobjectivefunctions import MultiAnalytical\n        import numpy as np\n        fun = MultiAnalytical(m=1)\n        # Input data\n        X = np.array([[0, 0, 0], [1, 1, 1]])\n        # Single objective\n        print(fun.fun_mo_linear(X))\n        # Output: [[0.]\n        #          [3.]]\n        # Two objectives\n        fun = MultiAnalytical(m=2)\n        print(fun.fun_mo_linear(X))\n        # Output: [[ 0. -0.]\n        #          [ 3. -3.]]\n        # Three objectives\n        fun = MultiAnalytical(m=3)\n        print(fun.fun_mo_linear(X))\n        # Output: [[ 0. -0.  0.]\n        #          [ 3. -3.  3.]]\n        # Four objectives\n        fun = MultiAnalytical(m=4)\n        print(fun.fun_mo_linear(X))\n        # Output: [[ 0. -0.  0. -0.]\n        #          [ 3. -3.  3. -3.]]\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    offset = np.ones(X.shape[1]) * self.offset\n\n    alpha = self.fun_control.get(\"alpha\", 0.0)\n    beta = self.fun_control.get(\"beta\", None)\n    if beta is not None:\n        # Check if beta is a numpy array\n        if not isinstance(beta, np.ndarray):\n            # Convert beta to numpy array of shape (n,), where n is the number of columns in X\n            beta = np.array(beta)\n        if beta.shape[0] != X.shape[1]:\n            raise Exception(\"beta must have the same number of elements as the number of columns in X\")\n\n    # Compute the linear response\n    if beta is not None:\n        # Weighted sum with intercept\n        y_0 = alpha + np.dot(X - offset, beta)\n    else:\n        # Original behavior: just sum the rows\n        y_0 = alpha + np.sum(X - offset, axis=1)\n\n    # Add noise to the primary objective\n    y_0 = self._add_noise(y_0)\n\n    # Generate multi-objective outputs\n    objectives = [y_0 if i % 2 == 0 else -y_0 for i in range(self.m)]\n    return np.column_stack(objectives)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/","title":"objectivefunctions","text":""},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical","title":"<code>Analytical</code>","text":"<p>Class for analytical test functions.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>float</code> <p>Offset value. Defaults to 0.0.</p> <code>0.0</code> <code>seed</code> <code>int</code> <p>Seed value for random number generation. Defaults to 126.</p> <code>126</code> <code>fun_control</code> <code>dict</code> <p>Dictionary containing control parameters for the function. Defaults to None.</p> <code>None</code> Notes <p>See Numpy Random Sampling</p> <p>Attributes:</p> Name Type Description <code>offset</code> <code>float</code> <p>Offset value.</p> <code>sigma</code> <code>float</code> <p>Noise level.</p> <code>seed</code> <code>int</code> <p>Seed value for random number generation.</p> <code>rng</code> <code>Generator</code> <p>Numpy random number generator object.</p> <code>fun_control</code> <code>dict</code> <p>Dictionary containing control parameters for the function.</p> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>class Analytical:\n    \"\"\"\n    Class for analytical test functions.\n\n    Args:\n        offset (float):\n            Offset value. Defaults to 0.0.\n        seed (int):\n            Seed value for random number generation. Defaults to 126.\n        fun_control (dict):\n            Dictionary containing control parameters for the function. Defaults to None.\n\n    Notes:\n        See [Numpy Random Sampling](https://numpy.org/doc/stable/reference/random/index.html#random-quick-start)\n\n    Attributes:\n        offset (float):\n            Offset value.\n        sigma (float):\n            Noise level.\n        seed (int):\n            Seed value for random number generation.\n        rng (Generator):\n            Numpy random number generator object.\n        fun_control (dict):\n            Dictionary containing control parameters for the function.\n    \"\"\"\n\n    def __init__(self, offset: float = 0.0, sigma=0.0, seed: int = 126, fun_control=None) -&gt; None:\n        self.offset = offset\n        self.sigma = sigma\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\"offset\": offset, \"sigma\": self.sigma, \"seed\": self.seed}\n        # overwrite fun_control with user input if provided\n        if fun_control is not None:\n            self.fun_control = fun_control\n        # check if fun_control contains offset, sigma and seed, if not, add them\n        if \"offset\" not in self.fun_control:\n            self.fun_control[\"offset\"] = self.offset\n        if \"sigma\" not in self.fun_control:\n            self.fun_control[\"sigma\"] = self.sigma\n        if \"seed\" not in self.fun_control:\n            self.fun_control[\"seed\"] = self.seed\n\n    def __repr__(self) -&gt; str:\n        return f\"analytical(offset={self.offset}, sigma={self.sigma}, seed={self.seed})\"\n\n    def _prepare_input_data(self, X, fun_control):\n        if fun_control is not None:\n            self.fun_control = fun_control\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        X = np.atleast_2d(X)\n        return X\n\n    def _add_noise(self, y: List[float]) -&gt; np.ndarray:\n        \"\"\"\n        Adds noise to the input data.\n        This method takes in a list of float values y as input and adds noise to\n        the data using a random number generator. The method returns a numpy array\n        containing the noisy data.\n\n        Args:\n            self (analytical): analytical class object.\n            y (List[float]): Input data.\n\n        Returns:\n            np.ndarray: Noisy data.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n                import numpy as np\n                y = np.array([1, 2, 3, 4, 5])\n                fun = analytical(sigma=1.0, seed=123)\n                fun._add_noise(y)\n            array([0.01087865, 1.63221335, 4.28792526, 4.19397442, 5.9202309 ])\n\n        \"\"\"\n        if self.fun_control[\"sigma\"] &gt; 0:\n            # Use own rng:\n            if self.fun_control[\"seed\"] is not None:\n                rng = default_rng(seed=self.fun_control[\"seed\"])\n            # Use class rng:\n            else:\n                rng = self.rng\n            noise_y = np.array([], dtype=float)\n            for y_i in y:\n                noise_y = np.append(\n                    noise_y,\n                    y_i + rng.normal(loc=0, scale=self.fun_control[\"sigma\"], size=1),\n                )\n            return noise_y\n        else:\n            return y\n\n    def fun_branin_factor(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"\n        Calculates the Branin function of (x1, x2) with an additional factor based on the value of x3.\n        If x3 = 1, the value of the Branin function is increased by 10.\n        If x3 = 2, the value of the Branin function is decreased by 10.\n        Otherwise, the value of the Branin function is not changed.\n\n        Args:\n            X (np.ndarray):\n                A 2D numpy array with shape (n, 3) where n is the number of samples.\n            fun_control (Optional[Dict]):\n                A dictionary containing control parameters for the function.\n                If None, self.fun_control is used. Defaults to None.\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n                import numpy as np\n                X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n                fun = analytical()\n                fun.fun_branin_factor(X)\n                array([55.60211264, 65.60211264, 45.60211264])\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        if X.shape[1] != 3:\n            raise Exception(\"X must have shape (n, 3)\")\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        x3 = X[:, 2]\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        r = 6\n        s = 10\n        t = 1 / (8 * np.pi)\n        y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n        for j in range(X.shape[0]):\n            if x3[j] == 1:\n                y[j] = y[j] + 10\n            elif x3[j] == 2:\n                y[j] = y[j] - 10\n        return self._add_noise(y)\n\n    def fun_linear(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Linear function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values, which were obtained by\n            summing the weighted input values after subtracting the offset. Noise can be added to the output. An intercept\n            can be provided by setting the `alpha` key in the `fun_control` dictionary. If the `beta` key is provided, the\n            weighted sum is computed. If `beta` is not provided, the sum of the input values is computed.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import Analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; # Without offset and without noise\n            &gt;&gt;&gt; user_fun = UserAnalytical()\n            &gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 1, 1]])\n            &gt;&gt;&gt; results = user_fun.fun_user_function(X)\n            &gt;&gt;&gt; print(results)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With offset and without noise\n            &gt;&gt;&gt; user_fun = UserAnalytical(offset=1.0)\n            &gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 1, 1]])\n            &gt;&gt;&gt; results = user_fun.fun_user_function(X)\n            &gt;&gt;&gt; print(results)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With offset and noise\n            &gt;&gt;&gt; user_fun = UserAnalytical(offset=1.0, sigma=0.1, seed=1)\n            &gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 1, 1]])\n            &gt;&gt;&gt; results = user_fun.fun_user_function(X)\n            &gt;&gt;&gt; print(results)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Provide alpha (intercept), no beta\n            &gt;&gt;&gt; fun_control = {\"alpha\": 10.0}\n            &gt;&gt;&gt; fun.fun_linear(X, fun_control=fun_control)\n            &gt;&gt;&gt; array([16., 25.])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Provide alpha and beta (weighted sum with intercept)\n            &gt;&gt;&gt; fun_control = {\"alpha\": 2.0, \"beta\": [1.0, 2.0, 3.0]}\n            &gt;&gt;&gt; fun.fun_linear(X, fun_control=fun_control)\n            array([14., 32.])\n                [0. 3.]\n                [3. 0.]\n                [3.03455842 0.08216181]\n\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        offset = np.ones(X.shape[1]) * self.offset\n\n        alpha = self.fun_control.get(\"alpha\", 0.0)\n        beta = self.fun_control.get(\"beta\", None)\n        if beta is not None:\n            # check if beta is a numpy array\n            if not isinstance(beta, np.ndarray):\n                # convert beta to numpy array of shape (n,), where n is the number of columns in X\n                beta = np.array(beta)\n            if beta.shape[0] != X.shape[1]:\n                raise Exception(\"beta must have the same number of elements as the number of columns in X\")\n\n        # Compute the linear response\n        if beta is not None:\n            # Weighted sum with intercept\n            y = alpha + np.dot(X - offset, beta)\n        else:\n            # Original behavior: just sum the rows\n            y = alpha + np.sum(X - offset, axis=1)\n\n        return self._add_noise(y)\n\n    def fun_sphere(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Sphere function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_sphere(X)\n            array([14., 77.])\n\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        offset = np.ones(X.shape[1]) * self.offset\n        y = np.sum((X - offset) ** 2, axis=1)\n        return self._add_noise(y)\n\n    def fun_cubed(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Cubed function. Implements the function f(x) = sum((x_i - offset)^3).\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6], [-1, -1, -1]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_cubed(X)\n            array([ 36., 405., -3.])\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        offset = np.ones(X.shape[1]) * self.offset\n        y = np.sum((X - offset) ** 3, axis=1)\n        return self._add_noise(y)\n\n    def fun_forrester(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Forrester function. Function used by [Forr08a, p.83].\n           f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].\n           Starts with three sample points at x=0, x=0.5, and x=1.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_forrester(X)\n            array([  0.        ,  11.99999999])\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        y = ((6.0 * X - 2) ** 2) * np.sin(12 * X - 4)\n        return self._add_noise(y)\n\n    def fun_branin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        r\"\"\"Branin function. The 2-dim Branin function is defined as\n            $$\n            y = a (x_2 - b x_1^2 + c x_1 - r) ^2 + s (1 - t) \\cos(x_1) + s,\n            $$\n            where values of $a, b, c, r, s$ and $t$ are:\n            $a = 1$, $b = 5.1 / (4\\pi^2)$, $c = 5 / \\pi$, $r = 6$, $s = 10$ and $t = 1 / (8\\pi)$.\n            It has three global minima with $f(x) = 0.39788736$ at\n            $$\n            (-\\pi, 12.275),\n            $$\n            $$\n            (\\pi, 2.275),\n            $$\n            and\n            $$\n            (9.42478, 2.475).\n            $$\n            Input domain: This function is usually evaluated on the square $x_1 \\in [-5, 10] \\times x_2 \\in [0, 15]$.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n                pi = np.pi\n                X = np.array([[0,0],\n                    [-pi, 12.275],\n                    [pi, 2.275],\n                    [9.42478, 2.475]])\n                fun = analytical()\n                fun.fun_branin(X)\n                array([55.60211264,  0.39788736,  0.39788736,  0.39788736])\n\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        if X.shape[1] != 2:\n            raise Exception\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        r = 6\n        s = 10\n        t = 1 / (8 * np.pi)\n        y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n        return self._add_noise(y)\n\n    def fun_branin_modified(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Modified Branin function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_branin_modified(X)\n            array([  0.        ,  11.99999999])\n\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        if X.shape[1] != 2:\n            raise Exception\n        x = X[:, 0]\n        y = X[:, 1]\n        X1 = 15 * x - 5\n        X2 = 15 * y\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        d = 6\n        e = 10\n        ff = 1 / (8 * np.pi)\n        y = (a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e) + 5 * x\n        return self._add_noise(y)\n\n    def fun_sin_cos(self, X, fun_control=None):\n        \"\"\"Sinusoidal function.\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_sin_cos(X)\n            array([-1.        , -0.41614684])\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        if X.shape[1] != 2:\n            raise Exception\n        x0 = X[:, 0]\n        x1 = X[:, 1]\n        y = 2.0 * np.sin(x0 - self.offset) + 0.5 * np.cos(x1 - self.offset)\n        return self._add_noise(y)\n\n    def fun_runge(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k &gt;= 1.\n           Interval: -5 &lt;= x &lt;= 5\n\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_runge(X)\n            array([0.0625    , 0.015625  , 0.00390625])\n\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        offset = np.ones(X.shape[1]) * self.offset\n        squared_diff = (X - offset) ** 2\n        sum_squared_diff = np.sum(squared_diff, axis=1)\n        y = 1 / (1 + sum_squared_diff)\n        return self._add_noise(y)\n\n    def fun_wingwt_to_nat(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        r\"\"\"Wing weight function.\n        Converts coded values to natural values, before applying the original `fun_wingwt` function (Eq. 1.4 in [Forr08a]).\n        Calculate the weight of an unpainted light aircraft wing based on design and operational parameters.\n        This function implements the wing weight model from Forrester et al., which aims to predict\n        the wing weight \\( W \\) using the following formula:\n\n        \\[\n        W = 0.036 \\times S_W^{0.758} \\times W_{fw}^{0.0035} \\times \\left( \\frac{A}{\\cos^2 \\Lambda} \\right)^{0.6}\n        \\times q^{0.006} \\times \\lambda^{0.04} \\times \\left( \\frac{100 \\times R_{tc}}{\\cos \\Lambda} \\right)^{-0.3}\n        \\times (N_z \\times W_{dg})^{0.49} + S_W \\times W_p\n        \\]\n\n        where:\n\n        - \\( S_W \\): Wing area \\((\\text{ft}^2)\\)\n        - \\( W_{fw} \\): Weight of fuel in the wing (lb)\n        - \\( A \\): Aspect ratio\n        - \\( \\Lambda \\): Quarter-chord sweep (degrees)\n        - \\( q \\): Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\)\n        - \\( \\lambda \\): Taper ratio\n        - \\( R_{tc} \\): Aerofoil thickness to chord ratio\n        - \\( N_z \\): Ultimate load factor\n        - \\( W_{dg} \\): Flight design gross weight (lb)\n        - \\( W_p \\): Paint weight \\((\\text{lb/ft}^2)\\)\n\n        Parameter Overview:\n\n        | Symbol    | Parameter                              | Baseline | Minimum | Maximum |\n        |-----------|----------------------------------------|----------|---------|---------|\n        | \\( S_W \\)     | Wing area \\((\\text{ft}^2)\\)                     | 174      | 150     | 200     |\n        | \\( W_{fw} \\)  | Weight of fuel in wing (lb)            | 252      | 220     | 300     |\n        | \\( A \\)       | Aspect ratio                          | 7.52     | 6       | 10      |\n        | \\( \\Lambda \\) | Quarter-chord sweep (deg)              | 0        | -10     | 10      |\n        | \\( q \\)       | Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\) | 34       | 16      | 45      |\n        | \\( \\lambda \\) | Taper ratio                            | 0.672    | 0.5     | 1       |\n        | \\( R_{tc} \\)  | Aerofoil thickness to chord ratio      | 0.12     | 0.08    | 0.18    |\n        | \\( N_z \\)     | Ultimate load factor                   | 3.8      | 2.5     | 6       |\n        | \\( W_{dg} \\)  | Flight design gross weight (lb)        | 2000     | 1700    | 2500    |\n        | \\( W_p \\)     | Paint weight \\((\\text{lb/ft}^2)\\)      | 0.064 |   0.025  | 0.08    |\n\n        Args:\n            X (np.ndarray):\n                A 2D numpy array where each row contains 10 parameters for which the wing weight will be calculated.\n            fun_control (Optional[Dict]):\n                A dictionary with keys `sigma` (noise level) and `seed` (random seed)\n                for incorporating randomness if required. Default is `None`.\n\n        Returns:\n            np.ndarray:\n            A 1D numpy array with shape (n,) containing the calculated wing weight values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([np.zeros(10), np.ones(10)])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_wingwt(X)\n            array([158.28245046, 409.33182691])\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        Sw = X[:, 0] * 50 + 150  # equivalent to (200 - 150) + 150\n        Wfw = X[:, 1] * 80 + 220  # equivalent to (300 - 220) + 220\n        A = X[:, 2] * 4 + 6  # equivalent to (10 - 6) + 6\n        L = (X[:, 3] * 20 - 10) * np.pi / 180  # equivalent to (10 - (-10)) - 10\n        q = X[:, 4] * 29 + 16  # equivalent to (45 - 16) + 16\n        la = X[:, 5] * 0.5 + 0.5  # equivalent to (1 - 0.5) + 0.5\n        Rtc = X[:, 6] * 0.1 + 0.08  # equivalent to (0.18 - 0.08) + 0.08\n        Nz = X[:, 7] * 3.5 + 2.5  # equivalent to (6 - 2.5) + 2.5\n        Wdg = X[:, 8] * 800 + 1700  # equivalent to (2500 - 1700) + 1700\n        Wp = X[:, 9] * 0.055 + 0.025  # equivalent to (0.08 - 0.025) + 0.025\n        # Calculate W for all rows in a vectorized manner\n        W = 0.036 * Sw**0.758 * Wfw**0.0035\n        W *= (A / np.cos(L) ** 2) ** 0.6 * q**0.006\n        W *= la**0.04\n        print(f\"W: {W}\")\n        print(f\"(100 * Rtc / np.cos(L)): {(100 * Rtc / np.cos(L))}\")\n        W *= (100 * Rtc / np.cos(L)) ** (-0.3)\n        print(f\"W: {W}\")\n        W *= (Nz * Wdg) ** (0.49)\n        W += Sw * Wp\n        return self._add_noise(y=W)\n\n    def fun_wingwt(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        r\"\"\"Wing weight function. Returns coded, not natural values.\n        Calculate the weight of an unpainted light aircraft wing based on design and operational parameters.\n        This function implements the wing weight model from Forrester et al., which aims to predict\n        the wing weight \\( W \\) using the following formula:\n\n        \\[\n        W = 0.036 \\times S_W^{0.758} \\times W_{fw}^{0.0035} \\times \\left( \\frac{A}{\\cos^2 \\Lambda} \\right)^{0.6}\n        \\times q^{0.006} \\times \\lambda^{0.04} \\times \\left( \\frac{100 \\times R_{tc}}{\\cos \\Lambda} \\right)^{-0.3}\n        \\times (N_z \\times W_{dg})^{0.49} + S_W \\times W_p\n        \\]\n\n        where:\n\n        - \\( S_W \\): Wing area \\((\\text{ft}^2)\\)\n        - \\( W_{fw} \\): Weight of fuel in the wing (lb)\n        - \\( A \\): Aspect ratio\n        - \\( \\Lambda \\): Quarter-chord sweep (degrees)\n        - \\( q \\): Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\)\n        - \\( \\lambda \\): Taper ratio\n        - \\( R_{tc} \\): Aerofoil thickness to chord ratio\n        - \\( N_z \\): Ultimate load factor\n        - \\( W_{dg} \\): Flight design gross weight (lb)\n        - \\( W_p \\): Paint weight \\((\\text{lb/ft}^2)\\)\n\n        Parameter Overview:\n\n        | Symbol    | Parameter                              | Baseline | Minimum | Maximum |\n        |-----------|----------------------------------------|----------|---------|---------|\n        | \\( S_W \\)     | Wing area \\((\\text{ft}^2)\\)            | 174      | 150     | 200     |\n        | \\( W_{fw} \\)  | Weight of fuel in wing (lb)            | 252      | 220     | 300     |\n        | \\( A \\)       | Aspect ratio                          | 7.52     | 6       | 10      |\n        | \\( \\Lambda \\) | Quarter-chord sweep (deg)              | 0        | -10     | 10      |\n        | \\( q \\)       | Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\) | 34       | 16      | 45      |\n        | \\( \\lambda \\) | Taper ratio                            | 0.672    | 0.5     | 1       |\n        | \\( R_{tc} \\)  | Aerofoil thickness to chord ratio      | 0.12     | 0.08    | 0.18    |\n        | \\( N_z \\)     | Ultimate load factor                   | 3.8      | 2.5     | 6       |\n        | \\( W_{dg} \\)  | Flight design gross weight (lb)        | 2000     | 1700    | 2500    |\n        | \\( W_p \\)     | Paint weight \\((\\text{lb/ft}^2)\\)      | 0.064 |   0.025  | 0.08    |\n\n        Args:\n            X (np.ndarray):\n                A 2D numpy array where each row contains 10 parameters for which the wing weight will be calculated.\n            fun_control (Optional[Dict]):\n                A dictionary with keys `sigma` (noise level) and `seed` (random seed)\n                for incorporating randomness if required. Default is `None`.\n\n        Returns:\n            np.ndarray:\n            A 1D numpy array with shape (n,) containing the calculated wing weight values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([np.zeros(10), np.ones(10)])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_wingwt(X)\n            array([158.28245046, 409.33182691])\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        Sw = X[:, 0]\n        Wfw = X[:, 1]\n        A = X[:, 2]\n        L = X[:, 3] * np.pi / 180\n        q = X[:, 4]\n        la = X[:, 5]\n        Rtc = X[:, 6]\n        Nz = X[:, 7]\n        Wdg = X[:, 8]\n        Wp = X[:, 9]\n        # Calculate W for all rows in a vectorized manner\n        W = 0.036 * Sw**0.758 * Wfw**0.0035\n        W *= (A / np.cos(L) ** 2) ** 0.6 * q**0.006\n        W *= la**0.04\n        W *= (100 * Rtc / np.cos(L)) ** (-0.3)\n        W *= (Nz * Wdg) ** (0.49)\n        W += Sw * Wp\n        return self._add_noise(y=W)\n\n    def fun_xsin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Example function.\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_xsin(X)\n            array([0.84147098, 0.90929743, 0.14112001])\n\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        y = X * np.sin(1.0 / X)\n        return self._add_noise(y)\n\n    def fun_rosen(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Rosenbrock function.\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_rosen(X)\n            array([24,  0])\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        if X.shape[1] != 2:\n            raise Exception\n        x0 = X[:, 0]\n        x1 = X[:, 1]\n        b = 10\n        y = (x0 - 1) ** 2 + b * (x1 - x0**2) ** 2\n        return self._add_noise(y)\n\n    def fun_random_error(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Return errors for testing spot stability.\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_random_error(X)\n            array([24,  0])\n\n        \"\"\"\n        X = self._prepare_input_data(X, fun_control)\n        # Compute the sum of rows of X\n        y = np.sum(X, axis=1)\n        # Determine which elements to set to np.nan\n        nan_mask = self.rng.random(size=y.shape) &lt; 0.1\n        y[nan_mask] = np.nan\n\n        return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_branin","title":"<code>fun_branin(X, fun_control=None)</code>","text":"<p>Branin function. The 2-dim Branin function is defined as     $$     y = a (x_2 - b x_1^2 + c x_1 - r) ^2 + s (1 - t) \\cos(x_1) + s,     $$     where values of \\(a, b, c, r, s\\) and \\(t\\) are:     \\(a = 1\\), \\(b = 5.1 / (4\\pi^2)\\), \\(c = 5 / \\pi\\), \\(r = 6\\), \\(s = 10\\) and \\(t = 1 / (8\\pi)\\).     It has three global minima with \\(f(x) = 0.39788736\\) at     $$     (-\\pi, 12.275),     $$     $$     (\\pi, 2.275),     $$     and     $$     (9.42478, 2.475).     $$     Input domain: This function is usually evaluated on the square \\(x_1 \\in [-5, 10] \\times x_2 \\in [0, 15]\\).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n    pi = np.pi\n    X = np.array([[0,0],\n        [-pi, 12.275],\n        [pi, 2.275],\n        [9.42478, 2.475]])\n    fun = analytical()\n    fun.fun_branin(X)\n    array([55.60211264,  0.39788736,  0.39788736,  0.39788736])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_branin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    r\"\"\"Branin function. The 2-dim Branin function is defined as\n        $$\n        y = a (x_2 - b x_1^2 + c x_1 - r) ^2 + s (1 - t) \\cos(x_1) + s,\n        $$\n        where values of $a, b, c, r, s$ and $t$ are:\n        $a = 1$, $b = 5.1 / (4\\pi^2)$, $c = 5 / \\pi$, $r = 6$, $s = 10$ and $t = 1 / (8\\pi)$.\n        It has three global minima with $f(x) = 0.39788736$ at\n        $$\n        (-\\pi, 12.275),\n        $$\n        $$\n        (\\pi, 2.275),\n        $$\n        and\n        $$\n        (9.42478, 2.475).\n        $$\n        Input domain: This function is usually evaluated on the square $x_1 \\in [-5, 10] \\times x_2 \\in [0, 15]$.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            pi = np.pi\n            X = np.array([[0,0],\n                [-pi, 12.275],\n                [pi, 2.275],\n                [9.42478, 2.475]])\n            fun = analytical()\n            fun.fun_branin(X)\n            array([55.60211264,  0.39788736,  0.39788736,  0.39788736])\n\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    if X.shape[1] != 2:\n        raise Exception\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    r = 6\n    s = 10\n    t = 1 / (8 * np.pi)\n    y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_branin_factor","title":"<code>fun_branin_factor(X, fun_control=None)</code>","text":"<p>Calculates the Branin function of (x1, x2) with an additional factor based on the value of x3. If x3 = 1, the value of the Branin function is increased by 10. If x3 = 2, the value of the Branin function is decreased by 10. Otherwise, the value of the Branin function is not changed.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D numpy array with shape (n, 3) where n is the number of samples.</p> required <code>fun_control</code> <code>Optional[Dict]</code> <p>A dictionary containing control parameters for the function. If None, self.fun_control is used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n    import numpy as np\n    X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n    fun = analytical()\n    fun.fun_branin_factor(X)\n    array([55.60211264, 65.60211264, 45.60211264])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_branin_factor(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the Branin function of (x1, x2) with an additional factor based on the value of x3.\n    If x3 = 1, the value of the Branin function is increased by 10.\n    If x3 = 2, the value of the Branin function is decreased by 10.\n    Otherwise, the value of the Branin function is not changed.\n\n    Args:\n        X (np.ndarray):\n            A 2D numpy array with shape (n, 3) where n is the number of samples.\n        fun_control (Optional[Dict]):\n            A dictionary containing control parameters for the function.\n            If None, self.fun_control is used. Defaults to None.\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n            import numpy as np\n            X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n            fun = analytical()\n            fun.fun_branin_factor(X)\n            array([55.60211264, 65.60211264, 45.60211264])\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    if X.shape[1] != 3:\n        raise Exception(\"X must have shape (n, 3)\")\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n    x3 = X[:, 2]\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    r = 6\n    s = 10\n    t = 1 / (8 * np.pi)\n    y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n    for j in range(X.shape[0]):\n        if x3[j] == 1:\n            y[j] = y[j] + 10\n        elif x3[j] == 2:\n            y[j] = y[j] - 10\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_branin_modified","title":"<code>fun_branin_modified(X, fun_control=None)</code>","text":"<p>Modified Branin function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_branin_modified(X)\narray([  0.        ,  11.99999999])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_branin_modified(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Modified Branin function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_branin_modified(X)\n        array([  0.        ,  11.99999999])\n\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    if X.shape[1] != 2:\n        raise Exception\n    x = X[:, 0]\n    y = X[:, 1]\n    X1 = 15 * x - 5\n    X2 = 15 * y\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    d = 6\n    e = 10\n    ff = 1 / (8 * np.pi)\n    y = (a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e) + 5 * x\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_cubed","title":"<code>fun_cubed(X, fun_control=None)</code>","text":"<p>Cubed function. Implements the function f(x) = sum((x_i - offset)^3).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6], [-1, -1, -1]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_cubed(X)\narray([ 36., 405., -3.])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_cubed(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Cubed function. Implements the function f(x) = sum((x_i - offset)^3).\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6], [-1, -1, -1]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_cubed(X)\n        array([ 36., 405., -3.])\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    offset = np.ones(X.shape[1]) * self.offset\n    y = np.sum((X - offset) ** 3, axis=1)\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_forrester","title":"<code>fun_forrester(X, fun_control=None)</code>","text":"<p>Forrester function. Function used by [Forr08a, p.83].    f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].    Starts with three sample points at x=0, x=0.5, and x=1.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_forrester(X)\narray([  0.        ,  11.99999999])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_forrester(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Forrester function. Function used by [Forr08a, p.83].\n       f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].\n       Starts with three sample points at x=0, x=0.5, and x=1.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_forrester(X)\n        array([  0.        ,  11.99999999])\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    y = ((6.0 * X - 2) ** 2) * np.sin(12 * X - 4)\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_linear","title":"<code>fun_linear(X, fun_control=None)</code>","text":"<p>Linear function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values, which were obtained by</p> <code>ndarray</code> <p>summing the weighted input values after subtracting the offset. Noise can be added to the output. An intercept</p> <code>ndarray</code> <p>can be provided by setting the <code>alpha</code> key in the <code>fun_control</code> dictionary. If the <code>beta</code> key is provided, the</p> <code>ndarray</code> <p>weighted sum is computed. If <code>beta</code> is not provided, the sum of the input values is computed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import Analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Without offset and without noise\n&gt;&gt;&gt; user_fun = UserAnalytical()\n&gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 1, 1]])\n&gt;&gt;&gt; results = user_fun.fun_user_function(X)\n&gt;&gt;&gt; print(results)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With offset and without noise\n&gt;&gt;&gt; user_fun = UserAnalytical(offset=1.0)\n&gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 1, 1]])\n&gt;&gt;&gt; results = user_fun.fun_user_function(X)\n&gt;&gt;&gt; print(results)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With offset and noise\n&gt;&gt;&gt; user_fun = UserAnalytical(offset=1.0, sigma=0.1, seed=1)\n&gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 1, 1]])\n&gt;&gt;&gt; results = user_fun.fun_user_function(X)\n&gt;&gt;&gt; print(results)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Provide alpha (intercept), no beta\n&gt;&gt;&gt; fun_control = {\"alpha\": 10.0}\n&gt;&gt;&gt; fun.fun_linear(X, fun_control=fun_control)\n&gt;&gt;&gt; array([16., 25.])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Provide alpha and beta (weighted sum with intercept)\n&gt;&gt;&gt; fun_control = {\"alpha\": 2.0, \"beta\": [1.0, 2.0, 3.0]}\n&gt;&gt;&gt; fun.fun_linear(X, fun_control=fun_control)\narray([14., 32.])\n    [0. 3.]\n    [3. 0.]\n    [3.03455842 0.08216181]\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_linear(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Linear function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values, which were obtained by\n        summing the weighted input values after subtracting the offset. Noise can be added to the output. An intercept\n        can be provided by setting the `alpha` key in the `fun_control` dictionary. If the `beta` key is provided, the\n        weighted sum is computed. If `beta` is not provided, the sum of the input values is computed.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import Analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; # Without offset and without noise\n        &gt;&gt;&gt; user_fun = UserAnalytical()\n        &gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 1, 1]])\n        &gt;&gt;&gt; results = user_fun.fun_user_function(X)\n        &gt;&gt;&gt; print(results)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With offset and without noise\n        &gt;&gt;&gt; user_fun = UserAnalytical(offset=1.0)\n        &gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 1, 1]])\n        &gt;&gt;&gt; results = user_fun.fun_user_function(X)\n        &gt;&gt;&gt; print(results)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With offset and noise\n        &gt;&gt;&gt; user_fun = UserAnalytical(offset=1.0, sigma=0.1, seed=1)\n        &gt;&gt;&gt; X = np.array([[0, 0, 0], [1, 1, 1]])\n        &gt;&gt;&gt; results = user_fun.fun_user_function(X)\n        &gt;&gt;&gt; print(results)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Provide alpha (intercept), no beta\n        &gt;&gt;&gt; fun_control = {\"alpha\": 10.0}\n        &gt;&gt;&gt; fun.fun_linear(X, fun_control=fun_control)\n        &gt;&gt;&gt; array([16., 25.])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Provide alpha and beta (weighted sum with intercept)\n        &gt;&gt;&gt; fun_control = {\"alpha\": 2.0, \"beta\": [1.0, 2.0, 3.0]}\n        &gt;&gt;&gt; fun.fun_linear(X, fun_control=fun_control)\n        array([14., 32.])\n            [0. 3.]\n            [3. 0.]\n            [3.03455842 0.08216181]\n\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    offset = np.ones(X.shape[1]) * self.offset\n\n    alpha = self.fun_control.get(\"alpha\", 0.0)\n    beta = self.fun_control.get(\"beta\", None)\n    if beta is not None:\n        # check if beta is a numpy array\n        if not isinstance(beta, np.ndarray):\n            # convert beta to numpy array of shape (n,), where n is the number of columns in X\n            beta = np.array(beta)\n        if beta.shape[0] != X.shape[1]:\n            raise Exception(\"beta must have the same number of elements as the number of columns in X\")\n\n    # Compute the linear response\n    if beta is not None:\n        # Weighted sum with intercept\n        y = alpha + np.dot(X - offset, beta)\n    else:\n        # Original behavior: just sum the rows\n        y = alpha + np.sum(X - offset, axis=1)\n\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_random_error","title":"<code>fun_random_error(X, fun_control=None)</code>","text":"<p>Return errors for testing spot stability. Args:     X (array): input     fun_control (dict): dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_random_error(X)\narray([24,  0])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_random_error(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Return errors for testing spot stability.\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_random_error(X)\n        array([24,  0])\n\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    # Compute the sum of rows of X\n    y = np.sum(X, axis=1)\n    # Determine which elements to set to np.nan\n    nan_mask = self.rng.random(size=y.shape) &lt; 0.1\n    y[nan_mask] = np.nan\n\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_rosen","title":"<code>fun_rosen(X, fun_control=None)</code>","text":"<p>Rosenbrock function. Args:     X (array): input     fun_control (dict): dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_rosen(X)\narray([24,  0])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_rosen(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Rosenbrock function.\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_rosen(X)\n        array([24,  0])\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    if X.shape[1] != 2:\n        raise Exception\n    x0 = X[:, 0]\n    x1 = X[:, 1]\n    b = 10\n    y = (x0 - 1) ** 2 + b * (x1 - x0**2) ** 2\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_runge","title":"<code>fun_runge(X, fun_control=None)</code>","text":"<p>Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k &gt;= 1.    Interval: -5 &lt;= x &lt;= 5</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_runge(X)\narray([0.0625    , 0.015625  , 0.00390625])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_runge(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k &gt;= 1.\n       Interval: -5 &lt;= x &lt;= 5\n\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_runge(X)\n        array([0.0625    , 0.015625  , 0.00390625])\n\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    offset = np.ones(X.shape[1]) * self.offset\n    squared_diff = (X - offset) ** 2\n    sum_squared_diff = np.sum(squared_diff, axis=1)\n    y = 1 / (1 + sum_squared_diff)\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_sin_cos","title":"<code>fun_sin_cos(X, fun_control=None)</code>","text":"<p>Sinusoidal function. Args:     X (array):         input     fun_control (dict):         dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_sin_cos(X)\narray([-1.        , -0.41614684])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_sin_cos(self, X, fun_control=None):\n    \"\"\"Sinusoidal function.\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_sin_cos(X)\n        array([-1.        , -0.41614684])\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    if X.shape[1] != 2:\n        raise Exception\n    x0 = X[:, 0]\n    x1 = X[:, 1]\n    y = 2.0 * np.sin(x0 - self.offset) + 0.5 * np.cos(x1 - self.offset)\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_sphere","title":"<code>fun_sphere(X, fun_control=None)</code>","text":"<p>Sphere function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_sphere(X)\narray([14., 77.])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_sphere(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Sphere function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_sphere(X)\n        array([14., 77.])\n\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    offset = np.ones(X.shape[1]) * self.offset\n    y = np.sum((X - offset) ** 2, axis=1)\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_wingwt","title":"<code>fun_wingwt(X, fun_control=None)</code>","text":"<p>Wing weight function. Returns coded, not natural values. Calculate the weight of an unpainted light aircraft wing based on design and operational parameters. This function implements the wing weight model from Forrester et al., which aims to predict the wing weight \\( W \\) using the following formula:</p> \\[ W = 0.036 \\times S_W^{0.758} \\times W_{fw}^{0.0035} \\times \\left( \\frac{A}{\\cos^2 \\Lambda} \\right)^{0.6} \\times q^{0.006} \\times \\lambda^{0.04} \\times \\left( \\frac{100 \\times R_{tc}}{\\cos \\Lambda} \\right)^{-0.3} \\times (N_z \\times W_{dg})^{0.49} + S_W \\times W_p \\] <p>where:</p> <ul> <li>\\( S_W \\): Wing area \\((\\text{ft}^2)\\)</li> <li>\\( W_{fw} \\): Weight of fuel in the wing (lb)</li> <li>\\( A \\): Aspect ratio</li> <li>\\( \\Lambda \\): Quarter-chord sweep (degrees)</li> <li>\\( q \\): Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\)</li> <li>\\( \\lambda \\): Taper ratio</li> <li>\\( R_{tc} \\): Aerofoil thickness to chord ratio</li> <li>\\( N_z \\): Ultimate load factor</li> <li>\\( W_{dg} \\): Flight design gross weight (lb)</li> <li>\\( W_p \\): Paint weight \\((\\text{lb/ft}^2)\\)</li> </ul> <p>Parameter Overview:</p> Symbol Parameter Baseline Minimum Maximum \\( S_W \\) Wing area \\((\\text{ft}^2)\\) 174 150 200 \\( W_{fw} \\) Weight of fuel in wing (lb) 252 220 300 \\( A \\) Aspect ratio 7.52 6 10 \\( \\Lambda \\) Quarter-chord sweep (deg) 0 -10 10 \\( q \\) Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\) 34 16 45 \\( \\lambda \\) Taper ratio 0.672 0.5 1 \\( R_{tc} \\) Aerofoil thickness to chord ratio 0.12 0.08 0.18 \\( N_z \\) Ultimate load factor 3.8 2.5 6 \\( W_{dg} \\) Flight design gross weight (lb) 2000 1700 2500 \\( W_p \\) Paint weight \\((\\text{lb/ft}^2)\\) 0.064 0.025 0.08 <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D numpy array where each row contains 10 parameters for which the wing weight will be calculated.</p> required <code>fun_control</code> <code>Optional[Dict]</code> <p>A dictionary with keys <code>sigma</code> (noise level) and <code>seed</code> (random seed) for incorporating randomness if required. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray:</p> <code>ndarray</code> <p>A 1D numpy array with shape (n,) containing the calculated wing weight values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([np.zeros(10), np.ones(10)])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_wingwt(X)\narray([158.28245046, 409.33182691])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_wingwt(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    r\"\"\"Wing weight function. Returns coded, not natural values.\n    Calculate the weight of an unpainted light aircraft wing based on design and operational parameters.\n    This function implements the wing weight model from Forrester et al., which aims to predict\n    the wing weight \\( W \\) using the following formula:\n\n    \\[\n    W = 0.036 \\times S_W^{0.758} \\times W_{fw}^{0.0035} \\times \\left( \\frac{A}{\\cos^2 \\Lambda} \\right)^{0.6}\n    \\times q^{0.006} \\times \\lambda^{0.04} \\times \\left( \\frac{100 \\times R_{tc}}{\\cos \\Lambda} \\right)^{-0.3}\n    \\times (N_z \\times W_{dg})^{0.49} + S_W \\times W_p\n    \\]\n\n    where:\n\n    - \\( S_W \\): Wing area \\((\\text{ft}^2)\\)\n    - \\( W_{fw} \\): Weight of fuel in the wing (lb)\n    - \\( A \\): Aspect ratio\n    - \\( \\Lambda \\): Quarter-chord sweep (degrees)\n    - \\( q \\): Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\)\n    - \\( \\lambda \\): Taper ratio\n    - \\( R_{tc} \\): Aerofoil thickness to chord ratio\n    - \\( N_z \\): Ultimate load factor\n    - \\( W_{dg} \\): Flight design gross weight (lb)\n    - \\( W_p \\): Paint weight \\((\\text{lb/ft}^2)\\)\n\n    Parameter Overview:\n\n    | Symbol    | Parameter                              | Baseline | Minimum | Maximum |\n    |-----------|----------------------------------------|----------|---------|---------|\n    | \\( S_W \\)     | Wing area \\((\\text{ft}^2)\\)            | 174      | 150     | 200     |\n    | \\( W_{fw} \\)  | Weight of fuel in wing (lb)            | 252      | 220     | 300     |\n    | \\( A \\)       | Aspect ratio                          | 7.52     | 6       | 10      |\n    | \\( \\Lambda \\) | Quarter-chord sweep (deg)              | 0        | -10     | 10      |\n    | \\( q \\)       | Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\) | 34       | 16      | 45      |\n    | \\( \\lambda \\) | Taper ratio                            | 0.672    | 0.5     | 1       |\n    | \\( R_{tc} \\)  | Aerofoil thickness to chord ratio      | 0.12     | 0.08    | 0.18    |\n    | \\( N_z \\)     | Ultimate load factor                   | 3.8      | 2.5     | 6       |\n    | \\( W_{dg} \\)  | Flight design gross weight (lb)        | 2000     | 1700    | 2500    |\n    | \\( W_p \\)     | Paint weight \\((\\text{lb/ft}^2)\\)      | 0.064 |   0.025  | 0.08    |\n\n    Args:\n        X (np.ndarray):\n            A 2D numpy array where each row contains 10 parameters for which the wing weight will be calculated.\n        fun_control (Optional[Dict]):\n            A dictionary with keys `sigma` (noise level) and `seed` (random seed)\n            for incorporating randomness if required. Default is `None`.\n\n    Returns:\n        np.ndarray:\n        A 1D numpy array with shape (n,) containing the calculated wing weight values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([np.zeros(10), np.ones(10)])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_wingwt(X)\n        array([158.28245046, 409.33182691])\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    Sw = X[:, 0]\n    Wfw = X[:, 1]\n    A = X[:, 2]\n    L = X[:, 3] * np.pi / 180\n    q = X[:, 4]\n    la = X[:, 5]\n    Rtc = X[:, 6]\n    Nz = X[:, 7]\n    Wdg = X[:, 8]\n    Wp = X[:, 9]\n    # Calculate W for all rows in a vectorized manner\n    W = 0.036 * Sw**0.758 * Wfw**0.0035\n    W *= (A / np.cos(L) ** 2) ** 0.6 * q**0.006\n    W *= la**0.04\n    W *= (100 * Rtc / np.cos(L)) ** (-0.3)\n    W *= (Nz * Wdg) ** (0.49)\n    W += Sw * Wp\n    return self._add_noise(y=W)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_wingwt_to_nat","title":"<code>fun_wingwt_to_nat(X, fun_control=None)</code>","text":"<p>Wing weight function. Converts coded values to natural values, before applying the original <code>fun_wingwt</code> function (Eq. 1.4 in [Forr08a]). Calculate the weight of an unpainted light aircraft wing based on design and operational parameters. This function implements the wing weight model from Forrester et al., which aims to predict the wing weight \\( W \\) using the following formula:</p> \\[ W = 0.036 \\times S_W^{0.758} \\times W_{fw}^{0.0035} \\times \\left( \\frac{A}{\\cos^2 \\Lambda} \\right)^{0.6} \\times q^{0.006} \\times \\lambda^{0.04} \\times \\left( \\frac{100 \\times R_{tc}}{\\cos \\Lambda} \\right)^{-0.3} \\times (N_z \\times W_{dg})^{0.49} + S_W \\times W_p \\] <p>where:</p> <ul> <li>\\( S_W \\): Wing area \\((\\text{ft}^2)\\)</li> <li>\\( W_{fw} \\): Weight of fuel in the wing (lb)</li> <li>\\( A \\): Aspect ratio</li> <li>\\( \\Lambda \\): Quarter-chord sweep (degrees)</li> <li>\\( q \\): Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\)</li> <li>\\( \\lambda \\): Taper ratio</li> <li>\\( R_{tc} \\): Aerofoil thickness to chord ratio</li> <li>\\( N_z \\): Ultimate load factor</li> <li>\\( W_{dg} \\): Flight design gross weight (lb)</li> <li>\\( W_p \\): Paint weight \\((\\text{lb/ft}^2)\\)</li> </ul> <p>Parameter Overview:</p> Symbol Parameter Baseline Minimum Maximum \\( S_W \\) Wing area \\((\\text{ft}^2)\\) 174 150 200 \\( W_{fw} \\) Weight of fuel in wing (lb) 252 220 300 \\( A \\) Aspect ratio 7.52 6 10 \\( \\Lambda \\) Quarter-chord sweep (deg) 0 -10 10 \\( q \\) Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\) 34 16 45 \\( \\lambda \\) Taper ratio 0.672 0.5 1 \\( R_{tc} \\) Aerofoil thickness to chord ratio 0.12 0.08 0.18 \\( N_z \\) Ultimate load factor 3.8 2.5 6 \\( W_{dg} \\) Flight design gross weight (lb) 2000 1700 2500 \\( W_p \\) Paint weight \\((\\text{lb/ft}^2)\\) 0.064 0.025 0.08 <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D numpy array where each row contains 10 parameters for which the wing weight will be calculated.</p> required <code>fun_control</code> <code>Optional[Dict]</code> <p>A dictionary with keys <code>sigma</code> (noise level) and <code>seed</code> (random seed) for incorporating randomness if required. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray:</p> <code>ndarray</code> <p>A 1D numpy array with shape (n,) containing the calculated wing weight values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([np.zeros(10), np.ones(10)])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_wingwt(X)\narray([158.28245046, 409.33182691])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_wingwt_to_nat(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    r\"\"\"Wing weight function.\n    Converts coded values to natural values, before applying the original `fun_wingwt` function (Eq. 1.4 in [Forr08a]).\n    Calculate the weight of an unpainted light aircraft wing based on design and operational parameters.\n    This function implements the wing weight model from Forrester et al., which aims to predict\n    the wing weight \\( W \\) using the following formula:\n\n    \\[\n    W = 0.036 \\times S_W^{0.758} \\times W_{fw}^{0.0035} \\times \\left( \\frac{A}{\\cos^2 \\Lambda} \\right)^{0.6}\n    \\times q^{0.006} \\times \\lambda^{0.04} \\times \\left( \\frac{100 \\times R_{tc}}{\\cos \\Lambda} \\right)^{-0.3}\n    \\times (N_z \\times W_{dg})^{0.49} + S_W \\times W_p\n    \\]\n\n    where:\n\n    - \\( S_W \\): Wing area \\((\\text{ft}^2)\\)\n    - \\( W_{fw} \\): Weight of fuel in the wing (lb)\n    - \\( A \\): Aspect ratio\n    - \\( \\Lambda \\): Quarter-chord sweep (degrees)\n    - \\( q \\): Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\)\n    - \\( \\lambda \\): Taper ratio\n    - \\( R_{tc} \\): Aerofoil thickness to chord ratio\n    - \\( N_z \\): Ultimate load factor\n    - \\( W_{dg} \\): Flight design gross weight (lb)\n    - \\( W_p \\): Paint weight \\((\\text{lb/ft}^2)\\)\n\n    Parameter Overview:\n\n    | Symbol    | Parameter                              | Baseline | Minimum | Maximum |\n    |-----------|----------------------------------------|----------|---------|---------|\n    | \\( S_W \\)     | Wing area \\((\\text{ft}^2)\\)                     | 174      | 150     | 200     |\n    | \\( W_{fw} \\)  | Weight of fuel in wing (lb)            | 252      | 220     | 300     |\n    | \\( A \\)       | Aspect ratio                          | 7.52     | 6       | 10      |\n    | \\( \\Lambda \\) | Quarter-chord sweep (deg)              | 0        | -10     | 10      |\n    | \\( q \\)       | Dynamic pressure at cruise \\((\\text{lb/ft}^2)\\) | 34       | 16      | 45      |\n    | \\( \\lambda \\) | Taper ratio                            | 0.672    | 0.5     | 1       |\n    | \\( R_{tc} \\)  | Aerofoil thickness to chord ratio      | 0.12     | 0.08    | 0.18    |\n    | \\( N_z \\)     | Ultimate load factor                   | 3.8      | 2.5     | 6       |\n    | \\( W_{dg} \\)  | Flight design gross weight (lb)        | 2000     | 1700    | 2500    |\n    | \\( W_p \\)     | Paint weight \\((\\text{lb/ft}^2)\\)      | 0.064 |   0.025  | 0.08    |\n\n    Args:\n        X (np.ndarray):\n            A 2D numpy array where each row contains 10 parameters for which the wing weight will be calculated.\n        fun_control (Optional[Dict]):\n            A dictionary with keys `sigma` (noise level) and `seed` (random seed)\n            for incorporating randomness if required. Default is `None`.\n\n    Returns:\n        np.ndarray:\n        A 1D numpy array with shape (n,) containing the calculated wing weight values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([np.zeros(10), np.ones(10)])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_wingwt(X)\n        array([158.28245046, 409.33182691])\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    Sw = X[:, 0] * 50 + 150  # equivalent to (200 - 150) + 150\n    Wfw = X[:, 1] * 80 + 220  # equivalent to (300 - 220) + 220\n    A = X[:, 2] * 4 + 6  # equivalent to (10 - 6) + 6\n    L = (X[:, 3] * 20 - 10) * np.pi / 180  # equivalent to (10 - (-10)) - 10\n    q = X[:, 4] * 29 + 16  # equivalent to (45 - 16) + 16\n    la = X[:, 5] * 0.5 + 0.5  # equivalent to (1 - 0.5) + 0.5\n    Rtc = X[:, 6] * 0.1 + 0.08  # equivalent to (0.18 - 0.08) + 0.08\n    Nz = X[:, 7] * 3.5 + 2.5  # equivalent to (6 - 2.5) + 2.5\n    Wdg = X[:, 8] * 800 + 1700  # equivalent to (2500 - 1700) + 1700\n    Wp = X[:, 9] * 0.055 + 0.025  # equivalent to (0.08 - 0.025) + 0.025\n    # Calculate W for all rows in a vectorized manner\n    W = 0.036 * Sw**0.758 * Wfw**0.0035\n    W *= (A / np.cos(L) ** 2) ** 0.6 * q**0.006\n    W *= la**0.04\n    print(f\"W: {W}\")\n    print(f\"(100 * Rtc / np.cos(L)): {(100 * Rtc / np.cos(L))}\")\n    W *= (100 * Rtc / np.cos(L)) ** (-0.3)\n    print(f\"W: {W}\")\n    W *= (Nz * Wdg) ** (0.49)\n    W += Sw * Wp\n    return self._add_noise(y=W)\n</code></pre>"},{"location":"reference/spotpython/fun/objectivefunctions/#spotpython.fun.objectivefunctions.Analytical.fun_xsin","title":"<code>fun_xsin(X, fun_control=None)</code>","text":"<p>Example function. Args:     X (array): input     fun_control (dict): dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_xsin(X)\narray([0.84147098, 0.90929743, 0.14112001])\n</code></pre> Source code in <code>spotpython/fun/objectivefunctions.py</code> <pre><code>def fun_xsin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Example function.\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_xsin(X)\n        array([0.84147098, 0.90929743, 0.14112001])\n\n    \"\"\"\n    X = self._prepare_input_data(X, fun_control)\n    y = X * np.sin(1.0 / X)\n    return self._add_noise(y)\n</code></pre>"},{"location":"reference/spotpython/fun/xai_hyperlight/","title":"xai_hyperlight","text":""},{"location":"reference/spotpython/fun/xai_hyperlight/#spotpython.fun.xai_hyperlight.XAI_HyperLight","title":"<code>XAI_HyperLight</code>","text":"<p>Hyperparameter Tuning for Lightning considering XAI inconsistency.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for the random number generator. See Numpy Random Sampling.</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for the random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n&gt;&gt;&gt; print(hyper_light.seed)\n    126\n</code></pre> Source code in <code>spotpython/fun/xai_hyperlight.py</code> <pre><code>class XAI_HyperLight:\n    \"\"\"\n    Hyperparameter Tuning for Lightning considering XAI inconsistency.\n\n    Args:\n        seed (int): seed for the random number generator. See Numpy Random Sampling.\n        log_level (int): log level for the logger.\n\n    Attributes:\n        seed (int): seed for the random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the hyperparameter tuning.\n        log_level (int): log level for the logger.\n\n    Examples:\n        &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n        &gt;&gt;&gt; print(hyper_light.seed)\n            126\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50) -&gt; None:\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.log_level = log_level\n        logger.setLevel(log_level)\n        logger.info(f\"Starting the logger at level {log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray, fun_control: dict) -&gt; np.ndarray:\n        \"\"\"\n        Checks the shape of the input array X and raises an exception if it is not valid.\n\n        Args:\n            X (np.ndarray):\n                input array.\n            fun_control (dict):\n                dictionary containing control parameters for the hyperparameter tuning.\n\n        Returns:\n            np.ndarray:\n                input array with valid shape.\n\n        Raises:\n            Exception:\n                if the shape of the input array is not valid.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.utils.init import fun_control_init\n                from spotpython.light.regression.netlightregression import NetLightRegression\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.hyperparameters.values import add_core_model_to_fun_control\n                from spotpython.fun.hyperlight import HyperLight\n                from spotpython.hyperparameters.values import get_var_name\n                fun_control = fun_control_init()\n                add_core_model_to_fun_control(core_model=NetLightRegression,\n                                            fun_control=fun_control,\n                                            hyper_dict=LightHyperDict)\n                hyper_light = HyperLight(seed=126, log_level=50)\n                n_hyperparams = len(get_var_name(fun_control))\n                # generate a random np.array X with shape (2, n_hyperparams)\n                X = np.random.rand(2, n_hyperparams)\n                X == hyper_light.check_X_shape(X, fun_control)\n                array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n                [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(get_var_name(fun_control)):\n            raise Exception(\"Invalid shape of input array X.\")\n        return X\n\n    def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluates the function for the given input array X and control parameters.\n        Calls the train_model function from spotpython.light.trainmodel\n        to train the model and evaluate the results.\n\n        Args:\n            X (np.ndarray):\n                input array.\n            fun_control (dict):\n                dictionary containing control parameters for the hyperparameter tuning.\n\n        Returns:\n            (np.ndarray):\n                array containing the evaluation results.\n\n        Examples:\n            &gt;&gt;&gt; from math import inf\n                import numpy as np\n                from spotpython.data.diabetes import Diabetes\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.fun.hyperlight import HyperLight\n                from spotpython.utils.init import fun_control_init\n                from spotpython.utils.eda import print_exp_table\n                from spotpython.spot import spot\n                from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n                PREFIX=\"000\"\n                data_set = Diabetes()\n                fun_control = fun_control_init(\n                    PREFIX=PREFIX,\n                    save_experiment=True,\n                    fun_evals=inf,\n                    max_time=1,\n                    data_set = data_set,\n                    core_model_name=\"light.regression.NNLinearRegressor\",\n                    hyperdict=LightHyperDict,\n                    _L_in=10,\n                    _L_out=1,\n                    TENSORBOARD_CLEAN=True,\n                    tensorboard_log=True,\n                    seed=42,)\n                print_exp_table(fun_control)\n                X = get_default_hyperparameters_as_array(fun_control)\n                # set epochs to 2^8:\n                X[0, 1] = 8\n                # set patience to 2^10:\n                X[0, 7] = 10\n                print(f\"X: {X}\")\n                # combine X and X to a np.array with shape (2, n_hyperparams)\n                # so that two values are returned\n                X = np.vstack((X, X))\n                hyper_light = HyperLight(seed=125, log_level=50)\n                hyper_light.fun(X, fun_control)\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        xai_res = np.array([], dtype=float)\n        xai_attr = np.nan\n        self.check_X_shape(X=X, fun_control=fun_control)\n        var_dict = assign_values(X, get_var_name(fun_control))\n        # type information and transformations are considered in generate_one_config_from_var_dict:\n        for config in generate_one_config_from_var_dict(var_dict, fun_control):\n            if fun_control[\"show_config\"]:\n                print(\"\\nIn fun(): config:\")\n                pprint.pprint(config)\n            logger.debug(f\"\\nconfig: {config}\")\n            # extract parameters like epochs, batch_size, lr, etc. from config\n            # config_id = generate_config_id(config)\n            try:\n                logger.debug(\"fun: Calling train_model\")\n                df_eval, xai_attr = train_model_xai(config, fun_control)\n                logger.debug(\"fun: train_model returned\")\n            except Exception as err:\n                if fun_control[\"verbosity\"] &gt; 0:\n                    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n                    if fun_control[\"verbosity\"] &gt; 1:\n                        pprint.pprint(fun_control)\n                    print(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                    print(\"Setting df_eval to np.nan\\n\")\n                logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                logger.error(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            # Multiply results by the weights. Positive weights mean that the result is to be minimized.\n            # Negative weights mean that the result is to be maximized, e.g., accuracy.\n            z_val = fun_control[\"weights\"] * df_eval\n\n            # Append, since several configurations can be evaluated at once.\n            z_res = np.append(z_res, z_val)\n            xai_res = np.append(xai_res, xai_attr)\n\n            # Combine z_res and xai_res into a single array\n            combined_res = np.column_stack((z_res, xai_res))\n\n            # print(\"combined: \", combined_res)\n            # print(\"shape: \", combined_res.shape)\n\n        return combined_res\n</code></pre>"},{"location":"reference/spotpython/fun/xai_hyperlight/#spotpython.fun.xai_hyperlight.XAI_HyperLight.check_X_shape","title":"<code>check_X_shape(X, fun_control)</code>","text":"<p>Checks the shape of the input array X and raises an exception if it is not valid.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: input array with valid shape.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if the shape of the input array is not valid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.init import fun_control_init\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    from spotpython.fun.hyperlight import HyperLight\n    from spotpython.hyperparameters.values import get_var_name\n    fun_control = fun_control_init()\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n    hyper_light = HyperLight(seed=126, log_level=50)\n    n_hyperparams = len(get_var_name(fun_control))\n    # generate a random np.array X with shape (2, n_hyperparams)\n    X = np.random.rand(2, n_hyperparams)\n    X == hyper_light.check_X_shape(X, fun_control)\n    array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n    [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n</code></pre> Source code in <code>spotpython/fun/xai_hyperlight.py</code> <pre><code>def check_X_shape(self, X: np.ndarray, fun_control: dict) -&gt; np.ndarray:\n    \"\"\"\n    Checks the shape of the input array X and raises an exception if it is not valid.\n\n    Args:\n        X (np.ndarray):\n            input array.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        np.ndarray:\n            input array with valid shape.\n\n    Raises:\n        Exception:\n            if the shape of the input array is not valid.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.init import fun_control_init\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            from spotpython.fun.hyperlight import HyperLight\n            from spotpython.hyperparameters.values import get_var_name\n            fun_control = fun_control_init()\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n            hyper_light = HyperLight(seed=126, log_level=50)\n            n_hyperparams = len(get_var_name(fun_control))\n            # generate a random np.array X with shape (2, n_hyperparams)\n            X = np.random.rand(2, n_hyperparams)\n            X == hyper_light.check_X_shape(X, fun_control)\n            array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n            [ True,  True,  True,  True,  True,  True,  True,  True,  True]])\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(get_var_name(fun_control)):\n        raise Exception(\"Invalid shape of input array X.\")\n    return X\n</code></pre>"},{"location":"reference/spotpython/fun/xai_hyperlight/#spotpython.fun.xai_hyperlight.XAI_HyperLight.fun","title":"<code>fun(X, fun_control=None)</code>","text":"<p>Evaluates the function for the given input array X and control parameters. Calls the train_model function from spotpython.light.trainmodel to train the model and evaluate the results.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing the evaluation results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from math import inf\n    import numpy as np\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.fun.hyperlight import HyperLight\n    from spotpython.utils.init import fun_control_init\n    from spotpython.utils.eda import print_exp_table\n    from spotpython.spot import spot\n    from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n    PREFIX=\"000\"\n    data_set = Diabetes()\n    fun_control = fun_control_init(\n        PREFIX=PREFIX,\n        save_experiment=True,\n        fun_evals=inf,\n        max_time=1,\n        data_set = data_set,\n        core_model_name=\"light.regression.NNLinearRegressor\",\n        hyperdict=LightHyperDict,\n        _L_in=10,\n        _L_out=1,\n        TENSORBOARD_CLEAN=True,\n        tensorboard_log=True,\n        seed=42,)\n    print_exp_table(fun_control)\n    X = get_default_hyperparameters_as_array(fun_control)\n    # set epochs to 2^8:\n    X[0, 1] = 8\n    # set patience to 2^10:\n    X[0, 7] = 10\n    print(f\"X: {X}\")\n    # combine X and X to a np.array with shape (2, n_hyperparams)\n    # so that two values are returned\n    X = np.vstack((X, X))\n    hyper_light = HyperLight(seed=125, log_level=50)\n    hyper_light.fun(X, fun_control)\n</code></pre> Source code in <code>spotpython/fun/xai_hyperlight.py</code> <pre><code>def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates the function for the given input array X and control parameters.\n    Calls the train_model function from spotpython.light.trainmodel\n    to train the model and evaluate the results.\n\n    Args:\n        X (np.ndarray):\n            input array.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        (np.ndarray):\n            array containing the evaluation results.\n\n    Examples:\n        &gt;&gt;&gt; from math import inf\n            import numpy as np\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.fun.hyperlight import HyperLight\n            from spotpython.utils.init import fun_control_init\n            from spotpython.utils.eda import print_exp_table\n            from spotpython.spot import spot\n            from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n            PREFIX=\"000\"\n            data_set = Diabetes()\n            fun_control = fun_control_init(\n                PREFIX=PREFIX,\n                save_experiment=True,\n                fun_evals=inf,\n                max_time=1,\n                data_set = data_set,\n                core_model_name=\"light.regression.NNLinearRegressor\",\n                hyperdict=LightHyperDict,\n                _L_in=10,\n                _L_out=1,\n                TENSORBOARD_CLEAN=True,\n                tensorboard_log=True,\n                seed=42,)\n            print_exp_table(fun_control)\n            X = get_default_hyperparameters_as_array(fun_control)\n            # set epochs to 2^8:\n            X[0, 1] = 8\n            # set patience to 2^10:\n            X[0, 7] = 10\n            print(f\"X: {X}\")\n            # combine X and X to a np.array with shape (2, n_hyperparams)\n            # so that two values are returned\n            X = np.vstack((X, X))\n            hyper_light = HyperLight(seed=125, log_level=50)\n            hyper_light.fun(X, fun_control)\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    xai_res = np.array([], dtype=float)\n    xai_attr = np.nan\n    self.check_X_shape(X=X, fun_control=fun_control)\n    var_dict = assign_values(X, get_var_name(fun_control))\n    # type information and transformations are considered in generate_one_config_from_var_dict:\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        if fun_control[\"show_config\"]:\n            print(\"\\nIn fun(): config:\")\n            pprint.pprint(config)\n        logger.debug(f\"\\nconfig: {config}\")\n        # extract parameters like epochs, batch_size, lr, etc. from config\n        # config_id = generate_config_id(config)\n        try:\n            logger.debug(\"fun: Calling train_model\")\n            df_eval, xai_attr = train_model_xai(config, fun_control)\n            logger.debug(\"fun: train_model returned\")\n        except Exception as err:\n            if fun_control[\"verbosity\"] &gt; 0:\n                print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n                if fun_control[\"verbosity\"] &gt; 1:\n                    pprint.pprint(fun_control)\n                print(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                print(\"Setting df_eval to np.nan\\n\")\n            logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n            logger.error(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        # Multiply results by the weights. Positive weights mean that the result is to be minimized.\n        # Negative weights mean that the result is to be maximized, e.g., accuracy.\n        z_val = fun_control[\"weights\"] * df_eval\n\n        # Append, since several configurations can be evaluated at once.\n        z_res = np.append(z_res, z_val)\n        xai_res = np.append(xai_res, xai_attr)\n\n        # Combine z_res and xai_res into a single array\n        combined_res = np.column_stack((z_res, xai_res))\n\n        # print(\"combined: \", combined_res)\n        # print(\"shape: \", combined_res.shape)\n\n    return combined_res\n</code></pre>"},{"location":"reference/spotpython/gp/covar/","title":"covar","text":""},{"location":"reference/spotpython/gp/covar/#spotpython.gp.covar.covar_sep","title":"<code>covar_sep(col, X1, n1, X2, n2, d, g)</code>","text":"<p>Calculate the correlation (K) between X1 and X2 with a separable power exponential correlation function with range d and nugget g.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>int</code> <p>Number of columns in the input matrices X1 and X2.</p> required <code>X1</code> <code>ndarray</code> <p>First input matrix of shape (n1, col).</p> required <code>n1</code> <code>int</code> <p>Number of rows in the first input matrix X1.</p> required <code>X2</code> <code>ndarray</code> <p>Second input matrix of shape (n2, col).</p> required <code>n2</code> <code>int</code> <p>Number of rows in the second input matrix X2.</p> required <code>d</code> <code>ndarray</code> <p>Array of length col representing the range parameters.</p> required <code>g</code> <code>float</code> <p>Nugget parameter.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The calculated covariance matrix K of shape (n1, n2).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.covar import covar_sep\n&gt;&gt;&gt; col = 2\n&gt;&gt;&gt; X1 = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; n1 = 3\n&gt;&gt;&gt; X2 = np.array([[7, 8], [9, 10]])\n&gt;&gt;&gt; n2 = 2\n&gt;&gt;&gt; d = np.array([1.0, 1.0])\n&gt;&gt;&gt; g = 0.1\n&gt;&gt;&gt; K = covar_sep(col, X1, n1, X2, n2, d, g)\n&gt;&gt;&gt; print(K)\n[[1.12535175e-07 3.72007598e-44]\n [3.72007598e-44 1.38389653e-87]\n [1.38389653e-87 5.14820022e-131]]\n</code></pre> Source code in <code>spotpython/gp/covar.py</code> <pre><code>def covar_sep(col, X1, n1, X2, n2, d, g) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the correlation (K) between X1 and X2 with\n    a separable power exponential correlation function\n    with range d and nugget g.\n\n    Args:\n        col (int): Number of columns in the input matrices X1 and X2.\n        X1 (ndarray): First input matrix of shape (n1, col).\n        n1 (int): Number of rows in the first input matrix X1.\n        X2 (ndarray): Second input matrix of shape (n2, col).\n        n2 (int): Number of rows in the second input matrix X2.\n        d (ndarray): Array of length col representing the range parameters.\n        g (float): Nugget parameter.\n\n    Returns:\n        ndarray: The calculated covariance matrix K of shape (n1, n2).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.covar import covar_sep\n        &gt;&gt;&gt; col = 2\n        &gt;&gt;&gt; X1 = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; n1 = 3\n        &gt;&gt;&gt; X2 = np.array([[7, 8], [9, 10]])\n        &gt;&gt;&gt; n2 = 2\n        &gt;&gt;&gt; d = np.array([1.0, 1.0])\n        &gt;&gt;&gt; g = 0.1\n        &gt;&gt;&gt; K = covar_sep(col, X1, n1, X2, n2, d, g)\n        &gt;&gt;&gt; print(K)\n        [[1.12535175e-07 3.72007598e-44]\n         [3.72007598e-44 1.38389653e-87]\n         [1.38389653e-87 5.14820022e-131]]\n    \"\"\"\n    K = np.zeros((n1, n2))\n    X1 = prepare_X(X1)\n    X2 = prepare_X(X2)\n\n    for i in range(n1):\n        for j in range(n2):\n            K[i, j] = 0.0\n            for k in range(col):\n                K[i, j] += (X1[i, k] - X2[j, k]) ** 2 / d[k]\n            if i == j and K[i, j] == 0.0:\n                K[i, j] = 1.0 + g\n            else:\n                K[i, j] = np.exp(0.0 - K[i, j])\n\n    return K\n</code></pre>"},{"location":"reference/spotpython/gp/covar/#spotpython.gp.covar.covar_sep_symm","title":"<code>covar_sep_symm(col, X, n, d, g)</code>","text":"<p>Calculate the correlation (K) between X1 and X2 with a separable power exponential correlation function with range d and nugget g.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>int</code> <p>Number of columns in the input matrix X (features).</p> required <code>X</code> <code>ndarray</code> <p>Input matrix of shape (n, col).</p> required <code>n</code> <code>int</code> <p>Number of rows in the input matrix X.</p> required <code>d</code> <code>ndarray</code> <p>Array of length col representing the range parameters, shape (col,).</p> required <code>g</code> <code>float</code> <p>Nugget parameter.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The calculated covariance matrix K of shape (n, n).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.gp.covar import covar_sep_symm\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; col = 2\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; n = 3\n&gt;&gt;&gt; d = np.array([1.0, 1.0])\n&gt;&gt;&gt; g = 0.1\n&gt;&gt;&gt; K = covar_sep_symm(col, X, n, d, g)\n&gt;&gt;&gt; print(K)\n[[1.1        0.01831564 0.00012341]\n [0.01831564 1.1        0.01831564]\n [0.00012341 0.01831564 1.1       ]]\n</code></pre> Source code in <code>spotpython/gp/covar.py</code> <pre><code>def covar_sep_symm(col, X, n, d, g) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the correlation (K) between X1 and X2 with a separable power exponential correlation function with range d and nugget g.\n\n    Args:\n        col (int): Number of columns in the input matrix X (features).\n        X (ndarray): Input matrix of shape (n, col).\n        n (int): Number of rows in the input matrix X.\n        d (ndarray): Array of length col representing the range parameters, shape (col,).\n        g (float): Nugget parameter.\n\n    Returns:\n        ndarray: The calculated covariance matrix K of shape (n, n).\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.gp.covar import covar_sep_symm\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; col = 2\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; n = 3\n        &gt;&gt;&gt; d = np.array([1.0, 1.0])\n        &gt;&gt;&gt; g = 0.1\n        &gt;&gt;&gt; K = covar_sep_symm(col, X, n, d, g)\n        &gt;&gt;&gt; print(K)\n        [[1.1        0.01831564 0.00012341]\n         [0.01831564 1.1        0.01831564]\n         [0.00012341 0.01831564 1.1       ]]\n    \"\"\"\n    K = np.zeros((n, n))\n    X = prepare_X(X)\n\n    # calculate the covariance matrix K\n    for i in range(n):\n        K[i, i] = 1.0 + g\n        for j in range(i + 1, n):\n            K[i, j] = 0.0\n            for k in range(col):\n                K[i, j] += (X[i, k] - X[j, k]) ** 2 / d[k]\n            K[i, j] = np.exp(-K[i, j])\n            K[j, i] = K[i, j]\n\n    return K\n</code></pre>"},{"location":"reference/spotpython/gp/covar/#spotpython.gp.covar.diff_covar_sep","title":"<code>diff_covar_sep(col, X1, n1, X2, n2, d, K)</code>","text":"<p>Calculate the first and second derivative (wrt d) of the correlation (K) between X1 and X2 with a separable power exponential correlation function with range d and nugget g (though g not needed).</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>int</code> <p>Number of columns in the input matrices X1 and X2.</p> required <code>X1</code> <code>ndarray</code> <p>First input matrix of shape (n1, col).</p> required <code>n1</code> <code>int</code> <p>Number of rows in the first input matrix X1.</p> required <code>X2</code> <code>ndarray</code> <p>Second input matrix of shape (n2, col).</p> required <code>n2</code> <code>int</code> <p>Number of rows in the second input matrix X2.</p> required <code>d</code> <code>ndarray</code> <p>Array of length col representing the range parameters.</p> required <code>K</code> <code>ndarray</code> <p>Covariance matrix of shape (n1, n2).</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The calculated derivative covariance matrix dK of shape (col, n1, n2).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; col = 2\n&gt;&gt;&gt; X1 = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; n1 = 3\n&gt;&gt;&gt; X2 = np.array([[7, 8], [9, 10]])\n&gt;&gt;&gt; n2 = 2\n&gt;&gt;&gt; d = np.array([1.0, 1.0])\n&gt;&gt;&gt; K = np.exp(-np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]))\n&gt;&gt;&gt; dK = diff_covar_sep(col, X1, n1, X2, n2, d, K)\n&gt;&gt;&gt; print(dK)\n[[[1.12535175e-07 3.72007598e-44]\n  [3.72007598e-44 1.38389653e-87]\n  [1.38389653e-87 5.14820022e-131]]\n [[1.12535175e-07 3.72007598e-44]\n  [3.72007598e-44 1.38389653e-87]\n  [1.38389653e-87 5.14820022e-131]]]\n</code></pre> Source code in <code>spotpython/gp/covar.py</code> <pre><code>def diff_covar_sep(col, X1, n1, X2, n2, d, K) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the first and second derivative (wrt d) of the correlation (K)\n    between X1 and X2 with a separable power exponential correlation function\n    with range d and nugget g (though g not needed).\n\n    Args:\n        col (int): Number of columns in the input matrices X1 and X2.\n        X1 (ndarray): First input matrix of shape (n1, col).\n        n1 (int): Number of rows in the first input matrix X1.\n        X2 (ndarray): Second input matrix of shape (n2, col).\n        n2 (int): Number of rows in the second input matrix X2.\n        d (ndarray): Array of length col representing the range parameters.\n        K (ndarray): Covariance matrix of shape (n1, n2).\n\n    Returns:\n        ndarray: The calculated derivative covariance matrix dK of shape (col, n1, n2).\n\n    Examples:\n        &gt;&gt;&gt; col = 2\n        &gt;&gt;&gt; X1 = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; n1 = 3\n        &gt;&gt;&gt; X2 = np.array([[7, 8], [9, 10]])\n        &gt;&gt;&gt; n2 = 2\n        &gt;&gt;&gt; d = np.array([1.0, 1.0])\n        &gt;&gt;&gt; K = np.exp(-np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]))\n        &gt;&gt;&gt; dK = diff_covar_sep(col, X1, n1, X2, n2, d, K)\n        &gt;&gt;&gt; print(dK)\n        [[[1.12535175e-07 3.72007598e-44]\n          [3.72007598e-44 1.38389653e-87]\n          [1.38389653e-87 5.14820022e-131]]\n         [[1.12535175e-07 3.72007598e-44]\n          [3.72007598e-44 1.38389653e-87]\n          [1.38389653e-87 5.14820022e-131]]]\n    \"\"\"\n    X1 = prepare_X(X1)\n    X2 = prepare_X(X2)\n    dK = np.zeros((col, n1, n2))\n\n    for k in range(col):\n        d2k = d[k] ** 2\n        for i in range(n1):\n            for j in range(n2):\n                dK[k, i, j] = K[i, j] * ((X1[i, k] - X2[j, k]) ** 2) / d2k\n\n    return dK\n</code></pre>"},{"location":"reference/spotpython/gp/covar/#spotpython.gp.covar.diff_covar_sep_symm","title":"<code>diff_covar_sep_symm(col, X, n, d, K)</code>","text":"<p>Calculate the first and second derivative (wrt d) of the correlation (K) between X1 and X2 with a separable power exponential correlation function with range d and nugget g (though g not needed) \u2013 assumes symmetric matrix.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>int</code> <p>Number of columns in the input matrix X.</p> required <code>X</code> <code>ndarray</code> <p>Input matrix of shape (n, col).</p> required <code>n</code> <code>int</code> <p>Number of rows in the input matrix X.</p> required <code>d</code> <code>ndarray</code> <p>Array of length col representing the range parameters.</p> required <code>K</code> <code>ndarray</code> <p>Covariance matrix of shape (n, n).</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The calculated derivative covariance matrix dK of shape (col, n, n).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; col = 2\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; n = 3\n&gt;&gt;&gt; d = np.array([1.0, 1.0])\n&gt;&gt;&gt; K = np.exp(-np.array([[0.0, 1.0, 2.0], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0]]))\n&gt;&gt;&gt; dK = diff_covar_sep_symm(col, X, n, d, K)\n&gt;&gt;&gt; print(dK)\n[[[0.         0.36787944 0.01831564]\n  [0.36787944 0.         0.36787944]\n  [0.01831564 0.36787944 0.        ]]\n [[0.         0.36787944 0.01831564]\n  [0.36787944 0.         0.36787944]\n  [0.01831564 0.36787944 0.        ]]]\n</code></pre> Source code in <code>spotpython/gp/covar.py</code> <pre><code>def diff_covar_sep_symm(col, X, n, d, K) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the first and second derivative (wrt d) of the correlation (K)\n    between X1 and X2 with a separable power exponential correlation function\n    with range d and nugget g (though g not needed) -- assumes symmetric matrix.\n\n    Args:\n        col (int): Number of columns in the input matrix X.\n        X (ndarray): Input matrix of shape (n, col).\n        n (int): Number of rows in the input matrix X.\n        d (ndarray): Array of length col representing the range parameters.\n        K (ndarray): Covariance matrix of shape (n, n).\n\n    Returns:\n        ndarray: The calculated derivative covariance matrix dK of shape (col, n, n).\n\n    Examples:\n        &gt;&gt;&gt; col = 2\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; n = 3\n        &gt;&gt;&gt; d = np.array([1.0, 1.0])\n        &gt;&gt;&gt; K = np.exp(-np.array([[0.0, 1.0, 2.0], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0]]))\n        &gt;&gt;&gt; dK = diff_covar_sep_symm(col, X, n, d, K)\n        &gt;&gt;&gt; print(dK)\n        [[[0.         0.36787944 0.01831564]\n          [0.36787944 0.         0.36787944]\n          [0.01831564 0.36787944 0.        ]]\n         [[0.         0.36787944 0.01831564]\n          [0.36787944 0.         0.36787944]\n          [0.01831564 0.36787944 0.        ]]]\n    \"\"\"\n    X = prepare_X(X)\n    dK = np.zeros((col, n, n))\n\n    for k in range(col):\n        d2k = d[k] ** 2\n        for i in range(n):\n            for j in range(i + 1, n):\n                dK[k, i, j] = dK[k, j, i] = K[i, j] * ((X[i, k] - X[j, k]) ** 2) / d2k\n            dK[k, i, i] = 0.0\n\n    return dK\n</code></pre>"},{"location":"reference/spotpython/gp/distances/","title":"distances","text":""},{"location":"reference/spotpython/gp/distances/#spotpython.gp.distances.covar_anisotropic","title":"<code>covar_anisotropic(X1=None, X2=None, d=None, g=None)</code>","text":"<p>Calculate the separable covariance matrix between the rows of X1 and X2 with lengthscale d and nugget g.</p> <p>Parameters:</p> Name Type Description Default <code>X1</code> <code>ndarray</code> <p>First input matrix.</p> <code>None</code> <code>X2</code> <code>ndarray</code> <p>Second input matrix (optional).</p> <code>None</code> <code>d</code> <code>ndarray</code> <p>Array of lengthscale parameters.</p> <code>None</code> <code>g</code> <code>float</code> <p>Nugget parameter.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Covariance matrix K.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.distances import covar_anisotropic\n&gt;&gt;&gt; X1 = np.array([[1.0, 2.0], [3.0, 4.0]])\n&gt;&gt;&gt; X2 = np.array([[5.0, 6.0], [7.0, 8.0]])\n&gt;&gt;&gt; d = np.array([1.0, 1.0])\n&gt;&gt;&gt; g = 0.1\n&gt;&gt;&gt; K_symm = covar_anisotropic(X1=X1, d=d, g=g)\n&gt;&gt;&gt; print(K_symm)\n[[1.1        0.36787944]\n [0.36787944 1.1       ]]\n&gt;&gt;&gt; K = covar_anisotropic(X1=X1, X2=X2, d=d, g=g)\n&gt;&gt;&gt; print(K)\n[[0.00012341 0.00033546]\n [0.00033546 0.000911\n</code></pre> Source code in <code>spotpython/gp/distances.py</code> <pre><code>def covar_anisotropic(X1=None, X2=None, d=None, g=None) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the separable covariance matrix between the rows of X1 and X2 with lengthscale d and nugget g.\n\n    Args:\n        X1 (np.ndarray): First input matrix.\n        X2 (np.ndarray): Second input matrix (optional).\n        d (np.ndarray): Array of lengthscale parameters.\n        g (float): Nugget parameter.\n\n    Returns:\n        np.ndarray: Covariance matrix K.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.distances import covar_anisotropic\n        &gt;&gt;&gt; X1 = np.array([[1.0, 2.0], [3.0, 4.0]])\n        &gt;&gt;&gt; X2 = np.array([[5.0, 6.0], [7.0, 8.0]])\n        &gt;&gt;&gt; d = np.array([1.0, 1.0])\n        &gt;&gt;&gt; g = 0.1\n        &gt;&gt;&gt; K_symm = covar_anisotropic(X1=X1, d=d, g=g)\n        &gt;&gt;&gt; print(K_symm)\n        [[1.1        0.36787944]\n         [0.36787944 1.1       ]]\n        &gt;&gt;&gt; K = covar_anisotropic(X1=X1, X2=X2, d=d, g=g)\n        &gt;&gt;&gt; print(K)\n        [[0.00012341 0.00033546]\n         [0.00033546 0.000911\n    \"\"\"\n    # Convert pandas dataframes to numpy arrays\n    X1 = np.asarray(X1)\n    if X2 is not None:\n        X2 = np.asarray(X2)\n    if X1 is None:\n        raise ValueError(\"X1 cannot be None\")\n\n    if not isinstance(X1, np.ndarray):\n        if X2 is None:\n            raise ValueError(\"X2 cannot be None in this context\")\n        m = X2.shape[1]\n        X1 = np.reshape(X1, (-1, m))\n    else:\n        m = X1.shape[1]\n    n1 = X1.shape[0]\n\n    if len(d) != m:\n        raise ValueError(\"bad d argument\")\n    if not isinstance(g, float):\n        raise ValueError(\"bad g argument\")\n\n    if X2 is None:\n        # Calculate K using covar_sep_symm_R\n        # K = covar_sep_symm_R(m, X1.flatten(), n1, d, g)\n        K = covar_sep_symm(m, X1, n1, d, g)\n        return K\n    else:\n        if X1.shape[1] != X2.shape[1]:\n            raise ValueError(\"col dim mismatch for X1 &amp; X2\")\n\n        X2 = np.asarray(X2)\n        n2 = X2.shape[0]\n\n        # Calculate K using covar_sep_R\n        # K = covar_sep_R(m, X1.flatten(), n1, X2.flatten(), n2, d, g)\n        K = covar_sep(m, X1, n1, X2, n2, d, g)\n        return K\n</code></pre>"},{"location":"reference/spotpython/gp/distances/#spotpython.gp.distances.dist","title":"<code>dist(X1, X2=None)</code>","text":"<p>Calculate the distance matrix between the rows of X1 and X2, or between X1 and itself when X2 is None.</p> <p>Parameters:</p> Name Type Description Default <code>X1</code> <code>ndarray</code> <p>First input matrix.</p> required <code>X2</code> <code>ndarray</code> <p>Second input matrix.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Distance matrix D.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.distances import dist\n&gt;&gt;&gt; X1 = np.array([[1.0, 2.0], [3.0, 4.0]])\n&gt;&gt;&gt; X2 = np.array([[5.0, 6.0], [7.0, 8.0]])\n&gt;&gt;&gt; D_symm = dist(X1)\n&gt;&gt;&gt; print(D_symm)\n[[ 0.  8.]\n [ 8.  0.]]\n&gt;&gt;&gt; D = dist(X1, X2)\n&gt;&gt;&gt; print(D)\n    [[32.  8.]\n    [18.  2.]]\n</code></pre> Source code in <code>spotpython/gp/distances.py</code> <pre><code>def dist(X1, X2=None) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the distance matrix between the rows of X1 and X2, or between X1 and itself when X2 is None.\n\n    Args:\n        X1 (np.ndarray): First input matrix.\n        X2 (np.ndarray, optional): Second input matrix.\n\n    Returns:\n        np.ndarray: Distance matrix D.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.distances import dist\n        &gt;&gt;&gt; X1 = np.array([[1.0, 2.0], [3.0, 4.0]])\n        &gt;&gt;&gt; X2 = np.array([[5.0, 6.0], [7.0, 8.0]])\n        &gt;&gt;&gt; D_symm = dist(X1)\n        &gt;&gt;&gt; print(D_symm)\n        [[ 0.  8.]\n         [ 8.  0.]]\n        &gt;&gt;&gt; D = dist(X1, X2)\n        &gt;&gt;&gt; print(D)\n            [[32.  8.]\n            [18.  2.]]\n    \"\"\"\n    # Coerce arguments and extract dimensions\n    X1 = np.asarray(X1)\n    n1, m = X1.shape\n\n    if X2 is None:\n        # Calculate D using distance_symm_R\n        D = distance_symm_R(X1.flatten(), n1, m)\n        return D\n    else:\n        # Coerce arguments and extract dimensions\n        X2 = np.asarray(X2)\n        n2 = X2.shape[0]\n\n        # Check inputs\n        if X1.shape[1] != X2.shape[1]:\n            raise ValueError(\"col dim mismatch for X1 &amp; X2\")\n\n        # Calculate D using distance_R\n        D = distance_R(X1.flatten(), n1, X2.flatten(), n2, m)\n        return D\n</code></pre>"},{"location":"reference/spotpython/gp/distances/#spotpython.gp.distances.distance","title":"<code>distance(X1, n1, X2, n2, m)</code>","text":"<p>Calculate the distance matrix (D) between X1 and X2.</p> <p>Parameters:</p> Name Type Description Default <code>X1</code> <code>ndarray</code> <p>First input matrix of shape (n1, m).</p> required <code>n1</code> <code>int</code> <p>Number of rows in X1.</p> required <code>X2</code> <code>ndarray</code> <p>Second input matrix of shape (n2, m).</p> required <code>n2</code> <code>int</code> <p>Number of rows in X2.</p> required <code>m</code> <code>int</code> <p>Number of columns (features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Distance matrix D of shape (n1, n2).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.distances import distance\n&gt;&gt;&gt; X1 = np.array([[1.0, 2.0], [3.0, 4.0]])\n&gt;&gt;&gt; n1 = 2\n&gt;&gt;&gt; X2 = np.array([[5.0, 6.0], [7.0, 8.0]])\n&gt;&gt;&gt; n2 = 2\n&gt;&gt;&gt; m = 2\n&gt;&gt;&gt; D_out = distance(X1, n1, X2, n2, m)\n&gt;&gt;&gt; print(D_out)\n[[32.  8.]\n [18.  2.]]\n</code></pre> Source code in <code>spotpython/gp/distances.py</code> <pre><code>def distance(X1, n1, X2, n2, m) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the distance matrix (D) between X1 and X2.\n\n    Args:\n        X1 (np.ndarray): First input matrix of shape (n1, m).\n        n1 (int): Number of rows in X1.\n        X2 (np.ndarray): Second input matrix of shape (n2, m).\n        n2 (int): Number of rows in X2.\n        m (int): Number of columns (features).\n\n    Returns:\n        np.ndarray: Distance matrix D of shape (n1, n2).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.distances import distance\n        &gt;&gt;&gt; X1 = np.array([[1.0, 2.0], [3.0, 4.0]])\n        &gt;&gt;&gt; n1 = 2\n        &gt;&gt;&gt; X2 = np.array([[5.0, 6.0], [7.0, 8.0]])\n        &gt;&gt;&gt; n2 = 2\n        &gt;&gt;&gt; m = 2\n        &gt;&gt;&gt; D_out = distance(X1, n1, X2, n2, m)\n        &gt;&gt;&gt; print(D_out)\n        [[32.  8.]\n         [18.  2.]]\n    \"\"\"\n    D = np.zeros((n1, n2))\n\n    for i in range(n1):\n        for j in range(n2):\n            D[i, j] = 0.0\n            for k in range(m):\n                D[i, j] += (X1[i, k] - X2[j, k]) ** 2\n\n    return D\n</code></pre>"},{"location":"reference/spotpython/gp/distances/#spotpython.gp.distances.distance_R","title":"<code>distance_R(X1_in, n1_in, X2_in, n2_in, m_in)</code>","text":"<p>Calculate the distance matrix between the rows of X1 and X2.</p> <p>Parameters:</p> Name Type Description Default <code>X1_in</code> <code>ndarray</code> <p>First input matrix of shape (n1, m).</p> required <code>n1_in</code> <code>int</code> <p>Number of rows in X1.</p> required <code>X2_in</code> <code>ndarray</code> <p>Second input matrix of shape (n2, m).</p> required <code>n2_in</code> <code>int</code> <p>Number of rows in X2.</p> required <code>m_in</code> <code>int</code> <p>Number of columns (features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Distance matrix D of shape (n1, n2).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.distances import distance_R\n&gt;&gt;&gt; X1_in = np.array([1.0, 2.0, 3.0, 4.0])\n&gt;&gt;&gt; n1_in = 2\n&gt;&gt;&gt; X2_in = np.array([5.0, 6.0, 7.0, 8.0])\n&gt;&gt;&gt; n2_in = 2\n&gt;&gt;&gt; m_in = 2\n&gt;&gt;&gt; D_out = distance_R(X1_in, n1_in, X2_in, n2_in, m_in)\n&gt;&gt;&gt; print(D_out)\n[[32.  8.]\n [18.  2.]]\n</code></pre> Source code in <code>spotpython/gp/distances.py</code> <pre><code>def distance_R(X1_in, n1_in, X2_in, n2_in, m_in) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the distance matrix between the rows of X1 and X2.\n\n    Args:\n        X1_in (np.ndarray): First input matrix of shape (n1, m).\n        n1_in (int): Number of rows in X1.\n        X2_in (np.ndarray): Second input matrix of shape (n2, m).\n        n2_in (int): Number of rows in X2.\n        m_in (int): Number of columns (features).\n\n    Returns:\n        np.ndarray: Distance matrix D of shape (n1, n2).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.distances import distance_R\n        &gt;&gt;&gt; X1_in = np.array([1.0, 2.0, 3.0, 4.0])\n        &gt;&gt;&gt; n1_in = 2\n        &gt;&gt;&gt; X2_in = np.array([5.0, 6.0, 7.0, 8.0])\n        &gt;&gt;&gt; n2_in = 2\n        &gt;&gt;&gt; m_in = 2\n        &gt;&gt;&gt; D_out = distance_R(X1_in, n1_in, X2_in, n2_in, m_in)\n        &gt;&gt;&gt; print(D_out)\n        [[32.  8.]\n         [18.  2.]]\n    \"\"\"\n    # Make matrix bones\n    X1 = np.reshape(X1_in, (n1_in, m_in))\n    X2 = np.reshape(X2_in, (n2_in, m_in))\n    D = distance(X1, n1_in, X2, n2_in, m_in)\n    return D\n</code></pre>"},{"location":"reference/spotpython/gp/distances/#spotpython.gp.distances.distance_symm_R","title":"<code>distance_symm_R(X_in, n_in, m_in)</code>","text":"<p>Calculate the distance matrix between the rows of X and itself, with output in the symmetric D_out matrix.</p> <p>Parameters:</p> Name Type Description Default <code>X_in</code> <code>ndarray</code> <p>Input matrix of shape (n, m).</p> required <code>n_in</code> <code>int</code> <p>Number of rows in X.</p> required <code>m_in</code> <code>int</code> <p>Number of columns (features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Symmetric distance matrix D of shape (n, n).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.distances import distance_symm_R\n&gt;&gt;&gt; X_in = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n&gt;&gt;&gt; n_in = 3\n&gt;&gt;&gt; m_in = 2\n&gt;&gt;&gt; D_out = distance_symm_R(X_in, n_in, m_in)\n&gt;&gt;&gt; print(D_out)\n[[ 0.  8. 32.]\n [ 8.  0.  8.]\n [32.  8.  0.]]\n</code></pre> Source code in <code>spotpython/gp/distances.py</code> <pre><code>def distance_symm_R(X_in, n_in, m_in) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the distance matrix between the rows of X and itself, with output in the symmetric D_out matrix.\n\n    Args:\n        X_in (np.ndarray): Input matrix of shape (n, m).\n        n_in (int): Number of rows in X.\n        m_in (int): Number of columns (features).\n\n    Returns:\n        np.ndarray: Symmetric distance matrix D of shape (n, n).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.distances import distance_symm_R\n        &gt;&gt;&gt; X_in = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n        &gt;&gt;&gt; n_in = 3\n        &gt;&gt;&gt; m_in = 2\n        &gt;&gt;&gt; D_out = distance_symm_R(X_in, n_in, m_in)\n        &gt;&gt;&gt; print(D_out)\n        [[ 0.  8. 32.]\n         [ 8.  0.  8.]\n         [32.  8.  0.]]\n    \"\"\"\n    n = n_in\n    m = m_in\n\n    # Make matrix bones\n    X = new_matrix_bones(X_in, n, m)\n    D = np.zeros((n, n))\n\n    # For each row of X and itself\n    for i in range(n):\n        D[i][i] = 0.0\n        for j in range(i + 1, n):\n            D[i][j] = 0.0\n            for k in range(m):\n                D[i][j] += (X[i][k] - X[j][k]) ** 2\n            D[j][i] = D[i][j]\n\n    return D\n</code></pre>"},{"location":"reference/spotpython/gp/distances/#spotpython.gp.distances.new_matrix_bones","title":"<code>new_matrix_bones(data, rows, cols)</code>","text":"<p>Create a matrix view of the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data.</p> required <code>rows</code> <code>int</code> <p>Number of rows.</p> required <code>cols</code> <code>int</code> <p>Number of columns.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Matrix view of the input data.</p> Source code in <code>spotpython/gp/distances.py</code> <pre><code>def new_matrix_bones(data, rows, cols) -&gt; np.ndarray:\n    \"\"\"\n    Create a matrix view of the given data.\n\n    Args:\n        data (np.ndarray): Input data.\n        rows (int): Number of rows.\n        cols (int): Number of columns.\n\n    Returns:\n        np.ndarray: Matrix view of the input data.\n    \"\"\"\n    return np.reshape(data, (rows, cols))\n</code></pre>"},{"location":"reference/spotpython/gp/functions/","title":"functions","text":""},{"location":"reference/spotpython/gp/functions/#spotpython.gp.functions.f2d","title":"<code>f2d(x, y=None)</code>","text":"<p>Simple 2-d test function used in Gramacy &amp; Apley (2015).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The x-coordinates.</p> required <code>y</code> <code>ndarray</code> <p>The y-coordinates. If None, x is assumed to be a 2D array.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The calculated z-values.</p> Source code in <code>spotpython/gp/functions.py</code> <pre><code>def f2d(x, y=None) -&gt; np.ndarray:\n    \"\"\"\n    Simple 2-d test function used in Gramacy &amp; Apley (2015).\n\n    Args:\n        x (ndarray): The x-coordinates.\n        y (ndarray, optional): The y-coordinates. If None, x is assumed to be a 2D array.\n\n    Returns:\n        ndarray: The calculated z-values.\n    \"\"\"\n    if y is None:\n        if not isinstance(x, np.ndarray) or x.ndim != 2:\n            x = np.array(x).reshape(-1, 2)\n        y = x[:, 1]\n        x = x[:, 0]\n\n    def g(z):\n        return np.exp(-((z - 1) ** 2)) + np.exp(-0.8 * (z + 1) ** 2) - 0.05 * np.sin(8 * (z + 0.1))\n\n    z = -g(x) * g(y)\n    return z\n</code></pre>"},{"location":"reference/spotpython/gp/functions/#spotpython.gp.functions.fried","title":"<code>fried(n=50, m=6)</code>","text":"<p>Generate a dataset using the Friedman function.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples.</p> <code>50</code> <code>m</code> <code>int</code> <p>Number of features (must be at least 5).</p> <code>6</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing the generated features, response values (Y), and true response values (Ytrue).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.gp.functions import fried\n&gt;&gt;&gt; data = fried(n=50, m=6)\n&gt;&gt;&gt; print(data.head())\n     X1        X2        X3        X4        X5        Y      Ytrue\n0  0.50  0.166667  0.333333  0.666667  0.833333  17.5728  17.5728\n1  0.25  0.500000  0.666667  0.166667  0.666667  16.8473  16.8473\n2  0.75  0.833333  0.000000  0.833333  0.333333  21.6731  21.6731\n3  0.00  0.333333  0.666667  0.500000  0.500000  14.6937  14.6937\n4  1.00  0.666667  0.333333  0.333333  0.166667  16.2804  16.2804\n</code></pre> Source code in <code>spotpython/gp/functions.py</code> <pre><code>def fried(n=50, m=6) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a dataset using the Friedman function.\n\n    Args:\n        n (int): Number of samples.\n        m (int): Number of features (must be at least 5).\n\n    Returns:\n        pd.DataFrame: DataFrame containing the generated features, response values (Y), and true response values (Ytrue).\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.gp.functions import fried\n        &gt;&gt;&gt; data = fried(n=50, m=6)\n        &gt;&gt;&gt; print(data.head())\n             X1        X2        X3        X4        X5        Y      Ytrue\n        0  0.50  0.166667  0.333333  0.666667  0.833333  17.5728  17.5728\n        1  0.25  0.500000  0.666667  0.166667  0.666667  16.8473  16.8473\n        2  0.75  0.833333  0.000000  0.833333  0.333333  21.6731  21.6731\n        3  0.00  0.333333  0.666667  0.500000  0.500000  14.6937  14.6937\n        4  1.00  0.666667  0.333333  0.333333  0.166667  16.2804  16.2804\n    \"\"\"\n    if m &lt; 5:\n        raise ValueError(\"must have at least 5 cols\")\n\n    # Generate Latin Hypercube samples\n    sampler = qmc.LatinHypercube(d=m)\n    X = sampler.random(n)\n\n    # Calculate the true response values\n    Ytrue = 10 * np.sin(np.pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:, 4]\n\n    # Add noise to the response values\n    Y = Ytrue + np.random.normal(0, 1, n)\n\n    # Create a DataFrame with the generated data\n    data = pd.DataFrame(X, columns=[f\"X{i+1}\" for i in range(m)])\n    data[\"Y\"] = Y\n    data[\"Ytrue\"] = Ytrue\n\n    return data\n</code></pre>"},{"location":"reference/spotpython/gp/gp/","title":"gp","text":""},{"location":"reference/spotpython/gp/gp_sep/","title":"gp_sep","text":""},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep","title":"<code>GPsep</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A class to represent a Gaussian Process with separable covariance.</p> <p>Attributes:</p> Name Type Description <code>m</code> <p>Number of input dimensions.</p> <code>n</code> <p>Number of observations.</p> <code>X</code> <p>Input data matrix.</p> <code>y</code> <p>Output data vector.</p> <code>d</code> <p>Length-scale parameters.</p> <code>g</code> <p>Nugget parameter.</p> <code>K</code> <p>Covariance matrix.</p> <code>Ki</code> <p>Inverse of covariance matrix.</p> <code>Kiy</code> <p>Product of Ki and y.</p> <code>phi</code> <p>Scalar value from y^T Ki y calculation.</p> <code>dK</code> <p>Boolean flag for calculating derivatives.</p> <code>DK</code> <p>Matrix of derivatives.</p> <code>ldetK</code> <p>Log determinant of K.</p> <code>nlsep_method</code> <p>Method for likelihood computation.</p> <code>gradnlsep_method</code> <p>Method for gradient computation.</p> <code>n_restarts_optimizer</code> <p>Number of restarts for optimization.</p> <code>samp_size</code> <p>Sample size for distance calculations.</p> <code>maxit</code> <p>Maximum number of optimization iterations.</p> <code>verbosity</code> <p>Verbosity level.</p> <code>auto_optimize</code> <p>Whether to automatically optimize hyperparameters.</p> <code>max_points</code> <p>Maximum number of points for model building.</p> <code>seed</code> <p>Random seed for reproducibility.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>class GPsep(BaseEstimator, RegressorMixin):\n    \"\"\"A class to represent a Gaussian Process with separable covariance.\n\n    Attributes:\n        m: Number of input dimensions.\n        n: Number of observations.\n        X: Input data matrix.\n        y: Output data vector.\n        d: Length-scale parameters.\n        g: Nugget parameter.\n        K: Covariance matrix.\n        Ki: Inverse of covariance matrix.\n        Kiy: Product of Ki and y.\n        phi: Scalar value from y^T Ki y calculation.\n        dK: Boolean flag for calculating derivatives.\n        DK: Matrix of derivatives.\n        ldetK: Log determinant of K.\n        nlsep_method: Method for likelihood computation.\n        gradnlsep_method: Method for gradient computation.\n        n_restarts_optimizer: Number of restarts for optimization.\n        samp_size: Sample size for distance calculations.\n        maxit: Maximum number of optimization iterations.\n        verbosity: Verbosity level.\n        auto_optimize: Whether to automatically optimize hyperparameters.\n        max_points: Maximum number of points for model building.\n        seed: Random seed for reproducibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        d=None,\n        g=None,\n        nlsep_method=\"inv\",\n        gradnlsep_method=\"inv\",\n        n_restarts_optimizer=9,\n        samp_size=1000,\n        maxit=100,\n        verbosity=0,\n        auto_optimize=True,\n        max_points=None,\n        seed=123,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the GP model with data and hyperparameters.\n\n        Args:\n            d (np.ndarray):\n                Length-scale parameters.\n            g (float):\n                Nugget parameter.\n            nlsep_method (str):\n                Method to use for likelihood optimization. Possible values are \"inv\" and \"chol\". Default is \"inv\".\n            gradnlsep_method (str):\n                Method to use for likelihood gradient optimization. Possible values are \"inv\", \"chol\", and \"direct\". Default is \"inv\".\n            n_restarts_optimizer (int):\n                Number of restarts for the optimizer. Default is 9.\n            samp_size (int):\n                sub-sample size for getDs(), darg() if the number of rows in X is large.\n            maxit (int):\n                Maximum number of iterations for the optimizer. Default is 100.\n            verbosity (int):\n                Verbosity level for optimization output. Default is 0.\n            auto_optimize (bool):\n                Whether to automatically optimize hyperparameters using MLE. Default is True.\n            max_points (int):\n                Maximum number of points to use for the model building. Default is None, which means all points are used.\n            seed (int):\n                Random seed for reproducibility. Default is 123.\n        \"\"\"\n        # Hyperparameters (do not store training data)\n        self.d = d\n        self.g = g\n        self.nlsep_method = nlsep_method\n        self.gradnlsep_method = gradnlsep_method\n        self.n_restarts_optimizer = n_restarts_optimizer\n        self.samp_size = samp_size\n        self.maxit = maxit\n        self.verbosity = verbosity\n        self.auto_optimize = auto_optimize\n        self.max_points = max_points\n        self.seed = seed\n\n        # Attributes set during fit\n        self.m = None\n        self.n = None\n        self.X_ = None\n        self.y_ = None\n        self.dk = None  # derivative flag\n        self.K = None\n        self.Ki = None\n        self.Kiy = None\n        self.phi = None\n        self.dK = None\n        self.DK = None\n        self.ldetK = None\n\n        # Internal flag to check if fitted\n        self._is_fitted = False\n\n        # need to store the initial parameters for the fit method (sklearn compatibility)\n        self.init_params = {\n            \"d\": d,\n            \"g\": g,\n            \"nlsep_method\": nlsep_method,\n            \"gradnlsep_method\": gradnlsep_method,\n            \"n_restarts_optimizer\": n_restarts_optimizer,\n            \"samp_size\": samp_size,\n            \"maxit\": maxit,\n            \"verbosity\": verbosity,\n            \"auto_optimize\": auto_optimize,\n            \"max_points\": max_points,\n            \"seed\": seed,\n        }\n\n    # Add these two methods required by scikit-learn\n    def get_params(self, deep=True) -&gt; dict:\n        \"\"\"Get parameters for this estimator.\n\n        This method is required for scikit-learn compatibility.\n\n        Args:\n            deep (bool): If True, will return the parameters for this estimator and\n                contained subobjects that are estimators. Defaults to True.\n\n        Returns:\n            dict: Parameter names mapped to their values.\n        \"\"\"\n        return {\n            \"d\": self.d,\n            \"g\": self.g,\n            \"nlsep_method\": self.nlsep_method,\n            \"gradnlsep_method\": self.gradnlsep_method,\n            \"n_restarts_optimizer\": self.n_restarts_optimizer,\n            \"samp_size\": self.samp_size,\n            \"maxit\": self.maxit,\n            \"verbosity\": self.verbosity,\n            \"auto_optimize\": self.auto_optimize,\n            \"max_points\": self.max_points,\n            \"seed\": self.seed,\n        }\n\n    def set_params(self, **parameters: dict) -&gt; \"GPsep\":\n        \"\"\"Set the parameters of this estimator.\n\n        This method is required for scikit-learn compatibility.\n\n        Args:\n            **parameters (dict): Estimator parameters as keyword arguments.\n\n        Returns:\n            self (GPsep): Estimator instance.\n        \"\"\"\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n\n        # Update the stored parameters for potential re-initialization\n        self.init_params.update(parameters)\n\n        return self\n\n    def fit(self, X: np.ndarray, y: np.ndarray, d=None, g=None, dK: bool = True, auto_optimize: bool = None, verbosity=0) -&gt; \"GPsep\":\n        \"\"\"Fit the GP model with training data and optionally auto-optimize hyperparameters.\n\n        Args:\n            X (np.ndarray):\n                Array-like of shape (n_samples, n_features).\n            y (np.ndarray):\n                Array-like of shape (n_samples,).\n            d (Optional[Union[np.ndarray, float]]):\n                The length-scale parameters. If None, will be determined\n                automatically. Defaults to None.\n            g (Optional[float]):\n                The nugget parameter. If None, will be determined automatically. Defaults to None.\n            dK (bool):\n                Flag to indicate whether to calculate derivatives.\n                Defaults to True.\n            auto_optimize (Optional[bool]):\n                Whether to automatically optimize hyperparameters\n                using MLE. If None, uses the default value from the object.\n                Defaults to None.\n            verbosity (int):\n                Verbosity level for optimization output. Defaults to 0.\n\n        Returns:\n            GPsep: The fitted GPsep object.\n\n        Raises:\n            ValueError: If X has no rows or if X and y dimensions mismatch.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.gp.gp_sep import GPsep\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n            &gt;&gt;&gt; y = np.array([1, 2, 3])\n            &gt;&gt;&gt; model = GPsep()\n            &gt;&gt;&gt; model.fit(X, y)\n        \"\"\"\n        # if X or y are pandas dataframes or series, convert them to numpy arrays\n        if hasattr(X, \"to_numpy\"):\n            X = X.to_numpy()\n        if hasattr(y, \"to_numpy\"):\n            y = y.to_numpy()\n        y = y.reshape(-1, 1)\n        if verbosity &gt; 0:\n            print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n        if self.max_points is not None:\n            if X.shape[0] &gt; self.max_points:\n                X, y = select_distant_points(X, y, self.max_points)\n                if verbosity &gt; 0:\n                    print(f\"Selected {self.max_points} points for the model.\")\n        if auto_optimize is None:\n            auto_optimize = self.auto_optimize\n        n, m = X.shape\n        if n == 0:\n            raise ValueError(\"X must be a matrix with rows.\")\n        if len(y) != n:\n            raise ValueError(f\"X has {n} rows but y length is {len(y)}\")\n\n        self.m = m\n        self.n = n\n        self.X = X\n        self.y = y\n        self.dk = dK\n\n        # Determine good hyperparameters if not explicitly provided\n        if d is None or g is None or auto_optimize:\n            # Process length-scale arguments\n            d_args = darg(d, X, samp_size=self.samp_size)\n\n            # Process nugget arguments\n            # TODO: Check if mle is True is correct\n            g_dict = {\"mle\": True} if g is None else g\n            g_args = garg(g_dict, y)\n\n            # Use the determined parameters if not provided\n            d_val = d_args[\"start\"] if d is None else d\n            g_val = g_args[\"start\"] if g is None else g\n\n            # Set the parameters\n            self.d = np.full(m, d_val) if isinstance(d_val, (int, float)) else d_val\n            if len(self.d) != m:\n                raise ValueError(f\"Length of d ({len(self.d)}) does not match ncol(X) ({m})\")\n            self.g = g_val\n\n            if auto_optimize:\n                tmin = [d_args[\"min\"], g_args[\"min\"]]  # Min bounds for d and g\n                tmax = [d_args[\"max\"], g_args[\"max\"]]  # Max bounds for d and g\n                ab = d_args[\"ab\"] + g_args[\"ab\"]  # Prior parameters (concatenated)\n                # Check arguments and set defaults\n                if tmin is None:\n                    tmin = [np.sqrt(np.finfo(float).eps)] * 2\n                if tmax is None:\n                    tmax = [-1, 1]\n                if ab is None:\n                    ab = [0.0, 0.0, 0.0, 0.0]\n\n                m = self.get_m()\n                # Expand tmin, tmax if necessary\n                if len(tmax) == 2:\n                    tmax = [tmax[0]] * m + [tmax[1]]\n                elif len(tmax) != m + 1:\n                    raise ValueError(\"length(tmax) must be 2 or m+1\")\n\n                if len(tmin) == 2:\n                    tmin = [tmin[0]] * m + [tmin[1]]\n                elif len(tmin) != m + 1:\n                    raise ValueError(\"length(tmin) must be 2 or m+1\")\n\n                if len(ab) != 4 or any(val &lt; 0 for val in ab):\n                    raise ValueError(\"ab must be a list of four non-negative numbers\")\n\n                # Possibly reset parameters\n                theta = np.concatenate((self.get_d(), [self.get_g()]))\n                # Check if theta is on the boundary. If not on the boundary,\n                # reset the  current parameters.\n                theta_new = crude_reset(theta, tmin, tmax, m)\n                if theta_new is not None:\n                    theta = theta_new[\"theta\"]\n                    # isuue a warning if the parameters are reset\n                    warnings.warn(f\"resetting due to init on lower boundary: {theta_new['msg']}\", RuntimeWarning)\n\n                # Convert ab to numpy array if it is a list\n                if not isinstance(ab, np.ndarray):\n                    ab = np.array(ab, dtype=float)\n\n                # check leghtscale bounds:\n                for j in range(self.m):\n                    if tmin[j] &lt;= 0:\n                        tmin[j] = np.finfo(float).eps\n                    if tmax[j] &lt;= 0:\n                        tmax[j] = self.m**2\n                    if self.d[j] &gt; tmax[j]:\n                        raise ValueError(f\"d[{j}]={self.d[j]} &gt; tmax[{j}]={tmax[j]}\")\n                    elif self.d[j] &lt; tmin[j]:\n                        raise ValueError(f\"d[{j}]={self.d[j]} &lt; tmin[{j}]={tmin[j]}\")\n\n                # check nugget bounds\n                if tmin[self.m] &lt;= 0:\n                    tmin[self.m] = np.finfo(float).eps\n                if self.g &gt; tmax[self.m]:\n                    raise ValueError(f\"g={self.g} &gt; tmax={tmax[self.m]}\")\n                elif self.g &lt; tmin[self.m]:\n                    raise ValueError(f\"g={self.g} &lt; tmin={tmin[self.m]}\")\n\n                # Check for negative entries in ab array\n                if np.any(ab &lt; 0):\n                    raise ValueError(\"ab must be a positive 4-vector\")\n\n                # TODO: check if this is necessary\n                # if self.DK is None:\n                #     raise ValueError(\"derivative info not in GPsep; use newGPsep with dK=True\")\n\n                # New: mleGPsep_optimize starts here:\n\n                # generate starting point p\n                p = np.concatenate([self.d, [self.g]])\n                bounds = [(tmin[i], tmax[i]) for i in range(len(p))]\n                if self.verbosity &gt; 0:\n                    print(f\"Starting MLE with d={self.d}, g={self.g}\")\n                    print(f\"Starting point: {p}\")\n                    print(f\"bounds: {bounds}\")\n                    print(f\"p: {p}\")\n                X = copy.deepcopy(self.X)\n                y = copy.deepcopy(self.y)\n\n                def objective(par):\n                    return nlsep(par, X, y, self.nlsep_method)\n\n                def gradient(par):\n                    return gradnlsep(par, X, y, self.gradnlsep_method)\n\n                result = run_minimize_with_restarts(\n                    objective=objective, gradient=gradient, x0=p, bounds=bounds, n_restarts_optimizer=self.n_restarts_optimizer, maxit=self.maxit, verb=self.verbosity, random_state=self.seed\n                )\n\n                d = result.x[:-1]\n                g = result.x[-1]\n\n                # set new parameters and build\n                self.set_new_params(d, g)\n                if self.verbosity &gt; 0:\n                    print(f\"result: {result}\")\n                    print(f\"Optimized d: {d}, g: {g}\")\n                    print(f\"Updated d: {self.d}, g: {self.g}\")\n                self._build()\n                new_theta = np.concatenate((self.get_d(), [self.get_g()]))\n                if np.sqrt(np.mean((result.x - new_theta) ** 2)) &gt; np.sqrt(np.finfo(float).eps):\n                    warnings.warn(\"stored theta not the same as theta-hat\", RuntimeWarning)\n                if verbosity &gt; 0:\n                    # Print mle optimization results\n                    print(\"MLE Optimization complete:\")\n                    print(f\"Optimized lengthscale (d): {self.get_d()}\")\n                    print(f\"Optimized nugget (g): {self.get_g()}\")\n                    print(f\"Message: {result['msg']}\")\n                    print(f\"Iterations: {result['its']}\")\n                self._is_fitted = True\n                return self\n            else:\n                # No optimization, just build the model with roughly estimated parameters using darg and garg\n                self._build()\n                self._is_fitted = True\n                return self\n        else:\n            # Original behavior for explicitly provided parameters\n            print(\"Using provided hyperparameters.\")\n            self.d = np.full(m, d) if isinstance(d, (int, float)) else d\n            if len(self.d) != m:\n                raise ValueError(f\"Length of d ({len(self.d)}) does not match ncol(X) ({m})\")\n            self.g = g\n            self._build()\n            self._is_fitted = True\n            return self\n\n    def calc_ytKiy(self) -&gt; None:\n        \"\"\"\n        Recalculate phi and related components from Ki and y.\n        \"\"\"\n        if self.Kiy is None:\n            self.Kiy = new_vector(self.n)\n\n        # Convert y to numpy array if it's a pandas Series\n        if hasattr(self.y, \"to_numpy\"):\n            y_array = self.y.to_numpy()\n        else:\n            y_array = np.asarray(self.y)\n\n        y = y_array.reshape(-1, 1)\n        Kiy = np.dot(self.Ki, y)\n        phi = np.dot(y.T, Kiy)\n        self.phi = phi[0, 0]\n        self.Kiy = Kiy\n\n    def _build(self) -&gt; None:\n        \"\"\"\n        Completes all correlation calculations after data is defined.\n        \"\"\"\n        # TODO: check if the following line is necessary\n        # if self.K is not None:\n        #     raise RuntimeError(\"Covariance matrix has already been built.\")\n        self.K = covar_anisotropic(self.X, d=self.d, g=self.g)\n        self.Ki = matrix_inversion_dispatcher(self.K, method=self.nlsep_method)\n        detK = det(self.K)\n        if detK &lt;= 1e-14:\n            detK = 1e-14  # TODO: Check if this can be improved\n        self.ldetK = np.log(detK)\n        self.calc_ytKiy()\n        # TODO: Check if this is necessary\n        # if self.dK:\n        #     # TODO: Check if this is necessary\n        #     # if self.dK is not None:\n        #     #     raise RuntimeError(\"dK calculations have already been initialized.\")\n        #     self.DK = diff_covar_sep_symm(self.m, self.X, self.n, self.d, self.K)\n\n    def _check_is_fitted(self) -&gt; None:\n        \"\"\"\n        Check if the GPsep instance is fitted.\n        \"\"\"\n        if not self._is_fitted:\n            raise ValueError(\"This GPsep instance is not fitted yet. Call 'fit' with \" \"appropriate arguments before using 'predict'.\")\n\n    def predict(self, X: np.ndarray, lite: bool = False, nonug: bool = False, return_full=False, return_std=False) -&gt; float:\n        \"\"\"Predict the Gaussian Process output at new input points.\n\n        Args:\n            X (np.ndarray):\n                The predictive locations.\n            lite (bool):\n                Flag to indicate whether to compute only the diagonal\n                of Sigma. Defaults to False.\n            nonug (bool):\n                Flag to indicate whether to exclude nugget.\n                Defaults to False.\n            return_full (bool):\n                Flag to indicate whether to return the full dictionary,\n                which includes the mean, Sigma, df, and llik. Defaults to False.\n            return_std (bool):\n                Flag to indicate whether to return the standard deviation.\n                Only applicable when return_full is False. Defaults to False.\n\n        Returns:\n            Various formats based on arguments:\n            - If return_full=True: Dictionary with 'mean', 'Sigma'/'s2', 'df', 'llik'\n            - If return_std=True: Tuple (mean, std_deviation)\n            - Otherwise: Mean predictions\n\n        Examples:\n            import numpy as np\n            from spotpython.gp.gp_sep import newGPsep\n            import matplotlib.pyplot as plt\n            # Simple sine data\n            X = np.linspace(0, 2 * np.pi, 7).reshape(-1, 1)\n            y = np.sin(X)\n            # New GP fit\n            gpsep = newGPsep(X, y, d=2, g=0.000001)\n            # Make predictions\n            XX = np.linspace(-1, 2 * np.pi + 1, 499).reshape(-1, 1)\n            p = gpsep.predict(XX, lite=False)\n            # Sample from the predictive distribution\n            N = 100\n            mean = p[\"mean\"]\n            Sigma = p[\"Sigma\"]\n            df = p[\"df\"]\n            # Generate samples from the multivariate t-distribution\n            yy = np.random.multivariate_normal(mean, Sigma, N)\n            yy = yy.T\n            # Plot the results\n            plt.figure(figsize=(10, 6))\n            for i in range(N):\n                plt.plot(XX, yy[:, i], color=\"gray\", linewidth=0.5)\n            plt.scatter(X, y, color=\"black\", s=50, zorder=5)\n            plt.xlabel(\"x\")\n            plt.ylabel(\"f-hat(x)\")\n            plt.title(\"Predictive Distribution\")\n            plt.show()\n        \"\"\"\n        self._check_is_fitted()\n        # if X is a pandas dataframe, convert it to a numpy array\n        if hasattr(X, \"to_numpy\"):\n            X = X.to_numpy()\n        if lite:\n            res = self._predict_lite(X, nonug)\n            if return_full:\n                return res\n            elif return_std:\n                return (res[\"mean\"], res[\"s2\"])\n            else:\n                return res[\"mean\"]\n        else:\n            res = self._predict_full(X, nonug)\n            if return_full:\n                return res\n            elif return_std:\n                return (res[\"mean\"], res[\"Sigma\"])\n            else:\n                return res[\"mean\"]\n\n    def _predict_lite(self, X: np.ndarray, nonug: bool) -&gt; dict:\n        \"\"\"\n        Predict only the diagonal of Sigma\u2014optimized for speed.\n\n        Args:\n            X (np.ndarray): The predictive locations.\n            nonug (bool): Flag to indicate whether to use nugget.\n\n        Returns:\n            dict: A dictionary containing the mean, s2, df, and llik.\n        \"\"\"\n        nn = X.shape[0]\n        m = X.shape[1]\n        mean_out, s2_out, df_out, llik_out = predGPsep_lite(self, m, nn, X, lite_in=True, nonug_in=nonug)\n        return {\"mean\": mean_out, \"s2\": s2_out, \"df\": df_out, \"llik\": llik_out}\n\n    def _predict_full(self, X: np.ndarray, nonug: bool) -&gt; dict:\n        \"\"\"\n        Compute full predictive covariance matrix.\n\n        Args:\n            X (np.ndarray): The predictive locations.\n            nonug (bool): Flag to indicate whether to use nugget.\n\n        Returns:\n            dict: A dictionary containing the mean, Sigma, df, and llik.\n        \"\"\"\n        nn, m = X.shape\n        if m != self.m:\n            raise ValueError(f\"ncol(X)={m} does not match GPsep model ({self.m})\")\n\n        mean = np.zeros(nn)\n        Sigma = np.zeros((nn, nn))\n        df = np.zeros(1)\n        llik = np.zeros(1)\n\n        n = self.n\n        g = np.finfo(float).eps if nonug else self.g\n        df[0] = float(n)\n        phidf = self.phi / df[0]\n        llik[0] = -0.5 * (df[0] * np.log(0.5 * self.phi) + self.ldetK)\n        k = covar_sep(self.m, self.X, n, X, nn, self.d, 0.0)\n        Sigma[...] = covar_sep_symm(self.m, X, nn, self.d, g)\n        ktKi = np.dot(k.T, self.Ki)\n        mean[:] = np.dot(ktKi, self.y).reshape(-1)\n        Sigma[...] = phidf * (Sigma - np.dot(ktKi, k))\n        return {\"mean\": mean, \"Sigma\": Sigma, \"df\": df, \"llik\": llik}\n\n    def get_d(self) -&gt; np.ndarray:\n        \"\"\"\n        Access the separable lengthscale parameter of the GP.\n\n        Returns:\n            np.ndarray: The lengthscale parameter.\n        \"\"\"\n        if self.d is None:\n            raise ValueError(\"Lengthscale parameter d is not allocated.\")\n        return np.copy(self.d)\n\n    def get_g(self) -&gt; float:\n        \"\"\"\n        Access the nugget parameter of the GP.\n\n        Returns:\n            float: The nugget parameter.\n        \"\"\"\n        if self.g is None:\n            raise ValueError(\"Nugget parameter g is not allocated.\")\n        return self.g\n\n    def get_m(self) -&gt; int:\n        \"\"\"\n        Access the input dimension m of the GP.\n\n        Returns:\n            int: The input dimension m.\n        \"\"\"\n        if self.m is None:\n            raise ValueError(\"Input dimension m is not allocated.\")\n        return self.m\n\n    def set_new_params(self, d: np.ndarray, g: float) -&gt; None:\n        \"\"\"\n        Change the parameterization of the GP without destroying and reallocating memory.\n\n        Args:\n            d (np.ndarray): The new length-scale parameters.\n            g (float): The new nugget parameter.\n        \"\"\"\n        if self.d is None or self.g is None:\n            raise ValueError(\"GP parameters are not allocated.\")\n\n        dsame = np.allclose(self.d, d)\n        if dsame and g == self.g:\n            return\n\n        self.d = np.where(d &lt;= 0, self.d, d)\n        self.g = g if g &gt;= 0 else self.g\n\n    def mleGPsep_optimize(self, tmin: np.ndarray, tmax: np.ndarray, ab: np.ndarray, maxit: int, verb: int) -&gt; dict:\n        \"\"\"\n        Optimize the separable GP to use its MLE separable lengthscale and multiple nugget parameterization using the current data.\n\n        Args:\n            tmin (np.ndarray): Minimum bounds for the parameters.\n            tmax (np.ndarray): Maximum bounds for the parameters.\n            ab (np.ndarray): Prior parameters. Currently unused.\n            maxit (int): Maximum number of iterations.\n            verb (int): Verbosity level.\n\n        Returns:\n            dict: A dictionary containing the optimized parameters, number of iterations, convergence status, and message.\n        \"\"\"\n        print(f\"Starting MLE with d={self.d}, g={self.g}\")\n        # generate starting point p\n        p = np.concatenate([self.d, [self.g]])\n        print(f\"Starting point: {p}\")\n        bounds = [(tmin[i], tmax[i]) for i in range(len(p))]\n        print(f\"bounds: {bounds}\")\n\n        def objective(par):\n            return nlsep(par, self.X, self.y, self.nlsep_method)\n\n        def gradient(par):\n            return gradnlsep(par, self.X, self.y, self.gradnlsep_method)\n\n        result = run_minimize_with_restarts(objective=objective, gradient=gradient, x0=p, bounds=bounds, n_restarts_optimizer=self.n_restarts_optimizer, maxit=maxit, verb=verb)\n\n        print(f\"result: {result}\")\n\n        d = result.x[:-1]\n        g = result.x[-1]\n        print(f\"Optimized d: {d}, g: {g}\")\n        # set new parameters and build\n        self.set_new_params(d, g)\n        print(f\"Updated d: {self.d}, g: {self.g}\")\n        self._build()\n        return {\"parameters\": result.x, \"iterations\": result.nit, \"convergence\": result.status, \"message\": result.message}\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.__init__","title":"<code>__init__(d=None, g=None, nlsep_method='inv', gradnlsep_method='inv', n_restarts_optimizer=9, samp_size=1000, maxit=100, verbosity=0, auto_optimize=True, max_points=None, seed=123)</code>","text":"<p>Initialize the GP model with data and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>ndarray</code> <p>Length-scale parameters.</p> <code>None</code> <code>g</code> <code>float</code> <p>Nugget parameter.</p> <code>None</code> <code>nlsep_method</code> <code>str</code> <p>Method to use for likelihood optimization. Possible values are \u201cinv\u201d and \u201cchol\u201d. Default is \u201cinv\u201d.</p> <code>'inv'</code> <code>gradnlsep_method</code> <code>str</code> <p>Method to use for likelihood gradient optimization. Possible values are \u201cinv\u201d, \u201cchol\u201d, and \u201cdirect\u201d. Default is \u201cinv\u201d.</p> <code>'inv'</code> <code>n_restarts_optimizer</code> <code>int</code> <p>Number of restarts for the optimizer. Default is 9.</p> <code>9</code> <code>samp_size</code> <code>int</code> <p>sub-sample size for getDs(), darg() if the number of rows in X is large.</p> <code>1000</code> <code>maxit</code> <code>int</code> <p>Maximum number of iterations for the optimizer. Default is 100.</p> <code>100</code> <code>verbosity</code> <code>int</code> <p>Verbosity level for optimization output. Default is 0.</p> <code>0</code> <code>auto_optimize</code> <code>bool</code> <p>Whether to automatically optimize hyperparameters using MLE. Default is True.</p> <code>True</code> <code>max_points</code> <code>int</code> <p>Maximum number of points to use for the model building. Default is None, which means all points are used.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 123.</p> <code>123</code> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def __init__(\n    self,\n    d=None,\n    g=None,\n    nlsep_method=\"inv\",\n    gradnlsep_method=\"inv\",\n    n_restarts_optimizer=9,\n    samp_size=1000,\n    maxit=100,\n    verbosity=0,\n    auto_optimize=True,\n    max_points=None,\n    seed=123,\n) -&gt; None:\n    \"\"\"\n    Initialize the GP model with data and hyperparameters.\n\n    Args:\n        d (np.ndarray):\n            Length-scale parameters.\n        g (float):\n            Nugget parameter.\n        nlsep_method (str):\n            Method to use for likelihood optimization. Possible values are \"inv\" and \"chol\". Default is \"inv\".\n        gradnlsep_method (str):\n            Method to use for likelihood gradient optimization. Possible values are \"inv\", \"chol\", and \"direct\". Default is \"inv\".\n        n_restarts_optimizer (int):\n            Number of restarts for the optimizer. Default is 9.\n        samp_size (int):\n            sub-sample size for getDs(), darg() if the number of rows in X is large.\n        maxit (int):\n            Maximum number of iterations for the optimizer. Default is 100.\n        verbosity (int):\n            Verbosity level for optimization output. Default is 0.\n        auto_optimize (bool):\n            Whether to automatically optimize hyperparameters using MLE. Default is True.\n        max_points (int):\n            Maximum number of points to use for the model building. Default is None, which means all points are used.\n        seed (int):\n            Random seed for reproducibility. Default is 123.\n    \"\"\"\n    # Hyperparameters (do not store training data)\n    self.d = d\n    self.g = g\n    self.nlsep_method = nlsep_method\n    self.gradnlsep_method = gradnlsep_method\n    self.n_restarts_optimizer = n_restarts_optimizer\n    self.samp_size = samp_size\n    self.maxit = maxit\n    self.verbosity = verbosity\n    self.auto_optimize = auto_optimize\n    self.max_points = max_points\n    self.seed = seed\n\n    # Attributes set during fit\n    self.m = None\n    self.n = None\n    self.X_ = None\n    self.y_ = None\n    self.dk = None  # derivative flag\n    self.K = None\n    self.Ki = None\n    self.Kiy = None\n    self.phi = None\n    self.dK = None\n    self.DK = None\n    self.ldetK = None\n\n    # Internal flag to check if fitted\n    self._is_fitted = False\n\n    # need to store the initial parameters for the fit method (sklearn compatibility)\n    self.init_params = {\n        \"d\": d,\n        \"g\": g,\n        \"nlsep_method\": nlsep_method,\n        \"gradnlsep_method\": gradnlsep_method,\n        \"n_restarts_optimizer\": n_restarts_optimizer,\n        \"samp_size\": samp_size,\n        \"maxit\": maxit,\n        \"verbosity\": verbosity,\n        \"auto_optimize\": auto_optimize,\n        \"max_points\": max_points,\n        \"seed\": seed,\n    }\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.calc_ytKiy","title":"<code>calc_ytKiy()</code>","text":"<p>Recalculate phi and related components from Ki and y.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def calc_ytKiy(self) -&gt; None:\n    \"\"\"\n    Recalculate phi and related components from Ki and y.\n    \"\"\"\n    if self.Kiy is None:\n        self.Kiy = new_vector(self.n)\n\n    # Convert y to numpy array if it's a pandas Series\n    if hasattr(self.y, \"to_numpy\"):\n        y_array = self.y.to_numpy()\n    else:\n        y_array = np.asarray(self.y)\n\n    y = y_array.reshape(-1, 1)\n    Kiy = np.dot(self.Ki, y)\n    phi = np.dot(y.T, Kiy)\n    self.phi = phi[0, 0]\n    self.Kiy = Kiy\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.fit","title":"<code>fit(X, y, d=None, g=None, dK=True, auto_optimize=None, verbosity=0)</code>","text":"<p>Fit the GP model with training data and optionally auto-optimize hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Array-like of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Array-like of shape (n_samples,).</p> required <code>d</code> <code>Optional[Union[ndarray, float]]</code> <p>The length-scale parameters. If None, will be determined automatically. Defaults to None.</p> <code>None</code> <code>g</code> <code>Optional[float]</code> <p>The nugget parameter. If None, will be determined automatically. Defaults to None.</p> <code>None</code> <code>dK</code> <code>bool</code> <p>Flag to indicate whether to calculate derivatives. Defaults to True.</p> <code>True</code> <code>auto_optimize</code> <code>Optional[bool]</code> <p>Whether to automatically optimize hyperparameters using MLE. If None, uses the default value from the object. Defaults to None.</p> <code>None</code> <code>verbosity</code> <code>int</code> <p>Verbosity level for optimization output. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>GPsep</code> <code>GPsep</code> <p>The fitted GPsep object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X has no rows or if X and y dimensions mismatch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.gp.gp_sep import GPsep\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; y = np.array([1, 2, 3])\n&gt;&gt;&gt; model = GPsep()\n&gt;&gt;&gt; model.fit(X, y)\n</code></pre> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray, d=None, g=None, dK: bool = True, auto_optimize: bool = None, verbosity=0) -&gt; \"GPsep\":\n    \"\"\"Fit the GP model with training data and optionally auto-optimize hyperparameters.\n\n    Args:\n        X (np.ndarray):\n            Array-like of shape (n_samples, n_features).\n        y (np.ndarray):\n            Array-like of shape (n_samples,).\n        d (Optional[Union[np.ndarray, float]]):\n            The length-scale parameters. If None, will be determined\n            automatically. Defaults to None.\n        g (Optional[float]):\n            The nugget parameter. If None, will be determined automatically. Defaults to None.\n        dK (bool):\n            Flag to indicate whether to calculate derivatives.\n            Defaults to True.\n        auto_optimize (Optional[bool]):\n            Whether to automatically optimize hyperparameters\n            using MLE. If None, uses the default value from the object.\n            Defaults to None.\n        verbosity (int):\n            Verbosity level for optimization output. Defaults to 0.\n\n    Returns:\n        GPsep: The fitted GPsep object.\n\n    Raises:\n        ValueError: If X has no rows or if X and y dimensions mismatch.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.gp.gp_sep import GPsep\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; y = np.array([1, 2, 3])\n        &gt;&gt;&gt; model = GPsep()\n        &gt;&gt;&gt; model.fit(X, y)\n    \"\"\"\n    # if X or y are pandas dataframes or series, convert them to numpy arrays\n    if hasattr(X, \"to_numpy\"):\n        X = X.to_numpy()\n    if hasattr(y, \"to_numpy\"):\n        y = y.to_numpy()\n    y = y.reshape(-1, 1)\n    if verbosity &gt; 0:\n        print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n    if self.max_points is not None:\n        if X.shape[0] &gt; self.max_points:\n            X, y = select_distant_points(X, y, self.max_points)\n            if verbosity &gt; 0:\n                print(f\"Selected {self.max_points} points for the model.\")\n    if auto_optimize is None:\n        auto_optimize = self.auto_optimize\n    n, m = X.shape\n    if n == 0:\n        raise ValueError(\"X must be a matrix with rows.\")\n    if len(y) != n:\n        raise ValueError(f\"X has {n} rows but y length is {len(y)}\")\n\n    self.m = m\n    self.n = n\n    self.X = X\n    self.y = y\n    self.dk = dK\n\n    # Determine good hyperparameters if not explicitly provided\n    if d is None or g is None or auto_optimize:\n        # Process length-scale arguments\n        d_args = darg(d, X, samp_size=self.samp_size)\n\n        # Process nugget arguments\n        # TODO: Check if mle is True is correct\n        g_dict = {\"mle\": True} if g is None else g\n        g_args = garg(g_dict, y)\n\n        # Use the determined parameters if not provided\n        d_val = d_args[\"start\"] if d is None else d\n        g_val = g_args[\"start\"] if g is None else g\n\n        # Set the parameters\n        self.d = np.full(m, d_val) if isinstance(d_val, (int, float)) else d_val\n        if len(self.d) != m:\n            raise ValueError(f\"Length of d ({len(self.d)}) does not match ncol(X) ({m})\")\n        self.g = g_val\n\n        if auto_optimize:\n            tmin = [d_args[\"min\"], g_args[\"min\"]]  # Min bounds for d and g\n            tmax = [d_args[\"max\"], g_args[\"max\"]]  # Max bounds for d and g\n            ab = d_args[\"ab\"] + g_args[\"ab\"]  # Prior parameters (concatenated)\n            # Check arguments and set defaults\n            if tmin is None:\n                tmin = [np.sqrt(np.finfo(float).eps)] * 2\n            if tmax is None:\n                tmax = [-1, 1]\n            if ab is None:\n                ab = [0.0, 0.0, 0.0, 0.0]\n\n            m = self.get_m()\n            # Expand tmin, tmax if necessary\n            if len(tmax) == 2:\n                tmax = [tmax[0]] * m + [tmax[1]]\n            elif len(tmax) != m + 1:\n                raise ValueError(\"length(tmax) must be 2 or m+1\")\n\n            if len(tmin) == 2:\n                tmin = [tmin[0]] * m + [tmin[1]]\n            elif len(tmin) != m + 1:\n                raise ValueError(\"length(tmin) must be 2 or m+1\")\n\n            if len(ab) != 4 or any(val &lt; 0 for val in ab):\n                raise ValueError(\"ab must be a list of four non-negative numbers\")\n\n            # Possibly reset parameters\n            theta = np.concatenate((self.get_d(), [self.get_g()]))\n            # Check if theta is on the boundary. If not on the boundary,\n            # reset the  current parameters.\n            theta_new = crude_reset(theta, tmin, tmax, m)\n            if theta_new is not None:\n                theta = theta_new[\"theta\"]\n                # isuue a warning if the parameters are reset\n                warnings.warn(f\"resetting due to init on lower boundary: {theta_new['msg']}\", RuntimeWarning)\n\n            # Convert ab to numpy array if it is a list\n            if not isinstance(ab, np.ndarray):\n                ab = np.array(ab, dtype=float)\n\n            # check leghtscale bounds:\n            for j in range(self.m):\n                if tmin[j] &lt;= 0:\n                    tmin[j] = np.finfo(float).eps\n                if tmax[j] &lt;= 0:\n                    tmax[j] = self.m**2\n                if self.d[j] &gt; tmax[j]:\n                    raise ValueError(f\"d[{j}]={self.d[j]} &gt; tmax[{j}]={tmax[j]}\")\n                elif self.d[j] &lt; tmin[j]:\n                    raise ValueError(f\"d[{j}]={self.d[j]} &lt; tmin[{j}]={tmin[j]}\")\n\n            # check nugget bounds\n            if tmin[self.m] &lt;= 0:\n                tmin[self.m] = np.finfo(float).eps\n            if self.g &gt; tmax[self.m]:\n                raise ValueError(f\"g={self.g} &gt; tmax={tmax[self.m]}\")\n            elif self.g &lt; tmin[self.m]:\n                raise ValueError(f\"g={self.g} &lt; tmin={tmin[self.m]}\")\n\n            # Check for negative entries in ab array\n            if np.any(ab &lt; 0):\n                raise ValueError(\"ab must be a positive 4-vector\")\n\n            # TODO: check if this is necessary\n            # if self.DK is None:\n            #     raise ValueError(\"derivative info not in GPsep; use newGPsep with dK=True\")\n\n            # New: mleGPsep_optimize starts here:\n\n            # generate starting point p\n            p = np.concatenate([self.d, [self.g]])\n            bounds = [(tmin[i], tmax[i]) for i in range(len(p))]\n            if self.verbosity &gt; 0:\n                print(f\"Starting MLE with d={self.d}, g={self.g}\")\n                print(f\"Starting point: {p}\")\n                print(f\"bounds: {bounds}\")\n                print(f\"p: {p}\")\n            X = copy.deepcopy(self.X)\n            y = copy.deepcopy(self.y)\n\n            def objective(par):\n                return nlsep(par, X, y, self.nlsep_method)\n\n            def gradient(par):\n                return gradnlsep(par, X, y, self.gradnlsep_method)\n\n            result = run_minimize_with_restarts(\n                objective=objective, gradient=gradient, x0=p, bounds=bounds, n_restarts_optimizer=self.n_restarts_optimizer, maxit=self.maxit, verb=self.verbosity, random_state=self.seed\n            )\n\n            d = result.x[:-1]\n            g = result.x[-1]\n\n            # set new parameters and build\n            self.set_new_params(d, g)\n            if self.verbosity &gt; 0:\n                print(f\"result: {result}\")\n                print(f\"Optimized d: {d}, g: {g}\")\n                print(f\"Updated d: {self.d}, g: {self.g}\")\n            self._build()\n            new_theta = np.concatenate((self.get_d(), [self.get_g()]))\n            if np.sqrt(np.mean((result.x - new_theta) ** 2)) &gt; np.sqrt(np.finfo(float).eps):\n                warnings.warn(\"stored theta not the same as theta-hat\", RuntimeWarning)\n            if verbosity &gt; 0:\n                # Print mle optimization results\n                print(\"MLE Optimization complete:\")\n                print(f\"Optimized lengthscale (d): {self.get_d()}\")\n                print(f\"Optimized nugget (g): {self.get_g()}\")\n                print(f\"Message: {result['msg']}\")\n                print(f\"Iterations: {result['its']}\")\n            self._is_fitted = True\n            return self\n        else:\n            # No optimization, just build the model with roughly estimated parameters using darg and garg\n            self._build()\n            self._is_fitted = True\n            return self\n    else:\n        # Original behavior for explicitly provided parameters\n        print(\"Using provided hyperparameters.\")\n        self.d = np.full(m, d) if isinstance(d, (int, float)) else d\n        if len(self.d) != m:\n            raise ValueError(f\"Length of d ({len(self.d)}) does not match ncol(X) ({m})\")\n        self.g = g\n        self._build()\n        self._is_fitted = True\n        return self\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.get_d","title":"<code>get_d()</code>","text":"<p>Access the separable lengthscale parameter of the GP.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The lengthscale parameter.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def get_d(self) -&gt; np.ndarray:\n    \"\"\"\n    Access the separable lengthscale parameter of the GP.\n\n    Returns:\n        np.ndarray: The lengthscale parameter.\n    \"\"\"\n    if self.d is None:\n        raise ValueError(\"Lengthscale parameter d is not allocated.\")\n    return np.copy(self.d)\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.get_g","title":"<code>get_g()</code>","text":"<p>Access the nugget parameter of the GP.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The nugget parameter.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def get_g(self) -&gt; float:\n    \"\"\"\n    Access the nugget parameter of the GP.\n\n    Returns:\n        float: The nugget parameter.\n    \"\"\"\n    if self.g is None:\n        raise ValueError(\"Nugget parameter g is not allocated.\")\n    return self.g\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.get_m","title":"<code>get_m()</code>","text":"<p>Access the input dimension m of the GP.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The input dimension m.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def get_m(self) -&gt; int:\n    \"\"\"\n    Access the input dimension m of the GP.\n\n    Returns:\n        int: The input dimension m.\n    \"\"\"\n    if self.m is None:\n        raise ValueError(\"Input dimension m is not allocated.\")\n    return self.m\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters for this estimator.</p> <p>This method is required for scikit-learn compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, will return the parameters for this estimator and contained subobjects that are estimators. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Parameter names mapped to their values.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def get_params(self, deep=True) -&gt; dict:\n    \"\"\"Get parameters for this estimator.\n\n    This method is required for scikit-learn compatibility.\n\n    Args:\n        deep (bool): If True, will return the parameters for this estimator and\n            contained subobjects that are estimators. Defaults to True.\n\n    Returns:\n        dict: Parameter names mapped to their values.\n    \"\"\"\n    return {\n        \"d\": self.d,\n        \"g\": self.g,\n        \"nlsep_method\": self.nlsep_method,\n        \"gradnlsep_method\": self.gradnlsep_method,\n        \"n_restarts_optimizer\": self.n_restarts_optimizer,\n        \"samp_size\": self.samp_size,\n        \"maxit\": self.maxit,\n        \"verbosity\": self.verbosity,\n        \"auto_optimize\": self.auto_optimize,\n        \"max_points\": self.max_points,\n        \"seed\": self.seed,\n    }\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.mleGPsep_optimize","title":"<code>mleGPsep_optimize(tmin, tmax, ab, maxit, verb)</code>","text":"<p>Optimize the separable GP to use its MLE separable lengthscale and multiple nugget parameterization using the current data.</p> <p>Parameters:</p> Name Type Description Default <code>tmin</code> <code>ndarray</code> <p>Minimum bounds for the parameters.</p> required <code>tmax</code> <code>ndarray</code> <p>Maximum bounds for the parameters.</p> required <code>ab</code> <code>ndarray</code> <p>Prior parameters. Currently unused.</p> required <code>maxit</code> <code>int</code> <p>Maximum number of iterations.</p> required <code>verb</code> <code>int</code> <p>Verbosity level.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the optimized parameters, number of iterations, convergence status, and message.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def mleGPsep_optimize(self, tmin: np.ndarray, tmax: np.ndarray, ab: np.ndarray, maxit: int, verb: int) -&gt; dict:\n    \"\"\"\n    Optimize the separable GP to use its MLE separable lengthscale and multiple nugget parameterization using the current data.\n\n    Args:\n        tmin (np.ndarray): Minimum bounds for the parameters.\n        tmax (np.ndarray): Maximum bounds for the parameters.\n        ab (np.ndarray): Prior parameters. Currently unused.\n        maxit (int): Maximum number of iterations.\n        verb (int): Verbosity level.\n\n    Returns:\n        dict: A dictionary containing the optimized parameters, number of iterations, convergence status, and message.\n    \"\"\"\n    print(f\"Starting MLE with d={self.d}, g={self.g}\")\n    # generate starting point p\n    p = np.concatenate([self.d, [self.g]])\n    print(f\"Starting point: {p}\")\n    bounds = [(tmin[i], tmax[i]) for i in range(len(p))]\n    print(f\"bounds: {bounds}\")\n\n    def objective(par):\n        return nlsep(par, self.X, self.y, self.nlsep_method)\n\n    def gradient(par):\n        return gradnlsep(par, self.X, self.y, self.gradnlsep_method)\n\n    result = run_minimize_with_restarts(objective=objective, gradient=gradient, x0=p, bounds=bounds, n_restarts_optimizer=self.n_restarts_optimizer, maxit=maxit, verb=verb)\n\n    print(f\"result: {result}\")\n\n    d = result.x[:-1]\n    g = result.x[-1]\n    print(f\"Optimized d: {d}, g: {g}\")\n    # set new parameters and build\n    self.set_new_params(d, g)\n    print(f\"Updated d: {self.d}, g: {self.g}\")\n    self._build()\n    return {\"parameters\": result.x, \"iterations\": result.nit, \"convergence\": result.status, \"message\": result.message}\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.predict","title":"<code>predict(X, lite=False, nonug=False, return_full=False, return_std=False)</code>","text":"<p>Predict the Gaussian Process output at new input points.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The predictive locations.</p> required <code>lite</code> <code>bool</code> <p>Flag to indicate whether to compute only the diagonal of Sigma. Defaults to False.</p> <code>False</code> <code>nonug</code> <code>bool</code> <p>Flag to indicate whether to exclude nugget. Defaults to False.</p> <code>False</code> <code>return_full</code> <code>bool</code> <p>Flag to indicate whether to return the full dictionary, which includes the mean, Sigma, df, and llik. Defaults to False.</p> <code>False</code> <code>return_std</code> <code>bool</code> <p>Flag to indicate whether to return the standard deviation. Only applicable when return_full is False. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>Various formats based on arguments:</p> <code>float</code> <ul> <li>If return_full=True: Dictionary with \u2018mean\u2019, \u2018Sigma\u2019/\u2019s2\u2019, \u2018df\u2019, \u2018llik\u2019</li> </ul> <code>float</code> <ul> <li>If return_std=True: Tuple (mean, std_deviation)</li> </ul> <code>float</code> <ul> <li>Otherwise: Mean predictions</li> </ul> <p>Examples:</p> <p>import numpy as np from spotpython.gp.gp_sep import newGPsep import matplotlib.pyplot as plt</p>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.predict--simple-sine-data","title":"Simple sine data","text":"<p>X = np.linspace(0, 2 * np.pi, 7).reshape(-1, 1) y = np.sin(X)</p>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.predict--new-gp-fit","title":"New GP fit","text":"<p>gpsep = newGPsep(X, y, d=2, g=0.000001)</p>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.predict--make-predictions","title":"Make predictions","text":"<p>XX = np.linspace(-1, 2 * np.pi + 1, 499).reshape(-1, 1) p = gpsep.predict(XX, lite=False)</p>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.predict--sample-from-the-predictive-distribution","title":"Sample from the predictive distribution","text":"<p>N = 100 mean = p[\u201cmean\u201d] Sigma = p[\u201cSigma\u201d] df = p[\u201cdf\u201d]</p>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.predict--generate-samples-from-the-multivariate-t-distribution","title":"Generate samples from the multivariate t-distribution","text":"<p>yy = np.random.multivariate_normal(mean, Sigma, N) yy = yy.T</p>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.predict--plot-the-results","title":"Plot the results","text":"<p>plt.figure(figsize=(10, 6)) for i in range(N):     plt.plot(XX, yy[:, i], color=\u201dgray\u201d, linewidth=0.5) plt.scatter(X, y, color=\u201dblack\u201d, s=50, zorder=5) plt.xlabel(\u201cx\u201d) plt.ylabel(\u201cf-hat(x)\u201d) plt.title(\u201cPredictive Distribution\u201d) plt.show()</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def predict(self, X: np.ndarray, lite: bool = False, nonug: bool = False, return_full=False, return_std=False) -&gt; float:\n    \"\"\"Predict the Gaussian Process output at new input points.\n\n    Args:\n        X (np.ndarray):\n            The predictive locations.\n        lite (bool):\n            Flag to indicate whether to compute only the diagonal\n            of Sigma. Defaults to False.\n        nonug (bool):\n            Flag to indicate whether to exclude nugget.\n            Defaults to False.\n        return_full (bool):\n            Flag to indicate whether to return the full dictionary,\n            which includes the mean, Sigma, df, and llik. Defaults to False.\n        return_std (bool):\n            Flag to indicate whether to return the standard deviation.\n            Only applicable when return_full is False. Defaults to False.\n\n    Returns:\n        Various formats based on arguments:\n        - If return_full=True: Dictionary with 'mean', 'Sigma'/'s2', 'df', 'llik'\n        - If return_std=True: Tuple (mean, std_deviation)\n        - Otherwise: Mean predictions\n\n    Examples:\n        import numpy as np\n        from spotpython.gp.gp_sep import newGPsep\n        import matplotlib.pyplot as plt\n        # Simple sine data\n        X = np.linspace(0, 2 * np.pi, 7).reshape(-1, 1)\n        y = np.sin(X)\n        # New GP fit\n        gpsep = newGPsep(X, y, d=2, g=0.000001)\n        # Make predictions\n        XX = np.linspace(-1, 2 * np.pi + 1, 499).reshape(-1, 1)\n        p = gpsep.predict(XX, lite=False)\n        # Sample from the predictive distribution\n        N = 100\n        mean = p[\"mean\"]\n        Sigma = p[\"Sigma\"]\n        df = p[\"df\"]\n        # Generate samples from the multivariate t-distribution\n        yy = np.random.multivariate_normal(mean, Sigma, N)\n        yy = yy.T\n        # Plot the results\n        plt.figure(figsize=(10, 6))\n        for i in range(N):\n            plt.plot(XX, yy[:, i], color=\"gray\", linewidth=0.5)\n        plt.scatter(X, y, color=\"black\", s=50, zorder=5)\n        plt.xlabel(\"x\")\n        plt.ylabel(\"f-hat(x)\")\n        plt.title(\"Predictive Distribution\")\n        plt.show()\n    \"\"\"\n    self._check_is_fitted()\n    # if X is a pandas dataframe, convert it to a numpy array\n    if hasattr(X, \"to_numpy\"):\n        X = X.to_numpy()\n    if lite:\n        res = self._predict_lite(X, nonug)\n        if return_full:\n            return res\n        elif return_std:\n            return (res[\"mean\"], res[\"s2\"])\n        else:\n            return res[\"mean\"]\n    else:\n        res = self._predict_full(X, nonug)\n        if return_full:\n            return res\n        elif return_std:\n            return (res[\"mean\"], res[\"Sigma\"])\n        else:\n            return res[\"mean\"]\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.set_new_params","title":"<code>set_new_params(d, g)</code>","text":"<p>Change the parameterization of the GP without destroying and reallocating memory.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>ndarray</code> <p>The new length-scale parameters.</p> required <code>g</code> <code>float</code> <p>The new nugget parameter.</p> required Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def set_new_params(self, d: np.ndarray, g: float) -&gt; None:\n    \"\"\"\n    Change the parameterization of the GP without destroying and reallocating memory.\n\n    Args:\n        d (np.ndarray): The new length-scale parameters.\n        g (float): The new nugget parameter.\n    \"\"\"\n    if self.d is None or self.g is None:\n        raise ValueError(\"GP parameters are not allocated.\")\n\n    dsame = np.allclose(self.d, d)\n    if dsame and g == self.g:\n        return\n\n    self.d = np.where(d &lt;= 0, self.d, d)\n    self.g = g if g &gt;= 0 else self.g\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.GPsep.set_params","title":"<code>set_params(**parameters)</code>","text":"<p>Set the parameters of this estimator.</p> <p>This method is required for scikit-learn compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>**parameters</code> <code>dict</code> <p>Estimator parameters as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <code>GPsep</code> <p>Estimator instance.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def set_params(self, **parameters: dict) -&gt; \"GPsep\":\n    \"\"\"Set the parameters of this estimator.\n\n    This method is required for scikit-learn compatibility.\n\n    Args:\n        **parameters (dict): Estimator parameters as keyword arguments.\n\n    Returns:\n        self (GPsep): Estimator instance.\n    \"\"\"\n    for parameter, value in parameters.items():\n        setattr(self, parameter, value)\n\n    # Update the stored parameters for potential re-initialization\n    self.init_params.update(parameters)\n\n    return self\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.crude_reset","title":"<code>crude_reset(theta, tmin, tmax, m)</code>","text":"<p>Check whether any elements of the parameter vector <code>theta</code> lie below the corresponding elements of the lower bound <code>tmin</code>. If so, reset <code>theta</code> to a new vector based on the weighted average of <code>tmin</code> and <code>tmax</code>, leaving bounds unmodified except for cases where <code>tmax</code> is negative.</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>ndarray</code> <p>The current parameter values.</p> required <code>tmin</code> <code>ndarray</code> <p>The lower bounds for the parameters.</p> required <code>tmax</code> <code>ndarray</code> <p>The upper bounds for the parameters (may be adjusted if negative).</p> required <code>m</code> <code>int</code> <p>The dimensionality or number of parameters (used to adjust negative <code>tmax</code> entries).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>(dict) or None: A dictionary containing: - \u201ctheta\u201d (np.ndarray): The reset parameter values. - \u201cits\u201d (int): Number of iterations (0, indicating immediate reset). - \u201cmsg\u201d (str): Reason for the reset. - \u201cconv\u201d (int): Reset code (102). Returns None if no reset is needed.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def crude_reset(theta, tmin, tmax, m) -&gt; dict:\n    \"\"\"\n    Check whether any elements of the parameter vector ``theta`` lie below the\n    corresponding elements of the lower bound ``tmin``. If so, reset ``theta``\n    to a new vector based on the weighted average of ``tmin`` and ``tmax``,\n    leaving bounds unmodified except for cases where ``tmax`` is negative.\n\n    Args:\n        theta (np.ndarray): The current parameter values.\n        tmin (np.ndarray): The lower bounds for the parameters.\n        tmax (np.ndarray): The upper bounds for the parameters (may be adjusted if negative).\n        m (int): The dimensionality or number of parameters (used to adjust negative ``tmax`` entries).\n\n    Returns:\n        (dict) or None: A dictionary containing:\n            - \"theta\" (np.ndarray): The reset parameter values.\n            - \"its\" (int): Number of iterations (0, indicating immediate reset).\n            - \"msg\" (str): Reason for the reset.\n            - \"conv\" (int): Reset code (102).\n            Returns None if no reset is needed.\n    \"\"\"\n    if np.any(theta &lt; tmin):\n        print(\"resetting due to init on lower boundary\")\n        print(f\"theta: {theta}\")\n        print(f\"tmin: {tmin}\")\n        for i in range(len(tmax)):\n            if tmax[i] &lt; 0:\n                tmax[i] = np.sqrt(m)\n        theta_new = 0.9 * np.maximum(tmin, 0) + 0.1 * np.array(tmax)\n        return {\n            \"theta\": theta_new,\n            \"its\": 0,\n            \"msg\": \"reset due to init on lower boundary\",\n            \"conv\": 102,\n        }\n    return None\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.darg","title":"<code>darg(d, X=None, samp_size=1000)</code>","text":"<p>Processes the \u2018d\u2019 dictionary/argument specifying length-scale priors, constraints, and whether MLE calculations should be used.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Union[Dict, float]</code> <p>Could be a dictionary, numeric, or None.</p> required <code>X</code> <code>ndarray</code> <p>The input data matrix.</p> <code>None</code> <code>samp_size</code> <code>int</code> <p>The sub-sample size if the number of rows in X is large.</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated \u2018d\u2019 with fields \u2018start\u2019, \u2018min\u2019, \u2018max\u2019, \u2018mle\u2019, \u2018ab\u2019, etc.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.gp.gp_sep import darg\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; d = 2.5\n&gt;&gt;&gt; result = darg(d=d, X=X, samp_size=10)\n&gt;&gt;&gt; print(result)\n</code></pre> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def darg(d, X: np.ndarray = None, samp_size: int = 1000) -&gt; dict:\n    \"\"\"\n    Processes the 'd' dictionary/argument specifying length-scale priors,\n    constraints, and whether MLE calculations should be used.\n\n    Args:\n        d (Union[Dict, float]): Could be a dictionary, numeric, or None.\n        X (np.ndarray): The input data matrix.\n        samp_size (int): The sub-sample size if the number of rows in X is large.\n\n    Returns:\n        dict: Updated 'd' with fields 'start', 'min', 'max', 'mle', 'ab', etc.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.gp.gp_sep import darg\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; d = 2.5\n        &gt;&gt;&gt; result = darg(d=d, X=X, samp_size=10)\n        &gt;&gt;&gt; print(result)\n    \"\"\"\n    if X is None:\n        raise ValueError(\"The GP model does not have valid data to calculate distances.\")\n\n    # Coerce 'd' into a dict if necessary\n    if d is None:\n        d = {}\n    elif isinstance(d, (int, float, np.number)):\n        d = {\"start\": float(d)}\n    elif not isinstance(d, dict):\n        raise ValueError(\"d should be a dictionary, numeric, or None.\")\n\n    # Check for 'mle'\n    if \"mle\" not in d:\n        d[\"mle\"] = True\n\n    # Possibly build Ds from getDs if needed\n    needsDs = (\"start\" not in d) or (d[\"mle\"] and ((\"max\" not in d) or (\"min\" not in d) or (\"ab\" not in d) or (d.get(\"ab\", [None, None])[1] is None)))\n    if needsDs:\n        Ds = getDs(X=X, p=0.1, samp_size=samp_size)\n\n    # Check for starting value\n    if \"start\" not in d:\n        d[\"start\"] = Ds[\"start\"]\n\n    # Check for max value\n    if \"max\" not in d:\n        if d[\"mle\"]:\n            d[\"max\"] = Ds[\"max\"]\n        else:\n            d[\"max\"] = float(np.max(d[\"start\"]))\n\n    # Check for min value\n    if \"min\" not in d:\n        if d[\"mle\"]:\n            d[\"min\"] = Ds[\"min\"] / 2.0\n        else:\n            d[\"min\"] = float(np.min(d[\"start\"]))\n        if d[\"min\"] &lt; math.sqrt(np.finfo(float).eps):\n            d[\"min\"] = math.sqrt(np.finfo(float).eps)\n\n    # Handle priors\n    if not d[\"mle\"]:\n        d[\"ab\"] = [0.0, 0.0]\n    else:\n        if \"ab\" not in d:\n            d[\"ab\"] = [1.5, None]\n        if d[\"ab\"][1] is None:\n            # Placeholder logic\n            d[\"ab\"][1] = 0.5 / Ds[\"max\"]\n\n    # Basic range checks\n    if d[\"max\"] &lt;= 0:\n        raise ValueError(\"d['max'] should be &gt; 0.\")\n    if d[\"min\"] &lt;= 0 or d[\"min\"] &gt; d[\"max\"]:\n        raise ValueError(\"d['min'] should be &gt; 0 and &lt; d['max'].\")\n\n    # Clamp 'start' into [min, max] rather than failing\n    start_array = np.atleast_1d(d[\"start\"])\n    if np.any(start_array &lt; d[\"min\"]) or np.any(start_array &gt; d[\"max\"]):\n        warnings.warn(f\"Some 'start' values are out of [{d['min']}, {d['max']}]; \" \"clamping them to the valid range.\", UserWarning)\n        start_array = np.clip(start_array, d[\"min\"], d[\"max\"])\n\n    # If start_array is length 1, store it back as a scalar\n    d[\"start\"] = start_array.item() if start_array.size == 1 else start_array\n\n    # Minimal check for 'ab' (placeholder)\n    ab_array = np.atleast_1d(d[\"ab\"])\n    if len(ab_array) != 2 or np.any(ab_array &lt; 0):\n        raise ValueError(\"d['ab'] must be a length-2, nonnegative array.\")\n\n    return d\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.garg","title":"<code>garg(g, y=None)</code>","text":"<p>Process the \u2018g\u2019 argument to set up proper starting values, ranges, and priors for the nugget parameter.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>dict}</code> <p>Could be a dictionary, numeric, or None. If numeric, turn it into {\u201cstart\u201d: g}.</p> required <code>y</code> <code>ndarray</code> <p>The response vector.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated \u2018g\u2019 with fields \u2018start\u2019, \u2018min\u2019, \u2018max\u2019, \u2018mle\u2019, \u2018ab\u2019, etc.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def garg(g, y: np.ndarray = None) -&gt; dict:\n    \"\"\"\n    Process the 'g' argument to set up proper starting values, ranges,\n    and priors for the nugget parameter.\n\n    Args:\n        g (dict}: Could be a dictionary, numeric, or None. If numeric, turn it into {\"start\": g}.\n        y (np.ndarray): The response vector.\n\n    Returns:\n        dict: Updated 'g' with fields 'start', 'min', 'max', 'mle', 'ab', etc.\n    \"\"\"\n    if y is None or len(y) == 0:\n        raise ValueError(\"No response data found (y is empty).\")\n\n    # Coerce 'g' into a dict if necessary\n    if g is None:\n        g = {}\n    elif isinstance(g, (int, float, np.number)):\n        g = {\"start\": float(g)}\n    elif not isinstance(g, dict):\n        raise ValueError(\"g should be a dictionary, numeric, or None.\")\n\n    # Check for 'mle'\n    if \"mle\" not in g:\n        g[\"mle\"] = False\n    if not isinstance(g[\"mle\"], bool):\n        raise ValueError(\"g['mle'] should be a scalar boolean.\")\n\n    # Check if we need r2s (squared residuals)\n    need_r2s = (\"start\" not in g) or (g[\"mle\"] and ((\"max\" not in g) or (\"ab\" not in g) or (g.get(\"ab\", [None, None])[1] is None)))\n    if need_r2s:\n        r2s = (y - np.mean(y)) ** 2\n\n    # Check for starting value\n    if \"start\" not in g:\n        g[\"start\"] = float(np.quantile(r2s, 0.025))\n\n    # Check for max value\n    if \"max\" not in g:\n        if g[\"mle\"]:\n            g[\"max\"] = float(np.max(r2s))\n        else:\n            g[\"max\"] = float(np.max(g[\"start\"]))\n\n    # Check for min value\n    if \"min\" not in g:\n        g[\"min\"] = float(np.sqrt(np.finfo(float).eps))\n\n    # Check for priors\n    if not g[\"mle\"]:\n        g[\"ab\"] = [0.0, 0.0]\n    else:\n        if \"ab\" not in g:\n            g[\"ab\"] = [1.5, None]\n        if g[\"ab\"][1] is None:\n            s2max = float(np.mean(r2s))\n            # Placeholder for Igamma.inv implementation\n            g[\"ab\"][1] = 0.5 / s2max  # simplified approximation\n\n    # Basic range checks\n    if g[\"max\"] &lt;= 0:\n        raise ValueError(\"g['max'] should be &gt; 0.\")\n    if g[\"min\"] &lt; 0 or g[\"min\"] &gt; g[\"max\"]:\n        raise ValueError(\"g['min'] should be &gt;= 0 and &lt;= g['max'].\")\n\n    # Clamp 'start' to valid range if needed\n    start_array = np.atleast_1d(g[\"start\"])\n    if np.any(start_array &lt; g[\"min\"]) or np.any(start_array &gt; g[\"max\"]):\n        warnings.warn(f\"Some 'start' values are out of [{g['min']}, {g['max']}]; \" \"clamping them to the valid range.\", UserWarning)\n        start_array = np.clip(start_array, g[\"min\"], g[\"max\"])\n\n    # If start_array is length 1, store it back as a scalar\n    g[\"start\"] = start_array.item() if start_array.size == 1 else start_array\n\n    # Check ab\n    ab_array = np.atleast_1d(g[\"ab\"])\n    if len(ab_array) != 2 or np.any(ab_array &lt; 0):\n        raise ValueError(\"g['ab'] must be a length-2, nonnegative array.\")\n\n    return g\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.getDs","title":"<code>getDs(X, p=0.1, samp_size=1000)</code>","text":"<p>Calculate a rough starting, minimum, and maximum length-scale from the data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data</p> required <code>p</code> <code>float</code> <p>quantile for the distance distribution (default 0.1).</p> <code>0.1</code> <code>samp_size</code> <code>int</code> <p>sub-sample size if the number of rows in X is large.</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>with \u2018start\u2019 (the p-th quantile),     \u2018min\u2019 (the minimum distance),     \u2018max\u2019 (the maximum distance).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.gp.gp_sep import getDs\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; getDs(X, p=0.1, samp_size=10)\n&gt;&gt;&gt; print(result)\n</code></pre> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def getDs(X: np.ndarray, p: float = 0.1, samp_size: int = 1000) -&gt; dict:\n    \"\"\"\n    Calculate a rough starting, minimum, and maximum length-scale from the data X.\n\n    Args:\n        X (np.ndarray): The input data\n        p (float): quantile for the distance distribution (default 0.1).\n        samp_size (int): sub-sample size if the number of rows in X is large.\n\n    Returns:\n        dict: with 'start' (the p-th quantile),\n                'min' (the minimum distance),\n                'max' (the maximum distance).\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.gp.gp_sep import getDs\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; getDs(X, p=0.1, samp_size=10)\n        &gt;&gt;&gt; print(result)\n    \"\"\"\n    if X is None:\n        raise ValueError(\"The GP model does not have valid data to calculate distances.\")\n\n    # Sample rows if needed\n    n = X.shape[0]\n    X_sub = X\n    if n &gt; samp_size:\n        idx = np.random.choice(n, samp_size, replace=False)\n        X_sub = X_sub[idx, :]\n\n    # Compute pairwise distances, get upper triangle, remove zeros\n    # dist_matrix = squareform(pdist(X_sub))\n    dist_matrix = dist(X_sub)\n    iu = np.triu_indices(dist_matrix.shape[0], k=1)\n    dvals = dist_matrix[iu]\n    dvals = dvals[dvals &gt; 0]\n\n    # Calculate start, min, max\n    dstart = np.quantile(dvals, p)\n    dmin = np.min(dvals)\n    dmax = np.max(dvals)\n\n    return {\"start\": dstart, \"min\": dmin, \"max\": dmax}\n</code></pre>"},{"location":"reference/spotpython/gp/gp_sep/#spotpython.gp.gp_sep.newGPsep","title":"<code>newGPsep(X, y, d=None, g=None, dK=True, optimize=True)</code>","text":"<p>Instantiate a new GPsep model with automatic hyperparameter optimization.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data matrix of shape (n, m).</p> required <code>y</code> <code>ndarray</code> <p>The output data vector of length n.</p> required <code>d</code> <code>optional</code> <p>The length-scale parameters. If None, will be determined automatically.</p> <code>None</code> <code>g</code> <code>optional</code> <p>The nugget parameter. If None, will be determined automatically.</p> <code>None</code> <code>dK</code> <code>bool</code> <p>Flag to indicate whether to calculate derivatives.</p> <code>True</code> <code>optimize</code> <code>bool</code> <p>Whether to optimize hyperparameters after initialization.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>GPsep</code> <code>GPsep</code> <p>The newly created and optimized GPsep object.</p> Source code in <code>spotpython/gp/gp_sep.py</code> <pre><code>def newGPsep(X: np.ndarray, y: np.ndarray, d=None, g=None, dK: bool = True, optimize: bool = True) -&gt; GPsep:\n    \"\"\"\n    Instantiate a new GPsep model with automatic hyperparameter optimization.\n\n    Args:\n        X (np.ndarray): The input data matrix of shape (n, m).\n        y (np.ndarray): The output data vector of length n.\n        d (optional): The length-scale parameters. If None, will be determined automatically.\n        g (optional): The nugget parameter. If None, will be determined automatically.\n        dK (bool): Flag to indicate whether to calculate derivatives.\n        optimize (bool): Whether to optimize hyperparameters after initialization.\n\n    Returns:\n        GPsep: The newly created and optimized GPsep object.\n    \"\"\"\n    gpsep = GPsep()\n    return gpsep.fit(X, y, d=d, g=g, dK=dK, auto_optimize=optimize)\n</code></pre>"},{"location":"reference/spotpython/gp/likelihood/","title":"likelihood","text":""},{"location":"reference/spotpython/gp/likelihood/#spotpython.gp.likelihood.gradnl","title":"<code>gradnl(par, D, Y, gradnl_method='inv')</code>","text":"<p>Calculate the gradient of the negative log-likelihood for an exponential correlation function.</p> <p>Parameters:</p> Name Type Description Default <code>par</code> <code>ndarray</code> <p>Array of parameters, where the first element is the range parameter               and the second element is the nugget parameter.</p> required <code>D</code> <code>ndarray</code> <p>Distance matrix of shape (n, n).</p> required <code>Y</code> <code>ndarray</code> <p>Response vector of shape (n,).</p> required <code>gradnl_method</code> <code>str</code> <p>The inversion method to use.</p> <code>'inv'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Gradient vector.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.likelihood import gradnl\n&gt;&gt;&gt; D = np.array([[0.0, 1.0, 2.0], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0]])\n&gt;&gt;&gt; Y = np.array([1.0, 2.0, 3.0])\n&gt;&gt;&gt; par = np.array([0.5, 0.1])\n&gt;&gt;&gt; grad = gradnl(par, D, Y)\n&gt;&gt;&gt; print(grad)\n[-0.000 -0.000]\n</code></pre> Source code in <code>spotpython/gp/likelihood.py</code> <pre><code>def gradnl(par, D, Y, gradnl_method=\"inv\") -&gt; np.ndarray:\n    \"\"\"\n    Calculate the gradient of the negative log-likelihood for an exponential correlation function.\n\n    Args:\n        par (np.ndarray): Array of parameters, where the first element is the range parameter\n                          and the second element is the nugget parameter.\n        D (np.ndarray): Distance matrix of shape (n, n).\n        Y (np.ndarray): Response vector of shape (n,).\n        gradnl_method (str): The inversion method to use.\n\n    Returns:\n        np.ndarray: Gradient vector.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.likelihood import gradnl\n        &gt;&gt;&gt; D = np.array([[0.0, 1.0, 2.0], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0]])\n        &gt;&gt;&gt; Y = np.array([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; par = np.array([0.5, 0.1])\n        &gt;&gt;&gt; grad = gradnl(par, D, Y)\n        &gt;&gt;&gt; print(grad)\n        [-0.000 -0.000]\n    \"\"\"\n    # Extract parameters\n    theta = par[0]\n    g = par[1]\n\n    # Calculate covariance quantities from data and parameters\n    n = len(Y)\n    K = np.exp(-D / theta) + np.diag([g] * n)\n    Ki = matrix_inversion_dispatcher(K, method=gradnl_method)\n    dotK = K * D / theta**2\n    KiY = Ki @ Y\n\n    # Theta component\n    dlltheta = (n / 2) * (KiY.T @ dotK @ KiY) / (Y.T @ KiY) - (1 / 2) * np.sum(np.diag(Ki @ dotK))\n\n    # G component\n    dllg = (n / 2) * (KiY.T @ KiY) / (Y.T @ KiY) - (1 / 2) * np.sum(np.diag(Ki))\n\n    # Combine the components into a gradient vector\n    return -np.array([dlltheta, dllg])\n</code></pre>"},{"location":"reference/spotpython/gp/likelihood/#spotpython.gp.likelihood.gradnlsep","title":"<code>gradnlsep(par, X, Y, gradnlsep_method='inv')</code>","text":"<p>Compute gradient of the negative log-likelihood using full matrix inverse.</p> <p>Parameters:</p> Name Type Description Default <code>par</code> <code>ndarray</code> <p>Array of parameters, where the first ncol(X) elements are the range parameters               and the last element is the nugget parameter.</p> required <code>X</code> <code>ndarray</code> <p>Input matrix of shape (n, col).</p> required <code>Y</code> <code>ndarray</code> <p>Response vector of shape (n,).</p> required <code>gradnlsep_method</code> <code>str</code> <p>The inversion method to use.</p> <code>'inv'</code> Source code in <code>spotpython/gp/likelihood.py</code> <pre><code>def gradnlsep(par, X, Y, gradnlsep_method=\"inv\") -&gt; np.ndarray:\n    \"\"\"\n    Compute gradient of the negative log-likelihood using full matrix inverse.\n\n    Args:\n        par (np.ndarray): Array of parameters, where the first ncol(X) elements are the range parameters\n                          and the last element is the nugget parameter.\n        X (np.ndarray): Input matrix of shape (n, col).\n        Y (np.ndarray): Response vector of shape (n,).\n        gradnlsep_method (str): The inversion method to use.\n\n    \"\"\"\n    n_col = X.shape[1]\n    if len(par) != n_col + 1:\n        raise ValueError(\"Parameter size must be ncol(X) + 1.\")\n    theta = par[:n_col]\n    g = par[n_col]\n    n = len(Y)\n\n    K = covar_anisotropic(X, d=theta, g=g)\n    Ki = matrix_inversion_dispatcher(K, method=gradnlsep_method)\n    KiY = Ki @ Y\n\n    dlltheta = np.empty(len(theta))\n    for k in range(len(dlltheta)):\n        dotK = K * dist(X[:, [k]]) / (theta[k] ** 2)\n        numerator = float(KiY.T @ dotK @ KiY)\n        denominator = float(Y.T @ KiY)\n        dlltheta[k] = (n / 2.0) * (numerator / denominator) - 0.5 * np.sum(np.diag(Ki @ dotK))\n\n    numerator_g = float(KiY.T @ KiY)\n    denominator_g = float(Y.T @ KiY)\n    dllg = (n / 2.0) * (numerator_g / denominator_g) - 0.5 * np.sum(np.diag(Ki))\n    return -np.concatenate([dlltheta, [dllg]])\n</code></pre>"},{"location":"reference/spotpython/gp/likelihood/#spotpython.gp.likelihood.nl","title":"<code>nl(par, D, Y, nl_method='inv')</code>","text":"<p>Calculate the negative log-likelihood for an exponential correlation function.</p> <p>Parameters:</p> Name Type Description Default <code>par</code> <code>ndarray</code> <p>Array of parameters, where the first element is the range parameter               and the second element is the nugget parameter.</p> required <code>D</code> <code>ndarray</code> <p>Distance matrix of shape (n, n).</p> required <code>Y</code> <code>ndarray</code> <p>Response vector of shape (n,).</p> required <code>nl_method</code> <code>str</code> <p>The inversion method to use.</p> <code>'inv'</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Negative log-likelihood.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.likelihood import nl\n&gt;&gt;&gt; D = np.array([[0.0, 1.0, 2.0], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0]])\n&gt;&gt;&gt; Y = np.array([1.0, 2.0, 3.0])\n&gt;&gt;&gt; par = np.array([0.5, 0.1])\n&gt;&gt;&gt; result = nl(par, D, Y)\n&gt;&gt;&gt; print(result)\n2.772\n</code></pre> Source code in <code>spotpython/gp/likelihood.py</code> <pre><code>def nl(par, D, Y, nl_method=\"inv\") -&gt; float:\n    \"\"\"\n    Calculate the negative log-likelihood for an exponential correlation function.\n\n    Args:\n        par (np.ndarray): Array of parameters, where the first element is the range parameter\n                          and the second element is the nugget parameter.\n        D (np.ndarray): Distance matrix of shape (n, n).\n        Y (np.ndarray): Response vector of shape (n,).\n        nl_method (str): The inversion method to use.\n\n    Returns:\n        float: Negative log-likelihood.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.likelihood import nl\n        &gt;&gt;&gt; D = np.array([[0.0, 1.0, 2.0], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0]])\n        &gt;&gt;&gt; Y = np.array([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; par = np.array([0.5, 0.1])\n        &gt;&gt;&gt; result = nl(par, D, Y)\n        &gt;&gt;&gt; print(result)\n        2.772\n    \"\"\"\n    theta = par[0]  # change 1\n    g = par[1]\n    n = len(Y)\n    K = np.exp(-D / theta) + np.diag([g] * n)  # change 2\n    Ki = matrix_inversion_dispatcher(K, method=nl_method)\n    ldetK = np.log(det(K))\n    ll = -(n / 2) * np.log(Y.T @ Ki @ Y) - (1 / 2) * ldetK\n    return -ll\n</code></pre>"},{"location":"reference/spotpython/gp/likelihood/#spotpython.gp.likelihood.nlsep","title":"<code>nlsep(par, X, Y, nlsep_method='inv')</code>","text":"<p>Calculate the negative log-likelihood for a separable power exponential correlation function.</p> <p>Parameters:</p> Name Type Description Default <code>par</code> <code>ndarray</code> <p>Array of parameters, where the first ncol(X) elements are the range parameters and the last element is the nugget parameter.</p> required <code>X</code> <code>ndarray</code> <p>Input matrix of shape (n, col).</p> required <code>Y</code> <code>ndarray</code> <p>Response vector of shape (n,).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Negative log-likelihood.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.likelihood import nlsep\n&gt;&gt;&gt; X = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n&gt;&gt;&gt; Y = np.array([1.0, 2.0, 3.0])\n&gt;&gt;&gt; par = np.array([0.5, 0.5, 0.1])\n&gt;&gt;&gt; result = nlsep(par, X, Y)\n&gt;&gt;&gt; print(result)\n2.772588722239781\n</code></pre> Source code in <code>spotpython/gp/likelihood.py</code> <pre><code>def nlsep(par, X, Y, nlsep_method=\"inv\") -&gt; float:\n    \"\"\"\n    Calculate the negative log-likelihood for a separable power exponential correlation function.\n\n    Args:\n        par (np.ndarray):\n            Array of parameters, where the first ncol(X) elements are the range parameters and the last element is the nugget parameter.\n        X (np.ndarray):\n            Input matrix of shape (n, col).\n        Y (np.ndarray):\n            Response vector of shape (n,).\n\n    Returns:\n        float: Negative log-likelihood.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.likelihood import nlsep\n        &gt;&gt;&gt; X = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n        &gt;&gt;&gt; Y = np.array([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; par = np.array([0.5, 0.5, 0.1])\n        &gt;&gt;&gt; result = nlsep(par, X, Y)\n        &gt;&gt;&gt; print(result)\n        2.772588722239781\n    \"\"\"\n    theta = par[: X.shape[1]]\n    g = par[X.shape[1]]\n    n = len(Y)\n    K = covar_anisotropic(X, d=theta, g=g)\n    Ki = matrix_inversion_dispatcher(K, method=nlsep_method)\n    detK = det(K)\n    if detK &lt;= 1e-14:\n        detK = 1e-14  # TODO: Check if this can be improved\n    ldetK = np.log(detK)\n    ll = -(n / 2) * np.log(Y.T @ Ki @ Y) - (1 / 2) * ldetK\n    return -ll\n</code></pre>"},{"location":"reference/spotpython/gp/linalg/","title":"linalg","text":""},{"location":"reference/spotpython/gp/linalg/#spotpython.gp.linalg.linalg_ddot","title":"<code>linalg_ddot(n, x, incx, y, incy)</code>","text":"<p>Perform the dot product of two vectors x and y.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of elements in the vectors.</p> required <code>x</code> <code>ndarray</code> <p>The vector x.</p> required <code>incx</code> <code>int</code> <p>The increment for the elements of x.</p> required <code>y</code> <code>ndarray</code> <p>The vector y.</p> required <code>incy</code> <code>int</code> <p>The increment for the elements of y.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The dot product of x and y.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; n = 3\n&gt;&gt;&gt; x = np.array([1, 2, 3], dtype=float)\n&gt;&gt;&gt; incx = 1\n&gt;&gt;&gt; y = np.array([4, 5, 6], dtype=float)\n&gt;&gt;&gt; incy = 1\n&gt;&gt;&gt; result = linalg_ddot(n, x, incx, y, incy)\n&gt;&gt;&gt; print(result)\n32.0\n</code></pre> Source code in <code>spotpython/gp/linalg.py</code> <pre><code>def linalg_ddot(n, x, incx, y, incy) -&gt; float:\n    \"\"\"\n    Perform the dot product of two vectors x and y.\n\n    Args:\n        n (int): The number of elements in the vectors.\n        x (ndarray): The vector x.\n        incx (int): The increment for the elements of x.\n        y (ndarray): The vector y.\n        incy (int): The increment for the elements of y.\n\n    Returns:\n        float: The dot product of x and y.\n\n    Examples:\n        &gt;&gt;&gt; n = 3\n        &gt;&gt;&gt; x = np.array([1, 2, 3], dtype=float)\n        &gt;&gt;&gt; incx = 1\n        &gt;&gt;&gt; y = np.array([4, 5, 6], dtype=float)\n        &gt;&gt;&gt; incy = 1\n        &gt;&gt;&gt; result = linalg_ddot(n, x, incx, y, incy)\n        &gt;&gt;&gt; print(result)\n        32.0\n    \"\"\"\n    x = x.reshape(-1)\n    y = y.reshape(-1)\n    return np.dot(x, y)\n</code></pre>"},{"location":"reference/spotpython/gp/linalg/#spotpython.gp.linalg.linalg_dposv","title":"<code>linalg_dposv(n, Mutil, Mi)</code>","text":"<p>Solve the linear equations A * x = B for x, where A is a symmetric positive definite matrix.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The order of the matrix Mutil.</p> required <code>Mutil</code> <code>ndarray</code> <p>The matrix A.</p> required <code>Mi</code> <code>ndarray</code> <p>The matrix B.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>The updated matrix Mi and an info flag. - Mi (ndarray): The solution matrix x. - Info flag (0 if successful, non-zero if an error occurred).</p> Notes <p>Analog of dposv in clapack and lapack where Mutil is with colmajor and uppertri or rowmajor and lowertri.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.gp_sep import linalg_dposv\n&gt;&gt;&gt; n = 3\n&gt;&gt;&gt; Mutil = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]], dtype=float)\n&gt;&gt;&gt; Mi = np.eye(n)\n&gt;&gt;&gt; info = linalg_dposv(n, Mutil, Mi)\n&gt;&gt;&gt; print(\"Info:\", info)\n&gt;&gt;&gt; print(\"Mi:\", Mi)\n        Info: 0\n        Mi: [[ 49.36111111 -13.55555556   2.11111111]\n        [-13.55555556   3.77777778  -0.55555556]\n        [  2.11111111  -0.55555556   0.11111111]]\n</code></pre> Source code in <code>spotpython/gp/linalg.py</code> <pre><code>def linalg_dposv(n, Mutil, Mi) -&gt; tuple:\n    \"\"\"\n    Solve the linear equations A * x = B for x, where A is a symmetric positive definite matrix.\n\n    Args:\n        n (int): The order of the matrix Mutil.\n        Mutil (ndarray): The matrix A.\n        Mi (ndarray): The matrix B.\n\n    Returns:\n        tuple: The updated matrix Mi and an info flag.\n            - Mi (ndarray): The solution matrix x.\n            - Info flag (0 if successful, non-zero if an error occurred).\n\n    Notes:\n     Analog of dposv in clapack and lapack where Mutil is with colmajor and\n     uppertri or rowmajor and lowertri.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.gp_sep import linalg_dposv\n        &gt;&gt;&gt; n = 3\n        &gt;&gt;&gt; Mutil = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]], dtype=float)\n        &gt;&gt;&gt; Mi = np.eye(n)\n        &gt;&gt;&gt; info = linalg_dposv(n, Mutil, Mi)\n        &gt;&gt;&gt; print(\"Info:\", info)\n        &gt;&gt;&gt; print(\"Mi:\", Mi)\n                Info: 0\n                Mi: [[ 49.36111111 -13.55555556   2.11111111]\n                [-13.55555556   3.77777778  -0.55555556]\n                [  2.11111111  -0.55555556   0.11111111]]\n    \"\"\"\n    try:\n        # Perform Cholesky decomposition\n        c, lower = cho_factor(Mutil, lower=True, overwrite_a=False, check_finite=True)\n\n        # Solve the system\n        Mi[:] = cho_solve((c, lower), Mi)\n\n        info = 0\n    except np.linalg.LinAlgError as e:\n        info = 1\n        print(f\"Error: {e}\")\n\n    return Mi, info\n</code></pre>"},{"location":"reference/spotpython/gp/linalg/#spotpython.gp.linalg.linalg_dsymv","title":"<code>linalg_dsymv(n, alpha, A, lda, x, incx, beta, y, incy)</code>","text":"<p>Perform the symmetric matrix-vector operation y := alphaAx + beta*y.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The order of the matrix A.</p> required <code>alpha</code> <code>float</code> <p>The scalar alpha.</p> required <code>A</code> <code>ndarray</code> <p>The n x n symmetric matrix.</p> required <code>lda</code> <code>int</code> <p>The leading dimension of A.</p> required <code>x</code> <code>ndarray</code> <p>The vector x.</p> required <code>incx</code> <code>int</code> <p>The increment for the elements of x.</p> required <code>beta</code> <code>float</code> <p>The scalar beta.</p> required <code>y</code> <code>ndarray</code> <p>The vector y.</p> required <code>incy</code> <code>int</code> <p>The increment for the elements of y.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The updated vector y.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; n = 3\n&gt;&gt;&gt; alpha = 1.0\n&gt;&gt;&gt; A = np.array([[1, 2, 3], [2, 4, 5], [3, 5, 6]], dtype=float)\n&gt;&gt;&gt; lda = 3\n&gt;&gt;&gt; x = np.array([1, 1, 1], dtype=float)\n&gt;&gt;&gt; incx = 1\n&gt;&gt;&gt; beta = 0.0\n&gt;&gt;&gt; y = np.zeros(3, dtype=float)\n&gt;&gt;&gt; incy = 1\n&gt;&gt;&gt; linalg_dsymv(n, alpha, A, lda, x, incx, beta, y, incy)\n&gt;&gt;&gt; print(y)\n[ 6. 11. 14.]\n</code></pre> Source code in <code>spotpython/gp/linalg.py</code> <pre><code>def linalg_dsymv(n, alpha, A, lda, x, incx, beta, y, incy) -&gt; np.ndarray:\n    \"\"\"\n    Perform the symmetric matrix-vector operation y := alpha*A*x + beta*y.\n\n    Args:\n        n (int): The order of the matrix A.\n        alpha (float): The scalar alpha.\n        A (ndarray): The n x n symmetric matrix.\n        lda (int): The leading dimension of A.\n        x (ndarray): The vector x.\n        incx (int): The increment for the elements of x.\n        beta (float): The scalar beta.\n        y (ndarray): The vector y.\n        incy (int): The increment for the elements of y.\n\n    Returns:\n        ndarray: The updated vector y.\n\n    Examples:\n        &gt;&gt;&gt; n = 3\n        &gt;&gt;&gt; alpha = 1.0\n        &gt;&gt;&gt; A = np.array([[1, 2, 3], [2, 4, 5], [3, 5, 6]], dtype=float)\n        &gt;&gt;&gt; lda = 3\n        &gt;&gt;&gt; x = np.array([1, 1, 1], dtype=float)\n        &gt;&gt;&gt; incx = 1\n        &gt;&gt;&gt; beta = 0.0\n        &gt;&gt;&gt; y = np.zeros(3, dtype=float)\n        &gt;&gt;&gt; incy = 1\n        &gt;&gt;&gt; linalg_dsymv(n, alpha, A, lda, x, incx, beta, y, incy)\n        &gt;&gt;&gt; print(y)\n        [ 6. 11. 14.]\n    \"\"\"\n    # print the dim of A\n    print(f\"dim of A = {A.shape}\")\n    # print the dim of x\n    print(f\"dim of x = {x.shape}\")\n    # modify the shape of x. If it is (n,1) make it (n,)\n    x = x.reshape(-1)\n    # print the dim of x\n    print(f\"dim of x = {x.shape}\")\n    # print the dim of y\n    print(f\"dim of y = {y.shape}\")\n    M = alpha * np.dot(A, x)\n    # print the dim of M\n    print(f\"dim of M = {M.shape}\")\n    B = beta * y\n    # print the dim of B\n    print(f\"dim of B = {B.shape}\")\n    y[:] = alpha * np.dot(A, x) + beta * y\n    print(f\"dim of y = {y.shape}\")\n    return y\n</code></pre>"},{"location":"reference/spotpython/gp/lite/","title":"lite","text":""},{"location":"reference/spotpython/gp/lite/#spotpython.gp.lite.new_predutilGPsep_lite","title":"<code>new_predutilGPsep_lite(gpsep, nn, XX)</code>","text":"<p>Utility function that allocates and calculates useful vectors and matrices for prediction; used by predGPsep_lite and dmus2GP.</p> <p>Parameters:</p> Name Type Description Default <code>gpsep</code> <code>GPsep</code> <p>The GPsep object.</p> required <code>nn</code> <code>int</code> <p>The number of predictive locations.</p> required <code>XX</code> <code>ndarray</code> <p>The predictive locations.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing k, ktKi, and ktKik.</p> Source code in <code>spotpython/gp/lite.py</code> <pre><code>def new_predutilGPsep_lite(gpsep, nn, XX) -&gt; tuple:\n    \"\"\"\n    Utility function that allocates and calculates useful vectors\n    and matrices for prediction; used by predGPsep_lite and dmus2GP.\n\n    Args:\n        gpsep (GPsep): The GPsep object.\n        nn (int): The number of predictive locations.\n        XX (ndarray): The predictive locations.\n\n    Returns:\n        tuple: A tuple containing k, ktKi, and ktKik.\n    \"\"\"\n    # k &lt;- covar(X1=X, X2=XX, d=Zt$d, g=0)\n    k = covar_sep(gpsep.m, gpsep.X, gpsep.n, XX, nn, gpsep.d, 0.0)\n\n    # Call generic function that would work for all GP covariance specs\n    ktKi, ktKik = new_predutil_generic_lite(gpsep.n, gpsep.Ki, nn, k)\n\n    return k, ktKi, ktKik\n</code></pre>"},{"location":"reference/spotpython/gp/lite/#spotpython.gp.lite.new_predutil_generic_lite","title":"<code>new_predutil_generic_lite(n, Ki, nn, k)</code>","text":"<p>Generic utility function for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of data points.</p> required <code>Ki</code> <code>ndarray</code> <p>The inverse covariance matrix.</p> required <code>nn</code> <code>int</code> <p>The number of predictive locations.</p> required <code>k</code> <code>ndarray</code> <p>The covariance matrix between training and predictive locations.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing ktKi and ktKik.</p> Source code in <code>spotpython/gp/lite.py</code> <pre><code>def new_predutil_generic_lite(n, Ki, nn, k) -&gt; tuple:\n    \"\"\"\n    Generic utility function for prediction.\n\n    Args:\n        n (int): The number of data points.\n        Ki (ndarray): The inverse covariance matrix.\n        nn (int): The number of predictive locations.\n        k (ndarray): The covariance matrix between training and predictive locations.\n\n    Returns:\n        tuple: A tuple containing ktKi and ktKik.\n    \"\"\"\n    # ktKi &lt;- t(k) %*% Ki\n    ktKi = np.dot(k.T, Ki)\n\n    # ktKik &lt;- ktKi %*% k\n    ktKik = np.dot(ktKi, k)\n\n    return ktKi, ktKik\n</code></pre>"},{"location":"reference/spotpython/gp/lite/#spotpython.gp.lite.predGPsep_lite","title":"<code>predGPsep_lite(gpsep, m, nn, XX, lite_in, nonug_in, mean_out, Sigma_out, df_out, llik_out)</code>","text":"<p>Perform a lite prediction using the GPsep model.</p> <p>Parameters:</p> Name Type Description Default <code>gpsep</code> <code>GPsep</code> <p>The GPsep object.</p> required <code>m</code> <code>int</code> <p>The number of input dimensions.</p> required <code>nn</code> <code>int</code> <p>The number of predictive locations.</p> required <code>XX</code> <code>ndarray</code> <p>The predictive locations.</p> required <code>lite_in</code> <code>bool</code> <p>Flag to indicate whether to use lite prediction.</p> required <code>nonug_in</code> <code>bool</code> <p>Flag to indicate whether to use nugget.</p> required <code>mean_out</code> <code>ndarray</code> <p>The output mean.</p> required <code>Sigma_out</code> <code>ndarray</code> <p>The output covariance matrix.</p> required <code>df_out</code> <code>ndarray</code> <p>The output degrees of freedom.</p> required <code>llik_out</code> <code>ndarray</code> <p>The output log-likelihood.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the mean, Sigma (or s2), df, and llik.</p> Source code in <code>spotpython/gp/lite.py</code> <pre><code>def predGPsep_lite(gpsep, m, nn, XX, lite_in, nonug_in, mean_out, Sigma_out, df_out, llik_out) -&gt; tuple:\n    \"\"\"\n    Perform a lite prediction using the GPsep model.\n\n    Args:\n        gpsep (GPsep): The GPsep object.\n        m (int): The number of input dimensions.\n        nn (int): The number of predictive locations.\n        XX (ndarray): The predictive locations.\n        lite_in (bool): Flag to indicate whether to use lite prediction.\n        nonug_in (bool): Flag to indicate whether to use nugget.\n        mean_out (ndarray): The output mean.\n        Sigma_out (ndarray): The output covariance matrix.\n        df_out (ndarray): The output degrees of freedom.\n        llik_out (ndarray): The output log-likelihood.\n\n    Returns:\n        tuple: A tuple containing the mean, Sigma (or s2), df, and llik.\n    \"\"\"\n    # Sanity checks\n    assert df_out is not None\n    df_out[0] = gpsep.n\n\n    # Are we using a nugget in the final calculation\n    if nonug_in:\n        g = np.finfo(float).eps\n    else:\n        g = gpsep.g\n\n    # Utility calculations\n    k, ktKi, ktKik = new_predutilGPsep_lite(gpsep, nn, XX)\n\n    # mean &lt;- ktKi %*% Z\n    if mean_out is not None:\n        mean_out[:] = np.dot(ktKi, gpsep.Z)\n\n    # Sigma &lt;- phi*(Sigma - ktKik)/df\n    if Sigma_out is not None:\n        phidf = gpsep.phi / df_out[0]\n        for i in range(nn):\n            Sigma_out[i] = phidf * (1.0 + g - ktKik[i])\n\n    # Calculate marginal likelihood (since we have the bits)\n    if llik_out is not None:\n        llik_out[0] = -0.5 * (gpsep.n * np.log(0.5 * gpsep.phi) + gpsep.ldetK)\n\n    return mean_out, Sigma_out, df_out, llik_out\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/","title":"matrix","text":""},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.dup_matrix","title":"<code>dup_matrix(m, M, n1, n2)</code>","text":"<p>Copy the contents of matrix M to matrix m.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>ndarray</code> <p>The destination matrix.</p> required <code>M</code> <code>ndarray</code> <p>The source matrix.</p> required <code>n1</code> <code>int</code> <p>The number of rows in the matrices.</p> required <code>n2</code> <code>int</code> <p>The number of columns in the matrices.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The updated destination matrix m.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; M = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; m = np.zeros((3, 2))\n&gt;&gt;&gt; dup_matrix(m, M, 3, 2)\n&gt;&gt;&gt; print(m)\n[[1. 2.]\n [3. 4.]\n [5. 6.]]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def dup_matrix(m, M, n1, n2) -&gt; np.ndarray:\n    \"\"\"\n    Copy the contents of matrix M to matrix m.\n\n    Args:\n        m (ndarray): The destination matrix.\n        M (ndarray): The source matrix.\n        n1 (int): The number of rows in the matrices.\n        n2 (int): The number of columns in the matrices.\n\n    Returns:\n        ndarray: The updated destination matrix m.\n\n    Examples:\n        &gt;&gt;&gt; M = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; m = np.zeros((3, 2))\n        &gt;&gt;&gt; dup_matrix(m, M, 3, 2)\n        &gt;&gt;&gt; print(m)\n        [[1. 2.]\n         [3. 4.]\n         [5. 6.]]\n    \"\"\"\n    for i in range(n1):\n        for j in range(n2):\n            m[i, j] = M[i, j]\n    return m\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.dupv","title":"<code>dupv(v, vold, n)</code>","text":"<p>Copies vold to v (assumes v has already been allocated).</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>ndarray</code> <p>The array to copy to.</p> required <code>vold</code> <code>ndarray</code> <p>The original array to copy from.</p> required <code>n</code> <code>int</code> <p>The size of the arrays.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The updated array v.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; v = np.empty(3)\n&gt;&gt;&gt; vold = np.array([1.0, 2.0, 3.0])\n&gt;&gt;&gt; n = 3\n&gt;&gt;&gt; dupv(v, vold, n)\n&gt;&gt;&gt; print(v)\n[1. 2. 3.]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def dupv(v, vold, n) -&gt; np.ndarray:\n    \"\"\"\n    Copies vold to v (assumes v has already been allocated).\n\n    Args:\n        v (ndarray): The array to copy to.\n        vold (ndarray): The original array to copy from.\n        n (int): The size of the arrays.\n\n    Returns:\n        ndarray: The updated array v.\n\n    Examples:\n        &gt;&gt;&gt; v = np.empty(3)\n        &gt;&gt;&gt; vold = np.array([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; n = 3\n        &gt;&gt;&gt; dupv(v, vold, n)\n        &gt;&gt;&gt; print(v)\n        [1. 2. 3.]\n    \"\"\"\n    for i in range(n):\n        v[i] = vold[i]\n    return v\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.new_dup_matrix","title":"<code>new_dup_matrix(M, n1, n2)</code>","text":"<p>Create a new n1 x n2 matrix which is allocated like an n1*n2 array, and copy the contents of n1 x n2 matrix M into it.</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <code>ndarray</code> <p>The source matrix.</p> required <code>n1</code> <code>int</code> <p>The number of rows in the matrix.</p> required <code>n2</code> <code>int</code> <p>The number of columns in the matrix.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The new n1 x n2 matrix with copied contents.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; M = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; n1 = 3\n&gt;&gt;&gt; n2 = 2\n&gt;&gt;&gt; m = new_dup_matrix(M, n1, n2)\n&gt;&gt;&gt; print(m)\n[[1. 2.]\n [3. 4.]\n [5. 6.]]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def new_dup_matrix(M, n1, n2) -&gt; np.ndarray:\n    \"\"\"\n    Create a new n1 x n2 matrix which is allocated like an n1*n2 array,\n    and copy the contents of n1 x n2 matrix M into it.\n\n    Args:\n        M (ndarray): The source matrix.\n        n1 (int): The number of rows in the matrix.\n        n2 (int): The number of columns in the matrix.\n\n    Returns:\n        ndarray: The new n1 x n2 matrix with copied contents.\n\n    Examples:\n        &gt;&gt;&gt; M = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; n1 = 3\n        &gt;&gt;&gt; n2 = 2\n        &gt;&gt;&gt; m = new_dup_matrix(M, n1, n2)\n        &gt;&gt;&gt; print(m)\n        [[1. 2.]\n         [3. 4.]\n         [5. 6.]]\n    \"\"\"\n    if n1 &lt;= 0 or n2 &lt;= 0:\n        return None\n\n    m = new_matrix(n1, n2)\n    dup_matrix(m, M, n1, n2)\n    return m\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.new_dup_vector","title":"<code>new_dup_vector(vold, n)</code>","text":"<p>Allocates a new numpy array of size n and fills it with the contents of vold.</p> <p>Parameters:</p> Name Type Description Default <code>vold</code> <code>ndarray</code> <p>The original array to duplicate.</p> required <code>n</code> <code>int</code> <p>The size of the new array.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The new array filled with the contents of vold.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; vold = np.array([1.0, 2.0, 3.0])\n&gt;&gt;&gt; n = 3\n&gt;&gt;&gt; v = new_dup_vector(vold, n)\n&gt;&gt;&gt; print(v)\n[1. 2. 3.]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def new_dup_vector(vold, n) -&gt; np.ndarray:\n    \"\"\"\n    Allocates a new numpy array of size n and fills it with the contents of vold.\n\n    Args:\n        vold (ndarray): The original array to duplicate.\n        n (int): The size of the new array.\n\n    Returns:\n        ndarray: The new array filled with the contents of vold.\n\n    Examples:\n        &gt;&gt;&gt; vold = np.array([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; n = 3\n        &gt;&gt;&gt; v = new_dup_vector(vold, n)\n        &gt;&gt;&gt; print(v)\n        [1. 2. 3.]\n    \"\"\"\n    v = np.empty(n)\n    v = dupv(v, vold, n)\n    return v\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.new_id_matrix","title":"<code>new_id_matrix(n)</code>","text":"<p>Create a new n x n identity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The size of the identity matrix.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The new n x n identity matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; n = 3\n&gt;&gt;&gt; m = new_id_matrix(n)\n&gt;&gt;&gt; print(m)\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def new_id_matrix(n) -&gt; np.ndarray:\n    \"\"\"\n    Create a new n x n identity matrix.\n\n    Args:\n        n (int): The size of the identity matrix.\n\n    Returns:\n        ndarray: The new n x n identity matrix.\n\n    Examples:\n        &gt;&gt;&gt; n = 3\n        &gt;&gt;&gt; m = new_id_matrix(n)\n        &gt;&gt;&gt; print(m)\n        [[1. 0. 0.]\n         [0. 1. 0.]\n         [0. 0. 1.]]\n    \"\"\"\n    return np.eye(n)\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.new_matrix","title":"<code>new_matrix(n1, n2)</code>","text":"<p>Create a new n1 x n2 matrix which is allocated like an n1*n2 array, but can be referenced as a 2-d array.</p> <p>Parameters:</p> Name Type Description Default <code>n1</code> <code>int</code> <p>The number of rows in the matrix.</p> required <code>n2</code> <code>int</code> <p>The number of columns in the matrix.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The new n1 x n2 matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; n1 = 3\n&gt;&gt;&gt; n2 = 2\n&gt;&gt;&gt; m = new_matrix(n1, n2)\n&gt;&gt;&gt; print(m)\n[[0. 0.]\n [0. 0.]\n [0. 0.]]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def new_matrix(n1, n2) -&gt; np.ndarray:\n    \"\"\"\n    Create a new n1 x n2 matrix which is allocated like an n1*n2 array,\n    but can be referenced as a 2-d array.\n\n    Args:\n        n1 (int): The number of rows in the matrix.\n        n2 (int): The number of columns in the matrix.\n\n    Returns:\n        ndarray: The new n1 x n2 matrix.\n\n    Examples:\n        &gt;&gt;&gt; n1 = 3\n        &gt;&gt;&gt; n2 = 2\n        &gt;&gt;&gt; m = new_matrix(n1, n2)\n        &gt;&gt;&gt; print(m)\n        [[0. 0.]\n         [0. 0.]\n         [0. 0.]]\n    \"\"\"\n    if n1 == 0 or n2 == 0:\n        return None\n\n    m = np.zeros((n1, n2))\n    return m\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.new_matrix_bones","title":"<code>new_matrix_bones(v, n1, n2)</code>","text":"<p>Create a 2D numpy array (matrix) from a 1D numpy array (vector). The resulting matrix shares the same memory as the input vector.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>ndarray</code> <p>The input 1D numpy array (vector).</p> required <code>n1</code> <code>int</code> <p>The number of rows in the resulting matrix.</p> required <code>n2</code> <code>int</code> <p>The number of columns in the resulting matrix.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The resulting 2D numpy array (matrix).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; v = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n&gt;&gt;&gt; n1 = 2\n&gt;&gt;&gt; n2 = 3\n&gt;&gt;&gt; M = new_matrix_bones(v, n1, n2)\n&gt;&gt;&gt; print(M)\n[[1. 2. 3.]\n [4. 5. 6.]]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def new_matrix_bones(v, n1, n2) -&gt; np.ndarray:\n    \"\"\"\n    Create a 2D numpy array (matrix) from a 1D numpy array (vector).\n    The resulting matrix shares the same memory as the input vector.\n\n    Args:\n        v (ndarray): The input 1D numpy array (vector).\n        n1 (int): The number of rows in the resulting matrix.\n        n2 (int): The number of columns in the resulting matrix.\n\n    Returns:\n        ndarray: The resulting 2D numpy array (matrix).\n\n    Examples:\n        &gt;&gt;&gt; v = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n        &gt;&gt;&gt; n1 = 2\n        &gt;&gt;&gt; n2 = 3\n        &gt;&gt;&gt; M = new_matrix_bones(v, n1, n2)\n        &gt;&gt;&gt; print(M)\n        [[1. 2. 3.]\n         [4. 5. 6.]]\n    \"\"\"\n    M = np.empty((n1, n2), dtype=v.dtype)\n    M[0] = v[:n2]\n    for i in range(1, n1):\n        M[i] = v[i * n2 : (i + 1) * n2]\n    return M\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.new_p_submatrix_rows","title":"<code>new_p_submatrix_rows(p, v, nrows, ncols, row_offset)</code>","text":"<p>Create a new matrix from the rows of v, specified by p. Must have ncol(v) == ncol(V) and nrow(V) &gt;= nrows and nrow(v) &gt;= max(p).</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>ndarray</code> <p>The array of row indices to copy.</p> required <code>v</code> <code>ndarray</code> <p>The source matrix.</p> required <code>nrows</code> <code>int</code> <p>The number of rows in the new matrix.</p> required <code>ncols</code> <code>int</code> <p>The number of columns in the new matrix.</p> required <code>row_offset</code> <code>int</code> <p>The row offset in the new matrix.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The new matrix with specified rows copied from v.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; p = np.array([0, 2])\n&gt;&gt;&gt; v = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; nrows = 2\n&gt;&gt;&gt; ncols = 3\n&gt;&gt;&gt; row_offset = 1\n&gt;&gt;&gt; V = new_p_submatrix_rows(p, v, nrows, ncols, row_offset)\n&gt;&gt;&gt; print(V)\n[[0. 0. 0.]\n [1. 2. 3.]\n [0. 0. 0.]\n [7. 8. 9.]\n [0. 0. 0.]]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def new_p_submatrix_rows(p, v, nrows, ncols, row_offset) -&gt; np.ndarray:\n    \"\"\"\n    Create a new matrix from the rows of v, specified by p.\n    Must have ncol(v) == ncol(V) and nrow(V) &gt;= nrows and nrow(v) &gt;= max(p).\n\n    Args:\n        p (ndarray): The array of row indices to copy.\n        v (ndarray): The source matrix.\n        nrows (int): The number of rows in the new matrix.\n        ncols (int): The number of columns in the new matrix.\n        row_offset (int): The row offset in the new matrix.\n\n    Returns:\n        ndarray: The new matrix with specified rows copied from v.\n\n    Examples:\n        &gt;&gt;&gt; p = np.array([0, 2])\n        &gt;&gt;&gt; v = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        &gt;&gt;&gt; nrows = 2\n        &gt;&gt;&gt; ncols = 3\n        &gt;&gt;&gt; row_offset = 1\n        &gt;&gt;&gt; V = new_p_submatrix_rows(p, v, nrows, ncols, row_offset)\n        &gt;&gt;&gt; print(V)\n        [[0. 0. 0.]\n         [1. 2. 3.]\n         [0. 0. 0.]\n         [7. 8. 9.]\n         [0. 0. 0.]]\n    \"\"\"\n    if nrows + row_offset == 0 or ncols == 0:\n        return None\n\n    V = np.zeros((nrows + row_offset, ncols))\n    if nrows &gt; 0:\n        V = sub_p_matrix_rows(V, p, v, ncols, nrows, row_offset)\n    return V\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.new_vector","title":"<code>new_vector(n)</code>","text":"<p>Allocates a new numpy array of size n.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The size of the new array.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The new array of size n.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; n = 3\n&gt;&gt;&gt; v = new_vector(n)\n&gt;&gt;&gt; print(v)\n[0. 0. 0.]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def new_vector(n) -&gt; np.ndarray:\n    \"\"\"\n    Allocates a new numpy array of size n.\n\n    Args:\n        n (int): The size of the new array.\n\n    Returns:\n        ndarray: The new array of size n.\n\n    Examples:\n        &gt;&gt;&gt; n = 3\n        &gt;&gt;&gt; v = new_vector(n)\n        &gt;&gt;&gt; print(v)\n        [0. 0. 0.]\n    \"\"\"\n    if n == 0:\n        return None\n    v = np.empty(n)\n    return v\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.sub_p_matrix","title":"<code>sub_p_matrix(V, p, v, nrows, lenp, col_offset)</code>","text":"<p>Copy the columns <code>v[1:n1][p[n2]]</code> to V. Must have nrow(v) == nrow(V) and ncol(V) &gt;= lenp and ncol(v) &gt;= max(p).</p> <p>Parameters:</p> Name Type Description Default <code>V</code> <code>ndarray</code> <p>The destination matrix.</p> required <code>p</code> <code>ndarray</code> <p>The array of column indices to copy.</p> required <code>v</code> <code>ndarray</code> <p>The source matrix.</p> required <code>nrows</code> <code>int</code> <p>The number of rows in the matrices.</p> required <code>lenp</code> <code>int</code> <p>The length of the array p.</p> required <code>col_offset</code> <code>int</code> <p>The column offset in the destination matrix.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The updated destination matrix V.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; V = np.zeros((3, 5))\n&gt;&gt;&gt; p = np.array([0, 2])\n&gt;&gt;&gt; v = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; nrows = 3\n&gt;&gt;&gt; lenp = 2\n&gt;&gt;&gt; col_offset = 1\n&gt;&gt;&gt; V = sub_p_matrix(V, p, v, nrows, lenp, col_offset)\n&gt;&gt;&gt; print(V)\n[[0. 1. 3. 0. 0.]\n [0. 4. 6. 0. 0.]\n [0. 7. 9. 0. 0.]]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def sub_p_matrix(V, p, v, nrows, lenp, col_offset) -&gt; np.ndarray:\n    \"\"\"\n    Copy the columns `v[1:n1][p[n2]]` to V.\n    Must have nrow(v) == nrow(V) and ncol(V) &gt;= lenp and ncol(v) &gt;= max(p).\n\n    Args:\n        V (ndarray): The destination matrix.\n        p (ndarray): The array of column indices to copy.\n        v (ndarray): The source matrix.\n        nrows (int): The number of rows in the matrices.\n        lenp (int): The length of the array p.\n        col_offset (int): The column offset in the destination matrix.\n\n    Returns:\n        ndarray: The updated destination matrix V.\n\n    Examples:\n        &gt;&gt;&gt; V = np.zeros((3, 5))\n        &gt;&gt;&gt; p = np.array([0, 2])\n        &gt;&gt;&gt; v = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        &gt;&gt;&gt; nrows = 3\n        &gt;&gt;&gt; lenp = 2\n        &gt;&gt;&gt; col_offset = 1\n        &gt;&gt;&gt; V = sub_p_matrix(V, p, v, nrows, lenp, col_offset)\n        &gt;&gt;&gt; print(V)\n        [[0. 1. 3. 0. 0.]\n         [0. 4. 6. 0. 0.]\n         [0. 7. 9. 0. 0.]]\n    \"\"\"\n    assert V is not None and p is not None and v is not None\n    assert nrows &gt; 0 and lenp &gt; 0\n\n    for i in range(nrows):\n        for j in range(lenp):\n            V[i, j + col_offset] = v[i, p[j]]\n    return V\n</code></pre>"},{"location":"reference/spotpython/gp/matrix/#spotpython.gp.matrix.sub_p_matrix_rows","title":"<code>sub_p_matrix_rows(V, p, v, ncols, lenp, row_offset)</code>","text":"<p>Copy the rows <code>v[1:n1][p[n2]]</code> to V. Must have ncol(v) == ncol(V) and nrow(V) &gt;= lenp and nrow(v) &gt;= max(p).</p> <p>Parameters:</p> Name Type Description Default <code>V</code> <code>ndarray</code> <p>The destination matrix.</p> required <code>p</code> <code>ndarray</code> <p>The array of row indices to copy.</p> required <code>v</code> <code>ndarray</code> <p>The source matrix.</p> required <code>ncols</code> <code>int</code> <p>The number of columns in the matrices.</p> required <code>lenp</code> <code>int</code> <p>The length of the array p.</p> required <code>row_offset</code> <code>int</code> <p>The row offset in the destination matrix.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The updated destination matrix V.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; V = np.zeros((5, 3))\n&gt;&gt;&gt; p = np.array([0, 2])\n&gt;&gt;&gt; v = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; ncols = 3\n&gt;&gt;&gt; lenp = 2\n&gt;&gt;&gt; row_offset = 1\n&gt;&gt;&gt; sub_p_matrix_rows(V, p, v, ncols, lenp, row_offset)\n&gt;&gt;&gt; print(V)\n[[0. 0. 0.]\n [1. 2. 3.]\n [0. 0. 0.]\n [7. 8. 9.]\n [0. 0. 0.]]\n</code></pre> Source code in <code>spotpython/gp/matrix.py</code> <pre><code>def sub_p_matrix_rows(V, p, v, ncols, lenp, row_offset) -&gt; np.ndarray:\n    \"\"\"\n    Copy the rows `v[1:n1][p[n2]]` to V.\n    Must have ncol(v) == ncol(V) and nrow(V) &gt;= lenp and nrow(v) &gt;= max(p).\n\n    Args:\n        V (ndarray): The destination matrix.\n        p (ndarray): The array of row indices to copy.\n        v (ndarray): The source matrix.\n        ncols (int): The number of columns in the matrices.\n        lenp (int): The length of the array p.\n        row_offset (int): The row offset in the destination matrix.\n\n    Returns:\n        ndarray: The updated destination matrix V.\n\n    Examples:\n        &gt;&gt;&gt; V = np.zeros((5, 3))\n        &gt;&gt;&gt; p = np.array([0, 2])\n        &gt;&gt;&gt; v = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        &gt;&gt;&gt; ncols = 3\n        &gt;&gt;&gt; lenp = 2\n        &gt;&gt;&gt; row_offset = 1\n        &gt;&gt;&gt; sub_p_matrix_rows(V, p, v, ncols, lenp, row_offset)\n        &gt;&gt;&gt; print(V)\n        [[0. 0. 0.]\n         [1. 2. 3.]\n         [0. 0. 0.]\n         [7. 8. 9.]\n         [0. 0. 0.]]\n    \"\"\"\n    assert V is not None and p is not None and v is not None\n    assert ncols &gt; 0 and lenp &gt; 0\n\n    for i in range(lenp):\n        V[i + row_offset, :ncols] = v[p[i], :ncols]\n    return V\n</code></pre>"},{"location":"reference/spotpython/gp/regressor/","title":"regressor","text":""},{"location":"reference/spotpython/gp/regressor/#spotpython.gp.regressor.GPRegressor","title":"<code>GPRegressor</code>","text":"Source code in <code>spotpython/gp/regressor.py</code> <pre><code>class GPRegressor:\n    def __init__(self):\n        self.eps = np.sqrt(np.finfo(float).eps)\n        self.fitted = False\n\n    def fit(self, X, y) -&gt; \"GPRegressor\":\n        \"\"\"\n        Fit the Gaussian Process model.\n\n        Args:\n            X (np.ndarray): Training input matrix of shape (n, m).\n            y (np.ndarray): Training response vector of shape (n,).\n\n        Returns:\n            self: Fitted model.\n        \"\"\"\n        self.n, self.m = X.shape\n\n        # Optimize parameters\n        outg = minimize(\n            lambda par: nlsep(par, X, y),\n            x0=np.concatenate([np.repeat(0.1, self.m), [0.1 * np.var(y)]]),\n            jac=lambda par: gradnlsep(par, X, y),\n            method=\"L-BFGS-B\",\n            bounds=[(self.eps, 10)] * self.m + [(self.eps, np.var(y))],\n        )\n\n        # Compute covariance matrices\n        K = covar_anisotropic(X, d=outg.x[: self.m], g=outg.x[self.m])\n        Ki = inv(K)\n        tau2hat = (y.T @ Ki @ y) / len(X)\n\n        self.X = X\n        self.y = y\n        self.outg = outg\n        self.Ki = Ki\n        self.tau2hat = tau2hat\n        self.fitted = True\n\n        return self\n\n    def predict(self, XX) -&gt; tuple:\n        \"\"\"\n        Predict using the Gaussian Process model.\n\n        Args:\n            XX (np.ndarray): Test input matrix of shape (n_test, m).\n\n        Returns:\n            tuple: Predicted mean (mup2) and covariance (Sigmap2).\n\n        Raises:\n            RuntimeError: If the model is not fitted.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotpython.gp.regressor import GPRegressor\n            &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n            &gt;&gt;&gt; y = np.array([1, 2, 3])\n            &gt;&gt;&gt; XX = np.array([[1, 2], [3, 4]])\n            &gt;&gt;&gt; gp_model = GPRegressor()\n            &gt;&gt;&gt; gp_model.fit(X, y)\n            &gt;&gt;&gt; mup2, Sigmap2 = gp_model.predict(XX)\n            &gt;&gt;&gt; print(mup2)\n            [1. 2.]\n            &gt;&gt;&gt; print(Sigmap2)\n            [[1. 1.]\n             [1. 1.]]\n\n        \"\"\"\n        if not self.fitted:\n            raise RuntimeError(\"The model must be fitted before calling predict.\")\n\n        KXX = covar_anisotropic(XX, d=self.outg.x[: self.m], g=self.outg.x[self.m])\n        KX = covar_anisotropic(XX, self.X, d=self.outg.x[: self.m], g=0.0)\n        mup2 = KX @ self.Ki @ self.y\n        Sigmap2 = self.tau2hat * (KXX - KX @ self.Ki @ KX.T)\n\n        return mup2, Sigmap2\n</code></pre>"},{"location":"reference/spotpython/gp/regressor/#spotpython.gp.regressor.GPRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the Gaussian Process model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training input matrix of shape (n, m).</p> required <code>y</code> <code>ndarray</code> <p>Training response vector of shape (n,).</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>GPRegressor</code> <p>Fitted model.</p> Source code in <code>spotpython/gp/regressor.py</code> <pre><code>def fit(self, X, y) -&gt; \"GPRegressor\":\n    \"\"\"\n    Fit the Gaussian Process model.\n\n    Args:\n        X (np.ndarray): Training input matrix of shape (n, m).\n        y (np.ndarray): Training response vector of shape (n,).\n\n    Returns:\n        self: Fitted model.\n    \"\"\"\n    self.n, self.m = X.shape\n\n    # Optimize parameters\n    outg = minimize(\n        lambda par: nlsep(par, X, y),\n        x0=np.concatenate([np.repeat(0.1, self.m), [0.1 * np.var(y)]]),\n        jac=lambda par: gradnlsep(par, X, y),\n        method=\"L-BFGS-B\",\n        bounds=[(self.eps, 10)] * self.m + [(self.eps, np.var(y))],\n    )\n\n    # Compute covariance matrices\n    K = covar_anisotropic(X, d=outg.x[: self.m], g=outg.x[self.m])\n    Ki = inv(K)\n    tau2hat = (y.T @ Ki @ y) / len(X)\n\n    self.X = X\n    self.y = y\n    self.outg = outg\n    self.Ki = Ki\n    self.tau2hat = tau2hat\n    self.fitted = True\n\n    return self\n</code></pre>"},{"location":"reference/spotpython/gp/regressor/#spotpython.gp.regressor.GPRegressor.predict","title":"<code>predict(XX)</code>","text":"<p>Predict using the Gaussian Process model.</p> <p>Parameters:</p> Name Type Description Default <code>XX</code> <code>ndarray</code> <p>Test input matrix of shape (n_test, m).</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>Predicted mean (mup2) and covariance (Sigmap2).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model is not fitted.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.gp.regressor import GPRegressor\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; y = np.array([1, 2, 3])\n&gt;&gt;&gt; XX = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; gp_model = GPRegressor()\n&gt;&gt;&gt; gp_model.fit(X, y)\n&gt;&gt;&gt; mup2, Sigmap2 = gp_model.predict(XX)\n&gt;&gt;&gt; print(mup2)\n[1. 2.]\n&gt;&gt;&gt; print(Sigmap2)\n[[1. 1.]\n [1. 1.]]\n</code></pre> Source code in <code>spotpython/gp/regressor.py</code> <pre><code>def predict(self, XX) -&gt; tuple:\n    \"\"\"\n    Predict using the Gaussian Process model.\n\n    Args:\n        XX (np.ndarray): Test input matrix of shape (n_test, m).\n\n    Returns:\n        tuple: Predicted mean (mup2) and covariance (Sigmap2).\n\n    Raises:\n        RuntimeError: If the model is not fitted.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.gp.regressor import GPRegressor\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; y = np.array([1, 2, 3])\n        &gt;&gt;&gt; XX = np.array([[1, 2], [3, 4]])\n        &gt;&gt;&gt; gp_model = GPRegressor()\n        &gt;&gt;&gt; gp_model.fit(X, y)\n        &gt;&gt;&gt; mup2, Sigmap2 = gp_model.predict(XX)\n        &gt;&gt;&gt; print(mup2)\n        [1. 2.]\n        &gt;&gt;&gt; print(Sigmap2)\n        [[1. 1.]\n         [1. 1.]]\n\n    \"\"\"\n    if not self.fitted:\n        raise RuntimeError(\"The model must be fitted before calling predict.\")\n\n    KXX = covar_anisotropic(XX, d=self.outg.x[: self.m], g=self.outg.x[self.m])\n    KX = covar_anisotropic(XX, self.X, d=self.outg.x[: self.m], g=0.0)\n    mup2 = KX @ self.Ki @ self.y\n    Sigmap2 = self.tau2hat * (KXX - KX @ self.Ki @ KX.T)\n\n    return mup2, Sigmap2\n</code></pre>"},{"location":"reference/spotpython/gp/util/","title":"util","text":""},{"location":"reference/spotpython/gp/util/#spotpython.gp.util.log_determinant_chol","title":"<code>log_determinant_chol(M)</code>","text":"<p>Returns the log determinant of the n x n Cholesky decomposition of a matrix M.</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <code>ndarray</code> <p>The n x n Cholesky decomposition of a matrix.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The log determinant of the matrix M.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; M = np.array([[2.0, 0.0], [1.0, 1.0]])\n&gt;&gt;&gt; log_det = log_determinant_chol(M)\n&gt;&gt;&gt; print(log_det)\n1.3862943611198906\n</code></pre> Source code in <code>spotpython/gp/util.py</code> <pre><code>def log_determinant_chol(M) -&gt; float:\n    \"\"\"\n    Returns the log determinant of the n x n Cholesky decomposition of a matrix M.\n\n    Args:\n        M (ndarray): The n x n Cholesky decomposition of a matrix.\n\n    Returns:\n        float: The log determinant of the matrix M.\n\n    Examples:\n        &gt;&gt;&gt; M = np.array([[2.0, 0.0], [1.0, 1.0]])\n        &gt;&gt;&gt; log_det = log_determinant_chol(M)\n        &gt;&gt;&gt; print(log_det)\n        1.3862943611198906\n    \"\"\"\n    log_det = 0.0\n    n = M.shape[0]\n\n    for i in range(n):\n        log_det += np.log(M[i, i])\n\n    log_det = 2 * log_det\n\n    return log_det\n</code></pre>"},{"location":"reference/spotpython/hyperdict/light_hyper_dict/","title":"light_hyper_dict","text":""},{"location":"reference/spotpython/hyperdict/light_hyper_dict/#spotpython.hyperdict.light_hyper_dict.LightHyperDict","title":"<code>LightHyperDict</code>","text":"<p>               Bases: <code>FileConfig</code></p> <p>Lightning hyperparameter dictionary.</p> <p>This class extends the FileConfig class to provide a dictionary for storing hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The name of the file where the hyperparameters are stored.</p> Source code in <code>spotpython/hyperdict/light_hyper_dict.py</code> <pre><code>class LightHyperDict(base.FileConfig):\n    \"\"\"Lightning hyperparameter dictionary.\n\n    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n\n    Attributes:\n        filename (str):\n            The name of the file where the hyperparameters are stored.\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"light_hyper_dict.json\",\n        directory: None = None,\n    ) -&gt; None:\n        super().__init__(filename=filename, directory=directory)\n        self.filename = filename\n        self.directory = directory\n        self.hyper_dict = self.load()\n\n    @property\n    def path(self):\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    def load(self) -&gt; dict:\n        \"\"\"Load the hyperparameters from the file.\n\n        Returns:\n            dict: A dictionary containing the hyperparameters.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                lhd = LightHyperDict()\n                lhd.hyper_dict\n                {'NetLightRegression': {'l1': {'type': 'int',\n                'default': 3,\n                'transform': 'transform_power_2_int',\n                'lower': 3,\n                'upper': 8},\n                'epochs': {'type': 'int',\n                'default': 4,\n                'transform': 'transform_power_2_int',\n                'lower': 4,\n                'upper': 9},\n                ...\n                'transform': 'None',\n                'class_name': 'torch.optim',\n                'core_model_parameter_type': 'str',\n                'lower': 0,\n                'upper': 11}}}\n            # Assume the user specified file `user_hyper_dict.json` is in the `./hyperdict/` directory.\n            &gt;&gt;&gt; user_lhd = LightHyperDict(filename='user_hyper_dict.json', directory='./hyperdict/')\n        \"\"\"\n        with open(self.path, \"r\") as f:\n            d = json.load(f)\n        return d\n</code></pre>"},{"location":"reference/spotpython/hyperdict/light_hyper_dict/#spotpython.hyperdict.light_hyper_dict.LightHyperDict.load","title":"<code>load()</code>","text":"<p>Load the hyperparameters from the file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    lhd = LightHyperDict()\n    lhd.hyper_dict\n    {'NetLightRegression': {'l1': {'type': 'int',\n    'default': 3,\n    'transform': 'transform_power_2_int',\n    'lower': 3,\n    'upper': 8},\n    'epochs': {'type': 'int',\n    'default': 4,\n    'transform': 'transform_power_2_int',\n    'lower': 4,\n    'upper': 9},\n    ...\n    'transform': 'None',\n    'class_name': 'torch.optim',\n    'core_model_parameter_type': 'str',\n    'lower': 0,\n    'upper': 11}}}\n# Assume the user specified file `user_hyper_dict.json` is in the `./hyperdict/` directory.\n&gt;&gt;&gt; user_lhd = LightHyperDict(filename='user_hyper_dict.json', directory='./hyperdict/')\n</code></pre> Source code in <code>spotpython/hyperdict/light_hyper_dict.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"Load the hyperparameters from the file.\n\n    Returns:\n        dict: A dictionary containing the hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            lhd = LightHyperDict()\n            lhd.hyper_dict\n            {'NetLightRegression': {'l1': {'type': 'int',\n            'default': 3,\n            'transform': 'transform_power_2_int',\n            'lower': 3,\n            'upper': 8},\n            'epochs': {'type': 'int',\n            'default': 4,\n            'transform': 'transform_power_2_int',\n            'lower': 4,\n            'upper': 9},\n            ...\n            'transform': 'None',\n            'class_name': 'torch.optim',\n            'core_model_parameter_type': 'str',\n            'lower': 0,\n            'upper': 11}}}\n        # Assume the user specified file `user_hyper_dict.json` is in the `./hyperdict/` directory.\n        &gt;&gt;&gt; user_lhd = LightHyperDict(filename='user_hyper_dict.json', directory='./hyperdict/')\n    \"\"\"\n    with open(self.path, \"r\") as f:\n        d = json.load(f)\n    return d\n</code></pre>"},{"location":"reference/spotpython/hyperdict/sklearn_hyper_dict/","title":"sklearn_hyper_dict","text":""},{"location":"reference/spotpython/hyperdict/sklearn_hyper_dict/#spotpython.hyperdict.sklearn_hyper_dict.SklearnHyperDict","title":"<code>SklearnHyperDict</code>","text":"<p>               Bases: <code>FileConfig</code></p> <p>Scikit-learn hyperparameter dictionary.</p> <p>This class extends the FileConfig class to provide a dictionary for storing hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The name of the file where the hyperparameters are stored.</p> Source code in <code>spotpython/hyperdict/sklearn_hyper_dict.py</code> <pre><code>class SklearnHyperDict(base.FileConfig):\n    \"\"\"Scikit-learn hyperparameter dictionary.\n\n    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n\n    Attributes:\n        filename (str): The name of the file where the hyperparameters are stored.\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"sklearn_hyper_dict.json\",\n        directory: None = None,\n    ) -&gt; None:\n        super().__init__(filename=filename, directory=directory)\n        self.filename = filename\n        self.directory = directory\n        self.hyper_dict = self.load()\n\n    @property\n    def path(self):\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    def load(self) -&gt; dict:\n        \"\"\"Load the hyperparameters from the file.\n\n        Returns:\n            (dict): A dictionary containing the hyperparameters.\n        Examples:\n            &gt;&gt;&gt; shd = SklearnHyperDict()\n            &gt;&gt;&gt; hyperparams = shd.load()\n            &gt;&gt;&gt; print(hyperparams)\n            {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n        \"\"\"\n        with open(self.path, \"r\") as f:\n            d = json.load(f)\n        return d\n</code></pre>"},{"location":"reference/spotpython/hyperdict/sklearn_hyper_dict/#spotpython.hyperdict.sklearn_hyper_dict.SklearnHyperDict.load","title":"<code>load()</code>","text":"<p>Load the hyperparameters from the file.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the hyperparameters.</p> <p>Examples:     &gt;&gt;&gt; shd = SklearnHyperDict()     &gt;&gt;&gt; hyperparams = shd.load()     &gt;&gt;&gt; print(hyperparams)     {\u2018learning_rate\u2019: 0.001, \u2018batch_size\u2019: 32, \u2018epochs\u2019: 10}</p> Source code in <code>spotpython/hyperdict/sklearn_hyper_dict.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"Load the hyperparameters from the file.\n\n    Returns:\n        (dict): A dictionary containing the hyperparameters.\n    Examples:\n        &gt;&gt;&gt; shd = SklearnHyperDict()\n        &gt;&gt;&gt; hyperparams = shd.load()\n        &gt;&gt;&gt; print(hyperparams)\n        {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n    \"\"\"\n    with open(self.path, \"r\") as f:\n        d = json.load(f)\n    return d\n</code></pre>"},{"location":"reference/spotpython/hyperdict/torch_hyper_dict/","title":"torch_hyper_dict","text":""},{"location":"reference/spotpython/hyperdict/torch_hyper_dict/#spotpython.hyperdict.torch_hyper_dict.TorchHyperDict","title":"<code>TorchHyperDict</code>","text":"<p>               Bases: <code>FileConfig</code></p> <p>PyTorch hyperparameter dictionary.</p> <p>This class extends the FileConfig class to provide a dictionary for storing hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The name of the file where the hyperparameters are stored.</p> Source code in <code>spotpython/hyperdict/torch_hyper_dict.py</code> <pre><code>class TorchHyperDict(base.FileConfig):\n    \"\"\"PyTorch hyperparameter dictionary.\n\n    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n\n    Attributes:\n        filename (str): The name of the file where the hyperparameters are stored.\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"torch_hyper_dict.json\",\n        directory: None = None,\n    ) -&gt; None:\n        super().__init__(filename=filename, directory=directory)\n        self.filename = filename\n        self.directory = directory\n        self.hyper_dict = self.load()\n\n    @property\n    def path(self):\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    def load(self) -&gt; dict:\n        \"\"\"Load the hyperparameters from the file.\n\n        Returns:\n            (dict): A dictionary containing the hyperparameters.\n        Examples:\n            &gt;&gt;&gt; thd = TorchHyperDict()\n            &gt;&gt;&gt; hyperparams = thd.load()\n            &gt;&gt;&gt; print(hyperparams)\n            {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n        \"\"\"\n        with open(self.path, \"r\") as f:\n            d = json.load(f)\n        return d\n</code></pre>"},{"location":"reference/spotpython/hyperdict/torch_hyper_dict/#spotpython.hyperdict.torch_hyper_dict.TorchHyperDict.load","title":"<code>load()</code>","text":"<p>Load the hyperparameters from the file.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the hyperparameters.</p> <p>Examples:     &gt;&gt;&gt; thd = TorchHyperDict()     &gt;&gt;&gt; hyperparams = thd.load()     &gt;&gt;&gt; print(hyperparams)     {\u2018learning_rate\u2019: 0.001, \u2018batch_size\u2019: 32, \u2018epochs\u2019: 10}</p> Source code in <code>spotpython/hyperdict/torch_hyper_dict.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"Load the hyperparameters from the file.\n\n    Returns:\n        (dict): A dictionary containing the hyperparameters.\n    Examples:\n        &gt;&gt;&gt; thd = TorchHyperDict()\n        &gt;&gt;&gt; hyperparams = thd.load()\n        &gt;&gt;&gt; print(hyperparams)\n        {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n    \"\"\"\n    with open(self.path, \"r\") as f:\n        d = json.load(f)\n    return d\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/architecture/","title":"architecture","text":""},{"location":"reference/spotpython/hyperparameters/architecture/#spotpython.hyperparameters.architecture.generate_div2_list","title":"<code>generate_div2_list(input_size, n_min, max_repeats=2)</code>","text":"<p>Generates a list of integers starting from <code>n</code> and repeatedly dividing by 2 until <code>n_min</code> is reached. Each integer is repeated a number of times, starting from 1 and increasing by 1 with each division, up to a maximum of 4 repeats.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>The starting integer.</p> required <code>n_min</code> <code>int</code> <p>The minimum integer value to stop the division.</p> required <code>max_repeats</code> <code>int</code> <p>The maximum number of times an integer can be repeated. Default is 2.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of integers with the described pattern.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.architecture import generate_div2_list\n    for n in range(5, 21):\n        print(generate_div2_list(input_size=n, n_min=5))\n    [5]\n    [6]\n    [7]\n    [8]\n    [9]\n    [10, 5, 5]\n    [11, 5, 5]\n    [12, 6, 6]\n    [13, 6, 6]\n    [14, 7, 7]\n    [15, 7, 7]\n    [16, 8, 8]\n    [17, 8, 8]\n    [18, 9, 9]\n    [19, 9, 9]\n    [20, 10, 10, 5, 5]\n</code></pre> Source code in <code>spotpython/hyperparameters/architecture.py</code> <pre><code>def generate_div2_list(input_size, n_min, max_repeats=2) -&gt; list:\n    \"\"\"\n    Generates a list of integers starting from `n` and repeatedly dividing by 2 until `n_min` is reached.\n    Each integer is repeated a number of times, starting from 1 and increasing by 1 with each division,\n    up to a maximum of 4 repeats.\n\n    Args:\n        input_size (int): The starting integer.\n        n_min (int): The minimum integer value to stop the division.\n        max_repeats (int): The maximum number of times an integer can be repeated. Default is 2.\n\n    Returns:\n        list: A list of integers with the described pattern.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.architecture import generate_div2_list\n            for n in range(5, 21):\n                print(generate_div2_list(input_size=n, n_min=5))\n            [5]\n            [6]\n            [7]\n            [8]\n            [9]\n            [10, 5, 5]\n            [11, 5, 5]\n            [12, 6, 6]\n            [13, 6, 6]\n            [14, 7, 7]\n            [15, 7, 7]\n            [16, 8, 8]\n            [17, 8, 8]\n            [18, 9, 9]\n            [19, 9, 9]\n            [20, 10, 10, 5, 5]\n    \"\"\"\n    result = []\n    current = input_size\n    repeats = 1\n    while current &gt;= n_min:\n        result.extend([current] * min(repeats, max_repeats))\n        current = current // 2\n        repeats = repeats + 1\n    # if result is an empty list, add n_min to it\n    if not result:\n        result.append(n_min)\n    return result\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/architecture/#spotpython.hyperparameters.architecture.get_hidden_sizes","title":"<code>get_hidden_sizes(_L_in, l1, max_n=10)</code>","text":"<p>Generates a list of hidden sizes for a neural network with a given input size. Starting with size l1, the list is generated by dividing the input size by 2 until the minimum size is reached.</p> <p>Parameters:</p> Name Type Description Default <code>_L_in</code> <code>int</code> <p>input size.</p> required <code>l1</code> <code>int</code> <p>number of neurons in the first hidden layer.</p> required <code>max_n</code> <code>int</code> <p>maximum number of hidden sizes to generate. Default is 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>list</code> <p>list of hidden sizes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.architecture import get_hidden_sizes\n    _L_in = 10\n    l1 = 20\n    n = 4\n    get_hidden_sizes(_L_in, l1, max_n=n)\n    [20, 10, 10, 5]\n</code></pre> Source code in <code>spotpython/hyperparameters/architecture.py</code> <pre><code>def get_hidden_sizes(_L_in, l1, max_n=10) -&gt; list:\n    \"\"\"\n    Generates a list of hidden sizes for a neural network with a given input size.\n    Starting with size l1, the list is generated by dividing the input size by 2 until the minimum size is reached.\n\n    Args:\n        _L_in (int):\n            input size.\n        l1 (int):\n            number of neurons in the first hidden layer.\n        max_n (int):\n            maximum number of hidden sizes to generate. Default is 10.\n\n    Returns:\n        (list):\n            list of hidden sizes.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.architecture import get_hidden_sizes\n            _L_in = 10\n            l1 = 20\n            n = 4\n            get_hidden_sizes(_L_in, l1, max_n=n)\n            [20, 10, 10, 5]\n    \"\"\"\n    if l1 &lt; 4:\n        l1 = 4\n    n_low = _L_in // 4\n    n_high = max(l1, 2 * n_low)\n    hidden_sizes = generate_div2_list(n_high, n_low)\n    # keep only the first n values of hidden_sizes list\n    if len(hidden_sizes) &gt; max_n:\n        hidden_sizes = hidden_sizes[:max_n]\n    return hidden_sizes\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/architecture/#spotpython.hyperparameters.architecture.get_three_layers","title":"<code>get_three_layers(_L_in, l1)</code>","text":"<p>Calculate three layers based on input values.</p> <p>Parameters:</p> Name Type Description Default <code>_L_in</code> <code>float</code> <p>The input value to be multiplied.</p> required <code>l1</code> <code>float</code> <p>The multiplier for the layers.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing three calculated layers [a, b, c] where: - a = 3 * l1 * _L_in - b = 2 * l1 * _L_in - c = l1 * _L_in</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.architecture import get_three_layers\n    _L_in = 10\n    l1 = 20\n    get_three_layers(_L_in, l1)\n    [600, 400, 200]\n</code></pre> Source code in <code>spotpython/hyperparameters/architecture.py</code> <pre><code>def get_three_layers(_L_in, l1) -&gt; list:\n    \"\"\"\n    Calculate three layers based on input values.\n\n    Args:\n        _L_in (float): The input value to be multiplied.\n        l1 (float): The multiplier for the layers.\n\n    Returns:\n        list: A list containing three calculated layers [a, b, c] where:\n            - a = 3 * l1 * _L_in\n            - b = 2 * l1 * _L_in\n            - c = l1 * _L_in\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.architecture import get_three_layers\n            _L_in = 10\n            l1 = 20\n            get_three_layers(_L_in, l1)\n            [600, 400, 200]\n    \"\"\"\n    a = 2 * l1 * _L_in\n    b = l1 * _L_in\n    c = ceil(l1 / 2) * _L_in\n    return [a, b, a, b, b, c, c]\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/categorical/","title":"categorical","text":""},{"location":"reference/spotpython/hyperparameters/categorical/#spotpython.hyperparameters.categorical.add_missing_elements","title":"<code>add_missing_elements(a, b)</code>","text":"<p>Add missing elements from list a to list b. Arguments:     a (list): List of elements to check.     b (list): List of elements to add to.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of elements with missing elements from list a added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; a = [1, 4]\n    b = [1, 2]\n    add_missing_elements(a, b)\n    [1, 2, 4]\n</code></pre> Source code in <code>spotpython/hyperparameters/categorical.py</code> <pre><code>def add_missing_elements(a: list, b: list) -&gt; list:\n    \"\"\"Add missing elements from list a to list b.\n    Arguments:\n        a (list): List of elements to check.\n        b (list): List of elements to add to.\n\n    Returns:\n        list: List of elements with missing elements from list a added.\n\n    Examples:\n        &gt;&gt;&gt; a = [1, 4]\n            b = [1, 2]\n            add_missing_elements(a, b)\n            [1, 2, 4]\n    \"\"\"\n    for element in a:\n        if element not in b:\n            b.append(element)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/categorical/#spotpython.hyperparameters.categorical.find_closest_key","title":"<code>find_closest_key(integer_value, encoding_dict)</code>","text":"<p>Given an integer value and an encoding dictionary that maps keys to binary values, this function finds the key in the dictionary whose binary value is closest to the binary representation of the integer value.</p> <p>Parameters:</p> Name Type Description Default <code>integer_value</code> <code>int</code> <p>The integer value to find the closest key for.</p> required <code>encoding_dict</code> <code>dict</code> <p>The encoding dictionary that maps keys to binary values.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The key in the encoding dictionary whose binary value is</p> <code>str</code> <p>closest to the binary representation of the integer value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; encoding_dict = {'A': [1, 0, 0], 'B': [0, 1, 0], 'C': [0, 0, 1]}\n    find_closest_key(6, encoding_dict)\n    'B'\n</code></pre> Source code in <code>spotpython/hyperparameters/categorical.py</code> <pre><code>def find_closest_key(integer_value: int, encoding_dict: dict) -&gt; str:\n    \"\"\"\n    Given an integer value and an encoding dictionary that maps keys to binary values,\n    this function finds the key in the dictionary whose binary value is closest to the binary\n    representation of the integer value.\n\n    Arguments:\n        integer_value (int): The integer value to find the closest key for.\n        encoding_dict (dict): The encoding dictionary that maps keys to binary values.\n\n    Returns:\n        str: The key in the encoding dictionary whose binary value is\n        closest to the binary representation of the integer value.\n\n    Examples:\n        &gt;&gt;&gt; encoding_dict = {'A': [1, 0, 0], 'B': [0, 1, 0], 'C': [0, 0, 1]}\n            find_closest_key(6, encoding_dict)\n            'B'\n    \"\"\"\n    binary_value = [int(x) for x in format(integer_value, f\"0{len(list(encoding_dict.values())[0])}b\")]\n    min_distance = float(\"inf\")\n    closest_key = None\n    for key, encoded_value in encoding_dict.items():\n        distance = sum([x != y for x, y in zip(binary_value, encoded_value)])\n        if distance &lt; min_distance:\n            min_distance = distance\n            closest_key = key\n    return closest_key\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/categorical/#spotpython.hyperparameters.categorical.get_one_hot","title":"<code>get_one_hot(alg, hyper_param, d=None, filename='data.json')</code>","text":"<p>Get one hot encoded values for a hyper parameter of an algorithm. Arguments:     alg (str): Name of the algorithm.     hyper_param (str): Name of the hyper parameter.     d (dict): Dictionary of algorithms and their hyperparameters.     filename (str): Name of the file containing the dictionary. Returns:     dict: Dictionary of hyper parameter values and their one hot encoded values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; alg = \"HoeffdingAdaptiveTreeClassifier\"\n    hyper_param = \"split_criterion\"\n    d = {\n        \"HoeffdingAdaptiveTreeClassifier\": {\n            \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"],\n            \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"],\n            \"bootstrap_sampling\": [\"0\", \"1\"]\n            },\n            \"HoeffdingTreeClassifier\": {\n                \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"],\n                \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"],\n                \"binary_split\": [\"0\", \"1\"],\n                \"stop_mem_management\": [\"0\", \"1\"]\n            }\n        }\n    get_one_hot(alg, hyper_param, d)\n    {'gini': [1, 0, 0], 'info_gain': [0, 1, 0], 'hellinger': [0, 0, 1]}\n</code></pre> Source code in <code>spotpython/hyperparameters/categorical.py</code> <pre><code>def get_one_hot(alg: str, hyper_param: str, d: dict = None, filename: str = \"data.json\") -&gt; dict:\n    \"\"\"Get one hot encoded values for a hyper parameter of an algorithm.\n    Arguments:\n        alg (str): Name of the algorithm.\n        hyper_param (str): Name of the hyper parameter.\n        d (dict): Dictionary of algorithms and their hyperparameters.\n        filename (str): Name of the file containing the dictionary.\n    Returns:\n        dict: Dictionary of hyper parameter values and their one hot encoded values.\n\n    Examples:\n        &gt;&gt;&gt; alg = \"HoeffdingAdaptiveTreeClassifier\"\n            hyper_param = \"split_criterion\"\n            d = {\n                \"HoeffdingAdaptiveTreeClassifier\": {\n                    \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"],\n                    \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"],\n                    \"bootstrap_sampling\": [\"0\", \"1\"]\n                    },\n                    \"HoeffdingTreeClassifier\": {\n                        \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"],\n                        \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"],\n                        \"binary_split\": [\"0\", \"1\"],\n                        \"stop_mem_management\": [\"0\", \"1\"]\n                    }\n                }\n            get_one_hot(alg, hyper_param, d)\n            {'gini': [1, 0, 0], 'info_gain': [0, 1, 0], 'hellinger': [0, 0, 1]}\n    \"\"\"\n    if d is None:\n        with open(filename, \"r\") as f:\n            d = json.load(f)\n    values = d[alg][hyper_param]\n    one_hot_encoded_values = one_hot_encode(values)\n    return one_hot_encoded_values\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/categorical/#spotpython.hyperparameters.categorical.one_hot_encode","title":"<code>one_hot_encode(strings)</code>","text":"<p>One hot encode a list of strings. Arguments:     strings (list): List of strings to encode. Returns:     dict: Dictionary of strings and their one hot encoded values. Examples:     &gt;&gt;&gt; one_hot_encode([\u2018a\u2019, \u2018b\u2019, \u2018c\u2019])     {\u2018a\u2019: [1, 0, 0], \u2018b\u2019: [0, 1, 0], \u2018c\u2019: [0, 0, 1]}</p> Source code in <code>spotpython/hyperparameters/categorical.py</code> <pre><code>def one_hot_encode(strings) -&gt; dict:\n    \"\"\"One hot encode a list of strings.\n    Arguments:\n        strings (list): List of strings to encode.\n    Returns:\n        dict: Dictionary of strings and their one hot encoded values.\n    Examples:\n        &gt;&gt;&gt; one_hot_encode(['a', 'b', 'c'])\n        {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]}\n    \"\"\"\n    n = len(strings)\n    encoding_dict = {}\n    for i, string in enumerate(strings):\n        one_hot_encoded_value = [0] * n\n        one_hot_encoded_value[i] = 1\n        encoding_dict[string] = one_hot_encoded_value\n    return encoding_dict\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/categorical/#spotpython.hyperparameters.categorical.sum_encoded_values","title":"<code>sum_encoded_values(strings, encoding_dict)</code>","text":"<p>Sum the encoded values of a list of strings.</p> <p>Parameters:</p> Name Type Description Default <code>strings</code> <code>list</code> <p>List of strings to encode.</p> required <code>encoding_dict</code> <code>dict</code> <p>Dictionary of strings and their one hot encoded values.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Decimal value of the sum of the encoded values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; encoding_dict = {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]}\n    sum_encoded_values(['a', 'b', 'c'], encoding_dict)\n    7\n    sum_encoded_values(['a', 'c'], encoding_dict)\n    5\n</code></pre> Source code in <code>spotpython/hyperparameters/categorical.py</code> <pre><code>def sum_encoded_values(strings, encoding_dict) -&gt; int:\n    \"\"\"Sum the encoded values of a list of strings.\n\n    Args:\n        strings (list): List of strings to encode.\n        encoding_dict (dict): Dictionary of strings and their one hot encoded values.\n\n    Returns:\n        int: Decimal value of the sum of the encoded values.\n\n    Examples:\n        &gt;&gt;&gt; encoding_dict = {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]}\n            sum_encoded_values(['a', 'b', 'c'], encoding_dict)\n            7\n            sum_encoded_values(['a', 'c'], encoding_dict)\n            5\n    \"\"\"\n    result = [0] * len(list(encoding_dict.values())[0])\n    for string in strings:\n        encoded_value = encoding_dict.get(string)\n        if encoded_value:\n            result = [sum(x) for x in zip(result, encoded_value)]\n    decimal_result = 0\n    for i, value in enumerate(result[::-1]):\n        decimal_result += value * (2**i)\n    return decimal_result\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/listgenerator/","title":"listgenerator","text":""},{"location":"reference/spotpython/hyperparameters/listgenerator/#spotpython.hyperparameters.listgenerator.ListGenerator","title":"<code>ListGenerator</code>","text":"<p>Generates a list of hidden layer sizes based on the input/output layer sizes and specified network shape.</p> <p>Parameters:</p> Name Type Description Default <code>hparams</code> <code>object</code> <p>An object containing network hyperparameters such as the layer sizes and the shape of the network.</p> required <code>L_in</code> <code>int</code> <p>The size of the input layer.</p> required <code>L_out</code> <code>int</code> <p>The size of the output layer.</p> required <p>Methods:</p> Name Description <code>_get_hidden_sizes</code> <p>Generates and returns a list of hidden layer sizes based on the specified network shape (e.g., Funnel, Diamond, Hourglass, Wave, Block).</p> <p>Attributes:</p> Name Type Description <code>hparams.nn_shape</code> <code>str</code> <p>The shape of the network. Options include \u201cFunnel\u201d, \u201cDiamond\u201d, \u201cHourglass\u201d, \u201cWave\u201d, \u201cBlock\u201d.</p> <code>hparams.l1</code> <code>int</code> <p>The size of the first hidden layer.</p> <code>hparams.l_n</code> <code>int</code> <p>The total number of hidden layers.</p> Source code in <code>spotpython/hyperparameters/listgenerator.py</code> <pre><code>class ListGenerator:\n    \"\"\"\n    Generates a list of hidden layer sizes based on the input/output layer sizes and specified network shape.\n\n    Args:\n        hparams (object): An object containing network hyperparameters such as the layer sizes and the shape of the network.\n        L_in (int): The size of the input layer.\n        L_out (int): The size of the output layer.\n\n    Methods:\n        _get_hidden_sizes() -&gt; list:\n            Generates and returns a list of hidden layer sizes based on the specified network shape (e.g., Funnel, Diamond, Hourglass, Wave, Block).\n\n    Attributes:\n        hparams.nn_shape (str): The shape of the network. Options include \"Funnel\", \"Diamond\", \"Hourglass\", \"Wave\", \"Block\".\n        hparams.l1 (int): The size of the first hidden layer.\n        hparams.l_n (int): The total number of hidden layers.\n    \"\"\"\n\n    def __init__(self, hparams, L_in, L_out):\n        self.hparams = hparams\n        self._L_in = L_in\n        self._L_out = L_out\n\n    def _get_hidden_sizes(self):\n        \"\"\"\n        Generate the hidden layer sizes for the network based on the specified shape.\n\n        Returns:\n            list: A list of hidden layer sizes that defines the architecture of the neural network.\n\n        Raises:\n            ValueError: If an unknown `nn_shape` is provided in the `hparams`.\n        \"\"\"\n\n        if self._L_in &lt; 8:\n            n_low = self._L_in  # Minimum number of neurons\n        elif self._L_in &lt; 16:\n            n_low = self._L_in // 2  # Minimum number of neurons\n        else:\n            n_low = self._L_in // 4  # Minimum number of neurons\n        n_high = max(self.hparams.l1, 2 * n_low)  # Maximum number of neurons\n\n        # TODO: \u00dcberlegen, wie rum es besser ist\n        if self.hparams.l_n &gt; self.hparams.l1:\n            self.hparams.l1 = self.hparams.l_n\n            # raise ValueError(\"l_n must be bigger than l1\")\n\n        if self.hparams.nn_shape == \"Funnel\":\n            step_size = (self.hparams.l1 - self._L_out) // self.hparams.l_n\n            hidden_sizes = list(range(self.hparams.l1, self._L_out, -step_size))\n\n        elif self.hparams.nn_shape == \"Diamond\":\n            mid_point = (self.hparams.l_n + 1) // 2\n            upper_limit = self.hparams.l1 * 2\n            step_size_up = (upper_limit - self.hparams.l1) // (mid_point - 1)\n\n            increasing_part = [self.hparams.l1]\n            for _ in range(1, mid_point):\n                next_size = increasing_part[-1] + step_size_up\n                increasing_part.append(min(upper_limit, next_size))\n\n            remaining_layers = self.hparams.l_n - mid_point\n            step_size_down = (increasing_part[-1] - self._L_out) // (remaining_layers + 1)\n\n            decreasing_part = []\n            current_size = increasing_part[-1]\n            for _ in range(remaining_layers):\n                current_size = max(self._L_out, current_size - step_size_down)\n                decreasing_part.append(current_size)\n\n            hidden_sizes = increasing_part + decreasing_part\n\n        elif self.hparams.nn_shape == \"Hourglass\":\n            mid_point = (self.hparams.l_n) // 2\n            step_size = (self.hparams.l1 - n_low) // (mid_point - 1)\n\n            decreasing_part = [self.hparams.l1]\n            for _ in range(1, mid_point):\n                next_size = decreasing_part[-1] - step_size\n                decreasing_part.append(max(n_low, next_size))\n\n            increasing_part = [decreasing_part[-1] + step_size]\n            for _ in range(mid_point, self.hparams.l_n - 2):\n                next_size = increasing_part[-1] + step_size\n                increasing_part.append(min(n_high, next_size))\n\n            last_step_size = (increasing_part[-1] - self._L_out) // 2\n            decreasing_to_output = max(self._L_out, increasing_part[-1] - last_step_size)\n\n            hidden_sizes = decreasing_part + increasing_part + [decreasing_to_output]\n\n        elif self.hparams.nn_shape == \"Wave\":\n            half_wave = (self.hparams.l_n) // 4\n            step_size = (self.hparams.l1 - n_low) // (half_wave - 1)\n\n            decreasing_part_1 = [self.hparams.l1]\n            for _ in range(1, half_wave):\n                next_size = decreasing_part_1[-1] - step_size\n                decreasing_part_1.append(max(n_low, next_size))\n\n            increasing_part_1 = [decreasing_part_1[-1] + step_size]\n            for _ in range(half_wave, 2 * half_wave - 1):\n                next_size = increasing_part_1[-1] + step_size\n                increasing_part_1.append(min(n_high, next_size))\n\n            decreasing_part_2 = [increasing_part_1[-1] - step_size]\n            for _ in range(2 * half_wave, 3 * half_wave - 1):\n                next_size = decreasing_part_2[-1] - step_size\n                decreasing_part_2.append(max(n_low, next_size))\n\n            increasing_part_2 = [decreasing_part_2[-1] + step_size]\n            for _ in range(3 * half_wave, self.hparams.l_n - 2):\n                next_size = increasing_part_2[-1] + step_size\n                increasing_part_2.append(min(n_high, next_size))\n\n            last_step_size = (increasing_part_2[-1] - self._L_out) // 2\n            decreasing_to_output = max(self._L_out, increasing_part_2[-1] - last_step_size)\n\n            hidden_sizes = decreasing_part_1 + increasing_part_1 + decreasing_part_2 + increasing_part_2 + [decreasing_to_output]\n\n        elif self.hparams.nn_shape == \"Block\":\n            hidden_sizes = [min(n_high, self.hparams.l1)] * self.hparams.l_n\n\n        else:\n            raise ValueError(f\"Unknown nn_shape: {self.hparams.nn_shape}\")\n\n        return hidden_sizes\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/optimizer/","title":"optimizer","text":""},{"location":"reference/spotpython/hyperparameters/optimizer/#spotpython.hyperparameters.optimizer.optimizer_handler","title":"<code>optimizer_handler(optimizer_name, params, lr_mult=1.0, **kwargs)</code>","text":"<p>Returns an instance of the specified optimizer. See Notes below for supported optimizers.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_name</code> <code>str</code> <p>The name of the optimizer to use.</p> required <code>params</code> <code>list or Tensor</code> <p>The parameters to optimize.</p> required <code>lr_mult</code> <code>float</code> <p>A multiplier for the learning rate. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the optimizer.</p> <code>{}</code> Notes <p>The following optimizers are supported (see also: https://pytorch.org/docs/stable/optim.html#base-class):</p> <pre><code>* Adadelta\n* Adagrad\n* Adam\n* AdamW\n* SparseAdam\n* ASGD\n* LBFGS\n* NAdam\n* RAdam\n* RMSprop\n* Rprop\n* SGD\n</code></pre> <p>Returns:</p> Type Description <code>Optimizer</code> <p>An instance of the specified optimizer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.netlightregression import NetLightRegression\n    from torch import nn\n    import lightning as L\n    BATCH_SIZE = 8\n    lr_mult=0.1\n    dataset = Diabetes()\n    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    # First example: Adam\n    net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                    initialization='xavier', act_fn=nn.ReLU(),\n                                    optimizer='Adam', dropout_prob=0.1, lr_mult=lr_mult,\n                                    patience=5, _L_in=10, _L_out=1)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n    trainer.fit(net_light_base, train_loader)\n    # Adam uses a lr which is calculated as lr=lr_mult * 0.001, so this value\n    # should be 0.1 * 0.001 = 0.0001\n    trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*0.001\n    # Second example: Adadelta\n    net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                    initialization='xavier', act_fn=nn.ReLU(),\n                                    optimizer='Adadelta', dropout_prob=0.1, lr_mult=lr_mult,\n                                    patience=5, _L_in=10, _L_out=1)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n    trainer.fit(net_light_base, train_loader)\n    # Adadelta uses a lr which is calculated as lr=lr_mult * 1.0, so this value\n    # should be 1.0 * 0.1 = 0.1\n    trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*1.0\n</code></pre> Source code in <code>spotpython/hyperparameters/optimizer.py</code> <pre><code>def optimizer_handler(optimizer_name: str, params: Union[list, torch.Tensor], lr_mult: float = 1.0, **kwargs: Any) -&gt; torch.optim.Optimizer:\n    \"\"\"Returns an instance of the specified optimizer. See Notes below for supported optimizers.\n\n    Args:\n        optimizer_name (str):\n            The name of the optimizer to use.\n        params (list or torch.Tensor):\n            The parameters to optimize.\n        lr_mult (float, optional):\n            A multiplier for the learning rate. Defaults to 1.0.\n        **kwargs:\n            Additional keyword arguments for the optimizer.\n\n    Notes:\n        The following optimizers are supported (see also: https://pytorch.org/docs/stable/optim.html#base-class):\n\n            * Adadelta\n            * Adagrad\n            * Adam\n            * AdamW\n            * SparseAdam\n            * ASGD\n            * LBFGS\n            * NAdam\n            * RAdam\n            * RMSprop\n            * Rprop\n            * SGD\n\n    Returns:\n        (torch.optim.Optimizer):\n            An instance of the specified optimizer.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.netlightregression import NetLightRegression\n            from torch import nn\n            import lightning as L\n            BATCH_SIZE = 8\n            lr_mult=0.1\n            dataset = Diabetes()\n            train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            # First example: Adam\n            net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=lr_mult,\n                                            patience=5, _L_in=10, _L_out=1)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n            trainer.fit(net_light_base, train_loader)\n            # Adam uses a lr which is calculated as lr=lr_mult * 0.001, so this value\n            # should be 0.1 * 0.001 = 0.0001\n            trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*0.001\n            # Second example: Adadelta\n            net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adadelta', dropout_prob=0.1, lr_mult=lr_mult,\n                                            patience=5, _L_in=10, _L_out=1)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n            trainer.fit(net_light_base, train_loader)\n            # Adadelta uses a lr which is calculated as lr=lr_mult * 1.0, so this value\n            # should be 1.0 * 0.1 = 0.1\n            trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*1.0\n    \"\"\"\n    if optimizer_name == \"Adadelta\":\n        return torch.optim.Adadelta(\n            params,\n            lr=lr_mult * 1.0,\n            rho=0.9,\n            eps=1e-06,\n            weight_decay=0,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"Adagrad\":\n        return torch.optim.Adagrad(\n            params,\n            lr=lr_mult * 0.01,\n            lr_decay=0,\n            weight_decay=0,\n            initial_accumulator_value=0,\n            eps=1e-10,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"Adam\":\n        return torch.optim.Adam(\n            params,\n            lr=lr_mult * 0.001,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            amsgrad=False,\n            foreach=None,\n            maximize=False,\n            capturable=False,\n            # differentiable=False,\n            fused=None,\n        )\n    elif optimizer_name == \"AdamW\":\n        return torch.optim.AdamW(\n            params,\n            lr=lr_mult * 0.001,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0.01,\n            amsgrad=False,\n            foreach=None,\n            maximize=False,\n            capturable=False,\n            # differentiable=False,\n            # fused=None,\n        )\n    elif optimizer_name == \"SparseAdam\":\n        return torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, maximize=False)\n    elif optimizer_name == \"Adamax\":\n        return torch.optim.Adamax(\n            params,\n            lr=lr_mult * 0.002,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"ASGD\":\n        return torch.optim.ASGD(\n            params,\n            lr=lr_mult * 0.01,\n            lambd=0.0001,\n            alpha=0.75,\n            t0=1000000.0,\n            weight_decay=0,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"LBFGS\":\n        return torch.optim.LBFGS(\n            params,\n            lr=lr_mult * 1,\n            max_iter=20,\n            max_eval=None,\n            tolerance_grad=1e-07,\n            tolerance_change=1e-09,\n            history_size=100,\n            line_search_fn=None,\n        )\n    elif optimizer_name == \"NAdam\":\n        return torch.optim.NAdam(\n            params,\n            lr=lr_mult * 0.002,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            momentum_decay=0.004,\n            foreach=None,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"RAdam\":\n        return torch.optim.RAdam(\n            params,\n            lr=0.001,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            foreach=None,\n            # differentiable=False\n        )\n    elif optimizer_name == \"RMSprop\":\n        return torch.optim.RMSprop(\n            params,\n            lr=lr_mult * 0.01,\n            alpha=0.99,\n            eps=1e-08,\n            weight_decay=0,\n            momentum=0,\n            centered=False,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"Rprop\":\n        return torch.optim.Rprop(\n            params,\n            lr=lr_mult * 0.01,\n            etas=(0.5, 1.2),\n            step_sizes=(1e-06, 50),\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"SGD\":\n        return torch.optim.SGD(\n            params,\n            lr=lr_mult * 1e-3,\n            momentum=0,\n            dampening=0,\n            weight_decay=0,\n            nesterov=False,\n            maximize=False,\n            foreach=None,\n            # differentiable=False,\n        )\n    else:\n        raise ValueError(f\"Optimizer {optimizer_name} not supported\")\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/","title":"values","text":""},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.add_core_model_to_fun_control","title":"<code>add_core_model_to_fun_control(fun_control, core_model, hyper_dict=None, filename=None)</code>","text":"<p>Add the core model to the function control dictionary. It updates the keys \u201ccore_model\u201d, \u201ccore_model_hyper_dict\u201d, \u201cvar_type\u201d, \u201cvar_name\u201d in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>core_model</code> <code>class</code> <p>The core model.</p> required <code>hyper_dict</code> <code>dict</code> <p>The hyper parameter dictionary. Optional. Default is None. If no hyper_dict is provided, the function will try to load the hyper_dict from the file specified by filename.</p> <code>None</code> <code>filename</code> <code>str</code> <p>The name of the json file that contains the hyper parameter dictionary. Optional. Default is None. If no filename is provided, the function will try to load the hyper_dict from the hyper_dict dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated fun_control dictionary.</p> Notes <p>The function adds the following keys to the fun_control dictionary: \u201ccore_model\u201d: The core model. \u201ccore_model_hyper_dict\u201d: The hyper parameter dictionary for the core model. \u201ccore_model_hyper_dict_default\u201d: The hyper parameter dictionary for the core model. \u201cvar_type\u201d: A list of variable types. \u201cvar_name\u201d: A list of variable names. The original hyperparameters of the core model are stored in the \u201ccore_model_hyper_dict_default\u201d key. These remain unmodified, while the \u201ccore_model_hyper_dict\u201d key is modified during the tuning process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                hyper_dict=LightHyperDict)\n    # or, if a user wants to use a custom hyper_dict:\n&gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                filename=\"./hyperdict/user_hyper_dict.json\")\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def add_core_model_to_fun_control(fun_control, core_model, hyper_dict=None, filename=None) -&gt; dict:\n    \"\"\"Add the core model to the function control dictionary. It updates the keys \"core_model\",\n    \"core_model_hyper_dict\", \"var_type\", \"var_name\" in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        core_model (class):\n            The core model.\n        hyper_dict (dict):\n            The hyper parameter dictionary. Optional. Default is None. If no hyper_dict is provided,\n            the function will try to load the hyper_dict from the file specified by filename.\n        filename (str):\n            The name of the json file that contains the hyper parameter dictionary.\n            Optional. Default is None. If no filename is provided, the function will try to load the\n            hyper_dict from the hyper_dict dictionary.\n\n    Returns:\n        (dict):\n            The updated fun_control dictionary.\n\n    Notes:\n        The function adds the following keys to the fun_control dictionary:\n        \"core_model\": The core model.\n        \"core_model_hyper_dict\": The hyper parameter dictionary for the core model.\n        \"core_model_hyper_dict_default\": The hyper parameter dictionary for the core model.\n        \"var_type\": A list of variable types.\n        \"var_name\": A list of variable names.\n        The original hyperparameters of the core model are stored in the \"core_model_hyper_dict_default\" key.\n        These remain unmodified, while the \"core_model_hyper_dict\" key is modified during the tuning process.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        hyper_dict=LightHyperDict)\n            # or, if a user wants to use a custom hyper_dict:\n        &gt;&gt;&gt; from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        filename=\"./hyperdict/user_hyper_dict.json\")\n\n    \"\"\"\n    fun_control.update({\"core_model\": core_model})\n    if filename is None:\n        new_hyper_dict = hyper_dict().load()\n    else:\n        with open(filename, \"r\") as f:\n            new_hyper_dict = json.load(f)\n    fun_control.update({\"core_model_hyper_dict\": new_hyper_dict[core_model.__name__]})\n    fun_control.update({\"core_model_hyper_dict_default\": copy.deepcopy(new_hyper_dict[core_model.__name__])})\n    var_type = get_var_type(fun_control)\n    var_name = get_var_name(fun_control)\n    lower = get_bound_values(fun_control, \"lower\", as_list=False)\n    upper = get_bound_values(fun_control, \"upper\", as_list=False)\n    fun_control.update({\"var_type\": var_type, \"var_name\": var_name, \"lower\": lower, \"upper\": upper})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.assign_values","title":"<code>assign_values(X, var_list)</code>","text":"<p>This function takes an np.array X and a list of variable names as input arguments and returns a dictionary where the keys are the variable names and the values are assigned from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>A 2D numpy array where each column represents a variable.</p> required <code>var_list</code> <code>list</code> <p>A list of strings representing variable names.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are assigned from X.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the length of var_list does not match the number of columns in X.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.hyperparameters.values import assign_values\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; var_list = ['a', 'b']\n&gt;&gt;&gt; result = assign_values(X, var_list)\n&gt;&gt;&gt; print(result)\n{'a': array([1, 3, 5]), 'b': array([2, 4, 6])}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def assign_values(X: np.array, var_list: list) -&gt; dict:\n    \"\"\"\n    This function takes an np.array X and a list of variable names as input arguments\n    and returns a dictionary where the keys are the variable names and the values are assigned from X.\n\n    Args:\n        X (np.array):\n            A 2D numpy array where each column represents a variable.\n        var_list (list):\n            A list of strings representing variable names.\n\n    Returns:\n        dict:\n            A dictionary where keys are variable names and values are assigned from X.\n\n    Raises:\n        ValueError: If the length of var_list does not match the number of columns in X.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import assign_values\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; var_list = ['a', 'b']\n        &gt;&gt;&gt; result = assign_values(X, var_list)\n        &gt;&gt;&gt; print(result)\n        {'a': array([1, 3, 5]), 'b': array([2, 4, 6])}\n    \"\"\"\n    if X.shape[1] != len(var_list):\n        raise ValueError(\"Length of var_list does not match the number of columns in X.\")\n\n    result = {var_list[i]: X[:, i] for i in range(len(var_list))}\n    return result\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.convert_keys","title":"<code>convert_keys(d, var_type)</code>","text":"<p>Convert values in a dictionary to integers or floats based on a list of variable types. This function processes a dictionary \u2018d\u2019 based on the list of variable types \u2018var_type\u2019. It handles the conversion of strings or other compatible types to \u2018int\u2019 or \u2018float\u2019 as specified. Specifically:     1. If <code>var_type[i]</code> is anything other than <code>\"num\"</code> or <code>\"float\"</code>, the value should be converted to <code>int()</code>.         If the conversion fails (i.e., if the value is not an integer value or representation), an error is raised.     2. If <code>var_type[i]</code> is <code>\"float\"</code>, the value should be converted to <code>float()</code>.     3. If <code>var_type[i]</code> is <code>\"num\"</code>, the function should decide whether to convert the value to <code>int()</code> or <code>float()</code>         based on whether it represents an integer or a float.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>The input dictionary with values to convert.</p> required <code>var_type</code> <code>list</code> <p>A list of variable types where: - Not \u201cnum\u201d or \u201cfloat\u201d: the value is converted to int(). If conversion to int() fails, an error is raised. - \u201cfloat\u201d: convert the value to a float. - \u201cnum\u201d: the value is converted to int() if it represents an integer, otherwise to float().</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Union[int, float]]</code> <p>A modified dictionary with values converted based on \u2018var_type\u2019 settings.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the conversion to an integer is not possible when required.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import convert_keys\n    d = {'a': 1, 'b': 2.1, 'c': 3}\n    var_type = [\"int\", \"num\", \"int\"]\n    convert_keys(d, var_type)\n        {'a': 1, 'b': 2.1, 'c': 3}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def convert_keys(d: Dict[str, Union[int, float, str]], var_type: List[str]) -&gt; Dict[str, Union[int, float]]:\n    \"\"\"Convert values in a dictionary to integers or floats based on a list of variable types.\n    This function processes a dictionary 'd' based on the list of variable types 'var_type'.\n    It handles the conversion of strings or other compatible types to 'int' or 'float' as specified.\n    Specifically:\n        1. If `var_type[i]` is anything other than `\"num\"` or `\"float\"`, the value should be converted to `int()`.\n            If the conversion fails (i.e., if the value is not an integer value or representation), an error is raised.\n        2. If `var_type[i]` is `\"float\"`, the value should be converted to `float()`.\n        3. If `var_type[i]` is `\"num\"`, the function should decide whether to convert the value to `int()` or `float()`\n            based on whether it represents an integer or a float.\n\n    Args:\n        d (dict): The input dictionary with values to convert.\n        var_type (list): A list of variable types where:\n            - Not \"num\" or \"float\": the value is converted to int(). If conversion to int() fails, an error is raised.\n            - \"float\": convert the value to a float.\n            - \"num\": the value is converted to int() if it represents an integer, otherwise to float().\n\n    Returns:\n        dict: A modified dictionary with values converted based on 'var_type' settings.\n\n    Raises:\n        ValueError: If the conversion to an integer is not possible when required.\n\n    Examples:\n            &gt;&gt;&gt; from spotpython.hyperparameters.values import convert_keys\n                d = {'a': 1, 'b': 2.1, 'c': 3}\n                var_type = [\"int\", \"num\", \"int\"]\n                convert_keys(d, var_type)\n                    {'a': 1, 'b': 2.1, 'c': 3}\n    \"\"\"\n    keys = list(d.keys())\n\n    for i in range(len(keys)):\n        try:\n            if var_type[i] not in [\"num\", \"float\"]:\n                value = float(d[keys[i]])\n                if value.is_integer():\n                    d[keys[i]] = int(value)\n                else:\n                    raise ValueError(f\"Invalid value for conversion at {keys[i]}: {d[keys[i]]} (not an integer)\")\n            elif var_type[i] == \"float\":\n                d[keys[i]] = float(d[keys[i]])\n            elif var_type[i] == \"num\":\n                value = float(d[keys[i]])\n                d[keys[i]] = int(value) if value.is_integer() else value\n        except (ValueError, TypeError) as e:\n            raise ValueError(f\"Invalid value for conversion at {keys[i]}: {d[keys[i]]}\")\n\n    return d\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.create_model","title":"<code>create_model(config, fun_control, **kwargs)</code>","text":"<p>Creates a model for the given configuration and control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>dictionary containing the configuration for the hyperparameter tuning.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <code>**kwargs</code> <code>Any</code> <p>additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>object</code> <p>model object.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def create_model(config, fun_control, **kwargs) -&gt; object:\n    \"\"\"\n    Creates a model for the given configuration and control parameters.\n\n    Args:\n        config (dict):\n            dictionary containing the configuration for the hyperparameter tuning.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n        **kwargs (Any):\n            additional keyword arguments.\n\n    Returns:\n        (object):\n            model object.\n    \"\"\"\n    return fun_control[\"core_model\"](**config, **kwargs)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.generate_one_config_from_var_dict","title":"<code>generate_one_config_from_var_dict(var_dict, fun_control, default=False)</code>","text":"<p>Generate one configuration from a dictionary of variables (as a generator).</p> <p>This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d.</p> required <code>default</code> <code>bool</code> <p>A boolean value indicating whether to use the default values from fun_control. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Generator[Dict[str, Union[int, float]], None, None]</code> <p>Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (get_core_model_from_name,\n        add_core_model_to_fun_control, generate_one_config_from_var_dict)\n    import pprint\n    core_model_name=\"light.regression.NNLinearRegressor\"\n    hyperdict=LightHyperDict\n    fun_control = {}\n    _ , core_model_instance = get_core_model_from_name(core_model_name)\n    add_core_model_to_fun_control(\n        core_model=core_model_instance,\n        fun_control=fun_control,\n        hyper_dict=hyperdict,\n        filename=None,\n    )\n    var_dict = {'l1': np.array([3.]),\n                'epochs': np.array([4.]),\n                'batch_size': np.array([4.]),\n                'act_fn': np.array([2.]),\n                'optimizer': np.array([11.]),\n                'dropout_prob': np.array([0.01]),\n                'lr_mult': np.array([1.]),\n                'patience': np.array([2.]),\n                'batch_norm': np.array([0.]),\n                'initialization': np.array([0.])}\n    g = generate_one_config_from_var_dict(var_dict=var_dict, fun_control=fun_control)\n    # Since g is an iterator, we need to call next to get the values\n    values = next(g)\n    pprint.pprint(values)\n        {'act_fn': ReLU(),\n        'batch_norm': False,\n        'batch_size': 16,\n        'dropout_prob': 0.01,\n        'epochs': 16,\n        'initialization': 'Default',\n        'l1': 8,\n        'lr_mult': 1.0,\n        'optimizer': 'SGD',\n        'patience': 4}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def generate_one_config_from_var_dict(\n    var_dict: Dict[str, np.ndarray],\n    fun_control: Dict[str, Union[List[str], str]],\n    default: bool = False,\n) -&gt; Generator[Dict[str, Union[int, float]], None, None]:\n    \"\"\"Generate one configuration from a dictionary of variables (as a generator).\n\n    This function takes a dictionary of variables as input arguments and returns a generator\n    that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Args:\n        var_dict (dict):\n            A dictionary where keys are variable names and values are numpy arrays.\n        fun_control (dict):\n            A dictionary which (at least) has an entry with the following key:\n            \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding\n            value will be converted to the type \"int\".\n        default (bool):\n            A boolean value indicating whether to use the default values from fun_control.\n            Default is False.\n\n    Returns:\n        Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Examples:\n            &gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.hyperparameters.values import (get_core_model_from_name,\n                    add_core_model_to_fun_control, generate_one_config_from_var_dict)\n                import pprint\n                core_model_name=\"light.regression.NNLinearRegressor\"\n                hyperdict=LightHyperDict\n                fun_control = {}\n                _ , core_model_instance = get_core_model_from_name(core_model_name)\n                add_core_model_to_fun_control(\n                    core_model=core_model_instance,\n                    fun_control=fun_control,\n                    hyper_dict=hyperdict,\n                    filename=None,\n                )\n                var_dict = {'l1': np.array([3.]),\n                            'epochs': np.array([4.]),\n                            'batch_size': np.array([4.]),\n                            'act_fn': np.array([2.]),\n                            'optimizer': np.array([11.]),\n                            'dropout_prob': np.array([0.01]),\n                            'lr_mult': np.array([1.]),\n                            'patience': np.array([2.]),\n                            'batch_norm': np.array([0.]),\n                            'initialization': np.array([0.])}\n                g = generate_one_config_from_var_dict(var_dict=var_dict, fun_control=fun_control)\n                # Since g is an iterator, we need to call next to get the values\n                values = next(g)\n                pprint.pprint(values)\n                    {'act_fn': ReLU(),\n                    'batch_norm': False,\n                    'batch_size': 16,\n                    'dropout_prob': 0.01,\n                    'epochs': 16,\n                    'initialization': 'Default',\n                    'l1': 8,\n                    'lr_mult': 1.0,\n                    'optimizer': 'SGD',\n                    'patience': 4}\n    \"\"\"\n    for values in iterate_dict_values(var_dict):\n        values = convert_keys(values, fun_control[\"var_type\"])\n        values = get_dict_with_levels_and_types(fun_control=fun_control, v=values, default=default)\n        values = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values)\n        yield values\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_bound_values","title":"<code>get_bound_values(fun_control, bound, as_list=False)</code>","text":"<p>Generate a list or array from a dictionary. This function takes the values from the keys \u201cbound\u201d in the fun_control[\u201ccore_model_hyper_dict\u201d] dictionary and returns a list or array of the values in the same order as the keys in the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>A dictionary containing a key \u201ccore_model_hyper_dict\u201d which is a dictionary with keys that have either an \u201cupper\u201d or \u201clower\u201d value.</p> required <code>bound</code> <code>str</code> <p>Either \u201cupper\u201d or \u201clower\u201d, indicating which value to extract from the inner dictionary.</p> required <code>as_list</code> <code>bool</code> <p>If True, return a list. If False, return a numpy array. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List, ndarray]</code> <p>list or np.ndarray: A list or array of the extracted values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If bound is not \u201cupper\u201d or \u201clower\u201d.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_bound_values\n&gt;&gt;&gt; fun_control = {\"core_model_hyper_dict\": {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}}}\n&gt;&gt;&gt; get_bound_values(fun_control, \"upper\", as_list=True)\n[1, 2]\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_bound_values(fun_control: dict, bound: str, as_list: bool = False) -&gt; Union[List, np.ndarray]:\n    \"\"\"Generate a list or array from a dictionary.\n    This function takes the values from the keys \"bound\" in the\n    fun_control[\"core_model_hyper_dict\"] dictionary and returns a list or array of the values\n    in the same order as the keys in the dictionary.\n\n    Args:\n        fun_control (dict):\n            A dictionary containing a key \"core_model_hyper_dict\"\n            which is a dictionary with keys that have either an \"upper\" or \"lower\" value.\n        bound (str):\n            Either \"upper\" or \"lower\",\n            indicating which value to extract from the inner dictionary.\n        as_list (bool):\n            If True, return a list.\n            If False, return a numpy array. Default is False.\n\n    Returns:\n        list or np.ndarray:\n            A list or array of the extracted values.\n\n    Raises:\n        ValueError:\n            If bound is not \"upper\" or \"lower\".\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_bound_values\n        &gt;&gt;&gt; fun_control = {\"core_model_hyper_dict\": {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}}}\n        &gt;&gt;&gt; get_bound_values(fun_control, \"upper\", as_list=True)\n        [1, 2]\n    \"\"\"\n    # Throw value error if bound is not upper or lower:\n    if bound not in [\"upper\", \"lower\"]:\n        raise ValueError(\"bound must be either 'upper' or 'lower'\")\n    # check if key \"core_model_hyper_dict\" exists in fun_control:\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        d = fun_control[\"core_model_hyper_dict\"]\n        b = []\n        for key, value in d.items():\n            b.append(value[bound])\n        if as_list:\n            return b\n        else:\n            return np.array(b)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_control_key_value","title":"<code>get_control_key_value(control_dict=None, key=None)</code>","text":"<p>This function gets the key value pair from the control_dict dictionary. If the key does not exist, return None. If the control_dict dictionary is None, return None.</p> <p>Parameters:</p> Name Type Description Default <code>control_dict</code> <code>dict</code> <p>control_dict dictionary</p> <code>None</code> <code>key</code> <code>str</code> <p>key</p> <code>None</code> <p>Returns:</p> Name Type Description <code>value</code> <code>Any</code> <p>value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import get_control_key_value\n    control_dict = fun_control_init()\n    get_control_key_value(control_dict=control_dict,\n                    key=\"key\")\n    \"value\"\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_control_key_value(control_dict=None, key=None) -&gt; Any:\n    \"\"\"\n    This function gets the key value pair from the control_dict dictionary.\n    If the key does not exist, return None.\n    If the control_dict dictionary is None, return None.\n\n    Args:\n        control_dict (dict):\n            control_dict dictionary\n        key (str): key\n\n    Returns:\n        value (Any):\n            value\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import get_control_key_value\n            control_dict = fun_control_init()\n            get_control_key_value(control_dict=control_dict,\n                            key=\"key\")\n            \"value\"\n    \"\"\"\n    if control_dict is None:\n        return None\n    else:\n        # check if key \"core_model_hyper_dict\" exists in fun_control:\n        if \"core_model_hyper_dict\" in control_dict.keys():\n            if key == \"lower\":\n                lower = get_bound_values(fun_control=control_dict, bound=\"lower\")\n                return lower\n            if key == \"upper\":\n                upper = get_bound_values(fun_control=control_dict, bound=\"upper\")\n                return upper\n            if key == \"var_name\":\n                var_name = get_var_name(fun_control=control_dict)\n                return var_name\n            if key == \"var_type\":\n                var_type = get_var_type(fun_control=control_dict)\n                return var_type\n            if key == \"transform\":\n                transform = get_transform(fun_control=control_dict)\n                return transform\n        # check if key exists in control_dict:\n        elif control_dict is None or key not in control_dict.keys():\n            return None\n        else:\n            return control_dict[key]\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_core_model_from_name","title":"<code>get_core_model_from_name(core_model_name)</code>","text":"<p>Returns the sklearn or spotpython lightning core model name and instance from a core model name.</p> <p>Parameters:</p> Name Type Description Default <code>core_model_name</code> <code>str</code> <p>The full name of the core model in the format \u2018module.Model\u2019.</p> required <p>Returns:</p> Type Description <code>(str, object)</code> <p>A tuple containing the core model name and an instance of the core model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model_name, model_instance = get_core_model_from_name(\"light.regression.NNLinearRegressor\")\n    print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")\n        Model Name:\n        NNLinearRegressor,\n        Model Instance:\n        &lt;class 'spotpython.light.regression.nn_linear_regressor.NNLinearRegressor'&gt;\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_core_model_from_name(core_model_name: str) -&gt; tuple:\n    \"\"\"\n    Returns the sklearn or spotpython lightning core model name and instance from a core model name.\n\n    Args:\n        core_model_name (str): The full name of the core model in the format 'module.Model'.\n\n    Returns:\n        (str, object): A tuple containing the core model name and an instance of the core model.\n\n    Examples:\n        &gt;&gt;&gt; model_name, model_instance = get_core_model_from_name(\"light.regression.NNLinearRegressor\")\n            print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")\n                Model Name:\n                NNLinearRegressor,\n                Model Instance:\n                &lt;class 'spotpython.light.regression.nn_linear_regressor.NNLinearRegressor'&gt;\n    \"\"\"\n    # Split the model name into its components\n    name_parts = core_model_name.split(\".\")\n    if len(name_parts) &lt; 2:\n        raise ValueError(f\"Invalid core model name: {core_model_name}. Expected format: 'module.ModelName'.\")\n    module_name = name_parts[0]\n    model_name = name_parts[1]\n    try:\n        # Try to get the model from the sklearn library\n        core_model_instance = getattr(getattr(sklearn, module_name), model_name)\n        return model_name, core_model_instance\n    except AttributeError:\n        try:\n            # Try to get the model from the spotpython library\n            submodule_name = name_parts[1]\n            model_name = name_parts[2] if len(name_parts) == 3 else model_name\n            print(f\"module_name: {module_name}\")\n            print(f\"submodule_name: {submodule_name}\")\n            print(f\"model_name: {model_name}\")\n            core_model_instance = getattr(getattr(getattr(spotpython, module_name), submodule_name), model_name)\n            return model_name, core_model_instance\n        except AttributeError:\n            raise ValueError(f\"Model '{core_model_name}' not found in either 'sklearn' or 'spotpython lightning' libraries.\")\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_core_model_parameter_type_from_var_name","title":"<code>get_core_model_parameter_type_from_var_name(fun_control, var_name)</code>","text":"<p>Extracts the core_model_parameter_type value from a dictionary for a specified key.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The dictionary containing the information.</p> required <code>var_name</code> <code>str</code> <p>The key for which to extract the core_model_parameter_type value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The core_model_parameter_type value if available, else None.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_core_model_parameter_type_from_var_name(fun_control, var_name) -&gt; str:\n    \"\"\"\n    Extracts the core_model_parameter_type value from a dictionary for a specified key.\n\n    Args:\n        fun_control (dict):\n            The dictionary containing the information.\n        var_name (str):\n            The key for which to extract the core_model_parameter_type value.\n\n    Returns:\n        (str):\n            The core_model_parameter_type value if available, else None.\n    \"\"\"\n    # Check if the key exists in the dictionary and it has a 'core_model_parameter_type' entry\n    if var_name in fun_control[\"core_model_hyper_dict\"] and \"core_model_parameter_type\" in fun_control[\"core_model_hyper_dict\"][var_name]:\n        return fun_control[\"core_model_hyper_dict\"][var_name][\"core_model_parameter_type\"]\n    else:\n        return None\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_default_hyperparameters_as_array","title":"<code>get_default_hyperparameters_as_array(fun_control)</code>","text":"<p>Get the default hyper parameters as array.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The default hyper parameters as array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    from spotpython.hyperparameters.values import (\n        get_default_hyperparameters_as_array,\n        add_core_model_to_fun_control)\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    get_default_hyperparameters_as_array(fun_control)\n    array([0, 0, 0, 0, 0])\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_default_hyperparameters_as_array(fun_control) -&gt; np.array:\n    \"\"\"Get the default hyper parameters as array.\n\n    Args:\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (np.array):\n            The default hyper parameters as array.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            from spotpython.hyperparameters.values import (\n                get_default_hyperparameters_as_array,\n                add_core_model_to_fun_control)\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            get_default_hyperparameters_as_array(fun_control)\n            array([0, 0, 0, 0, 0])\n    \"\"\"\n    X0 = get_default_values(fun_control)\n    X0 = replace_levels_with_positions(fun_control[\"core_model_hyper_dict_default\"], X0)\n    if X0 is None:\n        return None\n    else:\n        X0 = get_values_from_dict(X0)\n        X0 = np.array([X0])\n        X0.shape[1]\n        return X0\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_default_values","title":"<code>get_default_values(fun_control)</code>","text":"<p>Get the values from the \u201cdefault\u201d keys from the dictionary fun_control as a dict. If the key of the value has as \u201ctype\u201d the value \u201cint\u201d or \u201cfloat\u201d, convert the value to the corresponding type.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with levels and types</p> required <p>Returns:</p> Name Type Description <code>new_dict</code> <code>dict</code> <p>dictionary with default values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n    d = {\"core_model_hyper_dict\":{\n        \"leaf_prediction\": {\n            \"levels\": [\"mean\", \"model\", \"adaptive\"],\n            \"type\": \"factor\",\n            \"default\": \"mean\",\n            \"core_model_parameter_type\": \"str\"},\n        \"leaf_model\": {\n            \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n            \"type\": \"factor\",\n            \"default\": \"LinearRegression\",\n            \"core_model_parameter_type\": \"instance\"},\n        \"splitter\": {\n            \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n            \"type\": \"factor\",\n            \"default\": \"EBSTSplitter\",\n            \"core_model_parameter_type\": \"instance()\"},\n        \"binary_split\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"},\n        \"stop_mem_management\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"}}}\n    get_default_values(d)\n    {'leaf_prediction': 'mean',\n    'leaf_model': 'linear_model.LinearRegression',\n    'splitter': 'EBSTSplitter',\n    'binary_split': 0,\n    'stop_mem_management': 0}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_default_values(fun_control) -&gt; dict:\n    \"\"\"Get the values from the \"default\" keys from the dictionary fun_control as a dict.\n    If the key of the value has as \"type\" the value \"int\" or \"float\", convert the value to the corresponding type.\n\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n\n    Returns:\n        new_dict (dict):\n            dictionary with default values\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n            d = {\"core_model_hyper_dict\":{\n                \"leaf_prediction\": {\n                    \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                    \"type\": \"factor\",\n                    \"default\": \"mean\",\n                    \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}}\n            get_default_values(d)\n            {'leaf_prediction': 'mean',\n            'leaf_model': 'linear_model.LinearRegression',\n            'splitter': 'EBSTSplitter',\n            'binary_split': 0,\n            'stop_mem_management': 0}\n    \"\"\"\n    d = fun_control[\"core_model_hyper_dict_default\"]\n    new_dict = {}\n    for key, value in d.items():\n        if value[\"type\"] == \"int\":\n            new_dict[key] = int(value[\"default\"])\n        elif value[\"type\"] == \"float\":\n            new_dict[key] = float(value[\"default\"])\n        else:\n            new_dict[key] = value[\"default\"]\n    return new_dict\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_dict_with_levels_and_types","title":"<code>get_dict_with_levels_and_types(fun_control, v, default=False)</code>","text":"<p>Get dictionary with levels and types. The function maps the numerical output of the hyperparameter optimization to the corresponding levels of the hyperparameter needed by the core model, i.e., the tuned algorithm. The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. If the key value in the dictionary is 0, it takes the first value from the list, if it is 1, it takes the second and so on. If a key is not in fun_control, it takes the key from v. If the core_model_parameter_type value is instance, it returns the class of the value from the module via getattr(\u201cclass\u201d, value).</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>Dict[str, Any]</code> <p>A dictionary containing information about the core model hyperparameters.</p> required <code>v</code> <code>Dict[str, Any]</code> <p>A dictionary containing the numerical output of the hyperparameter optimization.</p> required <code>default</code> <code>bool</code> <p>A boolean value indicating whether to use the default values from fun_control.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n    from spotpython.hyperparameters.values import (assign_values, get_var_name, iterate_dict_values,\n        convert_keys, get_dict_with_levels_and_types, get_core_model_from_name, add_core_model_to_fun_control)\n    import pprint\n    core_model_name=\"light.regression.NNLinearRegressor\"\n    hyperdict=LightHyperDict\n    fun_control = {}\n    _ , core_model_instance = get_core_model_from_name(core_model_name)\n    add_core_model_to_fun_control(\n        core_model=core_model_instance,\n        fun_control=fun_control,\n        hyper_dict=hyperdict,\n        filename=None,\n    )\n    v = {'act_fn': 2,\n                'batch_norm': 0,\n                'batch_size': 4,\n                'dropout_prob': 0.01,\n                'epochs': 4,\n                'initialization': 0,\n                'l1': 3,\n                'lr_mult': 1.0,\n                'optimizer': 11,\n                'patience': 2}\n    get_dict_with_levels_and_types(fun_control=fun_control, v=v)\n        {'act_fn': ReLU(),\n        'batch_norm': False,\n        'batch_size': 4,\n        'dropout_prob': 0.01,\n        'epochs': 4,\n        'initialization': 'Default',\n        'l1': 3,\n        'lr_mult': 1.0,\n        'optimizer': 'SGD',\n        'patience': 2}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_dict_with_levels_and_types(fun_control: Dict[str, Any], v: Dict[str, Any], default=False) -&gt; Dict[str, Any]:\n    \"\"\"Get dictionary with levels and types.\n    The function maps the numerical output of the hyperparameter optimization to the corresponding levels\n    of the hyperparameter needed by the core model, i.e., the tuned algorithm.\n    The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v\n    but with the values of the levels of the keys from fun_control.\n    If the key value in the dictionary is 0, it takes the first value from the list,\n    if it is 1, it takes the second and so on.\n    If a key is not in fun_control, it takes the key from v.\n    If the core_model_parameter_type value is instance, it returns the class of the value from the module\n    via getattr(\"class\", value).\n\n    Args:\n        fun_control (Dict[str, Any]):\n            A dictionary containing information about the core model hyperparameters.\n        v (Dict[str, Any]):\n            A dictionary containing the numerical output of the hyperparameter optimization.\n        default (bool):\n            A boolean value indicating whether to use the default values from fun_control.\n\n    Returns:\n        Dict[str, Any]:\n            A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control.\n\n    Examples:\n            &gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n                from spotpython.hyperparameters.values import (assign_values, get_var_name, iterate_dict_values,\n                    convert_keys, get_dict_with_levels_and_types, get_core_model_from_name, add_core_model_to_fun_control)\n                import pprint\n                core_model_name=\"light.regression.NNLinearRegressor\"\n                hyperdict=LightHyperDict\n                fun_control = {}\n                _ , core_model_instance = get_core_model_from_name(core_model_name)\n                add_core_model_to_fun_control(\n                    core_model=core_model_instance,\n                    fun_control=fun_control,\n                    hyper_dict=hyperdict,\n                    filename=None,\n                )\n                v = {'act_fn': 2,\n                            'batch_norm': 0,\n                            'batch_size': 4,\n                            'dropout_prob': 0.01,\n                            'epochs': 4,\n                            'initialization': 0,\n                            'l1': 3,\n                            'lr_mult': 1.0,\n                            'optimizer': 11,\n                            'patience': 2}\n                get_dict_with_levels_and_types(fun_control=fun_control, v=v)\n                    {'act_fn': ReLU(),\n                    'batch_norm': False,\n                    'batch_size': 4,\n                    'dropout_prob': 0.01,\n                    'epochs': 4,\n                    'initialization': 'Default',\n                    'l1': 3,\n                    'lr_mult': 1.0,\n                    'optimizer': 'SGD',\n                    'patience': 2}\n    \"\"\"\n    if default:\n        d = fun_control[\"core_model_hyper_dict_default\"]\n    else:\n        d = fun_control[\"core_model_hyper_dict\"]\n    new_dict = {}\n    for key, value in v.items():\n        if key in d and d[key][\"type\"] == \"factor\":\n            if d[key][\"core_model_parameter_type\"] == \"instance\":\n                if \"class_name\" in d[key]:\n                    mdl = d[key][\"class_name\"]\n                c = d[key][\"levels\"][value]\n                new_dict[key] = class_for_name(mdl, c)\n            elif d[key][\"core_model_parameter_type\"] == \"instance()\":\n                mdl = d[key][\"class_name\"]\n                c = d[key][\"levels\"][value]\n                k = class_for_name(mdl, c)\n                new_dict[key] = k()\n            # bool() introduced to convert 0 and 1 to False and True in v0.14.54\n            elif d[key][\"core_model_parameter_type\"] == \"bool\":\n                new_dict[key] = bool(d[key][\"levels\"][value])\n            else:\n                new_dict[key] = d[key][\"levels\"][value]\n        else:\n            new_dict[key] = v[key]\n    return new_dict\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_ith_hyperparameter_name_from_fun_control","title":"<code>get_ith_hyperparameter_name_from_fun_control(fun_control, key, i)</code>","text":"<p>Get the ith hyperparameter name from the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>key</code> <code>str</code> <p>key</p> required <code>i</code> <code>int</code> <p>index</p> required <p>Returns:</p> Type Description <code>str</code> <p>hyperparameter name</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.device import getDevice\n    from spotpython.utils.init import fun_control_init\n    from spotpython.utils.file import get_experiment_name\n    import numpy as np\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.hyperparameters.values import set_control_hyperparameter_value\n    experiment_name = get_experiment_name(prefix=\"000\")\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        TENSORBOARD_CLEAN=True,\n        device=getDevice(),\n        enable_progress_bar=False,\n        fun_evals=15,\n        log_level=10,\n        max_time=1,\n        num_workers=0,\n        show_progress=True,\n        tolerance_x=np.sqrt(np.spacing(1)),\n        )\n    dataset = Diabetes()\n    set_control_key_value(control_dict=fun_control,\n                            key=\"data_set\",\n                            value=dataset,\n                            replace=True)\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n</code></pre> <pre><code>set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\nset_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\nget_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0)\nAdam\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_ith_hyperparameter_name_from_fun_control(fun_control, key, i):\n    \"\"\"\n    Get the ith hyperparameter name from the fun_control dictionary.\n\n    Args:\n        fun_control (dict): fun_control dictionary\n        key (str): key\n        i (int): index\n\n    Returns:\n        (str): hyperparameter name\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.device import getDevice\n            from spotpython.utils.init import fun_control_init\n            from spotpython.utils.file import get_experiment_name\n            import numpy as np\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.hyperparameters.values import set_control_hyperparameter_value\n            experiment_name = get_experiment_name(prefix=\"000\")\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                TENSORBOARD_CLEAN=True,\n                device=getDevice(),\n                enable_progress_bar=False,\n                fun_evals=15,\n                log_level=10,\n                max_time=1,\n                num_workers=0,\n                show_progress=True,\n                tolerance_x=np.sqrt(np.spacing(1)),\n                )\n            dataset = Diabetes()\n            set_control_key_value(control_dict=fun_control,\n                                    key=\"data_set\",\n                                    value=dataset,\n                                    replace=True)\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n\n            set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n            set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n            get_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0)\n            Adam\n\n    \"\"\"\n    if \"core_model_hyper_dict\" in fun_control:\n        if key in fun_control[\"core_model_hyper_dict\"]:\n            if \"levels\" in fun_control[\"core_model_hyper_dict\"][key]:\n                if i &lt; len(fun_control[\"core_model_hyper_dict\"][key][\"levels\"]):\n                    return fun_control[\"core_model_hyper_dict\"][key][\"levels\"][i]\n    return None\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_metric_sklearn","title":"<code>get_metric_sklearn(metric_name)</code>","text":"<p>Returns the sklearn metric from the metric name.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The name of the metric.</p> required <p>Returns:</p> Type Description <code>object</code> <p>sklearn.metrics (object): The sklearn metric.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_metric_sklearn(metric_name) -&gt; object:\n    \"\"\"\n    Returns the sklearn metric from the metric name.\n\n    Args:\n        metric_name (str): The name of the metric.\n\n    Returns:\n        sklearn.metrics (object): The sklearn metric.\n    \"\"\"\n    metric_sklearn = getattr(sklearn.metrics, metric_name)\n    return metric_sklearn\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_one_config_from_X","title":"<code>get_one_config_from_X(X, fun_control=None)</code>","text":"<p>Get one config from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The config dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n    from spotpython.hyperparameters.values import get_core_model_from_name, add_core_model_to_fun_control, get_one_config_from_X\n    core_model_name=\"light.regression.NNLinearRegressor\"\n    hyperdict=LightHyperDict\n    fun_control = {}\n    coremodel, core_model_instance = get_core_model_from_name(core_model_name)\n    add_core_model_to_fun_control(\n        core_model=core_model_instance,\n        fun_control=fun_control,\n        hyper_dict=hyperdict,\n        filename=None,\n    )\n    X = get_default_hyperparameters_as_array(fun_control)\n    get_one_config_from_X(X, fun_control)\n        {'l1': 8,\n        'epochs': 16,\n        'batch_size': 16,\n        'act_fn': ReLU(),\n        'optimizer': 'SGD',\n        'dropout_prob': 0.01,\n        'lr_mult': 1.0,\n        'patience': 4,\n        'batch_norm': False,\n        'initialization': 'Default'}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_one_config_from_X(X, fun_control=None) -&gt; dict:\n    \"\"\"Get one config from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The fun_control dictionary.\n\n    Returns:\n        (dict):\n            The config dictionary.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n            from spotpython.hyperparameters.values import get_core_model_from_name, add_core_model_to_fun_control, get_one_config_from_X\n            core_model_name=\"light.regression.NNLinearRegressor\"\n            hyperdict=LightHyperDict\n            fun_control = {}\n            coremodel, core_model_instance = get_core_model_from_name(core_model_name)\n            add_core_model_to_fun_control(\n                core_model=core_model_instance,\n                fun_control=fun_control,\n                hyper_dict=hyperdict,\n                filename=None,\n            )\n            X = get_default_hyperparameters_as_array(fun_control)\n            get_one_config_from_X(X, fun_control)\n                {'l1': 8,\n                'epochs': 16,\n                'batch_size': 16,\n                'act_fn': ReLU(),\n                'optimizer': 'SGD',\n                'dropout_prob': 0.01,\n                'lr_mult': 1.0,\n                'patience': 4,\n                'batch_norm': False,\n                'initialization': 'Default'}\n    \"\"\"\n    var_dict = assign_values(X, fun_control[\"var_name\"])\n    config = return_conf_list_from_var_dict(var_dict, fun_control)[0]\n    return config\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_one_core_model_from_X","title":"<code>get_one_core_model_from_X(X, fun_control=None, default=False)</code>","text":"<p>Get one core model from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <code>default</code> <code>bool</code> <p>A boolean value indicating whether to use the default values from fun_control.</p> <code>False</code> <p>Returns:</p> Type Description <code>class</code> <p>The core model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=fun_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_core_model_from_X(X, fun_control)\n    HoeffdingAdaptiveTreeRegressor()\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_one_core_model_from_X(\n    X,\n    fun_control=None,\n    default=False,\n):\n    \"\"\"Get one core model from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n        default (bool):\n            A boolean value indicating whether to use the default values from fun_control.\n\n    Returns:\n        (class):\n            The core model.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=fun_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_core_model_from_X(X, fun_control)\n            HoeffdingAdaptiveTreeRegressor()\n    \"\"\"\n    var_dict = assign_values(X, fun_control[\"var_name\"])\n    # var_dict = assign_values(X, get_var_name(fun_control))\n    config = return_conf_list_from_var_dict(var_dict, fun_control, default=default)[0]\n    core_model = fun_control[\"core_model\"](**config)\n    return core_model\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_one_river_model_from_X","title":"<code>get_one_river_model_from_X(X, fun_control=None)</code>","text":"<p>Get one river model from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>class</code> <p>The river model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotriver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_river_model_from_X(X, fun_control)\n    HoeffdingAdaptiveTreeRegressor()\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_one_river_model_from_X(X, fun_control=None):\n    \"\"\"Get one river model from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (class):\n            The river model.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotriver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_river_model_from_X(X, fun_control)\n            HoeffdingAdaptiveTreeRegressor()\n    \"\"\"\n    core_model = get_one_core_model_from_X(X=X, fun_control=fun_control)\n    if fun_control[\"prep_model\"] is not None:\n        model = compose.Pipeline(fun_control[\"prep_model\"], core_model)\n    else:\n        model = core_model\n    return model\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_one_sklearn_model_from_X","title":"<code>get_one_sklearn_model_from_X(X, fun_control=None)</code>","text":"<p>Get one sklearn model from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>class</code> <p>The sklearn model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n    from spotriver.data.sklearn_hyper_dict import SklearnHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=LinearRegression,\n        fun_control=func_control,\n        hyper_dict=SklearnHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_sklearn_model_from_X(X, fun_control)\n    LinearRegression()\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_one_sklearn_model_from_X(X, fun_control=None):\n    \"\"\"Get one sklearn model from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (class):\n            The sklearn model.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n            from spotriver.data.sklearn_hyper_dict import SklearnHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=LinearRegression,\n                fun_control=func_control,\n                hyper_dict=SklearnHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_sklearn_model_from_X(X, fun_control)\n            LinearRegression()\n    \"\"\"\n    core_model = get_one_core_model_from_X(X=X, fun_control=fun_control)\n    if fun_control[\"prep_model\"] is not None:\n        model = make_pipeline(fun_control[\"prep_model\"], core_model)\n    else:\n        model = core_model\n    return model\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_prep_model","title":"<code>get_prep_model(prepmodel_name)</code>","text":"<p>Get the sklearn preprocessing model from the name.</p> <p>Parameters:</p> Name Type Description Default <code>prepmodel_name</code> <code>str</code> <p>The name of the preprocessing model.</p> required <p>Returns:</p> Type Description <code>object</code> <p>river.preprocessing (object): The river preprocessing model.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_prep_model(prepmodel_name) -&gt; object:\n    \"\"\"\n    Get the sklearn preprocessing model from the name.\n\n    Args:\n        prepmodel_name (str): The name of the preprocessing model.\n\n    Returns:\n        river.preprocessing (object): The river preprocessing model.\n\n    \"\"\"\n    if prepmodel_name == \"None\":\n        prepmodel = None\n    else:\n        prepmodel = getattr(sklearn.preprocessing, prepmodel_name)\n    return prepmodel\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_river_core_model_from_name","title":"<code>get_river_core_model_from_name(core_model_name)</code>","text":"<p>Returns the river core model name and instance from a core model name.</p> <p>Parameters:</p> Name Type Description Default <code>core_model_name</code> <code>str</code> <p>The full name of the core model in the format \u2018module.Model\u2019.</p> required <p>Returns:</p> Type Description <code>(str, object)</code> <p>A tuple containing the core model name and an instance of the core model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_core_model_from_name\n    model_name, model_instance = get_core_model_from_name('tree.HoeffdingTreeRegressor')\n    print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")\n        Model Name:\n        HoeffdingTreeRegressor,\n        Model Instance:\n        &lt;class 'river.tree.hoeffding_tree_regressor.HoeffdingTreeRegressor'&gt;\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_river_core_model_from_name(core_model_name: str) -&gt; tuple:\n    \"\"\"\n    Returns the river core model name and instance from a core model name.\n\n    Args:\n        core_model_name (str): The full name of the core model in the format 'module.Model'.\n\n    Returns:\n        (str, object): A tuple containing the core model name and an instance of the core model.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_core_model_from_name\n            model_name, model_instance = get_core_model_from_name('tree.HoeffdingTreeRegressor')\n            print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")\n                Model Name:\n                HoeffdingTreeRegressor,\n                Model Instance:\n                &lt;class 'river.tree.hoeffding_tree_regressor.HoeffdingTreeRegressor'&gt;\n    \"\"\"\n    # Split the model name into its components\n    name_parts = core_model_name.split(\".\")\n    if len(name_parts) &lt; 2:\n        raise ValueError(f\"Invalid core model name: {core_model_name}. Expected format: 'module.ModelName'.\")\n    module_name = name_parts[0]\n    model_name = name_parts[1]\n    try:\n        # Try to get the model from the river library\n        core_model_instance = getattr(getattr(river, module_name), model_name)\n        return model_name, core_model_instance\n    except AttributeError:\n        raise ValueError(f\"Model '{core_model_name}' not found in either 'river' libraries.\")\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_river_prep_model","title":"<code>get_river_prep_model(prepmodel_name)</code>","text":"<p>Get the river preprocessing model from the name.</p> <p>Parameters:</p> Name Type Description Default <code>prepmodel_name</code> <code>str</code> <p>The name of the preprocessing model.</p> required <p>Returns:</p> Type Description <code>object</code> <p>river.preprocessing (object): The river preprocessing model.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_river_prep_model(prepmodel_name) -&gt; object:\n    \"\"\"\n    Get the river preprocessing model from the name.\n\n    Args:\n        prepmodel_name (str): The name of the preprocessing model.\n\n    Returns:\n        river.preprocessing (object): The river preprocessing model.\n\n    \"\"\"\n    if prepmodel_name == \"None\":\n        prepmodel = None\n    else:\n        prepmodel = getattr(river.preprocessing, prepmodel_name)\n    return prepmodel\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_sklearn_scaler","title":"<code>get_sklearn_scaler(scaler_name)</code>","text":"<p>Get the sklearn scaler model from the name.</p> <p>Parameters:</p> Name Type Description Default <code>scaler_name</code> <code>str</code> <p>The name of the preprocessing model.</p> required <p>Returns:</p> Type Description <code>object</code> <p>sklearn.preprocessing (object): The sklearn scaler.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_sklearn_scaler(scaler_name) -&gt; object:\n    \"\"\"\n    Get the sklearn scaler model from the name.\n\n    Args:\n        scaler_name (str): The name of the preprocessing model.\n\n    Returns:\n        sklearn.preprocessing (object): The sklearn scaler.\n\n    \"\"\"\n    if scaler_name == \"None\":\n        scaler = None\n    else:\n        scaler = getattr(sklearn.preprocessing, scaler_name)\n    return scaler\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_transform","title":"<code>get_transform(fun_control)</code>","text":"<p>Get the transformations of the values from the dictionary fun_control as a list.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with levels and types</p> required <p>Returns:</p> Type Description <code>list</code> <p>list with transformations</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_transform\n    d = {\"core_model_hyper_dict\":{\n    \"leaf_prediction\": {\n        \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        \"type\": \"factor\",\n        \"default\": \"mean\",\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"str\"},\n    \"leaf_model\": {\n        \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n        \"type\": \"factor\",\n        \"default\": \"LinearRegression\",\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"instance\"},\n    \"splitter\": {\n        \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n        \"type\": \"factor\",\n        \"default\": \"EBSTSplitter\",\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"instance()\"},\n    \"binary_split\": {\n        \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"bool\"},\n    \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"transform\": \"None\",\n        \"core_model_parameter_type\": \"bool\"}}}\n</code></pre> <pre><code>get_transform(d)\n['None', 'None', 'None', 'None', 'None']\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_transform(fun_control) -&gt; list:\n    \"\"\"Get the transformations of the values from the dictionary fun_control as a list.\n\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n\n    Returns:\n        (list):\n            list with transformations\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_transform\n            d = {\"core_model_hyper_dict\":{\n            \"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"str\"},\n            \"leaf_model\": {\n                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                \"type\": \"factor\",\n                \"default\": \"LinearRegression\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"instance\"},\n            \"splitter\": {\n                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                \"type\": \"factor\",\n                \"default\": \"EBSTSplitter\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"instance()\"},\n            \"binary_split\": {\n                \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"bool\"},\n            \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"bool\"}}}\n\n            get_transform(d)\n            ['None', 'None', 'None', 'None', 'None']\n    \"\"\"\n    return list(fun_control[\"core_model_hyper_dict\"][key][\"transform\"] for key in fun_control[\"core_model_hyper_dict\"].keys())\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_tuned_architecture","title":"<code>get_tuned_architecture(spot_tuner, force_minX=False)</code>","text":"<p>Returns the tuned architecture. If the spot tuner has noise, it returns the architecture with the lowest mean (.min_mean_X), otherwise it returns the architecture with the lowest value (.min_X).</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>spot tuner object.</p> required <code>force_minX</code> <code>bool</code> <p>If True, return the architecture with the lowest value (.min_X).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the tuned architecture.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.fun.hyperlight import HyperLight\n    from spotpython.utils.init import fun_control_init, design_control_init\n    from spotpython.spot import Spot\n    import numpy as np\n    from spotpython.hyperparameters.values import set_hyperparameter, get_tuned_architecture\n</code></pre> <pre><code>fun_control = fun_control_init(\n    force_run=False,\n    PREFIX=\"get_one_config_from_X\",\n    save_experiment=True,\n    fun_evals=10,\n    max_time=1,\n    data_set = Diabetes(),\n    core_model_name=\"light.regression.NNLinearRegressor\",\n    hyperdict=LightHyperDict,\n    _L_in=10,\n    _L_out=1)\n\nset_hyperparameter(fun_control, \"epochs\", [2,3])\nset_hyperparameter(fun_control, \"patience\", [1,2])\ndesign_control = design_control_init(init_size=5)\n\nfun = HyperLight().fun\nS = Spot(fun=fun,fun_control=fun_control, design_control=design_control)\nS.run()\nget_tuned_architecture(S)\n    {'l1': 16,\n    'epochs': 4,\n    'batch_size': 2,\n    'act_fn': LeakyReLU(),\n    'optimizer': 'SGD',\n    'dropout_prob': 0.034805674424520705,\n    'lr_mult': 7.981042243396318,\n    'patience': 2,\n    'batch_norm': True,\n    'initialization': 'Default'}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_tuned_architecture(spot_tuner, force_minX=False) -&gt; dict:\n    \"\"\"\n    Returns the tuned architecture. If the spot tuner has noise,\n    it returns the architecture with the lowest mean (.min_mean_X),\n    otherwise it returns the architecture with the lowest value (.min_X).\n\n    Args:\n        spot_tuner (object):\n            spot tuner object.\n        force_minX (bool):\n            If True, return the architecture with the lowest value (.min_X).\n\n    Returns:\n        (dict):\n            dictionary containing the tuned architecture.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.fun.hyperlight import HyperLight\n            from spotpython.utils.init import fun_control_init, design_control_init\n            from spotpython.spot import Spot\n            import numpy as np\n            from spotpython.hyperparameters.values import set_hyperparameter, get_tuned_architecture\n\n\n            fun_control = fun_control_init(\n                force_run=False,\n                PREFIX=\"get_one_config_from_X\",\n                save_experiment=True,\n                fun_evals=10,\n                max_time=1,\n                data_set = Diabetes(),\n                core_model_name=\"light.regression.NNLinearRegressor\",\n                hyperdict=LightHyperDict,\n                _L_in=10,\n                _L_out=1)\n\n            set_hyperparameter(fun_control, \"epochs\", [2,3])\n            set_hyperparameter(fun_control, \"patience\", [1,2])\n            design_control = design_control_init(init_size=5)\n\n            fun = HyperLight().fun\n            S = Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n            S.run()\n            get_tuned_architecture(S)\n                {'l1': 16,\n                'epochs': 4,\n                'batch_size': 2,\n                'act_fn': LeakyReLU(),\n                'optimizer': 'SGD',\n                'dropout_prob': 0.034805674424520705,\n                'lr_mult': 7.981042243396318,\n                'patience': 2,\n                'batch_norm': True,\n                'initialization': 'Default'}\n    \"\"\"\n    if not spot_tuner.noise or force_minX:\n        X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1, -1))\n    else:\n        # noise or force_minX is False:\n        X = spot_tuner.to_all_dim(spot_tuner.min_mean_X.reshape(1, -1))\n    fun_control = copy.copy(spot_tuner.fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    return config\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_tuned_hyperparameters","title":"<code>get_tuned_hyperparameters(spot_tuner, fun_control=None)</code>","text":"<p>Get the tuned hyperparameters from the spot tuner. This is just a wrapper function for the spot <code>get_tuned_hyperparameters</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning. Optional. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the tuned hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.device import getDevice\n    from math import inf\n    from spotpython.utils.init import fun_control_init\n    import numpy as np\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperparameters.values import get_tuned_hyperparameters\n    MAX_TIME = 1\n    FUN_EVALS = 10\n    INIT_SIZE = 5\n    WORKERS = 0\n    PREFIX=\"037\"\n    DEVICE = getDevice()\n    DEVICES = 1\n    TEST_SIZE = 0.4\n    TORCH_METRIC = \"mean_squared_error\"\n    dataset = Diabetes()\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        _torchmetric=TORCH_METRIC,\n        PREFIX=PREFIX,\n        TENSORBOARD_CLEAN=True,\n        data_set=dataset,\n        device=DEVICE,\n        enable_progress_bar=False,\n        fun_evals=FUN_EVALS,\n        log_level=50,\n        max_time=MAX_TIME,\n        num_workers=WORKERS,\n        show_progress=True,\n        test_size=TEST_SIZE,\n        tolerance_x=np.sqrt(np.spacing(1)),\n        )\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                hyper_dict=LightHyperDict)\n    from spotpython.hyperparameters.values import set_control_hyperparameter_value\n    set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n    set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n    set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n    set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                    \"Adam\",\n                    \"RAdam\",\n                ])\n    set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n    set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n    set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n    set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                    \"ReLU\",\n                    \"LeakyReLU\"\n                ] )\n    from spotpython.utils.init import design_control_init, surrogate_control_init\n    design_control = design_control_init(init_size=INIT_SIZE)\n    surrogate_control = surrogate_control_init(noise=True,\n                                                n_theta=2)\n    from spotpython.fun.hyperlight import HyperLight\n    fun = HyperLight(log_level=50).fun\n    from spotpython.spot import spot\n    spot_tuner = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control)\n    spot_tuner.run()\n    get_tuned_hyperparameters(spot_tuner)\n        {'l1': 7.0,\n        'epochs': 5.0,\n        'batch_size': 4.0,\n        'act_fn': 0.0,\n        'optimizer': 0.0,\n        'dropout_prob': 0.01,\n        'lr_mult': 5.0,\n        'patience': 3.0,\n        'initialization': 1.0}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_tuned_hyperparameters(spot_tuner, fun_control=None) -&gt; dict:\n    \"\"\"\n    Get the tuned hyperparameters from the spot tuner.\n    This is just a wrapper function for the spot `get_tuned_hyperparameters` method.\n\n    Args:\n        spot_tuner (object):\n            spot tuner object.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n            Optional. Default is None.\n\n    Returns:\n        (dict):\n            dictionary containing the tuned hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.device import getDevice\n            from math import inf\n            from spotpython.utils.init import fun_control_init\n            import numpy as np\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperparameters.values import get_tuned_hyperparameters\n            MAX_TIME = 1\n            FUN_EVALS = 10\n            INIT_SIZE = 5\n            WORKERS = 0\n            PREFIX=\"037\"\n            DEVICE = getDevice()\n            DEVICES = 1\n            TEST_SIZE = 0.4\n            TORCH_METRIC = \"mean_squared_error\"\n            dataset = Diabetes()\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                _torchmetric=TORCH_METRIC,\n                PREFIX=PREFIX,\n                TENSORBOARD_CLEAN=True,\n                data_set=dataset,\n                device=DEVICE,\n                enable_progress_bar=False,\n                fun_evals=FUN_EVALS,\n                log_level=50,\n                max_time=MAX_TIME,\n                num_workers=WORKERS,\n                show_progress=True,\n                test_size=TEST_SIZE,\n                tolerance_x=np.sqrt(np.spacing(1)),\n                )\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        hyper_dict=LightHyperDict)\n            from spotpython.hyperparameters.values import set_control_hyperparameter_value\n            set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n            set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n            set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n            set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                            \"Adam\",\n                            \"RAdam\",\n                        ])\n            set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n            set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n            set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n            set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                            \"ReLU\",\n                            \"LeakyReLU\"\n                        ] )\n            from spotpython.utils.init import design_control_init, surrogate_control_init\n            design_control = design_control_init(init_size=INIT_SIZE)\n            surrogate_control = surrogate_control_init(noise=True,\n                                                        n_theta=2)\n            from spotpython.fun.hyperlight import HyperLight\n            fun = HyperLight(log_level=50).fun\n            from spotpython.spot import spot\n            spot_tuner = spot.Spot(fun=fun,\n                                fun_control=fun_control,\n                                design_control=design_control,\n                                surrogate_control=surrogate_control)\n            spot_tuner.run()\n            get_tuned_hyperparameters(spot_tuner)\n                {'l1': 7.0,\n                'epochs': 5.0,\n                'batch_size': 4.0,\n                'act_fn': 0.0,\n                'optimizer': 0.0,\n                'dropout_prob': 0.01,\n                'lr_mult': 5.0,\n                'patience': 3.0,\n                'initialization': 1.0}\n    \"\"\"\n    return spot_tuner.get_tuned_hyperparameters(fun_control=fun_control)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_values_from_dict","title":"<code>get_values_from_dict(dictionary)</code>","text":"<p>Get the values from a dictionary as an array. Generate an np.array that contains the values of the keys of a dictionary in the same order as the keys of the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>dictionary with values</p> required <p>Returns:</p> Type Description <code>array</code> <p>array with values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_values_from_dict\n&gt;&gt;&gt; d = {\"a\": 1, \"b\": 2, \"c\": 3}\n&gt;&gt;&gt; get_values_from_dict(d)\narray([1, 2, 3])\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_values_from_dict(dictionary) -&gt; np.array:\n    \"\"\"Get the values from a dictionary as an array.\n    Generate an np.array that contains the values of the keys of a dictionary\n    in the same order as the keys of the dictionary.\n\n    Args:\n        dictionary (dict):\n            dictionary with values\n\n    Returns:\n        (np.array):\n            array with values\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_values_from_dict\n        &gt;&gt;&gt; d = {\"a\": 1, \"b\": 2, \"c\": 3}\n        &gt;&gt;&gt; get_values_from_dict(d)\n        array([1, 2, 3])\n    \"\"\"\n    return np.array(list(dictionary.values()))\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_var_name","title":"<code>get_var_name(fun_control)</code>","text":"<p>Get the names of the values from the dictionary fun_control as a list. If no \u201ccore_model_hyper_dict\u201d key exists in fun_control, return None.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with names</p> required <p>Returns:</p> Type Description <code>list</code> <p>ist with names</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_name\n    fun_control = {\"core_model_hyper_dict\":{\n                \"leaf_prediction\": {\n                    \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                    \"type\": \"factor\",\n                    \"default\": \"mean\",\n                    \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\",\n                                \"linear_model.PARegressor\",\n                                \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}}\n    get_var_name(fun_control)\n    ['leaf_prediction',\n        'leaf_model',\n        'splitter',\n        'binary_split',\n        'stop_mem_management']\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_var_name(fun_control) -&gt; list:\n    \"\"\"Get the names of the values from the dictionary fun_control as a list.\n    If no \"core_model_hyper_dict\" key exists in fun_control, return None.\n\n    Args:\n        fun_control (dict):\n            dictionary with names\n\n    Returns:\n        (list):\n            ist with names\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_name\n            fun_control = {\"core_model_hyper_dict\":{\n                        \"leaf_prediction\": {\n                            \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                            \"type\": \"factor\",\n                            \"default\": \"mean\",\n                            \"core_model_parameter_type\": \"str\"},\n                        \"leaf_model\": {\n                            \"levels\": [\"linear_model.LinearRegression\",\n                                        \"linear_model.PARegressor\",\n                                        \"linear_model.Perceptron\"],\n                            \"type\": \"factor\",\n                            \"default\": \"LinearRegression\",\n                            \"core_model_parameter_type\": \"instance\"},\n                        \"splitter\": {\n                            \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                            \"type\": \"factor\",\n                            \"default\": \"EBSTSplitter\",\n                            \"core_model_parameter_type\": \"instance()\"},\n                        \"binary_split\": {\n                            \"levels\": [0, 1],\n                            \"type\": \"factor\",\n                            \"default\": 0,\n                            \"core_model_parameter_type\": \"bool\"},\n                        \"stop_mem_management\": {\n                            \"levels\": [0, 1],\n                            \"type\": \"factor\",\n                            \"default\": 0,\n                            \"core_model_parameter_type\": \"bool\"}}}\n            get_var_name(fun_control)\n            ['leaf_prediction',\n                'leaf_model',\n                'splitter',\n                'binary_split',\n                'stop_mem_management']\n    \"\"\"\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        return list(fun_control[\"core_model_hyper_dict\"].keys())\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_var_type","title":"<code>get_var_type(fun_control)</code>","text":"<p>Get the types of the values from the dictionary fun_control as a list. If no \u201ccore_model_hyper_dict\u201d key exists in fun_control, return None.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>dictionary with levels and types</p> required <p>Returns:</p> Type Description <code>list</code> <p>list with types</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_type\n    d = {\"core_model_hyper_dict\":{\n    \"leaf_prediction\": {\n        \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        \"type\": \"factor\",\n        \"default\": \"mean\",\n        \"core_model_parameter_type\": \"str\"},\n    \"leaf_model\": {\n        \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n        \"type\": \"factor\",\n        \"default\": \"LinearRegression\",\n        \"core_model_parameter_type\": \"instance\"},\n    \"splitter\": {\n        \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n        \"type\": \"factor\",\n        \"default\": \"EBSTSplitter\",\n        \"core_model_parameter_type\": \"instance()\"},\n    \"binary_split\": {\n        \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"core_model_parameter_type\": \"bool\"},\n    \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n        \"type\": \"factor\",\n        \"default\": 0,\n        \"core_model_parameter_type\": \"bool\"}}}\n</code></pre> <pre><code>get_var_type(d)\n['factor', 'factor', 'factor', 'factor', 'factor']\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_var_type(fun_control) -&gt; list:\n    \"\"\"\n    Get the types of the values from the dictionary fun_control as a list.\n    If no \"core_model_hyper_dict\" key exists in fun_control, return None.\n\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n\n    Returns:\n        (list):\n            list with types\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_var_type\n            d = {\"core_model_hyper_dict\":{\n            \"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"core_model_parameter_type\": \"str\"},\n            \"leaf_model\": {\n                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                \"type\": \"factor\",\n                \"default\": \"LinearRegression\",\n                \"core_model_parameter_type\": \"instance\"},\n            \"splitter\": {\n                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                \"type\": \"factor\",\n                \"default\": \"EBSTSplitter\",\n                \"core_model_parameter_type\": \"instance()\"},\n            \"binary_split\": {\n                \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"core_model_parameter_type\": \"bool\"},\n            \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"core_model_parameter_type\": \"bool\"}}}\n\n            get_var_type(d)\n            ['factor', 'factor', 'factor', 'factor', 'factor']\n    \"\"\"\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        return list(fun_control[\"core_model_hyper_dict\"][key][\"type\"] for key in fun_control[\"core_model_hyper_dict\"].keys())\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.get_var_type_from_var_name","title":"<code>get_var_type_from_var_name(fun_control, var_name)</code>","text":"<p>This function gets the variable type from the variable name.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>var_name</code> <code>str</code> <p>variable name</p> required <p>Returns:</p> Type Description <code>str</code> <p>variable type</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import get_var_type_from_var_name\n    control_dict = fun_control_init()\n    get_var_type_from_var_name(var_name=\"max_depth\",\n                    fun_control=control_dict)\n    \"int\"\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def get_var_type_from_var_name(fun_control, var_name) -&gt; str:\n    \"\"\"\n    This function gets the variable type from the variable name.\n\n    Args:\n        fun_control (dict): fun_control dictionary\n        var_name (str): variable name\n\n    Returns:\n        (str): variable type\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import get_var_type_from_var_name\n            control_dict = fun_control_init()\n            get_var_type_from_var_name(var_name=\"max_depth\",\n                            fun_control=control_dict)\n            \"int\"\n    \"\"\"\n    var_type_list = get_control_key_value(control_dict=fun_control, key=\"var_type\")\n    var_name_list = get_control_key_value(control_dict=fun_control, key=\"var_name\")\n    return var_type_list[var_name_list.index(var_name)]\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.iterate_dict_values","title":"<code>iterate_dict_values(var_dict)</code>","text":"<p>Iterate over the values of a dictionary of variables. This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <p>Returns:</p> Type Description <code>Generator[Dict[str, Union[int, float]], None, None]</code> <p>Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the arrays in the dictionary do not have the same length.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.hyperparameters.values import iterate_dict_values\n&gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n&gt;&gt;&gt; print(var_dict)\n        {'a': array([1, 3, 5]), 'b': array([2, 4, 6])}\n&gt;&gt;&gt; list(iterate_dict_values(var_dict))\n        [{'a': np.int64(1), 'b': np.int64(2)},\n        {'a': np.int64(3), 'b': np.int64(4)},\n        {'a': np.int64(5), 'b': np.int64(6)}]\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def iterate_dict_values(var_dict: Dict[str, np.ndarray]) -&gt; Generator[Dict[str, Union[int, float]], None, None]:\n    \"\"\"Iterate over the values of a dictionary of variables.\n    This function takes a dictionary of variables as input arguments and returns a generator that\n    yields dictionaries with the values from the arrays in the input dictionary.\n\n    Args:\n        var_dict (dict): A dictionary where keys are variable names and values are numpy arrays.\n\n    Returns:\n        Generator[dict]:\n            A generator that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Raises:\n        ValueError: If the arrays in the dictionary do not have the same length.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import iterate_dict_values\n        &gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n        &gt;&gt;&gt; print(var_dict)\n                {'a': array([1, 3, 5]), 'b': array([2, 4, 6])}\n        &gt;&gt;&gt; list(iterate_dict_values(var_dict))\n                [{'a': np.int64(1), 'b': np.int64(2)},\n                {'a': np.int64(3), 'b': np.int64(4)},\n                {'a': np.int64(5), 'b': np.int64(6)}]\n    \"\"\"\n    # Check if the dictionary is empty\n    if not var_dict:\n        return\n\n    # Get the length of the first array\n    first_length = None\n    for value in var_dict.values():\n        if first_length is None:\n            first_length = len(value)\n        elif len(value) != first_length:\n            raise ValueError(\"All arrays must have the same length.\")\n\n    # Generate the output dictionaries\n    for i in range(first_length):\n        yield {key: value[i] for key, value in var_dict.items()}\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.modify_boolean_hyper_parameter_levels","title":"<code>modify_boolean_hyper_parameter_levels(fun_control, hyperparameter, levels)</code>","text":"<p>This function modifies the levels of a boolean hyperparameter in the fun_control dictionary. It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>levels</code> <code>list</code> <p>list of levels</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def modify_boolean_hyper_parameter_levels(fun_control, hyperparameter, levels) -&gt; None:\n    \"\"\"\n    This function modifies the levels of a boolean hyperparameter in the fun_control dictionary.\n    It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        levels (list):\n            list of levels\n\n    Returns:\n        None.\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"levels\": levels})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": levels[0]})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": levels[1]})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.modify_hyper_parameter_bounds","title":"<code>modify_hyper_parameter_bounds(fun_control, hyperparameter, bounds)</code>","text":"<p>Modify the bounds of a hyperparameter in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>bounds</code> <code>list</code> <p>list of two bound values. The first value represents the lower bound and the second value represents the upper bound.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n    fun_control = {}\n    core_model  = HoeffdingTreeRegressor\n    fun_control.update({\"core_model\": core_model})\n    fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n    bounds = [3, 11]\n    fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds)\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def modify_hyper_parameter_bounds(fun_control, hyperparameter, bounds) -&gt; None:\n    \"\"\"\n    Modify the bounds of a hyperparameter in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        bounds (list):\n            list of two bound values. The first value represents the lower bound\n            and the second value represents the upper bound.\n\n    Returns:\n        None.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n            fun_control = {}\n            core_model  = HoeffdingTreeRegressor\n            fun_control.update({\"core_model\": core_model})\n            fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n            bounds = [3, 11]\n            fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds)\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": bounds[0]})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": bounds[1]})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.modify_hyper_parameter_levels","title":"<code>modify_hyper_parameter_levels(fun_control, hyperparameter, levels)</code>","text":"<p>This function modifies the levels of a hyperparameter in the fun_control dictionary. It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>levels</code> <code>list</code> <p>list of levels</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fun_control = {}\n    from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n    core_model  = HoeffdingTreeRegressor\n    fun_control.update({\"core_model\": core_model})\n    fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n    levels = [\"mean\", \"model\"]\n    fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels)\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def modify_hyper_parameter_levels(fun_control, hyperparameter, levels) -&gt; None:\n    \"\"\"\n    This function modifies the levels of a hyperparameter in the fun_control dictionary.\n    It also sets the lower and upper bounds of the hyperparameter to 0 and len(levels) - 1, respectively.\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        levels (list):\n            list of levels\n\n    Returns:\n        None.\n\n    Examples:\n        &gt;&gt;&gt; fun_control = {}\n            from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n            core_model  = HoeffdingTreeRegressor\n            fun_control.update({\"core_model\": core_model})\n            fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n            levels = [\"mean\", \"model\"]\n            fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels)\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"levels\": levels})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": 0})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": len(levels) - 1})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.replace_levels_with_positions","title":"<code>replace_levels_with_positions(hyper_dict, hyper_dict_values)</code>","text":"<p>Replace the levels with the position in the levels list. The function that takes two dictionaries. The first contains as hyperparameters as keys. If the hyperparameter has the key \u201clevels\u201d, then the value of the corresponding hyperparameter in the second dictionary is replaced by the position of the value in the list of levels. The function returns a dictionary with the same keys as the second dictionary. For example, if the second dictionary is {\u201ca\u201d: 1, \u201cb\u201d: \u201cmodel1\u201d, \u201cc\u201d: 3} and the first dictionary is {     \u201ca\u201d: {\u201ctype\u201d: \u201cint\u201d},     \u201cb\u201d: {\u201clevels\u201d: [\u201cmodel4\u201d, \u201cmodel5\u201d, \u201cmodel1\u201d]},     \u201cd\u201d: {\u201ctype\u201d: \u201cfloat\u201d}}, then the function should return {\u201ca\u201d: 1, \u201cb\u201d: 2, \u201cc\u201d: 3}.</p> <p>Parameters:</p> Name Type Description Default <code>hyper_dict</code> <code>dict</code> <p>dictionary with levels</p> required <code>hyper_dict_values</code> <code>dict</code> <p>dictionary with values</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dictionary with values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import replace_levels_with_positions\n    hyper_dict = {\"leaf_prediction\": {\n        \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        \"type\": \"factor\",\n        \"default\": \"mean\",\n        \"core_model_parameter_type\": \"str\"},\n        \"leaf_model\": {\n            \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n            \"type\": \"factor\",\n            \"default\": \"LinearRegression\",\n            \"core_model_parameter_type\": \"instance\"},\n        \"splitter\": {\n            \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n            \"type\": \"factor\",\n            \"default\": \"EBSTSplitter\",\n            \"core_model_parameter_type\": \"instance()\"},\n        \"binary_split\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"},\n        \"stop_mem_management\": {\n            \"levels\": [0, 1],\n            \"type\": \"factor\",\n            \"default\": 0,\n            \"core_model_parameter_type\": \"bool\"}}\n    hyper_dict_values = {\"leaf_prediction\": \"mean\",\n        \"leaf_model\": \"linear_model.LinearRegression\",\n        \"splitter\": \"EBSTSplitter\",\n        \"binary_split\": 0,\n        \"stop_mem_management\": 0}\n    replace_levels_with_position(hyper_dict, hyper_dict_values)\n        {'leaf_prediction': 0,\n        'leaf_model': 0,\n        'splitter': 0,\n        'binary_split': 0,\n        'stop_mem_management': 0}\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def replace_levels_with_positions(hyper_dict, hyper_dict_values) -&gt; dict:\n    \"\"\"Replace the levels with the position in the levels list.\n    The function that takes two dictionaries.\n    The first contains as hyperparameters as keys.\n    If the hyperparameter has the key \"levels\",\n    then the value of the corresponding hyperparameter in the second dictionary is\n    replaced by the position of the value in the list of levels.\n    The function returns a dictionary with the same keys as the second dictionary.\n    For example, if the second dictionary is {\"a\": 1, \"b\": \"model1\", \"c\": 3}\n    and the first dictionary is {\n        \"a\": {\"type\": \"int\"},\n        \"b\": {\"levels\": [\"model4\", \"model5\", \"model1\"]},\n        \"d\": {\"type\": \"float\"}},\n    then the function should return {\"a\": 1, \"b\": 2, \"c\": 3}.\n\n    Args:\n        hyper_dict (dict):\n            dictionary with levels\n        hyper_dict_values (dict):\n            dictionary with values\n\n    Returns:\n        (dict):\n            dictionary with values\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import replace_levels_with_positions\n            hyper_dict = {\"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}\n            hyper_dict_values = {\"leaf_prediction\": \"mean\",\n                \"leaf_model\": \"linear_model.LinearRegression\",\n                \"splitter\": \"EBSTSplitter\",\n                \"binary_split\": 0,\n                \"stop_mem_management\": 0}\n            replace_levels_with_position(hyper_dict, hyper_dict_values)\n                {'leaf_prediction': 0,\n                'leaf_model': 0,\n                'splitter': 0,\n                'binary_split': 0,\n                'stop_mem_management': 0}\n    \"\"\"\n    hyper_dict_values_new = copy.deepcopy(hyper_dict_values)\n    # generate an error if the following code fails and write an error message:\n    try:\n        for key, value in hyper_dict_values.items():\n            if key in hyper_dict.keys():\n                if \"levels\" in hyper_dict[key].keys():\n                    hyper_dict_values_new[key] = hyper_dict[key][\"levels\"].index(value)\n    except Exception as e:\n        print(\"!!! Warning: \", e)\n        print(\"Did you modify lower and upper bounds so that the default values are not included?\")\n        print(\"Returning 'None'.\")\n        return None\n    return hyper_dict_values_new\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.return_conf_list_from_var_dict","title":"<code>return_conf_list_from_var_dict(var_dict, fun_control, default=False)</code>","text":"<p>Return a list of configurations from a dictionary of variables.</p> <p>This function takes a dictionary of variables and a dictionary of function control as input arguments. It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries of hyper parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d or not \u201cfloat\u201d the corresponding value will be converted to the type \u201cint\u201d.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, Union[int, float]]]</code> <p>A list of dictionaries of hyper parameter values. Transformations are applied to the values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import  get_core_model_from_name, add_core_model_to_fun_control, generate_one_config_from_var_dict\n    import pprint\n    core_model_name=\"light.regression.NNLinearRegressor\"\n    hyperdict=LightHyperDict\n    fun_control = {}\n    _ , core_model_instance = get_core_model_from_name(core_model_name)\n    add_core_model_to_fun_control(\n        core_model=core_model_instance,\n        fun_control=fun_control,\n        hyper_dict=hyperdict,\n        filename=None,\n    )\n    var_dict = {'l1': np.array([3., 4.]),\n                'epochs': np.array([4., 3.]),\n                'batch_size': np.array([4., 4.]),\n                'act_fn': np.array([2., 1.]),\n                'optimizer': np.array([11., 10.]),\n                'dropout_prob': np.array([0.01, 0.]),\n                'lr_mult': np.array([1., 1.1]),\n                'patience': np.array([2., 3.]),\n                'batch_norm': np.array([0., 1.]),\n                'initialization': np.array([0., 1.])}\n    g = generate_one_config_from_var_dict(var_dict=var_dict, fun_control=fun_control)\n    conf_list = []\n    for values in generate_one_config_from_var_dict(var_dict, fun_control):\n        conf_list.append(values)\n    conf_list\n        [{'l1': 8,\n        'epochs': 16,\n        'batch_size': 16,\n        'act_fn': ReLU(),\n        'optimizer': 'SGD',\n        'dropout_prob': 0.01,\n        'lr_mult': 1.0,\n        'patience': 4,\n        'batch_norm': False,\n        'initialization': 'Default'},\n        {'l1': 16,\n        'epochs': 8,\n        'batch_size': 16,\n        'act_fn': Tanh(),\n        'optimizer': 'Rprop',\n        'dropout_prob': 0.0,\n        'lr_mult': 1.1,\n        'patience': 8,\n        'batch_norm': True,\n        'initialization': 'kaiming_uniform'}]\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def return_conf_list_from_var_dict(\n    var_dict: Dict[str, np.ndarray],\n    fun_control: Dict[str, Union[List[str], str]],\n    default: bool = False,\n) -&gt; List[Dict[str, Union[int, float]]]:\n    \"\"\"Return a list of configurations from a dictionary of variables.\n\n    This function takes a dictionary of variables and a dictionary of function control as input arguments.\n    It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries\n    of hyper parameter values.\n\n    Args:\n        var_dict (dict): A dictionary where keys are variable names and values are numpy arrays.\n        fun_control (dict): A dictionary which (at least) has an entry with the following key:\n            \"var_type\" (list): A list of variable types. If the entry is not \"num\" or not \"float\" the corresponding\n            value will be converted to the type \"int\".\n\n    Returns:\n        list: A list of dictionaries of hyper parameter values. Transformations are applied to the values.\n\n    Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.hyperparameters.values import  get_core_model_from_name, add_core_model_to_fun_control, generate_one_config_from_var_dict\n                import pprint\n                core_model_name=\"light.regression.NNLinearRegressor\"\n                hyperdict=LightHyperDict\n                fun_control = {}\n                _ , core_model_instance = get_core_model_from_name(core_model_name)\n                add_core_model_to_fun_control(\n                    core_model=core_model_instance,\n                    fun_control=fun_control,\n                    hyper_dict=hyperdict,\n                    filename=None,\n                )\n                var_dict = {'l1': np.array([3., 4.]),\n                            'epochs': np.array([4., 3.]),\n                            'batch_size': np.array([4., 4.]),\n                            'act_fn': np.array([2., 1.]),\n                            'optimizer': np.array([11., 10.]),\n                            'dropout_prob': np.array([0.01, 0.]),\n                            'lr_mult': np.array([1., 1.1]),\n                            'patience': np.array([2., 3.]),\n                            'batch_norm': np.array([0., 1.]),\n                            'initialization': np.array([0., 1.])}\n                g = generate_one_config_from_var_dict(var_dict=var_dict, fun_control=fun_control)\n                conf_list = []\n                for values in generate_one_config_from_var_dict(var_dict, fun_control):\n                    conf_list.append(values)\n                conf_list\n                    [{'l1': 8,\n                    'epochs': 16,\n                    'batch_size': 16,\n                    'act_fn': ReLU(),\n                    'optimizer': 'SGD',\n                    'dropout_prob': 0.01,\n                    'lr_mult': 1.0,\n                    'patience': 4,\n                    'batch_norm': False,\n                    'initialization': 'Default'},\n                    {'l1': 16,\n                    'epochs': 8,\n                    'batch_size': 16,\n                    'act_fn': Tanh(),\n                    'optimizer': 'Rprop',\n                    'dropout_prob': 0.0,\n                    'lr_mult': 1.1,\n                    'patience': 8,\n                    'batch_norm': True,\n                    'initialization': 'kaiming_uniform'}]\n    \"\"\"\n    conf_list = []\n    for values in generate_one_config_from_var_dict(var_dict, fun_control, default=default):\n        conf_list.append(values)\n    return conf_list\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_boolean_hyperparameter_values","title":"<code>set_boolean_hyperparameter_values(fun_control, key, lower, upper)</code>","text":"<p>Set the boolean hyperparameter values in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>key</code> <code>str</code> <p>The key of the hyperparameter.</p> required <code>lower</code> <code>bool</code> <p>The lower bound of the hyperparameter.</p> required <code>upper</code> <code>bool</code> <p>The upper bound of the hyperparameter.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_boolean_hyperparameter_values\n    from spotpython.utils.eda import print_exp_table\n    fun_control = fun_control_init(\n        core_model_name=\"forest.AMFRegressor\",\n        hyperdict=RiverHyperDict,\n    )\n    print(\"Before modification:\")\n    print_exp_table(fun_control)\n    set_boolean_hyperparameter_values(fun_control, \"use_aggregation\", 0, 0)\n    print(\"After modification:\")\n    print_exp_table(fun_control)\n    Seed set to 123\n    Before modification:\n    | name            | type   |   default |   lower |   upper | transform   |\n    |-----------------|--------|-----------|---------|---------|-------------|\n    | n_estimators    | int    |        10 |     2   |    1000 | None        |\n    | step            | float  |         1 |     0.1 |      10 | None        |\n    | use_aggregation | factor |         1 |     0   |       1 | None        |\n    Setting hyperparameter use_aggregation to value [0, 0].\n    Variable type is factor.\n    Core type is bool.\n    Calling modify_boolean_hyper_parameter_levels().\n    After modification:\n    | name            | type   |   default |   lower |   upper | transform   |\n    |-----------------|--------|-----------|---------|---------|-------------|\n    | n_estimators    | int    |        10 |     2   |    1000 | None        |\n    | step            | float  |         1 |     0.1 |      10 | None        |\n    | use_aggregation | factor |         1 |     0   |       0 | None        |\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_boolean_hyperparameter_values(fun_control, key, lower, upper):\n    \"\"\"\n    Set the boolean hyperparameter values in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        key (str):\n            The key of the hyperparameter.\n        lower (bool):\n            The lower bound of the hyperparameter.\n        upper (bool):\n            The upper bound of the hyperparameter.\n\n    Examples:\n        &gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_boolean_hyperparameter_values\n            from spotpython.utils.eda import print_exp_table\n            fun_control = fun_control_init(\n                core_model_name=\"forest.AMFRegressor\",\n                hyperdict=RiverHyperDict,\n            )\n            print(\"Before modification:\")\n            print_exp_table(fun_control)\n            set_boolean_hyperparameter_values(fun_control, \"use_aggregation\", 0, 0)\n            print(\"After modification:\")\n            print_exp_table(fun_control)\n            Seed set to 123\n            Before modification:\n            | name            | type   |   default |   lower |   upper | transform   |\n            |-----------------|--------|-----------|---------|---------|-------------|\n            | n_estimators    | int    |        10 |     2   |    1000 | None        |\n            | step            | float  |         1 |     0.1 |      10 | None        |\n            | use_aggregation | factor |         1 |     0   |       1 | None        |\n            Setting hyperparameter use_aggregation to value [0, 0].\n            Variable type is factor.\n            Core type is bool.\n            Calling modify_boolean_hyper_parameter_levels().\n            After modification:\n            | name            | type   |   default |   lower |   upper | transform   |\n            |-----------------|--------|-----------|---------|---------|-------------|\n            | n_estimators    | int    |        10 |     2   |    1000 | None        |\n            | step            | float  |         1 |     0.1 |      10 | None        |\n            | use_aggregation | factor |         1 |     0   |       0 | None        |\n    \"\"\"\n    set_control_hyperparameter_value(\n        fun_control,\n        key,\n        [\n            lower,\n            upper,\n        ],\n    )\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_control_hyperparameter_value","title":"<code>set_control_hyperparameter_value(control_dict, hyperparameter, value)</code>","text":"<p>This function sets the hyperparameter values depending on the var_type via modify_hyperameter_levels or modify_hyperparameter_bounds in the control_dict dictionary. If the hyperparameter is a factor, it calls modify_hyper_parameter_levels. Otherwise, it calls modify_hyper_parameter_bounds.</p> <p>Parameters:</p> Name Type Description Default <code>control_dict</code> <code>dict</code> <p>control_dict dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>key</p> required <code>value</code> <code>Any</code> <p>value</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_control_hyperparameter_value(control_dict, hyperparameter, value) -&gt; None:\n    \"\"\"\n    This function sets the hyperparameter values depending on the var_type\n    via modify_hyperameter_levels or modify_hyperparameter_bounds in the control_dict dictionary.\n    If the hyperparameter is a factor, it calls modify_hyper_parameter_levels.\n    Otherwise, it calls modify_hyper_parameter_bounds.\n\n    Args:\n        control_dict (dict):\n            control_dict dictionary\n        hyperparameter (str): key\n        value (Any): value\n\n    Returns:\n        None.\n\n    \"\"\"\n    print(f\"Setting hyperparameter {hyperparameter} to value {value}.\")\n    vt = get_var_type_from_var_name(fun_control=control_dict, var_name=hyperparameter)\n    print(f\"Variable type is {vt}.\")\n    core_type = get_core_model_parameter_type_from_var_name(fun_control=control_dict, var_name=hyperparameter)\n    print(f\"Core type is {core_type}.\")\n    if vt == \"factor\" and core_type != \"bool\":\n        print(\"Calling modify_hyper_parameter_levels().\")\n        modify_hyper_parameter_levels(fun_control=control_dict, hyperparameter=hyperparameter, levels=value)\n    elif vt == \"factor\" and core_type == \"bool\":\n        print(\"Calling modify_boolean_hyper_parameter_levels().\")\n        modify_boolean_hyper_parameter_levels(fun_control=control_dict, hyperparameter=hyperparameter, levels=value)\n    else:\n        print(\"Calling modify_hyper_parameter_bounds().\")\n        modify_hyper_parameter_bounds(fun_control=control_dict, hyperparameter=hyperparameter, bounds=value)\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_control_key_value","title":"<code>set_control_key_value(control_dict, key, value, replace=False)</code>","text":"<p>This function sets the key value pair in the control_dict dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>control_dict</code> <code>dict</code> <p>control_dict dictionary</p> required <code>key</code> <code>str</code> <p>key</p> required <code>value</code> <code>Any</code> <p>value</p> required <code>replace</code> <code>bool</code> <p>replace value if key already exists. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>key</p> <code>value</code> <code>Any</code> <p>value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_control_key_value\n    control_dict = fun_control_init()\n    set_control_key_value(control_dict=control_dict,\n                  key=\"key\",\n                  value=\"value\")\n    control_dict[\"key\"]\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_control_key_value(control_dict, key, value, replace=False) -&gt; None:\n    \"\"\"\n    This function sets the key value pair in the control_dict dictionary.\n\n    Args:\n        control_dict (dict):\n            control_dict dictionary\n        key (str): key\n        value (Any): value\n        replace (bool): replace value if key already exists. Default is False.\n\n    Returns:\n        None.\n\n    Attributes:\n        key (str): key\n        value (Any): value\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_control_key_value\n            control_dict = fun_control_init()\n            set_control_key_value(control_dict=control_dict,\n                          key=\"key\",\n                          value=\"value\")\n            control_dict[\"key\"]\n\n    \"\"\"\n    if replace:\n        control_dict.update({key: value})\n    else:\n        if key not in control_dict.keys():\n            control_dict.update({key: value})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_factor_hyperparameter_values","title":"<code>set_factor_hyperparameter_values(fun_control, key, levels)</code>","text":"<p>Set the factor hyperparameter values in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>key</code> <code>str</code> <p>The key of the hyperparameter.</p> required <code>levels</code> <code>list</code> <p>The levels of the hyperparameter.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_factor_hyperparameter_values\n    from spotpython.utils.eda import print_exp_table\n    fun_control = fun_control_init(\n        core_model_name=\"tree.HoeffdingTreeRegressor\",\n        hyperdict=RiverHyperDict,\n    )\n    print(\"Before modification:\")\n    print_exp_tablefun_control)\n    set_factor_hyperparameter_values(fun_control, \"leaf_model\", ['LinearRegression',\n                                                        'Perceptron'])\n    print(\"After modification:\")\n    print_exp_table(fun_control))\n        Seed set to 123\n        Before modification:\n        | name                   | type   | default          |   lower |    upper | transform              |\n        |------------------------|--------|------------------|---------|----------|------------------------|\n        | grace_period           | int    | 200              |  10     | 1000     | None                   |\n        | max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n        | delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n        | tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n        | leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n        | leaf_model             | factor | LinearRegression |   0     |    2     | None                   |\n        | model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n        | splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n        | min_samples_split      | int    | 5                |   2     |   10     | None                   |\n        | binary_split           | factor | 0                |   0     |    1     | None                   |\n        | max_size               | float  | 500.0            | 100     | 1000     | None                   |\n        | memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n        | stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n        | remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n        | merit_preprune         | factor | 1                |   0     |    1     | None                   |\n        After modification:\n        | name                   | type   | default          |   lower |    upper | transform              |\n        |------------------------|--------|------------------|---------|----------|------------------------|\n        | grace_period           | int    | 200              |  10     | 1000     | None                   |\n        | max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n        | delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n        | tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n        | leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n        | leaf_model             | factor | LinearRegression |   0     |    1     | None                   |\n        | model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n        | splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n        | min_samples_split      | int    | 5                |   2     |   10     | None                   |\n        | binary_split           | factor | 0                |   0     |    1     | None                   |\n        | max_size               | float  | 500.0            | 100     | 1000     | None                   |\n        | memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n        | stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n        | remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n        | merit_preprune         | factor | 1                |   0     |    1     | None                   |\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_factor_hyperparameter_values(fun_control, key, levels):\n    \"\"\"\n    Set the factor hyperparameter values in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        key (str):\n            The key of the hyperparameter.\n        levels (list):\n            The levels of the hyperparameter.\n\n    Examples:\n        &gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_factor_hyperparameter_values\n            from spotpython.utils.eda import print_exp_table\n            fun_control = fun_control_init(\n                core_model_name=\"tree.HoeffdingTreeRegressor\",\n                hyperdict=RiverHyperDict,\n            )\n            print(\"Before modification:\")\n            print_exp_tablefun_control)\n            set_factor_hyperparameter_values(fun_control, \"leaf_model\", ['LinearRegression',\n                                                                'Perceptron'])\n            print(\"After modification:\")\n            print_exp_table(fun_control))\n                Seed set to 123\n                Before modification:\n                | name                   | type   | default          |   lower |    upper | transform              |\n                |------------------------|--------|------------------|---------|----------|------------------------|\n                | grace_period           | int    | 200              |  10     | 1000     | None                   |\n                | max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n                | delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n                | tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n                | leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n                | leaf_model             | factor | LinearRegression |   0     |    2     | None                   |\n                | model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n                | splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n                | min_samples_split      | int    | 5                |   2     |   10     | None                   |\n                | binary_split           | factor | 0                |   0     |    1     | None                   |\n                | max_size               | float  | 500.0            | 100     | 1000     | None                   |\n                | memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n                | stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n                | remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n                | merit_preprune         | factor | 1                |   0     |    1     | None                   |\n                After modification:\n                | name                   | type   | default          |   lower |    upper | transform              |\n                |------------------------|--------|------------------|---------|----------|------------------------|\n                | grace_period           | int    | 200              |  10     | 1000     | None                   |\n                | max_depth              | int    | 20               |   2     |   20     | transform_power_2_int  |\n                | delta                  | float  | 1e-07            |   1e-08 |    1e-06 | None                   |\n                | tau                    | float  | 0.05             |   0.01  |    0.1   | None                   |\n                | leaf_prediction        | factor | mean             |   0     |    2     | None                   |\n                | leaf_model             | factor | LinearRegression |   0     |    1     | None                   |\n                | model_selector_decay   | float  | 0.95             |   0.9   |    0.99  | None                   |\n                | splitter               | factor | EBSTSplitter     |   0     |    2     | None                   |\n                | min_samples_split      | int    | 5                |   2     |   10     | None                   |\n                | binary_split           | factor | 0                |   0     |    1     | None                   |\n                | max_size               | float  | 500.0            | 100     | 1000     | None                   |\n                | memory_estimate_period | int    | 6                |   3     |    8     | transform_power_10_int |\n                | stop_mem_management    | factor | 0                |   0     |    1     | None                   |\n                | remove_poor_attrs      | factor | 0                |   0     |    1     | None                   |\n                | merit_preprune         | factor | 1                |   0     |    1     | None                   |\n    \"\"\"\n    # check if levels is a list of strings. If not, convert it to a list\n    if not isinstance(levels, list):\n        levels = [levels]\n    # check if levels is a list of strings. Othewise, issue a warning and return None\n    if not all(isinstance(x, str) for x in levels):\n        print(\"!!! Warning: levels should be a list of strings.\")\n        return None\n    # check if key \"core_model_hyper_dict\" exists in fun_control:\n    if \"core_model_hyper_dict\" not in fun_control.keys():\n        return None\n    else:\n        fun_control[\"core_model_hyper_dict\"][key].update({\"levels\": levels})\n        fun_control[\"core_model_hyper_dict\"][key].update({\"upper\": len(levels) - 1})\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_float_hyperparameter_values","title":"<code>set_float_hyperparameter_values(fun_control, key, lower, upper)</code>","text":"<p>Set the float hyperparameter values in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>key</code> <code>str</code> <p>The key of the hyperparameter.</p> required <code>lower</code> <code>float</code> <p>The lower bound of the hyperparameter.</p> required <code>upper</code> <code>float</code> <p>The upper bound of the hyperparameter.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_float_hyperparameter_values\n    from spotpython.utils.eda import print_exp_table\n    fun_control = fun_control_init(\n        core_model_name=\"forest.AMFRegressor\",\n        hyperdict=RiverHyperDict,\n    )\n    print(\"Before modification:\")\n    print_exp_table(fun_control)\n    set_float_hyperparameter_values(fun_control, \"step\", 0.2, 5)\n    print(\"After modification:\")\n    print_exp_table(fun_control)\n    Seed set to 123\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_float_hyperparameter_values(fun_control, key, lower, upper) -&gt; None:\n    \"\"\"\n    Set the float hyperparameter values in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        key (str):\n            The key of the hyperparameter.\n        lower (float):\n            The lower bound of the hyperparameter.\n        upper (float):\n            The upper bound of the hyperparameter.\n\n    Examples:\n        &gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_float_hyperparameter_values\n            from spotpython.utils.eda import print_exp_table\n            fun_control = fun_control_init(\n                core_model_name=\"forest.AMFRegressor\",\n                hyperdict=RiverHyperDict,\n            )\n            print(\"Before modification:\")\n            print_exp_table(fun_control)\n            set_float_hyperparameter_values(fun_control, \"step\", 0.2, 5)\n            print(\"After modification:\")\n            print_exp_table(fun_control)\n            Seed set to 123\n    \"\"\"\n    set_control_hyperparameter_value(\n        fun_control,\n        key,\n        [\n            lower,\n            upper,\n        ],\n    )\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_hyperparameter","title":"<code>set_hyperparameter(fun_control, key, values)</code>","text":"<p>Set hyperparameter values in the fun_control dictionary based on the type of the values argument.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>key</code> <code>str</code> <p>The key of the hyperparameter.</p> required <code>values</code> <code>Union[int, float, bool, list]</code> <p>The values of the hyperparameter. This can be:     - For int and float: a list containing lower and upper bounds.     - For bool: a list containing two boolean values.     - For factor: a list of strings representing levels.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import set_hyperparameter\n&gt;&gt;&gt; fun_control = {\n        \"core_model_hyper_dict\": {\n            \"n_estimators\": {\"type\": \"int\", \"default\": 10, \"lower\": 2, \"upper\": 1000},\n            \"step\": {\"type\": \"float\", \"default\": 1.0, \"lower\": 0.1, \"upper\": 10.0},\n            \"use_aggregation\": {\"type\": \"factor\", \"default\": 1, \"lower\": 0, \"upper\": 1, \"levels\": [0, 1]},\n            \"leaf_model\": {\"type\": \"factor\", \"default\": \"LinearRegression\", \"upper\": 2}\n        }\n    }\n&gt;&gt;&gt; set_hyperparameter(fun_control, \"n_estimators\", [2, 5])\n&gt;&gt;&gt; set_hyperparameter(fun_control, \"step\", [0.2, 5.0])\n&gt;&gt;&gt; set_hyperparameter(fun_control, \"use_aggregation\", [False, True])\n&gt;&gt;&gt; set_hyperparameter(fun_control, \"leaf_model\", [\"LinearRegression\", \"Perceptron\"])\n&gt;&gt;&gt; set_hyperparameter(fun_control, \"leaf_model\", \"LinearRegression\")\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_hyperparameter(fun_control, key, values):\n    \"\"\"\n    Set hyperparameter values in the fun_control dictionary based on the type of the values argument.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        key (str):\n            The key of the hyperparameter.\n        values (Union[int, float, bool, list]):\n            The values of the hyperparameter. This can be:\n                - For int and float: a list containing lower and upper bounds.\n                - For bool: a list containing two boolean values.\n                - For factor: a list of strings representing levels.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import set_hyperparameter\n        &gt;&gt;&gt; fun_control = {\n                \"core_model_hyper_dict\": {\n                    \"n_estimators\": {\"type\": \"int\", \"default\": 10, \"lower\": 2, \"upper\": 1000},\n                    \"step\": {\"type\": \"float\", \"default\": 1.0, \"lower\": 0.1, \"upper\": 10.0},\n                    \"use_aggregation\": {\"type\": \"factor\", \"default\": 1, \"lower\": 0, \"upper\": 1, \"levels\": [0, 1]},\n                    \"leaf_model\": {\"type\": \"factor\", \"default\": \"LinearRegression\", \"upper\": 2}\n                }\n            }\n        &gt;&gt;&gt; set_hyperparameter(fun_control, \"n_estimators\", [2, 5])\n        &gt;&gt;&gt; set_hyperparameter(fun_control, \"step\", [0.2, 5.0])\n        &gt;&gt;&gt; set_hyperparameter(fun_control, \"use_aggregation\", [False, True])\n        &gt;&gt;&gt; set_hyperparameter(fun_control, \"leaf_model\", [\"LinearRegression\", \"Perceptron\"])\n        &gt;&gt;&gt; set_hyperparameter(fun_control, \"leaf_model\", \"LinearRegression\")\n    \"\"\"\n    # if values is only a string  and not a list of strings, convert it to a list\n    if isinstance(values, str):\n        values = [values]\n    if isinstance(values, list):\n        if all(isinstance(v, int) for v in values):\n            _set_int_hyperparameter_values(fun_control, key, values[0], values[1])\n        elif all(isinstance(v, float) for v in values):\n            _set_float_hyperparameter_values(fun_control, key, values[0], values[1])\n        elif all(isinstance(v, bool) for v in values):\n            _set_boolean_hyperparameter_values(fun_control, key, values[0], values[1])\n        elif all(isinstance(v, str) for v in values):\n            _set_factor_hyperparameter_values(fun_control, key, values)\n        else:\n            raise ValueError(\"Invalid type in values list.\")\n    else:\n        raise TypeError(\"values should be a list.\")\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.set_int_hyperparameter_values","title":"<code>set_int_hyperparameter_values(fun_control, key, lower, upper)</code>","text":"<p>Set (modify) the integer hyperparameter values in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary.</p> required <code>key</code> <code>str</code> <p>The key of the hyperparameter.</p> required <code>lower</code> <code>int</code> <p>The lower bound of the hyperparameter.</p> required <code>upper</code> <code>int</code> <p>The upper bound of the hyperparameter.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.hyperparameters.values import set_int_hyperparameter_values\n    from spotpython.utils.eda import print_exp_table\n    fun_control = fun_control_init(\n        core_model_name=\"forest.AMFRegressor\",\n        hyperdict=RiverHyperDict,\n    )\n    print(\"Before modification:\")\n    print_exp_table(fun_control)\n    set_int_hyperparameter_values(fun_control, \"n_estimators\", 2, 5)\n    print(\"After modification:\")\n    print_exp_table(fun_control)\n    Seed set to 123\n        Before modification:\n        | name            | type   |   default |   lower |   upper | transform   |\n        |-----------------|--------|-----------|---------|---------|-------------|\n        | n_estimators    | int    |        10 |     2   |    1000 | None        |\n        | step            | float  |         1 |     0.1 |      10 | None        |\n        | use_aggregation | factor |         1 |     0   |       1 | None        |\n        Setting hyperparameter n_estimators to value [2, 5].\n        Variable type is int.\n        Core type is None.\n        Calling modify_hyper_parameter_bounds().\n        After modification:\n        | name            | type   |   default |   lower |   upper | transform   |\n        |-----------------|--------|-----------|---------|---------|-------------|\n        | n_estimators    | int    |        10 |     2   |       5 | None        |\n        | step            | float  |         1 |     0.1 |      10 | None        |\n        | use_aggregation | factor |         1 |     0   |       1 | None        |\n</code></pre> Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def set_int_hyperparameter_values(fun_control, key, lower, upper) -&gt; None:\n    \"\"\"\n    Set (modify) the integer hyperparameter values in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary.\n        key (str):\n            The key of the hyperparameter.\n        lower (int):\n            The lower bound of the hyperparameter.\n        upper (int):\n            The upper bound of the hyperparameter.\n\n    Examples:\n        &gt;&gt;&gt; from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.hyperparameters.values import set_int_hyperparameter_values\n            from spotpython.utils.eda import print_exp_table\n            fun_control = fun_control_init(\n                core_model_name=\"forest.AMFRegressor\",\n                hyperdict=RiverHyperDict,\n            )\n            print(\"Before modification:\")\n            print_exp_table(fun_control)\n            set_int_hyperparameter_values(fun_control, \"n_estimators\", 2, 5)\n            print(\"After modification:\")\n            print_exp_table(fun_control)\n            Seed set to 123\n                Before modification:\n                | name            | type   |   default |   lower |   upper | transform   |\n                |-----------------|--------|-----------|---------|---------|-------------|\n                | n_estimators    | int    |        10 |     2   |    1000 | None        |\n                | step            | float  |         1 |     0.1 |      10 | None        |\n                | use_aggregation | factor |         1 |     0   |       1 | None        |\n                Setting hyperparameter n_estimators to value [2, 5].\n                Variable type is int.\n                Core type is None.\n                Calling modify_hyper_parameter_bounds().\n                After modification:\n                | name            | type   |   default |   lower |   upper | transform   |\n                |-----------------|--------|-----------|---------|---------|-------------|\n                | n_estimators    | int    |        10 |     2   |       5 | None        |\n                | step            | float  |         1 |     0.1 |      10 | None        |\n                | use_aggregation | factor |         1 |     0   |       1 | None        |\n    \"\"\"\n    set_control_hyperparameter_value(\n        fun_control,\n        key,\n        [\n            lower,\n            upper,\n        ],\n    )\n</code></pre>"},{"location":"reference/spotpython/hyperparameters/values/#spotpython.hyperparameters.values.update_fun_control_with_hyper_num_cat_dicts","title":"<code>update_fun_control_with_hyper_num_cat_dicts(fun_control, num_dict, cat_dict, dict)</code>","text":"<p>Update an existing fun_control dictionary with new hyperparameter values. All values from the hyperparameter dict (dict) are updated in the fun_control dictionary using the num_dict and cat_dict dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary. This dictionary is updated with the new hyperparameter values.</p> required <code>num_dict</code> <code>dict</code> <p>The dictionary containing the numerical hyperparameter values, which are used to update the fun_control dictionary.</p> required <code>cat_dict</code> <code>dict</code> <p>The dictionary containing the categorical hyperparameter values, which are used to update the fun_control dictionary.</p> required <code>dict</code> <code>dict</code> <p>The dictionary containing the \u201cold\u201d hyperparameter values.</p> required Source code in <code>spotpython/hyperparameters/values.py</code> <pre><code>def update_fun_control_with_hyper_num_cat_dicts(fun_control, num_dict, cat_dict, dict):\n    \"\"\"\n    Update an existing fun_control dictionary with new hyperparameter values.\n    All values from the hyperparameter dict (dict) are updated in the fun_control dictionary\n    using the num_dict and cat_dict dictionaries.\n\n    Args:\n        fun_control (dict):\n            The fun_control dictionary. This dictionary is updated with the new hyperparameter values.\n        num_dict (dict):\n            The dictionary containing the numerical hyperparameter values, which\n            are used to update the fun_control dictionary.\n        cat_dict (dict):\n            The dictionary containing the categorical hyperparameter values, which\n            are used to update the fun_control dictionary.\n        dict (dict):\n            The dictionary containing the \"old\" hyperparameter values.\n    \"\"\"\n    for i, (key, value) in enumerate(dict.items()):\n        if dict[key][\"type\"] == \"int\":\n            set_control_hyperparameter_value(\n                fun_control,\n                key,\n                [\n                    int(num_dict[key][\"lower\"]),\n                    int(num_dict[key][\"upper\"]),\n                ],\n            )\n        if (dict[key][\"type\"] == \"factor\") and (dict[key][\"core_model_parameter_type\"] == \"bool\"):\n            set_control_hyperparameter_value(\n                fun_control,\n                key,\n                [\n                    int(num_dict[key][\"lower\"]),\n                    int(num_dict[key][\"upper\"]),\n                ],\n            )\n        if dict[key][\"type\"] == \"float\":\n            set_control_hyperparameter_value(\n                fun_control,\n                key,\n                [\n                    float(num_dict[key][\"lower\"]),\n                    float(num_dict[key][\"upper\"]),\n                ],\n            )\n        if dict[key][\"type\"] == \"factor\" and dict[key][\"core_model_parameter_type\"] != \"bool\":\n            fle = cat_dict[key][\"levels\"]\n            # convert the string to a list of strings\n            fle = fle.split()\n            set_control_hyperparameter_value(fun_control, key, fle)\n            fun_control[\"core_model_hyper_dict\"][key].update({\"upper\": len(fle) - 1})\n</code></pre>"},{"location":"reference/spotpython/light/cvmodel/","title":"cvmodel","text":""},{"location":"reference/spotpython/light/cvmodel/#spotpython.light.cvmodel.cv_model","title":"<code>cv_model(config, fun_control)</code>","text":"<p>Performs k-fold cross-validation on a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean average precision at k (MAP@k) score of the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"_L_cond\": 0,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n...     \"k_folds\": 5,\n... }\n&gt;&gt;&gt; mapk_score = cv_model(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/cvmodel.py</code> <pre><code>def cv_model(config: dict, fun_control: dict) -&gt; float:\n    \"\"\"\n    Performs k-fold cross-validation on a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        (float): The mean average precision at k (MAP@k) score of the model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"_L_cond\": 0,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ...     \"k_folds\": 5,\n        ... }\n        &gt;&gt;&gt; mapk_score = cv_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _L_cond = fun_control[\"_L_cond\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"CV\" postfix to config_id\n    config_id = generate_config_id(config, timestamp=True) + \"_CV\"\n    results = []\n    num_folds = fun_control[\"k_folds\"]\n    split_seed = 12345\n\n    for k in range(num_folds):\n        print(\"k:\", k)\n\n        model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n\n        if fun_control[\"data_module\"] is None:\n            dm = LightCrossValidationDataModule(\n                k=k,\n                num_splits=num_folds,\n                split_seed=split_seed,\n                dataset=fun_control[\"data_set\"],\n                data_full_train=fun_control[\"data_full_train\"],\n                data_test=fun_control[\"data_test\"],\n                data_val=fun_control[\"data_val\"],\n                num_workers=fun_control[\"num_workers\"],\n                batch_size=config[\"batch_size\"],\n                data_dir=fun_control[\"DATASET_PATH\"],\n                scaler=fun_control[\"scaler\"],\n                collate_fn_name=fun_control[\"collate_fn_name\"],\n                shuffle_train=fun_control[\"shuffle_train\"],\n                shuffle_val=fun_control[\"shuffle_val\"],\n                shuffle_test=fun_control[\"shuffle_test\"],\n                verbosity=fun_control[\"verbosity\"],\n            )\n        else:\n            dm = fun_control[\"data_module\"]\n        dm.setup()\n        dm.prepare_data()\n\n        # TODO: Check if this is necessary:\n        # dm.setup()\n\n        # Init trainer\n        trainer = L.Trainer(\n            # Where to save models\n            default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n            max_epochs=model.hparams.epochs,\n            accelerator=fun_control[\"accelerator\"],\n            devices=fun_control[\"devices\"],\n            strategy=fun_control[\"strategy\"],\n            num_nodes=fun_control[\"num_nodes\"],\n            precision=fun_control[\"precision\"],\n            logger=TensorBoardLogger(\n                save_dir=fun_control[\"TENSORBOARD_PATH\"],\n                version=config_id,\n                default_hp_metric=True,\n                log_graph=fun_control[\"log_graph\"],\n            ),\n            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)],\n            enable_progress_bar=enable_progress_bar,\n        )\n        # Pass the datamodule as arg to trainer.fit to override model hooks :)\n        trainer.fit(model=model, datamodule=dm)\n        # Test best model on validation and test set\n        # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\n        verbose = fun_control[\"verbosity\"] &gt; 0\n        score = trainer.validate(model=model, datamodule=dm, verbose=verbose)\n        # unlist the result (from a list of one dict)\n        score = score[0]\n        print(f\"train_model result: {score}\")\n\n        results.append(score[\"val_loss\"])\n\n    score = sum(results) / num_folds\n    # print(f\"cv_model mapk result: {mapk_score}\")\n    return score\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/","title":"litmodel","text":""},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel","title":"<code>LitModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a simple neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>act_fn</code> <code>str</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>learning_rate</code> <code>float</code> <p>The learning rate for the optimizer.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>model</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; lit_model = LitModel(l1=128, epochs=10, batch_size=BATCH_SIZE, act_fn='relu', optimizer='adam')\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(lit_model, train_loader)\n</code></pre> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>class LitModel(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a simple neural network model.\n\n    Attributes:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        act_fn (str): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        learning_rate (float): The learning rate for the optimizer.\n        _L_in (int): The number of input features.\n        _L_out (int): The number of output classes.\n        model (nn.Sequential): The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; lit_model = LitModel(l1=128, epochs=10, batch_size=BATCH_SIZE, act_fn='relu', optimizer='adam')\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(lit_model, train_loader)\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        act_fn: str,\n        optimizer: str,\n        learning_rate: float = 2e-4,\n        _L_in: int = 28 * 28,\n        _L_out: int = 10,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the LitModel object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            act_fn (str): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            learning_rate (float, optional): The learning rate for the optimizer. Defaults to 2e-4.\n            _L_in (int, optional): The number of input features. Defaults to 28 * 28.\n            _L_out (int, optional): The number of output classes. Defaults to 10.\n\n        Returns:\n           (NoneType): None\n        \"\"\"\n        super().__init__()\n\n        # We take in input dimensions as parameters and use those to dynamically build model.\n        self._L_out = _L_out\n        self.l1 = l1\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.act_fn = act_fn\n        self.optimizer = optimizer\n        self.learning_rate = learning_rate\n\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(_L_in, l1),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(l1, l1),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(l1, _L_out),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the log probabilities for each class.\n        \"\"\"\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch: A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int) -&gt; None:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n\n        Returns:\n            None\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def test_step(self, batch: tuple, batch_idx: int) -&gt; tuple:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n\n        Returns:\n            tuple: A tuple containing the loss and accuracy for this batch.\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n        return loss, acc\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n        \"\"\"\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.__init__","title":"<code>__init__(l1, epochs, batch_size, act_fn, optimizer, learning_rate=0.0002, _L_in=28 * 28, _L_out=10, *args, **kwargs)</code>","text":"<p>Initializes the LitModel object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>act_fn</code> <code>str</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate for the optimizer. Defaults to 2e-4.</p> <code>0.0002</code> <code>_L_in</code> <code>int</code> <p>The number of input features. Defaults to 28 * 28.</p> <code>28 * 28</code> <code>_L_out</code> <code>int</code> <p>The number of output classes. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    act_fn: str,\n    optimizer: str,\n    learning_rate: float = 2e-4,\n    _L_in: int = 28 * 28,\n    _L_out: int = 10,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the LitModel object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        act_fn (str): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        learning_rate (float, optional): The learning rate for the optimizer. Defaults to 2e-4.\n        _L_in (int, optional): The number of input features. Defaults to 28 * 28.\n        _L_out (int, optional): The number of output classes. Defaults to 10.\n\n    Returns:\n       (NoneType): None\n    \"\"\"\n    super().__init__()\n\n    # We take in input dimensions as parameters and use those to dynamically build model.\n    self._L_out = _L_out\n    self.l1 = l1\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.act_fn = act_fn\n    self.optimizer = optimizer\n    self.learning_rate = learning_rate\n\n    self.model = nn.Sequential(\n        nn.Flatten(),\n        nn.Linear(_L_in, l1),\n        nn.ReLU(),\n        nn.Dropout(0.1),\n        nn.Linear(l1, l1),\n        nn.ReLU(),\n        nn.Dropout(0.1),\n        nn.Linear(l1, _L_out),\n    )\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n    \"\"\"\n    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the log probabilities for each class.</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the log probabilities for each class.\n    \"\"\"\n    x = self.model(x)\n    return F.log_softmax(x, dim=1)\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the loss and accuracy for this batch.</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int) -&gt; tuple:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n\n    Returns:\n        tuple: A tuple containing the loss and accuracy for this batch.\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.nll_loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n    self.log(\"val_loss\", loss, prog_bar=True)\n    self.log(\"val_acc\", acc, prog_bar=True)\n    return loss, acc\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch: A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.nll_loss(logits, y)\n    return loss\n</code></pre>"},{"location":"reference/spotpython/light/litmodel/#spotpython.light.litmodel.LitModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/light/litmodel.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int) -&gt; None:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n\n    Returns:\n        None\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.nll_loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n    self.log(\"val_loss\", loss, prog_bar=True)\n    self.log(\"val_acc\", acc, prog_bar=True)\n</code></pre>"},{"location":"reference/spotpython/light/loadmodel/","title":"loadmodel","text":""},{"location":"reference/spotpython/light/loadmodel/#spotpython.light.loadmodel.load_light_from_checkpoint","title":"<code>load_light_from_checkpoint(config, fun_control, postfix='_TEST')</code>","text":"<p>Loads a model from a checkpoint using the given configuration and function control parameters.</p> Notes <ul> <li><code>load_light_from_checkpoint</code> loads the last checkpoint of the model</li> <li>Randomness, dropout, etc\u2026 are disabled.</li> </ul> References <ul> <li>https://pytorch-lightning.readthedocs.io/en/0.8.5/weights_loading.html</li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <code>postfix</code> <code>str</code> <p>The postfix to append to the configuration ID when generating the checkpoint path. Default is \u201c_TEST\u201d. Can be set to \u201c_TRAIN\u201d for training checkpoints.</p> <code>'_TEST'</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The loaded model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"_torchmetric\": \"mean_squared_error\",\n...     \"core_model\": MyModel,\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; model = load_light_from_checkpoint(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/loadmodel.py</code> <pre><code>def load_light_from_checkpoint(config: dict, fun_control: dict, postfix: str = \"_TEST\") -&gt; Any:\n    \"\"\"\n    Loads a model from a checkpoint using the given configuration and function control parameters.\n\n    Notes:\n        * `load_light_from_checkpoint` loads the last checkpoint of the model\n        * Randomness, dropout, etc... are disabled.\n\n    References:\n        * https://pytorch-lightning.readthedocs.io/en/0.8.5/weights_loading.html\n\n    Args:\n        config (dict):\n            A dictionary containing the configuration parameters for the model.\n        fun_control (dict):\n            A dictionary containing the function control parameters.\n        postfix (str):\n            The postfix to append to the configuration ID when generating the checkpoint path.\n            Default is \"_TEST\". Can be set to \"_TRAIN\" for training checkpoints.\n\n    Returns:\n        Any: The loaded model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"_torchmetric\": \"mean_squared_error\",\n        ...     \"core_model\": MyModel,\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; model = load_light_from_checkpoint(config, fun_control)\n    \"\"\"\n    print(f\"config: {config}\")\n    # load a model from a checkpoint with the same config_id\n    # that was used in the test phase. Therefore, no timestamp is added.\n    config_id = generate_config_id(config, timestamp=False) + postfix\n    default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id, \"last.ckpt\")\n    print(f\"Loading model with {config_id} from {default_root_dir}\")\n    model = fun_control[\"core_model\"].load_from_checkpoint(\n        default_root_dir,\n        _L_in=fun_control[\"_L_in\"],\n        _L_out=fun_control[\"_L_out\"],\n        _L_cond=fun_control[\"_L_cond\"],\n        _torchmetric=fun_control[\"_torchmetric\"],\n    )\n    # disable randomness, dropout, etc...\n    print(f\"Model: {model}\")\n    model.eval()\n    return model\n</code></pre>"},{"location":"reference/spotpython/light/predictmodel/","title":"predictmodel","text":""},{"location":"reference/spotpython/light/predictmodel/#spotpython.light.predictmodel.predict_model","title":"<code>predict_model(config, fun_control)</code>","text":"<p>Predicts using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: The validation loss and the hyperparameter metric of the tested model.</p> Notes <ul> <li><code>test_model</code> saves the last checkpoint of the model from the training phase, which is called as follows:     <code>trainer.fit(model=model, datamodule=dm)</code>.</li> <li>The test result is evaluated with the following function call: <code>trainer.test(datamodule=dm, ckpt_path=\"last\")</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n     from spotpython.light.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n      get_default_hyperparameters_as_array)\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.hyperparameters.values import (get_var_name, assign_values,\n        generate_one_config_from_var_dict)\n    import spotpython.light.testmodel as tm\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\")\n    dataset = Diabetes()\n    set_control_key_value(control_dict=fun_control,\n                            key=\"data_set\",\n                            value=dataset)\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    var_dict = assign_values(X, get_var_name(fun_control))\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        y_test = tm.test_model(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/predictmodel.py</code> <pre><code>def predict_model(config: dict, fun_control: dict) -&gt; Tuple[float, float]:\n    \"\"\"\n    Predicts using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        Tuple[float, float]: The validation loss and the hyperparameter metric of the tested model.\n\n    Notes:\n        * `test_model` saves the last checkpoint of the model from the training phase, which is called as follows:\n            `trainer.fit(model=model, datamodule=dm)`.\n        * The test result is evaluated with the following function call:\n        `trainer.test(datamodule=dm, ckpt_path=\"last\")`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n             from spotpython.light.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n              get_default_hyperparameters_as_array)\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.hyperparameters.values import (get_var_name, assign_values,\n                generate_one_config_from_var_dict)\n            import spotpython.light.testmodel as tm\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\")\n            dataset = Diabetes()\n            set_control_key_value(control_dict=fun_control,\n                                    key=\"data_set\",\n                                    value=dataset)\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            var_dict = assign_values(X, get_var_name(fun_control))\n            for config in generate_one_config_from_var_dict(var_dict, fun_control):\n                y_test = tm.test_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _L_cond = fun_control[\"_L_cond\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"TEST\" postfix to config_id\n    # config id is unique. Since the model is loaded from a checkpoint,\n    # the config id is generated here without a timestamp. This differs from\n    # the config id generated in cvmodel.py and trainmodel.py.\n    config_id = generate_config_id(config, timestamp=False) + \"_TEST\"\n    if fun_control[\"data_module\"] is None:\n        dm = LightDataModule(\n            dataset=fun_control[\"data_set\"],\n            data_full_train=fun_control[\"data_full_train\"],\n            data_test=fun_control[\"data_test\"],\n            data_val=fun_control[\"data_val\"],\n            batch_size=config[\"batch_size\"],\n            num_workers=fun_control[\"num_workers\"],\n            test_size=fun_control[\"test_size\"],\n            test_seed=fun_control[\"test_seed\"],\n            scaler=fun_control[\"scaler\"],\n            collate_fn_name=fun_control[\"collate_fn_name\"],\n            shuffle_train=fun_control[\"shuffle_train\"],\n            shuffle_val=fun_control[\"shuffle_val\"],\n            shuffle_test=fun_control[\"shuffle_test\"],\n            verbosity=fun_control[\"verbosity\"],\n        )\n    else:\n        dm = fun_control[\"data_module\"]\n    # TODO: Check if this is necessary:\n    dm.setup(stage=\"train\")\n    # Init model from datamodule's attributes\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=fun_control[\"accelerator\"],\n        devices=fun_control[\"devices\"],\n        strategy=fun_control[\"strategy\"],\n        num_nodes=fun_control[\"num_nodes\"],\n        precision=fun_control[\"precision\"],\n        logger=TensorBoardLogger(\n            save_dir=fun_control[\"TENSORBOARD_PATH\"],\n            version=config_id,\n            default_hp_metric=True,\n            log_graph=fun_control[\"log_graph\"],\n        ),\n        callbacks=[\n            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False),\n            ModelCheckpoint(dirpath=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id), save_last=True),  # Save the last checkpoint\n        ],\n        enable_progress_bar=enable_progress_bar,\n    )\n    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n    trainer.fit(model=model, datamodule=dm)\n\n    # Changed in spotpython 0.18.12: commented out the following line\n    dm.setup(stage=\"predict\")\n\n    # predictions = trainer.predict(model=model, datamodule=dm)\n    # Changed in spotpython 0.18.12: use ckpt_path=\"last\" to load the last checkpoint and not the model\n    # predictions = trainer.predict(datamodule=dm, ckpt_path=\"last\")\n    # Changed in spotpython 0.19.5: use ckpt_path=\"best\" to load the best checkpoint and not the model\n    predictions = trainer.predict(datamodule=dm, ckpt_path=\"best\")\n\n    # # Load the last checkpoint\n    # test_result = trainer.test(datamodule=dm, ckpt_path=\"last\")\n    # test_result = test_result[0]\n    # print(f\"test_model result: {test_result}\")\n    return predictions\n</code></pre>"},{"location":"reference/spotpython/light/testmodel/","title":"testmodel","text":""},{"location":"reference/spotpython/light/testmodel/#spotpython.light.testmodel.test_model","title":"<code>test_model(config, fun_control)</code>","text":"<p>Tests a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: The validation loss and the hyperparameter metric of the tested model.</p> Notes <ul> <li><code>test_model</code> saves the last checkpoint of the model from the training phase, which is called as follows:     <code>trainer.fit(model=model, datamodule=dm)</code>.</li> <li>The test result is evaluated with the following function call: <code>trainer.test(datamodule=dm, ckpt_path=\"last\")</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n     from spotpython.light.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n      get_default_hyperparameters_as_array)\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.hyperparameters.values import (get_var_name, assign_values,\n        generate_one_config_from_var_dict)\n    import spotpython.light.testmodel as tm\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\")\n    dataset = Diabetes()\n    set_control_key_value(control_dict=fun_control,\n                            key=\"data_set\",\n                            value=dataset)\n    add_core_model_to_fun_control(core_model=NetLightRegression,\n                                fun_control=fun_control,\n                                hyper_dict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    var_dict = assign_values(X, get_var_name(fun_control))\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        y_test = tm.test_model(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/testmodel.py</code> <pre><code>def test_model(config: dict, fun_control: dict) -&gt; Tuple[float, float]:\n    \"\"\"\n    Tests a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        Tuple[float, float]: The validation loss and the hyperparameter metric of the tested model.\n\n    Notes:\n        * `test_model` saves the last checkpoint of the model from the training phase, which is called as follows:\n            `trainer.fit(model=model, datamodule=dm)`.\n        * The test result is evaluated with the following function call:\n        `trainer.test(datamodule=dm, ckpt_path=\"last\")`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n             from spotpython.light.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n              get_default_hyperparameters_as_array)\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.hyperparameters.values import (get_var_name, assign_values,\n                generate_one_config_from_var_dict)\n            import spotpython.light.testmodel as tm\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\")\n            dataset = Diabetes()\n            set_control_key_value(control_dict=fun_control,\n                                    key=\"data_set\",\n                                    value=dataset)\n            add_core_model_to_fun_control(core_model=NetLightRegression,\n                                        fun_control=fun_control,\n                                        hyper_dict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            var_dict = assign_values(X, get_var_name(fun_control))\n            for config in generate_one_config_from_var_dict(var_dict, fun_control):\n                y_test = tm.test_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _L_cond = fun_control[\"_L_cond\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"TEST\" postfix to config_id\n    # config id is unique. Since the model is loaded from a checkpoint,\n    # the config id is generated here without a timestamp. This differs from\n    # the config id generated in cvmodel.py and trainmodel.py.\n    config_id = generate_config_id(config, timestamp=False) + \"_TEST\"\n    if fun_control[\"data_module\"] is None:\n        dm = LightDataModule(\n            dataset=fun_control[\"data_set\"],\n            data_full_train=fun_control[\"data_full_train\"],\n            data_test=fun_control[\"data_test\"],\n            data_val=fun_control[\"data_val\"],\n            batch_size=config[\"batch_size\"],\n            num_workers=fun_control[\"num_workers\"],\n            test_size=fun_control[\"test_size\"],\n            test_seed=fun_control[\"test_seed\"],\n            scaler=fun_control[\"scaler\"],\n            collate_fn_name=fun_control[\"collate_fn_name\"],\n            shuffle_train=fun_control[\"shuffle_train\"],\n            shuffle_val=fun_control[\"shuffle_val\"],\n            shuffle_test=fun_control[\"shuffle_test\"],\n            verbosity=fun_control[\"verbosity\"],\n        )\n    else:\n        dm = fun_control[\"data_module\"]\n    # TODO: Check if this is necessary:\n    # dm.setup()\n    # Init model from datamodule's attributes\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=fun_control[\"accelerator\"],\n        devices=fun_control[\"devices\"],\n        strategy=fun_control[\"strategy\"],\n        num_nodes=fun_control[\"num_nodes\"],\n        precision=fun_control[\"precision\"],\n        logger=TensorBoardLogger(\n            save_dir=fun_control[\"TENSORBOARD_PATH\"],\n            version=config_id,\n            default_hp_metric=True,\n            log_graph=fun_control[\"log_graph\"],\n        ),\n        callbacks=[\n            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False),\n            ModelCheckpoint(dirpath=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id), save_last=True),  # Save the last checkpoint\n        ],\n        enable_progress_bar=enable_progress_bar,\n    )\n    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n    trainer.fit(model=model, datamodule=dm)\n\n    # Load the last checkpoint\n    # test_result = trainer.test(datamodule=dm, ckpt_path=\"last\")\n    test_result = trainer.test(datamodule=dm, ckpt_path=\"best\")\n    test_result = test_result[0]\n    print(f\"test_model result: {test_result}\")\n    return test_result[\"val_loss\"], test_result[\"hp_metric\"]\n</code></pre>"},{"location":"reference/spotpython/light/trainmodel/","title":"trainmodel","text":""},{"location":"reference/spotpython/light/trainmodel/#spotpython.light.trainmodel.build_model_instance","title":"<code>build_model_instance(config, fun_control)</code>","text":"<p>Builds the core model using the configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Model configuration parameters.</p> required <code>fun_control</code> <code>dict</code> <p>Function control parameters.</p> required <p>Returns:</p> Type Description <code>LightningModule</code> <p>The constructed core model.</p> Source code in <code>spotpython/light/trainmodel.py</code> <pre><code>def build_model_instance(config: dict, fun_control: dict) -&gt; L.LightningModule:\n    \"\"\"\n    Builds the core model using the configuration and function control parameters.\n\n    Args:\n        config (dict): Model configuration parameters.\n        fun_control (dict): Function control parameters.\n\n    Returns:\n        The constructed core model.\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _L_cond = fun_control[\"_L_cond\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    return fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _L_cond=_L_cond, _torchmetric=_torchmetric)\n</code></pre>"},{"location":"reference/spotpython/light/trainmodel/#spotpython.light.trainmodel.generate_config_id_with_timestamp","title":"<code>generate_config_id_with_timestamp(config, timestamp)</code>","text":"<p>Generates a configuration ID based on the given config and timestamp flag.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration parameters.</p> required <code>timestamp</code> <code>bool</code> <p>Indicates whether to include a timestamp in the config ID.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated configuration ID.</p> Source code in <code>spotpython/light/trainmodel.py</code> <pre><code>def generate_config_id_with_timestamp(config: dict, timestamp: bool) -&gt; str:\n    \"\"\"\n    Generates a configuration ID based on the given config and timestamp flag.\n\n    Args:\n        config (dict): The configuration parameters.\n        timestamp (bool): Indicates whether to include a timestamp in the config ID.\n\n    Returns:\n        str: The generated configuration ID.\n    \"\"\"\n    if timestamp:\n        # config id is unique. Since the model is not loaded from a checkpoint,\n        # the config id is generated here with a timestamp.\n        config_id = generate_config_id(config, timestamp=True)\n    else:\n        # config id is not time-dependent and therefore unique,\n        # so that the model can be loaded from a checkpoint,\n        # the config id is generated here without a timestamp.\n        config_id = generate_config_id(config, timestamp=False) + \"_TRAIN\"\n    return config_id\n</code></pre>"},{"location":"reference/spotpython/light/trainmodel/#spotpython.light.trainmodel.train_model","title":"<code>train_model(config, fun_control, timestamp=True)</code>","text":"<p>Trains a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <code>timestamp</code> <code>bool</code> <p>A boolean value indicating whether to include a timestamp in the config id. Default is True. If False, the string \u201c_TRAIN\u201d is appended to the config id.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The validation loss of the trained model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from math import inf\n    import numpy as np\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.utils.eda import print_exp_table\n    from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n    from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n    from spotpython.light.trainmodel import train_model\n    import pprint\n    PREFIX=\"000\"\n    data_set = Diabetes()\n    fun_control = fun_control_init(\n        PREFIX=PREFIX,\n        save_experiment=True,\n        fun_evals=inf,\n        max_time=1,\n        data_set = data_set,\n        core_model_name=\"light.regression.NNLinearRegressor\",\n        hyperdict=LightHyperDict,\n        _L_in=10,\n        _L_out=1,\n        TENSORBOARD_CLEAN=True,\n        tensorboard_log=True,\n        seed=42,)\n    print_exp_table(fun_control)\n    X = get_default_hyperparameters_as_array(fun_control)\n    # set epochs to 2^8:\n    # X[0, 1] = 8\n    # set patience to 2^10:\n    # X[0, 7] = 10\n    print(f\"X: {X}\")\n    # combine X and X to a np.array with shape (2, n_hyperparams)\n    # so that two values are returned\n    X = np.vstack((X, X))\n    var_dict = assign_values(X, get_var_name(fun_control))\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        pprint.pprint(config)\n        y = train_model(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/trainmodel.py</code> <pre><code>def train_model(config: dict, fun_control: dict, timestamp: bool = True) -&gt; float:\n    \"\"\"\n    Trains a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict):\n            A dictionary containing the configuration parameters for the model.\n        fun_control (dict):\n            A dictionary containing the function control parameters.\n        timestamp (bool):\n            A boolean value indicating whether to include a timestamp in the config id. Default is True.\n            If False, the string \"_TRAIN\" is appended to the config id.\n\n    Returns:\n        float: The validation loss of the trained model.\n\n    Examples:\n        &gt;&gt;&gt; from math import inf\n            import numpy as np\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.utils.eda import print_exp_table\n            from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n            from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n            from spotpython.light.trainmodel import train_model\n            import pprint\n            PREFIX=\"000\"\n            data_set = Diabetes()\n            fun_control = fun_control_init(\n                PREFIX=PREFIX,\n                save_experiment=True,\n                fun_evals=inf,\n                max_time=1,\n                data_set = data_set,\n                core_model_name=\"light.regression.NNLinearRegressor\",\n                hyperdict=LightHyperDict,\n                _L_in=10,\n                _L_out=1,\n                TENSORBOARD_CLEAN=True,\n                tensorboard_log=True,\n                seed=42,)\n            print_exp_table(fun_control)\n            X = get_default_hyperparameters_as_array(fun_control)\n            # set epochs to 2^8:\n            # X[0, 1] = 8\n            # set patience to 2^10:\n            # X[0, 7] = 10\n            print(f\"X: {X}\")\n            # combine X and X to a np.array with shape (2, n_hyperparams)\n            # so that two values are returned\n            X = np.vstack((X, X))\n            var_dict = assign_values(X, get_var_name(fun_control))\n            for config in generate_one_config_from_var_dict(var_dict, fun_control):\n                pprint.pprint(config)\n                y = train_model(config, fun_control)\n    \"\"\"\n    if fun_control[\"data_module\"] is None:\n        dm = LightDataModule(\n            dataset=fun_control[\"data_set\"],\n            data_full_train=fun_control[\"data_full_train\"],\n            data_test=fun_control[\"data_test\"],\n            data_val=fun_control[\"data_val\"],\n            batch_size=config[\"batch_size\"],\n            num_workers=fun_control[\"num_workers\"],\n            test_size=fun_control[\"test_size\"],\n            test_seed=fun_control[\"test_seed\"],\n            scaler=fun_control[\"scaler\"],\n            collate_fn_name=fun_control[\"collate_fn_name\"],\n            shuffle_train=fun_control[\"shuffle_train\"],\n            shuffle_val=fun_control[\"shuffle_val\"],\n            shuffle_test=fun_control[\"shuffle_test\"],\n            verbosity=fun_control[\"verbosity\"],\n        )\n    else:\n        dm = fun_control[\"data_module\"]\n\n    model = build_model_instance(config, fun_control)\n    # TODO: Check if this is necessary or if this is handled by the trainer\n    # dm.setup()\n    # print(f\"train_model(): Test set size: {len(dm.data_test)}\")\n    # print(f\"train_model(): Train set size: {len(dm.data_train)}\")\n    # print(f\"train_model(): Batch size: {config['batch_size']}\")\n\n    # Callbacks\n    #\n    # EarlyStopping:\n    # Stop training when a monitored quantity has stopped improving.\n    # The EarlyStopping callback runs at the end of every validation epoch by default.\n    # However, the frequency of validation can be modified by setting various parameters\n    # in the Trainer, for example check_val_every_n_epoch and val_check_interval.\n    # It must be noted that the patience parameter counts the number of validation checks\n    # with no improvement, and not the number of training epochs.\n    # Therefore, with parameters check_val_every_n_epoch=10 and patience=3,\n    # the trainer will perform at least 40 training epochs before being stopped.\n    # Args:\n    # - monitor:\n    #   Quantity to be monitored. Default: 'val_loss'.\n    # - patience:\n    #   Number of validation checks with no improvement after which training will be stopped.\n    #   In spotpython, this is a hyperparameter.\n    # - mode (str):\n    #   one of {min, max}. If save_top_k != 0, the decision to overwrite the current save file\n    #   is made based on either the maximization or the minimization of the monitored quantity.\n    #   For 'val_acc', this should be 'max', for 'val_loss' this should be 'min', etc.\n    # - strict:\n    #   Set to False.\n    # - verbose:\n    #   If True, prints a message to the logger.\n    #\n    # ModelCheckpoint:\n    # Save the model periodically by monitoring a quantity.\n    # Every metric logged with log() or log_dict() is a candidate for the monitor key.\n    # spotpython uses ModelCheckpoint if timestamp is set to False. In this case, the\n    # config_id has no timestamp and ends with the unique string \"_TRAIN\". This\n    # enables loading the model from a checkpoint, because the config_id is unique.\n    # Args:\n    # - dirpath:\n    #   Path to the directory where the checkpoints will be saved.\n    # - monitor (str):\n    #   Quantity to monitor.\n    #   By default it is None which saves a checkpoint only for the last epoch.\n    # - verbose (bool):\n    #   If True, prints a message to the logger.\n    # - save_last (Union[bool, Literal['link'], None]):\n    #   When True, saves a last.ckpt copy whenever a checkpoint file gets saved.\n    #   Can be set to 'link' on a local filesystem to create a symbolic link.\n    #   This allows accessing the latest checkpoint in a deterministic manner.\n    #   Default: None.\n    config_id = generate_config_id_with_timestamp(config=config, timestamp=timestamp)\n    callbacks = [EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)]\n    if not timestamp:\n        # add ModelCheckpoint only if timestamp is False\n        dirpath = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id)\n        callbacks.append(ModelCheckpoint(dirpath=dirpath, monitor=None, verbose=False, save_last=True))  # Save the last checkpoint\n\n    if fun_control[\"hacky\"]:\n        verbose = fun_control[\"verbosity\"] &gt; 0\n        ds = fun_control[\"data_full_train\"]\n        indices = list(range(len(ds)))\n        indice_results_val_loss = []\n        indice_results_hp_metric = []\n        for i in indices:\n            print(f\"train_model(): Hacky Implementation with Index {i}\")\n            test_indices = [indices[i]]\n            train_indices = [index for index in indices if index != test_indices[0]]\n\n            train_dataset = torch.utils.data.Subset(ds, train_indices)\n            test_dataset = torch.utils.data.Subset(ds, test_indices)\n\n            train_dl = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=PadSequenceManyToMany())\n            test_dl = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=PadSequenceManyToMany())\n\n            model = build_model_instance(config, fun_control)\n\n            enable_progress_bar = fun_control[\"enable_progress_bar\"] or False\n            trainer = L.Trainer(\n                # Where to save models\n                default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n                max_epochs=model.hparams.epochs,\n                accelerator=fun_control[\"accelerator\"],\n                devices=fun_control[\"devices\"],\n                strategy=fun_control[\"strategy\"],\n                num_nodes=fun_control[\"num_nodes\"],\n                precision=fun_control[\"precision\"],\n                logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True, log_graph=fun_control[\"log_graph\"], name=\"\"),\n                callbacks=callbacks,\n                enable_progress_bar=enable_progress_bar,\n                num_sanity_val_steps=fun_control[\"num_sanity_val_steps\"],\n                log_every_n_steps=fun_control[\"log_every_n_steps\"],\n                gradient_clip_val=None,\n                gradient_clip_algorithm=\"norm\",\n            )\n\n            trainer.fit(model=model, train_dataloaders=train_dl, ckpt_path=None)\n            result = trainer.validate(model=model, dataloaders=test_dl, ckpt_path=None, verbose=verbose)\n            result = result[0]\n\n            print(f\"results_dict: {result}\")\n\n            indice_results_val_loss.append(result[\"val_loss\"])\n            indice_results_hp_metric.append(result[\"hp_metric\"])\n\n        mean_val_loss = np.mean(indice_results_val_loss)\n        mean_hp_metric = np.mean(indice_results_hp_metric)\n\n        print(f\"train_model(): Mean Validation Loss: {mean_val_loss}\")\n        print(f\"train_model(): Mean Hyperparameter Metric: {mean_hp_metric}\")\n\n        results_dict = {\"val_loss\": mean_val_loss, \"hp_metric\": mean_hp_metric}\n\n        return results_dict[\"val_loss\"]\n\n    # Tensorboard logger. The tensorboard is passed to the trainer.\n    # See: https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.TensorBoardLogger.html\n    # It uses the following arguments:\n    # Args:\n    # - save_dir:\n    #   Where to save logs. Can be specified via fun_control[\"TENSORBOARD_PATH\"]\n    # - name:\n    #   Experiment name. Defaults to 'default'.\n    #   If it is the empty string then no per-experiment subdirectory is used.\n    #   Changed in spotpython 0.17.2 to the empty string.\n    # - version:\n    #   Experiment version. If version is not specified the logger inspects the save directory\n    #   for existing versions, then automatically assigns the next available version.\n    #   If it is a string then it is used as the run-specific subdirectory name,\n    #   otherwise 'version_${version}' is used. spotpython uses the config_id as version.\n    # - log_graph (bool):\n    #   Adds the computational graph to tensorboard.\n    #   This requires that the user has defined the self.example_input_array\n    #   attribute in their model. Set in spotpython to fun_control[\"log_graph\"].\n    # - default_hp_metric (bool):\n    #   Enables a placeholder metric with key hp_metric when log_hyperparams is called\n    #   without a metric (otherwise calls to log_hyperparams without a metric are ignored).\n    #   spotpython sets this to True.\n\n    # Init trainer. See: https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer\n    # Args used by spotpython (there are more):\n    # - default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n    #   Default: os.getcwd(). Can be remote file paths such as s3://mybucket/path or \u2018hdfs://path/\u2019\n    # - max_epochs: Stop training once this number of epochs is reached.\n    #   Disabled by default (None).\n    #   If both max_epochs and max_steps are not specified, defaults to max_epochs = 1000.\n    #   To enable infinite training, set max_epochs = -1.\n    # - accelerator: Supports passing different accelerator types\n    #   (\u201ccpu\u201d, \u201cgpu\u201d, \u201ctpu\u201d, \u201chpu\u201d, \u201cmps\u201d, \u201cauto\u201d) as well as custom accelerator instances.\n    # - devices: The devices to use. Can be set to a positive number (int or str),\n    #   a sequence of device indices (list or str), the value -1 to indicate all available devices\n    #   should be used, or \"auto\" for automatic selection based on the chosen accelerator.\n    #   Default: \"auto\".\n    # - strategy: Supports different training strategies with aliases as well custom strategies.\n    #   Default: \"auto\".\n    # - num_nodes: Number of GPU nodes for distributed training. Default: 1.\n    # - precision: Double precision (64, \u201864\u2019 or \u201864-true\u2019), full precision (32, \u201832\u2019 or \u201832-true\u2019),\n    #   16bit mixed precision (16, \u201816\u2019, \u201816-mixed\u2019) or bfloat16 mixed precision (\u2018bf16\u2019, \u2018bf16-mixed\u2019).\n    #   Can be used on CPU, GPU, TPUs, or HPUs. Default: '32-true'.\n    # - logger: Logger (or iterable collection of loggers) for experiment tracking.\n    #   A True value uses the default TensorBoardLogger if it is installed, otherwise CSVLogger.\n    #   False will disable logging. If multiple loggers are provided, local files (checkpoints,\n    #   profiler traces, etc.) are saved in the log_dir of the first logger. Default: True.\n    # - callbacks: List of callbacks to enable during training.Default: None.\n    # - enable_progress_bar: If True, enables the progress bar.\n    #   Whether to enable to progress bar by default. Default: True.\n    # - num_sanity_val_steps:\n    #   Sanity check runs n validation batches before starting the training routine.\n    #   Set it to -1 to run all batches in all validation dataloaders. Default: 2.\n    # - log_every_n_steps:\n    #   How often to log within steps. Default: 50.\n    # - gradient_clip_val:\n    #   The value at which to clip gradients. Passing gradient_clip_val=None\n    #   disables gradient clipping. If using Automatic Mixed Precision (AMP),\n    #   the gradients will be unscaled before. Default: None.\n    # - gradient_clip_algorithm (str):\n    #   The gradient clipping algorithm to use.\n    #   Pass gradient_clip_algorithm=\"value\" to clip by value,\n    #   and gradient_clip_algorithm=\"norm\" to clip by norm.\n    #   By default it will be set to \"norm\".\n\n    enable_progress_bar = fun_control[\"enable_progress_bar\"] or False\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=fun_control[\"accelerator\"],\n        devices=fun_control[\"devices\"],\n        strategy=fun_control[\"strategy\"],\n        num_nodes=fun_control[\"num_nodes\"],\n        precision=fun_control[\"precision\"],\n        logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True, log_graph=fun_control[\"log_graph\"], name=\"\"),\n        callbacks=callbacks,\n        enable_progress_bar=enable_progress_bar,\n        num_sanity_val_steps=fun_control[\"num_sanity_val_steps\"],\n        log_every_n_steps=fun_control[\"log_every_n_steps\"],\n        gradient_clip_val=None,\n        gradient_clip_algorithm=\"norm\",\n    )\n    # Fit the model\n    # Args:\n    # - model: Model to fit\n    # - datamodule: A LightningDataModule that defines the train_dataloader\n    #   hook. Pass the datamodule as arg to trainer.fit to override model hooks # :)\n    # - ckpt_path: Path/URL of the checkpoint from which training is resumed.\n    #   Could also be one of two special keywords \"last\" and \"hpc\".\n    #   If there is no checkpoint file at the path, an exception is raised.\n    try:\n        trainer.fit(model=model, datamodule=dm, ckpt_path=None)\n    except Exception as e:\n        print(f\"train_model(): trainer.fit failed with exception: {e}\")\n    # Test best model on validation and test set\n    verbose = fun_control[\"verbosity\"] &gt; 0\n\n    # Validate the model\n    # Perform one evaluation epoch over the validation set.\n    # Args:\n    # - model: The model to validate.\n    # - datamodule: A LightningDataModule that defines the val_dataloader hook.\n    # - verbose: If True, prints the validation results.\n    # - ckpt_path: Path to a specific checkpoint to load for validation.\n    #   Either \"best\", \"last\", \"hpc\" or path to the checkpoint you wish to validate.\n    #   If None and the model instance was passed, use the current weights.\n    #   Otherwise, the best model checkpoint from the previous trainer.fit call will\n    #   be loaded if a checkpoint callback is configured.\n    # Returns:\n    # - List of dictionaries with metrics logged during the validation phase,\n    #   e.g., in model- or callback hooks like validation_step() etc.\n    #   The length of the list corresponds to the number of validation dataloaders used.\n    result = trainer.validate(model=model, datamodule=dm, ckpt_path=None, verbose=verbose)\n\n    # unlist the result (from a list of one dict)\n    result = result[0]\n    print(f\"train_model result: {result}\")\n    return result[\"val_loss\"]\n</code></pre>"},{"location":"reference/spotpython/light/trainmodel/#spotpython.light.trainmodel.train_model_xai","title":"<code>train_model_xai(config, fun_control, timestamp=True)</code>","text":"<p>Trains a model using the given configuration and function control parameters. Performs feature attribution analysis and calculates consistency of these methods.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <code>timestamp</code> <code>bool</code> <p>A boolean value indicating whether to include a timestamp in the config id. Default is True. If False, the string \u201c_TRAIN\u201d is appended to the config id.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The validation loss and the feature attribution inconsitency of the trained model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from math import inf\n    import numpy as np\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.utils.init import fun_control_init\n    from spotpython.utils.eda import print_exp_table\n    from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n    from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n    from spotpython.light.trainmodel import train_model\n    import pprint\n    PREFIX=\"000\"\n    data_set = Diabetes()\n    fun_control = fun_control_init(\n        PREFIX=PREFIX,\n        save_experiment=True,\n        fun_evals=inf,\n        max_time=1,\n        data_set = data_set,\n        core_model_name=\"light.regression.NNLinearRegressor\",\n        hyperdict=LightHyperDict,\n        _L_in=10,\n        _L_out=1,\n        TENSORBOARD_CLEAN=True,\n        tensorboard_log=True,\n        seed=42,)\n    print_exp_table(fun_control)\n    X = get_default_hyperparameters_as_array(fun_control)\n    # set epochs to 2^8:\n    # X[0, 1] = 8\n    # set patience to 2^10:\n    # X[0, 7] = 10\n    print(f\"X: {X}\")\n    # combine X and X to a np.array with shape (2, n_hyperparams)\n    # so that two values are returned\n    X = np.vstack((X, X))\n    var_dict = assign_values(X, get_var_name(fun_control))\n    for config in generate_one_config_from_var_dict(var_dict, fun_control):\n        pprint.pprint(config)\n        y, xai_incons = train_model_xai(config, fun_control)\n</code></pre> Source code in <code>spotpython/light/trainmodel.py</code> <pre><code>def train_model_xai(config: dict, fun_control: dict, timestamp: bool = True) -&gt; float:\n    \"\"\"\n    Trains a model using the given configuration and function control parameters. Performs feature attribution analysis and calculates consistency of these methods.\n\n    Args:\n        config (dict):\n            A dictionary containing the configuration parameters for the model.\n        fun_control (dict):\n            A dictionary containing the function control parameters.\n        timestamp (bool):\n            A boolean value indicating whether to include a timestamp in the config id. Default is True.\n            If False, the string \"_TRAIN\" is appended to the config id.\n\n    Returns:\n        float: The validation loss and the feature attribution inconsitency of the trained model.\n\n    Examples:\n        &gt;&gt;&gt; from math import inf\n            import numpy as np\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.utils.init import fun_control_init\n            from spotpython.utils.eda import print_exp_table\n            from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n            from spotpython.hyperparameters.values import assign_values, generate_one_config_from_var_dict, get_var_name\n            from spotpython.light.trainmodel import train_model\n            import pprint\n            PREFIX=\"000\"\n            data_set = Diabetes()\n            fun_control = fun_control_init(\n                PREFIX=PREFIX,\n                save_experiment=True,\n                fun_evals=inf,\n                max_time=1,\n                data_set = data_set,\n                core_model_name=\"light.regression.NNLinearRegressor\",\n                hyperdict=LightHyperDict,\n                _L_in=10,\n                _L_out=1,\n                TENSORBOARD_CLEAN=True,\n                tensorboard_log=True,\n                seed=42,)\n            print_exp_table(fun_control)\n            X = get_default_hyperparameters_as_array(fun_control)\n            # set epochs to 2^8:\n            # X[0, 1] = 8\n            # set patience to 2^10:\n            # X[0, 7] = 10\n            print(f\"X: {X}\")\n            # combine X and X to a np.array with shape (2, n_hyperparams)\n            # so that two values are returned\n            X = np.vstack((X, X))\n            var_dict = assign_values(X, get_var_name(fun_control))\n            for config in generate_one_config_from_var_dict(var_dict, fun_control):\n                pprint.pprint(config)\n                y, xai_incons = train_model_xai(config, fun_control)\n    \"\"\"\n    if fun_control[\"data_module\"] is None:\n        dm = LightDataModule(\n            dataset=fun_control[\"data_set\"],\n            data_full_train=fun_control[\"data_full_train\"],\n            data_test=fun_control[\"data_test\"],\n            data_val=fun_control[\"data_val\"],\n            batch_size=config[\"batch_size\"],\n            num_workers=fun_control[\"num_workers\"],\n            test_size=fun_control[\"test_size\"],\n            test_seed=fun_control[\"test_seed\"],\n            scaler=fun_control[\"scaler\"],\n            collate_fn_name=fun_control[\"collate_fn_name\"],\n            shuffle_train=fun_control[\"shuffle_train\"],\n            shuffle_val=fun_control[\"shuffle_val\"],\n            shuffle_test=fun_control[\"shuffle_test\"],\n            verbosity=fun_control[\"verbosity\"],\n        )\n    else:\n        dm = fun_control[\"data_module\"]\n\n    model = build_model_instance(config, fun_control)\n    # TODO: Check if this is necessary or if this is handled by the trainer\n    # dm.setup()\n    # print(f\"train_model(): Test set size: {len(dm.data_test)}\")\n    # print(f\"train_model(): Train set size: {len(dm.data_train)}\")\n    # print(f\"train_model(): Batch size: {config['batch_size']}\")\n\n    # Callbacks\n    #\n    # EarlyStopping:\n    # Stop training when a monitored quantity has stopped improving.\n    # The EarlyStopping callback runs at the end of every validation epoch by default.\n    # However, the frequency of validation can be modified by setting various parameters\n    # in the Trainer, for example check_val_every_n_epoch and val_check_interval.\n    # It must be noted that the patience parameter counts the number of validation checks\n    # with no improvement, and not the number of training epochs.\n    # Therefore, with parameters check_val_every_n_epoch=10 and patience=3,\n    # the trainer will perform at least 40 training epochs before being stopped.\n    # Args:\n    # - monitor:\n    #   Quantity to be monitored. Default: 'val_loss'.\n    # - patience:\n    #   Number of validation checks with no improvement after which training will be stopped.\n    #   In spotpython, this is a hyperparameter.\n    # - mode (str):\n    #   one of {min, max}. If save_top_k != 0, the decision to overwrite the current save file\n    #   is made based on either the maximization or the minimization of the monitored quantity.\n    #   For 'val_acc', this should be 'max', for 'val_loss' this should be 'min', etc.\n    # - strict:\n    #   Set to False.\n    # - verbose:\n    #   If True, prints a message to the logger.\n    #\n    # ModelCheckpoint:\n    # Save the model periodically by monitoring a quantity.\n    # Every metric logged with log() or log_dict() is a candidate for the monitor key.\n    # spotpython uses ModelCheckpoint if timestamp is set to False. In this case, the\n    # config_id has no timestamp and ends with the unique string \"_TRAIN\". This\n    # enables loading the model from a checkpoint, because the config_id is unique.\n    # Args:\n    # - dirpath:\n    #   Path to the directory where the checkpoints will be saved.\n    # - monitor (str):\n    #   Quantity to monitor.\n    #   By default it is None which saves a checkpoint only for the last epoch.\n    # - verbose (bool):\n    #   If True, prints a message to the logger.\n    # - save_last (Union[bool, Literal['link'], None]):\n    #   When True, saves a last.ckpt copy whenever a checkpoint file gets saved.\n    #   Can be set to 'link' on a local filesystem to create a symbolic link.\n    #   This allows accessing the latest checkpoint in a deterministic manner.\n    #   Default: None.\n    config_id = generate_config_id_with_timestamp(config=config, timestamp=timestamp)\n    callbacks = [EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)]\n    if not timestamp:\n        # add ModelCheckpoint only if timestamp is False\n        dirpath = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id)\n        callbacks.append(ModelCheckpoint(dirpath=dirpath, monitor=None, verbose=False, save_last=True))  # Save the last checkpoint\n\n    # Tensorboard logger. The tensorboard is passed to the trainer.\n    # See: https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.TensorBoardLogger.html\n    # It uses the following arguments:\n    # Args:\n    # - save_dir:\n    #   Where to save logs. Can be specified via fun_control[\"TENSORBOARD_PATH\"]\n    # - name:\n    #   Experiment name. Defaults to 'default'.\n    #   If it is the empty string then no per-experiment subdirectory is used.\n    #   Changed in spotpython 0.17.2 to the empty string.\n    # - version:\n    #   Experiment version. If version is not specified the logger inspects the save directory\n    #   for existing versions, then automatically assigns the next available version.\n    #   If it is a string then it is used as the run-specific subdirectory name,\n    #   otherwise 'version_${version}' is used. spotpython uses the config_id as version.\n    # - log_graph (bool):\n    #   Adds the computational graph to tensorboard.\n    #   This requires that the user has defined the self.example_input_array\n    #   attribute in their model. Set in spotpython to fun_control[\"log_graph\"].\n    # - default_hp_metric (bool):\n    #   Enables a placeholder metric with key hp_metric when log_hyperparams is called\n    #   without a metric (otherwise calls to log_hyperparams without a metric are ignored).\n    #   spotpython sets this to True.\n\n    # Init trainer. See: https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer\n    # Args used by spotpython (there are more):\n    # - default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n    #   Default: os.getcwd(). Can be remote file paths such as s3://mybucket/path or \u2018hdfs://path/\u2019\n    # - max_epochs: Stop training once this number of epochs is reached.\n    #   Disabled by default (None).\n    #   If both max_epochs and max_steps are not specified, defaults to max_epochs = 1000.\n    #   To enable infinite training, set max_epochs = -1.\n    # - accelerator: Supports passing different accelerator types\n    #   (\u201ccpu\u201d, \u201cgpu\u201d, \u201ctpu\u201d, \u201chpu\u201d, \u201cmps\u201d, \u201cauto\u201d) as well as custom accelerator instances.\n    # - devices: The devices to use. Can be set to a positive number (int or str),\n    #   a sequence of device indices (list or str), the value -1 to indicate all available devices\n    #   should be used, or \"auto\" for automatic selection based on the chosen accelerator.\n    #   Default: \"auto\".\n    # - strategy: Supports different training strategies with aliases as well custom strategies.\n    #   Default: \"auto\".\n    # - num_nodes: Number of GPU nodes for distributed training. Default: 1.\n    # - precision: Double precision (64, \u201864\u2019 or \u201864-true\u2019), full precision (32, \u201832\u2019 or \u201832-true\u2019),\n    #   16bit mixed precision (16, \u201816\u2019, \u201816-mixed\u2019) or bfloat16 mixed precision (\u2018bf16\u2019, \u2018bf16-mixed\u2019).\n    #   Can be used on CPU, GPU, TPUs, or HPUs. Default: '32-true'.\n    # - logger: Logger (or iterable collection of loggers) for experiment tracking.\n    #   A True value uses the default TensorBoardLogger if it is installed, otherwise CSVLogger.\n    #   False will disable logging. If multiple loggers are provided, local files (checkpoints,\n    #   profiler traces, etc.) are saved in the log_dir of the first logger. Default: True.\n    # - callbacks: List of callbacks to enable during training.Default: None.\n    # - enable_progress_bar: If True, enables the progress bar.\n    #   Whether to enable to progress bar by default. Default: True.\n    # - num_sanity_val_steps:\n    #   Sanity check runs n validation batches before starting the training routine.\n    #   Set it to -1 to run all batches in all validation dataloaders. Default: 2.\n    # - log_every_n_steps:\n    #   How often to log within steps. Default: 50.\n    # - gradient_clip_val:\n    #   The value at which to clip gradients. Passing gradient_clip_val=None\n    #   disables gradient clipping. If using Automatic Mixed Precision (AMP),\n    #   the gradients will be unscaled before. Default: None.\n    # - gradient_clip_algorithm (str):\n    #   The gradient clipping algorithm to use.\n    #   Pass gradient_clip_algorithm=\"value\" to clip by value,\n    #   and gradient_clip_algorithm=\"norm\" to clip by norm.\n    #   By default it will be set to \"norm\".\n\n    enable_progress_bar = fun_control[\"enable_progress_bar\"] or False\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=fun_control[\"accelerator\"],\n        devices=fun_control[\"devices\"],\n        strategy=fun_control[\"strategy\"],\n        num_nodes=fun_control[\"num_nodes\"],\n        precision=fun_control[\"precision\"],\n        logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True, log_graph=fun_control[\"log_graph\"], name=\"\"),\n        callbacks=callbacks,\n        enable_progress_bar=enable_progress_bar,\n        num_sanity_val_steps=fun_control[\"num_sanity_val_steps\"],\n        log_every_n_steps=fun_control[\"log_every_n_steps\"],\n        gradient_clip_val=None,\n        gradient_clip_algorithm=\"norm\",\n    )\n    # Fit the model\n    # Args:\n    # - model: Model to fit\n    # - datamodule: A LightningDataModule that defines the train_dataloader\n    #   hook. Pass the datamodule as arg to trainer.fit to override model hooks # :)\n    # - ckpt_path: Path/URL of the checkpoint from which training is resumed.\n    #   Could also be one of two special keywords \"last\" and \"hpc\".\n    #   If there is no checkpoint file at the path, an exception is raised.\n    try:\n        trainer.fit(model=model, datamodule=dm, ckpt_path=None)\n    except Exception as e:\n        print(f\"train_model(): trainer.fit failed with exception: {e}\")\n    # Test best model on validation and test set\n    verbose = fun_control[\"verbosity\"] &gt; 0\n\n    # Validate the model\n    # Perform one evaluation epoch over the validation set.\n    # Args:\n    # - model: The model to validate.\n    # - datamodule: A LightningDataModule that defines the val_dataloader hook.\n    # - verbose: If True, prints the validation results.\n    # - ckpt_path: Path to a specific checkpoint to load for validation.\n    #   Either \"best\", \"last\", \"hpc\" or path to the checkpoint you wish to validate.\n    #   If None and the model instance was passed, use the current weights.\n    #   Otherwise, the best model checkpoint from the previous trainer.fit call will\n    #   be loaded if a checkpoint callback is configured.\n    # Returns:\n    # - List of dictionaries with metrics logged during the validation phase,\n    #   e.g., in model- or callback hooks like validation_step() etc.\n    #   The length of the list corresponds to the number of validation dataloaders used.\n    result = trainer.validate(model=model, datamodule=dm, ckpt_path=None, verbose=verbose)\n\n    # unlist the result (from a list of one dict)\n    result = result[0]\n    print(f\"train_model result: {result}\")\n\n    # -------------------------------------------------------------------------------------------------------------------\n    # Perform feature attribution analysis\n    model = trainer.model\n    print(\"MODEL :\", model)\n    model.eval()\n\n    # Get the validation dataloader from the LightningDataModule\n    val_dataloader: DataLoader = dm.val_dataloader()  # Fetch validation data loader\n\n    # Collect all validation data\n    X_val_list = []\n    y_val_list = []\n\n    # Iterate over the validation dataloader to gather all data\n    for batch in val_dataloader:\n        X_batch, y_batch = batch  # Extract inputs and labels\n        X_val_list.append(X_batch)\n        y_val_list.append(y_batch)\n\n    # Concatenate all batches into single tensors\n    X_val_tensor = torch.cat(X_val_list, dim=0).to(model.device)\n\n    # Perform feature attribution analysis\n\n    # Check if at least 2 elements are in list fun_control[\"xai_methods\"]\n    if len(fun_control[\"xai_methods\"]) &lt; 2:\n        raise ValueError(\"At least two XAI methods of 'IntegratedGradients', 'KernelShap', and 'DeepLift' should be selected.\")\n\n    # Validate XAI methods\n    valid_xai_methods = {\"IntegratedGradients\", \"KernelShap\", \"DeepLift\"}\n    for method in fun_control[\"xai_methods\"]:\n        if method not in valid_xai_methods:\n            raise ValueError(f\"Invalid XAI method: {method}. Valid methods are: {valid_xai_methods}\")\n\n    # Dictionary to store attributions\n    attributions_dict = {}\n\n    if fun_control[\"xai_baseline\"] is None:\n        fun_control[\"xai_baseline\"] = torch.zeros_like(X_val_tensor)\n        print(\"Baseline is None. Using zeros as baseline.\")\n    baseline = fun_control[\"xai_baseline\"]\n\n    if \"IntegratedGradients\" in fun_control[\"xai_methods\"]:\n        attr_ig = IntegratedGradients(model)\n        attribution_ig = attr_ig.attribute(X_val_tensor, baselines=baseline)\n        ig_attr_test_sum = attribution_ig.detach().numpy().sum(0)\n        ig_attr_test_norm_sum = ig_attr_test_sum / np.linalg.norm(ig_attr_test_sum, ord=1)\n        attributions_dict[\"IntegratedGradients\"] = ig_attr_test_norm_sum\n\n    if \"KernelShap\" in fun_control[\"xai_methods\"]:\n        attr_ks = KernelShap(model)\n        attribution_ks = attr_ks.attribute(X_val_tensor, baselines=baseline)\n        ks_attr_test_sum = attribution_ks.detach().numpy().sum(0)\n        ks_attr_test_norm_sum = ks_attr_test_sum / np.linalg.norm(ks_attr_test_sum, ord=1)\n        attributions_dict[\"KernelShap\"] = ks_attr_test_norm_sum\n\n    if \"DeepLift\" in fun_control[\"xai_methods\"]:\n        attr_dl = DeepLift(model)\n        attribution_dl = attr_dl.attribute(X_val_tensor, baselines=baseline)\n        dl_attr_test_sum = attribution_dl.detach().numpy().sum(0)\n        dl_attr_test_norm_sum = dl_attr_test_sum / np.linalg.norm(dl_attr_test_sum, ord=1)\n        attributions_dict[\"DeepLift\"] = dl_attr_test_norm_sum\n\n    attributions_list = [attributions_dict[method] for method in fun_control[\"xai_methods\"]]\n    attributions = np.stack(attributions_list, axis=1)\n\n    if fun_control[\"xai_metric\"] not in {\"max_diff\", \"variance\", \"spearman\"}:\n        print(\"Invalid or missing xai_metric. Setting it to 'max_diff'.\")\n        fun_control[\"xai_metric\"] = \"max_diff\"\n\n    if fun_control[\"xai_metric\"] == \"max_diff\":\n        # Compute the max difference of the attribution methods for each feature\n        result_xai = np.max(attributions, axis=1) - np.min(attributions, axis=1)\n        print(\"Maximum differences of feature attribution methods:\", result_xai)\n        result_xai = result_xai.sum()\n\n    if fun_control[\"xai_metric\"] == \"variance\":\n        result_xai = np.var(attributions, axis=1)\n        print(\"Variance of feature attribution methods:\", result_xai)\n        result_xai = result_xai.sum()\n\n    if fun_control[\"xai_metric\"] == \"spearman\":\n        num_methods = attributions.shape[1]\n        spearman_matrix = np.zeros((num_methods, num_methods))  # Store correlation values\n\n        for i in range(num_methods):\n            for j in range(i + 1, num_methods):  # Only compute upper triangle\n                corr, _ = spearmanr(attributions[:, i], attributions[:, j])  # Compute Spearman correlation\n                spearman_matrix[i, j] = corr\n                spearman_matrix[j, i] = corr  # Mirror value in symmetric matrix\n\n        # Extract upper triangular values (excluding diagonal)\n        upper_triangle_values = spearman_matrix[np.triu_indices(num_methods, k=1)]\n\n        # Compute mean correlation as the consistency score\n        # Negative sign to use the result as loss of the objective function for minimization\n        result_xai = -np.mean(upper_triangle_values)\n\n        print(\"Spearman rank correlation matrix:\\n\", spearman_matrix)\n        print(\"Consistency Score (Mean Spearman Correlation):\", -result_xai)\n\n    # -------------------------------------------------------------------------------------------------------------------\n\n    return result[\"val_loss\"], result_xai\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/","title":"netlightbasemapk","text":""},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK","title":"<code>NetLightBaseMAPK</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS,\n                       train=True,\n                       download=True,\n                       transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data,\n                              batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                  epochs=10,\n                                  batch_size=BATCH_SIZE,\n                                  initialization='xavier',\n                                  act_fn=nn.ReLU(),\n                                  optimizer='Adam',\n                                  dropout_prob=0.1,\n                                  lr_mult=0.1,\n                                  patience=5)\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n</code></pre> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>class NetLightBaseMAPK(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a neural network model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS,\n                               train=True,\n                               download=True,\n                               transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data,\n                                      batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                          epochs=10,\n                                          batch_size=BATCH_SIZE,\n                                          initialization='xavier',\n                                          act_fn=nn.ReLU(),\n                                          optimizer='Adam',\n                                          dropout_prob=0.1,\n                                          lr_mult=0.1,\n                                          patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the NetLightBase object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 4.\n\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n        if self.hparams.l1 &lt; 4:\n            raise ValueError(\"l1 must be at least 4\")\n\n        hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n        self.train_mapk = MAPK(k=3)\n        self.valid_mapk = MAPK(k=3)\n        self.test_mapk = MAPK(k=3)\n\n        # Create the network based on the specified hidden sizes\n        layers = []\n        layer_sizes = [self._L_in] + hidden_sizes\n        layer_size_last = layer_sizes[0]\n        for layer_size in layer_sizes[1:]:\n            layers += [\n                nn.Linear(layer_size_last, layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n            layer_size_last = layer_size\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the probabilities for each class.\n\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                              epochs=10,\n                                              batch_size=BATCH_SIZE,\n                                              initialization='xavier', act_fn=nn.ReLU(),\n                                              optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                              patience=5)\n\n        \"\"\"\n        x = self.layers(x)\n        return F.softmax(x, dim=1)\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = F.cross_entropy(logits, y)\n        # self.train_mapk(logits, y)\n        # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False):\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            (NoneType): None\n\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())\n            &gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, val_loader)\n\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = F.cross_entropy(logits, y)\n        # loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n        self.valid_mapk(logits, y)\n        self.log(\"valid_mapk\", self.valid_mapk, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", loss, prog_bar=prog_bar)\n        self.log(\"val_acc\", acc, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; tuple:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            tuple: A tuple containing the loss and accuracy for this batch.\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n        self.test_mapk(logits, y)\n        self.log(\"test_mapk\", self.test_mapk, on_step=True, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", loss, prog_bar=prog_bar)\n        self.log(\"val_acc\", acc, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n        return loss, acc\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.__init__","title":"<code>__init__(l1, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, *args, **kwargs)</code>","text":"<p>Initializes the NetLightBase object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 4.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                    initialization='xavier', act_fn=nn.ReLU(),\n                                    optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                    patience=5)\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n</code></pre> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the NetLightBase object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 4.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n    if self.hparams.l1 &lt; 4:\n        raise ValueError(\"l1 must be at least 4\")\n\n    hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n    self.train_mapk = MAPK(k=3)\n    self.valid_mapk = MAPK(k=3)\n    self.test_mapk = MAPK(k=3)\n\n    # Create the network based on the specified hidden sizes\n    layers = []\n    layer_sizes = [self._L_in] + hidden_sizes\n    layer_size_last = layer_sizes[0]\n    for layer_size in layer_sizes[1:]:\n        layers += [\n            nn.Linear(layer_size_last, layer_size),\n            self.hparams.act_fn,\n            nn.Dropout(self.hparams.dropout_prob),\n        ]\n        layer_size_last = layer_size\n    layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n    # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n    self.layers = nn.Sequential(*layers)\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n    return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the probabilities for each class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                  epochs=10,\n                                  batch_size=BATCH_SIZE,\n                                  initialization='xavier', act_fn=nn.ReLU(),\n                                  optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                  patience=5)\n</code></pre> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the probabilities for each class.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                          epochs=10,\n                                          batch_size=BATCH_SIZE,\n                                          initialization='xavier', act_fn=nn.ReLU(),\n                                          optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                          patience=5)\n\n    \"\"\"\n    x = self.layers(x)\n    return F.softmax(x, dim=1)\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the loss and accuracy for this batch.</p> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; tuple:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing the loss and accuracy for this batch.\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = F.cross_entropy(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n    self.test_mapk(logits, y)\n    self.log(\"test_mapk\", self.test_mapk, on_step=True, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", loss, prog_bar=prog_bar)\n    self.log(\"val_acc\", acc, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n    return loss, acc\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                    epochs=10,\n                                    batch_size=BATCH_SIZE,\n                                    initialization='xavier', act_fn=nn.ReLU(),\n                                    optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                    patience=5)\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n</code></pre> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                            epochs=10,\n                                            batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = F.cross_entropy(logits, y)\n    # self.train_mapk(logits, y)\n    # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False)\n    return loss\n</code></pre>"},{"location":"reference/spotpython/light/classification/netlightbasemapk/#spotpython.light.classification.netlightbasemapk.NetLightBaseMAPK.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())\n&gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                    epochs=10,\n                                    batch_size=BATCH_SIZE,\n                                    initialization='xavier', act_fn=nn.ReLU(),\n                                    optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                    patience=5)\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(net_light_base, val_loader)\n</code></pre> Source code in <code>spotpython/light/classification/netlightbasemapk.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False):\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())\n        &gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                            epochs=10,\n                                            batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, val_loader)\n\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = F.cross_entropy(logits, y)\n    # loss = F.nll_loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n    self.valid_mapk(logits, y)\n    self.log(\"valid_mapk\", self.valid_mapk, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", loss, prog_bar=prog_bar)\n    self.log(\"val_acc\", acc, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n</code></pre>"},{"location":"reference/spotpython/light/cnn/googlenet/","title":"googlenet","text":""},{"location":"reference/spotpython/light/cnn/googlenet/#spotpython.light.cnn.googlenet.GoogleNet","title":"<code>GoogleNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>GoogleNet architecture</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of classes for the classification task. Defaults to 10.</p> <code>10</code> <code>act_fn_name</code> <code>str</code> <p>Name of the activation function. Defaults to \u201crelu\u201d.</p> <code>'relu'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>hparams</code> <code>SimpleNamespace</code> <p>Namespace containing the hyperparameters.</p> <code>input_net</code> <code>Sequential</code> <p>Input network.</p> <code>inception_blocks</code> <code>Sequential</code> <p>Inception blocks.</p> <code>output_net</code> <code>Sequential</code> <p>Output network.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of the GoogleNet architecture</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.cnn.googlenet import GoogleNet\n    import torch\n    import torch.nn as nn\n    model = GoogleNet()\n    x = torch.randn(1, 3, 32, 32)\n    y = model(x)\n    y.shape\n    torch.Size([1, 10])\n</code></pre> Source code in <code>spotpython/light/cnn/googlenet.py</code> <pre><code>class GoogleNet(nn.Module):\n    \"\"\"GoogleNet architecture\n\n    Args:\n        num_classes (int):\n            Number of classes for the classification task. Defaults to 10.\n        act_fn_name (str):\n            Name of the activation function. Defaults to \"relu\".\n        **kwargs (Any):\n            Additional keyword arguments.\n\n    Attributes:\n        hparams (SimpleNamespace):\n            Namespace containing the hyperparameters.\n        input_net (nn.Sequential):\n            Input network.\n        inception_blocks (nn.Sequential):\n            Inception blocks.\n        output_net (nn.Sequential):\n            Output network.\n\n    Returns:\n        (torch.Tensor):\n            Output tensor of the GoogleNet architecture\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.cnn.googlenet import GoogleNet\n            import torch\n            import torch.nn as nn\n            model = GoogleNet()\n            x = torch.randn(1, 3, 32, 32)\n            y = model(x)\n            y.shape\n            torch.Size([1, 10])\n    \"\"\"\n\n    def __init__(self, num_classes: int = 10, act_fn_name: str = \"relu\", **kwargs):\n        super().__init__()\n        # TODO: Replace this by act_fn handlers specified in the config file:\n        act_fn_by_name = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"leakyrelu\": nn.LeakyReLU, \"gelu\": nn.GELU}\n        self.hparams = SimpleNamespace(num_classes=num_classes, act_fn_name=act_fn_name, act_fn=act_fn_by_name[act_fn_name])\n        self._create_network()\n        self._init_params()\n\n    def _create_network(self):\n        # A first convolution on the original image to scale up the channel size\n        self.input_net = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), self.hparams.act_fn())\n        # Stacking inception blocks\n        self.inception_blocks = nn.Sequential(\n            InceptionBlock(\n                64,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                64,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 24, \"3x3\": 48, \"5x5\": 12, \"max\": 12},\n                act_fn=self.hparams.act_fn,\n            ),\n            nn.MaxPool2d(3, stride=2, padding=1),  # 32x32 =&gt; 16x16\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 24, \"3x3\": 48, \"5x5\": 12, \"max\": 12},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 16, \"3x3\": 48, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 16, \"3x3\": 48, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 32, \"3x3\": 48, \"5x5\": 24, \"max\": 24},\n                act_fn=self.hparams.act_fn,\n            ),\n            nn.MaxPool2d(3, stride=2, padding=1),  # 16x16 =&gt; 8x8\n            InceptionBlock(\n                128,\n                c_red={\"3x3\": 48, \"5x5\": 16},\n                c_out={\"1x1\": 32, \"3x3\": 64, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                128,\n                c_red={\"3x3\": 48, \"5x5\": 16},\n                c_out={\"1x1\": 32, \"3x3\": 64, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n        )\n        # Mapping to classification output\n        self.output_net = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Linear(128, self.hparams.num_classes))\n\n    def _init_params(self):\n        # We should initialize the\n        # convolutions according to the activation function\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, nonlinearity=self.hparams.act_fn_name)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.input_net(x)\n        x = self.inception_blocks(x)\n        x = self.output_net(x)\n        return x\n</code></pre>"},{"location":"reference/spotpython/light/cnn/inceptionblock/","title":"inceptionblock","text":""},{"location":"reference/spotpython/light/cnn/inceptionblock/#spotpython.light.cnn.inceptionblock.InceptionBlock","title":"<code>InceptionBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Inception block as used in GoogLeNet.</p> Notes <p>Description from P. Lippe:INCEPTION, RESNET AND DENSENET An Inception block applies four convolution blocks separately on the same feature map: a 1x1, 3x3, and 5x5 convolution, and a max pool operation. This allows the network to look at the same data with different receptive fields. Of course, learning only 5x5 convolution would be theoretically more powerful. However, this is not only more computation and memory heavy but also tends to overfit much easier. The 1x1 convolutions are used to reduce the number of input channels to the 3x3 and 5x5 convolutions, which reduces the number of parameters and computation.</p> <p>Parameters:</p> Name Type Description Default <code>c_in</code> <code>int</code> <p>Number of input feature maps from the previous layers</p> required <code>c_red</code> <code>dict</code> <p>Dictionary with keys \u201c3x3\u201d and \u201c5x5\u201d specifying the output of the dimensionality reducing 1x1 convolutions</p> required <code>c_out</code> <code>dict</code> <p>Dictionary with keys \u201c1x1\u201d, \u201c3x3\u201d, \u201c5x5\u201d, and \u201cmax\u201d</p> required <code>act_fn</code> <code>Module</code> <p>Activation class constructor (e.g. nn.ReLU)</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.cnn.googlenet import InceptionBlock\n    import torch\n    import torch.nn as nn\n    block = InceptionBlock(3,\n                {\"3x3\": 32, \"5x5\": 16},\n                {\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8},\n                nn.ReLU)\n    x = torch.randn(1, 3, 32, 32)\n    y = block(x)\n    y.shape\n    torch.Size([1, 64, 32, 32])\n</code></pre> Source code in <code>spotpython/light/cnn/inceptionblock.py</code> <pre><code>class InceptionBlock(nn.Module):\n    \"\"\"\n    Inception block as used in GoogLeNet.\n\n    Notes:\n        Description from\n        [P. Lippe:INCEPTION, RESNET AND DENSENET](https://lightning.ai/docs/pytorch/stable/)\n        An Inception block applies four convolution blocks separately on the same feature map:\n        a 1x1, 3x3, and 5x5 convolution, and a max pool operation.\n        This allows the network to look at the same data with different receptive fields.\n        Of course, learning only 5x5 convolution would be theoretically more powerful.\n        However, this is not only more computation and memory heavy but also tends to overfit much easier.\n        The 1x1 convolutions are used to reduce the number of input channels to the 3x3 and 5x5 convolutions,\n        which reduces the number of parameters and computation.\n\n    Args:\n        c_in (int):\n            Number of input feature maps from the previous layers\n        c_red (dict):\n            Dictionary with keys \"3x3\" and \"5x5\" specifying\n            the output of the dimensionality reducing 1x1 convolutions\n        c_out (dict):\n            Dictionary with keys \"1x1\", \"3x3\", \"5x5\", and \"max\"\n        act_fn (nn.Module):\n            Activation class constructor (e.g. nn.ReLU)\n\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.cnn.googlenet import InceptionBlock\n            import torch\n            import torch.nn as nn\n            block = InceptionBlock(3,\n                        {\"3x3\": 32, \"5x5\": 16},\n                        {\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8},\n                        nn.ReLU)\n            x = torch.randn(1, 3, 32, 32)\n            y = block(x)\n            y.shape\n            torch.Size([1, 64, 32, 32])\n\n    \"\"\"\n\n    def __init__(self, c_in, c_red: dict, c_out: dict, act_fn):\n        super().__init__()\n\n        # 1x1 convolution branch\n        self.conv_1x1 = nn.Sequential(nn.Conv2d(c_in, c_out[\"1x1\"], kernel_size=1), nn.BatchNorm2d(c_out[\"1x1\"]), act_fn())\n\n        # 3x3 convolution branch\n        self.conv_3x3 = nn.Sequential(\n            nn.Conv2d(c_in, c_red[\"3x3\"], kernel_size=1),\n            nn.BatchNorm2d(c_red[\"3x3\"]),\n            act_fn(),\n            nn.Conv2d(c_red[\"3x3\"], c_out[\"3x3\"], kernel_size=3, padding=1),\n            nn.BatchNorm2d(c_out[\"3x3\"]),\n            act_fn(),\n        )\n\n        # 5x5 convolution branch\n        self.conv_5x5 = nn.Sequential(\n            nn.Conv2d(c_in, c_red[\"5x5\"], kernel_size=1),\n            nn.BatchNorm2d(c_red[\"5x5\"]),\n            act_fn(),\n            nn.Conv2d(c_red[\"5x5\"], c_out[\"5x5\"], kernel_size=5, padding=2),\n            nn.BatchNorm2d(c_out[\"5x5\"]),\n            act_fn(),\n        )\n\n        # Max-pool branch\n        self.max_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n            nn.Conv2d(c_in, c_out[\"max\"], kernel_size=1),\n            nn.BatchNorm2d(c_out[\"max\"]),\n            act_fn(),\n        )\n\n    def forward(self, x) -&gt; torch.Tensor:\n        x_1x1 = self.conv_1x1(x)\n        x_3x3 = self.conv_3x3(x)\n        x_5x5 = self.conv_5x5(x)\n        x_max = self.max_pool(x)\n        x_out = torch.cat([x_1x1, x_3x3, x_5x5, x_max], dim=1)\n        return x_out\n</code></pre>"},{"location":"reference/spotpython/light/cnn/netcnnbase/","title":"netcnnbase","text":""},{"location":"reference/spotpython/light/cnn/netcnnbase/#spotpython.light.cnn.netcnnbase.NetCNNBase","title":"<code>NetCNNBase</code>","text":"<p>               Bases: <code>LightningModule</code></p> Source code in <code>spotpython/light/cnn/netcnnbase.py</code> <pre><code>class NetCNNBase(L.LightningModule):\n    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n        \"\"\"\n        Initializes the CNN model.\n\n        Args:\n            model_name (str): name of the model.\n            model_hparams (dict): dictionary containing the hyperparameters for the model.\n            optimizer_name (str): name of the optimizer.\n            optimizer_hparams (dict): dictionary containing the hyperparameters for the optimizer.\n\n        Returns:\n            (object): model object.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.light.cnn.netcnnbase import NetCNNBase\n                from spotpython.light.cnn.googlenet import GoogleNet\n                import torch\n                import torch.nn as nn\n                model_hparams = {\"c_in\": 3, \"c_out\": 10, \"act_fn\": nn.ReLU, \"optimizer_name\": \"Adam\"}\n                fun_control = {\"core_model\": GoogleNet}\n                model = NetCNNBase(model_hparams, fun_control)\n                x = torch.randn(1, 3, 32, 32)\n                y = model(x)\n                y.shape\n                torch.Size([1, 10])\n\n        \"\"\"\n        super().__init__()\n        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n        self.save_hyperparameters()\n        print(f\"model_hparams: {model_hparams}\")\n        print(f\"self.hparams: {self.hparams}\")\n        # Create model\n        self.model = self.create_model(model_name, model_hparams)\n        # self.model = fun_control[\"core_model\"](**model_hparams)\n        print(f\"self.model: {self.model}\")\n        # Create loss module\n        self.loss_module = nn.CrossEntropyLoss()\n        # Example input for visualizing the graph in Tensorboard\n        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n\n    def forward(self, imgs):\n        # Forward function that is run when visualizing the graph\n        return self.model(imgs)\n\n    def configure_optimizers(self):\n        if self.hparams.optimizer_name == \"Adam\":\n            optimizer = optim.AdamW(self.parameters(), **self.hparams.optimizer_hparams)\n        elif self.hparams.optimizer_name == \"SGD\":\n            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n        else:\n            assert False, f'Unknown optimizer: \"{self.hparams.optimizer_name}\"'\n\n        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n        return [optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx):\n        # \"batch\" is the output of the training data loader.\n        imgs, labels = batch\n        preds = self.model(imgs)\n        loss = self.loss_module(preds, labels)\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n\n        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n        self.log(\"train_loss\", loss)\n        return loss  # Return tensor to call \".backward\" on\n\n    def validation_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs).argmax(dim=-1)\n        acc = (labels == preds).float().mean()\n        # By default logs it per epoch (weighted average over batches)\n        self.log(\"val_acc\", acc)\n\n    def test_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs).argmax(dim=-1)\n        acc = (labels == preds).float().mean()\n        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n        self.log(\"test_acc\", acc)\n\n    def create_model(self, model_name, model_hparams):\n        print(\"create_model: Starting\")\n        print(f\"model_name: {model_name}\")\n        print(f\"model_hparams: {model_hparams}\")\n        model_dict = {\"GoogleNet\": GoogleNet}\n        if model_name in model_dict:\n            return model_dict[model_name](**model_hparams)\n        else:\n            assert False, f'Unknown model name \"{model_name}\". Available models are: {str(model_dict.keys())}'\n</code></pre>"},{"location":"reference/spotpython/light/cnn/netcnnbase/#spotpython.light.cnn.netcnnbase.NetCNNBase.__init__","title":"<code>__init__(model_name, model_hparams, optimizer_name, optimizer_hparams)</code>","text":"<p>Initializes the CNN model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>name of the model.</p> required <code>model_hparams</code> <code>dict</code> <p>dictionary containing the hyperparameters for the model.</p> required <code>optimizer_name</code> <code>str</code> <p>name of the optimizer.</p> required <code>optimizer_hparams</code> <code>dict</code> <p>dictionary containing the hyperparameters for the optimizer.</p> required <p>Returns:</p> Type Description <code>object</code> <p>model object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.cnn.netcnnbase import NetCNNBase\n    from spotpython.light.cnn.googlenet import GoogleNet\n    import torch\n    import torch.nn as nn\n    model_hparams = {\"c_in\": 3, \"c_out\": 10, \"act_fn\": nn.ReLU, \"optimizer_name\": \"Adam\"}\n    fun_control = {\"core_model\": GoogleNet}\n    model = NetCNNBase(model_hparams, fun_control)\n    x = torch.randn(1, 3, 32, 32)\n    y = model(x)\n    y.shape\n    torch.Size([1, 10])\n</code></pre> Source code in <code>spotpython/light/cnn/netcnnbase.py</code> <pre><code>def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n    \"\"\"\n    Initializes the CNN model.\n\n    Args:\n        model_name (str): name of the model.\n        model_hparams (dict): dictionary containing the hyperparameters for the model.\n        optimizer_name (str): name of the optimizer.\n        optimizer_hparams (dict): dictionary containing the hyperparameters for the optimizer.\n\n    Returns:\n        (object): model object.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.cnn.netcnnbase import NetCNNBase\n            from spotpython.light.cnn.googlenet import GoogleNet\n            import torch\n            import torch.nn as nn\n            model_hparams = {\"c_in\": 3, \"c_out\": 10, \"act_fn\": nn.ReLU, \"optimizer_name\": \"Adam\"}\n            fun_control = {\"core_model\": GoogleNet}\n            model = NetCNNBase(model_hparams, fun_control)\n            x = torch.randn(1, 3, 32, 32)\n            y = model(x)\n            y.shape\n            torch.Size([1, 10])\n\n    \"\"\"\n    super().__init__()\n    # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n    self.save_hyperparameters()\n    print(f\"model_hparams: {model_hparams}\")\n    print(f\"self.hparams: {self.hparams}\")\n    # Create model\n    self.model = self.create_model(model_name, model_hparams)\n    # self.model = fun_control[\"core_model\"](**model_hparams)\n    print(f\"self.model: {self.model}\")\n    # Create loss module\n    self.loss_module = nn.CrossEntropyLoss()\n    # Example input for visualizing the graph in Tensorboard\n    self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/","title":"netlightregression","text":""},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression","title":"<code>NetLightRegression</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a regression neural network model. This is a very simple basic class. An enhanced version of this class is available as nn_linear_regression.py in the same directory.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function. If <code>None</code>, then \u201cmean_squared_error\u201d is used.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.netlightregression import NetLightRegression\n    from torch import nn\n    import lightning as L\n    PATH_DATASETS = './data'\n    BATCH_SIZE = 8\n    dataset = Diabetes()\n    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    batch_x, batch_y = next(iter(train_loader))\n    print(batch_x.shape)\n    print(batch_y.shape)\n    net_light_base = NetLightRegression(l1=128,\n                                        epochs=10,\n                                        batch_size=BATCH_SIZE,\n                                        initialization='xavier',\n                                        act_fn=nn.ReLU(),\n                                        optimizer='Adam',\n                                        dropout_prob=0.1,\n                                        lr_mult=0.1,\n                                        patience=5,\n                                        _L_in=10,\n                                        _L_out=1)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n    trainer.fit(net_light_base, train_loader)\n    trainer.validate(net_light_base, val_loader)\n    trainer.test(net_light_base, test_loader)\n    | Name   | Type       | Params | In sizes | Out sizes\n    -------------------------------------------------------------\n    0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n    -------------------------------------------------------------\n    15.9 K    Trainable params\n    0         Non-trainable params\n    15.9 K    Total params\n    0.064     Total estimated model params size (MB)\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        Validate metric           DataLoader 0\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            hp_metric              29010.7734375\n            val_loss               29010.7734375\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        Test metric             DataLoader 0\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            hp_metric              29010.7734375\n            val_loss               29010.7734375\n    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    [{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n</code></pre> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>class NetLightRegression(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a regression neural network model.\n    This is a very simple basic class. An enhanced version of this class is available\n    as nn_linear_regression.py in the same directory.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.netlightregression import NetLightRegression\n            from torch import nn\n            import lightning as L\n            PATH_DATASETS = './data'\n            BATCH_SIZE = 8\n            dataset = Diabetes()\n            train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            batch_x, batch_y = next(iter(train_loader))\n            print(batch_x.shape)\n            print(batch_y.shape)\n            net_light_base = NetLightRegression(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier',\n                                                act_fn=nn.ReLU(),\n                                                optimizer='Adam',\n                                                dropout_prob=0.1,\n                                                lr_mult=0.1,\n                                                patience=5,\n                                                _L_in=10,\n                                                _L_out=1)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n            trainer.fit(net_light_base, train_loader)\n            trainer.validate(net_light_base, val_loader)\n            trainer.test(net_light_base, test_loader)\n            | Name   | Type       | Params | In sizes | Out sizes\n            -------------------------------------------------------------\n            0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n            -------------------------------------------------------------\n            15.9 K    Trainable params\n            0         Non-trainable params\n            15.9 K    Total params\n            0.064     Total estimated model params size (MB)\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Validate metric           DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Test metric             DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            [{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the NetLightRegression object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 4.\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n        if self.hparams.l1 &lt; 4:\n            raise ValueError(\"l1 must be at least 4\")\n\n        # TODO: Implement a hidden_sizes generator function.\n        # This function is implemented in the updadated version of this class which\n        # is available as nn_linear_regression.py in the same directory.\n        hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n        # n_low = _L_in // 4\n        # # ensure that n_high is larger than n_low\n        # n_high = max(self.hparams.l1, 2 * n_low)\n        # hidden_sizes = generate_div2_list(n_high, n_low)\n\n        # Create the network based on the specified hidden sizes\n        layers = []\n        layer_sizes = [self._L_in] + hidden_sizes\n        layer_size_last = layer_sizes[0]\n        for layer_size in layer_sizes[1:]:\n            layers += [\n                nn.Linear(layer_size_last, layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n            layer_size_last = layer_size\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the output of the model.\n\n        \"\"\"\n        x = self.layers(x)\n        return x\n\n    def _calculate_loss(self, batch):\n        \"\"\"\n        Calculate the loss for the given batch.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            mode (str, optional): The mode of the model. Defaults to \"train\".\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        loss = self.metric(y_hat, y)\n        return loss\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n        return val_loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single prediction step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the prediction for this batch.\n        \"\"\"\n        x, y = batch\n        yhat = self(x)\n        y = y.view(len(y), 1)\n        yhat = yhat.view(len(yhat), 1)\n        print(f\"Predict step x: {x}\")\n        print(f\"Predict step y: {y}\")\n        print(f\"Predict step y_hat: {yhat}\")\n        # pred_loss = F.mse_loss(y_hat, y)\n        # pred loss not registered\n        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n        # MisconfigurationException: You are trying to `self.log()`\n        # but the loop's result collection is not registered yet.\n        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n        return (x, y, yhat)\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.__init__","title":"<code>__init__(l1, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, _torchmetric, *args, **kwargs)</code>","text":"<p>Initializes the NetLightRegression object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <p>_torchmetric (str):     The metric to use for the loss function. If <code>None</code>,     then \u201cmean_squared_error\u201d is used.</p> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 4.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    _torchmetric: str,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the NetLightRegression object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n    _torchmetric (str):\n        The metric to use for the loss function. If `None`,\n        then \"mean_squared_error\" is used.\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 4.\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    if _torchmetric is None:\n        _torchmetric = \"mean_squared_error\"\n    self._torchmetric = _torchmetric\n    self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_in))\n    if self.hparams.l1 &lt; 4:\n        raise ValueError(\"l1 must be at least 4\")\n\n    # TODO: Implement a hidden_sizes generator function.\n    # This function is implemented in the updadated version of this class which\n    # is available as nn_linear_regression.py in the same directory.\n    hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n    # n_low = _L_in // 4\n    # # ensure that n_high is larger than n_low\n    # n_high = max(self.hparams.l1, 2 * n_low)\n    # hidden_sizes = generate_div2_list(n_high, n_low)\n\n    # Create the network based on the specified hidden sizes\n    layers = []\n    layer_sizes = [self._L_in] + hidden_sizes\n    layer_size_last = layer_sizes[0]\n    for layer_size in layer_sizes[1:]:\n        layers += [\n            nn.Linear(layer_size_last, layer_size),\n            self.hparams.act_fn,\n            nn.Dropout(self.hparams.dropout_prob),\n        ]\n        layer_size_last = layer_size\n    layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n    # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n    self.layers = nn.Sequential(*layers)\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n    return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the output of the model.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the output of the model.\n\n    \"\"\"\n    x = self.layers(x)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.predict_step","title":"<code>predict_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the prediction for this batch.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single prediction step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the prediction for this batch.\n    \"\"\"\n    x, y = batch\n    yhat = self(x)\n    y = y.view(len(y), 1)\n    yhat = yhat.view(len(yhat), 1)\n    print(f\"Predict step x: {x}\")\n    print(f\"Predict step y: {y}\")\n    print(f\"Predict step y_hat: {yhat}\")\n    # pred_loss = F.mse_loss(y_hat, y)\n    # pred loss not registered\n    # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n    # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n    # MisconfigurationException: You are trying to `self.log()`\n    # but the loop's result collection is not registered yet.\n    # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n    # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n    return (x, y, yhat)\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n    # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/netlightregression/#spotpython.light.regression.netlightregression.NetLightRegression.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/netlightregression.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/","title":"nn_condnet_regressor","text":""},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor","title":"<code>NNCondNetRegressor</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a conditional neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>batch_norm</code> <code>bool</code> <p>Whether to use batch normalization or not.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_L_cond</code> <code>int</code> <p>The number of neurons in the conditional hidden layer.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function. If <code>None</code>, then \u201cmean_squared_error\u201d is used.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.light.regression import NNLinearRegressor\n    from torch import nn\n    import lightning as L\n    import torch\n    from torch.utils.data import TensorDataset\n    PATH_DATASETS = './data'\n    BATCH_SIZE = 128\n    # generate data\n    num_samples = 1_000\n    input_dim = 10\n    X = torch.randn(num_samples, input_dim)  # random data for example\n    Y = torch.randn(num_samples, 1)  # random target for example\n    data_set = TensorDataset(X, Y)\n    train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    batch_x, batch_y = next(iter(train_loader))\n    print(batch_x.shape)\n    print(batch_y.shape)\n    net_light_base = NNLinearRegressor(l1=128,\n                                    batch_norm=True,\n                                        epochs=10,\n                                        batch_size=BATCH_SIZE,\n                                        initialization='xavier',\n                                        act_fn=nn.ReLU(),\n                                        optimizer='Adam',\n                                        dropout_prob=0.1,\n                                        lr_mult=0.1,\n                                        patience=5,\n                                        _L_in=input_dim,\n                                        _L_out=1,\n                                        _torchmetric=\"mean_squared_error\",)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n    trainer.fit(net_light_base, train_loader)\n    # validation and test should give the same result, because the data is the same\n    trainer.validate(net_light_base, val_loader)\n    trainer.test(net_light_base, test_loader)\n        GPU available: True (mps), used: True\n        TPU available: False, using: 0 TPU cores\n        HPU available: False, using: 0 HPUs\n</code></pre> <pre><code>    | Name   | Type       | Params | Mode  | In sizes  | Out sizes\n    ----------------------------------------------------------------------\n    0 | layers | Sequential | 20.8 K | train | [128, 10] | [128, 1]\n    ----------------------------------------------------------------------\n    20.8 K    Trainable params\n    0         Non-trainable params\n    20.8 K    Total params\n    0.083     Total estimated model params size (MB)\n    69        Modules in train mode\n    0         Modules in eval mode\n    torch.Size([128, 10])\n    torch.Size([128, 1])\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503      Validate metric      \u2503       DataLoader 0        \u2503\n    \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n    \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n    \u2502         val_loss          \u2502     81.1978988647461      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503        Test metric        \u2503       DataLoader 0        \u2503\n    \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n    \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n    \u2502         val_loss          \u2502     81.1978988647461      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    [{'val_loss': 81.1978988647461, 'hp_metric': 81.1978988647461}]\n</code></pre> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>class NNCondNetRegressor(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a conditional neural network model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        batch_norm (bool):\n            Whether to use batch normalization or not.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _L_cond (int):\n            The number of neurons in the conditional hidden layer.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.light.regression import NNLinearRegressor\n            from torch import nn\n            import lightning as L\n            import torch\n            from torch.utils.data import TensorDataset\n            PATH_DATASETS = './data'\n            BATCH_SIZE = 128\n            # generate data\n            num_samples = 1_000\n            input_dim = 10\n            X = torch.randn(num_samples, input_dim)  # random data for example\n            Y = torch.randn(num_samples, 1)  # random target for example\n            data_set = TensorDataset(X, Y)\n            train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            batch_x, batch_y = next(iter(train_loader))\n            print(batch_x.shape)\n            print(batch_y.shape)\n            net_light_base = NNLinearRegressor(l1=128,\n                                            batch_norm=True,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier',\n                                                act_fn=nn.ReLU(),\n                                                optimizer='Adam',\n                                                dropout_prob=0.1,\n                                                lr_mult=0.1,\n                                                patience=5,\n                                                _L_in=input_dim,\n                                                _L_out=1,\n                                                _torchmetric=\"mean_squared_error\",)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n            trainer.fit(net_light_base, train_loader)\n            # validation and test should give the same result, because the data is the same\n            trainer.validate(net_light_base, val_loader)\n            trainer.test(net_light_base, test_loader)\n                GPU available: True (mps), used: True\n                TPU available: False, using: 0 TPU cores\n                HPU available: False, using: 0 HPUs\n\n                | Name   | Type       | Params | Mode  | In sizes  | Out sizes\n                ----------------------------------------------------------------------\n                0 | layers | Sequential | 20.8 K | train | [128, 10] | [128, 1]\n                ----------------------------------------------------------------------\n                20.8 K    Trainable params\n                0         Non-trainable params\n                20.8 K    Total params\n                0.083     Total estimated model params size (MB)\n                69        Modules in train mode\n                0         Modules in eval mode\n                torch.Size([128, 10])\n                torch.Size([128, 1])\n                \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n                \u2503      Validate metric      \u2503       DataLoader 0        \u2503\n                \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n                \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n                \u2502         val_loss          \u2502     81.1978988647461      \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n                \u2503        Test metric        \u2503       DataLoader 0        \u2503\n                \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n                \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n                \u2502         val_loss          \u2502     81.1978988647461      \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                [{'val_loss': 81.1978988647461, 'hp_metric': 81.1978988647461}]\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        batch_norm: bool,\n        _L_cond: int,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the NNLinearRegressor object.\n\n        Args:\n            l1 (int):\n                The number of neurons in the first hidden layer.\n            epochs (int):\n                The number of epochs to train the model for.\n            batch_size (int):\n                The batch size to use during training.\n            initialization (str):\n                The initialization method to use for the weights.\n            act_fn (nn.Module):\n                The activation function to use in the hidden layers.\n            optimizer (str):\n                The optimizer to use during training.\n            dropout_prob (float):\n                The probability of dropping out a neuron during training.\n            lr_mult (float):\n                The learning rate multiplier for the optimizer.\n            patience (int):\n                The number of epochs to wait before early stopping.\n            batch_norm (bool):\n                Whether to use batch normalization or not.\n            _L_cond (int):\n                The number of neurons in the conditional hidden layer.\n            _L_in (int):\n                The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int):\n                The number of output classes. Not a hyperparameter, but needed to create the network.\n            _torchmetric (str):\n                The metric to use for the loss function. If `None`,\n                then \"mean_squared_error\" is used.\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 4.\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_cond = _L_cond\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_cond, _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_cond\", \"_L_in\", \"_L_out\", \"_torchmetric\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_cond + self._L_in))\n        if self.hparams.l1 &lt; 4:\n            raise ValueError(\"l1 must be at least 4\")\n        hidden_sizes = self._get_hidden_sizes()\n\n        # Conditional Layer\n        self.cond_layer = ConditionalLayer(self._L_in, self._L_cond, self.hparams.l1)\n\n        if batch_norm:\n            # Add batch normalization layers\n            layers = []\n            layer_sizes = [self.hparams.l1] + hidden_sizes\n            for i in range(len(layer_sizes) - 1):\n                current_layer_size = layer_sizes[i]\n                next_layer_size = layer_sizes[i + 1]\n                layers += [\n                    nn.Linear(current_layer_size, next_layer_size),\n                    nn.BatchNorm1d(next_layer_size),\n                    self.hparams.act_fn,\n                    nn.Dropout(self.hparams.dropout_prob),\n                ]\n            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        else:\n            layers = []\n            layer_sizes = [self.hparams.l1] + hidden_sizes\n            for i in range(len(layer_sizes) - 1):\n                current_layer_size = layer_sizes[i]\n                next_layer_size = layer_sizes[i + 1]\n                layers += [\n                    nn.Linear(current_layer_size, next_layer_size),\n                    self.hparams.act_fn,\n                    nn.Dropout(self.hparams.dropout_prob),\n                ]\n            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n\n        # Wrap the layers into a sequential container\n        self.layers = nn.Sequential(*layers)\n\n        # Initialization (Xavier, Kaiming, or Default)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            if self.hparams.initialization == \"xavier_uniform\":\n                nn.init.xavier_uniform_(module.weight)\n            elif self.hparams.initialization == \"xavier_normal\":\n                nn.init.xavier_normal_(module.weight)\n            elif self.hparams.initialization == \"kaiming_uniform\":\n                nn.init.kaiming_uniform_(module.weight)\n            elif self.hparams.initialization == \"kaiming_normal\":\n                nn.init.kaiming_normal_(module.weight)\n            else:  # \"Default\"\n                nn.init.uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def _generate_div2_list(self, n, n_min) -&gt; list:\n        result = []\n        current = n\n        repeats = 1\n        max_repeats = 4\n        while current &gt;= n_min:\n            result.extend([current] * min(repeats, max_repeats))\n            current = current // 2\n            repeats = repeats + 1\n        return result\n\n    def _get_hidden_sizes(self):\n        n_low = self._L_in // 4\n        n_high = max(self.hparams.l1, 2 * n_low)\n        hidden_sizes = self._generate_div2_list(n_high, n_low)\n        return hidden_sizes\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the output of the model.\n\n        \"\"\"\n        # exxtract the condition from the input\n        condition = x[:, : self._L_cond]\n        x_1 = x[:, self._L_cond :]\n        x_1 = self.cond_layer(x_1, condition)\n        x_1 = self.layers(x_1)\n        return x_1\n\n    def _calculate_loss(self, batch):\n        \"\"\"\n        Calculate the loss for the given batch.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            mode (str, optional): The mode of the model. Defaults to \"train\".\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        loss = self.metric(y_hat, y)\n        return loss\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n        return val_loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single prediction step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the prediction for this batch.\n        \"\"\"\n        x, y = batch\n        yhat = self(x)\n        y = y.view(len(y), 1)\n        yhat = yhat.view(len(yhat), 1)\n        print(f\"Predict step x: {x}\")\n        print(f\"Predict step y: {y}\")\n        print(f\"Predict step y_hat: {yhat}\")\n        # pred_loss = F.mse_loss(y_hat, y)\n        # pred loss not registered\n        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n        # MisconfigurationException: You are trying to `self.log()`\n        # but the loop's result collection is not registered yet.\n        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n        return (x, y, yhat)\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n        num_milestones = 3  # Number of milestones to divide the epochs\n        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n        lr_scheduler_config = {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.__init__","title":"<code>__init__(l1, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, batch_norm, _L_cond, _L_in, _L_out, _torchmetric, *args, **kwargs)</code>","text":"<p>Initializes the NNLinearRegressor object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>batch_norm</code> <code>bool</code> <p>Whether to use batch normalization or not.</p> required <code>_L_cond</code> <code>int</code> <p>The number of neurons in the conditional hidden layer.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function. If <code>None</code>, then \u201cmean_squared_error\u201d is used.</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 4.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    batch_norm: bool,\n    _L_cond: int,\n    _L_in: int,\n    _L_out: int,\n    _torchmetric: str,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the NNLinearRegressor object.\n\n    Args:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        batch_norm (bool):\n            Whether to use batch normalization or not.\n        _L_cond (int):\n            The number of neurons in the conditional hidden layer.\n        _L_in (int):\n            The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int):\n            The number of output classes. Not a hyperparameter, but needed to create the network.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 4.\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_cond = _L_cond\n    self._L_in = _L_in\n    self._L_out = _L_out\n    if _torchmetric is None:\n        _torchmetric = \"mean_squared_error\"\n    self._torchmetric = _torchmetric\n    self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n    # _L_cond, _L_in and _L_out are not hyperparameters, but are needed to create the network\n    # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n    self.save_hyperparameters(ignore=[\"_L_cond\", \"_L_in\", \"_L_out\", \"_torchmetric\"])\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_cond + self._L_in))\n    if self.hparams.l1 &lt; 4:\n        raise ValueError(\"l1 must be at least 4\")\n    hidden_sizes = self._get_hidden_sizes()\n\n    # Conditional Layer\n    self.cond_layer = ConditionalLayer(self._L_in, self._L_cond, self.hparams.l1)\n\n    if batch_norm:\n        # Add batch normalization layers\n        layers = []\n        layer_sizes = [self.hparams.l1] + hidden_sizes\n        for i in range(len(layer_sizes) - 1):\n            current_layer_size = layer_sizes[i]\n            next_layer_size = layer_sizes[i + 1]\n            layers += [\n                nn.Linear(current_layer_size, next_layer_size),\n                nn.BatchNorm1d(next_layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n    else:\n        layers = []\n        layer_sizes = [self.hparams.l1] + hidden_sizes\n        for i in range(len(layer_sizes) - 1):\n            current_layer_size = layer_sizes[i]\n            next_layer_size = layer_sizes[i + 1]\n            layers += [\n                nn.Linear(current_layer_size, next_layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n\n    # Wrap the layers into a sequential container\n    self.layers = nn.Sequential(*layers)\n\n    # Initialization (Xavier, Kaiming, or Default)\n    self.apply(self._init_weights)\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n    num_milestones = 3  # Number of milestones to divide the epochs\n    milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n    lr_scheduler_config = {\n        \"scheduler\": scheduler,\n        \"interval\": \"epoch\",\n        \"frequency\": 1,\n    }\n\n    return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the output of the model.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the output of the model.\n\n    \"\"\"\n    # exxtract the condition from the input\n    condition = x[:, : self._L_cond]\n    x_1 = x[:, self._L_cond :]\n    x_1 = self.cond_layer(x_1, condition)\n    x_1 = self.layers(x_1)\n    return x_1\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.predict_step","title":"<code>predict_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the prediction for this batch.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single prediction step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the prediction for this batch.\n    \"\"\"\n    x, y = batch\n    yhat = self(x)\n    y = y.view(len(y), 1)\n    yhat = yhat.view(len(yhat), 1)\n    print(f\"Predict step x: {x}\")\n    print(f\"Predict step y: {y}\")\n    print(f\"Predict step y_hat: {yhat}\")\n    # pred_loss = F.mse_loss(y_hat, y)\n    # pred loss not registered\n    # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n    # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n    # MisconfigurationException: You are trying to `self.log()`\n    # but the loop's result collection is not registered yet.\n    # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n    # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n    return (x, y, yhat)\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n    # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_condnet_regressor/#spotpython.light.regression.nn_condnet_regressor.NNCondNetRegressor.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_condnet_regressor.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_funnel_regressor/","title":"nn_funnel_regressor","text":""},{"location":"reference/spotpython/light/regression/nn_funnel_regressor/#spotpython.light.regression.nn_funnel_regressor.NNFunnelRegressor","title":"<code>NNFunnelRegressor</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a regression neural network model. This is a funnel shape neural network with varying number of layers and neurons per layer. An enhanced version of this class is available as nn_linear_regression.py in the same directory.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>num_layers</code> <code>int</code> <p>The number of hidden layers in the model.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function. If <code>None</code>, then \u201cmean_squared_error\u201d is used.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> Source code in <code>spotpython/light/regression/nn_funnel_regressor.py</code> <pre><code>class NNFunnelRegressor(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a regression neural network model.\n    This is a funnel shape neural network with varying number of layers and neurons per layer. An enhanced version of this class is available\n    as nn_linear_regression.py in the same directory.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        num_layers (int):\n            The number of hidden layers in the model.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n        layers (nn.Sequential):\n            The neural network model.\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        num_layers: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the NetLightRegression object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            num_layers (int): The number of hidden layers in the model.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 8.\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n        if self.hparams.l1 &lt; 8:\n            raise ValueError(\"l1 must be at least 8\")\n\n        layers = []\n        in_features = self._L_in\n        hidden_size = self.hparams.l1\n        output_dim = self._L_out\n\n        for i in range(self.hparams.num_layers):\n            out_features = max(hidden_size // 2, 8)  # Enforce minimum of 8 units\n\n            layers.append(nn.Linear(in_features, hidden_size))\n\n            if self.hparams.batch_norm:\n                layers.append(nn.BatchNorm1d(hidden_size))  # Add BatchNorm if enabled\n\n            layers.append(self.hparams.act_fn)\n            layers.append(nn.Dropout(self.hparams.dropout_prob))\n\n            in_features = hidden_size\n            hidden_size = out_features\n\n        layers.append(nn.Linear(in_features, output_dim))\n\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the output of the model.\n\n        \"\"\"\n        x = self.layers(x)\n        return x\n\n    def _calculate_loss(self, batch):\n        \"\"\"\n        Calculate the loss for the given batch.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            mode (str, optional): The mode of the model. Defaults to \"train\".\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        loss = self.metric(y_hat, y)\n        return loss\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n        return val_loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single prediction step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the prediction for this batch.\n        \"\"\"\n        x, y = batch\n        yhat = self(x)\n        y = y.view(len(y), 1)\n        yhat = yhat.view(len(yhat), 1)\n        print(f\"Predict step x: {x}\")\n        print(f\"Predict step y: {y}\")\n        print(f\"Predict step y_hat: {yhat}\")\n        # pred_loss = F.mse_loss(y_hat, y)\n        # pred loss not registered\n        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n        # MisconfigurationException: You are trying to `self.log()`\n        # but the loop's result collection is not registered yet.\n        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n        return (x, y, yhat)\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n        # If the lr_sched hyperparameter is set to True, we will use a learning rate scheduler.\n        if self.hparams.lr_sched:\n            num_milestones = 3  # Number of milestones to divide the epochs\n            milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n            scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n            lr_scheduler_config = {\n                \"scheduler\": scheduler,\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            }\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": lr_scheduler_config,\n            }\n        #  If the lr_sched hyperparameter is not set to True, we return the optimizer only.\n        else:\n            return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_funnel_regressor/#spotpython.light.regression.nn_funnel_regressor.NNFunnelRegressor.__init__","title":"<code>__init__(l1, num_layers, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, _torchmetric, *args, **kwargs)</code>","text":"<p>Initializes the NetLightRegression object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>num_layers</code> <code>int</code> <p>The number of hidden layers in the model.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <p>_torchmetric (str):     The metric to use for the loss function. If <code>None</code>,     then \u201cmean_squared_error\u201d is used.</p> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 8.</p> Source code in <code>spotpython/light/regression/nn_funnel_regressor.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    num_layers: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    _torchmetric: str,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the NetLightRegression object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        num_layers (int): The number of hidden layers in the model.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n    _torchmetric (str):\n        The metric to use for the loss function. If `None`,\n        then \"mean_squared_error\" is used.\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 8.\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    if _torchmetric is None:\n        _torchmetric = \"mean_squared_error\"\n    self._torchmetric = _torchmetric\n    self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_in))\n    if self.hparams.l1 &lt; 8:\n        raise ValueError(\"l1 must be at least 8\")\n\n    layers = []\n    in_features = self._L_in\n    hidden_size = self.hparams.l1\n    output_dim = self._L_out\n\n    for i in range(self.hparams.num_layers):\n        out_features = max(hidden_size // 2, 8)  # Enforce minimum of 8 units\n\n        layers.append(nn.Linear(in_features, hidden_size))\n\n        if self.hparams.batch_norm:\n            layers.append(nn.BatchNorm1d(hidden_size))  # Add BatchNorm if enabled\n\n        layers.append(self.hparams.act_fn)\n        layers.append(nn.Dropout(self.hparams.dropout_prob))\n\n        in_features = hidden_size\n        hidden_size = out_features\n\n    layers.append(nn.Linear(in_features, output_dim))\n\n    self.layers = nn.Sequential(*layers)\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_funnel_regressor/#spotpython.light.regression.nn_funnel_regressor.NNFunnelRegressor.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/nn_funnel_regressor.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n    # If the lr_sched hyperparameter is set to True, we will use a learning rate scheduler.\n    if self.hparams.lr_sched:\n        num_milestones = 3  # Number of milestones to divide the epochs\n        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n        lr_scheduler_config = {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": lr_scheduler_config,\n        }\n    #  If the lr_sched hyperparameter is not set to True, we return the optimizer only.\n    else:\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_funnel_regressor/#spotpython.light.regression.nn_funnel_regressor.NNFunnelRegressor.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the output of the model.</p> Source code in <code>spotpython/light/regression/nn_funnel_regressor.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the output of the model.\n\n    \"\"\"\n    x = self.layers(x)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_funnel_regressor/#spotpython.light.regression.nn_funnel_regressor.NNFunnelRegressor.predict_step","title":"<code>predict_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the prediction for this batch.</p> Source code in <code>spotpython/light/regression/nn_funnel_regressor.py</code> <pre><code>def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single prediction step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the prediction for this batch.\n    \"\"\"\n    x, y = batch\n    yhat = self(x)\n    y = y.view(len(y), 1)\n    yhat = yhat.view(len(yhat), 1)\n    print(f\"Predict step x: {x}\")\n    print(f\"Predict step y: {y}\")\n    print(f\"Predict step y_hat: {yhat}\")\n    # pred_loss = F.mse_loss(y_hat, y)\n    # pred loss not registered\n    # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n    # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n    # MisconfigurationException: You are trying to `self.log()`\n    # but the loop's result collection is not registered yet.\n    # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n    # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n    return (x, y, yhat)\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_funnel_regressor/#spotpython.light.regression.nn_funnel_regressor.NNFunnelRegressor.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_funnel_regressor.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_funnel_regressor/#spotpython.light.regression.nn_funnel_regressor.NNFunnelRegressor.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_funnel_regressor.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n    # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_funnel_regressor/#spotpython.light.regression.nn_funnel_regressor.NNFunnelRegressor.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_funnel_regressor.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/","title":"nn_linear_regressor","text":""},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor","title":"<code>NNLinearRegressor</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a regression neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>batch_norm</code> <code>bool</code> <p>Whether to use batch normalization or not.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function. If <code>None</code>, then \u201cmean_squared_error\u201d is used.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.light.regression import NNLinearRegressor\n    from torch import nn\n    import lightning as L\n    import torch\n    from torch.utils.data import TensorDataset\n    torch.manual_seed(0)\n    PATH_DATASETS = './data'\n    BATCH_SIZE = 128\n    # generate data\n    num_samples = 1_000\n    input_dim = 10\n    X = torch.randn(num_samples, input_dim)  # random data for example\n    Y = torch.randn(num_samples, 1)  # random target for example\n    data_set = TensorDataset(X, Y)\n    train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n    batch_x, batch_y = next(iter(train_loader))\n    print(batch_x.shape)\n    print(batch_y.shape)\n    net_light_base = NNLinearRegressor(l1=128,\n                                    batch_norm=True,\n                                        epochs=10,\n                                        batch_size=BATCH_SIZE,\n                                        initialization='xavier',\n                                        act_fn=nn.ReLU(),\n                                        optimizer='Adam',\n                                        dropout_prob=0.1,\n                                        lr_mult=0.1,\n                                        patience=5,\n                                        _L_in=input_dim,\n                                        _L_out=1,\n                                        _torchmetric=\"mean_squared_error\",)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n    trainer.fit(net_light_base, train_loader)\n    # validation and test should give the same result, because the data is the same\n    trainer.validate(net_light_base, val_loader)\n    trainer.test(net_light_base, test_loader)\n        GPU available: True (mps), used: True\n        TPU available: False, using: 0 TPU cores\n        HPU available: False, using: 0 HPUs\n</code></pre> <pre><code>    | Name   | Type       | Params | Mode  | In sizes  | Out sizes\n    ----------------------------------------------------------------------\n    0 | layers | Sequential | 20.8 K | train | [128, 10] | [128, 1]\n    ----------------------------------------------------------------------\n    20.8 K    Trainable params\n    0         Non-trainable params\n    20.8 K    Total params\n    0.083     Total estimated model params size (MB)\n    69        Modules in train mode\n    0         Modules in eval mode\n    torch.Size([128, 10])\n    torch.Size([128, 1])\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503      Validate metric      \u2503       DataLoader 0        \u2503\n    \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n    \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n    \u2502         val_loss          \u2502     81.1978988647461      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503        Test metric        \u2503       DataLoader 0        \u2503\n    \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n    \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n    \u2502         val_loss          \u2502     81.1978988647461      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    [{'val_loss': 81.1978988647461, 'hp_metric': 81.1978988647461}]\n</code></pre> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>class NNLinearRegressor(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a regression neural network model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        batch_norm (bool):\n            Whether to use batch normalization or not.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _torchmetric (str):\n            The metric to use for the loss function. If `None`,\n            then \"mean_squared_error\" is used.\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.light.regression import NNLinearRegressor\n            from torch import nn\n            import lightning as L\n            import torch\n            from torch.utils.data import TensorDataset\n            torch.manual_seed(0)\n            PATH_DATASETS = './data'\n            BATCH_SIZE = 128\n            # generate data\n            num_samples = 1_000\n            input_dim = 10\n            X = torch.randn(num_samples, input_dim)  # random data for example\n            Y = torch.randn(num_samples, 1)  # random target for example\n            data_set = TensorDataset(X, Y)\n            train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n            batch_x, batch_y = next(iter(train_loader))\n            print(batch_x.shape)\n            print(batch_y.shape)\n            net_light_base = NNLinearRegressor(l1=128,\n                                            batch_norm=True,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier',\n                                                act_fn=nn.ReLU(),\n                                                optimizer='Adam',\n                                                dropout_prob=0.1,\n                                                lr_mult=0.1,\n                                                patience=5,\n                                                _L_in=input_dim,\n                                                _L_out=1,\n                                                _torchmetric=\"mean_squared_error\",)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n            trainer.fit(net_light_base, train_loader)\n            # validation and test should give the same result, because the data is the same\n            trainer.validate(net_light_base, val_loader)\n            trainer.test(net_light_base, test_loader)\n                GPU available: True (mps), used: True\n                TPU available: False, using: 0 TPU cores\n                HPU available: False, using: 0 HPUs\n\n                | Name   | Type       | Params | Mode  | In sizes  | Out sizes\n                ----------------------------------------------------------------------\n                0 | layers | Sequential | 20.8 K | train | [128, 10] | [128, 1]\n                ----------------------------------------------------------------------\n                20.8 K    Trainable params\n                0         Non-trainable params\n                20.8 K    Total params\n                0.083     Total estimated model params size (MB)\n                69        Modules in train mode\n                0         Modules in eval mode\n                torch.Size([128, 10])\n                torch.Size([128, 1])\n                \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n                \u2503      Validate metric      \u2503       DataLoader 0        \u2503\n                \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n                \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n                \u2502         val_loss          \u2502     81.1978988647461      \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n                \u2503        Test metric        \u2503       DataLoader 0        \u2503\n                \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n                \u2502         hp_metric         \u2502     81.1978988647461      \u2502\n                \u2502         val_loss          \u2502     81.1978988647461      \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                [{'val_loss': 81.1978988647461, 'hp_metric': 81.1978988647461}]\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        batch_norm: bool,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n        hidden_sizes = get_three_layers(self._L_in, self.hparams.l1)\n\n        if batch_norm:\n            # Add batch normalization layers\n            layers = []\n            layer_sizes = [self._L_in] + hidden_sizes\n            for i in range(len(layer_sizes) - 1):\n                current_layer_size = layer_sizes[i]\n                next_layer_size = layer_sizes[i + 1]\n                layers += [\n                    nn.Linear(current_layer_size, next_layer_size),\n                    nn.BatchNorm1d(next_layer_size),\n                    self.hparams.act_fn,\n                    nn.Dropout(self.hparams.dropout_prob),\n                ]\n            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        else:\n            layers = []\n            layer_sizes = [self._L_in] + hidden_sizes\n            for i in range(len(layer_sizes) - 1):\n                current_layer_size = layer_sizes[i]\n                next_layer_size = layer_sizes[i + 1]\n                layers += [\n                    nn.Linear(current_layer_size, next_layer_size),\n                    self.hparams.act_fn,\n                    nn.Dropout(self.hparams.dropout_prob),\n                ]\n            layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n\n        # Wrap the layers into a sequential container\n        self.layers = nn.Sequential(*layers)\n\n        # Initialization (Xavier, Kaiming, or Default)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            if self.hparams.initialization == \"xavier_uniform\":\n                nn.init.xavier_uniform_(module.weight)\n            elif self.hparams.initialization == \"xavier_normal\":\n                nn.init.xavier_normal_(module.weight)\n            elif self.hparams.initialization == \"kaiming_uniform\":\n                nn.init.kaiming_uniform_(module.weight)\n            elif self.hparams.initialization == \"kaiming_normal\":\n                nn.init.kaiming_normal_(module.weight)\n            else:  # \"Default\"\n                nn.init.uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the output of the model.\n\n        \"\"\"\n        x = self.layers(x)\n        return x\n\n    def _calculate_loss(self, batch):\n        \"\"\"\n        Calculate the loss for the given batch.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            mode (str, optional): The mode of the model. Defaults to \"train\".\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        loss = self.metric(y_hat, y)\n        return loss\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        loss = self._calculate_loss(batch)\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        return loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        return loss\n\n    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single prediction step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the prediction for this batch.\n        \"\"\"\n        x, y = batch\n        yhat = self(x)\n        y = y.view(len(y), 1)\n        yhat = yhat.view(len(yhat), 1)\n        print(f\"Predict step x: {x}\")\n        print(f\"Predict step y: {y}\")\n        print(f\"Predict step y_hat: {yhat}\")\n        # pred_loss = F.mse_loss(y_hat, y)\n        # pred loss not registered\n        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n        # MisconfigurationException: You are trying to `self.log()`\n        # but the loop's result collection is not registered yet.\n        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n        return (x, y, yhat)\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n        num_milestones = 3  # Number of milestones to divide the epochs\n        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n        lr_scheduler_config = {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n    num_milestones = 3  # Number of milestones to divide the epochs\n    milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n    lr_scheduler_config = {\n        \"scheduler\": scheduler,\n        \"interval\": \"epoch\",\n        \"frequency\": 1,\n    }\n\n    return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the output of the model.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the output of the model.\n\n    \"\"\"\n    x = self.layers(x)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.predict_step","title":"<code>predict_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the prediction for this batch.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single prediction step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the prediction for this batch.\n    \"\"\"\n    x, y = batch\n    yhat = self(x)\n    y = y.view(len(y), 1)\n    yhat = yhat.view(len(yhat), 1)\n    print(f\"Predict step x: {x}\")\n    print(f\"Predict step y: {y}\")\n    print(f\"Predict step y_hat: {yhat}\")\n    # pred_loss = F.mse_loss(y_hat, y)\n    # pred loss not registered\n    # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n    # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n    # MisconfigurationException: You are trying to `self.log()`\n    # but the loop's result collection is not registered yet.\n    # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n    # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n    return (x, y, yhat)\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    return loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    loss = self._calculate_loss(batch)\n    self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n    return loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_linear_regressor/#spotpython.light.regression.nn_linear_regressor.NNLinearRegressor.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/nn_linear_regressor.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    return loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_gru_regressor/","title":"nn_many_to_many_gru_regressor","text":""},{"location":"reference/spotpython/light/regression/nn_many_to_many_gru_regressor/#spotpython.light.regression.nn_many_to_many_gru_regressor.ManyToManyGRU","title":"<code>ManyToManyGRU</code>","text":"<p>               Bases: <code>Module</code></p> <p>A Many-to-Many GRU model for sequence-to-sequence regression tasks.</p> <p>This model uses a GRU layer followed by a fully connected layer and an output layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>The number of input features.</p> required <code>output_size</code> <code>int</code> <p>The number of output features. Defaults to 1.</p> <code>1</code> <code>gru_units</code> <code>int</code> <p>The number of units in the GRU layer. Defaults to 128.</p> <code>128</code> <code>fc_units</code> <code>int</code> <p>The number of units in the fully connected layer. Defaults to 128.</p> <code>128</code> <code>activation_fct</code> <code>Module</code> <p>The activation function to use after the fully connected layer. Defaults to nn.ReLU().</p> <code>ReLU()</code> <code>dropout</code> <code>float</code> <p>The dropout probability. Defaults to 0.2.</p> <code>0.2</code> <code>bidirectional</code> <code>bool</code> <p>Whether the GRU is bidirectional. Defaults to True.</p> <code>True</code> <code>num_layers</code> <code>int</code> <p>The number of GRU layers. Defaults to 2.</p> <code>2</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.regression.nn_many_to_many_gru_regressor import ManyToManyGRU\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; model = ManyToManyGRU(input_size=10, output_size=1)\n&gt;&gt;&gt; x = torch.randn(16, 10, 10)  # Batch of 16 sequences, each of length 10 with 10 features\n&gt;&gt;&gt; lengths = torch.tensor([10] * 16)  # All sequences have length 10\n&gt;&gt;&gt; output = model(x, lengths)\n&gt;&gt;&gt; print(output.shape)  # Output shape: (16, 10, 1)\n</code></pre> Source code in <code>spotpython/light/regression/nn_many_to_many_gru_regressor.py</code> <pre><code>class ManyToManyGRU(nn.Module):\n    \"\"\"A Many-to-Many GRU model for sequence-to-sequence regression tasks.\n\n    This model uses a GRU layer followed by a fully connected layer and an output layer.\n\n    Args:\n        input_size (int): The number of input features.\n        output_size (int): The number of output features. Defaults to 1.\n        gru_units (int): The number of units in the GRU layer. Defaults to 128.\n        fc_units (int): The number of units in the fully connected layer. Defaults to 128.\n        activation_fct (nn.Module): The activation function to use after the fully connected layer. Defaults to nn.ReLU().\n        dropout (float): The dropout probability. Defaults to 0.2.\n        bidirectional (bool): Whether the GRU is bidirectional. Defaults to True.\n        num_layers (int): The number of GRU layers. Defaults to 2.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.regression.nn_many_to_many_gru_regressor import ManyToManyGRU\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; model = ManyToManyGRU(input_size=10, output_size=1)\n        &gt;&gt;&gt; x = torch.randn(16, 10, 10)  # Batch of 16 sequences, each of length 10 with 10 features\n        &gt;&gt;&gt; lengths = torch.tensor([10] * 16)  # All sequences have length 10\n        &gt;&gt;&gt; output = model(x, lengths)\n        &gt;&gt;&gt; print(output.shape)  # Output shape: (16, 10, 1)\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size,\n        output_size=1,\n        gru_units=128,\n        fc_units=128,\n        activation_fct=nn.ReLU(),\n        dropout=0.2,\n        bidirectional=True,\n        num_layers=2,\n    ):\n        super(ManyToManyGRU, self).__init__()\n        self.gru_layer = nn.GRU(\n            input_size=input_size,\n            hidden_size=gru_units,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=bidirectional,\n            dropout=dropout if num_layers &gt; 1 else 0.0,\n        )\n        if bidirectional:\n            gru_units = gru_units * 2\n        self.fc = nn.Linear(gru_units, fc_units)\n        self.dropout = nn.Dropout(dropout)\n        self.output_layer = nn.Linear(fc_units, output_size)\n        self.activation_fct = activation_fct\n\n    def forward(self, x, lengths) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the ManyToManyGRU model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n            lengths (torch.Tensor): Tensor containing the lengths of each sequence in the batch.\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).\n\n        Raises:\n            ValueError: If the input tensor is empty or if the lengths tensor is empty.\n            RuntimeError: If the lengths tensor does not match the batch size of the input tensor.\n        \"\"\"\n        if x.size(0) == 0 or lengths.size(0) == 0:\n            raise ValueError(\"Input tensor or lengths tensor is empty.\")\n        if x.size(0) != lengths.size(0):\n            raise RuntimeError(f\"Batch size of input tensor ({x.size(0)}) and lengths tensor ({lengths.size(0)}) must match.\")\n\n        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n        packed_output, _ = self.gru_layer(x)\n        x, _ = pad_packed_sequence(packed_output, batch_first=True)\n        x = self.dropout(x)\n        x = self.fc(x)\n        x = self.activation_fct(x)\n        x = self.output_layer(x)\n        return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_gru_regressor/#spotpython.light.regression.nn_many_to_many_gru_regressor.ManyToManyGRU.forward","title":"<code>forward(x, lengths)</code>","text":"<p>Forward pass of the ManyToManyGRU model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, input_size).</p> required <code>lengths</code> <code>Tensor</code> <p>Tensor containing the lengths of each sequence in the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input tensor is empty or if the lengths tensor is empty.</p> <code>RuntimeError</code> <p>If the lengths tensor does not match the batch size of the input tensor.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_gru_regressor.py</code> <pre><code>def forward(self, x, lengths) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the ManyToManyGRU model.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n        lengths (torch.Tensor): Tensor containing the lengths of each sequence in the batch.\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).\n\n    Raises:\n        ValueError: If the input tensor is empty or if the lengths tensor is empty.\n        RuntimeError: If the lengths tensor does not match the batch size of the input tensor.\n    \"\"\"\n    if x.size(0) == 0 or lengths.size(0) == 0:\n        raise ValueError(\"Input tensor or lengths tensor is empty.\")\n    if x.size(0) != lengths.size(0):\n        raise RuntimeError(f\"Batch size of input tensor ({x.size(0)}) and lengths tensor ({lengths.size(0)}) must match.\")\n\n    x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n    packed_output, _ = self.gru_layer(x)\n    x, _ = pad_packed_sequence(packed_output, batch_first=True)\n    x = self.dropout(x)\n    x = self.fc(x)\n    x = self.activation_fct(x)\n    x = self.output_layer(x)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_gru_regressor/#spotpython.light.regression.nn_many_to_many_gru_regressor.ManyToManyGRURegressor","title":"<code>ManyToManyGRURegressor</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule for training and evaluating a Many-to-Many GRU regressor.</p> <p>Parameters:</p> Name Type Description Default <code>_L_in</code> <code>int</code> <p>The number of input features.</p> required <code>_L_out</code> <code>int</code> <p>The number of output features.</p> required <code>l1</code> <code>int</code> <p>Unused parameter. Defaults to 8.</p> <code>8</code> <code>gru_units</code> <code>int</code> <p>The number of units in the GRU layer. Defaults to 128.</p> <code>128</code> <code>fc_units</code> <code>int</code> <p>The number of units in the fully connected layer. Defaults to 128.</p> <code>128</code> <code>act_fn</code> <code>Module</code> <p>The activation function to use after the fully connected layer. Defaults to nn.ReLU().</p> <code>ReLU()</code> <code>dropout_prob</code> <code>float</code> <p>The dropout probability. Defaults to 0.2.</p> <code>0.2</code> <code>bidirectional</code> <code>bool</code> <p>Whether the GRU is bidirectional. Defaults to True.</p> <code>True</code> <code>num_layers</code> <code>int</code> <p>The number of GRU layers. Defaults to 2.</p> <code>2</code> <code>optimizer</code> <code>str</code> <p>The optimizer to use. Defaults to \u201cAdam\u201d.</p> <code>'Adam'</code> <code>lr_mult</code> <code>float</code> <p>Learning rate multiplier. Defaults to 1.0.</p> <code>1.0</code> <code>patience</code> <code>int</code> <p>Patience for learning rate scheduler. Defaults to 5.</p> <code>5</code> <code>epochs</code> <code>int</code> <p>Number of training epochs. Defaults to 100.</p> <code>100</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Defaults to 32.</p> <code>32</code> <code>_torchmetric</code> <code>str</code> <p>The metric to use for evaluation. Defaults to \u201cmean_squared_error\u201d.</p> <code>'mean_squared_error'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = ManyToManyGRURegressor(_L_in=10, _L_out=1)\n&gt;&gt;&gt; x = torch.randn(16, 10, 10)  # Batch of 16 sequences, each of length 10 with 10 features\n&gt;&gt;&gt; lengths = torch.tensor([10] * 16)  # All sequences have length 10\n&gt;&gt;&gt; output = model(x, lengths)\n&gt;&gt;&gt; print(output.shape)  # Output shape: (16, 10, 1)\n</code></pre> Source code in <code>spotpython/light/regression/nn_many_to_many_gru_regressor.py</code> <pre><code>class ManyToManyGRURegressor(L.LightningModule):\n    \"\"\"A LightningModule for training and evaluating a Many-to-Many GRU regressor.\n\n    Args:\n        _L_in (int): The number of input features.\n        _L_out (int): The number of output features.\n        l1 (int): Unused parameter. Defaults to 8.\n        gru_units (int): The number of units in the GRU layer. Defaults to 128.\n        fc_units (int): The number of units in the fully connected layer. Defaults to 128.\n        act_fn (nn.Module): The activation function to use after the fully connected layer. Defaults to nn.ReLU().\n        dropout_prob (float): The dropout probability. Defaults to 0.2.\n        bidirectional (bool): Whether the GRU is bidirectional. Defaults to True.\n        num_layers (int): The number of GRU layers. Defaults to 2.\n        optimizer (str): The optimizer to use. Defaults to \"Adam\".\n        lr_mult (float): Learning rate multiplier. Defaults to 1.0.\n        patience (int): Patience for learning rate scheduler. Defaults to 5.\n        epochs (int): Number of training epochs. Defaults to 100.\n        batch_size (int): Batch size for training. Defaults to 32.\n        _torchmetric (str): The metric to use for evaluation. Defaults to \"mean_squared_error\".\n\n    Examples:\n        &gt;&gt;&gt; model = ManyToManyGRURegressor(_L_in=10, _L_out=1)\n        &gt;&gt;&gt; x = torch.randn(16, 10, 10)  # Batch of 16 sequences, each of length 10 with 10 features\n        &gt;&gt;&gt; lengths = torch.tensor([10] * 16)  # All sequences have length 10\n        &gt;&gt;&gt; output = model(x, lengths)\n        &gt;&gt;&gt; print(output.shape)  # Output shape: (16, 10, 1)\n    \"\"\"\n\n    def __init__(\n        self,\n        _L_in: int,\n        _L_out: int,\n        l1: int = 8,\n        gru_units: int = 128,\n        fc_units: int = 128,\n        act_fn: nn.Module = nn.ReLU(),\n        dropout_prob: float = 0.2,\n        bidirectional: bool = True,\n        num_layers: int = 2,\n        optimizer: str = \"Adam\",\n        lr_mult: float = 1.0,\n        patience: int = 5,\n        epochs: int = 100,\n        batch_size: int = 32,\n        _torchmetric: str = \"mean_squared_error\",\n        *args,\n        **kwargs,\n    ):\n        super().__init__()\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        self.example_input_array = (torch.zeros((batch_size, 10, _L_in)), torch.tensor([10] * batch_size))\n\n        self.layers = ManyToManyGRU(\n            input_size=_L_in,\n            output_size=_L_out,\n            gru_units=self.hparams.gru_units,\n            fc_units=self.hparams.fc_units,\n            activation_fct=self.hparams.act_fn,\n            dropout=self.hparams.dropout_prob,\n            bidirectional=self.hparams.bidirectional,\n            num_layers=self.hparams.num_layers,\n        )\n\n    def forward(self, x, lengths) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the ManyToManyGRURegressor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n            lengths (torch.Tensor): Tensor containing the lengths of each sequence in the batch.\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).\n        \"\"\"\n        x = self.layers(x, lengths)\n        return x\n\n    def _calculate_loss(self, batch):\n        \"\"\"Calculates the loss for a given batch.\n\n        Args:\n            batch (tuple): A tuple containing (x, lengths, y), where:\n                - x: Input tensor of shape (batch_size, seq_len, input_size).\n                - lengths: Tensor containing the lengths of each sequence in the batch.\n                - y: Target tensor of shape (batch_size, seq_len, output_size).\n\n        Returns:\n            torch.Tensor: The calculated loss.\n        \"\"\"\n        x, lengths, y = batch\n        y_hat = self(x, lengths)\n        y = y.view_as(y_hat)\n        loss = self.metric(y_hat, y)\n        return loss\n\n    def training_step(self, batch: tuple, batch_idx) -&gt; torch.Tensor:\n        \"\"\"Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing (x, lengths, y).\n            batch_idx (int): The index of the batch.\n\n        Returns:\n            torch.Tensor: The training loss.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        return val_loss\n\n    def validation_step(self, batch: tuple, batch_idx, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing (x, lengths, y).\n            batch_idx (int): The index of the batch.\n            prog_bar (bool): Whether to log the loss to the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: The validation loss.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=True)\n        self.log(\"hp_metric\", val_loss, prog_bar=True)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing (x, lengths, y).\n            batch_idx (int): The index of the batch.\n            prog_bar (bool): Whether to log the loss to the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: The test loss.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"Configures the optimizer and learning rate scheduler.\n\n        Returns:\n            dict: A dictionary containing the optimizer and learning rate scheduler configuration.\n        \"\"\"\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n        num_milestones = 3\n        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n\n        lr_scheduler_config = {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_gru_regressor/#spotpython.light.regression.nn_many_to_many_gru_regressor.ManyToManyGRURegressor.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer and learning rate scheduler.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the optimizer and learning rate scheduler configuration.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_gru_regressor.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"Configures the optimizer and learning rate scheduler.\n\n    Returns:\n        dict: A dictionary containing the optimizer and learning rate scheduler configuration.\n    \"\"\"\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n    num_milestones = 3\n    milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n\n    lr_scheduler_config = {\n        \"scheduler\": scheduler,\n        \"interval\": \"epoch\",\n        \"frequency\": 1,\n    }\n\n    return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_gru_regressor/#spotpython.light.regression.nn_many_to_many_gru_regressor.ManyToManyGRURegressor.forward","title":"<code>forward(x, lengths)</code>","text":"<p>Forward pass of the ManyToManyGRURegressor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, input_size).</p> required <code>lengths</code> <code>Tensor</code> <p>Tensor containing the lengths of each sequence in the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).</p> Source code in <code>spotpython/light/regression/nn_many_to_many_gru_regressor.py</code> <pre><code>def forward(self, x, lengths) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the ManyToManyGRURegressor.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n        lengths (torch.Tensor): Tensor containing the lengths of each sequence in the batch.\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).\n    \"\"\"\n    x = self.layers(x, lengths)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_gru_regressor/#spotpython.light.regression.nn_many_to_many_gru_regressor.ManyToManyGRURegressor.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing (x, lengths, y).</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to log the loss to the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The test loss.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_gru_regressor.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing (x, lengths, y).\n        batch_idx (int): The index of the batch.\n        prog_bar (bool): Whether to log the loss to the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: The test loss.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_gru_regressor/#spotpython.light.regression.nn_many_to_many_gru_regressor.ManyToManyGRURegressor.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing (x, lengths, y).</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The training loss.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_gru_regressor.py</code> <pre><code>def training_step(self, batch: tuple, batch_idx) -&gt; torch.Tensor:\n    \"\"\"Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing (x, lengths, y).\n        batch_idx (int): The index of the batch.\n\n    Returns:\n        torch.Tensor: The training loss.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_gru_regressor/#spotpython.light.regression.nn_many_to_many_gru_regressor.ManyToManyGRURegressor.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing (x, lengths, y).</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to log the loss to the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The validation loss.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_gru_regressor.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing (x, lengths, y).\n        batch_idx (int): The index of the batch.\n        prog_bar (bool): Whether to log the loss to the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: The validation loss.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", val_loss, prog_bar=True)\n    self.log(\"hp_metric\", val_loss, prog_bar=True)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_lstm_regressor/","title":"nn_many_to_many_lstm_regressor","text":""},{"location":"reference/spotpython/light/regression/nn_many_to_many_lstm_regressor/#spotpython.light.regression.nn_many_to_many_lstm_regressor.ManyToManyLSTM","title":"<code>ManyToManyLSTM</code>","text":"<p>               Bases: <code>Module</code></p> <p>A Many-to-Many LSTM model for sequence-to-sequence regression tasks.</p> <p>This model uses an LSTM layer followed by a fully connected layer and an output layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>The number of input features.</p> required <code>output_size</code> <code>int</code> <p>The number of output features. Defaults to 1.</p> <code>1</code> <code>lstm_units</code> <code>int</code> <p>The number of units in the LSTM layer. Defaults to 256.</p> <code>256</code> <code>fc_units</code> <code>int</code> <p>The number of units in the fully connected layer. Defaults to 256.</p> <code>256</code> <code>activation_fct</code> <code>Module</code> <p>The activation function to use after the fully connected layer. Defaults to nn.ReLU().</p> <code>ReLU()</code> <code>dropout</code> <code>float</code> <p>The dropout probability. Defaults to 0.0.</p> <code>0.0</code> <code>bidirectional</code> <code>bool</code> <p>Whether the LSTM is bidirectional. Defaults to True.</p> <code>True</code> <code>num_layers</code> <code>int</code> <p>The number of LSTM layers. Defaults to 1.</p> <code>1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.regression.nn_many_to_many_lstm_regressor import ManyToManyLSTM\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; model = ManyToManyLSTM(input_size=10, output_size=1)\n&gt;&gt;&gt; x = torch.randn(16, 10, 10)  # Batch of 16 sequences, each of length 10 with 10 features\n&gt;&gt;&gt; lengths = torch.tensor([10] * 16)  # All sequences have length 10\n&gt;&gt;&gt; output = model(x, lengths)\n&gt;&gt;&gt; print(output.shape)  # Output shape: (16, 10, 1)\n</code></pre> Source code in <code>spotpython/light/regression/nn_many_to_many_lstm_regressor.py</code> <pre><code>class ManyToManyLSTM(nn.Module):\n    \"\"\"A Many-to-Many LSTM model for sequence-to-sequence regression tasks.\n\n    This model uses an LSTM layer followed by a fully connected layer and an output layer.\n\n    Args:\n        input_size (int): The number of input features.\n        output_size (int): The number of output features. Defaults to 1.\n        lstm_units (int): The number of units in the LSTM layer. Defaults to 256.\n        fc_units (int): The number of units in the fully connected layer. Defaults to 256.\n        activation_fct (nn.Module): The activation function to use after the fully connected layer. Defaults to nn.ReLU().\n        dropout (float): The dropout probability. Defaults to 0.0.\n        bidirectional (bool): Whether the LSTM is bidirectional. Defaults to True.\n        num_layers (int): The number of LSTM layers. Defaults to 1.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.regression.nn_many_to_many_lstm_regressor import ManyToManyLSTM\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; model = ManyToManyLSTM(input_size=10, output_size=1)\n        &gt;&gt;&gt; x = torch.randn(16, 10, 10)  # Batch of 16 sequences, each of length 10 with 10 features\n        &gt;&gt;&gt; lengths = torch.tensor([10] * 16)  # All sequences have length 10\n        &gt;&gt;&gt; output = model(x, lengths)\n        &gt;&gt;&gt; print(output.shape)  # Output shape: (16, 10, 1)\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size,\n        output_size=1,\n        lstm_units=256,\n        fc_units=256,\n        activation_fct=nn.ReLU(),\n        dropout=0.0,\n        bidirectional=True,\n        num_layers=1,\n    ):\n        super(ManyToManyLSTM, self).__init__()\n        self.lstm_layer = nn.LSTM(\n            input_size=input_size,\n            hidden_size=lstm_units,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=bidirectional,\n            dropout=dropout if num_layers &gt; 1 else 0.0,\n        )\n        if bidirectional:\n            lstm_units = lstm_units * 2\n        self.fc = nn.Linear(lstm_units, fc_units)\n        self.dropout = nn.Dropout(dropout)\n        self.output_layer = nn.Linear(fc_units, output_size)\n        self.activation_fct = activation_fct\n\n    def forward(self, x, lengths) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the ManyToManyLSTM model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n            lengths (torch.Tensor): Tensor containing the lengths of each sequence in the batch.\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).\n\n        Raises:\n            ValueError: If the input tensor is empty or if the lengths tensor is empty.\n            RuntimeError: If the lengths tensor does not match the batch size of the input tensor.\n        \"\"\"\n        if x.size(0) == 0 or lengths.size(0) == 0:\n            raise ValueError(\"Input tensor or lengths tensor is empty.\")\n        if x.size(0) != lengths.size(0):\n            raise RuntimeError(f\"Batch size of input tensor ({x.size(0)}) and lengths tensor ({lengths.size(0)}) must match.\")\n\n        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n        packed_output, _ = self.lstm_layer(x)\n        x, _ = pad_packed_sequence(packed_output, batch_first=True)\n        x = self.dropout(x)\n        x = self.fc(x)\n        x = self.activation_fct(x)\n        x = self.output_layer(x)\n        return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_lstm_regressor/#spotpython.light.regression.nn_many_to_many_lstm_regressor.ManyToManyLSTM.forward","title":"<code>forward(x, lengths)</code>","text":"<p>Forward pass of the ManyToManyLSTM model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, input_size).</p> required <code>lengths</code> <code>Tensor</code> <p>Tensor containing the lengths of each sequence in the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input tensor is empty or if the lengths tensor is empty.</p> <code>RuntimeError</code> <p>If the lengths tensor does not match the batch size of the input tensor.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_lstm_regressor.py</code> <pre><code>def forward(self, x, lengths) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the ManyToManyLSTM model.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n        lengths (torch.Tensor): Tensor containing the lengths of each sequence in the batch.\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).\n\n    Raises:\n        ValueError: If the input tensor is empty or if the lengths tensor is empty.\n        RuntimeError: If the lengths tensor does not match the batch size of the input tensor.\n    \"\"\"\n    if x.size(0) == 0 or lengths.size(0) == 0:\n        raise ValueError(\"Input tensor or lengths tensor is empty.\")\n    if x.size(0) != lengths.size(0):\n        raise RuntimeError(f\"Batch size of input tensor ({x.size(0)}) and lengths tensor ({lengths.size(0)}) must match.\")\n\n    x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n    packed_output, _ = self.lstm_layer(x)\n    x, _ = pad_packed_sequence(packed_output, batch_first=True)\n    x = self.dropout(x)\n    x = self.fc(x)\n    x = self.activation_fct(x)\n    x = self.output_layer(x)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_lstm_regressor/#spotpython.light.regression.nn_many_to_many_lstm_regressor.ManyToManyLSTMRegressor","title":"<code>ManyToManyLSTMRegressor</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule for training and evaluating a Many-to-Many LSTM regressor.</p> <p>Parameters:</p> Name Type Description Default <code>_L_in</code> <code>int</code> <p>The number of input features.</p> required <code>_L_out</code> <code>int</code> <p>The number of output features.</p> required <code>l1</code> <code>int</code> <p>Unused parameter. Defaults to 8.</p> <code>8</code> <code>lstm_units</code> <code>int</code> <p>The number of units in the LSTM layer. Defaults to 128.</p> <code>128</code> <code>fc_units</code> <code>int</code> <p>The number of units in the fully connected layer. Defaults to 128.</p> <code>128</code> <code>act_fn</code> <code>Module</code> <p>The activation function to use after the fully connected layer. Defaults to nn.ReLU().</p> <code>ReLU()</code> <code>dropout_prob</code> <code>float</code> <p>The dropout probability. Defaults to 0.2.</p> <code>0.2</code> <code>bidirectional</code> <code>bool</code> <p>Whether the LSTM is bidirectional. Defaults to True.</p> <code>True</code> <code>num_layers</code> <code>int</code> <p>The number of LSTM layers. Defaults to 2.</p> <code>2</code> <code>optimizer</code> <code>str</code> <p>The optimizer to use. Defaults to \u201cAdam\u201d.</p> <code>'Adam'</code> <code>lr_mult</code> <code>float</code> <p>Learning rate multiplier. Defaults to 1.0.</p> <code>1.0</code> <code>patience</code> <code>int</code> <p>Patience for learning rate scheduler. Defaults to 5.</p> <code>5</code> <code>epochs</code> <code>int</code> <p>Number of training epochs. Defaults to 100.</p> <code>100</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Defaults to 32.</p> <code>32</code> <code>_torchmetric</code> <code>str</code> <p>The metric to use for evaluation. Defaults to \u201cmean_squared_error\u201d.</p> <code>'mean_squared_error'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = ManyToManyLSTMRegressor(_L_in=10, _L_out=1)\n&gt;&gt;&gt; x = torch.randn(16, 10, 10)  # Batch of 16 sequences, each of length 10 with 10 features\n&gt;&gt;&gt; lengths = torch.tensor([10] * 16)  # All sequences have length 10\n&gt;&gt;&gt; output = model(x, lengths)\n&gt;&gt;&gt; print(output.shape)  # Output shape: (16, 10, 1)\n</code></pre> Source code in <code>spotpython/light/regression/nn_many_to_many_lstm_regressor.py</code> <pre><code>class ManyToManyLSTMRegressor(L.LightningModule):\n    \"\"\"A LightningModule for training and evaluating a Many-to-Many LSTM regressor.\n\n    Args:\n        _L_in (int): The number of input features.\n        _L_out (int): The number of output features.\n        l1 (int): Unused parameter. Defaults to 8.\n        lstm_units (int): The number of units in the LSTM layer. Defaults to 128.\n        fc_units (int): The number of units in the fully connected layer. Defaults to 128.\n        act_fn (nn.Module): The activation function to use after the fully connected layer. Defaults to nn.ReLU().\n        dropout_prob (float): The dropout probability. Defaults to 0.2.\n        bidirectional (bool): Whether the LSTM is bidirectional. Defaults to True.\n        num_layers (int): The number of LSTM layers. Defaults to 2.\n        optimizer (str): The optimizer to use. Defaults to \"Adam\".\n        lr_mult (float): Learning rate multiplier. Defaults to 1.0.\n        patience (int): Patience for learning rate scheduler. Defaults to 5.\n        epochs (int): Number of training epochs. Defaults to 100.\n        batch_size (int): Batch size for training. Defaults to 32.\n        _torchmetric (str): The metric to use for evaluation. Defaults to \"mean_squared_error\".\n\n    Examples:\n        &gt;&gt;&gt; model = ManyToManyLSTMRegressor(_L_in=10, _L_out=1)\n        &gt;&gt;&gt; x = torch.randn(16, 10, 10)  # Batch of 16 sequences, each of length 10 with 10 features\n        &gt;&gt;&gt; lengths = torch.tensor([10] * 16)  # All sequences have length 10\n        &gt;&gt;&gt; output = model(x, lengths)\n        &gt;&gt;&gt; print(output.shape)  # Output shape: (16, 10, 1)\n    \"\"\"\n\n    def __init__(\n        self,\n        _L_in: int,\n        _L_out: int,\n        l1: int = 8,\n        lstm_units: int = 128,\n        fc_units: int = 128,\n        act_fn: nn.Module = nn.ReLU(),\n        dropout_prob: float = 0.2,\n        bidirectional: bool = True,\n        num_layers: int = 2,\n        optimizer: str = \"Adam\",\n        lr_mult: float = 1.0,\n        patience: int = 5,\n        epochs: int = 100,\n        batch_size: int = 32,\n        _torchmetric: str = \"mean_squared_error\",\n        *args,\n        **kwargs,\n    ):\n        super().__init__()\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        self.example_input_array = (torch.zeros((batch_size, 10, _L_in)), torch.tensor([10] * batch_size))\n\n        self.layers = ManyToManyLSTM(\n            input_size=_L_in,\n            output_size=_L_out,\n            lstm_units=self.hparams.lstm_units,\n            fc_units=self.hparams.fc_units,\n            activation_fct=self.hparams.act_fn,\n            dropout=self.hparams.dropout_prob,\n            bidirectional=self.hparams.bidirectional,\n            num_layers=self.hparams.num_layers,\n        )\n\n    def forward(self, x, lengths) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the ManyToManyLSTMRegressor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n            lengths (torch.Tensor): Tensor containing the lengths of each sequence in the batch.\n\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).\n        \"\"\"\n        x = self.layers(x, lengths)\n        return x\n\n    def _calculate_loss(self, batch):\n        \"\"\"Calculates the loss for a given batch.\n\n        Args:\n            batch (tuple): A tuple containing (x, lengths, y), where:\n                - x: Input tensor of shape (batch_size, seq_len, input_size).\n                - lengths: Tensor containing the lengths of each sequence in the batch.\n                - y: Target tensor of shape (batch_size, seq_len, output_size).\n\n        Returns:\n            torch.Tensor: The calculated loss.\n        \"\"\"\n        x, lengths, y = batch\n        y_hat = self(x, lengths)\n        y = y.view_as(y_hat)\n        loss = self.metric(y_hat, y)\n        return loss\n\n    def training_step(self, batch: tuple, batch_idx) -&gt; torch.Tensor:\n        \"\"\"Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing (x, lengths, y).\n            batch_idx (int): The index of the batch.\n\n        Returns:\n            torch.Tensor: The training loss.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        return val_loss\n\n    def validation_step(self, batch: tuple, batch_idx, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing (x, lengths, y).\n            batch_idx (int): The index of the batch.\n            prog_bar (bool): Whether to log the loss to the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: The validation loss.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=True)\n        self.log(\"hp_metric\", val_loss, prog_bar=True)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing (x, lengths, y).\n            batch_idx (int): The index of the batch.\n            prog_bar (bool): Whether to log the loss to the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: The test loss.\n        \"\"\"\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"Configures the optimizer and learning rate scheduler.\n\n        Returns:\n            dict: A dictionary containing the optimizer and learning rate scheduler configuration.\n        \"\"\"\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n        num_milestones = 3\n        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n\n        lr_scheduler_config = {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_lstm_regressor/#spotpython.light.regression.nn_many_to_many_lstm_regressor.ManyToManyLSTMRegressor.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer and learning rate scheduler.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the optimizer and learning rate scheduler configuration.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_lstm_regressor.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"Configures the optimizer and learning rate scheduler.\n\n    Returns:\n        dict: A dictionary containing the optimizer and learning rate scheduler configuration.\n    \"\"\"\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n    num_milestones = 3\n    milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n\n    lr_scheduler_config = {\n        \"scheduler\": scheduler,\n        \"interval\": \"epoch\",\n        \"frequency\": 1,\n    }\n\n    return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_lstm_regressor/#spotpython.light.regression.nn_many_to_many_lstm_regressor.ManyToManyLSTMRegressor.forward","title":"<code>forward(x, lengths)</code>","text":"<p>Forward pass of the ManyToManyLSTMRegressor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_len, input_size).</p> required <code>lengths</code> <code>Tensor</code> <p>Tensor containing the lengths of each sequence in the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).</p> Source code in <code>spotpython/light/regression/nn_many_to_many_lstm_regressor.py</code> <pre><code>def forward(self, x, lengths) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the ManyToManyLSTMRegressor.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n        lengths (torch.Tensor): Tensor containing the lengths of each sequence in the batch.\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, seq_len, output_size).\n    \"\"\"\n    x = self.layers(x, lengths)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_lstm_regressor/#spotpython.light.regression.nn_many_to_many_lstm_regressor.ManyToManyLSTMRegressor.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing (x, lengths, y).</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to log the loss to the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The test loss.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_lstm_regressor.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing (x, lengths, y).\n        batch_idx (int): The index of the batch.\n        prog_bar (bool): Whether to log the loss to the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: The test loss.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_lstm_regressor/#spotpython.light.regression.nn_many_to_many_lstm_regressor.ManyToManyLSTMRegressor.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing (x, lengths, y).</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The training loss.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_lstm_regressor.py</code> <pre><code>def training_step(self, batch: tuple, batch_idx) -&gt; torch.Tensor:\n    \"\"\"Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing (x, lengths, y).\n        batch_idx (int): The index of the batch.\n\n    Returns:\n        torch.Tensor: The training loss.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_lstm_regressor/#spotpython.light.regression.nn_many_to_many_lstm_regressor.ManyToManyLSTMRegressor.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing (x, lengths, y).</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to log the loss to the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The validation loss.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_lstm_regressor.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing (x, lengths, y).\n        batch_idx (int): The index of the batch.\n        prog_bar (bool): Whether to log the loss to the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: The validation loss.\n    \"\"\"\n    val_loss = self._calculate_loss(batch)\n    self.log(\"val_loss\", val_loss, prog_bar=True)\n    self.log(\"hp_metric\", val_loss, prog_bar=True)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_rnn_regressor/","title":"nn_many_to_many_rnn_regressor","text":""},{"location":"reference/spotpython/light/regression/nn_many_to_many_rnn_regressor/#spotpython.light.regression.nn_many_to_many_rnn_regressor.ManyToManyRNNRegressor","title":"<code>ManyToManyRNNRegressor</code>","text":"<p>               Bases: <code>LightningModule</code></p> Source code in <code>spotpython/light/regression/nn_many_to_many_rnn_regressor.py</code> <pre><code>class ManyToManyRNNRegressor(L.LightningModule):\n    def __init__(\n        self,\n        _L_in: int,\n        _L_out: int,\n        l1: int = 8,\n        rnn_units: int = 256,\n        fc_units: int = 256,\n        act_fn: nn.Module = nn.ReLU(),\n        dropout_prob: float = 0.0,\n        bidirectional: bool = True,\n        optimizer: str = \"Adam\",\n        lr_mult: float = 1.0,\n        patience: int = 5,\n        epochs: int = 100,\n        batch_size: int = 32,\n        _torchmetric: str = \"mean_squared_error\",\n        *args,\n        **kwargs,\n    ):\n        super().__init__()\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        self.example_input_array = (torch.zeros((batch_size, 10, _L_in)), torch.tensor([10] * batch_size))\n\n        # Instantiate the RNN layers\n        self.layers = ManyToManyRNN(\n            input_size=_L_in,\n            output_size=_L_out,\n            rnn_units=self.hparams.rnn_units,\n            fc_units=self.hparams.fc_units,\n            activation_fct=self.hparams.act_fn,\n            dropout=self.hparams.dropout_prob,\n            bidirectional=self.hparams.bidirectional,\n        )\n\n    def forward(self, x, lengths) -&gt; torch.Tensor:\n        x = self.layers(x, lengths)\n        return x\n\n    def _calculate_loss(self, batch):\n        x, lengths, y = batch\n        y_hat = self(x, lengths)\n        y = y.view_as(y_hat)\n        loss = self.metric(y_hat, y)\n        return loss\n\n    def training_step(self, batch: tuple, batch_idx) -&gt; torch.Tensor:\n        val_loss = self._calculate_loss(batch)\n        return val_loss\n\n    def validation_step(self, batch: tuple, batch_idx, prog_bar: bool = False) -&gt; torch.Tensor:\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=True)\n        self.log(\"hp_metric\", val_loss, prog_bar=True)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        val_loss = self._calculate_loss(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n        num_milestones = 3  # Number of milestones to divide the epochs\n        milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n        lr_scheduler_config = {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_many_to_many_rnn_regressor/#spotpython.light.regression.nn_many_to_many_rnn_regressor.ManyToManyRNNRegressor.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>dict</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/nn_many_to_many_rnn_regressor.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n\n    num_milestones = 3  # Number of milestones to divide the epochs\n    milestones = [int(self.hparams.epochs / (num_milestones + 1) * (i + 1)) for i in range(num_milestones)]\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)  # Decay factor\n\n    lr_scheduler_config = {\n        \"scheduler\": scheduler,\n        \"interval\": \"epoch\",\n        \"frequency\": 1,\n    }\n\n    return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n</code></pre>"},{"location":"reference/spotpython/light/regression/nn_resnet_regressor/","title":"nn_resnet_regressor","text":""},{"location":"reference/spotpython/light/regression/nn_transformer_regressor/","title":"nn_transformer_regressor","text":""},{"location":"reference/spotpython/light/regression/pos_enc/","title":"pos_enc","text":""},{"location":"reference/spotpython/light/regression/rnnlightregression/","title":"rnnlightregression","text":""},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression","title":"<code>RNNLightRegression</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a RNN model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function, e.g., \u201cmean_squared_error\u201d.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.netlightregression import NetLightRegression\n    from torch import nn\n    import lightning as L\n    PATH_DATASETS = './data'\n    BATCH_SIZE = 8\n    dataset = Diabetes()\n    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    batch_x, batch_y = next(iter(train_loader))\n    print(batch_x.shape)\n    print(batch_y.shape)\n    net_light_base = NetLightRegression(l1=128,\n                                        epochs=10,\n                                        batch_size=BATCH_SIZE,\n                                        initialization='xavier',\n                                        act_fn=nn.ReLU(),\n                                        optimizer='Adam',\n                                        dropout_prob=0.1,\n                                        lr_mult=0.1,\n                                        patience=5,\n                                        _L_in=10,\n                                        _L_out=1,\n                                        _torchmetric=\"mean_squared_error\")\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n    trainer.fit(net_light_base, train_loader)\n    trainer.validate(net_light_base, val_loader)\n    trainer.test(net_light_base, test_loader)\n</code></pre> <pre><code>  | Name   | Type       | Params | In sizes | Out sizes\n-------------------------------------------------------------\n0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n-------------------------------------------------------------\n15.9 K    Trainable params\n0         Non-trainable params\n15.9 K    Total params\n0.064     Total estimated model params size (MB)\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Validate metric           DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        hp_metric              29010.7734375\n        val_loss               29010.7734375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        hp_metric              29010.7734375\n        val_loss               29010.7734375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n[{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n</code></pre> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>class RNNLightRegression(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a RNN model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _torchmetric (str):\n            The metric to use for the loss function, e.g., \"mean_squared_error\".\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.netlightregression import NetLightRegression\n            from torch import nn\n            import lightning as L\n            PATH_DATASETS = './data'\n            BATCH_SIZE = 8\n            dataset = Diabetes()\n            train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            batch_x, batch_y = next(iter(train_loader))\n            print(batch_x.shape)\n            print(batch_y.shape)\n            net_light_base = NetLightRegression(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier',\n                                                act_fn=nn.ReLU(),\n                                                optimizer='Adam',\n                                                dropout_prob=0.1,\n                                                lr_mult=0.1,\n                                                patience=5,\n                                                _L_in=10,\n                                                _L_out=1,\n                                                _torchmetric=\"mean_squared_error\")\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n            trainer.fit(net_light_base, train_loader)\n            trainer.validate(net_light_base, val_loader)\n            trainer.test(net_light_base, test_loader)\n\n              | Name   | Type       | Params | In sizes | Out sizes\n            -------------------------------------------------------------\n            0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n            -------------------------------------------------------------\n            15.9 K    Trainable params\n            0         Non-trainable params\n            15.9 K    Total params\n            0.064     Total estimated model params size (MB)\n\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Validate metric           DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Test metric             DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n            [{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the NetLightRegression object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n            _torchmetric (str): The metric to use for the loss function, e.g., \"mean_squared_error\".\n\n        Returns:\n            (NoneType): None\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n\n        # Initialize RNN\n        # input_size = number of features is defined via _L_in\n        # output size via _L_out\n        # num_layers=1: only a single RNN and not stacked\n        rnn_units = self.hparams.l1\n        fc_units = self.hparams.l1\n\n        # # TODO: make this a hyperparameter\n        rnn_nonlinearity = \"relu\"\n\n        self.rnn_layer = nn.RNN(\n            input_size=self._L_in,\n            hidden_size=rnn_units,\n            num_layers=1,\n            nonlinearity=rnn_nonlinearity,\n            bias=True,\n            batch_first=True,\n            bidirectional=False,\n        )\n\n        # # Initialize Hidden- and Output-Layer\n        self.fc = nn.Linear(rnn_units, fc_units)\n        self.output_layer = nn.Linear(fc_units, self._L_out)\n\n        # # Initialize Activation Function and Dropouts\n        # dropout = [0.2, 0, 0]\n        # self.dropout1 = nn.Dropout(dropout[0])\n        # self.dropout2 = nn.Dropout(dropout[1])\n        # self.dropout3 = nn.Dropout(dropout[2])\n        # # TODO: use enhanced dropout management for different layers\n        self.dropout1 = nn.Dropout(self.hparams.dropout_prob)\n        self.dropout2 = nn.Dropout(self.hparams.dropout_prob // 10.0)\n        self.dropout3 = nn.Dropout(self.hparams.dropout_prob // 100.0)\n\n        # TODO: Enable different activation functions\n        # activation_fct = nn.ReLU()\n        # self.activation_fct = activation_fct\n        self.activation_fct = self.hparams.act_fn\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the output of the model.\n\n        \"\"\"\n        # print(f\"input: {x.shape}\")\n        x = self.dropout1(x)\n        # print(f\"dropout1: {x.shape}\")\n        x, _ = self.rnn_layer(x)\n        # print(f\"rnn_layer: {x.shape}\")\n        # x = x[:, -1, :]\n        # print(f\"slicing: {x.shape}\")\n        x = self.dropout2(x)\n        # print(f\"dropout2: {x.shape}\")\n        x = self.activation_fct(self.fc(x))\n        # print(f\"activation_fct: {x.shape}\")\n        x = self.dropout3(x)\n        # print(f\"dropout3: {x.shape}\")\n        x = self.output_layer(x)\n        # print(f\"output_layer: {x.shape}\")\n        return x\n\n    def training_step(self, batch: tuple, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        # reshape the tensor y to be a column vector (len(y) rows and 1 column)\n        y = y.view(len(y), 1)\n        # Note: the number of rows in x is equal to the number of rows in y\n        y_hat = self(x)\n        # Note: the number of rows in y_hat is equal to the number of rows in y\n        # train_loss = F.mse_loss(y_hat, y)\n        metric = getattr(torchmetrics.functional.regression, self._torchmetric)\n        train_loss = metric(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n        return train_loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        # reshape the tensor y to be a column vector (len(y) rows and 1 column)\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        val_loss = F.mse_loss(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        val_loss = F.mse_loss(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.__init__","title":"<code>__init__(l1, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, _torchmetric, *args, **kwargs)</code>","text":"<p>Initializes the NetLightRegression object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function, e.g., \u201cmean_squared_error\u201d.</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    _torchmetric: str,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the NetLightRegression object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n        _torchmetric (str): The metric to use for the loss function, e.g., \"mean_squared_error\".\n\n    Returns:\n        (NoneType): None\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    if _torchmetric is None:\n        _torchmetric = \"mean_squared_error\"\n    self._torchmetric = _torchmetric\n    self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_in))\n\n    # Initialize RNN\n    # input_size = number of features is defined via _L_in\n    # output size via _L_out\n    # num_layers=1: only a single RNN and not stacked\n    rnn_units = self.hparams.l1\n    fc_units = self.hparams.l1\n\n    # # TODO: make this a hyperparameter\n    rnn_nonlinearity = \"relu\"\n\n    self.rnn_layer = nn.RNN(\n        input_size=self._L_in,\n        hidden_size=rnn_units,\n        num_layers=1,\n        nonlinearity=rnn_nonlinearity,\n        bias=True,\n        batch_first=True,\n        bidirectional=False,\n    )\n\n    # # Initialize Hidden- and Output-Layer\n    self.fc = nn.Linear(rnn_units, fc_units)\n    self.output_layer = nn.Linear(fc_units, self._L_out)\n\n    # # Initialize Activation Function and Dropouts\n    # dropout = [0.2, 0, 0]\n    # self.dropout1 = nn.Dropout(dropout[0])\n    # self.dropout2 = nn.Dropout(dropout[1])\n    # self.dropout3 = nn.Dropout(dropout[2])\n    # # TODO: use enhanced dropout management for different layers\n    self.dropout1 = nn.Dropout(self.hparams.dropout_prob)\n    self.dropout2 = nn.Dropout(self.hparams.dropout_prob // 10.0)\n    self.dropout3 = nn.Dropout(self.hparams.dropout_prob // 100.0)\n\n    # TODO: Enable different activation functions\n    # activation_fct = nn.ReLU()\n    # self.activation_fct = activation_fct\n    self.activation_fct = self.hparams.act_fn\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n    return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the output of the model.</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the output of the model.\n\n    \"\"\"\n    # print(f\"input: {x.shape}\")\n    x = self.dropout1(x)\n    # print(f\"dropout1: {x.shape}\")\n    x, _ = self.rnn_layer(x)\n    # print(f\"rnn_layer: {x.shape}\")\n    # x = x[:, -1, :]\n    # print(f\"slicing: {x.shape}\")\n    x = self.dropout2(x)\n    # print(f\"dropout2: {x.shape}\")\n    x = self.activation_fct(self.fc(x))\n    # print(f\"activation_fct: {x.shape}\")\n    x = self.dropout3(x)\n    # print(f\"dropout3: {x.shape}\")\n    x = self.output_layer(x)\n    # print(f\"output_layer: {x.shape}\")\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    x, y = batch\n    y = y.view(len(y), 1)\n    y_hat = self(x)\n    val_loss = F.mse_loss(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.training_step","title":"<code>training_step(batch, prog_bar=False)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def training_step(self, batch: tuple, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    x, y = batch\n    # reshape the tensor y to be a column vector (len(y) rows and 1 column)\n    y = y.view(len(y), 1)\n    # Note: the number of rows in x is equal to the number of rows in y\n    y_hat = self(x)\n    # Note: the number of rows in y_hat is equal to the number of rows in y\n    # train_loss = F.mse_loss(y_hat, y)\n    metric = getattr(torchmetrics.functional.regression, self._torchmetric)\n    train_loss = metric(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    # self.log(\"train_loss\", val_loss, on_step=True, on_epoch=True, prog_bar=True)\n    # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n    return train_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/rnnlightregression/#spotpython.light.regression.rnnlightregression.RNNLightRegression.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/rnnlightregression.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    x, y = batch\n    # reshape the tensor y to be a column vector (len(y) rows and 1 column)\n    y = y.view(len(y), 1)\n    y_hat = self(x)\n    val_loss = F.mse_loss(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/","title":"transformerlightregression","text":""},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression","title":"<code>TransformerLightRegression</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A LightningModule class for a transformer-based regression neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function, e.g., \u201cmean_squared_error\u201d.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n    from spotpython.data.diabetes import Diabetes\n    from torch import nn\n    import lightning as L\n    PATH_DATASETS = './data'\n    BATCH_SIZE = 8\n    dataset = Diabetes()\n    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n    batch_x, batch_y = next(iter(train_loader))\n    print(batch_x.shape)\n    print(batch_y.shape)\n    net_light_base = NetLightRegression2(l1=128,\n                                        epochs=10,\n                                        batch_size=BATCH_SIZE,\n                                        initialization='xavier',\n                                        act_fn=nn.ReLU(),\n                                        optimizer='Adam',\n                                        dropout_prob=0.1,\n                                        lr_mult=0.1,\n                                        patience=5,\n                                        _L_in=10,\n                                        _L_out=1)\n    trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n    trainer.fit(net_light_base, train_loader)\n    trainer.validate(net_light_base, val_loader)\n    trainer.test(net_light_base, test_loader)\n</code></pre> <pre><code>  | Name   | Type       | Params | In sizes | Out sizes\n-------------------------------------------------------------\n0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n-------------------------------------------------------------\n15.9 K    Trainable params\n0         Non-trainable params\n15.9 K    Total params\n0.064     Total estimated model params size (MB)\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Validate metric           DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        hp_metric              29010.7734375\n        val_loss               29010.7734375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        hp_metric              29010.7734375\n        val_loss               29010.7734375\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n[{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n</code></pre> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>class TransformerLightRegression(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a transformer-based regression neural network model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _torchmetric (str):\n            The metric to use for the loss function, e.g., \"mean_squared_error\".\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n            from spotpython.data.diabetes import Diabetes\n            from torch import nn\n            import lightning as L\n            PATH_DATASETS = './data'\n            BATCH_SIZE = 8\n            dataset = Diabetes()\n            train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n            batch_x, batch_y = next(iter(train_loader))\n            print(batch_x.shape)\n            print(batch_y.shape)\n            net_light_base = NetLightRegression2(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier',\n                                                act_fn=nn.ReLU(),\n                                                optimizer='Adam',\n                                                dropout_prob=0.1,\n                                                lr_mult=0.1,\n                                                patience=5,\n                                                _L_in=10,\n                                                _L_out=1)\n            trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n            trainer.fit(net_light_base, train_loader)\n            trainer.validate(net_light_base, val_loader)\n            trainer.test(net_light_base, test_loader)\n\n              | Name   | Type       | Params | In sizes | Out sizes\n            -------------------------------------------------------------\n            0 | layers | Sequential | 15.9 K | [8, 10]  | [8, 1]\n            -------------------------------------------------------------\n            15.9 K    Trainable params\n            0         Non-trainable params\n            15.9 K    Total params\n            0.064     Total estimated model params size (MB)\n\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Validate metric           DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                Test metric             DataLoader 0\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    hp_metric              29010.7734375\n                    val_loss               29010.7734375\n            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n            [{'val_loss': 28981.529296875, 'hp_metric': 28981.529296875}]\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        d_mult: int,\n        dim_feedforward: int,\n        nhead: int,\n        num_layers: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        _torchmetric: str,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the TransformerLightRegression object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n            _torchmetric (str): The metric to use for the loss function, e.g., \"mean_squared_error\".\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 4.\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        if _torchmetric is None:\n            _torchmetric = \"mean_squared_error\"\n        self._torchmetric = _torchmetric\n        self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n        self.d_mult = d_mult\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n\n        # self.l1 = l1\n        # self.dim_feedforward = dim_feedforward\n        # self.nhead = nhead\n        # self.num_layers = num_layers\n        # self.act_fn = act_fn\n        # self.dropout_prob = dropout_prob\n\n        l_nodes = d_mult * nhead * 2\n        # Each of the _L_1 inputs is forwarded to d_model nodes,\n        # e.g., if _L_in = 90 and d_model = 4, then the input is forwarded to 360 nodes\n        # self.embed = SkipLinear(90, 360)\n        self.embed = SkipLinear(_L_in, _L_in * l_nodes)\n\n        # Positional encoding\n        # self.pos_enc = PositionalEncoding(d_model=4, dropout_prob=dropout_prob)\n        self.pos_enc = PositionalEncoding(d_model=l_nodes, dropout_prob=self.hparams.dropout_prob)\n\n        # Transformer encoder layer\n        # embed_dim \"d_model\" must be divisible by num_heads\n        print(f\"l_nodes: {l_nodes} must be divisible by nhead: {self.hparams.nhead} and 2.\")\n        # self.enc_layer = torch.nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=10, batch_first=True)\n        self.enc_layer = torch.nn.TransformerEncoderLayer(\n            d_model=l_nodes,\n            nhead=self.hparams.nhead,\n            dim_feedforward=self.hparams.dim_feedforward,\n            batch_first=True,\n        )\n\n        # Transformer encoder\n        # self.trans_enc = torch.nn.TransformerEncoder(self.enc_layer, num_layers=2)\n        self.trans_enc = torch.nn.TransformerEncoder(self.enc_layer, num_layers=self.hparams.num_layers)\n\n        n_low = _L_in // 4\n        # ensure that n_high is larger than n_low\n        n_high = max(self.hparams.l1, 2 * n_low)\n        hidden_sizes = generate_div2_list(n_high, n_low)\n\n        # Create the network based on the specified hidden sizes\n        layers = []\n        layer_sizes = [self._L_in * l_nodes] + hidden_sizes\n        layer_size_last = layer_sizes[0]\n        for layer_size in layer_sizes[1:]:\n            layers += [\n                nn.Linear(layer_size_last, layer_size),\n                nn.BatchNorm1d(layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n            layer_size_last = layer_size\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        l_nodes = self.hparams.d_mult * self.hparams.nhead * 2\n        z = self.embed(x)\n\n        # z = z.reshape(-1, 90, 4)\n        z = z.reshape(-1, self._L_in, l_nodes)\n\n        z = self.pos_enc(z)\n        z = self.trans_enc(z)\n\n        # flatten\n        # z = z.reshape(-1, 360)\n        z = z.reshape(-1, self._L_in * l_nodes)\n\n        z = self.layers(z)\n        return z\n\n    def training_step(self, batch: tuple, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        val_loss = F.mse_loss(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        # self.log(\"train_loss\", val_loss, prog_bar=prog_bar)\n        # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n        return val_loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n\n        \"\"\"\n        x, y = batch\n        y = y.view(len(y), 1)\n        y_hat = self(x)\n        val_loss = F.mse_loss(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        x, y = batch\n        y_hat = self(x)\n        y = y.view(len(y), 1)\n        val_loss = F.mse_loss(y_hat, y)\n        # mae_loss = F.l1_loss(y_hat, y)\n        self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n        self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n        return val_loss\n\n    def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single prediction step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the prediction for this batch.\n        \"\"\"\n        x, y = batch\n        yhat = self(x)\n        y = y.view(len(y), 1)\n        yhat = yhat.view(len(yhat), 1)\n        print(f\"Predict step x: {x}\")\n        print(f\"Predict step y: {y}\")\n        print(f\"Predict step y_hat: {yhat}\")\n        # pred_loss = F.mse_loss(y_hat, y)\n        # pred loss not registered\n        # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n        # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n        # MisconfigurationException: You are trying to `self.log()`\n        # but the loop's result collection is not registered yet.\n        # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n        # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n        return (x, y, yhat)\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Notes:\n            The default Lightning way is to define an optimizer as\n            `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n            spotpython uses an optimizer handler to create the optimizer, which\n            adapts the learning rate according to the lr_mult hyperparameter as\n            well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n        return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.__init__","title":"<code>__init__(l1, d_mult, dim_feedforward, nhead, num_layers, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, _torchmetric, *args, **kwargs)</code>","text":"<p>Initializes the TransformerLightRegression object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <code>_torchmetric</code> <code>str</code> <p>The metric to use for the loss function, e.g., \u201cmean_squared_error\u201d.</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 4.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    d_mult: int,\n    dim_feedforward: int,\n    nhead: int,\n    num_layers: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    _torchmetric: str,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the TransformerLightRegression object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n        _torchmetric (str): The metric to use for the loss function, e.g., \"mean_squared_error\".\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 4.\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    if _torchmetric is None:\n        _torchmetric = \"mean_squared_error\"\n    self._torchmetric = _torchmetric\n    self.metric = getattr(torchmetrics.functional.regression, _torchmetric)\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    # _torchmetric is not a hyperparameter, but is needed to calculate the loss\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_torchmetric\"])\n    self.d_mult = d_mult\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_in))\n\n    # self.l1 = l1\n    # self.dim_feedforward = dim_feedforward\n    # self.nhead = nhead\n    # self.num_layers = num_layers\n    # self.act_fn = act_fn\n    # self.dropout_prob = dropout_prob\n\n    l_nodes = d_mult * nhead * 2\n    # Each of the _L_1 inputs is forwarded to d_model nodes,\n    # e.g., if _L_in = 90 and d_model = 4, then the input is forwarded to 360 nodes\n    # self.embed = SkipLinear(90, 360)\n    self.embed = SkipLinear(_L_in, _L_in * l_nodes)\n\n    # Positional encoding\n    # self.pos_enc = PositionalEncoding(d_model=4, dropout_prob=dropout_prob)\n    self.pos_enc = PositionalEncoding(d_model=l_nodes, dropout_prob=self.hparams.dropout_prob)\n\n    # Transformer encoder layer\n    # embed_dim \"d_model\" must be divisible by num_heads\n    print(f\"l_nodes: {l_nodes} must be divisible by nhead: {self.hparams.nhead} and 2.\")\n    # self.enc_layer = torch.nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=10, batch_first=True)\n    self.enc_layer = torch.nn.TransformerEncoderLayer(\n        d_model=l_nodes,\n        nhead=self.hparams.nhead,\n        dim_feedforward=self.hparams.dim_feedforward,\n        batch_first=True,\n    )\n\n    # Transformer encoder\n    # self.trans_enc = torch.nn.TransformerEncoder(self.enc_layer, num_layers=2)\n    self.trans_enc = torch.nn.TransformerEncoder(self.enc_layer, num_layers=self.hparams.num_layers)\n\n    n_low = _L_in // 4\n    # ensure that n_high is larger than n_low\n    n_high = max(self.hparams.l1, 2 * n_low)\n    hidden_sizes = generate_div2_list(n_high, n_low)\n\n    # Create the network based on the specified hidden sizes\n    layers = []\n    layer_sizes = [self._L_in * l_nodes] + hidden_sizes\n    layer_size_last = layer_sizes[0]\n    for layer_size in layer_sizes[1:]:\n        layers += [\n            nn.Linear(layer_size_last, layer_size),\n            nn.BatchNorm1d(layer_size),\n            self.hparams.act_fn,\n            nn.Dropout(self.hparams.dropout_prob),\n        ]\n        layer_size_last = layer_size\n    layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n    # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n    self.layers = nn.Sequential(*layers)\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> Notes <p>The default Lightning way is to define an optimizer as <code>optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)</code>. spotpython uses an optimizer handler to create the optimizer, which adapts the learning rate according to the lr_mult hyperparameter as well as other hyperparameters. See <code>spotpython.hyperparameters.optimizer.py</code> for details.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Notes:\n        The default Lightning way is to define an optimizer as\n        `optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)`.\n        spotpython uses an optimizer handler to create the optimizer, which\n        adapts the learning rate according to the lr_mult hyperparameter as\n        well as other hyperparameters. See `spotpython.hyperparameters.optimizer.py` for details.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult)\n    return optimizer\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.predict_step","title":"<code>predict_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the prediction for this batch.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def predict_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single prediction step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the prediction for this batch.\n    \"\"\"\n    x, y = batch\n    yhat = self(x)\n    y = y.view(len(y), 1)\n    yhat = yhat.view(len(yhat), 1)\n    print(f\"Predict step x: {x}\")\n    print(f\"Predict step y: {y}\")\n    print(f\"Predict step y_hat: {yhat}\")\n    # pred_loss = F.mse_loss(y_hat, y)\n    # pred loss not registered\n    # self.log(\"pred_loss\", pred_loss, prog_bar=prog_bar)\n    # self.log(\"hp_metric\", pred_loss, prog_bar=prog_bar)\n    # MisconfigurationException: You are trying to `self.log()`\n    # but the loop's result collection is not registered yet.\n    # This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging.\n    # If you want to manually log, please consider using `self.log_dict({'pred_loss': pred_loss})` instead.\n    return (x, y, yhat)\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    x, y = batch\n    y_hat = self(x)\n    y = y.view(len(y), 1)\n    val_loss = F.mse_loss(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.training_step","title":"<code>training_step(batch, prog_bar=False)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def training_step(self, batch: tuple, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    x, y = batch\n    y = y.view(len(y), 1)\n    y_hat = self(x)\n    val_loss = F.mse_loss(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    # self.log(\"train_loss\", val_loss, prog_bar=prog_bar)\n    # self.log(\"train_mae_loss\", mae_loss, on_step=True, on_epoch=True, prog_bar=True)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/regression/transformerlightregression/#spotpython.light.regression.transformerlightregression.TransformerLightRegression.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotpython/light/regression/transformerlightregression.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n\n    \"\"\"\n    x, y = batch\n    y = y.view(len(y), 1)\n    y_hat = self(x)\n    val_loss = F.mse_loss(y_hat, y)\n    # mae_loss = F.l1_loss(y_hat, y)\n    # self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", val_loss, prog_bar=prog_bar)\n    self.log(\"hp_metric\", val_loss, prog_bar=prog_bar)\n    return val_loss\n</code></pre>"},{"location":"reference/spotpython/light/transformer/attention/","title":"attention","text":""},{"location":"reference/spotpython/light/transformer/attention/#spotpython.light.transformer.attention.scaled_dot_product","title":"<code>scaled_dot_product(q, k, v, mask=None)</code>","text":"<pre><code>Compute scaled dot product attention.\nArgs:\n    q: Queries\n    k: Keys\n    v: Values\n    mask: Mask to apply to the attention logits\n\nReturns:\n    Tuple of (Values, Attention weights)\n\nExamples:\n&gt;&gt;&gt; from spotpython.light.transformer.attention import scaled_dot_product\n    seq_len, d_k = 1, 2\n    pl.seed_everything(42)\n    q = torch.randn(seq_len, d_k)\n    k = torch.randn(seq_len, d_k)\n    v = torch.randn(seq_len, d_k)\n    values, attention = scaled_dot_product(q, k, v)\n    print(\"Q\n</code></pre> <p>\u201d, q)         print(\u201cK \u201c, k)         print(\u201cV \u201c, v)         print(\u201cValues \u201c, values)         print(\u201cAttention \u201c, attention)</p> Source code in <code>spotpython/light/transformer/attention.py</code> <pre><code>def scaled_dot_product(q, k, v, mask=None):\n    \"\"\"\n    Compute scaled dot product attention.\n    Args:\n        q: Queries\n        k: Keys\n        v: Values\n        mask: Mask to apply to the attention logits\n\n    Returns:\n        Tuple of (Values, Attention weights)\n\n    Examples:\n    &gt;&gt;&gt; from spotpython.light.transformer.attention import scaled_dot_product\n        seq_len, d_k = 1, 2\n        pl.seed_everything(42)\n        q = torch.randn(seq_len, d_k)\n        k = torch.randn(seq_len, d_k)\n        v = torch.randn(seq_len, d_k)\n        values, attention = scaled_dot_product(q, k, v)\n        print(\"Q\\n\", q)\n        print(\"K\\n\", k)\n        print(\"V\\n\", v)\n        print(\"Values\\n\", values)\n        print(\"Attention\\n\", attention)\n    \"\"\"\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n    if mask is not None:\n        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n    attention = F.softmax(attn_logits, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention\n</code></pre>"},{"location":"reference/spotpython/light/transformer/encoder/","title":"encoder","text":""},{"location":"reference/spotpython/light/transformer/encoder/#spotpython.light.transformer.encoder.TransformerEncoder","title":"<code>TransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer encoder module. Consists of a stack of EncoderBlocks with layer norm at the end.</p> Source code in <code>spotpython/light/transformer/encoder.py</code> <pre><code>class TransformerEncoder(nn.Module):\n    \"\"\"Transformer encoder module.\n    Consists of a stack of EncoderBlocks with layer norm at the end.\n    \"\"\"\n\n    def __init__(self, num_layers, **block_args) -&gt; None:\n        \"\"\"Constructor.\n        Args:\n            num_layers: int, number of encoder blocks.\n            block_args: dict, arguments for EncoderBlock.\n\n        Returns:\n            None\n\n        Example:\n            &gt;&gt;&gt; from spotpython.light.transformer.encoder import TransformerEncoder\n            &gt;&gt;&gt; encoder = TransformerEncoder(num_layers=3,\n                                            model_dim=512,\n                                            num_heads=8,\n                                            dim_feedforward=2048,\n                                            dropout=0.1)\n            &gt;&gt;&gt; x = torch.rand(10, 32, 512)\n            &gt;&gt;&gt; encoder(x).shape\n            torch.Size([10, 32, 512])\n\n        \"\"\"\n        super().__init__()\n        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n\n    def forward(self, x, mask=None):\n        for layer in self.layers:\n            x = layer(x, mask=mask)\n        return x\n\n    def get_attention_maps(self, x, mask=None):\n        attention_maps = []\n        for layer in self.layers:\n            _, attn_map = layer.self_attn(x, mask=mask, return_attention=True)\n            attention_maps.append(attn_map)\n            x = layer(x)\n        return attention_maps\n</code></pre>"},{"location":"reference/spotpython/light/transformer/encoder/#spotpython.light.transformer.encoder.TransformerEncoder.__init__","title":"<code>__init__(num_layers, **block_args)</code>","text":"<p>Constructor. Args:     num_layers: int, number of encoder blocks.     block_args: dict, arguments for EncoderBlock.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>from spotpython.light.transformer.encoder import TransformerEncoder encoder = TransformerEncoder(num_layers=3,                                 model_dim=512,                                 num_heads=8,                                 dim_feedforward=2048,                                 dropout=0.1) x = torch.rand(10, 32, 512) encoder(x).shape torch.Size([10, 32, 512])</p> Source code in <code>spotpython/light/transformer/encoder.py</code> <pre><code>def __init__(self, num_layers, **block_args) -&gt; None:\n    \"\"\"Constructor.\n    Args:\n        num_layers: int, number of encoder blocks.\n        block_args: dict, arguments for EncoderBlock.\n\n    Returns:\n        None\n\n    Example:\n        &gt;&gt;&gt; from spotpython.light.transformer.encoder import TransformerEncoder\n        &gt;&gt;&gt; encoder = TransformerEncoder(num_layers=3,\n                                        model_dim=512,\n                                        num_heads=8,\n                                        dim_feedforward=2048,\n                                        dropout=0.1)\n        &gt;&gt;&gt; x = torch.rand(10, 32, 512)\n        &gt;&gt;&gt; encoder(x).shape\n        torch.Size([10, 32, 512])\n\n    \"\"\"\n    super().__init__()\n    self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n</code></pre>"},{"location":"reference/spotpython/light/transformer/encoderblock/","title":"encoderblock","text":""},{"location":"reference/spotpython/light/transformer/encoderblock/#spotpython.light.transformer.encoderblock.EncoderBlock","title":"<code>EncoderBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>spotpython/light/transformer/encoderblock.py</code> <pre><code>class EncoderBlock(nn.Module):\n    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0) -&gt; None:\n        \"\"\"\n        Initializes the EncoderBlock object.\n\n        Args:\n            input_dim (int): The dimensionality of the input.\n            num_heads (int): The number of heads to use in the attention block.\n            dim_feedforward (int): The dimensionality of the hidden layer in the MLP.\n            dropout (float): The dropout probability to use in the dropout layers.\n\n        Returns:\n            None\n        \"\"\"\n        super().__init__()\n\n        # Attention layer\n        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n\n        # Two-layer MLP\n        self.linear_net = nn.Sequential(\n            nn.Linear(input_dim, dim_feedforward),\n            nn.Dropout(dropout),\n            nn.ReLU(inplace=True),\n            nn.Linear(dim_feedforward, input_dim),\n        )\n\n        # Layers to apply in between the main layers\n        self.norm1 = nn.LayerNorm(input_dim)\n        self.norm2 = nn.LayerNorm(input_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Attention part\n        attn_out = self.self_attn(x, mask=mask)\n        x = x + self.dropout(attn_out)\n        x = self.norm1(x)\n\n        # MLP part\n        linear_out = self.linear_net(x)\n        x = x + self.dropout(linear_out)\n        x = self.norm2(x)\n\n        return x\n</code></pre>"},{"location":"reference/spotpython/light/transformer/encoderblock/#spotpython.light.transformer.encoderblock.EncoderBlock.__init__","title":"<code>__init__(input_dim, num_heads, dim_feedforward, dropout=0.0)</code>","text":"<p>Initializes the EncoderBlock object.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The dimensionality of the input.</p> required <code>num_heads</code> <code>int</code> <p>The number of heads to use in the attention block.</p> required <code>dim_feedforward</code> <code>int</code> <p>The dimensionality of the hidden layer in the MLP.</p> required <code>dropout</code> <code>float</code> <p>The dropout probability to use in the dropout layers.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/light/transformer/encoderblock.py</code> <pre><code>def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0) -&gt; None:\n    \"\"\"\n    Initializes the EncoderBlock object.\n\n    Args:\n        input_dim (int): The dimensionality of the input.\n        num_heads (int): The number of heads to use in the attention block.\n        dim_feedforward (int): The dimensionality of the hidden layer in the MLP.\n        dropout (float): The dropout probability to use in the dropout layers.\n\n    Returns:\n        None\n    \"\"\"\n    super().__init__()\n\n    # Attention layer\n    self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n\n    # Two-layer MLP\n    self.linear_net = nn.Sequential(\n        nn.Linear(input_dim, dim_feedforward),\n        nn.Dropout(dropout),\n        nn.ReLU(inplace=True),\n        nn.Linear(dim_feedforward, input_dim),\n    )\n\n    # Layers to apply in between the main layers\n    self.norm1 = nn.LayerNorm(input_dim)\n    self.norm2 = nn.LayerNorm(input_dim)\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/multiheadattention/","title":"multiheadattention","text":""},{"location":"reference/spotpython/light/transformer/multiheadattention/#spotpython.light.transformer.multiheadattention.MultiheadAttention","title":"<code>MultiheadAttention</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>spotpython/light/transformer/multiheadattention.py</code> <pre><code>class MultiheadAttention(nn.Module):\n    def __init__(self, input_dim, embed_dim, num_heads):\n        \"\"\"Constructor.\n\n        Args:\n            input_dim (int): input dimensionality.\n            embed_dim (int): embedding dimensionality.\n            num_heads (int): number of heads.\n        \"\"\"\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"Embedding dim. must be 0 modulo number of heads.\"\n\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n\n        # Stack all weight matrices 1...h together for efficiency\n        # Note that in many implementations you see \"bias=False\" which is optional\n        self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n        self.o_proj = nn.Linear(embed_dim, embed_dim)\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # Original Transformer initialization, see PyTorch documentation\n        nn.init.xavier_uniform_(self.qkv_proj.weight)\n        self.qkv_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.o_proj.weight)\n        self.o_proj.bias.data.fill_(0)\n\n    def forward(self, x, mask=None, return_attention=False):\n        batch_size, seq_length, _ = x.size()\n        if mask is not None:\n            mask = expand_mask(mask)\n        qkv = self.qkv_proj(x)\n\n        # Separate Q, K, V from linear output\n        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3)  # [Batch, Head, SeqLen, Dims]\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Determine value outputs\n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n        values = values.permute(0, 2, 1, 3)  # [Batch, SeqLen, Head, Dims]\n        values = values.reshape(batch_size, seq_length, self.embed_dim)\n        o = self.o_proj(values)\n\n        if return_attention:\n            return o, attention\n        else:\n            return o\n</code></pre>"},{"location":"reference/spotpython/light/transformer/multiheadattention/#spotpython.light.transformer.multiheadattention.MultiheadAttention.__init__","title":"<code>__init__(input_dim, embed_dim, num_heads)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>input dimensionality.</p> required <code>embed_dim</code> <code>int</code> <p>embedding dimensionality.</p> required <code>num_heads</code> <code>int</code> <p>number of heads.</p> required Source code in <code>spotpython/light/transformer/multiheadattention.py</code> <pre><code>def __init__(self, input_dim, embed_dim, num_heads):\n    \"\"\"Constructor.\n\n    Args:\n        input_dim (int): input dimensionality.\n        embed_dim (int): embedding dimensionality.\n        num_heads (int): number of heads.\n    \"\"\"\n    super().__init__()\n    assert embed_dim % num_heads == 0, \"Embedding dim. must be 0 modulo number of heads.\"\n\n    self.embed_dim = embed_dim\n    self.num_heads = num_heads\n    self.head_dim = embed_dim // num_heads\n\n    # Stack all weight matrices 1...h together for efficiency\n    # Note that in many implementations you see \"bias=False\" which is optional\n    self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n    self.o_proj = nn.Linear(embed_dim, embed_dim)\n\n    self._reset_parameters()\n</code></pre>"},{"location":"reference/spotpython/light/transformer/multiheadattention/#spotpython.light.transformer.multiheadattention.expand_mask","title":"<code>expand_mask(mask)</code>","text":"<p>Helper function to support different mask shapes. Expands the mask to the correct shape for the MultiheadAttention layer. Output shape supports (batch_size, number of heads, seq length, seq length). If 2D: broadcasted over batch size and number of heads. If 3D: broadcasted over number of heads. If 4D: leave as is.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Tensor</code> <p>Mask tensor of shape (batch_size, seq_length, seq_length) or (seq_length, seq_length).</p> required Source code in <code>spotpython/light/transformer/multiheadattention.py</code> <pre><code>def expand_mask(mask):\n    \"\"\"\n    Helper function to support different mask shapes.\n    Expands the mask to the correct shape for the MultiheadAttention layer.\n    Output shape supports (batch_size, number of heads, seq length, seq length).\n    If 2D: broadcasted over batch size and number of heads.\n    If 3D: broadcasted over number of heads.\n    If 4D: leave as is.\n\n    Args:\n        mask (torch.Tensor):\n            Mask tensor of shape (batch_size, seq_length, seq_length) or (seq_length, seq_length).\n    \"\"\"\n    assert mask.ndim &gt;= 2, \"Mask must be &gt;= 2-dim. with seq_length x seq_length\"\n    if mask.ndim == 3:\n        mask = mask.unsqueeze(1)\n    while mask.ndim &lt; 4:\n        mask = mask.unsqueeze(0)\n    return mask\n</code></pre>"},{"location":"reference/spotpython/light/transformer/positionalEncoding/","title":"positionalEncoding","text":""},{"location":"reference/spotpython/light/transformer/positionalEncoding/#spotpython.light.transformer.positionalEncoding.PositionalEncoding","title":"<code>PositionalEncoding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Positional encoding module injects some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings, so that the two can be summed. Here, we use <code>sine</code> and <code>cosine</code> functions of different frequencies.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>the embedding dimension. Should be even.</p> required <code>dropout_prob</code> <code>float</code> <p>the dropout value</p> required <code>max_len</code> <code>int</code> <p>the maximum length of the incoming sequence. Usually related to the max batch_size. Can be larger as the batch size, e.g., if prediction is done on a single test set. Default: 12552</p> <code>12552</code> Shape <p>Input:     x: Tensor, shape <code>[seq_len, batch_size, embedding_dim]</code> Output:     Tensor, shape <code>[seq_len, batch_size, embedding_dim]</code></p> Notes <ul> <li><code>No return value, but torch</code>\u2019s method <code>register_buffer</code> is used to register the positional encodings.</li> <li>Code adapted from PyTorch: \u201cTransformer Tutorial\u201d</li> </ul> Reference <p>https://pytorch.org/tutorials/beginner/transformer_tutorial.html#positional-encoding</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.transformer.positionalEncoding import PositionalEncoding\n    import torch\n    # number of tensors\n    n = 3\n    # dimension of each tensor, should be even\n    k = 10\n    pe = PositionalEncoding(d_model=k, dropout_prob=0)\n    input = torch.zeros(1, n, k)\n    # Generate a tensor of size (1, 10, 4) with values from 1 to 10\n    for i in range(n):\n        input[0, i, :] = i\n    print(f\"Input shape: {input.shape}\")\n    print(f\"Input: {input}\")\n    output = pe(input)\n    print(f\"Output shape: {output.shape}\")\n    print(f\"Output: {output}\")\n    position: tensor([[    0],\n                    [    1],\n                    [    2],\n                    ...,\n                    [99997],\n                    [99998],\n                    [99999]])\n    div_term: tensor([1.0000e+00, 1.5849e-01, 2.5119e-02, 3.9811e-03, 6.3096e-04])\n    Input shape: torch.Size([1, 3, 10])\n    Input: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n            [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n            [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]])\n    Output shape: torch.Size([1, 3, 10])\n    Output: tensor([[[0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n            [1., 2., 1., 2., 1., 2., 1., 2., 1., 2.],\n            [2., 3., 2., 3., 2., 3., 2., 3., 2., 3.]]])\n</code></pre> Source code in <code>spotpython/light/transformer/positionalEncoding.py</code> <pre><code>class PositionalEncoding(nn.Module):\n    \"\"\"\n    Positional encoding module injects some information\n    about the relative or absolute position of the tokens in the sequence.\n    The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n    Here, we use ``sine`` and ``cosine`` functions of different frequencies.\n\n    Args:\n        d_model (int):\n            the embedding dimension. Should be even.\n        dropout_prob (float):\n            the dropout value\n        max_len (int):\n            the maximum length of the incoming sequence. Usually related to the max batch_size.\n            Can be larger as the batch size, e.g., if prediction is done on a single test set.\n            Default: 12552\n\n    Shape:\n        Input:\n            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n        Output:\n            Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n\n    Notes:\n        * `No return value, but torch`'s method `register_buffer` is used to register the positional encodings.\n        * Code adapted from PyTorch: \"Transformer Tutorial\"\n\n\n    Reference:\n        https://pytorch.org/tutorials/beginner/transformer_tutorial.html#positional-encoding\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.transformer.positionalEncoding import PositionalEncoding\n            import torch\n            # number of tensors\n            n = 3\n            # dimension of each tensor, should be even\n            k = 10\n            pe = PositionalEncoding(d_model=k, dropout_prob=0)\n            input = torch.zeros(1, n, k)\n            # Generate a tensor of size (1, 10, 4) with values from 1 to 10\n            for i in range(n):\n                input[0, i, :] = i\n            print(f\"Input shape: {input.shape}\")\n            print(f\"Input: {input}\")\n            output = pe(input)\n            print(f\"Output shape: {output.shape}\")\n            print(f\"Output: {output}\")\n            position: tensor([[    0],\n                            [    1],\n                            [    2],\n                            ...,\n                            [99997],\n                            [99998],\n                            [99999]])\n            div_term: tensor([1.0000e+00, 1.5849e-01, 2.5119e-02, 3.9811e-03, 6.3096e-04])\n            Input shape: torch.Size([1, 3, 10])\n            Input: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                    [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n                    [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]])\n            Output shape: torch.Size([1, 3, 10])\n            Output: tensor([[[0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n                    [1., 2., 1., 2., 1., 2., 1., 2., 1., 2.],\n                    [2., 3., 2., 3., 2., 3., 2., 3., 2., 3.]]])\n    \"\"\"\n\n    def __init__(self, d_model: int, dropout_prob: float, max_len: int = 12552) -&gt; None:\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout_prob)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Add positional encoding to the input tensor.\n\n        Arguments:\n            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n\n        Returns:\n            Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n\n        Raises:\n            IndexError: if the positional encoding cannot be added to the input tensor\n        \"\"\"\n        x = x + self.pe[: x.size(0)]\n        return self.dropout(x)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/positionalEncoding/#spotpython.light.transformer.positionalEncoding.PositionalEncoding.forward","title":"<code>forward(x)</code>","text":"<p>Add positional encoding to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor, shape <code>[seq_len, batch_size, embedding_dim]</code></p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor, shape <code>[seq_len, batch_size, embedding_dim]</code></p> <p>Raises:</p> Type Description <code>IndexError</code> <p>if the positional encoding cannot be added to the input tensor</p> Source code in <code>spotpython/light/transformer/positionalEncoding.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Add positional encoding to the input tensor.\n\n    Arguments:\n        x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n\n    Returns:\n        Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n\n    Raises:\n        IndexError: if the positional encoding cannot be added to the input tensor\n    \"\"\"\n    x = x + self.pe[: x.size(0)]\n    return self.dropout(x)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/positionalEncodingBasic/","title":"positionalEncodingBasic","text":""},{"location":"reference/spotpython/light/transformer/positionalEncodingBasic/#spotpython.light.transformer.positionalEncodingBasic.PositionalEncodingBasic","title":"<code>PositionalEncodingBasic</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>spotpython/light/transformer/positionalEncodingBasic.py</code> <pre><code>class PositionalEncodingBasic(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        \"\"\"\n        Inputs\n            d_model - Hidden dimensionality of the input.\n            max_len - Maximum length of a sequence to expect.\n        \"\"\"\n        super().__init__()\n\n        # Create matrix of [SeqLen, HiddenDim] representing\n        # the positional encoding for max_len inputs\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n\n        # register_buffer =&gt; Tensor which is not a parameter,\n        # but should be part of the modules state.\n        # Used for tensors that need to be on the same device as the module.\n        # persistent=False tells PyTorch to not add the buffer to the\n        # state dict (e.g. when we save the model)\n        self.register_buffer(\"pe\", pe, persistent=False)\n\n    def forward(self, x):\n        x = x + self.pe[:, : x.size(1)]\n        return x\n</code></pre>"},{"location":"reference/spotpython/light/transformer/positionalEncodingBasic/#spotpython.light.transformer.positionalEncodingBasic.PositionalEncodingBasic.__init__","title":"<code>__init__(d_model, max_len=5000)</code>","text":"<p>Inputs     d_model - Hidden dimensionality of the input.     max_len - Maximum length of a sequence to expect.</p> Source code in <code>spotpython/light/transformer/positionalEncodingBasic.py</code> <pre><code>def __init__(self, d_model, max_len=5000):\n    \"\"\"\n    Inputs\n        d_model - Hidden dimensionality of the input.\n        max_len - Maximum length of a sequence to expect.\n    \"\"\"\n    super().__init__()\n\n    # Create matrix of [SeqLen, HiddenDim] representing\n    # the positional encoding for max_len inputs\n    pe = torch.zeros(max_len, d_model)\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    pe = pe.unsqueeze(0)\n\n    # register_buffer =&gt; Tensor which is not a parameter,\n    # but should be part of the modules state.\n    # Used for tensors that need to be on the same device as the module.\n    # persistent=False tells PyTorch to not add the buffer to the\n    # state dict (e.g. when we save the model)\n    self.register_buffer(\"pe\", pe, persistent=False)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/skiplinear/","title":"skiplinear","text":""},{"location":"reference/spotpython/light/transformer/skiplinear/#spotpython.light.transformer.skiplinear.SkipLinear","title":"<code>SkipLinear</code>","text":"<p>               Bases: <code>Module</code></p> <p>A skip linear layer.</p> <p>Notes: Code adapted from James D. McCaffrey: \u201cRegression Using a PyTorch Neural Network with a Transformer Component\u201d</p> Reference <p>https://jamesmccaffrey.wordpress.com/2023/12/01/regression-using-a-pytorch-neural-network-with-a-transformer-component/</p> <p>Parameters:</p> Name Type Description Default <code>n_in</code> <code>int</code> <p>the input dimension</p> required <code>n_out</code> <code>int</code> <p>the output dimension</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.light.transformer.skiplinear import SkipLinear\n    import torch\n    n_in = 2\n    n_out = 4\n    sl = SkipLinear(n_in, n_out)\n    input = torch.zeros(1, n_in)\n    for i in range(n_in):\n        input[0, i] = i\n    print(f\"Input shape: {input.shape}\")\n    print(f\"Input: {input}\")\n    output = sl(input)\n    print(f\"Output shape: {output.shape}\")\n    print(f\"Output: {output}\")\n    print(sl.lst_modules)\n    for i in sl.lst_modules:\n        print(f\"weights: {i.weights}\")\n    Input shape: torch.Size([1, 2])\n    Input: tensor([[0., 1.]])\n    Output shape: torch.Size([1, 4])\n    Output: tensor([[ 0.0000,  0.0000, -0.0062, -0.0032]], grad_fn=&lt;ViewBackward0&gt;)\n    ModuleList(\n    (0-1): 2 x Core()\n    )\n    weights: Parameter containing:\n    tensor([[-0.0098],\n            [ 0.0038]], requires_grad=True)\n    weights: Parameter containing:\n    tensor([[0.0041],\n            [0.0074]], requires_grad=True)\n</code></pre> Source code in <code>spotpython/light/transformer/skiplinear.py</code> <pre><code>class SkipLinear(torch.nn.Module):\n    \"\"\"\n    A skip linear layer.\n\n    Notes:\n    Code adapted from James D. McCaffrey:\n    \"Regression Using a PyTorch Neural Network with a Transformer Component\"\n\n    Reference:\n        https://jamesmccaffrey.wordpress.com/2023/12/01/regression-using-a-pytorch-neural-network-with-a-transformer-component/\n\n    Args:\n        n_in (int):\n            the input dimension\n        n_out (int):\n            the output dimension\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.light.transformer.skiplinear import SkipLinear\n            import torch\n            n_in = 2\n            n_out = 4\n            sl = SkipLinear(n_in, n_out)\n            input = torch.zeros(1, n_in)\n            for i in range(n_in):\n                input[0, i] = i\n            print(f\"Input shape: {input.shape}\")\n            print(f\"Input: {input}\")\n            output = sl(input)\n            print(f\"Output shape: {output.shape}\")\n            print(f\"Output: {output}\")\n            print(sl.lst_modules)\n            for i in sl.lst_modules:\n                print(f\"weights: {i.weights}\")\n            Input shape: torch.Size([1, 2])\n            Input: tensor([[0., 1.]])\n            Output shape: torch.Size([1, 4])\n            Output: tensor([[ 0.0000,  0.0000, -0.0062, -0.0032]], grad_fn=&lt;ViewBackward0&gt;)\n            ModuleList(\n            (0-1): 2 x Core()\n            )\n            weights: Parameter containing:\n            tensor([[-0.0098],\n                    [ 0.0038]], requires_grad=True)\n            weights: Parameter containing:\n            tensor([[0.0041],\n                    [0.0074]], requires_grad=True)\n    \"\"\"\n\n    class Core(torch.nn.Module):\n        \"\"\"A simple linear layer with n outputs.\"\"\"\n\n        def __init__(self, n):\n            \"\"\"\n            Initialize the layer.\n\n            Args:\n                n (int): The number of output nodes.\n            \"\"\"\n            super().__init__()\n            # initialize with random weights using normal distribution\n            self.weights = torch.nn.Parameter(torch.randn(1, n))\n            # self.weights = torch.nn.Parameter(torch.rand(1, n) * 2 - 1)\n            self.linear = torch.nn.Linear(1, n)\n\n        def forward(self, x) -&gt; torch.Tensor:\n            \"\"\"\n            Forward pass through the layer.\n\n            Args:\n                x (torch.Tensor): The input tensor.\n\n            Returns:\n                torch.Tensor: The output of the layer.\n            \"\"\"\n            return self.linear(x)\n\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.n_in = n_in\n        self.n_out = n_out\n        if n_out % n_in != 0:\n            raise ValueError(\"n_out % n_in != 0\")\n        n = n_out // n_in  # num nodes per input\n\n        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for _ in range(n_in)])\n\n    def forward(self, x):\n        # We want to apply each module to a slice of the input tensor x and collect the outputs.\n        # This applies the i-th module to the i-th column of x, reshaped as a column vector.\n        # The result is a list of output tensors, which are then concatenated to form the final output.\n        lst_nodes = [self.lst_modules[i](x[:, i].unsqueeze(1)) for i in range(self.n_in)]\n        result = torch.cat(lst_nodes, dim=1)\n        return result.reshape(-1, self.n_out)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/skiplinear/#spotpython.light.transformer.skiplinear.SkipLinear.Core","title":"<code>Core</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple linear layer with n outputs.</p> Source code in <code>spotpython/light/transformer/skiplinear.py</code> <pre><code>class Core(torch.nn.Module):\n    \"\"\"A simple linear layer with n outputs.\"\"\"\n\n    def __init__(self, n):\n        \"\"\"\n        Initialize the layer.\n\n        Args:\n            n (int): The number of output nodes.\n        \"\"\"\n        super().__init__()\n        # initialize with random weights using normal distribution\n        self.weights = torch.nn.Parameter(torch.randn(1, n))\n        # self.weights = torch.nn.Parameter(torch.rand(1, n) * 2 - 1)\n        self.linear = torch.nn.Linear(1, n)\n\n    def forward(self, x) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the layer.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The output of the layer.\n        \"\"\"\n        return self.linear(x)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/skiplinear/#spotpython.light.transformer.skiplinear.SkipLinear.Core.__init__","title":"<code>__init__(n)</code>","text":"<p>Initialize the layer.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of output nodes.</p> required Source code in <code>spotpython/light/transformer/skiplinear.py</code> <pre><code>def __init__(self, n):\n    \"\"\"\n    Initialize the layer.\n\n    Args:\n        n (int): The number of output nodes.\n    \"\"\"\n    super().__init__()\n    # initialize with random weights using normal distribution\n    self.weights = torch.nn.Parameter(torch.randn(1, n))\n    # self.weights = torch.nn.Parameter(torch.rand(1, n) * 2 - 1)\n    self.linear = torch.nn.Linear(1, n)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/skiplinear/#spotpython.light.transformer.skiplinear.SkipLinear.Core.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output of the layer.</p> Source code in <code>spotpython/light/transformer/skiplinear.py</code> <pre><code>def forward(self, x) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the layer.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The output of the layer.\n    \"\"\"\n    return self.linear(x)\n</code></pre>"},{"location":"reference/spotpython/light/transformer/transformerlightpredictor/","title":"transformerlightpredictor","text":""},{"location":"reference/spotpython/light/transformer/transformerlightpredictor/#spotpython.light.transformer.transformerlightpredictor.TransformerLightPredictor","title":"<code>TransformerLightPredictor</code>","text":"<p>               Bases: <code>LightningModule</code></p> Source code in <code>spotpython/light/transformer/transformerlightpredictor.py</code> <pre><code>class TransformerLightPredictor(L.LightningModule):\n    def __init__(\n        self,\n        l1: int,\n        d_mult: int,\n        dim_feedforward: int,\n        nhead: int,\n        num_layers: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        model_dim: int,\n        num_heads: int,\n        lr: float,\n        warmup: int,\n        max_iters: int,\n        input_dropout: float,\n        dropout: float,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the TransformerLightRegression object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int):\n                The number of input features. Not a hyperparameter, but needed to create the network. `input_dim`,\n                hidden dimensionality of the input.\n            _L_out (int):\n                The number of output classes. Not a hyperparameter, but needed to create the network. `num_classes`,\n                number of classes to predict per sequence element.\n            model_dim (int):\n                Hidden dimensionality to use inside the Transformer\n            num_heads (int):\n                Number of heads to use in the Multi-Head Attention blocks\n            num_layers (int):\n                Number of encoder blocks to use.\n            lr (float):\n                Learning rate in the optimizer\n            warmup (int):\n                Number of warmup steps. Usually between 50 and 500\n            max_iters (int):\n                Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n            input_dropout (float):\n                Dropout to apply on the input features\n            dropout (float):\n                Dropout to apply inside the Transformer\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        self.d_mult = d_mult\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n        # set dummy input array for Tensorboard Graphs\n        # set log_graph=True in Trainer to see the graph (in traintest.py)\n        self.example_input_array = torch.zeros((batch_size, self._L_in))\n        self._create_model()\n\n    def _create_model(self):\n        # Input dim -&gt; Model dim\n        self.input_net = nn.Sequential(nn.Dropout(self.hparams.input_dropout), nn.Linear(self.hparams.input_dim, self.hparams.model_dim))\n        # Positional encoding for sequences\n        self.positional_encoding = PositionalEncodingBasic(d_model=self.hparams.model_dim)\n        # Transformer\n        self.transformer = TransformerEncoder(\n            num_layers=self.hparams.num_layers,\n            input_dim=self.hparams.model_dim,\n            dim_feedforward=2 * self.hparams.model_dim,\n            num_heads=self.hparams.num_heads,\n            dropout=self.hparams.dropout,\n        )\n        # Output classifier per sequence element\n        self.output_net = nn.Sequential(\n            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n            nn.LayerNorm(self.hparams.model_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(self.hparams.dropout),\n            nn.Linear(self.hparams.model_dim, self.hparams.num_classes),\n        )\n\n    def forward(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Inputs:\n            x - Input features of shape [Batch, SeqLen, input_dim]\n            mask - Mask to apply on the attention outputs (optional)\n            add_positional_encoding - If True, we add the positional encoding to the input.\n                                      Might not be desired for some tasks.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        x = self.transformer(x, mask=mask)\n        x = self.output_net(x)\n        return x\n\n    @torch.no_grad()\n    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Function for extracting the attention matrices of the whole Transformer for a single batch.\n        Input arguments same as the forward pass.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n        return attention_maps\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n\n        # Apply lr scheduler per step\n        lr_scheduler = CosineWarmupScheduler(optimizer, warmup=self.hparams.warmup, max_iters=self.hparams.max_iters)\n        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n\n    def training_step(self, batch, batch_idx):\n        raise NotImplementedError\n\n    def validation_step(self, batch, batch_idx):\n        raise NotImplementedError\n\n    def test_step(self, batch, batch_idx):\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/spotpython/light/transformer/transformerlightpredictor/#spotpython.light.transformer.transformerlightpredictor.TransformerLightPredictor.__init__","title":"<code>__init__(l1, d_mult, dim_feedforward, nhead, num_layers, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out, model_dim, num_heads, lr, warmup, max_iters, input_dropout, dropout, *args, **kwargs)</code>","text":"<p>Initializes the TransformerLightRegression object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network. <code>input_dim</code>, hidden dimensionality of the input.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network. <code>num_classes</code>, number of classes to predict per sequence element.</p> required <code>model_dim</code> <code>int</code> <p>Hidden dimensionality to use inside the Transformer</p> required <code>num_heads</code> <code>int</code> <p>Number of heads to use in the Multi-Head Attention blocks</p> required <code>num_layers</code> <code>int</code> <p>Number of encoder blocks to use.</p> required <code>lr</code> <code>float</code> <p>Learning rate in the optimizer</p> required <code>warmup</code> <code>int</code> <p>Number of warmup steps. Usually between 50 and 500</p> required <code>max_iters</code> <code>int</code> <p>Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler</p> required <code>input_dropout</code> <code>float</code> <p>Dropout to apply on the input features</p> required <code>dropout</code> <code>float</code> <p>Dropout to apply inside the Transformer</p> required Source code in <code>spotpython/light/transformer/transformerlightpredictor.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    d_mult: int,\n    dim_feedforward: int,\n    nhead: int,\n    num_layers: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    model_dim: int,\n    num_heads: int,\n    lr: float,\n    warmup: int,\n    max_iters: int,\n    input_dropout: float,\n    dropout: float,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the TransformerLightRegression object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features. Not a hyperparameter, but needed to create the network. `input_dim`,\n            hidden dimensionality of the input.\n        _L_out (int):\n            The number of output classes. Not a hyperparameter, but needed to create the network. `num_classes`,\n            number of classes to predict per sequence element.\n        model_dim (int):\n            Hidden dimensionality to use inside the Transformer\n        num_heads (int):\n            Number of heads to use in the Multi-Head Attention blocks\n        num_layers (int):\n            Number of encoder blocks to use.\n        lr (float):\n            Learning rate in the optimizer\n        warmup (int):\n            Number of warmup steps. Usually between 50 and 500\n        max_iters (int):\n            Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n        input_dropout (float):\n            Dropout to apply on the input features\n        dropout (float):\n            Dropout to apply inside the Transformer\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    self.d_mult = d_mult\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n    # set dummy input array for Tensorboard Graphs\n    # set log_graph=True in Trainer to see the graph (in traintest.py)\n    self.example_input_array = torch.zeros((batch_size, self._L_in))\n    self._create_model()\n</code></pre>"},{"location":"reference/spotpython/light/transformer/transformerlightpredictor/#spotpython.light.transformer.transformerlightpredictor.TransformerLightPredictor.forward","title":"<code>forward(x, mask=None, add_positional_encoding=True)</code>","text":"Inputs <p>x - Input features of shape [Batch, SeqLen, input_dim] mask - Mask to apply on the attention outputs (optional) add_positional_encoding - If True, we add the positional encoding to the input.                           Might not be desired for some tasks.</p> Source code in <code>spotpython/light/transformer/transformerlightpredictor.py</code> <pre><code>def forward(self, x, mask=None, add_positional_encoding=True):\n    \"\"\"\n    Inputs:\n        x - Input features of shape [Batch, SeqLen, input_dim]\n        mask - Mask to apply on the attention outputs (optional)\n        add_positional_encoding - If True, we add the positional encoding to the input.\n                                  Might not be desired for some tasks.\n    \"\"\"\n    x = self.input_net(x)\n    if add_positional_encoding:\n        x = self.positional_encoding(x)\n    x = self.transformer(x, mask=mask)\n    x = self.output_net(x)\n    return x\n</code></pre>"},{"location":"reference/spotpython/light/transformer/transformerlightpredictor/#spotpython.light.transformer.transformerlightpredictor.TransformerLightPredictor.get_attention_maps","title":"<code>get_attention_maps(x, mask=None, add_positional_encoding=True)</code>","text":"<p>Function for extracting the attention matrices of the whole Transformer for a single batch. Input arguments same as the forward pass.</p> Source code in <code>spotpython/light/transformer/transformerlightpredictor.py</code> <pre><code>@torch.no_grad()\ndef get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n    \"\"\"\n    Function for extracting the attention matrices of the whole Transformer for a single batch.\n    Input arguments same as the forward pass.\n    \"\"\"\n    x = self.input_net(x)\n    if add_positional_encoding:\n        x = self.positional_encoding(x)\n    attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n    return attention_maps\n</code></pre>"},{"location":"reference/spotpython/mo/functions/","title":"functions","text":""},{"location":"reference/spotpython/mo/functions/#spotpython.mo.functions.activity_pred","title":"<code>activity_pred(X)</code>","text":"<p>Compute activity predictions for each row in the input array.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D array where each row is a configuration.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 1D array of activity predictions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.mo.functions import activity_pred\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; activity_pred(X)\narray([  1.5,  10.5])\n</code></pre> Source code in <code>spotpython/mo/functions.py</code> <pre><code>def activity_pred(X) -&gt; np.ndarray:\n    \"\"\"\n    Compute activity predictions for each row in the input array.\n\n    Args:\n        X (np.ndarray): 2D array where each row is a configuration.\n\n    Returns:\n        np.ndarray: 1D array of activity predictions.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.mo.functions import activity_pred\n        &gt;&gt;&gt; # Example input data\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; activity_pred(X)\n        array([  1.5,  10.5])\n    \"\"\"\n    return (\n        59.85\n        + 3.583 * X[:, 0]\n        + 0.2546 * X[:, 1]\n        + 2.2298 * X[:, 2]\n        + 0.83479 * X[:, 0] ** 2\n        + 0.07484 * X[:, 1] ** 2\n        + 0.05716 * X[:, 2] ** 2\n        - 0.3875 * X[:, 0] * X[:, 1]\n        - 0.375 * X[:, 0] * X[:, 2]\n        + 0.3125 * X[:, 1] * X[:, 2]\n    )\n</code></pre>"},{"location":"reference/spotpython/mo/functions/#spotpython.mo.functions.conversion_pred","title":"<code>conversion_pred(X)</code>","text":"<p>Compute conversion predictions for each row in the input array.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D array where each row is a configuration.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 1D array of conversion predictions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.mo.functions import conversion_pred\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; conversion_pred(X)\narray([  3.5,  19.5])\n</code></pre> Source code in <code>spotpython/mo/functions.py</code> <pre><code>def conversion_pred(X) -&gt; np.ndarray:\n    \"\"\"\n    Compute conversion predictions for each row in the input array.\n\n    Args:\n        X (np.ndarray): 2D array where each row is a configuration.\n\n    Returns:\n        np.ndarray: 1D array of conversion predictions.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.mo.functions import conversion_pred\n        &gt;&gt;&gt; # Example input data\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; conversion_pred(X)\n        array([  3.5,  19.5])\n\n    \"\"\"\n    return (\n        81.09\n        + 1.0284 * X[:, 0]\n        + 4.043 * X[:, 1]\n        + 6.2037 * X[:, 2]\n        - 1.8366 * X[:, 0] ** 2\n        + 2.9382 * X[:, 1] ** 2\n        - 5.1915 * X[:, 2] ** 2\n        + 2.2150 * X[:, 0] * X[:, 1]\n        + 11.375 * X[:, 0] * X[:, 2]\n        - 3.875 * X[:, 1] * X[:, 2]\n    )\n</code></pre>"},{"location":"reference/spotpython/mo/functions/#spotpython.mo.functions.fun_myer16a","title":"<code>fun_myer16a(X, fun_control=None)</code>","text":"<p>Compute both conversion and activity predictions for each row in the input array.</p> Notes <p>Implements a response surface experiment described by Myers, Montgomery, and Anderson-Cook (2016). The function computes two objectives: conversion and activity.</p> References <ul> <li>Myers, R. H., Montgomery, D. C., and Anderson-Cook, C. M. Response surface methodology: process and product optimization using designed experiments. John Wiley &amp; Sons, 2016.</li> <li>Kuhn, M. desirability: Function optimization and ranking via desirability functions. Tech. rep., 9 2016.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D array where each row is a configuration.</p> required <code>fun_control</code> <code>dict</code> <p>Additional control parameters (not used here).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 2D array where each row contains [conversion_pred, activity_pred].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.mo.functions import fun_myer16a\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun_myer16a(X)\narray([[  3.5,   1.5],\n       [ 19.5,  10.5]])\n</code></pre> Source code in <code>spotpython/mo/functions.py</code> <pre><code>def fun_myer16a(X, fun_control=None) -&gt; np.ndarray:\n    \"\"\"\n    Compute both conversion and activity predictions for each row in the input array.\n\n    Notes:\n        Implements a response surface experiment described by Myers, Montgomery, and Anderson-Cook (2016). The function computes two objectives: conversion and activity.\n\n    References:\n        - Myers, R. H., Montgomery, D. C., and Anderson-Cook, C. M. Response surface methodology: process and product optimization using designed experiments. John Wiley &amp; Sons, 2016.\n        - Kuhn, M. desirability: Function optimization and ranking via desirability functions. Tech. rep., 9 2016.\n\n    Args:\n        X (np.ndarray): 2D array where each row is a configuration.\n        fun_control (dict, optional): Additional control parameters (not used here).\n\n    Returns:\n        np.ndarray: 2D array where each row contains [conversion_pred, activity_pred].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.mo.functions import fun_myer16a\n        &gt;&gt;&gt; # Example input data\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun_myer16a(X)\n        array([[  3.5,   1.5],\n               [ 19.5,  10.5]])\n    \"\"\"\n    return np.column_stack((conversion_pred(X), activity_pred(X)))\n</code></pre>"},{"location":"reference/spotpython/mo/pareto/","title":"pareto","text":""},{"location":"reference/spotpython/mo/pareto/#spotpython.mo.pareto.is_pareto_efficient","title":"<code>is_pareto_efficient(costs, minimize=True)</code>","text":"<p>Find the Pareto-efficient points from a set of points.</p> <p>A point is Pareto-efficient if no other point exists that is better in all objectives. This function assumes that lower values are preferred for each objective when <code>minimize=True</code>, and higher values are preferred when <code>minimize=False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>costs</code> <code>ndarray</code> <p>An (N,M) array-like object of points, where N is the number of points and M is the number of objectives.</p> required <code>minimize</code> <code>bool</code> <p>If True, the function finds Pareto-efficient points assuming lower values are better. If False, it assumes higher values are better. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A boolean mask of length N, where True indicates that the corresponding point is Pareto-efficient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.mo.pareto import is_pareto_efficient\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; costs = np.array([[1, 2], [2, 1], [3, 3], [1.5, 1.5]])\n&gt;&gt;&gt; is_efficient = is_pareto_efficient(costs)\n&gt;&gt;&gt; print(is_efficient)\n[ True  True False  True]\n</code></pre> Source code in <code>spotpython/mo/pareto.py</code> <pre><code>def is_pareto_efficient(costs: np.ndarray, minimize: bool = True) -&gt; np.ndarray:\n    \"\"\"\n    Find the Pareto-efficient points from a set of points.\n\n    A point is Pareto-efficient if no other point exists that is better in all objectives.\n    This function assumes that lower values are preferred for each objective when `minimize=True`,\n    and higher values are preferred when `minimize=False`.\n\n    Args:\n        costs (np.ndarray):\n            An (N,M) array-like object of points, where N is the number of points and M is the number of objectives.\n        minimize (bool, optional):\n            If True, the function finds Pareto-efficient points assuming\n            lower values are better. If False, it assumes higher values are better.\n            Defaults to True.\n\n    Returns:\n        np.ndarray:\n            A boolean mask of length N, where True indicates that the corresponding point is Pareto-efficient.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.mo.pareto import is_pareto_efficient\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; costs = np.array([[1, 2], [2, 1], [3, 3], [1.5, 1.5]])\n        &gt;&gt;&gt; is_efficient = is_pareto_efficient(costs)\n        &gt;&gt;&gt; print(is_efficient)\n        [ True  True False  True]\n    \"\"\"\n    is_efficient = np.ones(costs.shape[0], dtype=bool)\n    for i, cost in enumerate(costs):\n        if is_efficient[i]:\n            if minimize:\n                is_efficient[is_efficient] = np.any(costs[is_efficient] &lt; cost, axis=1)\n            else:\n                is_efficient[is_efficient] = np.any(costs[is_efficient] &gt; cost, axis=1)\n            is_efficient[i] = True\n    return is_efficient\n</code></pre>"},{"location":"reference/spotpython/mo/plot/","title":"plot","text":""},{"location":"reference/spotpython/mo/plot/#spotpython.mo.plot.plot_mo","title":"<code>plot_mo(target_names, combinations, pareto, y_rf=None, pareto_front=False, y_best=None, y_add=None, y_add2=None, y_add_color='blue', y_add2_color='green', title='', y_orig=None, pareto_front_orig=False, pareto_label=False, y_rf_color='blue', y_best_color='red', x_axis_transformation='id', y_axis_transformation='id', y_best_label='Best', y_add_label='Add', y_add2_label='Add2', filename=None)</code>","text":"<p>Generates scatter plots for each combination of two targets from a multi-output prediction while highlighting Pareto optimal points.</p> <p>Parameters:</p> Name Type Description Default <code>y_rf</code> <code>ndarray</code> <p>The predicted target values with shape (n_samples, n_targets).</p> <code>None</code> <code>target_names</code> <code>list</code> <p>A list of target names corresponding to the columns of y_rf.</p> required <code>combinations</code> <code>list</code> <p>A list of tuples, where each tuple contains the indices of the target combinations to plot.</p> required <code>pareto</code> <code>str</code> <p>Specifies whether to compute Pareto front based on \u2018min\u2019 or \u2018max\u2019 criterion.</p> required <code>pareto_front</code> <code>bool</code> <p>If True, connect Pareto optimal points with a red line for y_rf.</p> <code>False</code> <code>y_best</code> <code>ndarray</code> <p>A NumPy array representing the best point to highlight in red. Defaults to None.</p> <code>None</code> <code>y_add</code> <code>ndarray</code> <p>A NumPy array representing the additional points to highlight in blue. Defaults to None.</p> <code>None</code> <code>y_add2</code> <code>ndarray</code> <p>A NumPy array representing the additional points to highlight in green. Defaults to None.</p> <code>None</code> <code>y_add_color</code> <code>str</code> <p>The color of the additional points. Defaults to \u201cblue\u201d.</p> <code>'blue'</code> <code>y_add2_color</code> <code>str</code> <p>The color of the additional points. Defaults to \u201cgreen\u201d.</p> <code>'green'</code> <code>y_best_label</code> <code>str</code> <p>The label for the best point. Defaults to \u201cBest\u201d.</p> <code>'Best'</code> <code>y_add_label</code> <code>str</code> <p>The label for the additional points. Defaults to \u201cAdd\u201d.</p> <code>'Add'</code> <code>y_add2_label</code> <code>str</code> <p>The label for the additional points. Defaults to \u201cAdd2\u201d.</p> <code>'Add2'</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \u201c\u201d (empty string).</p> <code>''</code> <code>y_orig</code> <code>ndarray</code> <p>The original target values with shape (n_samples, n_targets). Defaults to None.</p> <code>None</code> <code>pareto_front_orig</code> <code>bool</code> <p>If True, connect Pareto optimal points with a light blue line for y_orig. Defaults to False.</p> <code>False</code> <code>pareto_label</code> <code>bool</code> <p>If True, label Pareto points with their index. Defaults to False.</p> <code>False</code> <code>y_rf_color</code> <code>str</code> <p>The color of the predicted points. Defaults to \u201cblue\u201d.</p> <code>'blue'</code> <code>y_best_color</code> <code>str</code> <p>The color of the best point. Defaults to \u201cred\u201d.</p> <code>'red'</code> <code>x_axis_transformation</code> <code>str</code> <p>Transformation for the x-axis. Options are \u201cid\u201d (linear), \u201clog\u201d (logarithmic), and \u201cloglog\u201d (log-log). Defaults to \u201cid\u201d.</p> <code>'id'</code> <code>y_axis_transformation</code> <code>str</code> <p>Transformation for the y-axis. Options are \u201cid\u201d (linear), \u201clog\u201d (logarithmic), and \u201cloglog\u201d (log-log). Defaults to \u201cid\u201d.</p> <code>'id'</code> <code>filename</code> <code>str</code> <p>If provided, saves the plot to the specified file.  Supports \u201cpdf\u201d and \u201cpng\u201d formats. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Displays or saves the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.mo.plot import plot_mo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; target_names = [\"Target 1\", \"Target 2\"]\n&gt;&gt;&gt; combinations = [(0, 1)]\n&gt;&gt;&gt; pareto = \"min\"\n&gt;&gt;&gt; y_rf = np.random.rand(100, 2)\n&gt;&gt;&gt; y_orig = np.random.rand(100, 2)\n&gt;&gt;&gt; plot_mo(target_names, combinations, pareto, y_rf=y_rf, y_orig=y_orig, filename=\"plot.png\")\n</code></pre> Source code in <code>spotpython/mo/plot.py</code> <pre><code>def plot_mo(\n    target_names: list,\n    combinations: list,\n    pareto: str,\n    y_rf: np.ndarray = None,\n    pareto_front: bool = False,\n    y_best: np.ndarray = None,\n    y_add: np.ndarray = None,\n    y_add2: np.ndarray = None,\n    y_add_color=\"blue\",\n    y_add2_color=\"green\",\n    title: str = \"\",\n    y_orig: np.ndarray = None,\n    pareto_front_orig: bool = False,\n    pareto_label: bool = False,\n    y_rf_color=\"blue\",\n    y_best_color=\"red\",\n    x_axis_transformation: str = \"id\",\n    y_axis_transformation: str = \"id\",\n    y_best_label=\"Best\",\n    y_add_label=\"Add\",\n    y_add2_label=\"Add2\",\n    filename: str = None,\n) -&gt; None:\n    \"\"\"\n    Generates scatter plots for each combination of two targets from a multi-output prediction while highlighting Pareto optimal points.\n\n    Args:\n        y_rf (np.ndarray): The predicted target values with shape (n_samples, n_targets).\n        target_names (list): A list of target names corresponding to the columns of y_rf.\n        combinations (list): A list of tuples, where each tuple contains the indices of the target combinations to plot.\n        pareto (str): Specifies whether to compute Pareto front based on 'min' or 'max' criterion.\n        pareto_front (bool): If True, connect Pareto optimal points with a red line for y_rf.\n        y_best (np.ndarray, optional): A NumPy array representing the best point to highlight in red. Defaults to None.\n        y_add (np.ndarray, optional): A NumPy array representing the additional points to highlight in blue. Defaults to None.\n        y_add2 (np.ndarray, optional): A NumPy array representing the additional points to highlight in green. Defaults to None.\n        y_add_color (str): The color of the additional points. Defaults to \"blue\".\n        y_add2_color (str): The color of the additional points. Defaults to \"green\".\n        y_best_label (str): The label for the best point. Defaults to \"Best\".\n        y_add_label (str): The label for the additional points. Defaults to \"Add\".\n        y_add2_label (str): The label for the additional points. Defaults to \"Add2\".\n        title (str): The title of the plot. Defaults to \"\" (empty string).\n        y_orig (np.ndarray, optional): The original target values with shape (n_samples, n_targets). Defaults to None.\n        pareto_front_orig (bool): If True, connect Pareto optimal points with a light blue line for y_orig. Defaults to False.\n        pareto_label (bool): If True, label Pareto points with their index. Defaults to False.\n        y_rf_color (str): The color of the predicted points. Defaults to \"blue\".\n        y_best_color (str): The color of the best point. Defaults to \"red\".\n        x_axis_transformation (str): Transformation for the x-axis. Options are \"id\" (linear), \"log\" (logarithmic), and \"loglog\" (log-log). Defaults to \"id\".\n        y_axis_transformation (str): Transformation for the y-axis. Options are \"id\" (linear), \"log\" (logarithmic), and \"loglog\" (log-log). Defaults to \"id\".\n        filename (str, optional):\n            If provided, saves the plot to the specified file.  Supports \"pdf\" and \"png\" formats. Defaults to None.\n\n    Returns:\n        None: Displays or saves the plot.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.mo.plot import plot_mo\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; target_names = [\"Target 1\", \"Target 2\"]\n        &gt;&gt;&gt; combinations = [(0, 1)]\n        &gt;&gt;&gt; pareto = \"min\"\n        &gt;&gt;&gt; y_rf = np.random.rand(100, 2)\n        &gt;&gt;&gt; y_orig = np.random.rand(100, 2)\n        &gt;&gt;&gt; plot_mo(target_names, combinations, pareto, y_rf=y_rf, y_orig=y_orig, filename=\"plot.png\")\n    \"\"\"\n    # Convert y_rf to numpy array if it's a pandas DataFrame\n    if isinstance(y_rf, pd.DataFrame):\n        y_rf = y_rf.values\n\n    # Convert y_orig to numpy array if it's a pandas DataFrame\n    if isinstance(y_orig, pd.DataFrame):\n        y_orig = y_orig.values\n\n    for i, j in combinations:\n        plt.figure()\n        s = 50  # Base size for points\n        pareto_size = s  # Size for Pareto points\n        if pareto_label:\n            pareto_size = s * 4  # Increase the size for Pareto points\n        a = 0.4\n\n        # Plot original data if provided\n        if y_orig is not None:\n            minimize = pareto == \"min\"\n            pareto_mask_orig = is_pareto_efficient(y_orig[:, [i, j]], minimize)\n            plt.scatter(y_orig[:, i], y_orig[:, j], edgecolor=\"w\", c=\"gray\", s=s, marker=\"o\", alpha=a, label=\"Non-Pareto Points\")\n            plt.scatter(y_orig[pareto_mask_orig, i], y_orig[pareto_mask_orig, j], edgecolor=\"k\", c=\"gray\", s=pareto_size, marker=\"o\", alpha=a, label=\"Pareto Points\")\n            if pareto_label:\n                for idx in np.where(pareto_mask_orig)[0]:\n                    plt.text(y_orig[idx, i], y_orig[idx, j], str(idx), color=\"black\", fontsize=8, ha=\"center\", va=\"center\")\n            if pareto_front_orig:\n                sorted_indices_orig = np.argsort(y_orig[pareto_mask_orig, i])\n                plt.plot(y_orig[pareto_mask_orig, i][sorted_indices_orig], y_orig[pareto_mask_orig, j][sorted_indices_orig], \"k-\", alpha=a, label=\"Pareto Front\")\n\n        if y_rf is not None:\n            minimize = pareto == \"min\"\n            pareto_mask = is_pareto_efficient(y_rf[:, [i, j]], minimize)\n            plt.scatter(y_rf[:, i], y_rf[:, j], edgecolor=\"w\", c=y_rf_color, s=s, marker=\"^\", alpha=a, label=\"Predicted Points\")\n            plt.scatter(y_rf[pareto_mask, i], y_rf[pareto_mask, j], edgecolor=\"k\", c=y_rf_color, s=pareto_size, marker=\"s\", alpha=a, label=\"Predicted Pareto\")\n            if pareto_label:\n                for idx in np.where(pareto_mask)[0]:\n                    plt.text(y_rf[idx, i], y_rf[idx, j], str(idx), color=\"black\", fontsize=8, ha=\"center\", va=\"center\")\n            if pareto_front:\n                sorted_indices = np.argsort(y_rf[pareto_mask, i])\n                plt.plot(\n                    y_rf[pareto_mask, i][sorted_indices],\n                    y_rf[pareto_mask, j][sorted_indices],\n                    linestyle=\"-\",\n                    color=y_rf_color,\n                    alpha=a,\n                    label=\"Predicted Pareto Front\",\n                )\n\n        if y_best is not None:\n            plt.scatter(y_best[:, i], y_best[:, j], edgecolor=\"k\", c=y_best_color, s=s, marker=\"D\", alpha=1, label=y_best_label)\n        if y_add is not None:\n            plt.scatter(y_add[:, i], y_add[:, j], edgecolor=\"k\", c=y_add_color, s=s, marker=\"D\", alpha=1, label=y_add_label)\n        if y_add2 is not None:\n            plt.scatter(y_add2[:, i], y_add2[:, j], edgecolor=\"k\", c=y_add2_color, s=s, marker=\"D\", alpha=1, label=y_add2_label)\n\n        # Apply axis transformations\n        if x_axis_transformation == \"log\":\n            plt.xscale(\"log\")\n        if y_axis_transformation == \"log\":\n            plt.yscale(\"log\")\n        if x_axis_transformation == \"loglog\" or y_axis_transformation == \"loglog\":\n            plt.xscale(\"log\")\n            plt.yscale(\"log\")\n\n        plt.xlabel(target_names[i])\n        plt.ylabel(target_names[j])\n        plt.grid()\n        plt.title(title)\n        # Move the legend outside the plot\n        plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n        # Save or show the plot\n        if filename:\n            if filename.endswith(\".pdf\") or filename.endswith(\".png\"):\n                plt.savefig(filename, format=filename.split(\".\")[-1], bbox_inches=\"tight\")\n            else:\n                raise ValueError(\"Filename must have a valid suffix: '.pdf' or '.png'.\")\n        else:\n            plt.show()\n</code></pre>"},{"location":"reference/spotpython/pinns/solvers/","title":"solvers","text":""},{"location":"reference/spotpython/pinns/solvers/#spotpython.pinns.solvers.oscillatorb","title":"<code>oscillatorb(n_steps=3000, t_min=0.0, t_max=30.0, y0=1.0, alpha=0.1, omega=np.pi / 2)</code>","text":"<p>Solves the first-order ODE y\u2019 = -alphay + sin(omegat) using a two-stage explicit Runge-Kutta method as described in the reference. The ODE represents a damped harmonic oscillator with a sine forcing term.</p> <p>The specific numerical scheme used is: 1. y_intermediate = y_current + (t_step/2) * f(t_current + t_step/2, y_current) 2. y_next = y_current + t_step * f(t_current + t_step, y_intermediate) where f(t,y) = -alphay + sin(omegat). This is a second-order method.</p> <p>Parameters:</p> Name Type Description Default <code>n_steps</code> <code>int</code> <p>Number of time points in the discretized time domain, including the initial point. Defaults to 3000.</p> <code>3000</code> <code>t_min</code> <code>float</code> <p>Initial time. Defaults to 0.0.</p> <code>0.0</code> <code>t_max</code> <code>float</code> <p>Defines the nominal end of the time interval. The time step is calculated as (t_max - t_min) / n_steps. The actual last time point will be t_min + (n_steps - 1) * t_step. Defaults to 30.0.</p> <code>30.0</code> <code>y0</code> <code>float</code> <p>Initial condition for y at t_min. Defaults to 1.0.</p> <code>1.0</code> <code>alpha</code> <code>float</code> <p>Damping coefficient in the ODE. Defaults to 0.1.</p> <code>0.1</code> <code>omega</code> <code>float</code> <p>Angular frequency for the sine forcing term in the ODE. Defaults to np.pi / 2.</p> <code>pi / 2</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor]: A tuple containing two PyTorch tensors: - t_points_tensor: Tensor of time points, shape (n_steps, 1). - y_values_tensor: Tensor of corresponding y values, shape (n_steps, 1).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.pinns.solvers import oscillatorb\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; t_vals, y_vals = oscillatorb(n_steps=100, t_min=0.0, t_max=10.0, y0=1.0, alpha=0.1, omega=np.pi/2)\n&gt;&gt;&gt; print(t_vals.shape, y_vals.shape)\ntorch.Size([100, 1]) torch.Size([100, 1])\n&gt;&gt;&gt; print(f\"Initial t: {t_vals[0].item():.2f}, Initial y: {y_vals[0].item():.2f}\")\nInitial t: 0.00, Initial y: 1.00\n&gt;&gt;&gt; # Last t will be t_max - t_step for this configuration\n&gt;&gt;&gt; # t_step = (10.0 - 0.0) / 100 = 0.1\n&gt;&gt;&gt; # Last t = 0.0 + (100-1)*0.1 = 9.9\n&gt;&gt;&gt; print(f\"Final t: {t_vals[-1].item():.2f}, Final y: {y_vals[-1].item():.2f}\")\nFinal t: 9.90, Final y: ...\n</code></pre> References <ul> <li>Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests. Baty, Hubert and Baty, Leo. April 2023.</li> </ul> Source code in <code>spotpython/pinns/solvers.py</code> <pre><code>def oscillatorb(n_steps: int = 3000, t_min: float = 0.0, t_max: float = 30.0, y0: float = 1.0, alpha: float = 0.1, omega: float = np.pi / 2) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Solves the first-order ODE y' = -alpha*y + sin(omega*t) using a\n    two-stage explicit Runge-Kutta method as described in the reference.\n    The ODE represents a damped harmonic oscillator with a sine forcing term.\n\n    The specific numerical scheme used is:\n    1. y_intermediate = y_current + (t_step/2) * f(t_current + t_step/2, y_current)\n    2. y_next = y_current + t_step * f(t_current + t_step, y_intermediate)\n    where f(t,y) = -alpha*y + sin(omega*t). This is a second-order method.\n\n    Args:\n        n_steps (int): Number of time points in the discretized time domain,\n            including the initial point. Defaults to 3000.\n        t_min (float): Initial time. Defaults to 0.0.\n        t_max (float): Defines the nominal end of the time interval. The time step\n            is calculated as (t_max - t_min) / n_steps. The actual last\n            time point will be t_min + (n_steps - 1) * t_step. Defaults to 30.0.\n        y0 (float): Initial condition for y at t_min. Defaults to 1.0.\n        alpha (float): Damping coefficient in the ODE. Defaults to 0.1.\n        omega (float): Angular frequency for the sine forcing term in the ODE.\n            Defaults to np.pi / 2.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: A tuple containing two PyTorch tensors:\n            - t_points_tensor: Tensor of time points, shape (n_steps, 1).\n            - y_values_tensor: Tensor of corresponding y values, shape (n_steps, 1).\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.pinns.solvers import oscillatorb\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; t_vals, y_vals = oscillatorb(n_steps=100, t_min=0.0, t_max=10.0, y0=1.0, alpha=0.1, omega=np.pi/2)\n        &gt;&gt;&gt; print(t_vals.shape, y_vals.shape)\n        torch.Size([100, 1]) torch.Size([100, 1])\n        &gt;&gt;&gt; print(f\"Initial t: {t_vals[0].item():.2f}, Initial y: {y_vals[0].item():.2f}\")\n        Initial t: 0.00, Initial y: 1.00\n        &gt;&gt;&gt; # Last t will be t_max - t_step for this configuration\n        &gt;&gt;&gt; # t_step = (10.0 - 0.0) / 100 = 0.1\n        &gt;&gt;&gt; # Last t = 0.0 + (100-1)*0.1 = 9.9\n        &gt;&gt;&gt; print(f\"Final t: {t_vals[-1].item():.2f}, Final y: {y_vals[-1].item():.2f}\")\n        Final t: 9.90, Final y: ...\n\n    References:\n        - Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests. Baty, Hubert and Baty, Leo. April 2023.\n    \"\"\"\n    t_step = (t_max - t_min) / n_steps  # Time step\n    # Time points: t_min, t_min + t_step, ..., t_min + (n_steps-1)*t_step\n    t_points = np.arange(t_min, t_min + n_steps * t_step, t_step)[:n_steps]\n\n    y = [y0]  # List to store y values, starting with initial condition\n\n    # Solve for the time evolution\n    # t_points[0] corresponds to y0. Loop starts from t_points[1].\n    for t_current_step_end in t_points[1:]:\n        # t_midpoint is the midpoint of the current integration interval\n        # Interval: [t_current_step_end - t_step, t_current_step_end]\n        # Midpoint: (t_current_step_end - t_step) + t_step/2 = t_current_step_end - t_step/2\n        t_midpoint = t_current_step_end - t_step / 2.0\n        # y_prev is the last computed value of y\n        y_prev = y[-1]\n\n        # Stage 1: Calculate intermediate y value (y_intermediate)\n        # Uses slope at t_midpoint, with y_prev\n        # f(t,y) = -alpha*y + sin(omega*t)\n        slope_at_t_mid_using_y_prev = -alpha * y_prev + np.sin(omega * t_midpoint)\n        y_intermediate = y_prev + (t_step / 2.0) * slope_at_t_mid_using_y_prev\n\n        # Stage 2: Calculate y at t_current_step_end\n        # Uses slope at t_current_step_end, with y_intermediate\n        slope_at_t_end_using_y_intermediate = -alpha * y_intermediate + np.sin(omega * t_current_step_end)\n        y_next = y_prev + t_step * slope_at_t_end_using_y_intermediate\n        y.append(y_next)\n\n    t_points_tensor = torch.tensor(t_points, dtype=torch.float32).view(-1, 1)\n    y_values_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n    return t_points_tensor, y_values_tensor\n</code></pre>"},{"location":"reference/spotpython/pinns/nn/fcn/","title":"fcn","text":""},{"location":"reference/spotpython/pinns/nn/fcn/#spotpython.pinns.nn.fcn.FCN","title":"<code>FCN</code>","text":"<p>               Bases: <code>Module</code></p> <p>A Fully Connected Network (FCN).</p> <p>This network consists of an input layer, a specified number of hidden layers, and an output layer. All hidden layers use the Tanh activation function.</p> <p>Attributes:</p> Name Type Description <code>fcs</code> <code>Sequential</code> <p>Sequential container for the first linear layer                  (input to hidden) and its activation.</p> <code>fch</code> <code>Sequential</code> <p>Sequential container for the hidden layers. Each hidden                  layer consists of a linear transformation and an activation.</p> <code>fce</code> <code>Linear</code> <p>The final linear layer (hidden to output).</p> References <ul> <li>Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests. Baty, Hubert and Baty, Leo. April 2023.</li> </ul> Source code in <code>spotpython/pinns/nn/fcn.py</code> <pre><code>class FCN(nn.Module):\n    \"\"\"A Fully Connected Network (FCN).\n\n    This network consists of an input layer, a specified number of hidden layers,\n    and an output layer. All hidden layers use the Tanh activation function.\n\n    Attributes:\n        fcs (nn.Sequential): Sequential container for the first linear layer\n                             (input to hidden) and its activation.\n        fch (nn.Sequential): Sequential container for the hidden layers. Each hidden\n                             layer consists of a linear transformation and an activation.\n        fce (nn.Linear): The final linear layer (hidden to output).\n\n    References:\n        - Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests. Baty, Hubert and Baty, Leo. April 2023.\n    \"\"\"\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int, N_HIDDEN: int, N_LAYERS: int):\n        \"\"\"Initializes the FCN.\n\n        Args:\n            N_INPUT (int): The number of input features.\n            N_OUTPUT (int): The number of output features.\n            N_HIDDEN (int): The number of neurons in each hidden layer.\n            N_LAYERS (int): The total number of layers, including the input layer\n                            (which is N_INPUT -&gt; N_HIDDEN), hidden layers, but\n                            not counting the final output layer transformation.\n                            A N_LAYERS=1 means only input to hidden, then hidden to output.\n                            A N_LAYERS=2 means input to hidden, one hidden to hidden, then hidden to output.\n                            The number of hidden-to-hidden layers is N_LAYERS - 1.\n                            If N_LAYERS is 1, there are no fch layers.\n\n        Examples:\n            &gt;&gt;&gt; # Example of creating a FCN\n            &gt;&gt;&gt; from spotpython.pinns.nn.fcn import FCN\n            &gt;&gt;&gt; model = FCN(N_INPUT=1, N_OUTPUT=1, N_HIDDEN=10, N_LAYERS=3)\n            &gt;&gt;&gt; print(model)\n            FCN(\n              (fcs): Sequential(\n                (0): Linear(in_features=1, out_features=10, bias=True)\n                (1): Tanh()\n              )\n              (fch): Sequential(\n                (0): Sequential(\n                  (0): Linear(in_features=10, out_features=10, bias=True)\n                  (1): Tanh()\n                )\n                (1): Sequential(\n                  (0): Linear(in_features=10, out_features=10, bias=True)\n                  (1): Tanh()\n                )\n              )\n              (fce): Linear(in_features=10, out_features=1, bias=True)\n            )\n            &gt;&gt;&gt; # Example of a forward pass\n            &gt;&gt;&gt; input_tensor = torch.randn(5, 1) # Batch of 5, 1 input feature\n            &gt;&gt;&gt; output_tensor = model(input_tensor)\n            &gt;&gt;&gt; print(output_tensor.shape)\n            torch.Size([5, 1])\n\n            &gt;&gt;&gt; # Example with N_LAYERS = 1 (no hidden-to-hidden layers)\n            &gt;&gt;&gt; model_simple = FCN(N_INPUT=2, N_OUTPUT=1, N_HIDDEN=5, N_LAYERS=1)\n            &gt;&gt;&gt; print(model_simple)\n            FCN(\n              (fcs): Sequential(\n                (0): Linear(in_features=2, out_features=5, bias=True)\n                (1): Tanh()\n              )\n              (fch): Sequential()\n              (fce): Linear(in_features=5, out_features=1, bias=True)\n            )\n        \"\"\"\n        super().__init__()\n        activation = nn.Tanh\n\n        # Input layer: N_INPUT -&gt; N_HIDDEN\n        self.fcs = nn.Sequential(nn.Linear(N_INPUT, N_HIDDEN), activation())\n\n        # Hidden layers: N_HIDDEN -&gt; N_HIDDEN, (N_LAYERS - 1) times\n        # If N_LAYERS is 1, range(N_LAYERS-1) is range(0), so fch will be empty.\n        hidden_layers = []\n        if N_LAYERS &gt; 1:\n            for _ in range(N_LAYERS - 1):\n                hidden_layers.append(nn.Sequential(nn.Linear(N_HIDDEN, N_HIDDEN), activation()))\n        self.fch = nn.Sequential(*hidden_layers)\n\n        # Output layer: N_HIDDEN -&gt; N_OUTPUT\n        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Performs the forward pass of the network.\n\n        Args:\n            x (torch.Tensor): The input tensor. Shape (batch_size, N_INPUT).\n\n        Returns:\n            torch.Tensor: The output tensor. Shape (batch_size, N_OUTPUT).\n        \"\"\"\n        x = self.fcs(x)\n        x = self.fch(x)\n        x = self.fce(x)\n        return x\n</code></pre>"},{"location":"reference/spotpython/pinns/nn/fcn/#spotpython.pinns.nn.fcn.FCN.__init__","title":"<code>__init__(N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS)</code>","text":"<p>Initializes the FCN.</p> <p>Parameters:</p> Name Type Description Default <code>N_INPUT</code> <code>int</code> <p>The number of input features.</p> required <code>N_OUTPUT</code> <code>int</code> <p>The number of output features.</p> required <code>N_HIDDEN</code> <code>int</code> <p>The number of neurons in each hidden layer.</p> required <code>N_LAYERS</code> <code>int</code> <p>The total number of layers, including the input layer             (which is N_INPUT -&gt; N_HIDDEN), hidden layers, but             not counting the final output layer transformation.             A N_LAYERS=1 means only input to hidden, then hidden to output.             A N_LAYERS=2 means input to hidden, one hidden to hidden, then hidden to output.             The number of hidden-to-hidden layers is N_LAYERS - 1.             If N_LAYERS is 1, there are no fch layers.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Example of creating a FCN\n&gt;&gt;&gt; from spotpython.pinns.nn.fcn import FCN\n&gt;&gt;&gt; model = FCN(N_INPUT=1, N_OUTPUT=1, N_HIDDEN=10, N_LAYERS=3)\n&gt;&gt;&gt; print(model)\nFCN(\n  (fcs): Sequential(\n    (0): Linear(in_features=1, out_features=10, bias=True)\n    (1): Tanh()\n  )\n  (fch): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=10, out_features=10, bias=True)\n      (1): Tanh()\n    )\n    (1): Sequential(\n      (0): Linear(in_features=10, out_features=10, bias=True)\n      (1): Tanh()\n    )\n  )\n  (fce): Linear(in_features=10, out_features=1, bias=True)\n)\n&gt;&gt;&gt; # Example of a forward pass\n&gt;&gt;&gt; input_tensor = torch.randn(5, 1) # Batch of 5, 1 input feature\n&gt;&gt;&gt; output_tensor = model(input_tensor)\n&gt;&gt;&gt; print(output_tensor.shape)\ntorch.Size([5, 1])\n</code></pre> <pre><code>&gt;&gt;&gt; # Example with N_LAYERS = 1 (no hidden-to-hidden layers)\n&gt;&gt;&gt; model_simple = FCN(N_INPUT=2, N_OUTPUT=1, N_HIDDEN=5, N_LAYERS=1)\n&gt;&gt;&gt; print(model_simple)\nFCN(\n  (fcs): Sequential(\n    (0): Linear(in_features=2, out_features=5, bias=True)\n    (1): Tanh()\n  )\n  (fch): Sequential()\n  (fce): Linear(in_features=5, out_features=1, bias=True)\n)\n</code></pre> Source code in <code>spotpython/pinns/nn/fcn.py</code> <pre><code>def __init__(self, N_INPUT: int, N_OUTPUT: int, N_HIDDEN: int, N_LAYERS: int):\n    \"\"\"Initializes the FCN.\n\n    Args:\n        N_INPUT (int): The number of input features.\n        N_OUTPUT (int): The number of output features.\n        N_HIDDEN (int): The number of neurons in each hidden layer.\n        N_LAYERS (int): The total number of layers, including the input layer\n                        (which is N_INPUT -&gt; N_HIDDEN), hidden layers, but\n                        not counting the final output layer transformation.\n                        A N_LAYERS=1 means only input to hidden, then hidden to output.\n                        A N_LAYERS=2 means input to hidden, one hidden to hidden, then hidden to output.\n                        The number of hidden-to-hidden layers is N_LAYERS - 1.\n                        If N_LAYERS is 1, there are no fch layers.\n\n    Examples:\n        &gt;&gt;&gt; # Example of creating a FCN\n        &gt;&gt;&gt; from spotpython.pinns.nn.fcn import FCN\n        &gt;&gt;&gt; model = FCN(N_INPUT=1, N_OUTPUT=1, N_HIDDEN=10, N_LAYERS=3)\n        &gt;&gt;&gt; print(model)\n        FCN(\n          (fcs): Sequential(\n            (0): Linear(in_features=1, out_features=10, bias=True)\n            (1): Tanh()\n          )\n          (fch): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=10, out_features=10, bias=True)\n              (1): Tanh()\n            )\n            (1): Sequential(\n              (0): Linear(in_features=10, out_features=10, bias=True)\n              (1): Tanh()\n            )\n          )\n          (fce): Linear(in_features=10, out_features=1, bias=True)\n        )\n        &gt;&gt;&gt; # Example of a forward pass\n        &gt;&gt;&gt; input_tensor = torch.randn(5, 1) # Batch of 5, 1 input feature\n        &gt;&gt;&gt; output_tensor = model(input_tensor)\n        &gt;&gt;&gt; print(output_tensor.shape)\n        torch.Size([5, 1])\n\n        &gt;&gt;&gt; # Example with N_LAYERS = 1 (no hidden-to-hidden layers)\n        &gt;&gt;&gt; model_simple = FCN(N_INPUT=2, N_OUTPUT=1, N_HIDDEN=5, N_LAYERS=1)\n        &gt;&gt;&gt; print(model_simple)\n        FCN(\n          (fcs): Sequential(\n            (0): Linear(in_features=2, out_features=5, bias=True)\n            (1): Tanh()\n          )\n          (fch): Sequential()\n          (fce): Linear(in_features=5, out_features=1, bias=True)\n        )\n    \"\"\"\n    super().__init__()\n    activation = nn.Tanh\n\n    # Input layer: N_INPUT -&gt; N_HIDDEN\n    self.fcs = nn.Sequential(nn.Linear(N_INPUT, N_HIDDEN), activation())\n\n    # Hidden layers: N_HIDDEN -&gt; N_HIDDEN, (N_LAYERS - 1) times\n    # If N_LAYERS is 1, range(N_LAYERS-1) is range(0), so fch will be empty.\n    hidden_layers = []\n    if N_LAYERS &gt; 1:\n        for _ in range(N_LAYERS - 1):\n            hidden_layers.append(nn.Sequential(nn.Linear(N_HIDDEN, N_HIDDEN), activation()))\n    self.fch = nn.Sequential(*hidden_layers)\n\n    # Output layer: N_HIDDEN -&gt; N_OUTPUT\n    self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)\n</code></pre>"},{"location":"reference/spotpython/pinns/nn/fcn/#spotpython.pinns.nn.fcn.FCN.forward","title":"<code>forward(x)</code>","text":"<p>Performs the forward pass of the network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor. Shape (batch_size, N_INPUT).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output tensor. Shape (batch_size, N_OUTPUT).</p> Source code in <code>spotpython/pinns/nn/fcn.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Performs the forward pass of the network.\n\n    Args:\n        x (torch.Tensor): The input tensor. Shape (batch_size, N_INPUT).\n\n    Returns:\n        torch.Tensor: The output tensor. Shape (batch_size, N_OUTPUT).\n    \"\"\"\n    x = self.fcs(x)\n    x = self.fch(x)\n    x = self.fce(x)\n    return x\n</code></pre>"},{"location":"reference/spotpython/pinns/plot/result/","title":"result","text":""},{"location":"reference/spotpython/pinns/plot/result/#spotpython.pinns.plot.result.plot_result","title":"<code>plot_result(x, y, x_data, y_data, yh, current_step, xp=None, figure_size=(8, 4), xlims=(-1.25, 31.05), ylims=(-0.65, 2.25), show_plot=True, save_path=None)</code>","text":"<p>Plots the results of a PINN training, comparing predictions with exact solutions.</p> <p>Displays the neural network\u2019s prediction, the exact solution, training data points, and optionally, collocation points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Tensor, List[float], ndarray]</code> <p>The x-coordinates for the continuous plots (e.g., time points).</p> required <code>y</code> <code>Union[Tensor, List[float], ndarray]</code> <p>The y-coordinates of the exact solution corresponding to <code>x</code>.</p> required <code>x_data</code> <code>Union[Tensor, List[float], ndarray]</code> <p>The x-coordinates of the training data points.</p> required <code>y_data</code> <code>Union[Tensor, List[float], ndarray]</code> <p>The y-coordinates of the training data points.</p> required <code>yh</code> <code>Union[Tensor, List[float], ndarray]</code> <p>The y-coordinates of the neural network\u2019s prediction corresponding to <code>x</code>.</p> required <code>current_step</code> <code>int</code> <p>The current training step or epoch number to display on the plot.</p> required <code>xp</code> <code>Optional[Union[Tensor, List[float], ndarray]]</code> <p>The x-coordinates of the collocation points. If None, these are not plotted. Defaults to None.</p> <code>None</code> <code>figure_size</code> <code>tuple</code> <p>Size of the matplotlib figure. Defaults to (8, 4).</p> <code>(8, 4)</code> <code>xlims</code> <code>Optional[tuple]</code> <p>Tuple defining the x-axis limits. If None, matplotlib\u2019s default is used. Defaults to (-1.25, 31.05).</p> <code>(-1.25, 31.05)</code> <code>ylims</code> <code>Optional[tuple]</code> <p>Tuple defining the y-axis limits. If None, matplotlib\u2019s default is used. Defaults to (-0.65, 2.25).</p> <code>(-0.65, 2.25)</code> <code>show_plot</code> <code>bool</code> <p>Whether to display the plot using <code>plt.show()</code>. Defaults to True.</p> <code>True</code> <code>save_path</code> <code>Optional[str]</code> <p>If provided, the path to save the figure to. If None, the figure is not saved. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.pinns.plot.result import plot_result\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Generate some dummy data\n&gt;&gt;&gt; x_plot = torch.linspace(0, 30, 100)\n&gt;&gt;&gt; y_exact = torch.sin(x_plot / 5)\n&gt;&gt;&gt; y_pred = torch.sin(x_plot / 5 + 0.1) # Slightly off prediction\n&gt;&gt;&gt; x_train = torch.rand(10) * 30\n&gt;&gt;&gt; y_train = torch.sin(x_train / 5)\n&gt;&gt;&gt; collocation_points = torch.rand(50) * 30\n&gt;&gt;&gt; current_training_step = 1000\n&gt;&gt;&gt; # plot_result( # This would show a plot if run in an interactive environment\n... #     x_plot, y_exact, x_train, y_train, y_pred,\n... #     current_training_step, xp=collocation_points,\n... #     show_plot=False, save_path=\"temp_plot.png\"\n... # )\n&gt;&gt;&gt; # To avoid actual plotting in doctest, we'll just confirm it runs\n&gt;&gt;&gt; try:\n...     plot_result(\n...         x_plot.numpy(), y_exact.numpy(), x_train.numpy(), y_train.numpy(), y_pred.numpy(),\n...         current_training_step, xp=collocation_points.numpy(),\n...         show_plot=False\n...     )\n... except Exception as e:\n...     print(f\"Plotting failed: {e}\")\n</code></pre> Note <p>If using PyTorch tensors as input, they will be detached and moved to CPU before plotting. Consider converting to NumPy arrays beforehand if preferred.</p> References <ul> <li>Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests. Baty, Hubert and Baty, Leo. April 2023.</li> </ul> Source code in <code>spotpython/pinns/plot/result.py</code> <pre><code>def plot_result(\n    x: Union[torch.Tensor, List[float], \"np.ndarray\"],\n    y: Union[torch.Tensor, List[float], \"np.ndarray\"],\n    x_data: Union[torch.Tensor, List[float], \"np.ndarray\"],\n    y_data: Union[torch.Tensor, List[float], \"np.ndarray\"],\n    yh: Union[torch.Tensor, List[float], \"np.ndarray\"],\n    current_step: int,\n    xp: Optional[Union[torch.Tensor, List[float], \"np.ndarray\"]] = None,\n    figure_size: tuple = (8, 4),\n    xlims: Optional[tuple] = (-1.25, 31.05),\n    ylims: Optional[tuple] = (-0.65, 2.25),\n    show_plot: bool = True,\n    save_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Plots the results of a PINN training, comparing predictions with exact solutions.\n\n    Displays the neural network's prediction, the exact solution, training data points,\n    and optionally, collocation points.\n\n    Args:\n        x (Union[torch.Tensor, List[float], \"np.ndarray\"]):\n            The x-coordinates for the continuous plots (e.g., time points).\n        y (Union[torch.Tensor, List[float], \"np.ndarray\"]):\n            The y-coordinates of the exact solution corresponding to `x`.\n        x_data (Union[torch.Tensor, List[float], \"np.ndarray\"]):\n            The x-coordinates of the training data points.\n        y_data (Union[torch.Tensor, List[float], \"np.ndarray\"]):\n            The y-coordinates of the training data points.\n        yh (Union[torch.Tensor, List[float], \"np.ndarray\"]):\n            The y-coordinates of the neural network's prediction corresponding to `x`.\n        current_step (int):\n            The current training step or epoch number to display on the plot.\n        xp (Optional[Union[torch.Tensor, List[float], \"np.ndarray\"]], optional):\n            The x-coordinates of the collocation points. If None, these are not plotted.\n            Defaults to None.\n        figure_size (tuple, optional):\n            Size of the matplotlib figure. Defaults to (8, 4).\n        xlims (Optional[tuple], optional):\n            Tuple defining the x-axis limits. If None, matplotlib's default is used.\n            Defaults to (-1.25, 31.05).\n        ylims (Optional[tuple], optional):\n            Tuple defining the y-axis limits. If None, matplotlib's default is used.\n            Defaults to (-0.65, 2.25).\n        show_plot (bool, optional):\n            Whether to display the plot using `plt.show()`. Defaults to True.\n        save_path (Optional[str], optional):\n            If provided, the path to save the figure to. If None, the figure is not saved.\n            Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.pinns.plot.result import plot_result\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; # Generate some dummy data\n        &gt;&gt;&gt; x_plot = torch.linspace(0, 30, 100)\n        &gt;&gt;&gt; y_exact = torch.sin(x_plot / 5)\n        &gt;&gt;&gt; y_pred = torch.sin(x_plot / 5 + 0.1) # Slightly off prediction\n        &gt;&gt;&gt; x_train = torch.rand(10) * 30\n        &gt;&gt;&gt; y_train = torch.sin(x_train / 5)\n        &gt;&gt;&gt; collocation_points = torch.rand(50) * 30\n        &gt;&gt;&gt; current_training_step = 1000\n        &gt;&gt;&gt; # plot_result( # This would show a plot if run in an interactive environment\n        ... #     x_plot, y_exact, x_train, y_train, y_pred,\n        ... #     current_training_step, xp=collocation_points,\n        ... #     show_plot=False, save_path=\"temp_plot.png\"\n        ... # )\n        &gt;&gt;&gt; # To avoid actual plotting in doctest, we'll just confirm it runs\n        &gt;&gt;&gt; try:\n        ...     plot_result(\n        ...         x_plot.numpy(), y_exact.numpy(), x_train.numpy(), y_train.numpy(), y_pred.numpy(),\n        ...         current_training_step, xp=collocation_points.numpy(),\n        ...         show_plot=False\n        ...     )\n        ... except Exception as e:\n        ...     print(f\"Plotting failed: {e}\")\n\n    Note:\n        If using PyTorch tensors as input, they will be detached and moved to CPU\n        before plotting. Consider converting to NumPy arrays beforehand if preferred.\n\n    References:\n        - Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests. Baty, Hubert and Baty, Leo. April 2023.\n    \"\"\"\n\n    # Convert tensors to numpy arrays for plotting if they are tensors\n    def to_numpy(data):\n        if isinstance(data, torch.Tensor):\n            return data.detach().cpu().numpy()\n        return data\n\n    x_np = to_numpy(x)\n    y_np = to_numpy(y)\n    x_data_np = to_numpy(x_data)\n    y_data_np = to_numpy(y_data)\n    yh_np = to_numpy(yh)\n    if xp is not None:\n        xp_np = to_numpy(xp)\n    else:\n        xp_np = None\n\n    plt.figure(figsize=figure_size)\n    plt.plot(x_np, yh_np, color=\"tab:red\", linewidth=2, alpha=0.8, label=\"NN prediction\")\n    plt.plot(x_np, y_np, color=\"blue\", linewidth=2, alpha=0.8, linestyle=\"--\", label=\"Exact solution\")\n    plt.scatter(x_data_np, y_data_np, s=60, color=\"tab:red\", alpha=0.4, label=\"Training data\")\n\n    if xp_np is not None:\n        # Create y-values for collocation points at y=0 or a specified level\n        # Original code used -0*torch.ones_like(xp), which is just zeros.\n        xp_y_values = np.zeros_like(xp_np)\n        plt.scatter(xp_np, xp_y_values, s=30, color=\"tab:green\", alpha=0.4, label=\"Collocation points\")\n\n    legend_handle = plt.legend(loc=(0.67, 0.62), frameon=False, fontsize=\"large\")\n    plt.setp(legend_handle.get_texts(), color=\"k\")\n\n    if xlims:\n        plt.xlim(xlims)\n    if ylims:\n        plt.ylim(ylims)\n\n    plt.text(0.05, 0.95, f\"Training step: {current_step}\", fontsize=\"xx-large\", color=\"k\", transform=plt.gca().transAxes, ha=\"left\", va=\"top\")\n\n    plt.ylabel(\"y\", fontsize=\"xx-large\")\n    plt.xlabel(\"Time\", fontsize=\"xx-large\")\n    plt.axis(\"on\")\n    plt.grid(True, linestyle=\"--\", alpha=0.7)\n\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n\n    if show_plot:\n        plt.show()\n    else:\n        plt.close()  # Close the figure if not shown to free memory\n</code></pre>"},{"location":"reference/spotpython/plot/contour/","title":"contour","text":""},{"location":"reference/spotpython/plot/contour/#spotpython.plot.contour.contour_plot","title":"<code>contour_plot(data, x_col, y_col, z_col, facet_col=None, aspect=1, as_table=True, figsize=(4, 4), levels=10, cmap='viridis', highlight_point=None, highlight_color='red', highlight_size=50, highlight_label=None, highlight_legend_loc='upper right', highlight_legend_fontsize=8)</code>","text":"<p>Creates contour plots (single or faceted) using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data.</p> required <code>x_col</code> <code>str</code> <p>The name of the column to use for the x-axis.</p> required <code>y_col</code> <code>str</code> <p>The name of the column to use for the y-axis.</p> required <code>z_col</code> <code>str</code> <p>The name of the column to use for the z-axis (contour values).</p> required <code>facet_col</code> <code>str</code> <p>The name of the column to use for faceting (creating subplots). Defaults to None.</p> <code>None</code> <code>aspect</code> <code>float</code> <p>The aspect ratio of the plot. Defaults to 1.</p> <code>1</code> <code>as_table</code> <code>bool</code> <p>Whether to arrange facets as a table. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>The size of the figure. Defaults to (4, 4).</p> <code>(4, 4)</code> <code>levels</code> <code>int</code> <p>The number of contour levels. Defaults to 5.</p> <code>10</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cviridis\u201d.</p> <code>'viridis'</code> <code>highlight_point</code> <code>array</code> <p>A 1-dimensional array specifying a single point [x, y] to highlight. Defaults to None.</p> <code>None</code> <code>highlight_color</code> <code>str</code> <p>Color for the highlighted point. Defaults to \u201cred\u201d.</p> <code>'red'</code> <code>highlight_size</code> <code>int</code> <p>Size of the highlighted point. Defaults to 50.</p> <code>50</code> <code>highlight_label</code> <code>str</code> <p>Label for the highlighted point. Defaults to \u201cHighlighted Point\u201d.</p> <code>None</code> <code>highlight_legend_loc</code> <code>str</code> <p>Location for the legend. Defaults to \u201cupper right\u201d.</p> <code>'upper right'</code> <code>highlight_legend_fontsize</code> <code>int</code> <p>Font size for the legend. Defaults to 8.</p> <code>8</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Displays the contour plot(s).</p> Source code in <code>spotpython/plot/contour.py</code> <pre><code>def contour_plot(\n    data,\n    x_col,\n    y_col,\n    z_col,\n    facet_col=None,\n    aspect=1,\n    as_table=True,\n    figsize=(4, 4),\n    levels=10,\n    cmap=\"viridis\",\n    highlight_point=None,\n    highlight_color=\"red\",\n    highlight_size=50,\n    highlight_label=None,\n    highlight_legend_loc=\"upper right\",\n    highlight_legend_fontsize=8,\n) -&gt; None:\n    \"\"\"\n    Creates contour plots (single or faceted) using matplotlib.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing the data.\n        x_col (str): The name of the column to use for the x-axis.\n        y_col (str): The name of the column to use for the y-axis.\n        z_col (str): The name of the column to use for the z-axis (contour values).\n        facet_col (str, optional): The name of the column to use for faceting (creating subplots). Defaults to None.\n        aspect (float, optional): The aspect ratio of the plot. Defaults to 1.\n        as_table (bool, optional): Whether to arrange facets as a table. Defaults to True.\n        figsize (tuple, optional): The size of the figure. Defaults to (4, 4).\n        levels (int, optional): The number of contour levels. Defaults to 5.\n        cmap (str, optional): The colormap to use. Defaults to \"viridis\".\n        highlight_point (np.array, optional): A 1-dimensional array specifying a single point [x, y] to highlight. Defaults to None.\n        highlight_color (str, optional): Color for the highlighted point. Defaults to \"red\".\n        highlight_size (int, optional): Size of the highlighted point. Defaults to 50.\n        highlight_label (str, optional): Label for the highlighted point. Defaults to \"Highlighted Point\".\n        highlight_legend_loc (str, optional): Location for the legend. Defaults to \"upper right\".\n        highlight_legend_fontsize (int, optional): Font size for the legend. Defaults to 8.\n\n    Returns:\n        None: Displays the contour plot(s).\n    \"\"\"\n    if facet_col:\n        facet_values = data[facet_col].unique()\n        num_facets = len(facet_values)\n\n        # Determine subplot layout\n        if as_table:\n            num_cols = int(np.ceil(np.sqrt(num_facets)))\n            num_rows = int(np.ceil(num_facets / num_cols))\n        else:\n            num_cols = num_facets\n            num_rows = 1\n\n        fig, axes = plt.subplots(num_rows, num_cols, figsize=(figsize[0] * num_cols, figsize[1] * num_rows))\n        axes = np.array(axes).flatten()  # Flatten the axes array for easy indexing\n\n        for i, facet_value in enumerate(facet_values):\n            ax = axes[i]\n            facet_data = data[data[facet_col] == facet_value]\n\n            # Create grid for contour plot\n            x = np.unique(facet_data[x_col])\n            y = np.unique(facet_data[y_col])\n            X, Y = np.meshgrid(x, y)\n            Z = facet_data.pivot_table(index=y_col, columns=x_col, values=z_col).values\n\n            # Plot contour\n            contour = ax.contour(X, Y, Z, levels=levels, cmap=cmap)\n            ax.clabel(contour, inline=True, fontsize=8)\n\n            # Highlight the specified point\n            if highlight_point is not None:\n                ax.scatter(highlight_point[0], highlight_point[1], color=highlight_color, s=highlight_size, label=highlight_label, zorder=10)\n                if highlight_label:\n                    ax.legend(loc=highlight_legend_loc, fontsize=highlight_legend_fontsize)\n\n            # Set labels and title\n            ax.set_xlabel(x_col)\n            ax.set_ylabel(y_col)\n            ax.set_title(f\"{facet_col} = {np.round(facet_value, 2)}\")\n            ax.set_aspect(aspect)\n\n        # Remove empty subplots\n        for i in range(num_facets, len(axes)):\n            fig.delaxes(axes[i])\n\n        fig.tight_layout()\n        plt.show()\n\n    else:\n        # Create grid for contour plot\n        x = np.unique(data[x_col])\n        y = np.unique(data[y_col])\n        X, Y = np.meshgrid(x, y)\n        Z = data.pivot_table(index=y_col, columns=x_col, values=z_col).values\n\n        # Plot contour\n        fig, ax = plt.subplots(figsize=figsize)\n        contour = ax.contour(X, Y, Z, levels=levels, cmap=cmap)\n        ax.clabel(contour, inline=True, fontsize=8)\n\n        # Highlight the specified point\n        if highlight_point is not None:\n            ax.scatter(highlight_point[0], highlight_point[1], color=highlight_color, s=highlight_size, label=highlight_label, zorder=10)\n            if highlight_label:\n                ax.legend(loc=highlight_legend_loc, fontsize=highlight_legend_fontsize)\n\n        # Set labels and title\n        ax.set_xlabel(x_col)\n        ax.set_ylabel(y_col)\n        ax.set_title(f\"Contour Plot of {z_col}\")\n        ax.set_aspect(aspect)\n\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/contour/#spotpython.plot.contour.contourf_plot","title":"<code>contourf_plot(data, x_col, y_col, z_col, facet_col=None, aspect=1, as_table=True, figsize=(4, 4), levels=10, cmap='viridis', show_contour_lines=True, contour_line_color='black', contour_line_width=0.5, colorbar_orientation='vertical', wspace=0.4, hspace=0.4, highlight_point=None, highlight_color='red', highlight_size=50, highlight_label=None, highlight_legend_loc='upper right', highlight_legend_fontsize=8)</code>","text":"<p>Creates filled contour plots (single or faceted) using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data.</p> required <code>x_col</code> <code>str</code> <p>The name of the column to use for the x-axis.</p> required <code>y_col</code> <code>str</code> <p>The name of the column to use for the y-axis.</p> required <code>z_col</code> <code>str</code> <p>The name of the column to use for the z-axis (contour values).</p> required <code>facet_col</code> <code>str</code> <p>The name of the column to use for faceting (creating subplots). Defaults to None.</p> <code>None</code> <code>aspect</code> <code>float</code> <p>The aspect ratio of the plot. Defaults to 1.</p> <code>1</code> <code>as_table</code> <code>bool</code> <p>Whether to arrange facets as a table. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>The size of the figure. Defaults to (4, 4).</p> <code>(4, 4)</code> <code>levels</code> <code>int</code> <p>The number of contour levels. Defaults to 10.</p> <code>10</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cviridis\u201d.</p> <code>'viridis'</code> <code>show_contour_lines</code> <code>bool</code> <p>Whether to overlay contour lines on the filled plot. Defaults to False.</p> <code>True</code> <code>contour_line_color</code> <code>str</code> <p>Color of the contour lines. Defaults to \u201cblack\u201d.</p> <code>'black'</code> <code>contour_line_width</code> <code>float</code> <p>Width of the contour lines. Defaults to 0.5.</p> <code>0.5</code> <code>colorbar_orientation</code> <code>str</code> <p>Orientation of the colorbar (\u201cvertical\u201d or \u201chorizontal\u201d). Defaults to \u201cvertical\u201d.</p> <code>'vertical'</code> <code>wspace</code> <code>float</code> <p>Horizontal spacing between subplots. Defaults to 0.4.</p> <code>0.4</code> <code>hspace</code> <code>float</code> <p>Vertical spacing between subplots. Defaults to 0.4.</p> <code>0.4</code> <code>highlight_point</code> <code>array</code> <p>A 1-dimensional array specifying a single point [x, y] to highlight. Defaults to None.</p> <code>None</code> <code>highlight_color</code> <code>str</code> <p>Color for the highlighted point. Defaults to \u201cred\u201d.</p> <code>'red'</code> <code>highlight_size</code> <code>int</code> <p>Size of the highlighted point. Defaults to 50.</p> <code>50</code> <code>highlight_label</code> <code>str</code> <p>Label for the highlighted point. Defaults to None.</p> <code>None</code> <code>highlight_legend_loc</code> <code>str</code> <p>Location for the legend. Defaults to \u201cupper right\u201d.</p> <code>'upper right'</code> <code>highlight_legend_fontsize</code> <code>int</code> <p>Font size for the legend. Defaults to 8.</p> <code>8</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Displays the filled contour plot(s).</p> Source code in <code>spotpython/plot/contour.py</code> <pre><code>def contourf_plot(\n    data,\n    x_col,\n    y_col,\n    z_col,\n    facet_col=None,\n    aspect=1,\n    as_table=True,\n    figsize=(4, 4),\n    levels=10,\n    cmap=\"viridis\",\n    show_contour_lines=True,\n    contour_line_color=\"black\",\n    contour_line_width=0.5,\n    colorbar_orientation=\"vertical\",\n    wspace=0.4,\n    hspace=0.4,\n    highlight_point=None,  # New argument to specify a single point to highlight\n    highlight_color=\"red\",  # Color for the highlighted point\n    highlight_size=50,  # Size of the highlighted point\n    highlight_label=None,  # Label for the highlighted point\n    highlight_legend_loc=\"upper right\",  # Legend location\n    highlight_legend_fontsize=8,  # Font size for the legend\n) -&gt; None:\n    \"\"\"\n    Creates filled contour plots (single or faceted) using matplotlib.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing the data.\n        x_col (str): The name of the column to use for the x-axis.\n        y_col (str): The name of the column to use for the y-axis.\n        z_col (str): The name of the column to use for the z-axis (contour values).\n        facet_col (str, optional): The name of the column to use for faceting (creating subplots). Defaults to None.\n        aspect (float, optional): The aspect ratio of the plot. Defaults to 1.\n        as_table (bool, optional): Whether to arrange facets as a table. Defaults to True.\n        figsize (tuple, optional): The size of the figure. Defaults to (4, 4).\n        levels (int, optional): The number of contour levels. Defaults to 10.\n        cmap (str, optional): The colormap to use. Defaults to \"viridis\".\n        show_contour_lines (bool, optional): Whether to overlay contour lines on the filled plot. Defaults to False.\n        contour_line_color (str, optional): Color of the contour lines. Defaults to \"black\".\n        contour_line_width (float, optional): Width of the contour lines. Defaults to 0.5.\n        colorbar_orientation (str, optional): Orientation of the colorbar (\"vertical\" or \"horizontal\"). Defaults to \"vertical\".\n        wspace (float, optional): Horizontal spacing between subplots. Defaults to 0.4.\n        hspace (float, optional): Vertical spacing between subplots. Defaults to 0.4.\n        highlight_point (np.array, optional): A 1-dimensional array specifying a single point [x, y] to highlight. Defaults to None.\n        highlight_color (str, optional): Color for the highlighted point. Defaults to \"red\".\n        highlight_size (int, optional): Size of the highlighted point. Defaults to 50.\n        highlight_label (str, optional): Label for the highlighted point. Defaults to None.\n        highlight_legend_loc (str, optional): Location for the legend. Defaults to \"upper right\".\n        highlight_legend_fontsize (int, optional): Font size for the legend. Defaults to 8.\n\n    Returns:\n        None: Displays the filled contour plot(s).\n    \"\"\"\n    if facet_col:\n        facet_values = data[facet_col].unique()\n        num_facets = len(facet_values)\n\n        # Determine subplot layout\n        if as_table:\n            num_cols = int(np.ceil(np.sqrt(num_facets)))\n            num_rows = int(np.ceil(num_facets / num_cols))\n        else:\n            num_cols = num_facets\n            num_rows = 1\n\n        # Create figure with gridspec for colorbar placement\n        if colorbar_orientation == \"vertical\":\n            fig = plt.figure(figsize=(figsize[0] * num_cols, figsize[1] * num_rows))\n            spec = gridspec.GridSpec(num_rows, num_cols + 1, width_ratios=[1] * num_cols + [0.05], wspace=wspace, hspace=hspace)\n        else:  # Horizontal colorbar\n            fig = plt.figure(figsize=(figsize[0] * num_cols, figsize[1] * num_rows + 1))\n            spec = gridspec.GridSpec(num_rows + 1, num_cols, height_ratios=[1] * num_rows + [0.05], wspace=wspace, hspace=hspace)\n\n        axes = []\n        for row in range(num_rows):\n            for col in range(num_cols):\n                if row * num_cols + col &lt; num_facets:\n                    axes.append(fig.add_subplot(spec[row, col]))\n\n        for i, facet_value in enumerate(facet_values):\n            ax = axes[i]\n            facet_data = data[data[facet_col] == facet_value]\n\n            # Create grid for contour plot\n            x = np.unique(facet_data[x_col])\n            y = np.unique(facet_data[y_col])\n            X, Y = np.meshgrid(x, y)\n            Z = facet_data.pivot_table(index=y_col, columns=x_col, values=z_col).values\n\n            # Plot filled contour\n            contour = ax.contourf(X, Y, Z, levels=levels, cmap=cmap)\n\n            # Optionally overlay contour lines\n            if show_contour_lines:\n                contour_lines = ax.contour(X, Y, Z, levels=levels, colors=contour_line_color, linewidths=contour_line_width)\n                ax.clabel(contour_lines, inline=True, fontsize=8)\n\n            # Highlight the specified point\n            if highlight_point is not None:\n                ax.scatter(highlight_point[0], highlight_point[1], color=highlight_color, s=highlight_size, label=highlight_label, zorder=10)\n                if highlight_label:\n                    ax.legend(loc=highlight_legend_loc, fontsize=highlight_legend_fontsize)\n\n            # Set labels and title\n            ax.set_xlabel(x_col)\n            ax.set_ylabel(y_col)\n            ax.set_title(f\"{facet_col} = {np.round(facet_value, 2)}\")\n            ax.set_aspect(aspect)\n\n        # Add colorbar\n        if colorbar_orientation == \"vertical\":\n            cbar_ax = fig.add_subplot(spec[:, -1])  # Last column for vertical colorbar\n        else:\n            cbar_ax = fig.add_subplot(spec[-1, :])  # Last row for horizontal colorbar\n        fig.colorbar(contour, cax=cbar_ax, orientation=colorbar_orientation, label=z_col)\n\n        plt.show()\n\n    else:\n        # Create grid for contour plot\n        x = np.unique(data[x_col])\n        y = np.unique(data[y_col])\n        X, Y = np.meshgrid(x, y)\n        Z = data.pivot_table(index=y_col, columns=x_col, values=z_col).values\n\n        # Create figure\n        fig, ax = plt.subplots(figsize=figsize)\n\n        # Plot filled contour\n        contour = ax.contourf(X, Y, Z, levels=levels, cmap=cmap)\n\n        # Optionally overlay contour lines\n        if show_contour_lines:\n            contour_lines = ax.contour(X, Y, Z, levels=levels, colors=contour_line_color, linewidths=contour_line_width)\n            ax.clabel(contour_lines, inline=True, fontsize=8)\n\n        # Highlight the specified point\n        if highlight_point is not None:\n            ax.scatter(highlight_point[0], highlight_point[1], color=highlight_color, s=highlight_size, label=highlight_label, zorder=10)\n            if highlight_label:\n                ax.legend(loc=highlight_legend_loc, fontsize=highlight_legend_fontsize)\n\n        # Set labels and title\n        ax.set_xlabel(x_col)\n        ax.set_ylabel(y_col)\n        ax.set_title(f\"Filled Contour Plot of {z_col}\")\n        ax.set_aspect(aspect)\n\n        # Add colorbar\n        if colorbar_orientation == \"vertical\":\n            cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Position for vertical colorbar\n        else:\n            cbar_ax = fig.add_axes([0.15, 0.05, 0.7, 0.02])  # Position for horizontal colorbar\n        fig.colorbar(contour, cax=cbar_ax, orientation=colorbar_orientation, label=z_col)\n\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/contour/#spotpython.plot.contour.create_contour_plot","title":"<code>create_contour_plot(data, x_col, y_col, z_col, facet_col=None, aspect=1, as_table=True, figsize=(3, 3), levels=5, cmap='viridis')</code>","text":"<p>Creates contour plots similar to R\u2019s contourplot function using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data.</p> required <code>x_col</code> <code>str</code> <p>The name of the column to use for the x-axis.</p> required <code>y_col</code> <code>str</code> <p>The name of the column to use for the y-axis.</p> required <code>z_col</code> <code>str</code> <p>The name of the column to use for the z-axis (contour values).</p> required <code>facet_col</code> <code>str</code> <p>The name of the column to use for faceting (creating subplots). Defaults to None.</p> <code>None</code> <code>aspect</code> <code>float</code> <p>The aspect ratio of the plot. Defaults to 1.</p> <code>1</code> <code>as_table</code> <code>bool</code> <p>Whether to arrange facets as a table. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>The size of the figure. Defaults to (3, 3).</p> <code>(3, 3)</code> <code>levels</code> <code>int</code> <p>The number of contour levels. Defaults to 5.</p> <code>5</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cviridis\u201d.</p> <code>'viridis'</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Displays the contour plot(s).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified columns are not found in the DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.contour import create_contour_plot\n    import numpy as np\n    import pandas as pd\n    # Create a grid of x and y values\n    x = np.linspace(-5, 5, 100)\n    y = np.linspace(-5, 5, 100)\n    x_grid, y_grid = np.meshgrid(x, y)\n    # Calculate z = x^2 + y^2\n    z = x_grid**2 + y_grid**2\n    # Flatten the grid and create a DataFrame\n    data = pd.DataFrame({\n        'x': x_grid.flatten(),\n        'y': y_grid.flatten(),\n        'z': z.flatten()\n    })\n    # Create the contour plot\n    create_contour_plot(data, 'x', 'y', 'z', facet_col=None)\n&gt;&gt;&gt; # Create a contour plot with faceting\n    from spotpython.plot.contour import create_contour_plot\n    import numpy as np\n    import pandas as pd\n    # Create a grid of x and y values\n    x = np.linspace(-5, 5, 50)\n    y = np.linspace(-5, 5, 50)\n    x_grid, y_grid = np.meshgrid(x, y)\n    # Calculate z = x^2 + y^2 for two different facets\n    z1 = x_grid**2 + y_grid**2\n    z2 = (x_grid - 2)**2 + (y_grid - 2)**2\n    # Flatten the grids and create a DataFrame\n    data = pd.DataFrame({\n        'x': np.tile(x, len(y) * 2),  # Repeat x values for both facets\n        'y': np.repeat(y, len(x) * 2),  # Repeat y values for both facets\n        'z': np.concatenate([z1.flatten(), z2.flatten()]),  # Combine z values for both facets\n        'facet': ['Facet A'] * len(z1.flatten()) + ['Facet B'] * len(z2.flatten())  # Create facet column\n    })\n    # Create the contour plot with facets\n    create_contour_plot(data, 'x', 'y', 'z', facet_col='facet')\n</code></pre> Source code in <code>spotpython/plot/contour.py</code> <pre><code>def create_contour_plot(data, x_col, y_col, z_col, facet_col=None, aspect=1, as_table=True, figsize=(3, 3), levels=5, cmap=\"viridis\") -&gt; None:\n    \"\"\"\n    Creates contour plots similar to R's contourplot function using matplotlib.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing the data.\n        x_col (str): The name of the column to use for the x-axis.\n        y_col (str): The name of the column to use for the y-axis.\n        z_col (str): The name of the column to use for the z-axis (contour values).\n        facet_col (str, optional): The name of the column to use for faceting (creating subplots). Defaults to None.\n        aspect (float, optional): The aspect ratio of the plot. Defaults to 1.\n        as_table (bool, optional): Whether to arrange facets as a table. Defaults to True.\n        figsize (tuple, optional): The size of the figure. Defaults to (3, 3).\n        levels (int, optional): The number of contour levels. Defaults to 5.\n        cmap (str, optional): The colormap to use. Defaults to \"viridis\".\n\n    Returns:\n        None: Displays the contour plot(s).\n\n    Raises:\n        ValueError: If the specified columns are not found in the DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.contour import create_contour_plot\n            import numpy as np\n            import pandas as pd\n            # Create a grid of x and y values\n            x = np.linspace(-5, 5, 100)\n            y = np.linspace(-5, 5, 100)\n            x_grid, y_grid = np.meshgrid(x, y)\n            # Calculate z = x^2 + y^2\n            z = x_grid**2 + y_grid**2\n            # Flatten the grid and create a DataFrame\n            data = pd.DataFrame({\n                'x': x_grid.flatten(),\n                'y': y_grid.flatten(),\n                'z': z.flatten()\n            })\n            # Create the contour plot\n            create_contour_plot(data, 'x', 'y', 'z', facet_col=None)\n        &gt;&gt;&gt; # Create a contour plot with faceting\n            from spotpython.plot.contour import create_contour_plot\n            import numpy as np\n            import pandas as pd\n            # Create a grid of x and y values\n            x = np.linspace(-5, 5, 50)\n            y = np.linspace(-5, 5, 50)\n            x_grid, y_grid = np.meshgrid(x, y)\n            # Calculate z = x^2 + y^2 for two different facets\n            z1 = x_grid**2 + y_grid**2\n            z2 = (x_grid - 2)**2 + (y_grid - 2)**2\n            # Flatten the grids and create a DataFrame\n            data = pd.DataFrame({\n                'x': np.tile(x, len(y) * 2),  # Repeat x values for both facets\n                'y': np.repeat(y, len(x) * 2),  # Repeat y values for both facets\n                'z': np.concatenate([z1.flatten(), z2.flatten()]),  # Combine z values for both facets\n                'facet': ['Facet A'] * len(z1.flatten()) + ['Facet B'] * len(z2.flatten())  # Create facet column\n            })\n            # Create the contour plot with facets\n            create_contour_plot(data, 'x', 'y', 'z', facet_col='facet')\n    \"\"\"\n\n    if facet_col:\n        facet_values = data[facet_col].unique()\n        num_facets = len(facet_values)\n\n        # Determine subplot layout\n        if as_table:\n            num_cols = int(np.ceil(np.sqrt(num_facets)))\n            num_rows = int(np.ceil(num_facets / num_cols))\n        else:\n            num_cols = num_facets\n            num_rows = 1\n\n        fig, axes = plt.subplots(num_rows, num_cols, figsize=(figsize[0] * num_cols, figsize[1] * num_rows))\n        axes = np.array(axes).flatten()  # Flatten the axes array for easy indexing\n\n        for i, facet_value in enumerate(facet_values):\n            ax = axes[i]\n            facet_data = data[data[facet_col] == facet_value]\n\n            # Create grid for contour plot\n            x = np.unique(facet_data[x_col])\n            y = np.unique(facet_data[y_col])\n            X, Y = np.meshgrid(x, y)\n            Z = facet_data.pivot_table(index=y_col, columns=x_col, values=z_col).values\n\n            # Plot contour\n            contour = ax.contour(X, Y, Z, levels=levels, cmap=cmap)  # Adjust levels and cmap as needed\n            ax.clabel(contour, inline=True, fontsize=8)\n\n            # Set labels and title\n            ax.set_xlabel(x_col)\n            ax.set_ylabel(y_col)\n            ax.set_title(f\"{facet_col} = {np.round(facet_value, 2)}\")\n            ax.set_aspect(aspect)\n\n        # Remove empty subplots\n        for i in range(num_facets, len(axes)):\n            fig.delaxes(axes[i])\n\n        fig.tight_layout()\n        plt.show()\n\n    else:\n        # Create grid for contour plot\n        x = np.unique(data[x_col])\n        y = np.unique(data[y_col])\n        X, Y = np.meshgrid(x, y)\n        Z = data.pivot_table(index=y_col, columns=x_col, values=z_col).values\n\n        # Plot contour\n        fig, ax = plt.subplots(figsize=figsize)\n        contour = ax.contour(X, Y, Z, levels=levels, cmap=\"viridis\")  # Adjust levels and cmap as needed\n        ax.clabel(contour, inline=True, fontsize=8)\n\n        # Set labels and title\n        ax.set_xlabel(x_col)\n        ax.set_ylabel(y_col)\n        ax.set_title(f\"Contour Plot of {z_col}\")\n        ax.set_aspect(aspect)\n\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/contour/#spotpython.plot.contour.mo_generate_plot_grid","title":"<code>mo_generate_plot_grid(variables, resolutions, functions)</code>","text":"<p>Generate a grid of input variables and apply objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>dict</code> <p>A dictionary where keys are variable names (e.g., \u201ctime\u201d, \u201ctemperature\u201d)               and values are tuples of (min_value, max_value).</p> required <code>resolutions</code> <code>dict</code> <p>A dictionary where keys are variable names and values are the number of points.</p> required <code>functions</code> <code>dict</code> <p>A dictionary where keys are function names and values are callable functions.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the grid and the results of the objective functions.</p> Source code in <code>spotpython/plot/contour.py</code> <pre><code>def mo_generate_plot_grid(variables, resolutions, functions) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a grid of input variables and apply objective functions.\n\n    Args:\n        variables (dict): A dictionary where keys are variable names (e.g., \"time\", \"temperature\")\n                          and values are tuples of (min_value, max_value).\n        resolutions (dict): A dictionary where keys are variable names and values are the number of points.\n        functions (dict): A dictionary where keys are function names and values are callable functions.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the grid and the results of the objective functions.\n    \"\"\"\n    # Create a meshgrid for all variables\n    grids = [np.linspace(variables[var][0], variables[var][1], resolutions[var]) for var in variables]\n    grid = np.array(np.meshgrid(*grids)).T.reshape(-1, len(variables))\n\n    # Create a DataFrame for the grid\n    plot_grid = pd.DataFrame(grid, columns=variables.keys())\n\n    # Apply each function to the grid\n    for func_name, func in functions.items():\n        plot_grid[func_name] = plot_grid.apply(lambda row: func(row.values), axis=1)\n\n    return plot_grid\n</code></pre>"},{"location":"reference/spotpython/plot/contour/#spotpython.plot.contour.plotCombinations","title":"<code>plotCombinations(model, X=None, lower=None, upper=None, x_vars=None, y_vars=None, min_z=None, max_z=None, var_type=None, var_name=None, show=True, save_dir=None, n_grid=50, contour_levels=10, dpi=200, title_prefix='', figsize=(12, 6), use_min=False, use_max=False, margin=0.1, aspect_equal=False, legend_fontsize=12, cmap='viridis', X_points=None, y_points=None, plot_points=True, points_color='white', points_size=30, point_color_below='blue', point_color_above='red', atol=1e-06)</code>","text":"<p>Plot model surfaces for multiple combinations of input variables.</p> <p>This function generates contour and 3D surface plots for all specified combinations of input variables, avoiding redundancies and meaningless combinations.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>A fitted model with a predict method.</p> required <code>X</code> <code>Optional[ndarray]</code> <p>Array of training points. If provided, used to derive bounds and dimension count.</p> <code>None</code> <code>lower</code> <code>Optional[Union[ndarray, List[float]]]</code> <p>Array-like lower bounds for each dimension. If None, derived from X.</p> <code>None</code> <code>upper</code> <code>Optional[Union[ndarray, List[float]]]</code> <p>Array-like upper bounds for each dimension. If None, derived from X.</p> <code>None</code> <code>x_vars</code> <code>Optional[List[int]]</code> <p>List of indices for x-axis variables. Defaults to all if None or empty.</p> <code>None</code> <code>y_vars</code> <code>Optional[List[int]]</code> <p>List of indices for y-axis variables. Defaults to all if None or empty.</p> <code>None</code> <code>min_z</code> <code>Optional[float]</code> <p>Min value for color scale. If None, auto-calculated.</p> <code>None</code> <code>max_z</code> <code>Optional[float]</code> <p>Max value for color scale. If None, auto-calculated.</p> <code>None</code> <code>var_type</code> <code>Optional[List[str]]</code> <p>List of variable types. If None, assumed numeric.</p> <code>None</code> <code>var_name</code> <code>Optional[List[str]]</code> <p>List of variable names. If None, named x0, x1, \u2026</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plots. Defaults to True.</p> <code>True</code> <code>save_dir</code> <code>Optional[str]</code> <p>Directory for saving plots. If None, not saved.</p> <code>None</code> <code>n_grid</code> <code>int</code> <p>Number of grid points along each axis. Defaults to 50.</p> <code>50</code> <code>contour_levels</code> <code>int</code> <p>Number of contour levels. Defaults to 10.</p> <code>10</code> <code>dpi</code> <code>int</code> <p>DPI for saving figures. Defaults to 200.</p> <code>200</code> <code>title_prefix</code> <code>str</code> <p>Prefix string for plot titles.</p> <code>''</code> <code>figsize</code> <code>Tuple[float, float]</code> <p>Figure size (width, height). Defaults to (12, 6).</p> <code>(12, 6)</code> <code>use_min</code> <code>bool</code> <p>Use lower bounds for non-plotted dimensions. Defaults to False.</p> <code>False</code> <code>use_max</code> <code>bool</code> <p>Use upper bounds for non-plotted dimensions. Defaults to False.</p> <code>False</code> <code>margin</code> <code>float</code> <p>Fraction of range added as margin to bounds when derived from X. Defaults to 0.1.</p> <code>0.1</code> <code>aspect_equal</code> <code>bool</code> <p>Whether to set equal aspect ratio. Defaults to False.</p> <code>False</code> <code>legend_fontsize</code> <code>int</code> <p>Font size for labels and legends. Defaults to 12.</p> <code>12</code> <code>cmap</code> <code>str</code> <p>Colormap for the plots. Defaults to \u201cviridis\u201d.</p> <code>'viridis'</code> <code>X_points</code> <code>Optional[ndarray]</code> <p>Original data points to plot.</p> <code>None</code> <code>y_points</code> <code>Optional[ndarray]</code> <p>Original target values to plot.</p> <code>None</code> <code>plot_points</code> <code>bool</code> <p>Whether to plot X_points. Defaults to True.</p> <code>True</code> <code>points_color</code> <code>str</code> <p>Color for data points. Defaults to \u201cwhite\u201d.</p> <code>'white'</code> <code>points_size</code> <code>int</code> <p>Marker size for data points. Defaults to 30.</p> <code>30</code> <code>point_color_below</code> <code>str</code> <p>Color if actual z &lt; predicted z. Defaults to \u201clightgrey\u201d.</p> <code>'blue'</code> <code>point_color_above</code> <code>str</code> <p>Color if actual z &gt;= predicted z. Defaults to \u201cwhite\u201d.</p> <code>'red'</code> <code>atol</code> <code>float</code> <p>Absolute tolerance for comparing actual and predicted z-values. Defaults to 1e-6.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/contour.py</code> <pre><code>def plotCombinations(\n    model,\n    X=None,\n    lower=None,\n    upper=None,\n    x_vars=None,\n    y_vars=None,\n    min_z=None,\n    max_z=None,\n    var_type=None,\n    var_name=None,\n    show=True,\n    save_dir=None,\n    n_grid=50,\n    contour_levels=10,\n    dpi=200,\n    title_prefix=\"\",\n    figsize=(12, 6),\n    use_min=False,\n    use_max=False,\n    margin=0.1,\n    aspect_equal=False,\n    legend_fontsize=12,\n    cmap=\"viridis\",\n    X_points=None,\n    y_points=None,\n    plot_points=True,\n    points_color=\"white\",\n    points_size=30,\n    point_color_below=\"blue\",\n    point_color_above=\"red\",\n    atol=1e-6,\n) -&gt; None:\n    \"\"\"Plot model surfaces for multiple combinations of input variables.\n\n    This function generates contour and 3D surface plots for all specified\n    combinations of input variables, avoiding redundancies and meaningless combinations.\n\n    Args:\n        model (object): A fitted model with a predict method.\n        X (Optional[np.ndarray]): Array of training points. If provided, used to derive bounds and dimension count.\n        lower (Optional[Union[np.ndarray, List[float]]]): Array-like lower bounds for each dimension. If None, derived from X.\n        upper (Optional[Union[np.ndarray, List[float]]]): Array-like upper bounds for each dimension. If None, derived from X.\n        x_vars (Optional[List[int]]): List of indices for x-axis variables. Defaults to all if None or empty.\n        y_vars (Optional[List[int]]): List of indices for y-axis variables. Defaults to all if None or empty.\n        min_z (Optional[float]): Min value for color scale. If None, auto-calculated.\n        max_z (Optional[float]): Max value for color scale. If None, auto-calculated.\n        var_type (Optional[List[str]]): List of variable types. If None, assumed numeric.\n        var_name (Optional[List[str]]): List of variable names. If None, named x0, x1, ...\n        show (bool): Whether to display the plots. Defaults to True.\n        save_dir (Optional[str]): Directory for saving plots. If None, not saved.\n        n_grid (int): Number of grid points along each axis. Defaults to 50.\n        contour_levels (int): Number of contour levels. Defaults to 10.\n        dpi (int): DPI for saving figures. Defaults to 200.\n        title_prefix (str): Prefix string for plot titles.\n        figsize (Tuple[float, float]): Figure size (width, height). Defaults to (12, 6).\n        use_min (bool): Use lower bounds for non-plotted dimensions. Defaults to False.\n        use_max (bool): Use upper bounds for non-plotted dimensions. Defaults to False.\n        margin (float): Fraction of range added as margin to bounds when derived from X. Defaults to 0.1.\n        aspect_equal (bool): Whether to set equal aspect ratio. Defaults to False.\n        legend_fontsize (int): Font size for labels and legends. Defaults to 12.\n        cmap (str): Colormap for the plots. Defaults to \"viridis\".\n        X_points (Optional[np.ndarray]): Original data points to plot.\n        y_points (Optional[np.ndarray]): Original target values to plot.\n        plot_points (bool): Whether to plot X_points. Defaults to True.\n        points_color (str): Color for data points. Defaults to \"white\".\n        points_size (int): Marker size for data points. Defaults to 30.\n        point_color_below (str): Color if actual z &lt; predicted z. Defaults to \"lightgrey\".\n        point_color_above (str): Color if actual z &gt;= predicted z. Defaults to \"white\".\n        atol (float): Absolute tolerance for comparing actual and predicted z-values. Defaults to 1e-6.\n\n    Returns:\n        None\n    \"\"\"\n    # Derive bounds from X if needed\n    if X is not None:\n        if hasattr(X, \"to_numpy\"):\n            X = X.to_numpy()\n        n_vars_X = X.shape[1]\n        if lower is None:\n            min_vals = np.min(X, axis=0)\n            range_vals = np.ptp(X, axis=0)\n            lower = min_vals - margin * range_vals\n        if upper is None:\n            max_vals = np.max(X, axis=0)\n            range_vals = np.ptp(X, axis=0)\n            upper = max_vals + margin * range_vals\n\n    # Determine the number of variables\n    if lower is not None:\n        n_vars = len(lower)\n    elif upper is not None:\n        n_vars = len(upper)\n    elif X is not None:\n        n_vars = n_vars_X\n    else:\n        raise ValueError(\"Cannot determine the number of variables without X, lower, or upper.\")\n\n    # Default to all variables if x_vars or y_vars are missing\n    if not x_vars:\n        x_vars = list(range(n_vars))\n    if not y_vars:\n        y_vars = list(range(n_vars))\n\n    # Keep track of plotted pairs\n    plotted_pairs = set()\n\n    # Generate combinations\n    for i in x_vars:\n        for j in y_vars:\n            if i == j:\n                continue\n            pair = tuple(sorted([i, j]))\n            if pair in plotted_pairs:\n                continue\n            plotted_pairs.add(pair)\n\n            var_i_name = var_name[i] if var_name and i &lt; len(var_name) else f\"x{i}\"\n            var_j_name = var_name[j] if var_name and j &lt; len(var_name) else f\"x{j}\"\n            plot_title = f\"{title_prefix}{var_i_name} vs {var_j_name}\"\n\n            filename = None\n            if save_dir is not None:\n                os.makedirs(save_dir, exist_ok=True)\n                filename = os.path.join(save_dir, f\"plot_{var_i_name}_vs_{var_j_name}.png\")\n\n            # Call plotModel with the new arguments\n            plotModel(\n                model=model,\n                lower=lower,\n                upper=upper,\n                i=i,\n                j=j,\n                min_z=min_z,\n                max_z=max_z,\n                var_type=var_type,\n                var_name=var_name,\n                show=show,\n                filename=filename,\n                n_grid=n_grid,\n                contour_levels=contour_levels,\n                dpi=dpi,\n                title=plot_title,\n                figsize=figsize,\n                use_min=use_min,\n                use_max=use_max,\n                aspect_equal=aspect_equal,\n                legend_fontsize=legend_fontsize,\n                cmap=cmap,  # Pass colormap\n                X_points=X_points,  # Pass original data points\n                y_points=y_points,  # Pass original target values\n                plot_points=plot_points,\n                points_color=points_color,\n                points_size=points_size,\n                point_color_below=point_color_below,\n                point_color_above=point_color_above,\n                atol=atol,\n            )\n\n    return None\n</code></pre>"},{"location":"reference/spotpython/plot/contour/#spotpython.plot.contour.plotModel","title":"<code>plotModel(model, lower, upper, i=0, j=1, min_z=None, max_z=None, var_type=None, var_name=None, show=True, filename=None, n_grid=50, contour_levels=10, dpi=200, title='', figsize=(12, 6), use_min=False, use_max=False, aspect_equal=True, legend_fontsize=12, cmap='viridis', X_points=None, y_points=None, plot_points=True, points_color='white', points_size=30, point_color_below='blue', point_color_above='red', atol=1e-06)</code>","text":"<p>Generate 2D contour and 3D surface plots for a model\u2019s predictions.</p> <p>Even if the data is not strictly 3D, each point in X_points will have its \u201cpredicted surface z-value\u201d computed by:   1) Taking the i-th and j-th coordinates directly from that point.   2) Setting all other dimensions (leftover dims) based on use_min, use_max,      or their mean (if both are False). Then, we compare the newly-computed \u201cactual z\u201d for that point with its model-predicted z-value. If \u2018actual z\u2019 &lt; \u2018predicted z\u2019, the point is colored with point_color_below; otherwise, it is colored with point_color_above.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>A model with a predict method.</p> required <code>lower</code> <code>array_like</code> <p>Lower bounds for each dimension.</p> required <code>upper</code> <code>array_like</code> <p>Upper bounds for each dimension.</p> required <code>i</code> <code>int</code> <p>Index for the x-axis dimension.</p> <code>0</code> <code>j</code> <code>int</code> <p>Index for the y-axis dimension.</p> <code>1</code> <code>min_z</code> <code>float</code> <p>Min value for color scaling. Defaults to None.</p> <code>None</code> <code>max_z</code> <code>float</code> <p>Max value for color scaling. Defaults to None.</p> <code>None</code> <code>var_type</code> <code>list</code> <p>Variable types for each dimension. Defaults to None.</p> <code>None</code> <code>var_name</code> <code>list</code> <p>Variable names for labeling axes. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>File path to save the figure. Defaults to None.</p> <code>None</code> <code>n_grid</code> <code>int</code> <p>Resolution for each axis. Defaults to 50.</p> <code>50</code> <code>contour_levels</code> <code>int</code> <p>Number of contour levels. Defaults to 10.</p> <code>10</code> <code>dpi</code> <code>int</code> <p>DPI for saving. Defaults to 200.</p> <code>200</code> <code>title</code> <code>str</code> <p>Title for the figure. Defaults to \u201c\u201d.</p> <code>''</code> <code>figsize</code> <code>tuple</code> <p>Figure size. Defaults to (12, 6).</p> <code>(12, 6)</code> <code>use_min</code> <code>bool</code> <p>If True, leftover dims are set to lower bounds.</p> <code>False</code> <code>use_max</code> <code>bool</code> <p>If True, leftover dims are set to upper bounds.</p> <code>False</code> <code>aspect_equal</code> <code>bool</code> <p>Whether axes have equal scaling. Defaults to True.</p> <code>True</code> <code>legend_fontsize</code> <code>int</code> <p>Font size for labels and legends. Defaults to 12.</p> <code>12</code> <code>cmap</code> <code>str</code> <p>Colormap. Defaults to \u201cviridis\u201d.</p> <code>'viridis'</code> <code>X_points</code> <code>ndarray</code> <p>Original data points. Shape: (N, D).</p> <code>None</code> <code>y_points</code> <code>ndarray</code> <p>Original target values. Shape: (N,).</p> <code>None</code> <code>plot_points</code> <code>bool</code> <p>Whether to plot X_points. Defaults to True.</p> <code>True</code> <code>points_color</code> <code>str</code> <p>Fallback color for data points. Defaults to \u201cwhite\u201d.</p> <code>'white'</code> <code>points_size</code> <code>int</code> <p>Marker size for data points. Defaults to 30.</p> <code>30</code> <code>point_color_below</code> <code>str</code> <p>Color if actual z &lt; predicted z. Defaults to \u201clightgrey\u201d.</p> <code>'blue'</code> <code>point_color_above</code> <code>str</code> <p>Color if actual z &gt;= predicted z. Defaults to \u201cwhite\u201d.</p> <code>'red'</code> <code>atol</code> <code>float</code> <p>Absolute tolerance for comparing actual and predicted z-values. Defaults to 1e-6.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>(fig, (ax_contour, ax_surface))</code> <p>Figure and axes for the contour and surface plots.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>For mismatched dimensions or invalid i/j indices.</p> Source code in <code>spotpython/plot/contour.py</code> <pre><code>def plotModel(\n    model,\n    lower,\n    upper,\n    i=0,\n    j=1,\n    min_z=None,\n    max_z=None,\n    var_type=None,\n    var_name=None,\n    show=True,\n    filename=None,\n    n_grid=50,\n    contour_levels=10,\n    dpi=200,\n    title=\"\",\n    figsize=(12, 6),\n    use_min=False,\n    use_max=False,\n    aspect_equal=True,\n    legend_fontsize=12,\n    cmap=\"viridis\",\n    X_points=None,\n    y_points=None,\n    plot_points=True,\n    points_color=\"white\",\n    points_size=30,\n    point_color_below=\"blue\",\n    point_color_above=\"red\",\n    atol=1e-6,\n) -&gt; None:\n    \"\"\"\n    Generate 2D contour and 3D surface plots for a model's predictions.\n\n    Even if the data is not strictly 3D, each point in X_points will have its\n    \"predicted surface z-value\" computed by:\n      1) Taking the i-th and j-th coordinates directly from that point.\n      2) Setting all other dimensions (leftover dims) based on use_min, use_max,\n         or their mean (if both are False).\n    Then, we compare the newly-computed \"actual z\" for that point with its\n    model-predicted z-value. If 'actual z' &lt; 'predicted z', the point is colored\n    with point_color_below; otherwise, it is colored with point_color_above.\n\n    Args:\n        model (object): A model with a predict method.\n        lower (array_like): Lower bounds for each dimension.\n        upper (array_like): Upper bounds for each dimension.\n        i (int): Index for the x-axis dimension.\n        j (int): Index for the y-axis dimension.\n        min_z (float, optional): Min value for color scaling. Defaults to None.\n        max_z (float, optional): Max value for color scaling. Defaults to None.\n        var_type (list, optional): Variable types for each dimension. Defaults to None.\n        var_name (list, optional): Variable names for labeling axes. Defaults to None.\n        show (bool): Whether to display the plot. Defaults to True.\n        filename (str, optional): File path to save the figure. Defaults to None.\n        n_grid (int): Resolution for each axis. Defaults to 50.\n        contour_levels (int): Number of contour levels. Defaults to 10.\n        dpi (int): DPI for saving. Defaults to 200.\n        title (str): Title for the figure. Defaults to \"\".\n        figsize (tuple): Figure size. Defaults to (12, 6).\n        use_min (bool): If True, leftover dims are set to lower bounds.\n        use_max (bool): If True, leftover dims are set to upper bounds.\n        aspect_equal (bool): Whether axes have equal scaling. Defaults to True.\n        legend_fontsize (int): Font size for labels and legends. Defaults to 12.\n        cmap (str): Colormap. Defaults to \"viridis\".\n        X_points (ndarray): Original data points. Shape: (N, D).\n        y_points (ndarray): Original target values. Shape: (N,).\n        plot_points (bool): Whether to plot X_points. Defaults to True.\n        points_color (str): Fallback color for data points. Defaults to \"white\".\n        points_size (int): Marker size for data points. Defaults to 30.\n        point_color_below (str): Color if actual z &lt; predicted z. Defaults to \"lightgrey\".\n        point_color_above (str): Color if actual z &gt;= predicted z. Defaults to \"white\".\n        atol (float): Absolute tolerance for comparing actual and predicted z-values. Defaults to 1e-6.\n\n    Returns:\n        (fig, (ax_contour, ax_surface)): Figure and axes for the contour and surface plots.\n\n    Raises:\n        ValueError: For mismatched dimensions or invalid i/j indices.\n    \"\"\"\n    # --- Validate inputs ---\n    lower = np.asarray(lower)\n    upper = np.asarray(upper)\n    n_dims = len(lower)\n    if len(upper) != n_dims:\n        raise ValueError(\"Mismatch in dimension count between lower and upper.\")\n    if i &lt; 0 or j &lt; 0 or i &gt;= n_dims or j &gt;= n_dims:\n        raise ValueError(f\"Invalid dimension indices i={i} or j={j} for {n_dims}-dimensional data.\")\n    if i == j:\n        raise ValueError(\"Dimensions i and j must be different.\")\n\n    if var_name is None:\n        var_name = [f\"x{k}\" for k in range(n_dims)]\n    elif len(var_name) != n_dims:\n        raise ValueError(\"var_name length must match the number of dimensions.\")\n\n    # --- 2D grid for contour/surface ---\n    x_vals = np.linspace(lower[i], upper[i], n_grid)\n    y_vals = np.linspace(lower[j], upper[j], n_grid)\n    X_grid, Y_grid = np.meshgrid(x_vals, y_vals)\n\n    # Helper for leftover dims\n    def hidden_value(dim_index):\n        if use_min:\n            return lower[dim_index]\n        if use_max:\n            return upper[dim_index]\n        return 0.5 * (lower[dim_index] + upper[dim_index])\n\n    # Build all grid points\n    grid_points = []\n    for row in range(n_grid):\n        for col in range(n_grid):\n            p = np.zeros(n_dims)\n            p[i] = X_grid[row, col]\n            p[j] = Y_grid[row, col]\n            for dim in range(n_dims):\n                if dim not in (i, j):\n                    p[dim] = hidden_value(dim)\n            grid_points.append(p)\n    grid_points = np.array(grid_points)\n\n    # Predict for the grid\n    Z_pred = model.predict(grid_points)\n    if isinstance(Z_pred, dict):\n        Z_pred = Z_pred.get(\"mean\", list(Z_pred.values())[0])\n    elif isinstance(Z_pred, tuple):\n        Z_pred = Z_pred[0]\n    Z_pred = Z_pred.reshape(n_grid, n_grid)\n\n    # Determine min/max color scale\n    if min_z is None:\n        min_z = np.min(Z_pred)\n    if max_z is None:\n        max_z = np.max(Z_pred)\n\n    # --- Set up figure ---\n    fig = plt.figure(figsize=figsize)\n    ax_contour = fig.add_subplot(1, 2, 1)\n    ax_surface = fig.add_subplot(1, 2, 2, projection=\"3d\")\n\n    # --- 2D contour ---\n    cont = ax_contour.contourf(\n        X_grid,\n        Y_grid,\n        Z_pred,\n        levels=contour_levels,\n        cmap=cmap,\n        vmin=min_z,\n        vmax=max_z,\n    )\n    cb1 = plt.colorbar(cont, ax=ax_contour)\n    cb1.ax.tick_params(labelsize=legend_fontsize - 2)\n\n    ax_contour.set_xlabel(var_name[i], fontsize=legend_fontsize)\n    ax_contour.set_ylabel(var_name[j], fontsize=legend_fontsize)\n    ax_contour.tick_params(labelsize=legend_fontsize - 2)\n    if aspect_equal:\n        ax_contour.set_aspect(\"equal\")\n\n    # --- 3D surface ---\n    surf = ax_surface.plot_surface(\n        X_grid,\n        Y_grid,\n        Z_pred,\n        cmap=cmap,\n        vmin=min_z,\n        vmax=max_z,\n        linewidth=0,\n        antialiased=True,\n        alpha=0.8,\n    )\n    cb2 = fig.colorbar(surf, ax=ax_surface, shrink=0.7, pad=0.1)\n    cb2.ax.tick_params(labelsize=legend_fontsize - 2)\n\n    ax_surface.set_xlabel(var_name[i], fontsize=legend_fontsize)\n    ax_surface.set_ylabel(var_name[j], fontsize=legend_fontsize)\n    ax_surface.set_zlabel(\"f(x)\", fontsize=legend_fontsize)\n    ax_surface.tick_params(labelsize=legend_fontsize - 2)\n\n    # --- Optionally plot points ---\n    if plot_points and X_points is not None:\n        # Build + predict each point individually, using the same i/j from the row\n        # and the use_min/use_max logic for leftover dims. Store these predicted values\n        # as \"z_pred_for_point\".\n        z_pred_for_point = []\n        for row_idx in range(X_points.shape[0]):\n            single_p = np.zeros(n_dims)\n            single_p[i] = X_points[row_idx, i]\n            single_p[j] = X_points[row_idx, j]\n            for dim_idx in range(n_dims):\n                if dim_idx not in (i, j):\n                    single_p[dim_idx] = hidden_value(dim_idx)\n            val = model.predict(single_p.reshape(1, -1))\n            val = np.atleast_1d(val)  # ensure at least 1D\n            if isinstance(val, dict):\n                val = val.get(\"mean\", list(val.values())[0])\n            elif isinstance(val, tuple):\n                val = val[0]\n            z_pred_for_point.append(val[0] if hasattr(val, \"__len__\") else val)\n        z_pred_for_point = np.array(z_pred_for_point)\n\n        # Use the ground-truth y_points directly:\n        z_actual = np.array(y_points).flatten()  # ensures a 1D shape\n\n        on_mask = np.isclose(z_actual, z_pred_for_point, atol=atol)\n        below_mask = z_actual - atol / 2.0 &lt; z_pred_for_point\n        above_mask = z_actual + atol / 2.0 &gt; z_pred_for_point\n        num_correct = np.count_nonzero(on_mask)\n\n        # 2D contour scatter\n        ax_contour.scatter(\n            X_points[below_mask, i],\n            X_points[below_mask, j],\n            c=point_color_below,\n            edgecolor=\"black\",\n            s=points_size,\n            alpha=0.9,\n            zorder=5,\n        )\n        ax_contour.scatter(\n            X_points[above_mask, i],\n            X_points[above_mask, j],\n            c=point_color_above,\n            edgecolor=\"black\",\n            s=points_size,\n            alpha=0.9,\n            zorder=5,\n        )\n        ax_contour.scatter(\n            X_points[on_mask, i],\n            X_points[on_mask, j],\n            c=points_color,\n            edgecolor=\"black\",\n            s=points_size,\n            alpha=0.9,\n            zorder=5,\n        )\n        # 3D plot scatter\n        ax_surface.scatter(\n            X_points[below_mask, i],\n            X_points[below_mask, j],\n            z_actual[below_mask],\n            c=point_color_below,\n            edgecolor=\"black\",\n            s=points_size,\n            alpha=0.9,\n            zorder=10,  # Ensure points are rendered on top\n        )\n        ax_surface.scatter(\n            X_points[above_mask, i],\n            X_points[above_mask, j],\n            z_actual[above_mask],\n            c=point_color_above,\n            edgecolor=\"black\",\n            s=points_size,\n            alpha=0.9,\n            zorder=10,  # Ensure points are rendered on top\n        )\n        ax_surface.scatter(\n            X_points[on_mask, i],\n            X_points[on_mask, j],\n            z_actual[on_mask],\n            c=points_color,\n            edgecolor=\"black\",\n            s=points_size,\n            alpha=0.9,\n            zorder=10,  # Ensure points are rendered on top\n        )\n\n    # --- Optionally set aspect in 3D ---\n    if aspect_equal:\n        x_range = upper[i] - lower[i]\n        y_range = upper[j] - lower[j]\n        z_range = max_z - min_z if max_z &gt; min_z else 1\n        scale_z = (x_range + y_range) / (2.0 * z_range) if z_range else 1\n        ax_surface.set_box_aspect([1, (y_range / x_range) if x_range else 1, scale_z])\n\n    # --- Title, save, and show ---\n    if title:\n        updated_title = f\"{title}  Correct Points: {num_correct}\"\n        fig.suptitle(updated_title, fontsize=legend_fontsize + 2)\n\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n\n    if filename:\n        plt.savefig(filename, bbox_inches=\"tight\", dpi=dpi)\n\n    if show:\n        plt.show()\n\n    return fig, (ax_contour, ax_surface)\n</code></pre>"},{"location":"reference/spotpython/plot/contour/#spotpython.plot.contour.simple_contour","title":"<code>simple_contour(fun, min_x=-1, max_x=1, min_y=-1, max_y=1, min_z=None, max_z=None, n_samples=100, n_levels=30)</code>","text":"<p>Simple contour plot</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>_type_</code> <p>description</p> required <code>min_x</code> <code>int</code> <p>description. Defaults to -1.</p> <code>-1</code> <code>max_x</code> <code>int</code> <p>description. Defaults to 1.</p> <code>1</code> <code>min_y</code> <code>int</code> <p>description. Defaults to -1.</p> <code>-1</code> <code>max_y</code> <code>int</code> <p>description. Defaults to 1.</p> <code>1</code> <code>min_z</code> <code>int</code> <p>description. Defaults to 0.</p> <code>None</code> <code>max_z</code> <code>int</code> <p>description. Defaults to 1.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>description. Defaults to 100.</p> <code>100</code> <code>n_levels</code> <code>int</code> <p>description. Defaults to 5.</p> <code>30</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n    import numpy as np\n    from spotpython.fun.objectivefunctions import analytical\n    fun = analytical().fun_branin\n    simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15)\n</code></pre> Source code in <code>spotpython/plot/contour.py</code> <pre><code>def simple_contour(\n    fun,\n    min_x=-1,\n    max_x=1,\n    min_y=-1,\n    max_y=1,\n    min_z=None,\n    max_z=None,\n    n_samples=100,\n    n_levels=30,\n) -&gt; None:\n    \"\"\"\n    Simple contour plot\n\n    Args:\n        fun (_type_): _description_\n        min_x (int, optional): _description_. Defaults to -1.\n        max_x (int, optional): _description_. Defaults to 1.\n        min_y (int, optional): _description_. Defaults to -1.\n        max_y (int, optional): _description_. Defaults to 1.\n        min_z (int, optional): _description_. Defaults to 0.\n        max_z (int, optional): _description_. Defaults to 1.\n        n_samples (int, optional): _description_. Defaults to 100.\n        n_levels (int, optional): _description_. Defaults to 5.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n            import numpy as np\n            from spotpython.fun.objectivefunctions import analytical\n            fun = analytical().fun_branin\n            simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15)\n\n    \"\"\"\n    XX, YY = np.meshgrid(np.linspace(min_x, max_x, n_samples), np.linspace(min_y, max_y, n_samples))\n    zz = np.array([fun(np.array([xi, yi]).reshape(-1, 2)) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(n_samples, n_samples)\n    fig, ax = plt.subplots(figsize=(5, 2.7), layout=\"constrained\")\n    if min_z is None:\n        min_z = np.min(zz)\n    if max_z is None:\n        max_z = np.max(zz)\n    plt.contourf(\n        XX,\n        YY,\n        zz,\n        levels=np.linspace(min_z, max_z, n_levels),\n        zorder=1,\n        cmap=\"jet\",\n        vmin=min_z,\n        vmax=max_z,\n    )\n    plt.colorbar()\n</code></pre>"},{"location":"reference/spotpython/plot/importance/","title":"importance","text":""},{"location":"reference/spotpython/plot/importance/#spotpython.plot.importance.generate_imp","title":"<code>generate_imp(X_train, X_test, y_train, y_test, random_state=42, n_repeats=10, use_test=True)</code>","text":"<p>Generates permutation importances from a RandomForestRegressor.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame or ndarray</code> <p>The training feature set.</p> required <code>X_test</code> <code>DataFrame or ndarray</code> <p>The test feature set.</p> required <code>y_train</code> <code>Series or ndarray</code> <p>The training target variable.</p> required <code>y_test</code> <code>Series or ndarray</code> <p>The test target variable.</p> required <code>random_state</code> <code>int</code> <p>Random state for the RandomForestRegressor. Defaults to 42.</p> <code>42</code> <code>n_repeats</code> <code>int</code> <p>Number of repeats for permutation importance. Defaults to 10.</p> <code>10</code> <code>use_test</code> <code>bool</code> <p>If True, computes permutation importance on the test set. If False, uses the training set. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>permutation_importance</code> <code>permutation_importance</code> <p>Permutation importances object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.importance import generate_imp\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n&gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n&gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n&gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n&gt;&gt;&gt; y_train_series = pd.Series(y_train)\n&gt;&gt;&gt; y_test_series = pd.Series(y_test)\n&gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n&gt;&gt;&gt; print(perm_imp)\n</code></pre> Source code in <code>spotpython/plot/importance.py</code> <pre><code>def generate_imp(X_train, X_test, y_train, y_test, random_state=42, n_repeats=10, use_test=True) -&gt; permutation_importance:\n    \"\"\"\n    Generates permutation importances from a RandomForestRegressor.\n\n    Args:\n        X_train (pd.DataFrame or np.ndarray): The training feature set.\n        X_test (pd.DataFrame or np.ndarray): The test feature set.\n        y_train (pd.Series or np.ndarray): The training target variable.\n        y_test (pd.Series or np.ndarray): The test target variable.\n        random_state (int, optional): Random state for the RandomForestRegressor. Defaults to 42.\n        n_repeats (int, optional): Number of repeats for permutation importance. Defaults to 10.\n        use_test (bool, optional): If True, computes permutation importance on the test set. If False, uses the training set. Defaults to True.\n\n    Returns:\n        permutation_importance: Permutation importances object.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.importance import generate_imp\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n        &gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n        &gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n        &gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n        &gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n        &gt;&gt;&gt; y_train_series = pd.Series(y_train)\n        &gt;&gt;&gt; y_test_series = pd.Series(y_test)\n        &gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n        &gt;&gt;&gt; print(perm_imp)\n    \"\"\"\n    # Convert inputs to pandas DataFrames/Series if they are not already\n    if not isinstance(X_train, pd.DataFrame):\n        X_train = pd.DataFrame(X_train)\n    if not isinstance(X_test, pd.DataFrame):\n        X_test = pd.DataFrame(X_test)\n    if not isinstance(y_train, pd.Series):\n        y_train = pd.Series(np.ravel(y_train))  # Use np.ravel instead of flatten\n    if not isinstance(y_test, pd.Series):\n        y_test = pd.Series(np.ravel(y_test))  # Use np.ravel instead of flatten\n\n    # Train a Random Forest Regressor\n    rf = RandomForestRegressor(random_state=random_state)\n    rf.fit(X_train, y_train)\n\n    # Select the dataset for permutation importance\n    X_eval = X_test if use_test else X_train\n    y_eval = y_test if use_test else y_train\n\n    # Calculate permutation importances\n    perm_imp = permutation_importance(rf, X_eval, y_eval, n_repeats=n_repeats, random_state=random_state)\n\n    return perm_imp\n</code></pre>"},{"location":"reference/spotpython/plot/importance/#spotpython.plot.importance.generate_mdi","title":"<code>generate_mdi(X, y, feature_names=None, random_state=42)</code>","text":"<p>Generates a DataFrame with Gini importances from a RandomForestRegressor.</p> Notes <p>There are two limitations of impurity-based feature importances:    - impurity-based importances are biased towards high cardinality features;    - impurity-based importances are computed on training set statistics    and therefore do not reflect the ability of feature to be useful to    make predictions that generalize to the test set. Permutation    importances can mitigate the last limitation, because ti can be computed on the    test set.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame or ndarray</code> <p>The feature set.</p> required <code>y</code> <code>Series or ndarray</code> <p>The target variable.</p> required <code>feature_names</code> <code>list</code> <p>List of feature names for labeling. Defaults to None.</p> <code>None</code> <code>random_state</code> <code>int</code> <p>Random state for the RandomForestRegressor. Defaults to 42.</p> <code>42</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with \u2018Feature\u2019 and \u2018Importance\u2019 columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.importance import generate_mdi\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_df = pd.DataFrame(X)\n&gt;&gt;&gt; y_series = pd.Series(y)\n&gt;&gt;&gt; result = generate_mdi(X_df, y_series)\n&gt;&gt;&gt; print(result)\n</code></pre> Source code in <code>spotpython/plot/importance.py</code> <pre><code>def generate_mdi(X, y, feature_names=None, random_state=42) -&gt; pd.DataFrame:\n    \"\"\"\n    Generates a DataFrame with Gini importances from a RandomForestRegressor.\n\n    Notes:\n     There are two limitations of impurity-based feature importances:\n        - impurity-based importances are biased towards high cardinality features;\n        - impurity-based importances are computed on training set statistics\n        and therefore do not reflect the ability of feature to be useful to\n        make predictions that generalize to the test set. Permutation\n        importances can mitigate the last limitation, because ti can be computed on the\n        test set.\n\n    Args:\n        X (pd.DataFrame or np.ndarray): The feature set.\n        y (pd.Series or np.ndarray): The target variable.\n        feature_names (list, optional): List of feature names for labeling. Defaults to None.\n        random_state (int, optional): Random state for the RandomForestRegressor. Defaults to 42.\n\n    Returns:\n        pd.DataFrame: DataFrame with 'Feature' and 'Importance' columns.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.importance import generate_mdi\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n        &gt;&gt;&gt; X_df = pd.DataFrame(X)\n        &gt;&gt;&gt; y_series = pd.Series(y)\n        &gt;&gt;&gt; result = generate_mdi(X_df, y_series)\n        &gt;&gt;&gt; print(result)\n\n    \"\"\"\n    # Convert X and y to pandas DataFrames if they are not already\n    if not isinstance(X, pd.DataFrame):\n        X = pd.DataFrame(X)\n    if not isinstance(y, pd.Series):\n        y = pd.Series(np.ravel(y))  # Use np.ravel instead of flatten\n\n    # Train a Random Forest Regressor\n    rf = RandomForestRegressor(random_state=random_state)\n    rf.fit(X, y)\n\n    # Get feature importances\n    importances = rf.feature_importances_\n\n    # Create a DataFrame\n    if feature_names is None:\n        df_mdi = pd.DataFrame({\"Feature\": X.columns, \"Importance\": importances})\n    else:\n        df_mdi = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n    df_mdi = df_mdi.sort_values(\"Importance\", ascending=False).reset_index(drop=True)\n\n    return df_mdi\n</code></pre>"},{"location":"reference/spotpython/plot/importance/#spotpython.plot.importance.plot_importances","title":"<code>plot_importances(df_mdi, perm_imp, X_test, target_name=None, feature_names=None, k=10, show=True)</code>","text":"<p>Plots the impurity-based and permutation-based feature importances for a given classifier.</p> <p>Parameters:</p> Name Type Description Default <code>df_mdi</code> <code>DataFrame</code> <p>DataFrame with Gini importances.</p> required <code>perm_imp</code> <code>object</code> <p>Permutation importances object.</p> required <code>X_test</code> <code>DataFrame</code> <p>The test feature set for permutation importance.</p> required <code>target_name</code> <code>str</code> <p>Name of the target variable for labeling. Defaults to None.</p> <code>None</code> <code>feature_names</code> <code>list</code> <p>List of feature names for labeling. Defaults to None.</p> <code>None</code> <code>k</code> <code>int</code> <p>Number of top features to display based on importance. Default is 10.</p> <code>10</code> <code>show</code> <code>bool</code> <p>If True, displays the plot immediately. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.importance import generate_mdi, generate_imp, plot_importances\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n&gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n&gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n&gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n&gt;&gt;&gt; y_train_series = pd.Series(y_train)\n&gt;&gt;&gt; y_test_series = pd.Series(y_test)\n&gt;&gt;&gt; df_mdi = generate_mdi(X_train_df, y_train_series)\n&gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n&gt;&gt;&gt; plot_importances(df_mdi, perm_imp, X_test_df)\n</code></pre> Source code in <code>spotpython/plot/importance.py</code> <pre><code>def plot_importances(df_mdi, perm_imp, X_test, target_name=None, feature_names=None, k=10, show=True) -&gt; None:\n    \"\"\"\n    Plots the impurity-based and permutation-based feature importances for a given classifier.\n\n    Args:\n        df_mdi (pd.DataFrame):\n            DataFrame with Gini importances.\n        perm_imp (object):\n            Permutation importances object.\n        X_test (pd.DataFrame):\n            The test feature set for permutation importance.\n        target_name (str, optional):\n            Name of the target variable for labeling. Defaults to None.\n        feature_names (list, optional):\n            List of feature names for labeling. Defaults to None.\n        k (int, optional):\n            Number of top features to display based on importance. Default is 10.\n        show (bool, optional):\n            If True, displays the plot immediately. Default is True.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.importance import generate_mdi, generate_imp, plot_importances\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n        &gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n        &gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n        &gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n        &gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n        &gt;&gt;&gt; y_train_series = pd.Series(y_train)\n        &gt;&gt;&gt; y_test_series = pd.Series(y_test)\n        &gt;&gt;&gt; df_mdi = generate_mdi(X_train_df, y_train_series)\n        &gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n        &gt;&gt;&gt; plot_importances(df_mdi, perm_imp, X_test_df)\n\n    \"\"\"\n\n    # Plot impurity-based importances for top-k features\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n\n    sorted_mdi_importances = df_mdi.set_index(\"Feature\")[\"Importance\"]\n    sorted_mdi_importances[:k].sort_values().plot.barh(ax=ax1)\n    ax1.set_xlabel(\"Gini importance\")\n    if target_name:\n        ax1.set_title(f\"Impurity-based feature importances for target: {target_name}\")\n    else:\n        ax1.set_title(\"Impurity-based feature importances\")\n\n    # Ensure X_test is a DataFrame\n    if not isinstance(X_test, pd.DataFrame):\n        X_test = pd.DataFrame(X_test)\n\n    perm_sorted_idx = perm_imp.importances_mean.argsort()[-k:]\n    if feature_names is not None:\n        ax2.boxplot(perm_imp.importances[perm_sorted_idx].T, vert=False, labels=np.array(feature_names)[perm_sorted_idx])\n    else:\n        ax2.boxplot(perm_imp.importances[perm_sorted_idx].T, vert=False, labels=X_test.columns[perm_sorted_idx])\n    ax2.axvline(x=0, color=\"k\", linestyle=\"--\")\n    if target_name:\n        ax2.set_xlabel(f\"Decrease in mse for target: {target_name}\")\n    else:\n        ax2.set_xlabel(\"Decrease in mse\")\n    ax2.set_title(\"Permutation-based feature importances\")\n\n    # fig.suptitle(\"Impurity-based vs. permutation importances\")\n    fig.tight_layout()\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/ts/","title":"ts","text":""},{"location":"reference/spotpython/plot/ts/#spotpython.plot.ts.plot_friedman_drift_data","title":"<code>plot_friedman_drift_data(n_samples, seed, change_point1, change_point2, constant=True, show=True, filename=None)</code>","text":"<p>Plot the Friedman dataset with drifts at change_point1 and change_point2.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> required <code>seed</code> <code>int</code> <p>Seed for the random number generator.</p> required <code>change_point1</code> <code>int</code> <p>Index of the first drift point.</p> required <code>change_point2</code> <code>int</code> <p>Index of the second drift point.</p> required <code>constant</code> <code>bool</code> <p>If True, the drifts are constant. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>Name of the file to save the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.ts import plot_friedman_drift_data\n&gt;&gt;&gt; plot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\n&gt;&gt;&gt; plot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=True)\n</code></pre> Source code in <code>spotpython/plot/ts.py</code> <pre><code>def plot_friedman_drift_data(n_samples, seed, change_point1, change_point2, constant=True, show=True, filename=None) -&gt; None:\n    \"\"\"Plot the Friedman dataset with drifts at change_point1 and change_point2.\n\n    Args:\n        n_samples (int):\n            Number of samples to generate.\n        seed (int):\n            Seed for the random number generator.\n        change_point1 (int):\n            Index of the first drift point.\n        change_point2 (int):\n            Index of the second drift point.\n        constant (bool, optional):\n            If True, the drifts are constant. Defaults to True.\n        filename (str, optional):\n            Name of the file to save the plot. Defaults to None.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.ts import plot_friedman_drift_data\n        &gt;&gt;&gt; plot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\n        &gt;&gt;&gt; plot_friedman_drift_data(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=True)\n    \"\"\"\n    data_generator = FriedmanDriftDataset(n_samples=n_samples, seed=seed, change_point1=change_point1, change_point2=change_point2, constant=constant)\n    data = [data for data in data_generator]\n    indices = [i for _, _, i in data]\n    values = {f\"x{i}\": [] for i in range(6)}\n    values[\"y\"] = []\n    for x, y, _ in data:\n        for i in range(6):\n            values[f\"x{i}\"].append(x[i])\n        values[\"y\"].append(y)\n\n    plt.figure(figsize=(10, 6))\n    for label, series in values.items():\n        plt.plot(indices, series, label=label)\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n    plt.axvline(x=change_point1, color=\"k\", linestyle=\"--\", label=\"Drift Point 1\")\n    plt.axvline(x=change_point2, color=\"r\", linestyle=\"--\", label=\"Drift Point 2\")\n    plt.legend()\n    plt.grid(True)\n    if filename is not None:\n        plt.savefig(filename)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/utils/","title":"utils","text":""},{"location":"reference/spotpython/plot/utils/#spotpython.plot.utils.save_or_show_plot","title":"<code>save_or_show_plot(plt, filename=None)</code>","text":"<p>Save or show the plot based on the provided filename.</p> <p>Parameters:</p> Name Type Description Default <code>plt</code> <code>pyplot</code> <p>The matplotlib pyplot object to save or show.</p> required <code>filename</code> <code>str</code> <p>The name of the file to save the plot. If None, the plot will be shown instead. Supported formats: \u2018pdf\u2019, \u2018png\u2019. If a filename is provided, it must end with either \u2018.pdf\u2019 or \u2018.png\u2019. If the filename is invalid, a ValueError will be raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the filename does not end with \u2018.pdf\u2019 or \u2018.png\u2019.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.utils import save_or_show_plot\n&gt;&gt;&gt; save_or_show_plot(\"plot.pdf\")\n&gt;&gt;&gt; save_or_show_plot(\"plot.png\")\n&gt;&gt;&gt; save_or_show_plot()  # This will show the plot\n</code></pre> Source code in <code>spotpython/plot/utils.py</code> <pre><code>def save_or_show_plot(plt, filename=None) -&gt; None:\n    \"\"\"\n    Save or show the plot based on the provided filename.\n\n    Args:\n        plt (matplotlib.pyplot):\n            The matplotlib pyplot object to save or show.\n        filename (str, optional):\n            The name of the file to save the plot. If None, the plot will be shown instead.\n            Supported formats: 'pdf', 'png'.\n            If a filename is provided, it must end with either '.pdf' or '.png'.\n            If the filename is invalid, a ValueError will be raised.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the filename does not end with '.pdf' or '.png'.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.utils import save_or_show_plot\n        &gt;&gt;&gt; save_or_show_plot(\"plot.pdf\")\n        &gt;&gt;&gt; save_or_show_plot(\"plot.png\")\n        &gt;&gt;&gt; save_or_show_plot()  # This will show the plot\n    \"\"\"\n    # Save or show the plot\n    if filename:\n        if filename.endswith(\".pdf\") or filename.endswith(\".png\"):\n            plt.savefig(filename, format=filename.split(\".\")[-1], bbox_inches=\"tight\")\n        else:\n            raise ValueError(\"Filename must have a valid suffix: '.pdf' or '.png'.\")\n    else:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/validation/","title":"validation","text":""},{"location":"reference/spotpython/plot/validation/#spotpython.plot.validation.plot_actual_vs_predicted","title":"<code>plot_actual_vs_predicted(y_test, y_pred, title=None, show=True, filename=None)</code>","text":"<p>Plot actual vs. predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>ndarray</code> <p>True values.</p> required <code>y_pred</code> <code>ndarray</code> <p>Predicted values.</p> required <code>title</code> <code>str</code> <p>Title of the plot. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>Name of the file to save the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n    from sklearn.linear_model import LinearRegression\n    from spotpython.plot.validation import plot_actual_vs_predicted\n    X, y = load_diabetes(return_X_y=True)\n    lr = LinearRegression()\n    lr.fit(X, y)\n    y_pred = lr.predict(X)\n    plot_actual_vs_predicted(y, y_pred)\n</code></pre> Source code in <code>spotpython/plot/validation.py</code> <pre><code>def plot_actual_vs_predicted(y_test, y_pred, title=None, show=True, filename=None) -&gt; None:\n    \"\"\"Plot actual vs. predicted values.\n\n    Args:\n        y_test (np.ndarray):\n            True values.\n        y_pred (np.ndarray):\n            Predicted values.\n        title (str, optional):\n            Title of the plot. Defaults to None.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to True.\n        filename (str, optional):\n            Name of the file to save the plot. Defaults to None.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n            from sklearn.linear_model import LinearRegression\n            from spotpython.plot.validation import plot_actual_vs_predicted\n            X, y = load_diabetes(return_X_y=True)\n            lr = LinearRegression()\n            lr.fit(X, y)\n            y_pred = lr.predict(X)\n            plot_actual_vs_predicted(y, y_pred)\n    \"\"\"\n    fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"actual_vs_predicted\",\n        subsample=100,\n        ax=axs[0],\n        random_state=0,\n        scatter_kwargs={\"alpha\": 0.5},\n    )\n    axs[0].set_title(\"Actual vs. Predicted values\")\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"residual_vs_predicted\",\n        subsample=100,\n        ax=axs[1],\n        random_state=0,\n    )\n    axs[1].set_title(\"Residuals vs. Predicted Values\")\n    if title is not None:\n        fig.suptitle(title)\n    plt.tight_layout()\n    if filename is not None:\n        plt.savefig(filename)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/validation/#spotpython.plot.validation.plot_confusion_matrix","title":"<code>plot_confusion_matrix(model=None, fun_control=None, df=None, title=None, target_names=None, y_true_name=None, y_pred_name=None, show=False, ax=None)</code>","text":"<p>Plotting a confusion matrix. If a model and the fun_control dictionary are passed, the confusion matrix is computed. If a dataframe is passed, the confusion matrix is computed from the dataframe. In this case, the names of the columns with the true and the predicted values must be specified. Default the dataframe is None.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Sklearn model. The model to be used for cross-validation. Defaults to None.</p> <code>None</code> <code>fun_control</code> <code>Dict</code> <p>Dictionary containing the data and the target column. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to None.</p> <code>None</code> <code>df</code> <code>DataFrame</code> <p>Dataframe containing the predictions and the target column. Defaults to None.</p> <code>None</code> <code>target_names</code> <code>List[str]</code> <p>List of target names. Defaults to None.</p> <code>None</code> <code>y_true_name</code> <code>str</code> <p>Name of the column with the true values if a dataframe is specified. Defaults to None.</p> <code>None</code> <code>y_pred_name</code> <code>str</code> <p>Name of the column with the predicted values if a dataframe is specified. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to False.</p> <code>False</code> <code>ax</code> <code>AxesSubplot</code> <p>Axes to plot the confusion matrix. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotpython/plot/validation.py</code> <pre><code>def plot_confusion_matrix(\n    model=None,\n    fun_control=None,\n    df=None,\n    title=None,\n    target_names=None,\n    y_true_name=None,\n    y_pred_name=None,\n    show=False,\n    ax=None,\n):\n    \"\"\"\n    Plotting a confusion matrix. If a model and the fun_control dictionary are passed,\n    the confusion matrix is computed. If a dataframe is passed, the confusion matrix is\n    computed from the dataframe. In this case, the names of the columns with the true and\n    the predicted values must be specified. Default the dataframe is None.\n\n    Args:\n        model (Any, optional):\n            Sklearn model. The model to be used for cross-validation. Defaults to None.\n        fun_control (Dict, optional):\n            Dictionary containing the data and the target column. Defaults to None.\n        title (str, optional):\n            Title of the plot. Defaults to None.\n        df (pd.DataFrame, optional):\n            Dataframe containing the predictions and the target column. Defaults to None.\n        target_names (List[str], optional):\n            List of target names. Defaults to None.\n        y_true_name (str, optional):\n            Name of the column with the true values if a dataframe is specified. Defaults to None.\n        y_pred_name (str, optional):\n            Name of the column with the predicted values if a dataframe is specified. Defaults to None.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to False.\n        ax (matplotlib.axes._subplots.AxesSubplot, optional):\n            Axes to plot the confusion matrix. Defaults to None.\n\n    Returns:\n        (NoneType): None\n\n    \"\"\"\n    if df is not None:\n        # assign the column y_true_name from df to y_true\n        y_true = df[y_true_name]\n        # assign the column y_pred_name from df to y_pred\n        y_pred = df[y_pred_name]\n    else:\n        X_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n        X_test, y_true = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 5))\n    ConfusionMatrixDisplay.from_predictions(y_true=y_true, y_pred=y_pred, ax=ax, colorbar=False)\n    if target_names is not None:\n        ax.xaxis.set_ticklabels(target_names)\n        ax.yaxis.set_ticklabels(target_names)\n    if title is not None:\n        _ = ax.set_title(title)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/validation/#spotpython.plot.validation.plot_cv_predictions","title":"<code>plot_cv_predictions(model, fun_control, show=True)</code>","text":"<p>Plots cross-validated predictions for regression.</p> <p>Uses <code>sklearn.model_selection.cross_val_predict</code> together with <code>sklearn.metrics.PredictionErrorDisplay</code> to visualize prediction errors. It is based on the example from the scikit-learn documentation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-download-auto-examples-model-selection-plot-cv-predict-py</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Sklearn model. The model to be used for cross-validation.</p> required <code>fun_control</code> <code>Dict</code> <p>Dictionary containing the data and the target column.</p> required <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; X, y = load_diabetes(return_X_y=True)\n&gt;&gt;&gt; lr = LinearRegression()\n&gt;&gt;&gt; plot_cv_predictions(lr, fun_control)\n</code></pre> Source code in <code>spotpython/plot/validation.py</code> <pre><code>def plot_cv_predictions(model: Any, fun_control: Dict, show=True) -&gt; None:\n    \"\"\"\n    Plots cross-validated predictions for regression.\n\n    Uses `sklearn.model_selection.cross_val_predict` together with\n    `sklearn.metrics.PredictionErrorDisplay` to visualize prediction errors.\n    It is based on the example from the scikit-learn documentation:\n    https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-download-auto-examples-model-selection-plot-cv-predict-py\n\n    Args:\n        model (Any):\n            Sklearn model. The model to be used for cross-validation.\n        fun_control (Dict):\n            Dictionary containing the data and the target column.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to True.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; X, y = load_diabetes(return_X_y=True)\n        &gt;&gt;&gt; lr = LinearRegression()\n        &gt;&gt;&gt; plot_cv_predictions(lr, fun_control)\n    \"\"\"\n    X_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n    # cross_val_predict returns an array of the same size of y\n    # where each entry is a prediction obtained by cross validation.\n    y_pred = cross_val_predict(model, X_test, y_test, cv=10)\n    fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"actual_vs_predicted\",\n        subsample=100,\n        ax=axs[0],\n        random_state=0,\n    )\n    axs[0].set_title(\"Actual vs. Predicted values\")\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"residual_vs_predicted\",\n        subsample=100,\n        ax=axs[1],\n        random_state=0,\n    )\n    axs[1].set_title(\"Residuals vs. Predicted Values\")\n    fig.suptitle(\"Plotting cross-validated predictions\")\n    plt.tight_layout()\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/validation/#spotpython.plot.validation.plot_roc","title":"<code>plot_roc(model_list, fun_control, alpha=0.8, model_names=None, show=True)</code>","text":"<p>Plots ROC curves for a list of models using the Visualization API from scikit-learn.</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>List[BaseEstimator]</code> <p>A list of scikit-learn models to plot ROC curves for.</p> required <code>fun_control</code> <code>Dict[str, Union[str, DataFrame]]</code> <p>A dictionary containing the train and test dataframes and the target column name.</p> required <code>alpha</code> <code>float</code> <p>The alpha value for the ROC curve. Defaults to 0.8.</p> <code>0.8</code> <code>model_names</code> <code>List[str]</code> <p>A list of names for the models. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n&gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X_train = iris.data[:100]\n&gt;&gt;&gt; y_train = iris.target[:100]\n&gt;&gt;&gt; X_test = iris.data[100:]\n&gt;&gt;&gt; y_test = iris.target[100:]\n&gt;&gt;&gt; train_df = pd.DataFrame(X_train, columns=iris.feature_names)\n&gt;&gt;&gt; train_df['target'] = y_train\n&gt;&gt;&gt; test_df = pd.DataFrame(X_test, columns=iris.feature_names)\n&gt;&gt;&gt; test_df['target'] = y_test\n&gt;&gt;&gt; fun_control = {\"train\": train_df, \"test\": test_df, \"target_column\": \"target\"}\n&gt;&gt;&gt; model_list = [LogisticRegression(), DecisionTreeClassifier()]\n&gt;&gt;&gt; model_names = [\"Logistic Regression\", \"Decision Tree\"]\n&gt;&gt;&gt; plot_roc(model_list, fun_control, model_names=model_names)\n</code></pre> Source code in <code>spotpython/plot/validation.py</code> <pre><code>def plot_roc(\n    model_list: List[BaseEstimator],\n    fun_control: Dict[str, Union[str, pd.DataFrame]],\n    alpha: float = 0.8,\n    model_names: List[str] = None,\n    show=True,\n) -&gt; None:\n    \"\"\"\n    Plots ROC curves for a list of models using the Visualization API from scikit-learn.\n\n    Args:\n        model_list (List[BaseEstimator]):\n            A list of scikit-learn models to plot ROC curves for.\n        fun_control (Dict[str, Union[str, pd.DataFrame]]):\n            A dictionary containing the train and test dataframes and the target column name.\n        alpha (float, optional):\n            The alpha value for the ROC curve. Defaults to 0.8.\n        model_names (List[str], optional):\n            A list of names for the models. Defaults to None.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to True.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n        &gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; X_train = iris.data[:100]\n        &gt;&gt;&gt; y_train = iris.target[:100]\n        &gt;&gt;&gt; X_test = iris.data[100:]\n        &gt;&gt;&gt; y_test = iris.target[100:]\n        &gt;&gt;&gt; train_df = pd.DataFrame(X_train, columns=iris.feature_names)\n        &gt;&gt;&gt; train_df['target'] = y_train\n        &gt;&gt;&gt; test_df = pd.DataFrame(X_test, columns=iris.feature_names)\n        &gt;&gt;&gt; test_df['target'] = y_test\n        &gt;&gt;&gt; fun_control = {\"train\": train_df, \"test\": test_df, \"target_column\": \"target\"}\n        &gt;&gt;&gt; model_list = [LogisticRegression(), DecisionTreeClassifier()]\n        &gt;&gt;&gt; model_names = [\"Logistic Regression\", \"Decision Tree\"]\n        &gt;&gt;&gt; plot_roc(model_list, fun_control, model_names=model_names)\n    \"\"\"\n    X_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n    X_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n    ax = plt.gca()\n    for i, model in enumerate(model_list):\n        model.fit(X_train, y_train)\n        if model_names is not None:\n            model_name = model_names[i]\n        else:\n            model_name = None\n        y_pred = model.predict(X_test)\n        RocCurveDisplay.from_predictions(y_test, y_pred, ax=ax, alpha=alpha, name=model_name)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/validation/#spotpython.plot.validation.plot_roc_from_dataframes","title":"<code>plot_roc_from_dataframes(df_list, alpha=0.8, model_names=None, target_column=None, show=True, title='', tkagg=False)</code>","text":"<p>Plot ROC curve for a list of dataframes from model evaluations.</p> <p>Parameters:</p> Name Type Description Default <code>df_list</code> <code>List[DataFrame]</code> <p>List of dataframes with results from models.</p> required <code>alpha</code> <code>float</code> <p>Transparency of the plotted lines.</p> <code>0.8</code> <code>model_names</code> <code>List[str]</code> <p>List of model names.</p> <code>None</code> <code>target_column</code> <code>str</code> <p>Name of the target column.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown.</p> <code>True</code> <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>''</code> <code>tkagg</code> <code>bool</code> <p>If True, the TkAgg backend is used. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n    from spotpython.plot.validation import plot_roc_from_dataframes\n    df1 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,0,0]})\n    df2 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,1,1]})\n    df_list = [df1, df2]\n    model_names = [\"Model 1\", \"Model 2\"]\n    plot_roc_from_dataframes(df_list, model_names=model_names, target_column=\"y\")\n</code></pre> Source code in <code>spotpython/plot/validation.py</code> <pre><code>def plot_roc_from_dataframes(\n    df_list: List[pd.DataFrame],\n    alpha: float = 0.8,\n    model_names: List[str] = None,\n    target_column: str = None,\n    show: bool = True,\n    title: str = \"\",\n    tkagg: bool = False,\n) -&gt; None:\n    \"\"\"\n    Plot ROC curve for a list of dataframes from model evaluations.\n\n    Args:\n        df_list:\n            List of dataframes with results from models.\n        alpha:\n            Transparency of the plotted lines.\n        model_names:\n            List of model names.\n        target_column:\n            Name of the target column.\n        show:\n            If True, the plot is shown.\n        title:\n            Title of the plot.\n        tkagg:\n            If True, the TkAgg backend is used.\n            Default is False.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n            from spotpython.plot.validation import plot_roc_from_dataframes\n            df1 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,0,0]})\n            df2 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,1,1]})\n            df_list = [df1, df2]\n            model_names = [\"Model 1\", \"Model 2\"]\n            plot_roc_from_dataframes(df_list, model_names=model_names, target_column=\"y\")\n\n    \"\"\"\n    if tkagg:\n        matplotlib.use(\"TkAgg\")\n    fig, ax = plt.subplots(figsize=(10, 5))\n    for i, df in enumerate(df_list):\n        y_test = df[target_column]\n        y_pred = df[\"Prediction\"]\n        if model_names is not None:\n            model_name = model_names[i]\n        else:\n            model_name = None\n        RocCurveDisplay.from_predictions(y_test, y_pred, ax=ax, alpha=alpha, name=model_name)\n    # add a title to the plot\n    ax.set_title(title)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/xai/","title":"xai","text":""},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.check_for_nans","title":"<code>check_for_nans(data, layer_index)</code>","text":"<p>Checks for NaN values in the tensor data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The tensor to check for NaN values.</p> required <code>layer_index</code> <code>int</code> <p>The index of the layer for logging purposes.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if NaNs are found, False otherwise.</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def check_for_nans(data, layer_index) -&gt; bool:\n    \"\"\"Checks for NaN values in the tensor data.\n\n    Args:\n        data (torch.Tensor): The tensor to check for NaN values.\n        layer_index (int): The index of the layer for logging purposes.\n\n    Returns:\n        bool: True if NaNs are found, False otherwise.\n    \"\"\"\n    if torch.isnan(data).any():\n        print(f\"NaN detected after layer {layer_index}\")\n        return True\n    return False\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_activations","title":"<code>get_activations(net, fun_control, batch_size, device='cpu', normalize=False)</code>","text":"<p>Computes the activations for each layer of the network, the mean activations, and the sizes of the activations for each layer.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>The neural network model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for the data loader.</p> required <code>device</code> <code>str</code> <p>The device to run the model on. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the activations, mean activations, and layer sizes for each layer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.xai import get_activations\n    import torch\n    import numpy as np\n    import torch.nn as nn\n    from spotpython.utils.init import fun_control_init\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (\n            get_default_hyperparameters_as_array, get_one_config_from_X)\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.plot.xai import get_gradients\n    fun_control = fun_control_init(\n        _L_in=10, # 10: diabetes\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\",\n        data_set=Diabetes(),\n        core_model=NNLinearRegressor,\n        hyperdict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    batch_size = 16\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n    activations, mean_activations, layer_sizes = get_activations(net=model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n    plot_nn_values_scatter(nn_values=activations, layer_sizes=layer_sizes, nn_values_names=\"Activations\")\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_activations(net, fun_control, batch_size, device=\"cpu\", normalize=False) -&gt; tuple:\n    \"\"\"\n    Computes the activations for each layer of the network, the mean activations,\n    and the sizes of the activations for each layer.\n\n    Args:\n        net (nn.Module): The neural network model.\n        fun_control (dict): A dictionary containing the dataset.\n        batch_size (int): The batch size for the data loader.\n        device (str): The device to run the model on. Defaults to \"cpu\".\n        normalize (bool): Whether to normalize the input data. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing the activations, mean activations, and layer sizes for each layer.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.xai import get_activations\n            import torch\n            import numpy as np\n            import torch.nn as nn\n            from spotpython.utils.init import fun_control_init\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (\n                    get_default_hyperparameters_as_array, get_one_config_from_X)\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.plot.xai import get_gradients\n            fun_control = fun_control_init(\n                _L_in=10, # 10: diabetes\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\",\n                data_set=Diabetes(),\n                core_model=NNLinearRegressor,\n                hyperdict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            config = get_one_config_from_X(X, fun_control)\n            _L_in = fun_control[\"_L_in\"]\n            _L_out = fun_control[\"_L_out\"]\n            _torchmetric = fun_control[\"_torchmetric\"]\n            batch_size = 16\n            model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n            activations, mean_activations, layer_sizes = get_activations(net=model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n            plot_nn_values_scatter(nn_values=activations, layer_sizes=layer_sizes, nn_values_names=\"Activations\")\n    \"\"\"\n    activations = {}\n    mean_activations = {}\n    layer_sizes = {}\n    net.eval()  # Set the model to evaluation mode\n\n    dataset = fun_control[\"data_set\"]\n    data_module = LightDataModule(\n        dataset=dataset,\n        batch_size=batch_size,\n        test_size=fun_control[\"test_size\"],\n        scaler=fun_control[\"scaler\"],\n        verbosity=10,\n    )\n    data_module.setup(stage=\"fit\")\n    train_loader = data_module.train_dataloader()\n    inputs, _ = next(iter(train_loader))\n    inputs = inputs.to(device)\n\n    if normalize:\n        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n\n    with torch.no_grad():\n        inputs = inputs.view(inputs.size(0), -1)\n        # Loop through all layers\n        for layer_index, layer in enumerate(net.layers[:-1]):\n            inputs = layer(inputs)  # Forward pass through the layer\n\n            # Check for NaNs\n            if check_for_nans(inputs, layer_index):\n                break\n\n            # Collect activations for Linear layers\n            if isinstance(layer, nn.Linear):\n                activations[layer_index] = inputs.view(-1).cpu().numpy()\n                mean_activations[layer_index] = inputs.mean(dim=0).cpu().numpy()\n                # Record the size of the activations and set the first dimension to 1\n                layer_size = np.array(inputs.size())\n                layer_size[0] = 1  # Set the first dimension to 1\n                layer_sizes[layer_index] = layer_size\n\n    return activations, mean_activations, layer_sizes\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_all_layers_conductance","title":"<code>get_all_layers_conductance(spot_tuner, fun_control, device='cpu', remove_spot_attributes=False)</code>","text":"<p>Get the conductance of all layers.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>The spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>remove_spot_attributes</code> <code>bool</code> <p>Whether to remove the spot attributes. Defaults to False.</p> <code>False</code> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_all_layers_conductance(spot_tuner, fun_control, device=\"cpu\", remove_spot_attributes=False) -&gt; dict:\n    \"\"\"\n    Get the conductance of all layers.\n\n    Args:\n        spot_tuner (object):\n            The spot tuner object.\n        fun_control (dict):\n            A dictionary with the function control.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        remove_spot_attributes (bool, optional):\n            Whether to remove the spot attributes. Defaults to False.\n    \"\"\"\n    config = get_tuned_architecture(spot_tuner, fun_control)\n    train_model(config, fun_control, timestamp=False)\n    model_loaded = load_light_from_checkpoint(config, fun_control, postfix=\"_TRAIN\")\n    if remove_spot_attributes:\n        removed_attributes, model = get_removed_attributes_and_base_net(net=model_loaded)\n    else:\n        model = model_loaded\n    model = model.to(device)\n    model.eval()\n    _, index, _ = get_weights(model, return_index=True)\n    layer_conductance = {}\n    for i in index:\n        layer_conductance[i] = get_layer_conductance(spot_tuner, fun_control, layer_idx=i)\n    return layer_conductance\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_attributions","title":"<code>get_attributions(spot_tuner, fun_control, attr_method='IntegratedGradients', baseline=None, abs_attr=True, n_rel=5, device='cpu', normalize=True, remove_spot_attributes=False)</code>","text":"<p>Get the attributions of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>The spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>attr_method</code> <code>str</code> <p>The attribution method. Defaults to \u201cIntegratedGradients\u201d.</p> <code>'IntegratedGradients'</code> <code>baseline</code> <code>Tensor</code> <p>The baseline for the attribution methods. Defaults to None.</p> <code>None</code> <code>abs_attr</code> <code>bool</code> <p>Whether the method should sort by the absolute attribution values. Defaults to True.</p> <code>True</code> <code>n_rel</code> <code>int</code> <p>The number of relevant features. Defaults to 5.</p> <code>5</code> <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to True.</p> <code>True</code> <code>remove_spot_attributes</code> <code>bool</code> <p>Whether to remove the spot attributes. If True, a torch model is created via <code>get_removed_attributes</code>. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame (object): A DataFrame with the attributions.</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_attributions(\n    spot_tuner,\n    fun_control,\n    attr_method=\"IntegratedGradients\",\n    baseline=None,\n    abs_attr=True,\n    n_rel=5,\n    device=\"cpu\",\n    normalize=True,\n    remove_spot_attributes=False,\n) -&gt; pd.DataFrame:\n    \"\"\"Get the attributions of a neural network.\n\n    Args:\n        spot_tuner (object):\n            The spot tuner object.\n        fun_control (dict):\n            A dictionary with the function control.\n        attr_method (str, optional):\n            The attribution method. Defaults to \"IntegratedGradients\".\n        baseline (torch.Tensor, optional):\n            The baseline for the attribution methods. Defaults to None.\n        abs_attr (bool, optional):\n            Whether the method should sort by the absolute attribution values. Defaults to True.\n        n_rel (int, optional):\n            The number of relevant features. Defaults to 5.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        normalize (bool, optional):\n            Whether to normalize the input data. Defaults to True.\n        remove_spot_attributes (bool, optional):\n            Whether to remove the spot attributes.\n            If True, a torch model is created via `get_removed_attributes`. Defaults to False.\n\n    Returns:\n        pd.DataFrame (object): A DataFrame with the attributions.\n    \"\"\"\n    try:\n        fun_control[\"data_set\"].names\n    except AttributeError:\n        fun_control[\"data_set\"].names = None\n    feature_names = fun_control[\"data_set\"].names\n    total_attributions = None\n    config = get_tuned_architecture(spot_tuner, fun_control)\n    train_model(config, fun_control, timestamp=False)\n    model_loaded = load_light_from_checkpoint(config, fun_control, postfix=\"_TRAIN\")\n    if remove_spot_attributes:\n        removed_attributes, model = get_removed_attributes_and_base_net(net=model_loaded)\n    else:\n        model = model_loaded\n    model = model.to(device)\n    model.eval()\n    # get feature names\n    dataset = fun_control[\"data_set\"]\n    try:\n        n_features = dataset.data.shape[1]\n    except AttributeError:\n        n_features = dataset.tensors[0].shape[1]\n    if feature_names is None:\n        feature_names = [f\"x{i}\" for i in range(n_features)]\n    # get batch size\n    batch_size = config[\"batch_size\"]\n    # test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    data_module = LightDataModule(\n        dataset=dataset,\n        batch_size=batch_size,\n        test_size=fun_control[\"test_size\"],\n        scaler=fun_control[\"scaler\"],\n        verbosity=10,\n    )\n    data_module.setup(stage=\"test\")\n    test_loader = data_module.test_dataloader()\n\n    if attr_method == \"IntegratedGradients\":\n        attr = IntegratedGradients(model)\n    elif attr_method == \"DeepLift\":\n        attr = DeepLift(model)\n    elif attr_method == \"GradientShap\":  # Todo: would need a baseline\n        if baseline is None:\n            raise ValueError(\"baseline cannot be 'None' for GradientShap\")\n        attr = GradientShap(model)\n    elif attr_method == \"FeatureAblation\":\n        attr = FeatureAblation(model)\n    else:\n        raise ValueError(\n            \"\"\"\n            Unsupported attribution method.\n            Please choose from 'IntegratedGradients', 'DeepLift', 'GradientShap', or 'FeatureAblation'.\n            \"\"\"\n        )\n    for inputs, _ in test_loader:\n        if normalize:\n            inputs = (inputs - inputs.mean()) / inputs.std()\n        inputs.requires_grad_()\n        attributions = attr.attribute(inputs, return_convergence_delta=False, baselines=baseline)\n        if total_attributions is None:\n            total_attributions = attributions\n        else:\n            if len(attributions) == len(total_attributions):\n                total_attributions += attributions\n\n    # Calculation of average attribution across all batches\n    avg_attributions = total_attributions.mean(dim=0).detach().numpy()\n\n    # Transformation to the absolute attribution values if abs_attr is True\n    # Get indices of the n most important features\n    if abs_attr is True:\n        abs_avg_attributions = abs(avg_attributions)\n        top_n_indices = abs_avg_attributions.argsort()[-n_rel:][::-1]\n    else:\n        top_n_indices = avg_attributions.argsort()[-n_rel:][::-1]\n\n    # Get the importance values for the top n features\n    top_n_importances = avg_attributions[top_n_indices]\n\n    df = pd.DataFrame(\n        {\n            \"Feature Index\": top_n_indices,\n            \"Feature\": [feature_names[i] for i in top_n_indices],\n            attr_method + \"Attribution\": top_n_importances,\n        }\n    )\n    return df\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_gradients","title":"<code>get_gradients(net, fun_control, batch_size, device='cpu', normalize=False)</code>","text":"<p>Get the gradients of a neural network and the size of each layer.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - grads: A dictionary with the gradients of the neural network. - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.xai import get_gradients\n    import torch\n    import numpy as np\n    import torch.nn as nn\n    from spotpython.utils.init import fun_control_init\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (\n            get_default_hyperparameters_as_array, get_one_config_from_X)\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.data.lightdatamodule import LightDataModule\n    # from spotpython.plot.xai import get_gradients\n    fun_control = fun_control_init(\n        _L_in=10, # 10: diabetes\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\",\n        data_set=Diabetes(),\n        core_model=NNLinearRegressor,\n        hyperdict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    batch_size = 16\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n    gradients, layer_sizes = get_gradients(net=model)\n    gradients, layer_sizes\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_gradients(net, fun_control, batch_size, device=\"cpu\", normalize=False) -&gt; tuple:\n    \"\"\"\n    Get the gradients of a neural network and the size of each layer.\n\n    Args:\n        net (object):\n            A neural network.\n        fun_control (dict):\n            A dictionary with the function control.\n        batch_size (int, optional):\n            The batch size.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        normalize (bool, optional):\n            Whether to normalize the input data. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing:\n            - grads: A dictionary with the gradients of the neural network.\n            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.xai import get_gradients\n            import torch\n            import numpy as np\n            import torch.nn as nn\n            from spotpython.utils.init import fun_control_init\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (\n                    get_default_hyperparameters_as_array, get_one_config_from_X)\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.data.lightdatamodule import LightDataModule\n            # from spotpython.plot.xai import get_gradients\n            fun_control = fun_control_init(\n                _L_in=10, # 10: diabetes\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\",\n                data_set=Diabetes(),\n                core_model=NNLinearRegressor,\n                hyperdict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            config = get_one_config_from_X(X, fun_control)\n            _L_in = fun_control[\"_L_in\"]\n            _L_out = fun_control[\"_L_out\"]\n            _torchmetric = fun_control[\"_torchmetric\"]\n            batch_size = 16\n            model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n            gradients, layer_sizes = get_gradients(net=model)\n            gradients, layer_sizes\n    \"\"\"\n    net.eval()\n    dataset = fun_control[\"data_set\"]\n    data_module = LightDataModule(\n        dataset=dataset,\n        batch_size=batch_size,\n        test_size=fun_control[\"test_size\"],\n        scaler=fun_control[\"scaler\"],\n        verbosity=10,\n    )\n    data_module.setup(stage=\"fit\")\n    train_loader = data_module.train_dataloader()\n    inputs, targets = next(iter(train_loader))\n    if normalize:\n        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n    inputs, targets = inputs.to(device), targets.to(device)\n\n    # Pass one batch through the network, and calculate the gradients for the weights\n    net.zero_grad()\n    preds = net(inputs)\n    preds = preds.squeeze(-1)  # Remove the last dimension if it's 1\n    loss = F.mse_loss(preds, targets)\n    loss.backward()\n\n    grads = {}\n    layer_sizes = {}\n    for name, params in net.named_parameters():\n        if \"weight\" in name:\n            # Collect gradient information\n            grads[name] = params.grad.view(-1).cpu().clone().numpy()\n            # Collect size information\n            layer_sizes[name] = np.array(params.size())\n\n    net.zero_grad()\n    return grads, layer_sizes\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_layer_conductance","title":"<code>get_layer_conductance(spot_tuner, fun_control, layer_idx, device='cpu', normalize=True, remove_spot_attributes=False)</code>","text":"<p>Compute the average layer conductance attributions for a specified layer in the model.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>Spot</code> <p>The spot tuner object containing the trained model.</p> required <code>fun_control</code> <code>dict</code> <p>The fun_control dictionary containing the hyperparameters used to train the model.</p> required <code>layer_idx</code> <code>int</code> <p>Index of the layer for which to compute layer conductance attributions.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to True.</p> <code>True</code> <code>remove_spot_attributes</code> <code>bool</code> <p>Whether to remove the spot attributes. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: An array containing the average layer conductance attributions for the specified layer. The shape of the array corresponds to the shape of the attributions.</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_layer_conductance(spot_tuner, fun_control, layer_idx, device=\"cpu\", normalize=True, remove_spot_attributes=False) -&gt; np.ndarray:\n    \"\"\"\n    Compute the average layer conductance attributions for a specified layer in the model.\n\n    Args:\n        spot_tuner (spot.Spot):\n            The spot tuner object containing the trained model.\n        fun_control (dict):\n            The fun_control dictionary containing the hyperparameters used to train the model.\n        layer_idx (int):\n            Index of the layer for which to compute layer conductance attributions.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        normalize (bool, optional):\n            Whether to normalize the input data. Defaults to True.\n        remove_spot_attributes (bool, optional):\n            Whether to remove the spot attributes. Defaults to False.\n\n    Returns:\n        numpy.ndarray:\n            An array containing the average layer conductance attributions for the specified layer.\n            The shape of the array corresponds to the shape of the attributions.\n    \"\"\"\n    try:\n        fun_control[\"data_set\"].names\n    except AttributeError:\n        fun_control[\"data_set\"].names = None\n    feature_names = fun_control[\"data_set\"].names\n\n    config = get_tuned_architecture(spot_tuner, fun_control)\n    train_model(config, fun_control, timestamp=False)\n    model_loaded = load_light_from_checkpoint(config, fun_control, postfix=\"_TRAIN\")\n    if remove_spot_attributes:\n        removed_attributes, model = get_removed_attributes_and_base_net(net=model_loaded)\n    else:\n        model = model_loaded\n    model = model.to(device)\n    model.eval()\n\n    dataset = fun_control[\"data_set\"]\n    try:\n        n_features = dataset.data.shape[1]\n    except AttributeError:\n        n_features = dataset.tensors[0].shape[1]\n    if feature_names is None:\n        feature_names = [f\"x{i}\" for i in range(n_features)]\n    batch_size = config[\"batch_size\"]\n    test_loader = DataLoader(dataset, batch_size=batch_size)\n    total_layer_attributions = None\n    layers = model.layers\n    print(\"Conductance analysis for layer: \", layers[layer_idx])\n    lc = LayerConductance(model, layers[layer_idx])\n\n    for inputs, labels in test_loader:\n        if normalize:\n            inputs = (inputs - inputs.mean()) / inputs.std()\n        lc_attr_test = lc.attribute(inputs, n_steps=10, attribute_to_layer_input=True)\n        if total_layer_attributions is None:\n            total_layer_attributions = lc_attr_test\n        else:\n            if len(lc_attr_test) == len(total_layer_attributions):\n                total_layer_attributions += lc_attr_test\n\n    avg_layer_attributions = total_layer_attributions.mean(dim=0).detach().numpy()\n\n    return avg_layer_attributions\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_weights","title":"<code>get_weights(net, return_index=False)</code>","text":"<p>Get the weights of a neural network and the size of each layer.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>return_index</code> <code>bool</code> <p>Whether to return the index. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - weights: A dictionary with the weights of the neural network. - index: The layer index list (only if return_index is True). - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.xai import get_weights\n    import torch\n    import numpy as np\n    import torch.nn as nn\n    from spotpython.utils.init import fun_control_init\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (\n            get_default_hyperparameters_as_array, get_one_config_from_X)\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.data.lightdatamodule import LightDataModule\n    from spotpython.plot.xai import get_gradients\n    fun_control = fun_control_init(\n        _L_in=10, # 10: diabetes\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\",\n        data_set=Diabetes(),\n        core_model=NNLinearRegressor,\n        hyperdict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    batch_size = 16\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n    weights, layer_sizes = get_weights(net=model)\n    weights, layer_sizes\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_weights(net, return_index=False) -&gt; tuple:\n    \"\"\"\n    Get the weights of a neural network and the size of each layer.\n\n    Args:\n        net (object):\n            A neural network.\n        return_index (bool, optional):\n            Whether to return the index. Defaults to False.\n\n    Returns:\n        tuple:\n            A tuple containing:\n            - weights: A dictionary with the weights of the neural network.\n            - index: The layer index list (only if return_index is True).\n            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.xai import get_weights\n            import torch\n            import numpy as np\n            import torch.nn as nn\n            from spotpython.utils.init import fun_control_init\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (\n                    get_default_hyperparameters_as_array, get_one_config_from_X)\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.data.lightdatamodule import LightDataModule\n            from spotpython.plot.xai import get_gradients\n            fun_control = fun_control_init(\n                _L_in=10, # 10: diabetes\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\",\n                data_set=Diabetes(),\n                core_model=NNLinearRegressor,\n                hyperdict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            config = get_one_config_from_X(X, fun_control)\n            _L_in = fun_control[\"_L_in\"]\n            _L_out = fun_control[\"_L_out\"]\n            _torchmetric = fun_control[\"_torchmetric\"]\n            batch_size = 16\n            model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n            weights, layer_sizes = get_weights(net=model)\n            weights, layer_sizes\n    \"\"\"\n    weights = {}\n    index = []\n    layer_sizes = {}\n\n    for name, param in net.named_parameters():\n        if name.endswith(\".bias\"):\n            continue\n\n        # Extract layer number\n        layer_number = int(name.split(\".\")[1])\n        index.append(layer_number)\n\n        # Create dictionary key for this layer\n        key_name = f\"Layer {layer_number}\"\n\n        # Store weight information\n        weights[key_name] = param.detach().view(-1).cpu().numpy()\n\n        # Store layer size as a NumPy array\n        layer_sizes[key_name] = np.array(param.size())\n\n    if return_index:\n        return weights, index, layer_sizes\n    else:\n        return weights, layer_sizes\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.get_weights_conductance_last_layer","title":"<code>get_weights_conductance_last_layer(spot_tuner, fun_control, device='cpu', remove_spot_attributes=False)</code>","text":"<p>Get the weights and the conductance of the last layer.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>The spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>remove_spot_attributes</code> <code>bool</code> <p>Whether to remove the spot attributes. Defaults to False.</p> <code>False</code> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def get_weights_conductance_last_layer(spot_tuner, fun_control, device=\"cpu\", remove_spot_attributes=False) -&gt; tuple:\n    \"\"\"\n    Get the weights and the conductance of the last layer.\n\n    Args:\n        spot_tuner (object):\n            The spot tuner object.\n        fun_control (dict):\n            A dictionary with the function control.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        remove_spot_attributes (bool, optional):\n            Whether to remove the spot attributes. Defaults to False.\n    \"\"\"\n    config = get_tuned_architecture(spot_tuner, fun_control)\n    train_model(config, fun_control, timestamp=False)\n    model_loaded = load_light_from_checkpoint(config, fun_control, postfix=\"_TRAIN\")\n    if remove_spot_attributes:\n        removed_attributes, model = get_removed_attributes_and_base_net(net=model_loaded)\n    else:\n        model = model_loaded\n    model = model.to(device)\n    model.eval()\n\n    weights, index, _ = get_weights(model, return_index=True)\n    layer_idx = index[-1]\n    weights_last = weights[f\"Layer {layer_idx}\"]\n    weights_last\n    layer_conductance_last = get_layer_conductance(spot_tuner, fun_control, layer_idx=layer_idx)\n\n    return weights_last, layer_conductance_last\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.is_square","title":"<code>is_square(n)</code>","text":"<p>Check if a number is a square number.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the number is a square number, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_square(4)\nTrue\n&gt;&gt;&gt; is_square(5)\nFalse\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def is_square(n) -&gt; bool:\n    \"\"\"Check if a number is a square number.\n\n    Args:\n        n (int): The number to check.\n\n    Returns:\n        bool: True if the number is a square number, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; is_square(4)\n        True\n        &gt;&gt;&gt; is_square(5)\n        False\n    \"\"\"\n    return n == int(math.sqrt(n)) ** 2\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.plot_attributions","title":"<code>plot_attributions(df, attr_method='IntegratedGradients')</code>","text":"<p>Plot the attributions of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame with the attributions.</p> required <code>attr_method</code> <code>str</code> <p>The attribution method. Defaults to \u201cIntegratedGradients\u201d.</p> <code>'IntegratedGradients'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def plot_attributions(df, attr_method=\"IntegratedGradients\") -&gt; None:\n    \"\"\"\n    Plot the attributions of a neural network.\n\n    Args:\n        df (pd.DataFrame):\n            A DataFrame with the attributions.\n        attr_method (str, optional):\n            The attribution method. Defaults to \"IntegratedGradients\".\n\n    Returns:\n        None\n\n    \"\"\"\n    sns.set_theme(style=\"whitegrid\")\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=attr_method + \"Attribution\", y=\"Feature\", data=df, palette=\"viridis\", hue=\"Feature\")\n    plt.title(f\"Top {df.shape[0]} Features by {attr_method} Attribution\")\n    plt.xlabel(f\"{attr_method} Attribution Value\")\n    plt.ylabel(\"Feature\")\n    plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.plot_conductance_last_layer","title":"<code>plot_conductance_last_layer(weights_last, layer_conductance_last, figsize=(12, 6), show=True)</code>","text":"<p>Plot the conductance of the last layer.</p> <p>Parameters:</p> Name Type Description Default <code>weights_last</code> <code>ndarray</code> <p>The weights of the last layer.</p> required <code>layer_conductance_last</code> <code>ndarray</code> <p>The conductance of the last layer.</p> required <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 6).</p> <code>(12, 6)</code> <code>show</code> <code>bool</code> <p>Whether to show the plot. Defaults</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.plot.xai import plot_conductance_last_layer\n    weights_last = np.random.rand(10)\n    layer_conductance_last = np.random.rand(10)\n    plot_conductance_last_layer(weights_last, layer_conductance_last, show=True)\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def plot_conductance_last_layer(weights_last, layer_conductance_last, figsize=(12, 6), show=True) -&gt; None:\n    \"\"\"\n    Plot the conductance of the last layer.\n\n    Args:\n        weights_last (np.ndarray):\n            The weights of the last layer.\n        layer_conductance_last (np.ndarray):\n            The conductance of the last layer.\n        figsize (tuple, optional):\n            The figure size. Defaults to (12, 6).\n        show (bool, optional):\n            Whether to show the plot. Defaults\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.plot.xai import plot_conductance_last_layer\n            weights_last = np.random.rand(10)\n            layer_conductance_last = np.random.rand(10)\n            plot_conductance_last_layer(weights_last, layer_conductance_last, show=True)\n    \"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.bar(range(len(weights_last)), weights_last / weights_last.max(), label=\"Weights\", alpha=0.5)\n    ax.bar(\n        range(len(layer_conductance_last)),\n        layer_conductance_last / layer_conductance_last.max(),\n        label=\"Layer Conductance\",\n        alpha=0.5,\n    )\n    ax.set_xlabel(\"Weight Index\")\n    ax.set_ylabel(\"Normalized Value\")\n    ax.set_title(\"Layer Conductance vs. Weights\")\n    ax.legend()\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.plot_nn_values_hist","title":"<code>plot_nn_values_hist(nn_values, net, nn_values_names='', color='C0', columns=2)</code>","text":"<p>Plot the values of a neural network. Can be used to plot the weights, gradients, or activations of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>nn_values</code> <code>dict</code> <p>A dictionary with the values of the neural network. For example, the weights, gradients, or activations.</p> required <code>net</code> <code>object</code> <p>A neural network.</p> required <code>color</code> <code>str</code> <p>The color to use. Defaults to \u201cC0\u201d.</p> <code>'C0'</code> <code>columns</code> <code>int</code> <p>The number of columns. Defaults to 2.</p> <code>2</code> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def plot_nn_values_hist(nn_values, net, nn_values_names=\"\", color=\"C0\", columns=2) -&gt; None:\n    \"\"\"\n    Plot the values of a neural network.\n    Can be used to plot the weights, gradients, or activations of a neural network.\n\n    Args:\n        nn_values (dict):\n            A dictionary with the values of the neural network. For example,\n            the weights, gradients, or activations.\n        net (object):\n            A neural network.\n        color (str, optional):\n            The color to use. Defaults to \"C0\".\n        columns (int, optional):\n            The number of columns. Defaults to 2.\n\n    \"\"\"\n    n = len(nn_values)\n    print(f\"n:{n}\")\n    rows = n // columns + int(n % columns &gt; 0)\n    fig, ax = plt.subplots(rows, columns, figsize=(columns * 2.7, rows * 2.5))\n    fig_index = 0\n    for key in nn_values:\n        key_ax = ax[fig_index // columns][fig_index % columns]\n        sns.histplot(data=nn_values[key], bins=50, ax=key_ax, color=color, kde=True, stat=\"density\")\n        hidden_dim_str = r\"(%i $\\to$ %i)\" % (nn_values[key].shape[1], nn_values[key].shape[0]) if len(nn_values[key].shape) &gt; 1 else \"\"\n        key_ax.set_title(f\"{key} {hidden_dim_str}\")\n        # key_ax.set_title(f\"Layer {key} - {net.layers[key].__class__.__name__}\")\n        fig_index += 1\n    fig.suptitle(f\"{nn_values_names} distribution for activation function {net.hparams.act_fn}\", fontsize=14)\n    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n    plt.show()\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.plot_nn_values_scatter","title":"<code>plot_nn_values_scatter(nn_values, layer_sizes, nn_values_names='', absolute=True, cmap='gray', figsize=(6, 6), return_reshaped=False, show=True, colorbar_orientation='auto')</code>","text":"<p>Plot the values of a neural network including a marker for padding values.</p> <p>Parameters:</p> Name Type Description Default <code>nn_values</code> <code>dict</code> <p>A dictionary with the values of the neural network. For example, the weights, gradients, or activations.</p> required <code>layer_sizes</code> <code>dict</code> <p>A dictionary with layer names as keys and their sizes as entries in NumPy array format.</p> required <code>nn_values_names</code> <code>str</code> <p>The name of the values. Defaults to \u201c\u201d.</p> <code>''</code> <code>absolute</code> <code>bool</code> <p>Whether to use the absolute values. Defaults to True.</p> <code>True</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cgray\u201d.</p> <code>'gray'</code> <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (6, 6).</p> <code>(6, 6)</code> <code>return_reshaped</code> <code>bool</code> <p>Whether to return the reshaped values. Defaults to False.</p> <code>False</code> <code>show</code> <code>bool</code> <p>Whether to show the plot. Defaults to True.</p> <code>True</code> <code>colorbar_orientation</code> <code>str</code> <p>The orientation of the colorbar. Can be \u201cauto\u201d, \u201chorizontal\u201d, \u201cvertical\u201d, or \u201cnone\u201d. \u201cauto\u201d will choose the orientation based on the geometry of the plot. \u201cnone\u201d will not show the colorbar. Defaults to \u201cauto\u201d.</p> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the reshaped values.</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def plot_nn_values_scatter(\n    nn_values,\n    layer_sizes,\n    nn_values_names=\"\",\n    absolute=True,\n    cmap=\"gray\",\n    figsize=(6, 6),\n    return_reshaped=False,\n    show=True,\n    colorbar_orientation=\"auto\",\n) -&gt; dict:\n    \"\"\"\n    Plot the values of a neural network including a marker for padding values.\n\n    Args:\n        nn_values (dict):\n            A dictionary with the values of the neural network. For example,\n            the weights, gradients, or activations.\n        layer_sizes (dict):\n            A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n        nn_values_names (str, optional):\n            The name of the values. Defaults to \"\".\n        absolute (bool, optional):\n            Whether to use the absolute values. Defaults to True.\n        cmap (str, optional):\n            The colormap to use. Defaults to \"gray\".\n        figsize (tuple, optional):\n            The figure size. Defaults to (6, 6).\n        return_reshaped (bool, optional):\n            Whether to return the reshaped values. Defaults to False.\n        show (bool, optional):\n            Whether to show the plot. Defaults to True.\n        colorbar_orientation (str, optional):\n            The orientation of the colorbar. Can be \"auto\", \"horizontal\", \"vertical\", or \"none\".\n            \"auto\" will choose the orientation based on the geometry of the plot.\n            \"none\" will not show the colorbar.\n            Defaults to \"auto\".\n\n    Returns:\n        dict: A dictionary with the reshaped values.\n    \"\"\"\n    if cmap == \"gray\":\n        cmap = \"gray\"\n    elif cmap == \"BlueWhiteRed\":\n        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"blue\", \"white\", \"red\"])\n    elif cmap == \"GreenYellowRed\":\n        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"green\", \"yellow\", \"red\"])\n    else:\n        cmap = \"viridis\"\n\n    res = {}\n    padding_marker = np.nan  # Use NaN as a special marker for padding\n    for layer, values in nn_values.items():\n        if layer not in layer_sizes:\n            print(f\"Layer {layer} size not defined, skipping.\")\n            continue\n\n        layer_shape = layer_sizes[layer]\n        height, width = layer_shape if len(layer_shape) == 2 else (layer_shape[0], 1)  # Support linear layers\n\n        print(f\"{len(values)} values in Layer {layer}. Geometry: ({height}, {width})\")\n\n        total_size = height * width\n        if len(values) &lt; total_size:\n            padding_needed = total_size - len(values)\n            print(f\"{padding_needed} padding values added to Layer {layer}.\")\n            values = np.append(values, [padding_marker] * padding_needed)  # Append padding values\n\n        if absolute:\n            reshaped_values = np.abs(values).reshape((height, width))\n            # Mark padding values distinctly by setting them back to NaN\n            reshaped_values[reshaped_values == np.abs(padding_marker)] = np.nan\n        else:\n            reshaped_values = values.reshape((height, width))\n\n        _, ax = plt.subplots(figsize=figsize)\n        cax = ax.imshow(reshaped_values, cmap=cmap, interpolation=\"nearest\")\n\n        for i in range(height):\n            for j in range(width):\n                if np.isnan(reshaped_values[i, j]):\n                    ax.text(j, i, \"P\", ha=\"center\", va=\"center\", color=\"red\")\n\n        if colorbar_orientation == \"auto\":\n            if height &lt; width:\n                plt.colorbar(cax, orientation=\"horizontal\", label=\"Value\")\n            else:\n                plt.colorbar(cax, orientation=\"vertical\", label=\"Value\")\n\n        if colorbar_orientation in [\"horizontal\", \"vertical\"]:\n            plt.colorbar(cax, orientation=colorbar_orientation, label=\"Value\")\n        plt.title(f\"{nn_values_names} Plot for {layer}\")\n        if show:\n            plt.show()\n\n        # Add reshaped_values to the dictionary res\n        res[layer] = reshaped_values\n\n    if return_reshaped:\n        return res\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.sort_layers","title":"<code>sort_layers(data_dict)</code>","text":"<p>Sorts a dictionary with keys in the format \u201cLayer X\u201d based on the numerical value X.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict</code> <p>A dictionary with keys in the format \u201cLayer X\u201d.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the keys sorted based on the numerical value X.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data_dict = {\n...     \"Layer 1\": [1, 2, 3],\n...     \"Layer 3\": [4, 5, 6],\n...     \"Layer 2\": [7, 8, 9]\n... }\n&gt;&gt;&gt; sort_layers(data_dict)\n{'Layer 1': [1, 2, 3], 'Layer 2': [7, 8, 9], 'Layer 3': [4,\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def sort_layers(data_dict) -&gt; dict:\n    \"\"\"\n    Sorts a dictionary with keys in the format \"Layer X\" based on the numerical value X.\n\n    Args:\n        data_dict (dict): A dictionary with keys in the format \"Layer X\".\n\n    Returns:\n        dict: A dictionary with the keys sorted based on the numerical value X.\n\n    Examples:\n        &gt;&gt;&gt; data_dict = {\n        ...     \"Layer 1\": [1, 2, 3],\n        ...     \"Layer 3\": [4, 5, 6],\n        ...     \"Layer 2\": [7, 8, 9]\n        ... }\n        &gt;&gt;&gt; sort_layers(data_dict)\n        {'Layer 1': [1, 2, 3], 'Layer 2': [7, 8, 9], 'Layer 3': [4,\n\n    \"\"\"\n    # Use a lambda function to extract the number X from \"Layer X\" and sort based on that number\n    sorted_items = sorted(data_dict.items(), key=lambda item: int(item[0].split()[1]))\n    # Create a new dictionary from the sorted items\n    sorted_dict = dict(sorted_items)\n    return sorted_dict\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_activations_distributions","title":"<code>visualize_activations_distributions(activations, net, color='C0', columns=4, bins=50, show=True)</code>","text":"<p>Plots the distribution of activations for each layer     that were determined via the get_activations function.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>dict</code> <p>A dictionary containing activations for each layer.</p> required <code>net</code> <code>Module</code> <p>The neural network model.</p> required <code>color</code> <code>str</code> <p>The color for the plot histogram. Defaults to \u201cC0\u201d.</p> <code>'C0'</code> <code>columns</code> <code>int</code> <p>The number of columns for the subplots. Defaults to 4.</p> <code>4</code> <code>bins</code> <code>int</code> <p>The number of bins for the histogram. Defaults to 50.</p> <code>50</code> <code>show</code> <code>bool</code> <p>Whether to show the plot. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_activations_distributions(activations, net, color=\"C0\", columns=4, bins=50, show=True) -&gt; None:\n    \"\"\"Plots the distribution of activations for each layer\n        that were determined via the get_activations function.\n\n    Args:\n        activations (dict): A dictionary containing activations for each layer.\n        net (nn.Module): The neural network model.\n        color (str): The color for the plot histogram. Defaults to \"C0\".\n        columns (int): The number of columns for the subplots. Defaults to 4.\n        bins (int): The number of bins for the histogram. Defaults to 50.\n        show (bool): Whether to show the plot. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    rows = math.ceil(len(activations) / columns)\n    fig, ax = plt.subplots(rows, columns, figsize=(columns * 2.7, rows * 2.5))\n    fig_index = 0\n    for key in activations:\n        key_ax = ax[fig_index // columns][fig_index % columns]\n        sns.histplot(data=activations[key], bins=bins, ax=key_ax, color=color, kde=True, stat=\"density\")\n        key_ax.set_title(f\"Layer {key} - {net.layers[key].__class__.__name__}\")\n        fig_index += 1\n    fig.suptitle(\"Activation distribution\", fontsize=14)\n    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n    if show:\n        plt.show()\n    plt.close()\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_gradient_distributions","title":"<code>visualize_gradient_distributions(net, fun_control, batch_size, device='cpu', color='C0', xlabel=None, stat='count', use_kde=True, columns=2, normalize=True)</code>","text":"<p>Plot the gradients distributions of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>color</code> <code>str</code> <p>The color to use. Defaults to \u201cC0\u201d.</p> <code>'C0'</code> <code>xlabel</code> <code>str</code> <p>The x label. Defaults to None.</p> <code>None</code> <code>stat</code> <code>str</code> <p>The stat. Defaults to \u201ccount\u201d.</p> <code>'count'</code> <code>use_kde</code> <code>bool</code> <p>Whether to use kde. Defaults to True.</p> <code>True</code> <code>columns</code> <code>int</code> <p>The number of columns. Defaults to 2.</p> <code>2</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_gradient_distributions(\n    net,\n    fun_control,\n    batch_size,\n    device=\"cpu\",\n    color=\"C0\",\n    xlabel=None,\n    stat=\"count\",\n    use_kde=True,\n    columns=2,\n    normalize=True,\n) -&gt; None:\n    \"\"\"\n    Plot the gradients distributions of a neural network.\n\n    Args:\n        net (object):\n            A neural network.\n        fun_control (dict):\n            A dictionary with the function control.\n        batch_size (int, optional):\n            The batch size.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        color (str, optional):\n            The color to use. Defaults to \"C0\".\n        xlabel (str, optional):\n            The x label. Defaults to None.\n        stat (str, optional):\n            The stat. Defaults to \"count\".\n        use_kde (bool, optional):\n            Whether to use kde. Defaults to True.\n        columns (int, optional):\n            The number of columns. Defaults to 2.\n        normalize (bool, optional):\n            Whether to normalize the input data. Defaults to True.\n\n    Returns:\n        None\n\n    \"\"\"\n    grads, _ = get_gradients(net, fun_control, batch_size, device, normalize=normalize)\n    plot_nn_values_hist(grads, net, nn_values_names=\"Gradients\", color=color, columns=columns)\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_gradients","title":"<code>visualize_gradients(net, fun_control, batch_size, absolute=True, cmap='gray', figsize=(6, 6), device='cpu', normalize=True)</code>","text":"<p>Scatter plots the gradients of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary with the function control.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>absolute</code> <code>bool</code> <p>Whether to use the absolute values. Defaults to True.</p> <code>True</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cgray\u201d.</p> <code>'gray'</code> <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (6, 6).</p> <code>(6, 6)</code> <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the input data. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_gradients(net, fun_control, batch_size, absolute=True, cmap=\"gray\", figsize=(6, 6), device=\"cpu\", normalize=True) -&gt; None:\n    \"\"\"\n    Scatter plots the gradients of a neural network.\n\n    Args:\n        net (object):\n            A neural network.\n        fun_control (dict):\n            A dictionary with the function control.\n        batch_size (int, optional):\n            The batch size.\n        absolute (bool, optional):\n            Whether to use the absolute values. Defaults to True.\n        cmap (str, optional):\n            The colormap to use. Defaults to \"gray\".\n        figsize (tuple, optional):\n            The figure size. Defaults to (6, 6).\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        normalize (bool, optional):\n            Whether to normalize the input data. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    grads, layer_sizes = get_gradients(\n        net=net,\n        fun_control=fun_control,\n        batch_size=batch_size,\n        device=device,\n        normalize=normalize,\n    )\n    plot_nn_values_scatter(\n        nn_values=grads,\n        layer_sizes=layer_sizes,\n        nn_values_names=\"Gradients\",\n        absolute=absolute,\n        cmap=cmap,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_mean_activations","title":"<code>visualize_mean_activations(mean_activations, layer_sizes, absolute=True, cmap='gray', figsize=(6, 6))</code>","text":"<p>Scatter plots the mean activations of a neural network for each layer. means_activations is a dictionary with the mean activations of the neural network computed via the get_activations function.</p> <p>Parameters:</p> Name Type Description Default <code>mean_activations</code> <code>dict</code> <p>A dictionary with the mean activations of the neural network.</p> required <code>layer_sizes</code> <code>dict</code> <p>A dictionary with layer names as keys and their sizes as entries in NumPy array format.</p> required <code>absolute</code> <code>bool</code> <p>Whether to use the absolute values. Defaults to True.</p> <code>True</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cgray\u201d.</p> <code>'gray'</code> <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (6, 6).</p> <code>(6, 6)</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.xai import get_activations\n    activations, mean_activations, layer_sizes = get_activations(net, fun_control)\n    visualize_mean_activations(mean_activations, layer_sizes)\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_mean_activations(mean_activations, layer_sizes, absolute=True, cmap=\"gray\", figsize=(6, 6)) -&gt; None:\n    \"\"\"\n    Scatter plots the mean activations of a neural network for each layer.\n    means_activations is a dictionary with the mean activations of the neural network computed via\n    the get_activations function.\n\n    Args:\n        mean_activations (dict):\n            A dictionary with the mean activations of the neural network.\n        layer_sizes (dict):\n            A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n        absolute (bool, optional):\n            Whether to use the absolute values. Defaults to True.\n        cmap (str, optional):\n            The colormap to use. Defaults to \"gray\".\n        figsize (tuple, optional):\n            The figure size. Defaults to (6, 6).\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.xai import get_activations\n            activations, mean_activations, layer_sizes = get_activations(net, fun_control)\n            visualize_mean_activations(mean_activations, layer_sizes)\n\n    \"\"\"\n    plot_nn_values_scatter(\n        nn_values=mean_activations,\n        layer_sizes=layer_sizes,\n        nn_values_names=\"Average Activations\",\n        absolute=absolute,\n        cmap=cmap,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_weights","title":"<code>visualize_weights(net, absolute=True, cmap='gray', figsize=(6, 6))</code>","text":"<p>Scatter plots the weights of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>absolute</code> <code>bool</code> <p>Whether to use the absolute values. Defaults to True.</p> <code>True</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \u201cgray\u201d.</p> <code>'gray'</code> <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (6, 6).</p> <code>(6, 6)</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (\n            get_default_hyperparameters_as_array, get_one_config_from_X)\n    from spotpython.plot.xai import visualize_weights\n    fun_control = fun_control_init(\n        _L_in=10, # 10: diabetes\n        _L_out=1,\n        _torchmetric=\"mean_squared_error\",\n        data_set=Diabetes(),\n        core_model=NNLinearRegressor,\n        hyperdict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    _torchmetric = fun_control[\"_torchmetric\"]\n    batch_size = 16\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n    visualize_weights(net=model, absolute=True, cmap=\"gray\", figsize=(6, 6))\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_weights(net, absolute=True, cmap=\"gray\", figsize=(6, 6)) -&gt; None:\n    \"\"\"\n    Scatter plots the weights of a neural network.\n\n    Args:\n        net (object):\n            A neural network.\n        absolute (bool, optional):\n            Whether to use the absolute values. Defaults to True.\n        cmap (str, optional):\n            The colormap to use. Defaults to \"gray\".\n        figsize (tuple, optional):\n            The figure size. Defaults to (6, 6).\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (\n                    get_default_hyperparameters_as_array, get_one_config_from_X)\n            from spotpython.plot.xai import visualize_weights\n            fun_control = fun_control_init(\n                _L_in=10, # 10: diabetes\n                _L_out=1,\n                _torchmetric=\"mean_squared_error\",\n                data_set=Diabetes(),\n                core_model=NNLinearRegressor,\n                hyperdict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            config = get_one_config_from_X(X, fun_control)\n            _L_in = fun_control[\"_L_in\"]\n            _L_out = fun_control[\"_L_out\"]\n            _torchmetric = fun_control[\"_torchmetric\"]\n            batch_size = 16\n            model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n            visualize_weights(net=model, absolute=True, cmap=\"gray\", figsize=(6, 6))\n    \"\"\"\n    weights, layer_sizes = get_weights(net)\n    plot_nn_values_scatter(\n        nn_values=weights,\n        layer_sizes=layer_sizes,\n        nn_values_names=\"Weights\",\n        absolute=absolute,\n        cmap=cmap,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.visualize_weights_distributions","title":"<code>visualize_weights_distributions(net, color='C0', columns=2)</code>","text":"<p>Plot the weights distributions of a neural network.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>color</code> <code>str</code> <p>The color to use. Defaults to \u201cC0\u201d.</p> <code>'C0'</code> <code>columns</code> <code>int</code> <p>The number of columns. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def visualize_weights_distributions(net, color=\"C0\", columns=2) -&gt; None:\n    \"\"\"\n    Plot the weights distributions of a neural network.\n\n    Args:\n        net (object):\n            A neural network.\n        color (str, optional):\n            The color to use. Defaults to \"C0\".\n        columns (int, optional):\n            The number of columns. Defaults to 2.\n\n    Returns:\n        None\n\n    \"\"\"\n    weights, _ = get_weights(net)\n    plot_nn_values_hist(weights, net, nn_values_names=\"Weights\", color=color, columns=columns)\n</code></pre>"},{"location":"reference/spotpython/plot/xai/#spotpython.plot.xai.viz_net","title":"<code>viz_net(net, device='cpu', show_attrs=False, show_saved=False, max_attr_chars=50, filename='model_architecture', format='png')</code>","text":"<p>Visualize the architecture of a linear neural network. Produces Graphviz representation of PyTorch autograd graph. If a node represents a backward function, it is gray. Otherwise, the node represents a tensor and is either blue, orange, or green: - Blue: reachable leaf tensors that requires grad (tensors whose .grad fields will be populated during .backward()) - Orange: saved tensors of custom autograd functions as well as those saved by built-in backward nodes - Green: tensor passed in as outputs - Dark green: if any output is a view, we represent its base tensor with a dark green node. If <code>show_attrs</code>=True and <code>show_saved</code>=True it is shown what autograd saves for the backward pass.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>Module</code> <p>The neural network model.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>show_attrs</code> <code>bool</code> <p>whether to display non-tensor attributes of backward nodes (Requires PyTorch version &gt;= 1.9)</p> <code>False</code> <code>show_saved</code> <code>bool</code> <p>whether to display saved tensor nodes that are not by custom autograd functions. Saved tensor nodes for custom functions, if present, are always displayed. (Requires PyTorch version &gt;= 1.9)</p> <code>False</code> <code>max_attr_chars</code> <code>int</code> <p>if show_attrs is True, sets max number of characters to display for any given attribute. Defaults to 50.</p> <code>50</code> <code>filename</code> <code>str</code> <p>The filename. Defaults to \u201cmodel_architecture\u201d.</p> <code>'model_architecture'</code> <code>format</code> <code>str</code> <p>The output format. Defaults to \u201cpng\u201d.</p> <code>'png'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model does not have a linear layer.</p> <code>TypeError</code> <p>If the network structure or parameters are invalid.</p> <code>RuntimeError</code> <p>If an unexpected error occurs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.plot.xai import viz_net\n    from spotpython.utils.init import fun_control_init\n    from spotpython.data.diabetes import Diabetes\n    from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import (\n            get_default_hyperparameters_as_array, get_one_config_from_X)\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    _L_in=10\n    _L_out=1\n    _torchmetric=\"mean_squared_error\"\n    fun_control = fun_control_init(\n        _L_in=_L_in,\n        _L_out=_L_out,\n        _torchmetric=_torchmetric,\n        data_set=Diabetes(),\n        core_model=NNLinearRegressor,\n        hyperdict=LightHyperDict)\n    X = get_default_hyperparameters_as_array(fun_control)\n    config = get_one_config_from_X(X, fun_control)\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n    viz_net(net=model, device=\"cpu\", show_attrs=True, show_saved=True, filename=\"model_architecture3\", format=\"png\")\n</code></pre> Source code in <code>spotpython/plot/xai.py</code> <pre><code>def viz_net(\n    net,\n    device=\"cpu\",\n    show_attrs=False,\n    show_saved=False,\n    max_attr_chars=50,\n    filename=\"model_architecture\",\n    format=\"png\",\n) -&gt; None:\n    \"\"\"\n    Visualize the architecture of a linear neural network.\n    Produces Graphviz representation of PyTorch autograd graph.\n    If a node represents a backward function, it is gray. Otherwise, the node represents a tensor and is either blue, orange, or green:\n    - Blue: reachable leaf tensors that requires grad (tensors whose .grad fields will be populated during .backward())\n    - Orange: saved tensors of custom autograd functions as well as those saved by built-in backward nodes\n    - Green: tensor passed in as outputs\n    - Dark green: if any output is a view, we represent its base tensor with a dark green node.\n    If `show_attrs`=True and `show_saved`=True it is shown what autograd saves for the backward pass.\n\n    Args:\n        net (nn.Module):\n            The neural network model.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        show_attrs (bool, optional):\n            whether to display non-tensor attributes of backward nodes (Requires PyTorch version &gt;= 1.9)\n        show_saved (bool, optional):\n            whether to display saved tensor nodes that are not by custom autograd functions. Saved tensor nodes for custom functions, if present, are always displayed.\n            (Requires PyTorch version &gt;= 1.9)\n        max_attr_chars (int, optional):\n            if show_attrs is True, sets max number of characters to display for any given attribute. Defaults to 50.\n        filename (str, optional):\n            The filename. Defaults to \"model_architecture\".\n        format (str, optional):\n            The output format. Defaults to \"png\".\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the model does not have a linear layer.\n        TypeError: If the network structure or parameters are invalid.\n        RuntimeError: If an unexpected error occurs.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.plot.xai import viz_net\n            from spotpython.utils.init import fun_control_init\n            from spotpython.data.diabetes import Diabetes\n            from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import (\n                    get_default_hyperparameters_as_array, get_one_config_from_X)\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            _L_in=10\n            _L_out=1\n            _torchmetric=\"mean_squared_error\"\n            fun_control = fun_control_init(\n                _L_in=_L_in,\n                _L_out=_L_out,\n                _torchmetric=_torchmetric,\n                data_set=Diabetes(),\n                core_model=NNLinearRegressor,\n                hyperdict=LightHyperDict)\n            X = get_default_hyperparameters_as_array(fun_control)\n            config = get_one_config_from_X(X, fun_control)\n            model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n            viz_net(net=model, device=\"cpu\", show_attrs=True, show_saved=True, filename=\"model_architecture3\", format=\"png\")\n\n    \"\"\"\n    try:\n        dim = extract_linear_dims(net)\n    except ValueError as ve:\n        error_message = \"The model does not have a linear layer: \" + str(ve)\n        raise ValueError(error_message)\n    except TypeError as te:\n        error_message = \"Invalid network structure or parameters: \" + str(te)\n        raise TypeError(error_message)\n    except Exception as e:\n        # Catch any other unforeseen exceptions and log them for debugging purposes\n        error_message = \"An unexpected error occurred: \" + str(e)\n        raise RuntimeError(error_message)\n\n    # Proceed with the rest of the logic if dimensions were extracted successfully\n    x = torch.randn(1, dim[0]).requires_grad_(True)\n    x = x.to(device)\n    output = net(x)\n    dot = make_dot(\n        output,\n        params=dict(net.named_parameters()),\n        show_attrs=show_attrs,\n        show_saved=show_saved,\n        max_attr_chars=max_attr_chars,\n    )\n    dot.render(filename, format=format)\n</code></pre>"},{"location":"reference/spotpython/plot/xy/","title":"xy","text":""},{"location":"reference/spotpython/plot/xy/#spotpython.plot.xy.plot_y_vs_X","title":"<code>plot_y_vs_X(X, y, nrows=5, ncols=2, figsize=(30, 20), ylabel='y', feature_names=None)</code>","text":"<p>Plots y versus each feature in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D array of input features.</p> required <code>y</code> <code>ndarray</code> <p>1D array of target values.</p> required <code>nrows</code> <code>int</code> <p>Number of rows in the subplot grid. Defaults to 5.</p> <code>5</code> <code>ncols</code> <code>int</code> <p>Number of columns in the subplot grid. Defaults to 2.</p> <code>2</code> <code>figsize</code> <code>tuple</code> <p>Size of the entire figure. Defaults to (30, 20).</p> <code>(30, 20)</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis. Defaults to \u2018y\u2019.</p> <code>'y'</code> <code>feature_names</code> <code>list of str</code> <p>List of feature names. Defaults to None. If None, generates feature names as x0, x1, etc.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from spotpython.plot.xy import plot_y_vs_X\n&gt;&gt;&gt; data = load_diabetes()\n&gt;&gt;&gt; X, y = data.data, data.target\n&gt;&gt;&gt; plot_y_vs_X(X, y, nrows=5, ncols=2, figsize=(20, 15))\n</code></pre> Source code in <code>spotpython/plot/xy.py</code> <pre><code>def plot_y_vs_X(X, y, nrows=5, ncols=2, figsize=(30, 20), ylabel=\"y\", feature_names=None):\n    \"\"\"\n    Plots y versus each feature in X.\n\n    Args:\n        X (ndarray):\n            2D array of input features.\n        y (ndarray):\n            1D array of target values.\n        nrows (int, optional):\n            Number of rows in the subplot grid. Defaults to 5.\n        ncols (int, optional):\n            Number of columns in the subplot grid. Defaults to 2.\n        figsize (tuple, optional):\n            Size of the entire figure. Defaults to (30, 20).\n        ylabel (str, optional):\n            Label for the y-axis. Defaults to 'y'.\n        feature_names (list of str, optional):\n            List of feature names. Defaults to None. If None, generates feature names as x0, x1, etc.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from spotpython.plot.xy import plot_y_vs_X\n        &gt;&gt;&gt; data = load_diabetes()\n        &gt;&gt;&gt; X, y = data.data, data.target\n        &gt;&gt;&gt; plot_y_vs_X(X, y, nrows=5, ncols=2, figsize=(20, 15))\n    \"\"\"\n    if feature_names is None:\n        feature_names = [f\"x{i}\" for i in range(X.shape[1])]\n\n    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n\n    for i, (ax, col) in enumerate(zip(axs.flat, feature_names)):\n        x = X[:, i]\n        pf = np.polyfit(x, y, 1)\n        p = np.poly1d(pf)\n\n        ax.plot(x, y, \"o\")\n        ax.plot(x, p(x), \"r--\")\n\n        ax.set_title(col + \" \" + ylabel)\n        ax.set_xlabel(col)\n        ax.set_ylabel(ylabel)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/spotpython/sklearn/traintest/","title":"traintest","text":""},{"location":"reference/spotpython/sklearn/traintest/#spotpython.sklearn.traintest.evaluate_hold_out","title":"<code>evaluate_hold_out(model, fun_control)</code>","text":"<p>Evaluate a model using hold-out validation. A validation set is created from the training set. The test set is not used in this evaluation.</p> <p>Note: In contrast to <code>evaluate_model()</code>, this function creates a validation set as a subset of the training set. It can be selected by setting <code>fun_control[\"eval\"] = \"evaluate_hold_out\"</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>sklearn model</code> <p>sklearn model.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing evaluation results.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if call to train_test_split() or fit() or predict() fails.</p> Source code in <code>spotpython/sklearn/traintest.py</code> <pre><code>def evaluate_hold_out(model, fun_control) -&gt; np.ndarray:\n    \"\"\"Evaluate a model using hold-out validation.\n    A validation set is created from the training set.\n    The test set is not used in this evaluation.\n\n    Note:\n    In contrast to `evaluate_model()`, this function creates a validation set as\n    a subset of the training set.\n    It can be selected by setting `fun_control[\"eval\"] = \"evaluate_hold_out\"`.\n\n    Args:\n        model (sklearn model):\n            sklearn model.\n        fun_control (dict):\n            dictionary containing control parameters for the function.\n\n    Returns:\n        (np.ndarray): array containing evaluation results.\n\n    Raises:\n        Exception: if call to train_test_split() or fit() or predict() fails.\n    \"\"\"\n    train_df = fun_control[\"train\"]\n    target_column = fun_control[\"target_column\"]\n    try:\n        X_train, X_val, y_train, y_val = train_test_split(\n            train_df.drop(target_column, axis=1),\n            train_df[target_column],\n            random_state=42,\n            test_size=fun_control[\"test_size\"],\n            # stratify=train_df[target_column],\n        )\n    except Exception as err:\n        print(f\"Error in evaluate_hold_out(). Call to train_test_split() failed. {err=}, {type(err)=}\")\n    try:\n        if fun_control[\"scaler\"] is not None:\n            scaler = fun_control[\"scaler\"]()\n            X_train = scaler.fit_transform(X_train)\n            X_train = pd.DataFrame(X_train, columns=train_df.drop(target_column, axis=1).columns)  # Maintain column names\n        model.fit(X_train, y_train)\n    except Exception as err:\n        print(f\"Error in evaluate_hold_out(). Call to fit() failed. {err=}, {type(err)=}\")\n    try:\n        if fun_control[\"scaler\"] is not None:\n            X_val = scaler.transform(X_val)\n            X_val = pd.DataFrame(X_val, columns=train_df.drop(target_column, axis=1).columns)  # Maintain column names\n        y_val = np.array(y_val)\n        if fun_control[\"predict_proba\"] or fun_control[\"task\"] == \"classification\":\n            df_preds = model.predict_proba(X_val)\n        else:\n            df_preds = model.predict(X_val)\n        df_eval = fun_control[\"metric_sklearn\"](y_val, df_preds, **fun_control[\"metric_params\"])\n    except Exception as err:\n        print(f\"Error in evaluate_hold_out(). Call to predict() failed. {err=}, {type(err)=}\")\n        df_eval = np.nan\n    return df_eval, df_preds\n</code></pre>"},{"location":"reference/spotpython/sklearn/traintest/#spotpython.sklearn.traintest.evaluate_model","title":"<code>evaluate_model(model, fun_control)</code>","text":"<p>Evaluate a model using the test set. First, the model is trained on the training set. If a scaler is provided, the data is transformed using the scaler and <code>fit_transform(X_train)</code>. Then, the model is evaluated using the test set from <code>fun_control</code>, the scaler with <code>transform(X_test)</code>, the model.predict() method and the <code>metric_params</code> specified in <code>fun_control</code>.</p> <p>Note: In contrast to <code>evaluate_hold_out()</code>, this function uses the test set. It can be selected by setting <code>fun_control[\"eval\"] = \"eval_test\"</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>sklearn model</code> <p>sklearn model.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing evaluation results.</p> Source code in <code>spotpython/sklearn/traintest.py</code> <pre><code>def evaluate_model(model, fun_control) -&gt; np.ndarray:\n    \"\"\"Evaluate a model using the test set.\n    First, the model is trained on the training set. If a scaler\n    is provided, the data is transformed using the scaler and `fit_transform(X_train)`.\n    Then, the model is evaluated using the test set from `fun_control`,\n    the scaler with `transform(X_test)`,\n    the model.predict() method and the\n    `metric_params` specified in `fun_control`.\n\n    Note:\n    In contrast to `evaluate_hold_out()`, this function uses the test set.\n    It can be selected by setting `fun_control[\"eval\"] = \"eval_test\"`.\n\n    Args:\n        model (sklearn model):\n            sklearn model.\n        fun_control (dict):\n            dictionary containing control parameters for the function.\n\n    Returns:\n        (np.ndarray): array containing evaluation results.\n    \"\"\"\n    try:\n        X_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n        X_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n        if fun_control[\"scaler\"] is not None:\n            X_train = fun_control[\"scaler\"]().fit_transform(X_train)\n            X_test = fun_control[\"scaler\"]().transform(X_test)\n        model.fit(X_train, y_train)\n        if fun_control[\"predict_proba\"]:\n            df_preds = model.predict_proba(X_test)\n        else:\n            df_preds = model.predict(X_test)\n        df_eval = fun_control[\"metric_sklearn\"](y_test, df_preds, **fun_control[\"metric_params\"])\n    except Exception as err:\n        print(f\"Error in fun_sklearn(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n        df_eval = np.nan\n        df_eval = np.nan\n    return df_eval, df_preds\n</code></pre>"},{"location":"reference/spotpython/sklearn/traintest/#spotpython.sklearn.traintest.evaluate_model_oob","title":"<code>evaluate_model_oob(model, fun_control)</code>","text":"<p>Out-of-bag evaluation (Only for RandomForestClassifier). If fun_control[\u201ceval\u201d] == \u201ceval_oob_score\u201d.</p> Source code in <code>spotpython/sklearn/traintest.py</code> <pre><code>def evaluate_model_oob(model, fun_control):\n    \"\"\"Out-of-bag evaluation (Only for RandomForestClassifier).\n    If fun_control[\"eval\"] == \"eval_oob_score\".\n    \"\"\"\n    try:\n        X, y = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n        model.fit(X, y)\n        df_preds = model.oob_decision_function_\n        df_eval = fun_control[\"metric_sklearn\"](y, df_preds, **fun_control[\"metric_params\"])\n    except Exception as err:\n        print(f\"Error in fun_sklearn(). Call to evaluate_model_oob failed. {err=}, {type(err)=}\")\n        df_eval = np.nan\n        df_eval = np.nan\n    return df_eval, df_preds\n</code></pre>"},{"location":"reference/spotpython/spot/spot/","title":"spot","text":""},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot","title":"<code>Spot</code>","text":"<p>Spot base class to handle the following tasks in a uniform manner:</p> <ul> <li>Getting and setting parameters. This is done via the <code>Spot</code> initialization.</li> <li>Running surrogate based hyperparameter optimization. After the class is initialized, hyperparameter tuning runs can be performed via the <code>run</code> method.</li> <li>Displaying information. The <code>plot</code> method can be used for visualizing results. The <code>print</code> methods summarizes information about the tuning run.</li> </ul> <p>The <code>Spot</code> class is built in a modular manner. It combines the following components:</p> <pre><code>1. Fun (objective function)\n2. Design (experimental design)\n3. Optimizer to be used on the surrogate model\n4. Surrogate (model)\n</code></pre> <p>For each of the components different implementations can be selected and combined. Internal components are selected as default. These can be replaced by components from other packages, e.g., scikit-learn or scikit-optimize.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>objective function</p> <code>None</code> <code>fun_control</code> <code>Dict[str, Union[int, float]]</code> <p>objective function information stored as a dictionary. Default value is <code>fun_control_init()</code>.</p> <code>fun_control_init()</code> <code>design</code> <code>object</code> <p>experimental design. If <code>None</code>, spotpython\u2019s <code>SpaceFilling</code> is used. Default value is <code>None</code>.</p> <code>None</code> <code>design_control</code> <code>Dict[str, Union[int, float]]</code> <p>experimental design information stored as a dictionary. Default value is <code>design_control_init()</code>.</p> <code>design_control_init()</code> <code>optimizer</code> <code>object</code> <p>optimizer on the surrogate. If <code>None</code>, <code>scipy.optimize</code>\u2019s <code>differential_evolution</code> is used. Default value is <code>None</code>.</p> <code>None</code> <code>optimizer_control</code> <code>Dict[str, Union[int, float]]</code> <p>information about the optimizer stored as a dictionary. Default value is <code>optimizer_control_init()</code>.</p> <code>optimizer_control_init()</code> <code>surrogate</code> <code>object</code> <p>surrogate model. If <code>None</code>, spotpython\u2019s <code>kriging</code> is used. Default value is <code>None</code>.</p> <code>None</code> <code>surrogate_control</code> <code>Dict[str, Union[int, float]]</code> <p>surrogate model information stored as a dictionary. Default value is <code>surrogate_control_init()</code>.</p> <code>surrogate_control_init()</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Note <p>Description in the source code refers to [bart21i]: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches. In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67\u2013114.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from math import inf\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init,\n        design_control_init,\n        surrogate_control_init,\n        optimizer_control_init)\n    def objective_function(X, fun_control=None):\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n        if X.shape[1] != 2:\n            raise Exception\n        x0 = X[:, 0]\n        x1 = X[:, 1]\n        y = x0**2 + 10*x1**2\n        return y\n    fun_control = fun_control_init(\n                lower = np.array([0, 0]),\n                upper = np.array([10, 10]),\n                fun_evals=8,\n                fun_repeats=1,\n                max_time=inf,\n                noise=False,\n                tolerance_x=0,\n                ocba_delta=0,\n                var_type=[\"num\", \"num\"],\n                infill_criterion=\"ei\",\n                n_points=1,\n                seed=123,\n                log_level=20,\n                show_models=False,\n                show_progress=True)\n    design_control = design_control_init(\n                init_size=5,\n                repeats=1)\n    surrogate_control = surrogate_control_init(\n                model_optimizer=differential_evolution,\n                model_fun_evals=10000,\n                min_theta=-3,\n                max_theta=3,\n                n_theta=2,\n                theta_init_zero=False,\n                n_p=1,\n                optim_p=False,\n                var_type=[\"num\", \"num\"],\n                metric_factorial=\"canberra\",\n                seed=124)\n    optimizer_control = optimizer_control_init(\n                max_iter=1000,\n                seed=125)\n    spot = spot.Spot(fun=objective_function,\n                fun_control=fun_control,\n                design_control=design_control,\n                surrogate_control=surrogate_control,\n                optimizer_control=optimizer_control)\n    spot.run()\n    spot.plot_progress()\n    spot.plot_contour(i=0, j=1)\n    spot.plot_importance()\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>class Spot:\n    \"\"\"\n    Spot base class to handle the following tasks in a uniform manner:\n\n    * Getting and setting parameters. This is done via the `Spot` initialization.\n    * Running surrogate based hyperparameter optimization. After the class is initialized, hyperparameter tuning\n    runs can be performed via the `run` method.\n    * Displaying information. The `plot` method can be used for visualizing results. The `print` methods summarizes\n    information about the tuning run.\n\n    The `Spot` class is built in a modular manner. It combines the following components:\n\n        1. Fun (objective function)\n        2. Design (experimental design)\n        3. Optimizer to be used on the surrogate model\n        4. Surrogate (model)\n\n    For each of the components different implementations can be selected and combined.\n    Internal components are selected as default.\n    These can be replaced by components from other packages, e.g., scikit-learn or scikit-optimize.\n\n    Args:\n        fun (Callable):\n            objective function\n        fun_control (Dict[str, Union[int, float]]):\n            objective function information stored as a dictionary.\n            Default value is `fun_control_init()`.\n        design (object):\n            experimental design. If `None`, spotpython's `SpaceFilling` is used.\n            Default value is `None`.\n        design_control (Dict[str, Union[int, float]]):\n            experimental design information stored as a dictionary.\n            Default value is `design_control_init()`.\n        optimizer (object):\n            optimizer on the surrogate. If `None`, `scipy.optimize`'s `differential_evolution` is used.\n            Default value is `None`.\n        optimizer_control (Dict[str, Union[int, float]]):\n            information about the optimizer stored as a dictionary.\n            Default value is `optimizer_control_init()`.\n        surrogate (object):\n            surrogate model. If `None`, spotpython's `kriging` is used. Default value is `None`.\n        surrogate_control (Dict[str, Union[int, float]]):\n            surrogate model information stored as a dictionary.\n            Default value is `surrogate_control_init()`.\n\n    Returns:\n        (NoneType): None\n\n    Note:\n        Description in the source code refers to [bart21i]:\n        Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches.\n        In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide,\n        E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67\u2013114.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from math import inf\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init,\n                design_control_init,\n                surrogate_control_init,\n                optimizer_control_init)\n            def objective_function(X, fun_control=None):\n                if not isinstance(X, np.ndarray):\n                    X = np.array(X)\n                if X.shape[1] != 2:\n                    raise Exception\n                x0 = X[:, 0]\n                x1 = X[:, 1]\n                y = x0**2 + 10*x1**2\n                return y\n            fun_control = fun_control_init(\n                        lower = np.array([0, 0]),\n                        upper = np.array([10, 10]),\n                        fun_evals=8,\n                        fun_repeats=1,\n                        max_time=inf,\n                        noise=False,\n                        tolerance_x=0,\n                        ocba_delta=0,\n                        var_type=[\"num\", \"num\"],\n                        infill_criterion=\"ei\",\n                        n_points=1,\n                        seed=123,\n                        log_level=20,\n                        show_models=False,\n                        show_progress=True)\n            design_control = design_control_init(\n                        init_size=5,\n                        repeats=1)\n            surrogate_control = surrogate_control_init(\n                        model_optimizer=differential_evolution,\n                        model_fun_evals=10000,\n                        min_theta=-3,\n                        max_theta=3,\n                        n_theta=2,\n                        theta_init_zero=False,\n                        n_p=1,\n                        optim_p=False,\n                        var_type=[\"num\", \"num\"],\n                        metric_factorial=\"canberra\",\n                        seed=124)\n            optimizer_control = optimizer_control_init(\n                        max_iter=1000,\n                        seed=125)\n            spot = spot.Spot(fun=objective_function,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control,\n                        optimizer_control=optimizer_control)\n            spot.run()\n            spot.plot_progress()\n            spot.plot_contour(i=0, j=1)\n            spot.plot_importance()\n    \"\"\"\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __init__(\n        self,\n        design: object = None,\n        design_control: dict = design_control_init(),\n        fun: Callable = None,\n        fun_control: dict = fun_control_init(),\n        optimizer: object = None,\n        optimizer_control: dict = optimizer_control_init(),\n        surrogate: object = None,\n        surrogate_control: dict = surrogate_control_init(),\n    ):\n        self.fun_control = fun_control\n        self.design_control = design_control\n        self.optimizer_control = optimizer_control\n        self.surrogate_control = surrogate_control\n\n        # small value:\n        self.eps = sqrt(spacing(1))\n\n        self._set_fun(fun)\n\n        self._set_bounds_and_dim()\n\n        # Random number generator:\n        self.rng = default_rng(self.fun_control[\"seed\"])\n        set_all_seeds(self.fun_control[\"seed\"])\n\n        self._set_var_type()\n\n        self._set_var_name()\n\n        # Reduce dim based on lower == upper logic:\n        # modifies lower, upper, var_type, and var_name\n        self.to_red_dim()\n\n        # Additional self attributes updates:\n        self._set_additional_attributes()\n\n        # Bounds are internal, because they are functions of self.lower and self.upper\n        # and used by the optimizer:\n        de_bounds = []\n        for j in range(self.lower.size):\n            de_bounds.append([self.lower[j], self.upper[j]])\n        self.de_bounds = de_bounds\n\n        self._design_setup(design)\n\n        self._optimizer_setup(optimizer)\n\n        self._surrogate_control_setup()\n\n        # The writer (Tensorboard) must be initialized before the surrogate model,\n        # because the writer is passed to the surrogate model:\n        self._init_spot_writer()\n\n        self._surrogate_setup(surrogate)\n\n        if self.fun_control.get(\"save_experiment\"):\n            self.save_experiment(verbosity=self.verbosity)\n\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n        logger.debug(\"In Spot() init(): fun_control: %s\", self.fun_control)\n        logger.debug(\"In Spot() init(): design_control: %s\", self.design_control)\n        logger.debug(\"In Spot() init(): optimizer_control: %s\", self.optimizer_control)\n        logger.debug(\"In Spot() init(): surrogate_control: %s\", self.surrogate_control)\n        logger.debug(\"In Spot() init(): self.get_spot_attributes_as_df(): %s\", self.get_spot_attributes_as_df())\n\n    def _set_fun(self, fun):\n        \"\"\"Set the objective function.\n\n        Args:\n            fun (Callable): objective function\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            Exception: No objective function specified.\n            Exception: Objective function is not callable\n\n        \"\"\"\n        self.fun = fun\n        if self.fun is None:\n            raise Exception(\"No objective function specified.\")\n        if not callable(self.fun):\n            raise Exception(\"Objective function is not callable.\")\n\n    def _set_bounds_and_dim(self) -&gt; None:\n        \"\"\"\n        Set the lower and upper bounds and the number of dimensions.\n\n        Returns:\n            (NoneType): None\n\n        \"\"\"\n        # lower attribute updates:\n        # if lower is in the fun_control dictionary, use the value of the key \"lower\" as the lower bound\n        if get_control_key_value(control_dict=self.fun_control, key=\"lower\") is not None:\n            self.lower = get_control_key_value(control_dict=self.fun_control, key=\"lower\")\n        # Number of dimensions is based on lower\n        self.k = self.lower.size\n\n        # upper attribute updates:\n        # if upper is in fun_control dictionary, use the value of the key \"upper\" as the upper bound\n        if get_control_key_value(control_dict=self.fun_control, key=\"upper\") is not None:\n            self.upper = get_control_key_value(control_dict=self.fun_control, key=\"upper\")\n\n    def _set_var_type(self) -&gt; None:\n        \"\"\"\n        Set the variable types. If the variable types are not specified,\n        all variable types are forced to 'num'.\n        \"\"\"\n        self.var_type = self.fun_control[\"var_type\"]\n        # Force numeric type as default in every dim:\n        # assume all variable types are \"num\" if \"num\" is\n        # specified less than k times\n        if len(self.var_type) &lt; self.k:\n            self.var_type = self.var_type * self.k\n            logger.warning(\"All variable types forced to 'num'.\")\n\n    def _set_var_name(self) -&gt; None:\n        \"\"\"\n        Set the variable names. If the variable names are not specified,\n        all variable names are set to x0, x1, x2, ...\n        \"\"\"\n        self.var_name = self.fun_control[\"var_name\"]\n        # use x0, x1, ... as default variable names:\n        if self.var_name is None:\n            self.var_name = [\"x\" + str(i) for i in range(len(self.lower))]\n\n    def _set_additional_attributes(self) -&gt; None:\n        \"\"\"\n        Set additional attributes based on the fun_control dictionary\n        \"\"\"\n        self.fun_evals = self.fun_control[\"fun_evals\"]\n        self.fun_repeats = self.fun_control[\"fun_repeats\"]\n        self.max_time = self.fun_control[\"max_time\"]\n        self.noise = self.fun_control[\"noise\"]\n        self.tolerance_x = self.fun_control[\"tolerance_x\"]\n        self.ocba_delta = self.fun_control[\"ocba_delta\"]\n        self.log_level = self.fun_control[\"log_level\"]\n        self.show_models = self.fun_control[\"show_models\"]\n        self.show_progress = self.fun_control[\"show_progress\"]\n        self.infill_criterion = self.fun_control[\"infill_criterion\"]\n        self.n_points = self.fun_control[\"n_points\"]\n        self.max_surrogate_points = self.fun_control[\"max_surrogate_points\"]\n        self.progress_file = self.fun_control[\"progress_file\"]\n        self.tkagg = self.fun_control[\"tkagg\"]\n        if self.tkagg:\n            matplotlib.use(\"TkAgg\")\n        self.verbosity = self.fun_control[\"verbosity\"]\n\n        # Internal attributes:\n        self.X = None\n        self.y = None\n\n        # Logging information:\n        self.counter = 0\n        self.min_y = None\n        self.min_X = None\n        self.min_mean_X = None\n        self.min_mean_y = None\n        self.mean_X = None\n        self.mean_y = None\n        self.var_y = None\n        self.y_mo = None\n\n    def _design_setup(self, design) -&gt; None:\n        \"\"\"\n        Design related information:\n        If no design is specified, use the internal spacefilling design.\n        \"\"\"\n        self.design = design\n        if self.design is None:\n            self.design = SpaceFilling(k=self.k, seed=self.fun_control[\"seed\"])\n\n    def _optimizer_setup(self, optimizer) -&gt; None:\n        \"\"\"\n        Optimizer setup. If no optimizer is specified, use Differential Evolution.\n        \"\"\"\n        self.optimizer = optimizer\n        if self.optimizer is None:\n            self.optimizer = optimize.differential_evolution\n\n    def _surrogate_control_setup(self) -&gt; None:\n        self.surrogate_control.update({\"var_type\": self.var_type})\n        # Surrogate control updates:\n        # The default value for `method` from the surrogate_control dictionary\n        # based on surrogate_control.init() is None. This value is updated\n        # to the value of the key \"method\" from the fun_control dictionary.\n        # If the value is set (i.e., not None), it is not updated.\n        if self.surrogate_control[\"method\"] is None:\n            self.surrogate_control.update({\"method\": self.fun_control.method})\n        if self.surrogate_control[\"model_fun_evals\"] is None:\n            self.surrogate_control.update({\"model_fun_evals\": self.optimizer_control[\"max_iter\"]})\n        # self.optimizer is not None here. If 1) the key \"model_optimizer\"\n        # is still None or 2) a user specified optimizer is provided, update the value of\n        # the key \"model_optimizer\" to the value of self.optimizer.\n        if self.surrogate_control[\"model_optimizer\"] is None or self.optimizer is not None:\n            self.surrogate_control.update({\"model_optimizer\": self.optimizer})\n\n        # if self.surrogate_control[\"n_theta\"] is a string and == isotropic, use 1 theta value:\n        if isinstance(self.surrogate_control[\"n_theta\"], str):\n            if self.surrogate_control[\"n_theta\"] == \"anisotropic\":\n                self.surrogate_control.update({\"n_theta\": self.k})\n            else:\n                # case \"isotropic\":\n                self.surrogate_control.update({\"n_theta\": 1})\n        if isinstance(self.surrogate_control[\"n_theta\"], int):\n            if self.surrogate_control[\"n_theta\"] &gt; 1:\n                self.surrogate_control.update({\"n_theta\": self.k})\n\n    def _surrogate_setup(self, surrogate) -&gt; None:\n        # Surrogate related information:\n        self.surrogate = surrogate\n        # If no surrogate model is specified, use the internal\n        # spotpython kriging surrogate:\n        if self.surrogate is None:\n            # Call kriging with surrogate_control parameters:\n            self.surrogate = Kriging(\n                method=self.surrogate_control[\"method\"],\n                var_type=self.surrogate_control[\"var_type\"],\n                seed=self.surrogate_control[\"seed\"],\n                model_optimizer=self.surrogate_control[\"model_optimizer\"],\n                model_fun_evals=self.surrogate_control[\"model_fun_evals\"],\n                min_theta=self.surrogate_control[\"min_theta\"],\n                max_theta=self.surrogate_control[\"max_theta\"],\n                n_theta=self.surrogate_control[\"n_theta\"],\n                theta_init_zero=self.surrogate_control[\"theta_init_zero\"],\n                p_val=self.surrogate_control[\"p_val\"],\n                n_p=self.surrogate_control[\"n_p\"],\n                optim_p=self.surrogate_control[\"optim_p\"],\n                min_Lambda=self.surrogate_control[\"min_Lambda\"],\n                max_Lambda=self.surrogate_control[\"max_Lambda\"],\n                log_level=self.log_level,\n                spot_writer=self.spot_writer,\n                counter=self.design_control[\"init_size\"] * self.design_control[\"repeats\"] - 1,\n                metric_factorial=self.surrogate_control[\"metric_factorial\"],\n            )\n\n    def get_spot_attributes_as_df(self) -&gt; pd.DataFrame:\n        \"\"\"Get all attributes of the spot object as a pandas dataframe.\n\n        Returns:\n            (pandas.DataFrame): dataframe with all attributes of the spot object.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from math import inf\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 7\n                # number of points\n                n = 10\n                fun = Analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1]),\n                    upper = np.array([1]),\n                    fun_evals=n)\n                design_control=design_control_init(init_size=ni)\n                spot_1 = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                spot_1.run()\n                df = spot_1.get_spot_attributes_as_df()\n            df\n                df\n                    Attribute Name                                    Attribute Value\n                0                   X  [[-0.3378148180708981], [0.698908280342222], [...\n                1           all_lower                                               [-1]\n                2           all_upper                                                [1]\n                3        all_var_name                                               [x0]\n                4        all_var_type                                              [num]\n                5             counter                                                 10\n                6           de_bounds                                          [[-1, 1]]\n                7              design  &lt;spotpython.design.spacefilling.SpaceFilling o...\n                8      design_control                     {'init_size': 7, 'repeats': 1}\n                9                 eps                                                0.0\n                10        fun_control                         {'sigma': 0, 'seed': None}\n                11          fun_evals                                                 10\n                12        fun_repeats                                                  1\n                13              ident                                            [False]\n                14   infill_criterion                                                  y\n                15                  k                                                  1\n                16          log_level                                                 50\n                17              lower                                               [-1]\n                18           max_time                                                inf\n                19             mean_X                                               None\n                20             mean_y                                               None\n                21              min_X                           [1.5392206722432657e-05]\n                22         min_mean_X                                               None\n                23         min_mean_y                                               None\n                24              min_y                                                0.0\n                25           n_points                                                  1\n                26              noise                                               True\n                27         ocba_delta                                                  0\n                28  optimizer_control                    {'max_iter': 1000, 'seed': 125}\n                29            red_dim                                              False\n                30                rng                                   Generator(PCG64)\n                31               seed                                                123\n                32        show_models                                              False\n                33      show_progress                                               True\n                34        spot_writer                                               None\n                35          surrogate  &lt;spotpython.build.kriging.Kriging object at 0x...\n                36  surrogate_control  {'method': \"regession\", 'model_optimizer': &lt;function ...\n                37        tolerance_x                                                  0\n                38              upper                                                [1]\n                39           var_name                                               [x0]\n                40           var_type                                              [num]\n                41              var_y                                               None\n                42                  y  [0.11411885130827397, 0.48847278433092195, 0.0...\n\n        \"\"\"\n\n        attributes = [attr for attr in dir(self) if not callable(getattr(self, attr)) and not attr.startswith(\"__\")]\n        values = [getattr(self, attr) for attr in attributes]\n        df = pd.DataFrame({\"Attribute Name\": attributes, \"Attribute Value\": values})\n        return df\n\n    def to_red_dim(self) -&gt; None:\n        \"\"\"\n        Reduces the dimensionality of the optimization problem by removing dimensions\n        where lower and upper bounds are equal, indicating that those variables are fixed.\n        This function modifies the lower bounds, upper bounds, variable types, and variable names\n        by filtering out the non-varying dimensions. If any dimension is reduced, the `red_dim` attribute\n        is set to True, and the count of dimensions (`k`) is updated accordingly.\n\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (NoneType): None\n\n        Attributes:\n            self.lower (numpy.ndarray):\n                lower bound\n            self.upper (numpy.ndarray):\n                upper bound\n            self.var_type (List[str]):\n                list of variable types\n            self.ident (numpy.ndarray):\n                array of boolean values indicating fixed dimensions\n            self.red_dim (bool):\n                True if dimensions are reduced, False otherwise. Checks if any dimension is fixed.\n            self.all_lower (numpy.ndarray):\n                backup of the original lower bounds\n            self.all_upper (numpy.ndarray):\n                backup of the original upper bounds\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n                lower = np.array([-1, -1, 0, 0])\n                upper = np.array([1, -1, 0, 5])  # Second and third dimensions are fixed\n                fun_evals = 10\n                var_type = ['float', 'int', 'float', 'int']\n                var_name = ['x1', 'x2', 'x3', 'x4']\n                spot_instance = spot.Spot(\n                    # Assuming Spot takes fun, fun_control, design_control as arguments\n                    fun = Analytical().fun_sphere,  # Replace with appropriate function\n                    fun_control=fun_control_init(lower=lower, upper=upper, fun_evals=fun_evals, show_progress=True, log_level=50),\n                )\n                spot_instance.var_type = var_type\n                spot_instance.var_name = var_name\n                spot_instance.to_red_dim()\n                # Assert: Check if dimensions were reduced correctly\n                assert spot_instance.lower.size == 2, \"Expected size of lower to be 2 after reduction\"\n                assert spot_instance.upper.size == 2, \"Expected size of upper to be 2 after reduction\"\n                assert len(spot_instance.var_type) == 2, \"Expected size of var_type to be 2 after reduction\"\n                assert spot_instance.k == 2, \"Expected k to reflect the reduced dimensions\"\n                # Check remaining values\n                expected_lower = np.array([-1, 0])\n                expected_upper = np.array([1, 5])\n                expected_var_type = ['float', 'int']\n                # there are two remaining variables, they are named x1 and x2\n                expected_var_name = ['x1', 'x2']\n                np.testing.assert_array_equal(spot_instance.lower, expected_lower)\n                np.testing.assert_array_equal(spot_instance.upper, expected_upper)\n                assert spot_instance.var_type == expected_var_type\n                assert spot_instance.var_name == expected_var_name\n        \"\"\"\n        # Backup of the original values:\n        self.all_lower = self.lower\n        self.all_upper = self.upper\n        # Select only lower != upper:\n        self.ident = (self.upper - self.lower) == 0\n        # Determine if dimension is reduced:\n        self.red_dim = self.ident.any()\n        # Modifications:\n        # Modify lower and upper:\n        self.lower = self.lower[~self.ident]\n        self.upper = self.upper[~self.ident]\n        # Modify k (number of dim):\n        self.k = self.lower.size\n        # Filter out types and names corresponding to non-varying dimensions\n        if self.var_type is not None:\n            self.all_var_type = self.var_type.copy()\n            self.var_type = [vtype for vtype, fixed in zip(self.all_var_type, self.ident) if not fixed]\n\n        if self.var_name is not None:\n            self.all_var_name = self.var_name.copy()\n            self.var_name = [vname for vname, fixed in zip(self.all_var_name, self.ident) if not fixed]\n\n    def to_all_dim(self, X0: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Expands reduced-dimensional design points back to their full-dimensional representation\n        by reinserting fixed values for dimensions that were removed during the dimensionality\n        reduction process.\n        When `to_red_dim()` is called, dimensions where the lower and upper bounds are equal are\n        removed from the design points. `to_all_dim()` reverses this process by adding back these fixed\n        dimensions with their respective fixed values.\n\n        Args:\n            X0 (numpy.ndarray): reduced dimension design points\n\n        Returns:\n            (numpy.ndarray): full dimension design points\n\n        Atributes:\n            self.ident (numpy.ndarray):\n                array of boolean values indicating fixed dimensions\n            self.all_lower (numpy.ndarray):\n                backup of the original lower bounds.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n                lower = np.array([-1, -1, 0, 0])\n                upper = np.array([1, -1, 0, 5])  # Second and third dimensions are fixed\n                fun_evals = 10\n                var_type = ['float', 'int', 'float', 'int']\n                var_name = ['x1', 'x2', 'x3', 'x4']\n                spot_instance = spot.Spot(\n                    fun = Analytical().fun_sphere,\n                    fun_control=fun_control_init(lower=lower, upper=upper, fun_evals=fun_evals)\n                )\n                X0 = np.array([[2.5, 3.5], [4.5, 5.5]])\n                X_full_dim = spot_instance.to_all_dim(X0)\n                print(X_full_dim)\n                    [[ 2.5 -1.   0.   3.5]\n                    [ 4.5 -1.   0.   5.5]]\n        \"\"\"\n        # Number of design points (samples):\n        n = X0.shape[0]\n        # Number of dimensions:\n        k = len(self.ident)\n        # Initialize full dimension design points:\n        X = np.zeros((n, k))\n        # The following code was modified in 0.20.5:\n        # Index for navigating X0's compressed dimension\n        reduced_index = 0\n        # Iterate through each dimension, reconstructing full dimensionality\n        for i in range(k):\n            if self.ident[i]:\n                # Assign fixed dimension values using stored lower bounds\n                X[:, i] = self.all_lower[i]\n            else:\n                # Assign variable dimension values from the reduced array\n                X[:, i] = X0[:, reduced_index]\n                # Move to the next variable dimension in the compact X0 array\n                reduced_index += 1\n        return X\n\n    def to_all_dim_if_needed(self, X: np.ndarray) -&gt; np.array:\n        \"\"\"\n        Conditionally expand reduced-dimensional design points back to their full-dimensional representation,\n        if dimensionality reduction was performed.\n        This method checks whether dimensionality reduction occurred (i.e., whether some dimensions were\n        fixed and thus removed). If so, it uses `to_all_dim()` to restore the full-dimensional representation\n        by reinserting the fixed dimensions. Otherwise, it returns the input design points unaltered.\n\n        Args:\n            X (np.ndarray): A 2D numpy array of shape (n, m), where `n` is the number of samples, and `m`\n                            corresponds to the reduced or full number of dimensions depending on the\n                            `red_dim` status.\n\n        Returns:\n            np.ndarray: A 2D numpy array of shape (n, k). If `red_dim` is True, `k` will be the full number\n                        of dimensions (including both fixed and variable). If `red_dim` is False, `k` is\n                        identical to `m`.\n\n        Attributes:\n            self.red_dim (bool): A boolean attribute indicating if dimensionality was reduced\n                                (True if dimensions were reduced, False otherwise).\n        \"\"\"\n\n        if self.red_dim:\n            return self.to_all_dim(X)\n        else:\n            return X\n\n    def get_new_X0(self) -&gt; np.array:\n        \"\"\"\n        Generate new design points for the optimization process.\n        This method attempts to suggest and repair new design points using the surrogate model\n        and experimental design techniques. If no valid new points are found within the specified\n        tolerance, a new experimental design is generated.\n\n        Calls `suggest_new_X()` and repairs the new design points, e.g.,\n        by `repair_non_numeric()` and `selectNew()`.\n\n        Returns:\n            np.ndarray: New design points, possibly repeated according to `self.fun_repeats`.\n\n        Attributes:\n            self.design (object): An experimental design object used to generate new points\n            self.n_points (int): The expected number of new points\n            self.fun_repeats (int): The number of times to repeat new points\n            self.tolerance_x (float): Minimum distance required between new and existing solutions\n            self.var_type (List[str]): Variable types for the design points\n            self.X (np.ndarray): Existing solution points\n            self.k (int): Number of dimensions\n            self.fun_control (Dict): Control parameters for the function\n            self.counter (int): Iteration counter\n\n        Notes:\n            - If no new valid design points are suggested, the function resorts\n              to a space-filling design technique to generate the required points.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.utils.init import (\n                    fun_control_init,  design_control_init\n                    )\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init\n                # number of initial points:\n                ni = 3\n                X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n                fun = Analytical().fun_sphere\n                fun_control = fun_control_init(\n                            n_points=10,\n                            ocba_delta=0,\n                            lower = np.array([-1, -1]),\n                            upper = np.array([1, 1])\n                )\n                design_control=design_control_init(init_size=ni)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,\n                )\n                S.initialize_design(X_start=X_start)\n                S.update_stats()\n                S.fit_surrogate()\n                X0 = S.get_new_X0()\n                assert X0.shape[0] == S.n_points\n                assert X0.shape[1] == S.lower.size\n                # assert new points are in the interval [lower, upper]\n                assert np.all(X0 &gt;= S.lower)\n                assert np.all(X0 &lt;= S.upper)\n                # print using 20 digits precision\n                np.set_printoptions(precision=20)\n                print(f\"X0: {X0}\")\n                X0: [[-0.43905273463270317 -0.20947824142606025]\n                    [-0.4390526520612617  -0.20947735118625146]\n                    [-0.4390526516559971  -0.20947735345727678]\n                    [-0.4390526491133424  -0.20947735153559494]\n                    [-0.43905264887606393 -0.209477347335596  ]\n                    [-0.43905264815296263 -0.20947734884431773]\n                    [-0.4390526481478378  -0.2094773501907511 ]\n                    [-0.43905264791185933 -0.20947734931732975]\n                    [-0.43905264783691894 -0.20947734910961185]\n                    [-0.4390526473921517  -0.2094773511154602 ]]\n        \"\"\"\n        # Try to generate self.fun_repeats new X0 points:\n        X0 = self.suggest_new_X()\n        # Repair non-numeric variables based on their types\n        X0 = repair_non_numeric(X0, self.var_type)\n        # Condition: select only X0 that have min distance self.tolerance_x\n        # to existing solutions\n        X0, X0_ind = selectNew(A=X0, X=self.X, tolerance=self.tolerance_x)\n        if X0.shape[0] &gt; 0:\n            # If valid new points are found, repeat them as specified\n            # There are X0 that fullfil the condition.\n            # Note: The number of new X0 can be smaller than self.n_points!\n            logger.debug(\"XO values are new: %s %s\", X0_ind, X0)\n            return repeat(X0, self.fun_repeats, axis=0)\n        # If no X0 found, then generate self.n_points new solutions:\n        else:\n            self.design = SpaceFilling(k=self.k, seed=self.fun_control[\"seed\"] + self.counter)\n            X0 = self.generate_design(size=self.n_points, repeats=self.design_control[\"repeats\"], lower=self.lower, upper=self.upper)\n            X0 = repair_non_numeric(X0, self.var_type)\n            logger.warning(\"No new XO found on surrogate. Generate new solution %s\", X0)\n            return X0\n\n    def run(self, X_start: np.ndarray = None) -&gt; Spot:\n        \"\"\"\n        Run the surrogate based optimization.\n        The optimization process is controlled by the following steps:\n            1. Initialize design\n            2. Update stats\n            3. Fit surrogate\n            4. Update design\n            5. Update stats\n            6. Update writer\n            7. Fit surrogate\n            8. Show progress if needed\n\n        Args:\n            X_start (numpy.ndarray, optional):\n                initial design. The initial design must have shape (n, k), where n is the number of points and k is the number of dimensions. Defaults to None.\n\n        Returns:\n            Spot: The `Spot` instance configured and updated based on the optimization process.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import Spot\n                from spotpython.utils.init import (\n                    fun_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 7\n                # start point X_0\n                X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n                fun = Analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]))\n                design_control=design_control_init(init_size=ni)\n                S = Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                S.run(X_start=X_start)\n                    spotpython tuning: 0.0 [########--] 80.00%\n                    spotpython tuning: 0.0 [#########-] 86.67%\n                    spotpython tuning: 0.0 [#########-] 93.33%\n                    spotpython tuning: 0.0 [##########] 100.00% Done...\n            &gt;&gt;&gt; S.print_results()\n                min y: 0.0\n                x0: 0.0\n                x1: 0.0\n            &gt;&gt;&gt; S.X\n                array([[ 0.0000000000000000e+00,  0.0000000000000000e+00],\n                [ 0.0000000000000000e+00,  1.0000000000000000e+00],\n                [ 1.0000000000000000e+00,  0.0000000000000000e+00],\n                [ 1.0000000000000000e+00,  1.0000000000000000e+00],\n                [-9.0924338949946959e-01, -1.5823457680063502e-01],\n                [-2.0581710650646035e-01, -4.8124908877104844e-01],\n                [ 9.4974117111856260e-01, -9.4631271618736390e-01],\n                [-1.2095571372201608e-01,  6.3835886343683867e-02],\n                [-6.6278701759800063e-01,  1.7431637339680406e-01],\n                [ 2.8200844136299108e-01,  9.3001011398034406e-01],\n                [ 4.7878811540073962e-01,  6.5321058189282999e-01],\n                [ 1.5404061268479530e-04,  4.1895410759355553e-03],\n                [-1.7027205448129213e-04,  4.7698567182254507e-03],\n                [-4.4080972128058849e-04,  5.2785168039883147e-03],\n                [ 3.7700880321788425e-03,  1.8909833144458731e-02]])\n            &gt;&gt;&gt; S.y\n                array([0.0000000000000000e+00, 1.0000000000000000e+00,\n                1.0000000000000000e+00, 2.0000000000000000e+00,\n                8.5176172264376016e-01, 2.7396136677365612e-01,\n                1.7975160489355650e+00, 1.8705305067286033e-02,\n                4.6967282873066640e-01, 9.4444757310571603e-01,\n                6.5592212374576153e-01, 1.7575982937307560e-05,\n                2.2780525684937748e-05, 2.8057052860362483e-05,\n                3.7179535332164810e-04])\n\n        \"\"\"\n        #\n        PREFIX = self.fun_control[\"PREFIX\"]\n        filename = get_result_filename(PREFIX)\n        if os.path.exists(filename) and not self.fun_control.get(\"force_run\"):\n            # print a warning and load the result\n            print(f\"Result file {filename} exists. Loading the result.\")\n            S = load_result(filename=filename)\n            self._copy_from(S)\n            return self\n        else:\n            self.initialize_design(X_start)\n            self.update_stats()\n            self.fit_surrogate()\n            if self.verbosity &gt; 0:\n                print(\"\\n------ Starting optimization on the Surrogate for the given Budget ------\\n\")\n            timeout_start = time.time()\n            while self.should_continue(timeout_start):\n                self.update_design()\n                self.update_stats()\n                self.update_writer()\n                self.fit_surrogate()\n                self.show_progress_if_needed(timeout_start)\n\n            if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n                self.spot_writer.flush()\n                self.spot_writer.close()\n            if self.fun_control.get(\"db_dict_name\") is not None:\n                self._write_db_dict()\n\n            if self.fun_control.get(\"save_result\"):\n                self.save_result(verbosity=self.verbosity)\n            return self\n\n    def _copy_from(self, other) -&gt; None:\n        \"\"\"Copy attributes from another object.\n        This method copies all attributes from the `other` object to the current\n        object (`self`). It assumes that both objects are instances of a class\n        that share similar attributes.\n\n        Args:\n            other: An instance of a class from which attributes will be copied to\n            the current instance.\n\n        \"\"\"\n        for attr in other.__dict__:\n            setattr(self, attr, getattr(other, attr))\n\n    def initialize_design(self, X_start=None) -&gt; None:\n        \"\"\"\n        Initialize design. Generate and evaluate initial design.\n        If `X_start` is not `None`, append it to the initial design.\n        Therefore, the design size is `init_size` + `X_start.shape[0]`.\n\n        Args:\n            X_start (numpy.ndarray, optional):\n                initial design. Must be of shape (n, k), where n is the number\n                of points and k is the number of dimensions. Defaults to None.\n\n        Attributes:\n            self.X (numpy.ndarray): initial design\n            self.y (numpy.ndarray): initial design values\n\n        Note:\n            * If `X_start` is has the wrong shape, it is ignored.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init,  design_control_init\n                    )\n                # number of initial points:\n                ni = 7\n                # start point X_0\n                X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n                fun = Analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]))\n                design_control=design_control_init(init_size=ni)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                S.initialize_design(X_start=X_start)\n                print(f\"S.X: {S.X}\")\n                    S.X: [[ 0.          0.        ]\n                        [ 0.          1.        ]\n                        [ 1.          0.        ]\n                        [ 1.          1.        ]\n                        [-0.90924339 -0.15823458]\n                        [-0.20581711 -0.48124909]\n                        [ 0.94974117 -0.94631272]\n                        [-0.12095571  0.06383589]\n                        [-0.66278702  0.17431637]\n                        [ 0.28200844  0.93001011]\n                        [ 0.47878812  0.65321058]]\n                print(f\"S.y: {S.y}\")\n                        S.y: [0.         1.         1.         2.         0.85176172 0.27396137\n                            1.79751605 0.01870531 0.46967283 0.94444757 0.65592212]\n        \"\"\"\n        self.initialize_design_matrix(X_start)\n\n        self.evaluate_initial_design()\n\n        self.write_initial_tensorboard_log()\n\n    def initialize_design_matrix(self, X_start=None) -&gt; None:\n        \"\"\"\n        Initialize the design matrix for the optimization process.\n        This method generates an initial design matrix, optionally\n        appending any provided starting points (`X_start`). The resulting\n        design matrix is sanitized for non-numeric values and stored in `self.X`.\n\n        Args:\n            X_start (numpy.ndarray, optional): User-provided starting points\n                for the design. Shape should be (n=n_samples, k=n_features).\n                Defaults to None.\n\n        Returns:\n            numpy.ndarray: The design matrix that combines the generated design\n                        with the provided starting points.\n\n        Raises:\n            Exception: If the resulting design matrix has zero rows.\n\n        Notes:\n            * If `X_start` is not in the expected shape, it is ignored.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun import Analytical\n                from spotpython.spot import Spot\n                from spotpython.utils.init import fun_control_init\n                fun_control = fun_control_init(\n                    tensorboard_log=True,\n                    TENSORBOARD_CLEAN=True,\n                    lower = np.array([-1]),\n                    upper = np.array([1])\n                    )\n                fun = Analytical().fun_sphere\n                S = Spot(fun=fun,\n                            fun_control=fun_control,\n                            )\n                X_start = np.array([[0.5, 0.5], [0.4, 0.4]])\n                design_matrix = S.initialize_design_matrix(X_start)\n                print(f\"Design matrix: {design_matrix}\")\n                    Design matrix: [[ 0.1         0.2       ]\n                    [ 0.3         0.4       ]\n                    [ 0.86352963  0.7892358 ]\n                    [-0.24407197 -0.83687436]\n                    [ 0.36481882  0.8375811 ]\n                    [ 0.415331    0.54468512]\n                    [-0.56395091 -0.77797854]\n                    [-0.90259409 -0.04899292]\n                    [-0.16484832  0.35724741]\n                    [ 0.05170659  0.07401196]\n                    [-0.78548145 -0.44638164]\n                    [ 0.64017497 -0.30363301]]\n        \"\"\"\n        if self.design_control[\"init_size\"] &gt; 0:\n            X0 = self.generate_design(\n                size=self.design_control[\"init_size\"],\n                repeats=self.design_control[\"repeats\"],\n                lower=self.lower,\n                upper=self.upper,\n            )\n\n        if X_start is not None:\n            if not isinstance(X_start, np.ndarray):\n                X_start = np.array(X_start)\n            X_start = np.atleast_2d(X_start)\n            try:\n                if self.design_control[\"init_size\"] &gt; 0:\n                    X0 = np.append(X_start, X0, axis=0)\n                else:\n                    X0 = X_start\n            except ValueError:\n                logger.warning(\"X_start has wrong shape. Ignoring it.\")\n\n        if X0.shape[0] == 0:\n            raise Exception(\"X0 has zero rows. Check design_control['init_size'] or X_start.\")\n\n        self.X = repair_non_numeric(X0, self.var_type)\n\n    def _store_mo(self, y_mo) -&gt; None:\n        # store y_mo in self.y_mo (append new values) if mo, otherwise self.y_mo is None\n        if self.y_mo is None and y_mo.ndim == 2:\n            self.y_mo = y_mo\n        else:  # append new values\n            # before stacking the arrays, check if the number of columns is the same in the mo case\n            if y_mo.ndim == 2 and self.y_mo.ndim == 2:\n                if self.y_mo.shape[1] != y_mo.shape[1]:\n                    print(f\"Shape of y_mo: {y_mo.shape}\")\n                    print(f\"y_mo: {y_mo}\")\n                    print(f\"Shape of self.y_mo: {self.y_mo.shape}\")\n                    print(f\"self.y_mo: {self.y_mo}\")\n                    raise ValueError(f\"Number of columns (objectives) in y_mo ({y_mo.shape[1]}) \" f\"does not match the number of columns in self.y_mo ({self.y_mo.shape[1]})\")\n                self.y_mo = np.vstack((self.y_mo, y_mo))\n\n    def _mo2so(self, y_mo) -&gt; None:\n        \"\"\"\n        Converts multi-objective values to a single-objective value by applying a user-defined\n        function from ``fun_control['fun_mo2so']``. If no user-defined function is given, the\n        values in the first objective row are used.\n\n        This method is called after the objective function evaluation (i.e., after ``self.fun()``).\n        It typically returns a 1D array with the single-objective values.\n\n        Args:\n            y_mo (numpy.ndarray):\n                If multi-objective values are present, this is an array of shape (n, m), where ``m`` is\n                the number of objectives and ``n`` is the number of data points.\n                Otherwise, it is an array of shape (n,) with single-objective values.\n        Returns:\n            numpy.ndarray:\n                A 1D array of shape (n,) with single-objective values.\n\n        \"\"\"\n        n, m = get_shape(y_mo)\n        self._store_mo(y_mo)\n        # do not use m as a condition, because m can be None, use ndim instead\n        if y_mo.ndim == 2:\n            if self.fun_control[\"fun_mo2so\"] is not None:\n                y0 = self.fun_control[\"fun_mo2so\"](y_mo)\n            else:\n                # Select the first column of an (n,m) array\n                if y_mo.size &gt; 0:\n                    y0 = y_mo[:, 0]\n                else:\n                    y0 = y_mo\n        else:\n            y0 = y_mo\n        return y0\n\n    def evaluate_initial_design(self) -&gt; None:\n        \"\"\"\n        Evaluate the initial design.\n\n        This method evaluates the initial design matrix `X0` by applying the objective function\n        and handling NaN values. The results are stored in `self.X` and `self.y`.\n\n        Raises:\n            ValueError: If the resulting design matrix has zero rows after removing NaN values.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import Spot\n                from spotpython.utils.init import fun_control_init\n                fun_control = fun_control_init(\n                    lower=np.array([-1, -1]),\n                    upper=np.array([1, 1])\n                )\n                fun = Analytical().fun_sphere\n                S = Spot(fun=fun, fun_control=fun_control)\n                X0 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n                S.initialize_design_matrix(X_start=X0)\n                S.evaluate_initial_design()\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n                    S.X: [[ 0.          0.        ]\n                    [ 0.          1.        ]\n                    [ 1.          0.        ]\n                    [ 1.          1.        ]\n                    [ 0.86352963  0.7892358 ]\n                    [-0.24407197 -0.83687436]\n                    [ 0.36481882  0.8375811 ]\n                    [ 0.415331    0.54468512]\n                    [-0.56395091 -0.77797854]\n                    [-0.90259409 -0.04899292]\n                    [-0.16484832  0.35724741]\n                    [ 0.05170659  0.07401196]\n                    [-0.78548145 -0.44638164]\n                    [ 0.64017497 -0.30363301]]\n                    S.y: [0.         1.         1.         2.         1.36857656 0.75992983\n                    0.83463487 0.46918172 0.92329124 0.8170764  0.15480068 0.00815134\n                    0.81623768 0.502017  ]\n        \"\"\"\n        # check that self.X has at leat one row and is not None\n        if self.X is None or self.X.shape[0] == 0:\n            raise ValueError(\"The design matrix has zero rows. Check design_control['init_size'] or X_start.\")\n\n        X_all = self.to_all_dim_if_needed(self.X)\n        logger.debug(\"In Spot() evaluate_initial_design(), before calling self.fun: X_all: %s\", X_all)\n        logger.debug(\"In Spot() evaluate_initial_design(), before calling self.fun: fun_control: %s\", self.fun_control)\n\n        y_mo = self.fun(X=X_all, fun_control=self.fun_control)\n        if self.verbosity &gt; 1:\n            print(f\"y_mo as returned from fun(): {y_mo}\")\n            print(f\"y_mo shape: {y_mo.shape}\")\n\n        #  Convert multi-objective values to single-objective values\n        # TODO: Store y_mo in self.y_mo (append new values)\n        self.y = self._mo2so(y_mo)\n        self.y = apply_penalty_NA(self.y, self.fun_control[\"penalty_NA\"], verbosity=self.verbosity)\n        logger.debug(\"In Spot() evaluate_initial_design(), after calling self.fun: self.y: %s\", self.y)\n\n        # TODO: Error if only nan values are returned\n        logger.debug(\"New y value: %s\", self.y)\n\n        self.counter = self.y.size\n        self.X, self.y = remove_nan(self.X, self.y, stop_on_zero_return=True)\n\n        if self.X.shape[0] == 0:\n            raise ValueError(\"The resulting design matrix has zero rows after removing NaN values.\")\n\n        logger.debug(\"In Spot() evaluate_initial_design(), final X val, after remove nan: self.X: %s\", self.X)\n        logger.debug(\"In Spot() evaluate_initial_design(), final y val, after remove nan: self.y: %s\", self.y)\n\n    def write_initial_tensorboard_log(self) -&gt; None:\n        \"\"\"Writes initial design data using the spot_writer. The spot_writer\n        is a tensorboard writer that writes the data to a tensorboard file.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init\n                fun = Analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1]),\n                    upper = np.array([1])\n                    )\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            )\n                S.initialize_design()\n                S.write_initial_tensorboard_log()\n                    Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_01_12_09_24_15\n                    Created spot_tensorboard_path: runs/spot_logs/00_p040025_2025-01-12_09-24-15 for SummaryWriter()\n        \"\"\"\n        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n            # range goes to init_size -1 because the last value is added by update_stats(),\n            # which always adds the last value.\n            # Changed in 0.5.9:\n            for j in range(len(self.y)):\n                X_j = self.X[j].copy()\n                y_j = self.y[j].copy()\n                config = {self.var_name[i]: X_j[i] for i in range(self.k)}\n                # var_dict = assign_values(X, get_var_name(fun_control))\n                # config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n                # see: https://github.com/pytorch/pytorch/issues/32651\n                # self.spot_writer.add_hparams(config, {\"spot_y\": y_j}, run_name=self.spot_tensorboard_path)\n                self.spot_writer.add_hparams(config, {\"hp_metric\": y_j})\n                self.spot_writer.flush()\n\n    def update_stats(self) -&gt; None:\n        \"\"\"\n        Update the following stats:\n        1. `min_y`, 2. `min_X`, and 3. `counter`\n        If `noise` is `True`, additionally the following stats are computed:\n        1. `mean_X`,  2. `mean_y`,  3. `var_y`, 4. `min_mean_X`(X value of the best mean y value so far),\n        5. `min_mean_y` (best mean y value so far), and 6. `min_var_y` (ariance of the best mean y value so far).\n\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (NoneType): None\n\n        Attributes:\n            self.min_y (float): minimum y value\n            self.min_X (numpy.ndarray): X value of the minimum y value\n            self.counter (int): number of function evaluations\n            self.mean_X (numpy.ndarray): mean X values\n            self.mean_y (numpy.ndarray): mean y values\n            self.var_y (numpy.ndarray): variance of y values\n            self.min_mean_y (float): minimum mean y value\n            self.min_mean_X (numpy.ndarray): X value of the minimum mean y value\n\n        \"\"\"\n        self.min_y = min(self.y)\n        self.min_X = self.X[argmin(self.y)]\n        self.counter = self.y.size\n        self.fun_control.update({\"counter\": self.counter})\n        # Update aggregated x and y values (if noise):\n        if self.noise:\n            Z = aggregate_mean_var(X=self.X, y=self.y)\n            self.mean_X = Z[0]\n            self.mean_y = Z[1]\n            self.var_y = Z[2]\n            # X value of the best mean y value so far:\n            self.min_mean_X = self.mean_X[argmin(self.mean_y)]\n            # best mean y value so far:\n            self.min_mean_y = self.mean_y[argmin(self.mean_y)]\n            # variance of the best mean y value so far:\n            self.min_var_y = self.var_y[argmin(self.mean_y)]\n\n    def fit_surrogate(self) -&gt; None:\n        \"\"\"\n        Fit surrogate model. The surrogate model\n        is fitted to the data stored in `self.X` and `self.y`.\n        It uses the generic `fit()` method of the\n        surrogate model `surrogate`. The default surrogate model is\n        an instance from spotpython's `Kriging` class.\n        If `show_models` is `True`, the model is plotted.\n        If the number of points is greater than `max_surrogate_points`,\n        the surrogate model is fitted to a subset of the data points.\n        The subset is selected using the `select_distant_points()` function.\n\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (NoneType): None\n\n        Attributes:\n            self.surrogate (object):\n                surrogate model\n            self.X (numpy.ndarray):\n                design points\n            self.y (numpy.ndarray):\n                function values\n            self.max_surrogate_points (int):\n                maximum number of points to fit the surrogate model\n            self.show_models (bool):\n                if True, the model is plotted\n\n        Note:\n            * As shown in https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/\n            other surrogate models can be used as well.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n                # number of initial points:\n                ni = 0\n                X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1])\n                    )\n                design_control=design_control_init(init_size=ni)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                S.initialize_design(X_start=X_start)\n                S.update_stats()\n                S.fit_surrogate()\n                S.surrogate.predict(np.array([[0, 0]]))\n                    array([1.49011612e-08])\n\n        \"\"\"\n        logger.debug(\"In fit_surrogate(): self.X: %s\", self.X)\n        logger.debug(\"In fit_surrogate(): self.y: %s\", self.y)\n        logger.debug(\"In fit_surrogate(): self.X.shape: %s\", self.X.shape)\n        logger.debug(\"In fit_surrogate(): self.y.shape: %s\", self.y.shape)\n        X_points = self.X.shape[0]\n        y_points = self.y.shape[0]\n        if X_points == y_points:\n            if X_points &gt; self.max_surrogate_points:\n                logger.info(\"Selecting distant points for surrogate fitting.\")\n                X_S, y_S = select_distant_points(X=self.X, y=self.y, k=self.max_surrogate_points)\n            else:\n                X_S = self.X\n                y_S = self.y\n            self.surrogate.fit(X_S, y_S)\n        else:\n            logger.warning(\"X and y have different sizes. Surrogate not fitted.\")\n        if self.show_models:\n            self.plot_model()\n\n    def update_design(self) -&gt; None:\n        \"\"\"\n        Update design. Generate and evaluate new design points.\n        It is basically a call to the method `get_new_X0()`.\n        If `noise` is `True`, additionally the following steps\n        (from `get_X_ocba()`) are performed:\n        1. Compute OCBA points.\n        2. Evaluate OCBA points.\n        3. Append OCBA points to the new design points.\n\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (NoneType): None\n\n        Attributes:\n            self.X (numpy.ndarray): updated design\n            self.y (numpy.ndarray): updated design values\n\n        Examples:\n            &gt;&gt;&gt; # 1. Without OCBA points:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                from spotpython.spot import Spot\n                # number of initial points:\n                ni = 0\n                X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n                fun = Analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1])\n                    )\n                design_control=design_control_init(init_size=ni)\n                S = Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,)\n                S.initialize_design(X_start=X_start)\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n                X_shape_before = S.X.shape\n                print(f\"X_shape_before: {X_shape_before}\")\n                print(f\"y_size_before: {S.y.size}\")\n                y_size_before = S.y.size\n                S.update_stats()\n                S.fit_surrogate()\n                S.update_design()\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n                print(f\"S.n_points: {S.n_points}\")\n                print(f\"X_shape_after: {S.X.shape}\")\n                print(f\"y_size_after: {S.y.size}\")\n            &gt;&gt;&gt; #\n            &gt;&gt;&gt; # 2. Using the OCBA points:\n                import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import Spot\n                from spotpython.utils.init import fun_control_init, design_control_init\n                # number of initial points:\n                ni = 3\n                X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n                fun = Analytical().fun_sphere\n                fun_control = fun_control_init(\n                        sigma=0.02,\n                        lower = np.array([-1, -1]),\n                        upper = np.array([1, 1]),\n                        noise=True,\n                        ocba_delta=1,\n                    )\n                design_control=design_control_init(init_size=ni, repeats=2)\n                S = Spot(fun=fun,\n                            design_control=design_control,\n                            fun_control=fun_control\n                )\n                S.initialize_design(X_start=X_start)\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n                X_shape_before = S.X.shape\n                print(f\"X_shape_before: {X_shape_before}\")\n                print(f\"y_size_before: {S.y.size}\")\n                y_size_before = S.y.size\n                S.update_stats()\n                S.fit_surrogate()\n                S.update_design()\n                print(f\"S.X: {S.X}\")\n                print(f\"S.y: {S.y}\")\n                print(f\"S.n_points: {S.n_points}\")\n                print(f\"S.ocba_delta: {S.ocba_delta}\")\n                print(f\"X_shape_after: {S.X.shape}\")\n                print(f\"y_size_after: {S.y.size}\")\n                # compare the shapes of the X and y values before and after the update_design method\n                assert X_shape_before[0] + S.ocba_delta == S.X.shape[0]\n                assert X_shape_before[1] == S.X.shape[1]\n                assert y_size_before + S.ocba_delta == S.y.size\n                Experiment saved to 000_exp.pkl\n                    S.X: [[ 0.          1.        ]\n                    [ 1.          0.        ]\n                    [ 1.          1.        ]\n                    [ 1.          1.        ]\n                    [ 0.54509876 -0.36921401]\n                    [ 0.54509876 -0.36921401]\n                    [ 0.18642675  0.87708546]\n                    [ 0.18642675  0.87708546]\n                    [-0.45060393 -0.208063  ]\n                    [-0.45060393 -0.208063  ]]\n                    S.y: [0.98021757 0.99264427 2.02575851 2.00387949 0.45185626 0.44499372\n                    0.79130456 0.81487288 0.24000221 0.23988634]\n                    X_shape_before: (10, 2)\n                    y_size_before: 10\n                    S.X: [[ 0.          1.        ]\n                    [ 1.          0.        ]\n                    [ 1.          1.        ]\n                    [ 1.          1.        ]\n                    [ 0.54509876 -0.36921401]\n                    [ 0.54509876 -0.36921401]\n                    [ 0.18642675  0.87708546]\n                    [ 0.18642675  0.87708546]\n                    [-0.45060393 -0.208063  ]\n                    [-0.45060393 -0.208063  ]\n                    [-0.02292587  0.0145145 ]]\n                    S.y: [ 0.98021757  0.99264427  2.02575851  2.00387949  0.45185626  0.44499372\n                    0.79130456  0.81487288  0.24000221  0.23988634 -0.01904616]\n                    S.n_points: 1\n                    S.ocba_delta: 1\n                    X_shape_after: (11, 2)\n                    y_size_after: 11\n        \"\"\"\n        # OCBA (only if noise). Determination of the OCBA points depends on the\n        # old X and y values.\n        if self.noise and self.ocba_delta &gt; 0 and not np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &lt;= 2):\n            logger.warning(\"self.var_y &lt;= 0. OCBA points are not generated:\")\n            logger.warning(\"There are less than 3 points or points with no variance information.\")\n            logger.debug(\"In update_design(): self.mean_X: %s\", self.mean_X)\n            logger.debug(\"In update_design(): self.var_y: %s\", self.var_y)\n        if self.noise and self.ocba_delta &gt; 0 and np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &gt; 2):\n            X_ocba = get_ocba_X(self.mean_X, self.mean_y, self.var_y, self.ocba_delta)\n        else:\n            X_ocba = None\n        # Determine the new X0 values based on the old X and y values:\n        X0 = self.get_new_X0()\n        # Append OCBA points to the new design points:\n        if self.noise and self.ocba_delta &gt; 0 and np.all(self.var_y &gt; 0):\n            X0 = append(X_ocba, X0, axis=0)\n        X_all = self.to_all_dim_if_needed(X0)\n        logger.debug(\n            \"In update_design(): self.fun_control sigma and seed passed to fun(): %s %s\",\n            self.fun_control[\"sigma\"],\n            self.fun_control[\"seed\"],\n        )\n        # (S-18): Evaluating New Solutions:\n        y_mo = self.fun(X=X_all, fun_control=self.fun_control)\n        # Convert multi-objective values to single-objective values:\n        y0 = self._mo2so(y_mo)\n        # Apply penalty for NA values works only on so values:\n        y0 = apply_penalty_NA(y0, self.fun_control[\"penalty_NA\"], verbosity=self.verbosity)\n        X0, y0 = remove_nan(X0, y0, stop_on_zero_return=False)\n        # Append New Solutions (only if they are not nan):\n        if y0.shape[0] &gt; 0:\n            self.X = np.append(self.X, X0, axis=0)\n            self.y = np.append(self.y, y0)\n        else:\n            # otherwise, generate a random point and append it to the design\n            Xr, yr = self.generate_random_point()\n            self.X = np.append(self.X, Xr, axis=0)\n            self.y = np.append(self.y, yr)\n\n    def save_result(self, filename=None, path=None, overwrite=True, verbosity=0) -&gt; None:\n        \"\"\"\n        Save the results to a file.\n        If filename is not provided, the filename is generated based on the PREFIX using the\n        `get_result_filename()` function. The results file is saved in the current working directory\n        unless a path is provided. The file is saved in pickle format using the highest protocol.\n        If no arguments are provided, the file is saved with the default name PREFIX + \"_res.pkl\".\n\n        Args:\n            filename (str):\n                The filename of the results file. If not provided,\n                the filename is generated based on the PREFIX using the\n                `get_result_filename()` function. Default is `None`.\n            path (str):\n                The path to the results file. If not provided, the file\n                is saved in the current working directory. Default is `None`.\n            overwrite (bool):\n                If `True`, the file will be overwritten if it already exists.\n                Default is `True`.\n            verbosity (int):\n                The level of verbosity. Default is 0.\n\n        Returns:\n            None\n        \"\"\"\n        PREFIX = self.fun_control.get(\"PREFIX\", \"result\")\n        if filename is None:\n            filename = get_result_filename(PREFIX)\n        self.save_experiment(filename=filename, path=path, overwrite=overwrite, unpickleables=\"file_io\", verbosity=verbosity)\n\n    def save_experiment(self, filename=None, path=None, overwrite=True, unpickleables=\"file_io\", verbosity=0) -&gt; None:\n        \"\"\"\n        Save the experiment to a file.\n        If no filename is provided, the filename is generated based on the PREFIX using the\n        `get_experiment_filename()` function. The experiment file is saved in the current working directory\n        unless a path is provided. The file is saved in pickle format using the highest protocol.\n        If no arguments are provided, the file is saved with the default name PREFIX + \"_exp.pkl\".\n\n        Args:\n            filename (str):\n                The filename of the experiment file. If not provided,\n                the filename is generated based on the PREFIX using the\n                `get_experiment_filename()` function. Default is `None`.\n            path (str):\n                The path to the experiment file. If not provided, the file\n                is saved in the current working directory. Default is `None`.\n            overwrite (bool):\n                If `True`, the file will be overwritten if it already exists.\n                Default is `True`.\n            unpickleables (str):\n                The type of unpickleable components to exclude. Default is \"file_io\", which\n                excludes file I/O components like the spot_writer and logger.\n                If set to any other value, the function will exclude the function, optimizer,\n                surrogate, data_set, scaler, rng, and design components.\n                Default is \"file_io\".\n            verbosity (int):\n                The level of verbosity. Default is 0.\n\n        Returns:\n            None\n        \"\"\"\n        # Ensure we don't accidentally try to pickle unpicklable components\n        self._close_and_del_spot_writer()\n        self._remove_logger_handlers()\n\n        S = self._get_pickle_safe_spot_tuner(unpickleables=unpickleables, verbosity=verbosity)\n\n        # Determine the filename based on PREFIX if not provided\n        PREFIX = self.fun_control.get(\"PREFIX\", \"experiment\")\n        if filename is None:\n            filename = get_experiment_filename(PREFIX)\n\n        if path is not None:\n            filename = os.path.join(path, filename)\n            if not os.path.exists(path):\n                os.makedirs(path)\n\n        # Check if the file already exists\n        if filename is not None and os.path.exists(filename) and not overwrite:\n            print(f\"Error: File {filename} already exists. Use overwrite=True to overwrite the file.\")\n            return\n\n        # Serialize the experiment dictionary to the pickle file\n        if filename is not None:\n            with open(filename, \"wb\") as handle:\n                try:\n                    pickle.dump(S, handle, protocol=pickle.HIGHEST_PROTOCOL)\n                except Exception as e:\n                    print(f\"Error during pickling: {e}\")\n                    raise e\n            print(f\"Experiment saved to {filename}\")\n\n    def _remove_logger_handlers(self) -&gt; None:\n        \"\"\"\n        Remove handlers from the logger to avoid pickling issues.\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        for handler in list(logger.handlers):  # Copy the list to avoid modification during iteration\n            logger.removeHandler(handler)\n\n    def _close_and_del_spot_writer(self) -&gt; None:\n        \"\"\"\n        Delete the spot_writer attribute from the object\n        if it exists and close the writer.\n        \"\"\"\n        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n            self.spot_writer.flush()\n            self.spot_writer.close()\n            del self.spot_writer\n\n    def _get_pickle_safe_spot_tuner(self, unpickleables=\"file_io\", verbosity=0) -&gt; Spot:\n        \"\"\"\n        Create a copy of self excluding unpickleable components for safe pickling.\n        This ensures no unpicklable components are passed to pickle.dump().\n\n        Args:\n            unpickleables (str):\n                The type of unpickleable components to exclude. Default is \"file_io\", which\n                excludes file I/O components like the spot_writer and logger.\n                If set to any other value, the function will exclude the function, optimizer,\n                surrogate, data_set, scaler, rng, and design components.\n                Default is \"file_io\".\n            verbosity (int):\n                The level of verbosity. Default is 0.\n\n        Returns:\n            Spot: A copy of the Spot instance with unpickleable components removed.\n        \"\"\"\n        # List of attributes that can't be pickled\n        if unpickleables == \"file_io\":\n            unpickleable_attrs = [\"spot_writer\", \"logger\"]\n        else:\n            unpickleable_attrs = [\"spot_writer\", \"logger\", \"fun\", \"optimizer\", \"surrogate\", \"data_set\", \"scaler\", \"rng\", \"design\"]\n        # Prepare a dictionary to store picklable state\n        picklable_state = {}\n\n        # Copy picklable attributes to the dictionary\n        for key, value in self.__dict__.items():\n            if key not in unpickleable_attrs:\n                try:\n                    # Test if the attribute can be pickled\n                    copy.deepcopy(value)\n                    picklable_state[key] = value\n                    if verbosity &gt; 1:\n                        print(f\"Attribute {key} is picklable and will be included in the experiment file.\")\n                except Exception:\n                    if verbosity &gt; 0:\n                        print(f\"Attribute {key} is not picklable and will be excluded from the experiment file.\")\n                    continue\n\n        # Use the dictionary to create a new instance\n        picklable_instance = self.__class__.__new__(self.__class__)\n        picklable_instance.__dict__.update(picklable_state)\n        if verbosity &gt; 1:\n            print(f\"Picklable instance created: {picklable_instance.__dict__}\")\n\n        return picklable_instance\n\n    def _init_spot_writer(self) -&gt; None:\n        \"\"\"\n        Initialize the spot_writer for the current experiment if in fun_control\n        the tensorboard_log is set to True\n        and the spot_tensorboard_path is not None.\n        Otherwise, the spot_writer is set to None.\n        \"\"\"\n        if self.fun_control[\"tensorboard_log\"] and self.fun_control[\"spot_tensorboard_path\"] is not None:\n            self.spot_writer = SummaryWriter(log_dir=self.fun_control[\"spot_tensorboard_path\"])\n            if self.verbosity &gt; 0:\n                print(f\"_init_spot_writer(): Created spot_tensorboard_path: {self.fun_control['spot_tensorboard_path']} for SummaryWriter()\")\n        else:\n            self.spot_writer = None\n            if self.verbosity &gt; 0:\n                print(\"No tensorboard log created.\")\n\n    def should_continue(self, timeout_start) -&gt; bool:\n        return (self.counter &lt; self.fun_evals) and (time.time() &lt; timeout_start + self.max_time * 60)\n\n    def generate_random_point(self):\n        \"\"\"Generate a random point in the design space.\n\n        Returns:\n            (tuple): tuple containing:\n                X0 (numpy.ndarray): random point in the design space\n                y0 (numpy.ndarray): function value at X\n\n        Notes:\n            If the evaluation fails, the function returns arrays of shape[0] == 0.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1])\n                    )\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            )\n                X0, y0 = S.generate_random_point()\n                print(f\"X0: {X0}\")\n                print(f\"y0: {y0}\")\n                assert X0.size == 2\n                assert y0.size == 1\n                assert np.all(X0 &gt;= S.lower)\n                assert np.all(X0 &lt;= S.upper)\n                assert y0 &gt;= 0\n        \"\"\"\n        X0 = self.generate_design(\n            size=1,\n            repeats=1,\n            lower=self.lower,\n            upper=self.upper,\n        )\n        X0 = repair_non_numeric(X=X0, var_type=self.var_type)\n        X_all = self.to_all_dim_if_needed(X0)\n        logger.debug(\"In Spot() generate_random_point(), before calling self.fun: X_all: %s\", X_all)\n        logger.debug(\"In Spot() generate_random_point(), before calling self.fun: fun_control: %s\", self.fun_control)\n        # Convert multi-objective values to single-objective values\n        # TODO: Store y_mo in self.y_mo (append new values)\n        y_mo = self.fun(X=X_all, fun_control=self.fun_control)\n        y0 = self._mo2so(y_mo)\n        # Apply penalty for NA values works only on so values:\n        y0 = apply_penalty_NA(y0, self.fun_control[\"penalty_NA\"], verbosity=self.verbosity)\n        X0, y0 = remove_nan(X0, y0, stop_on_zero_return=False)\n        return X0, y0\n\n    def show_progress_if_needed(self, timeout_start) -&gt; None:\n        \"\"\"Show progress bar if `show_progress` is `True`. If\n        self.progress_file is not `None`, the progress bar is saved\n        in the file with the name `self.progress_file`.\n\n        Args:\n            self (object): Spot object\n            timeout_start (float): start time\n\n        Returns:\n            (NoneType): None\n        \"\"\"\n        if not self.show_progress:\n            return\n        if isfinite(self.fun_evals):\n            progress_bar(progress=self.counter / self.fun_evals, y=self.min_y, filename=self.progress_file)\n        else:\n            progress_bar(progress=(time.time() - timeout_start) / (self.max_time * 60), y=self.min_y, filename=self.progress_file)\n\n    def generate_design(self, size, repeats, lower, upper) -&gt; np.array:\n        \"\"\"Generate a design with `size` points in the interval [lower, upper].\n\n        Args:\n            size (int): number of points\n            repeats (int): number of repeats\n            lower (numpy.ndarray): lower bound of the design space\n            upper (numpy.ndarray): upper bound of the design space\n\n        Returns:\n            (numpy.ndarray): design points\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.spot import spot\n                from spotpython.utils.init import design_control_init\n                from spotpython.fun.objectivefunctions import Analytical\n                design_control = design_control_init(init_size=3)\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]),\n                    fun_evals=fun_evals,\n                    tolerance_x = np.sqrt(np.spacing(1))\n                    )\n                S = spot.Spot(fun = analytical().fun_sphere,\n                            fun_control = fun_control,\n                            design_control = design_control)\n                X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n                assert X.shape[0] == 3\n                assert X.shape[1] == 2\n                print(X)\n                    array([[77.25493789,  0.31539299],\n                    [59.32133757,  0.93854273],\n                    [27.4698033 ,  0.3959685 ]])\n        \"\"\"\n        return self.design.scipy_lhd(n=size, repeats=repeats, lower=lower, upper=upper)\n\n    def update_writer(self) -&gt; None:\n        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n            # get the last y value:\n            y_last = self.y[-1].copy()\n            if self.noise is False:\n                y_min = self.min_y.copy()\n                X_min = self.min_X.copy()\n                # y_min: best y value so far\n                # y_last: last y value, can be worse than y_min\n                self.spot_writer.add_scalars(\"spot_y\", {\"min\": y_min, \"last\": y_last}, self.counter)\n                # X_min: X value of the best y value so far\n                self.spot_writer.add_scalars(\"spot_X\", {f\"X_{i}\": X_min[i] for i in range(self.k)}, self.counter)\n            else:\n                # get the last n y values:\n                y_last_n = self.y[-self.fun_repeats :].copy()\n                # y_min_mean: best mean y value so far\n                y_min_mean = self.min_mean_y.copy()\n                # X_min_mean: X value of the best mean y value so far\n                X_min_mean = self.min_mean_X.copy()\n                # y_min_var: variance of the min y value so far\n                y_min_var = self.min_var_y.copy()\n                self.spot_writer.add_scalar(\"spot_y_min_var\", y_min_var, self.counter)\n                # y_min_mean: best mean y value so far (see above)\n                self.spot_writer.add_scalar(\"spot_y\", y_min_mean, self.counter)\n                # last n y values (noisy):\n                self.spot_writer.add_scalars(\"spot_y\", {f\"y_last_n{i}\": y_last_n[i] for i in range(self.fun_repeats)}, self.counter)\n                # X_min_mean: X value of the best mean y value so far (see above)\n                self.spot_writer.add_scalars(\"spot_X_noise\", {f\"X_min_mean{i}\": X_min_mean[i] for i in range(self.k)}, self.counter)\n            # get last value of self.X and convert to dict. take the values from self.var_name as keys:\n            X_last = self.X[-1].copy()\n            config = {self.var_name[i]: X_last[i] for i in range(self.k)}\n            # var_dict = assign_values(X, get_var_name(fun_control))\n            # config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n            # hyperparameters X and value y of the last configuration:\n            # see: https://github.com/pytorch/pytorch/issues/32651\n            # self.spot_writer.add_hparams(config, {\"spot_y\": y_last}, run_name=self.spot_tensorboard_path)\n            self.spot_writer.add_hparams(config, {\"hp_metric\": y_last})\n            self.spot_writer.flush()\n            if self.verbosity &gt; 0:\n                print(\"update_writer(): Done.\")\n        else:\n            if self.verbosity &gt; 0:\n                print(\"No spot_writer available.\")\n\n    def suggest_new_X(self) -&gt; np.array:\n        \"\"\"\n        Compute `n_points` new infill points in natural units.\n        These diffrent points are computed by the optimizer using increasing seed.\n        The optimizer searches in the ranges from `lower_j` to `upper_j`.\n        The method `infill()` is used as the objective function.\n\n        Returns:\n            (numpy.ndarray): `n_points` infill points in natural units, each of dim k\n\n        Note:\n            This is step (S-14a) in [bart21i].\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.spot import Spot\n                from spotpython.fun import Analytical\n                from spotpython.utils.init import fun_control_init\n                nn = 3\n                fun_sphere = Analytical().fun_sphere\n                fun_control = fun_control_init(\n                        lower = np.array([-1, -1]),\n                        upper = np.array([1, 1]),\n                        n_points=nn,\n                        )\n                S = Spot(\n                    fun=fun_sphere,\n                    fun_control=fun_control,\n                    )\n                S.X = S.design.scipy_lhd(\n                    S.design_control[\"init_size\"], lower=S.lower, upper=S.upper\n                )\n                print(f\"S.X: {S.X}\")\n                S.y = S.fun(S.X)\n                print(f\"S.y: {S.y}\")\n                S.fit_surrogate()\n                X0 = S.suggest_new_X()\n                print(f\"X0: {X0}\")\n                assert X0.size == S.n_points * S.k\n                assert X0.ndim == 2\n                assert X0.shape[0] == nn\n                assert X0.shape[1] == 2\n                spot_1.X: [[ 0.86352963  0.7892358 ]\n                            [-0.24407197 -0.83687436]\n                            [ 0.36481882  0.8375811 ]\n                            [ 0.415331    0.54468512]\n                            [-0.56395091 -0.77797854]\n                            [-0.90259409 -0.04899292]\n                            [-0.16484832  0.35724741]\n                            [ 0.05170659  0.07401196]\n                            [-0.78548145 -0.44638164]\n                            [ 0.64017497 -0.30363301]]\n                spot_1.y: [1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764\n                0.15480068 0.00815134 0.81623768 0.502017  ]\n                X0: [[0.00154544 0.003962  ]\n                    [0.00165526 0.00410847]\n                    [0.00165685 0.0039177 ]]\n        \"\"\"\n        # (S-14a) Optimization on the surrogate:\n        new_X = np.zeros([self.n_points, self.k], dtype=float)\n        optimizer_name = self.optimizer.__name__\n        optimizers = {\n            \"dual_annealing\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n            \"differential_evolution\": lambda: self.optimizer(\n                func=self.infill,\n                bounds=self.de_bounds,\n                maxiter=self.optimizer_control[\"max_iter\"],\n                seed=self.optimizer_control[\"seed\"],\n            ),\n            \"direct\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds, eps=1e-2),\n            \"shgo\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n            \"basinhopping\": lambda: self.optimizer(func=self.infill, x0=self.min_X, minimizer_kwargs={\"method\": \"Nelder-Mead\"}),\n            \"default\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n        }\n        for i in range(self.n_points):\n            self.optimizer_control[\"seed\"] = self.optimizer_control[\"seed\"] + i\n            result = optimizers.get(optimizer_name, optimizers[\"default\"])()\n            new_X[i][:] = result.x\n        return np.unique(new_X, axis=0)\n\n    def infill(self, x) -&gt; float:\n        \"\"\"\n        Infill (acquisition) function. Evaluates one point on the surrogate via `surrogate.predict(x.reshape(1,-1))`,\n        if `sklearn` surrogates are used or `surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)`\n        if the internal surrogate `kriging` is selected.\n        This method is passed to the optimizer in `suggest_new_X`, i.e., the optimizer is called via\n        `self.optimizer(func=self.infill)`.\n\n        Args:\n            x (array): point in natural units with shape `(1, dim)`.\n\n        Returns:\n            (numpy.ndarray): value based on infill criterion, e.g., `\"ei\"`. Shape `(1,)`.\n                The objective function value `y` that is used as a base value for the\n                infill criterion is calculated in natural units.\n\n        Note:\n            This is step (S-12) in [bart21i].\n        \"\"\"\n        # Reshape x to have shape (1, -1) because the predict method expects a 2D array\n        X = x.reshape(1, -1)\n        if isinstance(self.surrogate, Kriging) and getattr(self.surrogate, \"name\", None) == \"kriging\":\n            return self.surrogate.predict(X, return_val=self.infill_criterion)\n        else:\n            return self.surrogate.predict(X)\n\n    def plot_progress(self, show=True, log_x=False, log_y=False, filename=\"plot.png\", style=[\"ko\", \"k\", \"ro-\"], dpi=300, tkagg=False) -&gt; None:\n        \"\"\"Plot the progress of the hyperparameter tuning (optimization).\n\n        Args:\n            show (bool):\n                Show the plot.\n            log_x (bool):\n                Use logarithmic scale for x-axis.\n            log_y (bool):\n                Use logarithmic scale for y-axis.\n            filename (str):\n                Filename to save the plot.\n            style (list):\n                Style of the plot. Default: ['k', 'ro-'], i.e., the initial points are plotted as a black line\n                and the subsequent points as red dots connected by a line.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 7\n                # number of points\n                fun_evals = 10\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1])\n                    fun_evals=fun_evals,\n                    tolerance_x = np.sqrt(np.spacing(1))\n                    )\n                design_control=design_control_init(init_size=ni)\n                surrogate_control=surrogate_control_init(n_theta=3)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control\n                            design_control=design_control,\n                            surrogate_control=surrogate_control,)\n                S.run()\n                S.plot_progress(log_y=True)\n\n        \"\"\"\n        if tkagg:\n            matplotlib.use(\"TkAgg\")\n        fig = pylab.figure(figsize=(9, 6))\n        s_y = pd.Series(self.y)\n        s_c = s_y.cummin()\n        n_init = self.design_control[\"init_size\"] * self.design_control[\"repeats\"]\n\n        ax = fig.add_subplot(211)\n        if n_init &lt;= len(s_y):\n            ax.plot(\n                range(1, n_init + 1),\n                s_y[:n_init],\n                style[0],\n                range(1, n_init + 2),\n                [s_c[:n_init].min()] * (n_init + 1),\n                style[1],\n                range(n_init + 1, len(s_c) + 1),\n                s_c[n_init:],\n                style[2],\n            )\n        else:\n            # plot only s_y values:\n            ax.plot(range(1, len(s_y) + 1), s_y, style[0])\n            logger.warning(\"Less evaluations ({len(s_y)}) than initial design points ({n_init}).\")\n        ax.set_xlabel(\"Iteration\")\n        if log_x:\n            ax.set_xscale(\"log\")\n        if log_y:\n            ax.set_yscale(\"log\")\n        if filename is not None:\n            pylab.savefig(filename, dpi=dpi, bbox_inches=\"tight\")\n        if show:\n            pylab.show()\n\n    def plot_model(self, y_min=None, y_max=None) -&gt; None:\n        \"\"\"\n        Plot the model fit for 1-dim objective functions.\n\n        Args:\n            self (object):\n                spot object\n            y_min (float, optional):\n                y range, lower bound.\n            y_max (float, optional):\n                y range, upper bound.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                # number of initial points:\n                ni = 3\n                # number of points\n                fun_evals = 7\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1]),\n                    upper = np.array([1]),\n                    fun_evals=fun_evals,\n                    tolerance_x = np.sqrt(np.spacing(1))\n                    )\n                design_control=design_control_init(init_size=ni)\n\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control\n                S.run()\n                S.plot_model()\n        \"\"\"\n        if self.k == 1:\n            X_test = np.linspace(self.lower[0], self.upper[0], 100)\n            y_mo = self.fun(X=X_test.reshape(-1, 1), fun_control=self.fun_control)\n            # convert multi-objective values to single-objective values\n            y_test = self._mo2so(y_mo)\n            # Apply penalty for NA values works only on so values:\n            y_test = apply_penalty_NA(y_test, self.fun_control[\"penalty_NA\"], verbosity=self.verbosity)\n            if isinstance(self.surrogate, Kriging) and getattr(self.surrogate, \"name\", None) == \"kriging\":\n                y_hat = self.surrogate.predict(X_test[:, np.newaxis], return_val=\"y\")\n            else:\n                y_hat = self.surrogate.predict(X_test[:, np.newaxis])\n            plt.plot(X_test, y_hat, label=\"Model\")\n            plt.plot(X_test, y_test, label=\"True function\")\n            plt.scatter(self.X, self.y, edgecolor=\"b\", s=20, label=\"Samples\")\n            plt.scatter(self.X[-1], self.y[-1], edgecolor=\"r\", s=30, label=\"Last Sample\")\n            if self.noise:\n                plt.scatter(self.min_mean_X, self.min_mean_y, edgecolor=\"g\", s=30, label=\"Best Sample (mean)\")\n            else:\n                plt.scatter(self.min_X, self.min_y, edgecolor=\"g\", s=30, label=\"Best Sample\")\n            plt.xlabel(\"x\")\n            plt.ylabel(\"y\")\n            plt.xlim((self.lower[0], self.upper[0]))\n            if y_min is None:\n                y_min = min([min(self.y), min(y_test)])\n            if y_max is None:\n                y_max = max([max(self.y), max(y_test)])\n            plt.ylim((y_min, y_max))\n            plt.legend(loc=\"best\")\n            # plt.title(self.surrogate.__class__.__name__ + \". \" + str(self.counter) + \": \" + str(self.min_y))\n            if self.noise:\n                plt.title(\"fun_evals: \" + str(self.counter) + \". min_y (noise): \" + str(np.round(self.min_y, 6)) + \" min_mean_y: \" + str(np.round(self.min_mean_y, 6)))\n            else:\n                plt.title(\"fun_evals: \" + str(self.counter) + \". min_y: \" + str(np.round(self.min_y, 6)))\n            plt.show()\n\n    def print_results(self, print_screen=True, dict=None) -&gt; list[str]:\n        \"\"\"Print results from the run:\n            1. min y\n            2. min X\n            If `noise == True`, additionally the following values are printed:\n            3. min mean y\n            4. min mean X\n\n        Args:\n            print_screen (bool, optional):\n                print results to screen\n\n        Returns:\n            output (list):\n                list of results\n        \"\"\"\n        output = []\n        if print_screen:\n            print(f\"min y: {self.min_y}\")\n            if self.noise:\n                print(f\"min mean y: {self.min_mean_y}\")\n        if self.noise:\n            res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n        else:\n            res = self.to_all_dim(self.min_X.reshape(1, -1))\n        for i in range(res.shape[1]):\n            if self.all_var_name is None:\n                var_name = \"x\" + str(i)\n            else:\n                var_name = self.all_var_name[i]\n                var_type = self.all_var_type[i]\n                if var_type == \"factor\" and dict is not None:\n                    val = get_ith_hyperparameter_name_from_fun_control(fun_control=dict, key=var_name, i=int(res[0][i]))\n                else:\n                    val = res[0][i]\n            if print_screen:\n                print(var_name + \":\", val)\n            output.append([var_name, val])\n        return output\n\n    def get_tuned_hyperparameters(self, fun_control=None) -&gt; dict:\n        \"\"\"Return the tuned hyperparameter values from the run.\n        If `noise == True`, the mean values are returned.\n\n        Args:\n            fun_control (dict, optional):\n                fun_control dictionary\n\n        Returns:\n            (dict): dictionary of tuned hyperparameters.\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.utils.device import getDevice\n                from math import inf\n                from spotpython.utils.init import fun_control_init\n                import numpy as np\n                from spotpython.hyperparameters.values import set_control_key_value\n                from spotpython.data.diabetes import Diabetes\n                MAX_TIME = 1\n                FUN_EVALS = 10\n                INIT_SIZE = 5\n                WORKERS = 0\n                PREFIX=\"037\"\n                DEVICE = getDevice()\n                DEVICES = 1\n                TEST_SIZE = 0.4\n                TORCH_METRIC = \"mean_squared_error\"\n                dataset = Diabetes()\n                fun_control = fun_control_init(\n                    _L_in=10,\n                    _L_out=1,\n                    _torchmetric=TORCH_METRIC,\n                    PREFIX=PREFIX,\n                    TENSORBOARD_CLEAN=True,\n                    data_set=dataset,\n                    device=DEVICE,\n                    enable_progress_bar=False,\n                    fun_evals=FUN_EVALS,\n                    log_level=50,\n                    max_time=MAX_TIME,\n                    num_workers=WORKERS,\n                    show_progress=True,\n                    test_size=TEST_SIZE,\n                    tolerance_x=np.sqrt(np.spacing(1)),\n                    )\n                from spotpython.light.regression.netlightregression import NetLightRegression\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.hyperparameters.values import add_core_model_to_fun_control\n                add_core_model_to_fun_control(fun_control=fun_control,\n                                            core_model=NetLightRegression,\n                                            hyper_dict=LightHyperDict)\n                from spotpython.hyperparameters.values import set_control_hyperparameter_value\n                set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n                set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n                set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n                set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                                \"Adam\",\n                                \"RAdam\",\n                            ])\n                set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n                set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n                set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n                set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                                \"ReLU\",\n                                \"LeakyReLU\"\n                            ] )\n                from spotpython.utils.init import design_control_init, surrogate_control_init\n                design_control = design_control_init(init_size=INIT_SIZE)\n                surrogate_control = surrogate_control_init(method=\"regression\",\n                                                            n_theta=2)\n                from spotpython.fun.hyperlight import HyperLight\n                fun = HyperLight(log_level=50).fun\n                from spotpython.spot import spot\n                spot_tuner = spot.Spot(fun=fun,\n                                    fun_control=fun_control,\n                                    design_control=design_control,\n                                    surrogate_control=surrogate_control)\n                spot_tuner.run()\n                spot_tuner.get_tuned_hyperparameters()\n                    {'l1': 7.0,\n                    'epochs': 5.0,\n                    'batch_size': 4.0,\n                    'act_fn': 0.0,\n                    'optimizer': 0.0,\n                    'dropout_prob': 0.01,\n                    'lr_mult': 5.0,\n                    'patience': 3.0,\n                    'initialization': 1.0}\n\n        \"\"\"\n        output = []\n        if self.noise:\n            res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n        else:\n            res = self.to_all_dim(self.min_X.reshape(1, -1))\n        for i in range(res.shape[1]):\n            if self.all_var_name is None:\n                var_name = \"x\" + str(i)\n            else:\n                var_name = self.all_var_name[i]\n                var_type = self.all_var_type[i]\n                if var_type == \"factor\" and fun_control is not None:\n                    val = get_ith_hyperparameter_name_from_fun_control(fun_control=fun_control, key=var_name, i=int(res[0][i]))\n                else:\n                    val = res[0][i]\n            output.append([var_name, val])\n        # convert list to a dictionary\n        output = dict(output)\n        return output\n\n    def chg(self, x, y, z0, i, j) -&gt; list:\n        \"\"\"\n        Change the values of elements at indices `i` and `j` in the array `z0` to `x` and `y`, respectively.\n\n        Args:\n            x (int or float):\n                The new value for the element at index `i`.\n            y (int or float):\n                The new value for the element at index `j`.\n            z0 (list or numpy.ndarray):\n                The array to be modified.\n            i (int):\n                The index of the element to be changed to `x`.\n            j (int):\n                The index of the element to be changed to `y`.\n\n        Returns:\n            (list) or (numpy.ndarray): The modified array.\n\n        Examples:\n                &gt;&gt;&gt; import numpy as np\n                    from spotpython.fun.objectivefunctions import Analytical\n                    from spotpython.spot import spot\n                    from spotpython.utils.init import (\n                        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                    fun = analytical().fun_sphere\n                    fun_control = fun_control_init(\n                        lower = np.array([-1]),\n                        upper = np.array([1]),\n                    )\n                    S = spot.Spot(fun=fun,\n                                func_control=fun_control)\n                    z0 = [1, 2, 3]\n                    print(f\"Before: {z0}\")\n                    new_val_1 = 4\n                    new_val_2 = 5\n                    index_1 = 0\n                    index_2 = 2\n                    S.chg(x=new_val_1, y=new_val_2, z0=z0, i=index_1, j=index_2)\n                    print(f\"After: {z0}\")\n                    Before: [1, 2, 3]\n                    After: [4, 2, 5]\n        \"\"\"\n        z0[i] = x\n        z0[j] = y\n        return z0\n\n    def process_z00(self, z00, use_min=True) -&gt; list:\n        \"\"\"Process each entry in the `z00` array according to the corresponding type\n        in the `self.var_type` list.\n        Specifically, if the type is \"float\", the function will calculate the mean of the two `z00` values.\n        If the type is not \"float\", the function will retrun the maximum of the two `z00` values.\n\n        Args:\n            z00 (numpy.ndarray):\n                Array of values to process.\n            use_min (bool):\n                If `True`, the minimum value is returned. If `False`, the maximum value is returned.\n\n        Returns:\n            (list): Processed values.\n\n        Examples:\n            from spotpython.spot import spot\n            import numpy as np\n            import random\n            z00 = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n            spot.var_type = [\"float\", \"int\", \"int\", \"float\"]\n            spot.process_z00(z00)\n            [3, 6, 7, 6]\n\n        \"\"\"\n        result = []\n        for i in range(len(self.var_type)):\n            if self.var_type[i] == \"float\":\n                mean_value = np.mean(z00[:, i])\n                result.append(mean_value)\n            else:  # var_type[i] == 'int'\n                if use_min:\n                    min_value = min(z00[:, i])\n                    result.append(min_value)\n                else:\n                    max_value = max(z00[:, i])\n                    result.append(max_value)\n        return result\n\n    def plot_contour(\n        self,\n        i=0,\n        j=1,\n        min_z=None,\n        max_z=None,\n        show=True,\n        filename=None,\n        n_grid=50,\n        contour_levels=10,\n        dpi=200,\n        title=\"\",\n        figsize=(12, 6),\n        use_min=False,\n        use_max=True,\n        tkagg=False,\n    ) -&gt; None:\n        \"\"\"Plot the contour of any dimension.\"\"\"\n\n        def generate_mesh_grid(lower, upper, grid_points):\n            \"\"\"Generate a mesh grid for the given range.\"\"\"\n            x = np.linspace(lower[i], upper[i], num=grid_points)\n            y = np.linspace(lower[j], upper[j], num=grid_points)\n            return np.meshgrid(x, y), x, y\n\n        def validate_types(var_type, lower, upper):\n            \"\"\"Validate if the dimensions of var_type, lower, and upper are the same.\"\"\"\n            if var_type is not None:\n                if len(var_type) != len(lower) or len(var_type) != len(upper):\n                    raise ValueError(\"The dimensions of var_type, lower, and upper must be the same.\")\n\n        def setup_plot():\n            \"\"\"Setup the plot with specified figure size.\"\"\"\n            fig = pylab.figure(figsize=figsize)\n            return fig\n\n        def predict_contour_values(X, Y, z0):\n            \"\"\"Predict contour values based on the surrogate model.\"\"\"\n            grid_points = np.c_[np.ravel(X), np.ravel(Y)]\n            predictions = []\n\n            for x, y in grid_points:\n                adjusted_z0 = self.chg(x, y, z0.copy(), i, j)\n                prediction = self.surrogate.predict(np.array([adjusted_z0]))\n                predictions.append(prediction[0])\n\n            Z = np.array(predictions).reshape(X.shape)\n            return Z\n\n        def plot_contour_subplots(X, Y, Z, ax, min_z, max_z, contour_levels):\n            \"\"\"Plot the contour and 3D surface subplots.\"\"\"\n            contour = ax.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\", vmin=min_z, vmax=max_z)\n            pylab.colorbar(contour, ax=ax)\n\n        if tkagg:\n            matplotlib.use(\"TkAgg\")\n        fig = setup_plot()\n\n        (X, Y), x, y = generate_mesh_grid(self.lower, self.upper, n_grid)\n        validate_types(self.var_type, self.lower, self.upper)\n\n        z00 = np.array([self.lower, self.upper])\n        Z_list, X_list, Y_list = [], [], []\n\n        if use_min:\n            z0_min = self.process_z00(z00, use_min=True)\n            Z_min = predict_contour_values(X, Y, z0_min)\n            Z_list.append(Z_min)\n            X_list.append(X)\n            Y_list.append(Y)\n\n        if use_max:\n            z0_max = self.process_z00(z00, use_min=False)\n            Z_max = predict_contour_values(X, Y, z0_max)\n            Z_list.append(Z_max)\n            X_list.append(X)\n            Y_list.append(Y)\n\n        if Z_list:  # Ensure that there is at least one Z to stack\n            Z_combined = np.vstack(Z_list)\n            X_combined = np.vstack(X_list)\n            Y_combined = np.vstack(Y_list)\n\n        if min_z is None:\n            min_z = np.min(Z_combined)\n        if max_z is None:\n            max_z = np.max(Z_combined)\n\n        ax_contour = fig.add_subplot(221)\n        plot_contour_subplots(X_combined, Y_combined, Z_combined, ax_contour, min_z, max_z, contour_levels)\n\n        if self.var_name is None:\n            ax_contour.set_xlabel(f\"x{i}\")\n            ax_contour.set_ylabel(f\"x{j}\")\n        else:\n            ax_contour.set_xlabel(f\"x{i}: {self.var_name[i]}\")\n            ax_contour.set_ylabel(f\"x{j}: {self.var_name[j]}\")\n\n        ax_3d = fig.add_subplot(222, projection=\"3d\")\n        ax_3d.plot_surface(X_combined, Y_combined, Z_combined, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\", vmin=min_z, vmax=max_z)\n\n        if self.var_name is None:\n            ax_3d.set_xlabel(f\"x{i}\")\n            ax_3d.set_ylabel(f\"x{j}\")\n        else:\n            ax_3d.set_xlabel(f\"x{i}: {self.var_name[i]}\")\n            ax_3d.set_ylabel(f\"x{j}: {self.var_name[j]}\")\n\n        plt.title(title)\n\n        if filename:\n            pylab.savefig(filename, bbox_inches=\"tight\", dpi=dpi, pad_inches=0)\n\n        if show:\n            pylab.show()\n\n    def plot_important_hyperparameter_contour(\n        self,\n        threshold=0.0,\n        filename=None,\n        show=True,\n        max_imp=None,\n        title=\"\",\n        scale_global=False,\n        n_grid=50,\n        contour_levels=10,\n        dpi=200,\n        use_min=False,\n        use_max=True,\n        tkagg=False,\n    ) -&gt; None:\n        \"\"\"\n        Plot the contour of important hyperparameters.\n        Calls `plot_contour` for each pair of important hyperparameters.\n        Importance can be specified by the threshold.\n\n        Args:\n            threshold (float):\n                threshold for the importance. Not used any more in spotpython &gt;= 0.13.2.\n            filename (str):\n                filename of the plot\n            show (bool):\n                show the plot. Default is `True`.\n            max_imp (int):\n                maximum number of important hyperparameters. If there are more important hyperparameters\n                than `max_imp`, only the max_imp important ones are selected.\n            title (str):\n                title of the plots\n            scale_global (bool):\n                scale the z-axis globally. Default is `False`.\n            n_grid (int):\n                number of grid points. Default is 50.\n            contour_levels (int):\n                number of contour levels. Default is 10.\n            dpi (int):\n                dpi of the plot. Default is 200.\n            use_min (bool):\n                Use the minimum value for determing the hidden dimensions in the plot for categorical and\n                integer parameters.\n                In 3d-plots, only two variables can be independent. The remaining input variables are set\n                to their minimum value.\n                Default is `False`.\n                If use_min and use_max are both `True`, both values are used.\n            use_max (bool):\n                Use the minimum value for determing the hidden dimensions in the plot for categorical and\n                integer parameters.\n                In 3d-plots, only two variables can be independent. The remaining input variables are set\n                to their minimum value.\n                Default is `True`.\n                If use_min and use_max are both `True`, both values are used.\n\n        Returns:\n            None.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 5\n                # number of points\n                fun_evals = 10\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1, -1]),\n                    upper = np.array([1, 1, 1]),\n                    fun_evals=fun_evals,\n                    tolerance_x = np.sqrt(np.spacing(1))\n                    )\n                design_control=design_control_init(init_size=ni)\n                surrogate_control=surrogate_control_init(n_theta=3)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,\n                            surrogate_control=surrogate_control,)\n                S.run()\n                S.plot_important_hyperparameter_contour()\n\n        \"\"\"\n        impo = self.print_importance(threshold=threshold, print_screen=True)\n        indices = sort_by_kth_and_return_indices(array=impo, k=1)\n        # take the first max_imp values from the indices array\n        if max_imp is not None:\n            indices = indices[:max_imp]\n        if scale_global:\n            min_z = min(self.y)\n            max_z = max(self.y)\n        else:\n            min_z = None\n            max_z = None\n        for i in indices:\n            for j in indices:\n                if j &gt; i:\n                    if filename is not None:\n                        filename_full = filename + \"_contour_\" + str(i) + \"_\" + str(j) + \".png\"\n                    else:\n                        filename_full = None\n                    self.plot_contour(\n                        i=i,\n                        j=j,\n                        min_z=min_z,\n                        max_z=max_z,\n                        filename=filename_full,\n                        show=show,\n                        title=title,\n                        n_grid=n_grid,\n                        contour_levels=contour_levels,\n                        dpi=dpi,\n                        use_max=use_max,\n                        use_min=use_min,\n                        tkagg=tkagg,\n                    )\n\n    def get_importance(self) -&gt; list:\n        \"\"\"Get importance of each variable and return the results as a list.\n\n        Returns:\n            output (list):\n                list of results. If the surrogate has more than one theta values,\n                the importance is calculated. Otherwise, a list of zeros is returned.\n\n        \"\"\"\n        if self.surrogate.n_theta &gt; 1 and self.var_name is not None:\n            output = [0] * len(self.all_var_name)\n            theta = np.power(10, self.surrogate.theta)\n            imp = 100 * theta / np.max(theta)\n            ind = find_indices(A=self.var_name, B=self.all_var_name)\n            j = 0\n            for i in ind:\n                output[i] = imp[j]\n                j = j + 1\n            return output\n        else:\n            print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n            # return a list of zeros of length len(all_var_name)\n            return [0] * len(self.all_var_name)\n\n    def print_importance(self, threshold=0.1, print_screen=True) -&gt; list:\n        \"\"\"Print importance of each variable and return the results as a list.\n\n        Args:\n            threshold (float):\n                threshold for printing\n            print_screen (boolean):\n                if `True`, values are also printed on the screen. Default is `True`.\n\n        Returns:\n            output (list):\n                list of results\n        \"\"\"\n        output = []\n        if self.surrogate.n_theta &gt; 1:\n            theta = np.power(10, self.surrogate.theta)\n            imp = 100 * theta / np.max(theta)\n            # imp = imp[imp &gt;= threshold]\n            if self.var_name is None:\n                for i in range(len(imp)):\n                    if imp[i] &gt;= threshold:\n                        if print_screen:\n                            print(\"x\", i, \": \", imp[i])\n                        output.append(\"x\" + str(i) + \": \" + str(imp[i]))\n            else:\n                var_name = [self.var_name[i] for i in range(len(imp))]\n                for i in range(len(imp)):\n                    if imp[i] &gt;= threshold:\n                        if print_screen:\n                            print(var_name[i] + \": \", imp[i])\n                    output.append([var_name[i], imp[i]])\n        else:\n            print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n        return output\n\n    def plot_importance(self, threshold=0.1, filename=None, dpi=300, show=True, tkagg=False) -&gt; None:\n        \"\"\"Plot the importance of each variable.\n\n        Args:\n            threshold (float):\n                The threshold of the importance.\n            filename (str):\n                The filename of the plot.\n            dpi (int):\n                The dpi of the plot.\n            show (bool):\n                Show the plot. Default is `True`.\n\n        Returns:\n            None\n        \"\"\"\n        if self.surrogate.n_theta &gt; 1:\n            if tkagg:\n                matplotlib.use(\"TkAgg\")\n            theta = np.power(10, self.surrogate.theta)\n            imp = 100 * theta / np.max(theta)\n            idx = np.where(imp &gt; threshold)[0]\n            if self.var_name is None:\n                plt.bar(range(len(imp[idx])), imp[idx])\n                plt.xticks(range(len(imp[idx])), [\"x\" + str(i) for i in idx])\n            else:\n                var_name = [self.var_name[i] for i in idx]\n                plt.bar(range(len(imp[idx])), imp[idx])\n                plt.xticks(range(len(imp[idx])), var_name)\n            if filename is not None:\n                plt.savefig(filename, bbox_inches=\"tight\", dpi=dpi)\n            if show:\n                plt.show()\n\n    def parallel_plot(self, show=False) -&gt; go.Figure:\n        \"\"\"\n        Parallel plot.\n\n        Args:\n            self (object):\n                Spot object\n            show (bool):\n                show the plot. Default is `False`.\n\n        Returns:\n                fig (plotly.graph_objects.Figure): figure object\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                    )\n                # number of initial points:\n                ni = 5\n                # number of points\n                fun_evals = 10\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1, -1, -1]),\n                    upper = np.array([1, 1, 1]),\n                    fun_evals=fun_evals,\n                    tolerance_x = np.sqrt(np.spacing(1))\n                    )\n                design_control=design_control_init(init_size=ni)\n                surrogate_control=surrogate_control_init(n_theta=3)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control,\n                            surrogate_control=surrogate_control,)\n                S.run()\n                S.parallel_plot()\n\n        \"\"\"\n        X = self.X\n        y = self.y\n        df = pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=self.var_name + [\"y\"])\n\n        fig = go.Figure(\n            data=go.Parcoords(\n                line=dict(color=df[\"y\"], colorscale=\"Jet\", showscale=True, cmin=min(df[\"y\"]), cmax=max(df[\"y\"])),\n                dimensions=list([dict(range=[min(df.iloc[:, i]), max(df.iloc[:, i])], label=df.columns[i], values=df.iloc[:, i]) for i in range(len(df.columns) - 1)]),\n            )\n        )\n        if show:\n            fig.show()\n        return fig\n\n    def _reattach_logger_handlers(self) -&gt; None:\n        \"\"\"\n        Reattach handlers to the logger after unpickling.\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        # configure the handler and formatter as needed\n        py_handler = logging.FileHandler(f\"{__name__}.log\", mode=\"w\")\n        py_formatter = logging.Formatter(\"%(name)s %(asctime)s %(levelname)s %(message)s\")\n        # add formatter to the handler\n        py_handler.setFormatter(py_formatter)\n        # add handler to the logger\n        logger.addHandler(py_handler)\n\n    def _de_serialize_dicts(self) -&gt; tuple:\n        \"\"\"\n        Deserialize the spot object and return the dictionaries.\n\n        Args:\n            self (object):\n                Spot object\n\n        Returns:\n            (tuple):\n                tuple containing dictionaries of spot object:\n                fun_control (dict): function control dictionary,\n                design_control (dict): design control dictionary,\n                optimizer_control (dict): optimizer control dictionary,\n                spot_tuner_control (dict): spot tuner control dictionary, and\n                surrogate_control (dict): surrogate control dictionary\n        \"\"\"\n        spot_tuner = copy.deepcopy(self)\n        spot_tuner_control = vars(spot_tuner)\n\n        fun_control = copy.deepcopy(spot_tuner_control[\"fun_control\"])\n        design_control = copy.deepcopy(spot_tuner_control[\"design_control\"])\n        optimizer_control = copy.deepcopy(spot_tuner_control[\"optimizer_control\"])\n        surrogate_control = copy.deepcopy(spot_tuner_control[\"surrogate_control\"])\n\n        # remove keys from the dictionaries:\n        spot_tuner_control.pop(\"fun_control\", None)\n        spot_tuner_control.pop(\"design_control\", None)\n        spot_tuner_control.pop(\"optimizer_control\", None)\n        spot_tuner_control.pop(\"surrogate_control\", None)\n        spot_tuner_control.pop(\"spot_writer\", None)\n        spot_tuner_control.pop(\"design\", None)\n        spot_tuner_control.pop(\"fun\", None)\n        spot_tuner_control.pop(\"optimizer\", None)\n        spot_tuner_control.pop(\"rng\", None)\n        spot_tuner_control.pop(\"surrogate\", None)\n\n        fun_control.pop(\"core_model\", None)\n        fun_control.pop(\"metric_river\", None)\n        fun_control.pop(\"metric_sklearn\", None)\n        fun_control.pop(\"metric_torch\", None)\n        fun_control.pop(\"prep_model\", None)\n        fun_control.pop(\"spot_writer\", None)\n        fun_control.pop(\"test\", None)\n        fun_control.pop(\"train\", None)\n\n        surrogate_control.pop(\"model_optimizer\", None)\n        surrogate_control.pop(\"surrogate\", None)\n\n        return (fun_control, design_control, optimizer_control, spot_tuner_control, surrogate_control)\n\n    def _write_db_dict(self) -&gt; None:\n        \"\"\"Writes a dictionary with the experiment parameters to the json file spotpython_db.json.\n\n        Args:\n            self (object): Spot object\n\n        Returns:\n            (NoneType): None\n\n        \"\"\"\n        # get the time in seconds from 1.1.1970 and convert the time to a string\n        t_str = str(time.time())\n        ident = str(self.fun_control[\"PREFIX\"]) + \"_\" + t_str\n\n        (\n            fun_control,\n            design_control,\n            optimizer_control,\n            spot_tuner_control,\n            surrogate_control,\n        ) = self._de_serialize_dicts()\n        print(\"\\n**\")\n        print(\"The following dictionaries are written to the json file spotpython_db.json:\")\n        print(\"fun_control:\")\n        pprint.pprint(fun_control)\n\n        # Iterate over a list of the keys to avoid modifying the dictionary during iteration\n        for key in list(fun_control.keys()):\n            if not isinstance(fun_control[key], (int, float, str, list, dict)):\n                # remove the key from the dictionary\n                print(f\"Removing non-serializable key: {key}\")\n                fun_control.pop(key)\n\n        print(\"fun_control after removing non-serializabel keys:\")\n        pprint.pprint(fun_control)\n        pprint.pprint(fun_control)\n        print(\"design_control:\")\n        pprint.pprint(design_control)\n        print(\"optimizer_control:\")\n        pprint.pprint(optimizer_control)\n        print(\"spot_tuner_control:\")\n        pprint.pprint(spot_tuner_control)\n        print(\"surrogate_control:\")\n        pprint.pprint(surrogate_control)\n        #\n        # Generate a description of the results:\n        # if spot_tuner_control['min_y'] exists:\n        try:\n            result = f\"\"\"\n                      Results for {ident}: Finally, the best value is {spot_tuner_control['min_y']}\n                      at {spot_tuner_control['min_X']}.\"\"\"\n            #\n            db_dict = {\n                \"data\": {\n                    \"id\": str(ident),\n                    \"result\": result,\n                    \"fun_control\": fun_control,\n                    \"design_control\": design_control,\n                    \"surrogate_control\": surrogate_control,\n                    \"optimizer_control\": optimizer_control,\n                    \"spot_tuner_control\": spot_tuner_control,\n                }\n            }\n            # Check if the directory \"db_dicts\" exists.\n            if not os.path.exists(\"db_dicts\"):\n                try:\n                    os.makedirs(\"db_dicts\")\n                except OSError as e:\n                    raise Exception(f\"Error creating directory: {e}\")\n\n            if os.path.exists(\"db_dicts\"):\n                try:\n                    # Open the file in append mode to add each new dict as a new line\n                    with open(\"db_dicts/\" + self.fun_control[\"db_dict_name\"], \"a\") as f:\n                        # Using json.dumps to convert the dict to a JSON formatted string\n                        # We then write this string to the file followed by a newline character\n                        # This ensures that each dict is on its own line, conforming to the JSON Lines format\n                        f.write(json.dumps(db_dict, cls=NumpyEncoder) + \"\\n\")\n                except OSError as e:\n                    raise Exception(f\"Error writing to file: {e}\")\n        except KeyError:\n            print(\"No results to write.\")\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.chg","title":"<code>chg(x, y, z0, i, j)</code>","text":"<p>Change the values of elements at indices <code>i</code> and <code>j</code> in the array <code>z0</code> to <code>x</code> and <code>y</code>, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int or float</code> <p>The new value for the element at index <code>i</code>.</p> required <code>y</code> <code>int or float</code> <p>The new value for the element at index <code>j</code>.</p> required <code>z0</code> <code>list or ndarray</code> <p>The array to be modified.</p> required <code>i</code> <code>int</code> <p>The index of the element to be changed to <code>x</code>.</p> required <code>j</code> <code>int</code> <p>The index of the element to be changed to <code>y</code>.</p> required <p>Returns:</p> Type Description <code>list) or (numpy.ndarray</code> <p>The modified array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n    )\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1]),\n        upper = np.array([1]),\n    )\n    S = spot.Spot(fun=fun,\n                func_control=fun_control)\n    z0 = [1, 2, 3]\n    print(f\"Before: {z0}\")\n    new_val_1 = 4\n    new_val_2 = 5\n    index_1 = 0\n    index_2 = 2\n    S.chg(x=new_val_1, y=new_val_2, z0=z0, i=index_1, j=index_2)\n    print(f\"After: {z0}\")\n    Before: [1, 2, 3]\n    After: [4, 2, 5]\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def chg(self, x, y, z0, i, j) -&gt; list:\n    \"\"\"\n    Change the values of elements at indices `i` and `j` in the array `z0` to `x` and `y`, respectively.\n\n    Args:\n        x (int or float):\n            The new value for the element at index `i`.\n        y (int or float):\n            The new value for the element at index `j`.\n        z0 (list or numpy.ndarray):\n            The array to be modified.\n        i (int):\n            The index of the element to be changed to `x`.\n        j (int):\n            The index of the element to be changed to `y`.\n\n    Returns:\n        (list) or (numpy.ndarray): The modified array.\n\n    Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import (\n                    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n                fun = analytical().fun_sphere\n                fun_control = fun_control_init(\n                    lower = np.array([-1]),\n                    upper = np.array([1]),\n                )\n                S = spot.Spot(fun=fun,\n                            func_control=fun_control)\n                z0 = [1, 2, 3]\n                print(f\"Before: {z0}\")\n                new_val_1 = 4\n                new_val_2 = 5\n                index_1 = 0\n                index_2 = 2\n                S.chg(x=new_val_1, y=new_val_2, z0=z0, i=index_1, j=index_2)\n                print(f\"After: {z0}\")\n                Before: [1, 2, 3]\n                After: [4, 2, 5]\n    \"\"\"\n    z0[i] = x\n    z0[j] = y\n    return z0\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.evaluate_initial_design","title":"<code>evaluate_initial_design()</code>","text":"<p>Evaluate the initial design.</p> <p>This method evaluates the initial design matrix <code>X0</code> by applying the objective function and handling NaN values. The results are stored in <code>self.X</code> and <code>self.y</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the resulting design matrix has zero rows after removing NaN values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import Spot\n    from spotpython.utils.init import fun_control_init\n    fun_control = fun_control_init(\n        lower=np.array([-1, -1]),\n        upper=np.array([1, 1])\n    )\n    fun = Analytical().fun_sphere\n    S = Spot(fun=fun, fun_control=fun_control)\n    X0 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    S.initialize_design_matrix(X_start=X0)\n    S.evaluate_initial_design()\n    print(f\"S.X: {S.X}\")\n    print(f\"S.y: {S.y}\")\n        S.X: [[ 0.          0.        ]\n        [ 0.          1.        ]\n        [ 1.          0.        ]\n        [ 1.          1.        ]\n        [ 0.86352963  0.7892358 ]\n        [-0.24407197 -0.83687436]\n        [ 0.36481882  0.8375811 ]\n        [ 0.415331    0.54468512]\n        [-0.56395091 -0.77797854]\n        [-0.90259409 -0.04899292]\n        [-0.16484832  0.35724741]\n        [ 0.05170659  0.07401196]\n        [-0.78548145 -0.44638164]\n        [ 0.64017497 -0.30363301]]\n        S.y: [0.         1.         1.         2.         1.36857656 0.75992983\n        0.83463487 0.46918172 0.92329124 0.8170764  0.15480068 0.00815134\n        0.81623768 0.502017  ]\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def evaluate_initial_design(self) -&gt; None:\n    \"\"\"\n    Evaluate the initial design.\n\n    This method evaluates the initial design matrix `X0` by applying the objective function\n    and handling NaN values. The results are stored in `self.X` and `self.y`.\n\n    Raises:\n        ValueError: If the resulting design matrix has zero rows after removing NaN values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import Spot\n            from spotpython.utils.init import fun_control_init\n            fun_control = fun_control_init(\n                lower=np.array([-1, -1]),\n                upper=np.array([1, 1])\n            )\n            fun = Analytical().fun_sphere\n            S = Spot(fun=fun, fun_control=fun_control)\n            X0 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n            S.initialize_design_matrix(X_start=X0)\n            S.evaluate_initial_design()\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n                S.X: [[ 0.          0.        ]\n                [ 0.          1.        ]\n                [ 1.          0.        ]\n                [ 1.          1.        ]\n                [ 0.86352963  0.7892358 ]\n                [-0.24407197 -0.83687436]\n                [ 0.36481882  0.8375811 ]\n                [ 0.415331    0.54468512]\n                [-0.56395091 -0.77797854]\n                [-0.90259409 -0.04899292]\n                [-0.16484832  0.35724741]\n                [ 0.05170659  0.07401196]\n                [-0.78548145 -0.44638164]\n                [ 0.64017497 -0.30363301]]\n                S.y: [0.         1.         1.         2.         1.36857656 0.75992983\n                0.83463487 0.46918172 0.92329124 0.8170764  0.15480068 0.00815134\n                0.81623768 0.502017  ]\n    \"\"\"\n    # check that self.X has at leat one row and is not None\n    if self.X is None or self.X.shape[0] == 0:\n        raise ValueError(\"The design matrix has zero rows. Check design_control['init_size'] or X_start.\")\n\n    X_all = self.to_all_dim_if_needed(self.X)\n    logger.debug(\"In Spot() evaluate_initial_design(), before calling self.fun: X_all: %s\", X_all)\n    logger.debug(\"In Spot() evaluate_initial_design(), before calling self.fun: fun_control: %s\", self.fun_control)\n\n    y_mo = self.fun(X=X_all, fun_control=self.fun_control)\n    if self.verbosity &gt; 1:\n        print(f\"y_mo as returned from fun(): {y_mo}\")\n        print(f\"y_mo shape: {y_mo.shape}\")\n\n    #  Convert multi-objective values to single-objective values\n    # TODO: Store y_mo in self.y_mo (append new values)\n    self.y = self._mo2so(y_mo)\n    self.y = apply_penalty_NA(self.y, self.fun_control[\"penalty_NA\"], verbosity=self.verbosity)\n    logger.debug(\"In Spot() evaluate_initial_design(), after calling self.fun: self.y: %s\", self.y)\n\n    # TODO: Error if only nan values are returned\n    logger.debug(\"New y value: %s\", self.y)\n\n    self.counter = self.y.size\n    self.X, self.y = remove_nan(self.X, self.y, stop_on_zero_return=True)\n\n    if self.X.shape[0] == 0:\n        raise ValueError(\"The resulting design matrix has zero rows after removing NaN values.\")\n\n    logger.debug(\"In Spot() evaluate_initial_design(), final X val, after remove nan: self.X: %s\", self.X)\n    logger.debug(\"In Spot() evaluate_initial_design(), final y val, after remove nan: self.y: %s\", self.y)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.fit_surrogate","title":"<code>fit_surrogate()</code>","text":"<p>Fit surrogate model. The surrogate model is fitted to the data stored in <code>self.X</code> and <code>self.y</code>. It uses the generic <code>fit()</code> method of the surrogate model <code>surrogate</code>. The default surrogate model is an instance from spotpython\u2019s <code>Kriging</code> class. If <code>show_models</code> is <code>True</code>, the model is plotted. If the number of points is greater than <code>max_surrogate_points</code>, the surrogate model is fitted to a subset of the data points. The subset is selected using the <code>select_distant_points()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Attributes:</p> Name Type Description <code>self.surrogate</code> <code>object</code> <p>surrogate model</p> <code>self.X</code> <code>ndarray</code> <p>design points</p> <code>self.y</code> <code>ndarray</code> <p>function values</p> <code>self.max_surrogate_points</code> <code>int</code> <p>maximum number of points to fit the surrogate model</p> <code>self.show_models</code> <code>bool</code> <p>if True, the model is plotted</p> Note <ul> <li>As shown in https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/ other surrogate models can be used as well.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n    )\n    # number of initial points:\n    ni = 0\n    X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1])\n        )\n    design_control=design_control_init(init_size=ni)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    S.initialize_design(X_start=X_start)\n    S.update_stats()\n    S.fit_surrogate()\n    S.surrogate.predict(np.array([[0, 0]]))\n        array([1.49011612e-08])\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def fit_surrogate(self) -&gt; None:\n    \"\"\"\n    Fit surrogate model. The surrogate model\n    is fitted to the data stored in `self.X` and `self.y`.\n    It uses the generic `fit()` method of the\n    surrogate model `surrogate`. The default surrogate model is\n    an instance from spotpython's `Kriging` class.\n    If `show_models` is `True`, the model is plotted.\n    If the number of points is greater than `max_surrogate_points`,\n    the surrogate model is fitted to a subset of the data points.\n    The subset is selected using the `select_distant_points()` function.\n\n    Args:\n        self (object): Spot object\n\n    Returns:\n        (NoneType): None\n\n    Attributes:\n        self.surrogate (object):\n            surrogate model\n        self.X (numpy.ndarray):\n            design points\n        self.y (numpy.ndarray):\n            function values\n        self.max_surrogate_points (int):\n            maximum number of points to fit the surrogate model\n        self.show_models (bool):\n            if True, the model is plotted\n\n    Note:\n        * As shown in https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/\n        other surrogate models can be used as well.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n            fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n            )\n            # number of initial points:\n            ni = 0\n            X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1])\n                )\n            design_control=design_control_init(init_size=ni)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,)\n            S.initialize_design(X_start=X_start)\n            S.update_stats()\n            S.fit_surrogate()\n            S.surrogate.predict(np.array([[0, 0]]))\n                array([1.49011612e-08])\n\n    \"\"\"\n    logger.debug(\"In fit_surrogate(): self.X: %s\", self.X)\n    logger.debug(\"In fit_surrogate(): self.y: %s\", self.y)\n    logger.debug(\"In fit_surrogate(): self.X.shape: %s\", self.X.shape)\n    logger.debug(\"In fit_surrogate(): self.y.shape: %s\", self.y.shape)\n    X_points = self.X.shape[0]\n    y_points = self.y.shape[0]\n    if X_points == y_points:\n        if X_points &gt; self.max_surrogate_points:\n            logger.info(\"Selecting distant points for surrogate fitting.\")\n            X_S, y_S = select_distant_points(X=self.X, y=self.y, k=self.max_surrogate_points)\n        else:\n            X_S = self.X\n            y_S = self.y\n        self.surrogate.fit(X_S, y_S)\n    else:\n        logger.warning(\"X and y have different sizes. Surrogate not fitted.\")\n    if self.show_models:\n        self.plot_model()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.generate_design","title":"<code>generate_design(size, repeats, lower, upper)</code>","text":"<p>Generate a design with <code>size</code> points in the interval [lower, upper].</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>number of points</p> required <code>repeats</code> <code>int</code> <p>number of repeats</p> required <code>lower</code> <code>ndarray</code> <p>lower bound of the design space</p> required <code>upper</code> <code>ndarray</code> <p>upper bound of the design space</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>design points</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.spot import spot\n    from spotpython.utils.init import design_control_init\n    from spotpython.fun.objectivefunctions import Analytical\n    design_control = design_control_init(init_size=3)\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1]),\n        fun_evals=fun_evals,\n        tolerance_x = np.sqrt(np.spacing(1))\n        )\n    S = spot.Spot(fun = analytical().fun_sphere,\n                fun_control = fun_control,\n                design_control = design_control)\n    X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n    assert X.shape[0] == 3\n    assert X.shape[1] == 2\n    print(X)\n        array([[77.25493789,  0.31539299],\n        [59.32133757,  0.93854273],\n        [27.4698033 ,  0.3959685 ]])\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def generate_design(self, size, repeats, lower, upper) -&gt; np.array:\n    \"\"\"Generate a design with `size` points in the interval [lower, upper].\n\n    Args:\n        size (int): number of points\n        repeats (int): number of repeats\n        lower (numpy.ndarray): lower bound of the design space\n        upper (numpy.ndarray): upper bound of the design space\n\n    Returns:\n        (numpy.ndarray): design points\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.spot import spot\n            from spotpython.utils.init import design_control_init\n            from spotpython.fun.objectivefunctions import Analytical\n            design_control = design_control_init(init_size=3)\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1]),\n                fun_evals=fun_evals,\n                tolerance_x = np.sqrt(np.spacing(1))\n                )\n            S = spot.Spot(fun = analytical().fun_sphere,\n                        fun_control = fun_control,\n                        design_control = design_control)\n            X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n            assert X.shape[0] == 3\n            assert X.shape[1] == 2\n            print(X)\n                array([[77.25493789,  0.31539299],\n                [59.32133757,  0.93854273],\n                [27.4698033 ,  0.3959685 ]])\n    \"\"\"\n    return self.design.scipy_lhd(n=size, repeats=repeats, lower=lower, upper=upper)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.generate_random_point","title":"<code>generate_random_point()</code>","text":"<p>Generate a random point in the design space.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>tuple containing: X0 (numpy.ndarray): random point in the design space y0 (numpy.ndarray): function value at X</p> Notes <p>If the evaluation fails, the function returns arrays of shape[0] == 0.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1])\n        )\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                )\n    X0, y0 = S.generate_random_point()\n    print(f\"X0: {X0}\")\n    print(f\"y0: {y0}\")\n    assert X0.size == 2\n    assert y0.size == 1\n    assert np.all(X0 &gt;= S.lower)\n    assert np.all(X0 &lt;= S.upper)\n    assert y0 &gt;= 0\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def generate_random_point(self):\n    \"\"\"Generate a random point in the design space.\n\n    Returns:\n        (tuple): tuple containing:\n            X0 (numpy.ndarray): random point in the design space\n            y0 (numpy.ndarray): function value at X\n\n    Notes:\n        If the evaluation fails, the function returns arrays of shape[0] == 0.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1])\n                )\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        )\n            X0, y0 = S.generate_random_point()\n            print(f\"X0: {X0}\")\n            print(f\"y0: {y0}\")\n            assert X0.size == 2\n            assert y0.size == 1\n            assert np.all(X0 &gt;= S.lower)\n            assert np.all(X0 &lt;= S.upper)\n            assert y0 &gt;= 0\n    \"\"\"\n    X0 = self.generate_design(\n        size=1,\n        repeats=1,\n        lower=self.lower,\n        upper=self.upper,\n    )\n    X0 = repair_non_numeric(X=X0, var_type=self.var_type)\n    X_all = self.to_all_dim_if_needed(X0)\n    logger.debug(\"In Spot() generate_random_point(), before calling self.fun: X_all: %s\", X_all)\n    logger.debug(\"In Spot() generate_random_point(), before calling self.fun: fun_control: %s\", self.fun_control)\n    # Convert multi-objective values to single-objective values\n    # TODO: Store y_mo in self.y_mo (append new values)\n    y_mo = self.fun(X=X_all, fun_control=self.fun_control)\n    y0 = self._mo2so(y_mo)\n    # Apply penalty for NA values works only on so values:\n    y0 = apply_penalty_NA(y0, self.fun_control[\"penalty_NA\"], verbosity=self.verbosity)\n    X0, y0 = remove_nan(X0, y0, stop_on_zero_return=False)\n    return X0, y0\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.get_importance","title":"<code>get_importance()</code>","text":"<p>Get importance of each variable and return the results as a list.</p> <p>Returns:</p> Name Type Description <code>output</code> <code>list</code> <p>list of results. If the surrogate has more than one theta values, the importance is calculated. Otherwise, a list of zeros is returned.</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def get_importance(self) -&gt; list:\n    \"\"\"Get importance of each variable and return the results as a list.\n\n    Returns:\n        output (list):\n            list of results. If the surrogate has more than one theta values,\n            the importance is calculated. Otherwise, a list of zeros is returned.\n\n    \"\"\"\n    if self.surrogate.n_theta &gt; 1 and self.var_name is not None:\n        output = [0] * len(self.all_var_name)\n        theta = np.power(10, self.surrogate.theta)\n        imp = 100 * theta / np.max(theta)\n        ind = find_indices(A=self.var_name, B=self.all_var_name)\n        j = 0\n        for i in ind:\n            output[i] = imp[j]\n            j = j + 1\n        return output\n    else:\n        print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n        # return a list of zeros of length len(all_var_name)\n        return [0] * len(self.all_var_name)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.get_new_X0","title":"<code>get_new_X0()</code>","text":"<p>Generate new design points for the optimization process. This method attempts to suggest and repair new design points using the surrogate model and experimental design techniques. If no valid new points are found within the specified tolerance, a new experimental design is generated.</p> <p>Calls <code>suggest_new_X()</code> and repairs the new design points, e.g., by <code>repair_non_numeric()</code> and <code>selectNew()</code>.</p> <p>Returns:</p> Type Description <code>array</code> <p>np.ndarray: New design points, possibly repeated according to <code>self.fun_repeats</code>.</p> <p>Attributes:</p> Name Type Description <code>self.design</code> <code>object</code> <p>An experimental design object used to generate new points</p> <code>self.n_points</code> <code>int</code> <p>The expected number of new points</p> <code>self.fun_repeats</code> <code>int</code> <p>The number of times to repeat new points</p> <code>self.tolerance_x</code> <code>float</code> <p>Minimum distance required between new and existing solutions</p> <code>self.var_type</code> <code>List[str]</code> <p>Variable types for the design points</p> <code>self.X</code> <code>ndarray</code> <p>Existing solution points</p> <code>self.k</code> <code>int</code> <p>Number of dimensions</p> <code>self.fun_control</code> <code>Dict</code> <p>Control parameters for the function</p> <code>self.counter</code> <code>int</code> <p>Iteration counter</p> Notes <ul> <li>If no new valid design points are suggested, the function resorts   to a space-filling design technique to generate the required points.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.utils.init import (\n        fun_control_init,  design_control_init\n        )\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init\n    # number of initial points:\n    ni = 3\n    X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n    fun = Analytical().fun_sphere\n    fun_control = fun_control_init(\n                n_points=10,\n                ocba_delta=0,\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1])\n    )\n    design_control=design_control_init(init_size=ni)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,\n    )\n    S.initialize_design(X_start=X_start)\n    S.update_stats()\n    S.fit_surrogate()\n    X0 = S.get_new_X0()\n    assert X0.shape[0] == S.n_points\n    assert X0.shape[1] == S.lower.size\n    # assert new points are in the interval [lower, upper]\n    assert np.all(X0 &gt;= S.lower)\n    assert np.all(X0 &lt;= S.upper)\n    # print using 20 digits precision\n    np.set_printoptions(precision=20)\n    print(f\"X0: {X0}\")\n    X0: [[-0.43905273463270317 -0.20947824142606025]\n        [-0.4390526520612617  -0.20947735118625146]\n        [-0.4390526516559971  -0.20947735345727678]\n        [-0.4390526491133424  -0.20947735153559494]\n        [-0.43905264887606393 -0.209477347335596  ]\n        [-0.43905264815296263 -0.20947734884431773]\n        [-0.4390526481478378  -0.2094773501907511 ]\n        [-0.43905264791185933 -0.20947734931732975]\n        [-0.43905264783691894 -0.20947734910961185]\n        [-0.4390526473921517  -0.2094773511154602 ]]\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def get_new_X0(self) -&gt; np.array:\n    \"\"\"\n    Generate new design points for the optimization process.\n    This method attempts to suggest and repair new design points using the surrogate model\n    and experimental design techniques. If no valid new points are found within the specified\n    tolerance, a new experimental design is generated.\n\n    Calls `suggest_new_X()` and repairs the new design points, e.g.,\n    by `repair_non_numeric()` and `selectNew()`.\n\n    Returns:\n        np.ndarray: New design points, possibly repeated according to `self.fun_repeats`.\n\n    Attributes:\n        self.design (object): An experimental design object used to generate new points\n        self.n_points (int): The expected number of new points\n        self.fun_repeats (int): The number of times to repeat new points\n        self.tolerance_x (float): Minimum distance required between new and existing solutions\n        self.var_type (List[str]): Variable types for the design points\n        self.X (np.ndarray): Existing solution points\n        self.k (int): Number of dimensions\n        self.fun_control (Dict): Control parameters for the function\n        self.counter (int): Iteration counter\n\n    Notes:\n        - If no new valid design points are suggested, the function resorts\n          to a space-filling design technique to generate the required points.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.utils.init import (\n                fun_control_init,  design_control_init\n                )\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init\n            # number of initial points:\n            ni = 3\n            X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n            fun = Analytical().fun_sphere\n            fun_control = fun_control_init(\n                        n_points=10,\n                        ocba_delta=0,\n                        lower = np.array([-1, -1]),\n                        upper = np.array([1, 1])\n            )\n            design_control=design_control_init(init_size=ni)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,\n            )\n            S.initialize_design(X_start=X_start)\n            S.update_stats()\n            S.fit_surrogate()\n            X0 = S.get_new_X0()\n            assert X0.shape[0] == S.n_points\n            assert X0.shape[1] == S.lower.size\n            # assert new points are in the interval [lower, upper]\n            assert np.all(X0 &gt;= S.lower)\n            assert np.all(X0 &lt;= S.upper)\n            # print using 20 digits precision\n            np.set_printoptions(precision=20)\n            print(f\"X0: {X0}\")\n            X0: [[-0.43905273463270317 -0.20947824142606025]\n                [-0.4390526520612617  -0.20947735118625146]\n                [-0.4390526516559971  -0.20947735345727678]\n                [-0.4390526491133424  -0.20947735153559494]\n                [-0.43905264887606393 -0.209477347335596  ]\n                [-0.43905264815296263 -0.20947734884431773]\n                [-0.4390526481478378  -0.2094773501907511 ]\n                [-0.43905264791185933 -0.20947734931732975]\n                [-0.43905264783691894 -0.20947734910961185]\n                [-0.4390526473921517  -0.2094773511154602 ]]\n    \"\"\"\n    # Try to generate self.fun_repeats new X0 points:\n    X0 = self.suggest_new_X()\n    # Repair non-numeric variables based on their types\n    X0 = repair_non_numeric(X0, self.var_type)\n    # Condition: select only X0 that have min distance self.tolerance_x\n    # to existing solutions\n    X0, X0_ind = selectNew(A=X0, X=self.X, tolerance=self.tolerance_x)\n    if X0.shape[0] &gt; 0:\n        # If valid new points are found, repeat them as specified\n        # There are X0 that fullfil the condition.\n        # Note: The number of new X0 can be smaller than self.n_points!\n        logger.debug(\"XO values are new: %s %s\", X0_ind, X0)\n        return repeat(X0, self.fun_repeats, axis=0)\n    # If no X0 found, then generate self.n_points new solutions:\n    else:\n        self.design = SpaceFilling(k=self.k, seed=self.fun_control[\"seed\"] + self.counter)\n        X0 = self.generate_design(size=self.n_points, repeats=self.design_control[\"repeats\"], lower=self.lower, upper=self.upper)\n        X0 = repair_non_numeric(X0, self.var_type)\n        logger.warning(\"No new XO found on surrogate. Generate new solution %s\", X0)\n        return X0\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.get_spot_attributes_as_df","title":"<code>get_spot_attributes_as_df()</code>","text":"<p>Get all attributes of the spot object as a pandas dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dataframe with all attributes of the spot object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from math import inf\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 7\n    # number of points\n    n = 10\n    fun = Analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1]),\n        upper = np.array([1]),\n        fun_evals=n)\n    design_control=design_control_init(init_size=ni)\n    spot_1 = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    spot_1.run()\n    df = spot_1.get_spot_attributes_as_df()\ndf\n    df\n        Attribute Name                                    Attribute Value\n    0                   X  [[-0.3378148180708981], [0.698908280342222], [...\n    1           all_lower                                               [-1]\n    2           all_upper                                                [1]\n    3        all_var_name                                               [x0]\n    4        all_var_type                                              [num]\n    5             counter                                                 10\n    6           de_bounds                                          [[-1, 1]]\n    7              design  &lt;spotpython.design.spacefilling.SpaceFilling o...\n    8      design_control                     {'init_size': 7, 'repeats': 1}\n    9                 eps                                                0.0\n    10        fun_control                         {'sigma': 0, 'seed': None}\n    11          fun_evals                                                 10\n    12        fun_repeats                                                  1\n    13              ident                                            [False]\n    14   infill_criterion                                                  y\n    15                  k                                                  1\n    16          log_level                                                 50\n    17              lower                                               [-1]\n    18           max_time                                                inf\n    19             mean_X                                               None\n    20             mean_y                                               None\n    21              min_X                           [1.5392206722432657e-05]\n    22         min_mean_X                                               None\n    23         min_mean_y                                               None\n    24              min_y                                                0.0\n    25           n_points                                                  1\n    26              noise                                               True\n    27         ocba_delta                                                  0\n    28  optimizer_control                    {'max_iter': 1000, 'seed': 125}\n    29            red_dim                                              False\n    30                rng                                   Generator(PCG64)\n    31               seed                                                123\n    32        show_models                                              False\n    33      show_progress                                               True\n    34        spot_writer                                               None\n    35          surrogate  &lt;spotpython.build.kriging.Kriging object at 0x...\n    36  surrogate_control  {'method': \"regession\", 'model_optimizer': &lt;function ...\n    37        tolerance_x                                                  0\n    38              upper                                                [1]\n    39           var_name                                               [x0]\n    40           var_type                                              [num]\n    41              var_y                                               None\n    42                  y  [0.11411885130827397, 0.48847278433092195, 0.0...\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def get_spot_attributes_as_df(self) -&gt; pd.DataFrame:\n    \"\"\"Get all attributes of the spot object as a pandas dataframe.\n\n    Returns:\n        (pandas.DataFrame): dataframe with all attributes of the spot object.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from math import inf\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 7\n            # number of points\n            n = 10\n            fun = Analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1]),\n                upper = np.array([1]),\n                fun_evals=n)\n            design_control=design_control_init(init_size=ni)\n            spot_1 = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,)\n            spot_1.run()\n            df = spot_1.get_spot_attributes_as_df()\n        df\n            df\n                Attribute Name                                    Attribute Value\n            0                   X  [[-0.3378148180708981], [0.698908280342222], [...\n            1           all_lower                                               [-1]\n            2           all_upper                                                [1]\n            3        all_var_name                                               [x0]\n            4        all_var_type                                              [num]\n            5             counter                                                 10\n            6           de_bounds                                          [[-1, 1]]\n            7              design  &lt;spotpython.design.spacefilling.SpaceFilling o...\n            8      design_control                     {'init_size': 7, 'repeats': 1}\n            9                 eps                                                0.0\n            10        fun_control                         {'sigma': 0, 'seed': None}\n            11          fun_evals                                                 10\n            12        fun_repeats                                                  1\n            13              ident                                            [False]\n            14   infill_criterion                                                  y\n            15                  k                                                  1\n            16          log_level                                                 50\n            17              lower                                               [-1]\n            18           max_time                                                inf\n            19             mean_X                                               None\n            20             mean_y                                               None\n            21              min_X                           [1.5392206722432657e-05]\n            22         min_mean_X                                               None\n            23         min_mean_y                                               None\n            24              min_y                                                0.0\n            25           n_points                                                  1\n            26              noise                                               True\n            27         ocba_delta                                                  0\n            28  optimizer_control                    {'max_iter': 1000, 'seed': 125}\n            29            red_dim                                              False\n            30                rng                                   Generator(PCG64)\n            31               seed                                                123\n            32        show_models                                              False\n            33      show_progress                                               True\n            34        spot_writer                                               None\n            35          surrogate  &lt;spotpython.build.kriging.Kriging object at 0x...\n            36  surrogate_control  {'method': \"regession\", 'model_optimizer': &lt;function ...\n            37        tolerance_x                                                  0\n            38              upper                                                [1]\n            39           var_name                                               [x0]\n            40           var_type                                              [num]\n            41              var_y                                               None\n            42                  y  [0.11411885130827397, 0.48847278433092195, 0.0...\n\n    \"\"\"\n\n    attributes = [attr for attr in dir(self) if not callable(getattr(self, attr)) and not attr.startswith(\"__\")]\n    values = [getattr(self, attr) for attr in attributes]\n    df = pd.DataFrame({\"Attribute Name\": attributes, \"Attribute Value\": values})\n    return df\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.get_tuned_hyperparameters","title":"<code>get_tuned_hyperparameters(fun_control=None)</code>","text":"<p>Return the tuned hyperparameter values from the run. If <code>noise == True</code>, the mean values are returned.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary of tuned hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.device import getDevice\n    from math import inf\n    from spotpython.utils.init import fun_control_init\n    import numpy as np\n    from spotpython.hyperparameters.values import set_control_key_value\n    from spotpython.data.diabetes import Diabetes\n    MAX_TIME = 1\n    FUN_EVALS = 10\n    INIT_SIZE = 5\n    WORKERS = 0\n    PREFIX=\"037\"\n    DEVICE = getDevice()\n    DEVICES = 1\n    TEST_SIZE = 0.4\n    TORCH_METRIC = \"mean_squared_error\"\n    dataset = Diabetes()\n    fun_control = fun_control_init(\n        _L_in=10,\n        _L_out=1,\n        _torchmetric=TORCH_METRIC,\n        PREFIX=PREFIX,\n        TENSORBOARD_CLEAN=True,\n        data_set=dataset,\n        device=DEVICE,\n        enable_progress_bar=False,\n        fun_evals=FUN_EVALS,\n        log_level=50,\n        max_time=MAX_TIME,\n        num_workers=WORKERS,\n        show_progress=True,\n        test_size=TEST_SIZE,\n        tolerance_x=np.sqrt(np.spacing(1)),\n        )\n    from spotpython.light.regression.netlightregression import NetLightRegression\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.hyperparameters.values import add_core_model_to_fun_control\n    add_core_model_to_fun_control(fun_control=fun_control,\n                                core_model=NetLightRegression,\n                                hyper_dict=LightHyperDict)\n    from spotpython.hyperparameters.values import set_control_hyperparameter_value\n    set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n    set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n    set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n    set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                    \"Adam\",\n                    \"RAdam\",\n                ])\n    set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n    set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n    set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n    set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                    \"ReLU\",\n                    \"LeakyReLU\"\n                ] )\n    from spotpython.utils.init import design_control_init, surrogate_control_init\n    design_control = design_control_init(init_size=INIT_SIZE)\n    surrogate_control = surrogate_control_init(method=\"regression\",\n                                                n_theta=2)\n    from spotpython.fun.hyperlight import HyperLight\n    fun = HyperLight(log_level=50).fun\n    from spotpython.spot import spot\n    spot_tuner = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control)\n    spot_tuner.run()\n    spot_tuner.get_tuned_hyperparameters()\n        {'l1': 7.0,\n        'epochs': 5.0,\n        'batch_size': 4.0,\n        'act_fn': 0.0,\n        'optimizer': 0.0,\n        'dropout_prob': 0.01,\n        'lr_mult': 5.0,\n        'patience': 3.0,\n        'initialization': 1.0}\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def get_tuned_hyperparameters(self, fun_control=None) -&gt; dict:\n    \"\"\"Return the tuned hyperparameter values from the run.\n    If `noise == True`, the mean values are returned.\n\n    Args:\n        fun_control (dict, optional):\n            fun_control dictionary\n\n    Returns:\n        (dict): dictionary of tuned hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.device import getDevice\n            from math import inf\n            from spotpython.utils.init import fun_control_init\n            import numpy as np\n            from spotpython.hyperparameters.values import set_control_key_value\n            from spotpython.data.diabetes import Diabetes\n            MAX_TIME = 1\n            FUN_EVALS = 10\n            INIT_SIZE = 5\n            WORKERS = 0\n            PREFIX=\"037\"\n            DEVICE = getDevice()\n            DEVICES = 1\n            TEST_SIZE = 0.4\n            TORCH_METRIC = \"mean_squared_error\"\n            dataset = Diabetes()\n            fun_control = fun_control_init(\n                _L_in=10,\n                _L_out=1,\n                _torchmetric=TORCH_METRIC,\n                PREFIX=PREFIX,\n                TENSORBOARD_CLEAN=True,\n                data_set=dataset,\n                device=DEVICE,\n                enable_progress_bar=False,\n                fun_evals=FUN_EVALS,\n                log_level=50,\n                max_time=MAX_TIME,\n                num_workers=WORKERS,\n                show_progress=True,\n                test_size=TEST_SIZE,\n                tolerance_x=np.sqrt(np.spacing(1)),\n                )\n            from spotpython.light.regression.netlightregression import NetLightRegression\n            from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n            from spotpython.hyperparameters.values import add_core_model_to_fun_control\n            add_core_model_to_fun_control(fun_control=fun_control,\n                                        core_model=NetLightRegression,\n                                        hyper_dict=LightHyperDict)\n            from spotpython.hyperparameters.values import set_control_hyperparameter_value\n            set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n            set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n            set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n            set_control_hyperparameter_value(fun_control, \"optimizer\", [\n                            \"Adam\",\n                            \"RAdam\",\n                        ])\n            set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n            set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n            set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n            set_control_hyperparameter_value(fun_control, \"act_fn\",[\n                            \"ReLU\",\n                            \"LeakyReLU\"\n                        ] )\n            from spotpython.utils.init import design_control_init, surrogate_control_init\n            design_control = design_control_init(init_size=INIT_SIZE)\n            surrogate_control = surrogate_control_init(method=\"regression\",\n                                                        n_theta=2)\n            from spotpython.fun.hyperlight import HyperLight\n            fun = HyperLight(log_level=50).fun\n            from spotpython.spot import spot\n            spot_tuner = spot.Spot(fun=fun,\n                                fun_control=fun_control,\n                                design_control=design_control,\n                                surrogate_control=surrogate_control)\n            spot_tuner.run()\n            spot_tuner.get_tuned_hyperparameters()\n                {'l1': 7.0,\n                'epochs': 5.0,\n                'batch_size': 4.0,\n                'act_fn': 0.0,\n                'optimizer': 0.0,\n                'dropout_prob': 0.01,\n                'lr_mult': 5.0,\n                'patience': 3.0,\n                'initialization': 1.0}\n\n    \"\"\"\n    output = []\n    if self.noise:\n        res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n    else:\n        res = self.to_all_dim(self.min_X.reshape(1, -1))\n    for i in range(res.shape[1]):\n        if self.all_var_name is None:\n            var_name = \"x\" + str(i)\n        else:\n            var_name = self.all_var_name[i]\n            var_type = self.all_var_type[i]\n            if var_type == \"factor\" and fun_control is not None:\n                val = get_ith_hyperparameter_name_from_fun_control(fun_control=fun_control, key=var_name, i=int(res[0][i]))\n            else:\n                val = res[0][i]\n        output.append([var_name, val])\n    # convert list to a dictionary\n    output = dict(output)\n    return output\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.infill","title":"<code>infill(x)</code>","text":"<p>Infill (acquisition) function. Evaluates one point on the surrogate via <code>surrogate.predict(x.reshape(1,-1))</code>, if <code>sklearn</code> surrogates are used or <code>surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)</code> if the internal surrogate <code>kriging</code> is selected. This method is passed to the optimizer in <code>suggest_new_X</code>, i.e., the optimizer is called via <code>self.optimizer(func=self.infill)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>point in natural units with shape <code>(1, dim)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>value based on infill criterion, e.g., <code>\"ei\"</code>. Shape <code>(1,)</code>. The objective function value <code>y</code> that is used as a base value for the infill criterion is calculated in natural units.</p> Note <p>This is step (S-12) in [bart21i].</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def infill(self, x) -&gt; float:\n    \"\"\"\n    Infill (acquisition) function. Evaluates one point on the surrogate via `surrogate.predict(x.reshape(1,-1))`,\n    if `sklearn` surrogates are used or `surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)`\n    if the internal surrogate `kriging` is selected.\n    This method is passed to the optimizer in `suggest_new_X`, i.e., the optimizer is called via\n    `self.optimizer(func=self.infill)`.\n\n    Args:\n        x (array): point in natural units with shape `(1, dim)`.\n\n    Returns:\n        (numpy.ndarray): value based on infill criterion, e.g., `\"ei\"`. Shape `(1,)`.\n            The objective function value `y` that is used as a base value for the\n            infill criterion is calculated in natural units.\n\n    Note:\n        This is step (S-12) in [bart21i].\n    \"\"\"\n    # Reshape x to have shape (1, -1) because the predict method expects a 2D array\n    X = x.reshape(1, -1)\n    if isinstance(self.surrogate, Kriging) and getattr(self.surrogate, \"name\", None) == \"kriging\":\n        return self.surrogate.predict(X, return_val=self.infill_criterion)\n    else:\n        return self.surrogate.predict(X)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.initialize_design","title":"<code>initialize_design(X_start=None)</code>","text":"<p>Initialize design. Generate and evaluate initial design. If <code>X_start</code> is not <code>None</code>, append it to the initial design. Therefore, the design size is <code>init_size</code> + <code>X_start.shape[0]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_start</code> <code>ndarray</code> <p>initial design. Must be of shape (n, k), where n is the number of points and k is the number of dimensions. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>self.X</code> <code>ndarray</code> <p>initial design</p> <code>self.y</code> <code>ndarray</code> <p>initial design values</p> Note <ul> <li>If <code>X_start</code> is has the wrong shape, it is ignored.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init,  design_control_init\n        )\n    # number of initial points:\n    ni = 7\n    # start point X_0\n    X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    fun = Analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1]))\n    design_control=design_control_init(init_size=ni)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    S.initialize_design(X_start=X_start)\n    print(f\"S.X: {S.X}\")\n        S.X: [[ 0.          0.        ]\n            [ 0.          1.        ]\n            [ 1.          0.        ]\n            [ 1.          1.        ]\n            [-0.90924339 -0.15823458]\n            [-0.20581711 -0.48124909]\n            [ 0.94974117 -0.94631272]\n            [-0.12095571  0.06383589]\n            [-0.66278702  0.17431637]\n            [ 0.28200844  0.93001011]\n            [ 0.47878812  0.65321058]]\n    print(f\"S.y: {S.y}\")\n            S.y: [0.         1.         1.         2.         0.85176172 0.27396137\n                1.79751605 0.01870531 0.46967283 0.94444757 0.65592212]\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def initialize_design(self, X_start=None) -&gt; None:\n    \"\"\"\n    Initialize design. Generate and evaluate initial design.\n    If `X_start` is not `None`, append it to the initial design.\n    Therefore, the design size is `init_size` + `X_start.shape[0]`.\n\n    Args:\n        X_start (numpy.ndarray, optional):\n            initial design. Must be of shape (n, k), where n is the number\n            of points and k is the number of dimensions. Defaults to None.\n\n    Attributes:\n        self.X (numpy.ndarray): initial design\n        self.y (numpy.ndarray): initial design values\n\n    Note:\n        * If `X_start` is has the wrong shape, it is ignored.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init,  design_control_init\n                )\n            # number of initial points:\n            ni = 7\n            # start point X_0\n            X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n            fun = Analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1]))\n            design_control=design_control_init(init_size=ni)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,)\n            S.initialize_design(X_start=X_start)\n            print(f\"S.X: {S.X}\")\n                S.X: [[ 0.          0.        ]\n                    [ 0.          1.        ]\n                    [ 1.          0.        ]\n                    [ 1.          1.        ]\n                    [-0.90924339 -0.15823458]\n                    [-0.20581711 -0.48124909]\n                    [ 0.94974117 -0.94631272]\n                    [-0.12095571  0.06383589]\n                    [-0.66278702  0.17431637]\n                    [ 0.28200844  0.93001011]\n                    [ 0.47878812  0.65321058]]\n            print(f\"S.y: {S.y}\")\n                    S.y: [0.         1.         1.         2.         0.85176172 0.27396137\n                        1.79751605 0.01870531 0.46967283 0.94444757 0.65592212]\n    \"\"\"\n    self.initialize_design_matrix(X_start)\n\n    self.evaluate_initial_design()\n\n    self.write_initial_tensorboard_log()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.initialize_design_matrix","title":"<code>initialize_design_matrix(X_start=None)</code>","text":"<p>Initialize the design matrix for the optimization process. This method generates an initial design matrix, optionally appending any provided starting points (<code>X_start</code>). The resulting design matrix is sanitized for non-numeric values and stored in <code>self.X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_start</code> <code>ndarray</code> <p>User-provided starting points for the design. Shape should be (n=n_samples, k=n_features). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>numpy.ndarray: The design matrix that combines the generated design         with the provided starting points.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the resulting design matrix has zero rows.</p> Notes <ul> <li>If <code>X_start</code> is not in the expected shape, it is ignored.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun import Analytical\n    from spotpython.spot import Spot\n    from spotpython.utils.init import fun_control_init\n    fun_control = fun_control_init(\n        tensorboard_log=True,\n        TENSORBOARD_CLEAN=True,\n        lower = np.array([-1]),\n        upper = np.array([1])\n        )\n    fun = Analytical().fun_sphere\n    S = Spot(fun=fun,\n                fun_control=fun_control,\n                )\n    X_start = np.array([[0.5, 0.5], [0.4, 0.4]])\n    design_matrix = S.initialize_design_matrix(X_start)\n    print(f\"Design matrix: {design_matrix}\")\n        Design matrix: [[ 0.1         0.2       ]\n        [ 0.3         0.4       ]\n        [ 0.86352963  0.7892358 ]\n        [-0.24407197 -0.83687436]\n        [ 0.36481882  0.8375811 ]\n        [ 0.415331    0.54468512]\n        [-0.56395091 -0.77797854]\n        [-0.90259409 -0.04899292]\n        [-0.16484832  0.35724741]\n        [ 0.05170659  0.07401196]\n        [-0.78548145 -0.44638164]\n        [ 0.64017497 -0.30363301]]\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def initialize_design_matrix(self, X_start=None) -&gt; None:\n    \"\"\"\n    Initialize the design matrix for the optimization process.\n    This method generates an initial design matrix, optionally\n    appending any provided starting points (`X_start`). The resulting\n    design matrix is sanitized for non-numeric values and stored in `self.X`.\n\n    Args:\n        X_start (numpy.ndarray, optional): User-provided starting points\n            for the design. Shape should be (n=n_samples, k=n_features).\n            Defaults to None.\n\n    Returns:\n        numpy.ndarray: The design matrix that combines the generated design\n                    with the provided starting points.\n\n    Raises:\n        Exception: If the resulting design matrix has zero rows.\n\n    Notes:\n        * If `X_start` is not in the expected shape, it is ignored.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun import Analytical\n            from spotpython.spot import Spot\n            from spotpython.utils.init import fun_control_init\n            fun_control = fun_control_init(\n                tensorboard_log=True,\n                TENSORBOARD_CLEAN=True,\n                lower = np.array([-1]),\n                upper = np.array([1])\n                )\n            fun = Analytical().fun_sphere\n            S = Spot(fun=fun,\n                        fun_control=fun_control,\n                        )\n            X_start = np.array([[0.5, 0.5], [0.4, 0.4]])\n            design_matrix = S.initialize_design_matrix(X_start)\n            print(f\"Design matrix: {design_matrix}\")\n                Design matrix: [[ 0.1         0.2       ]\n                [ 0.3         0.4       ]\n                [ 0.86352963  0.7892358 ]\n                [-0.24407197 -0.83687436]\n                [ 0.36481882  0.8375811 ]\n                [ 0.415331    0.54468512]\n                [-0.56395091 -0.77797854]\n                [-0.90259409 -0.04899292]\n                [-0.16484832  0.35724741]\n                [ 0.05170659  0.07401196]\n                [-0.78548145 -0.44638164]\n                [ 0.64017497 -0.30363301]]\n    \"\"\"\n    if self.design_control[\"init_size\"] &gt; 0:\n        X0 = self.generate_design(\n            size=self.design_control[\"init_size\"],\n            repeats=self.design_control[\"repeats\"],\n            lower=self.lower,\n            upper=self.upper,\n        )\n\n    if X_start is not None:\n        if not isinstance(X_start, np.ndarray):\n            X_start = np.array(X_start)\n        X_start = np.atleast_2d(X_start)\n        try:\n            if self.design_control[\"init_size\"] &gt; 0:\n                X0 = np.append(X_start, X0, axis=0)\n            else:\n                X0 = X_start\n        except ValueError:\n            logger.warning(\"X_start has wrong shape. Ignoring it.\")\n\n    if X0.shape[0] == 0:\n        raise Exception(\"X0 has zero rows. Check design_control['init_size'] or X_start.\")\n\n    self.X = repair_non_numeric(X0, self.var_type)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.parallel_plot","title":"<code>parallel_plot(show=False)</code>","text":"<p>Parallel plot.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <code>show</code> <code>bool</code> <p>show the plot. Default is <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>figure object</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 5\n    # number of points\n    fun_evals = 10\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1, -1]),\n        upper = np.array([1, 1, 1]),\n        fun_evals=fun_evals,\n        tolerance_x = np.sqrt(np.spacing(1))\n        )\n    design_control=design_control_init(init_size=ni)\n    surrogate_control=surrogate_control_init(n_theta=3)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,\n                surrogate_control=surrogate_control,)\n    S.run()\n    S.parallel_plot()\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def parallel_plot(self, show=False) -&gt; go.Figure:\n    \"\"\"\n    Parallel plot.\n\n    Args:\n        self (object):\n            Spot object\n        show (bool):\n            show the plot. Default is `False`.\n\n    Returns:\n            fig (plotly.graph_objects.Figure): figure object\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 5\n            # number of points\n            fun_evals = 10\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1, -1]),\n                upper = np.array([1, 1, 1]),\n                fun_evals=fun_evals,\n                tolerance_x = np.sqrt(np.spacing(1))\n                )\n            design_control=design_control_init(init_size=ni)\n            surrogate_control=surrogate_control_init(n_theta=3)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control,)\n            S.run()\n            S.parallel_plot()\n\n    \"\"\"\n    X = self.X\n    y = self.y\n    df = pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=self.var_name + [\"y\"])\n\n    fig = go.Figure(\n        data=go.Parcoords(\n            line=dict(color=df[\"y\"], colorscale=\"Jet\", showscale=True, cmin=min(df[\"y\"]), cmax=max(df[\"y\"])),\n            dimensions=list([dict(range=[min(df.iloc[:, i]), max(df.iloc[:, i])], label=df.columns[i], values=df.iloc[:, i]) for i in range(len(df.columns) - 1)]),\n        )\n    )\n    if show:\n        fig.show()\n    return fig\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.plot_contour","title":"<code>plot_contour(i=0, j=1, min_z=None, max_z=None, show=True, filename=None, n_grid=50, contour_levels=10, dpi=200, title='', figsize=(12, 6), use_min=False, use_max=True, tkagg=False)</code>","text":"<p>Plot the contour of any dimension.</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def plot_contour(\n    self,\n    i=0,\n    j=1,\n    min_z=None,\n    max_z=None,\n    show=True,\n    filename=None,\n    n_grid=50,\n    contour_levels=10,\n    dpi=200,\n    title=\"\",\n    figsize=(12, 6),\n    use_min=False,\n    use_max=True,\n    tkagg=False,\n) -&gt; None:\n    \"\"\"Plot the contour of any dimension.\"\"\"\n\n    def generate_mesh_grid(lower, upper, grid_points):\n        \"\"\"Generate a mesh grid for the given range.\"\"\"\n        x = np.linspace(lower[i], upper[i], num=grid_points)\n        y = np.linspace(lower[j], upper[j], num=grid_points)\n        return np.meshgrid(x, y), x, y\n\n    def validate_types(var_type, lower, upper):\n        \"\"\"Validate if the dimensions of var_type, lower, and upper are the same.\"\"\"\n        if var_type is not None:\n            if len(var_type) != len(lower) or len(var_type) != len(upper):\n                raise ValueError(\"The dimensions of var_type, lower, and upper must be the same.\")\n\n    def setup_plot():\n        \"\"\"Setup the plot with specified figure size.\"\"\"\n        fig = pylab.figure(figsize=figsize)\n        return fig\n\n    def predict_contour_values(X, Y, z0):\n        \"\"\"Predict contour values based on the surrogate model.\"\"\"\n        grid_points = np.c_[np.ravel(X), np.ravel(Y)]\n        predictions = []\n\n        for x, y in grid_points:\n            adjusted_z0 = self.chg(x, y, z0.copy(), i, j)\n            prediction = self.surrogate.predict(np.array([adjusted_z0]))\n            predictions.append(prediction[0])\n\n        Z = np.array(predictions).reshape(X.shape)\n        return Z\n\n    def plot_contour_subplots(X, Y, Z, ax, min_z, max_z, contour_levels):\n        \"\"\"Plot the contour and 3D surface subplots.\"\"\"\n        contour = ax.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\", vmin=min_z, vmax=max_z)\n        pylab.colorbar(contour, ax=ax)\n\n    if tkagg:\n        matplotlib.use(\"TkAgg\")\n    fig = setup_plot()\n\n    (X, Y), x, y = generate_mesh_grid(self.lower, self.upper, n_grid)\n    validate_types(self.var_type, self.lower, self.upper)\n\n    z00 = np.array([self.lower, self.upper])\n    Z_list, X_list, Y_list = [], [], []\n\n    if use_min:\n        z0_min = self.process_z00(z00, use_min=True)\n        Z_min = predict_contour_values(X, Y, z0_min)\n        Z_list.append(Z_min)\n        X_list.append(X)\n        Y_list.append(Y)\n\n    if use_max:\n        z0_max = self.process_z00(z00, use_min=False)\n        Z_max = predict_contour_values(X, Y, z0_max)\n        Z_list.append(Z_max)\n        X_list.append(X)\n        Y_list.append(Y)\n\n    if Z_list:  # Ensure that there is at least one Z to stack\n        Z_combined = np.vstack(Z_list)\n        X_combined = np.vstack(X_list)\n        Y_combined = np.vstack(Y_list)\n\n    if min_z is None:\n        min_z = np.min(Z_combined)\n    if max_z is None:\n        max_z = np.max(Z_combined)\n\n    ax_contour = fig.add_subplot(221)\n    plot_contour_subplots(X_combined, Y_combined, Z_combined, ax_contour, min_z, max_z, contour_levels)\n\n    if self.var_name is None:\n        ax_contour.set_xlabel(f\"x{i}\")\n        ax_contour.set_ylabel(f\"x{j}\")\n    else:\n        ax_contour.set_xlabel(f\"x{i}: {self.var_name[i]}\")\n        ax_contour.set_ylabel(f\"x{j}: {self.var_name[j]}\")\n\n    ax_3d = fig.add_subplot(222, projection=\"3d\")\n    ax_3d.plot_surface(X_combined, Y_combined, Z_combined, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\", vmin=min_z, vmax=max_z)\n\n    if self.var_name is None:\n        ax_3d.set_xlabel(f\"x{i}\")\n        ax_3d.set_ylabel(f\"x{j}\")\n    else:\n        ax_3d.set_xlabel(f\"x{i}: {self.var_name[i]}\")\n        ax_3d.set_ylabel(f\"x{j}: {self.var_name[j]}\")\n\n    plt.title(title)\n\n    if filename:\n        pylab.savefig(filename, bbox_inches=\"tight\", dpi=dpi, pad_inches=0)\n\n    if show:\n        pylab.show()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.plot_importance","title":"<code>plot_importance(threshold=0.1, filename=None, dpi=300, show=True, tkagg=False)</code>","text":"<p>Plot the importance of each variable.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold of the importance.</p> <code>0.1</code> <code>filename</code> <code>str</code> <p>The filename of the plot.</p> <code>None</code> <code>dpi</code> <code>int</code> <p>The dpi of the plot.</p> <code>300</code> <code>show</code> <code>bool</code> <p>Show the plot. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def plot_importance(self, threshold=0.1, filename=None, dpi=300, show=True, tkagg=False) -&gt; None:\n    \"\"\"Plot the importance of each variable.\n\n    Args:\n        threshold (float):\n            The threshold of the importance.\n        filename (str):\n            The filename of the plot.\n        dpi (int):\n            The dpi of the plot.\n        show (bool):\n            Show the plot. Default is `True`.\n\n    Returns:\n        None\n    \"\"\"\n    if self.surrogate.n_theta &gt; 1:\n        if tkagg:\n            matplotlib.use(\"TkAgg\")\n        theta = np.power(10, self.surrogate.theta)\n        imp = 100 * theta / np.max(theta)\n        idx = np.where(imp &gt; threshold)[0]\n        if self.var_name is None:\n            plt.bar(range(len(imp[idx])), imp[idx])\n            plt.xticks(range(len(imp[idx])), [\"x\" + str(i) for i in idx])\n        else:\n            var_name = [self.var_name[i] for i in idx]\n            plt.bar(range(len(imp[idx])), imp[idx])\n            plt.xticks(range(len(imp[idx])), var_name)\n        if filename is not None:\n            plt.savefig(filename, bbox_inches=\"tight\", dpi=dpi)\n        if show:\n            plt.show()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.plot_important_hyperparameter_contour","title":"<code>plot_important_hyperparameter_contour(threshold=0.0, filename=None, show=True, max_imp=None, title='', scale_global=False, n_grid=50, contour_levels=10, dpi=200, use_min=False, use_max=True, tkagg=False)</code>","text":"<p>Plot the contour of important hyperparameters. Calls <code>plot_contour</code> for each pair of important hyperparameters. Importance can be specified by the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>threshold for the importance. Not used any more in spotpython &gt;= 0.13.2.</p> <code>0.0</code> <code>filename</code> <code>str</code> <p>filename of the plot</p> <code>None</code> <code>show</code> <code>bool</code> <p>show the plot. Default is <code>True</code>.</p> <code>True</code> <code>max_imp</code> <code>int</code> <p>maximum number of important hyperparameters. If there are more important hyperparameters than <code>max_imp</code>, only the max_imp important ones are selected.</p> <code>None</code> <code>title</code> <code>str</code> <p>title of the plots</p> <code>''</code> <code>scale_global</code> <code>bool</code> <p>scale the z-axis globally. Default is <code>False</code>.</p> <code>False</code> <code>n_grid</code> <code>int</code> <p>number of grid points. Default is 50.</p> <code>50</code> <code>contour_levels</code> <code>int</code> <p>number of contour levels. Default is 10.</p> <code>10</code> <code>dpi</code> <code>int</code> <p>dpi of the plot. Default is 200.</p> <code>200</code> <code>use_min</code> <code>bool</code> <p>Use the minimum value for determing the hidden dimensions in the plot for categorical and integer parameters. In 3d-plots, only two variables can be independent. The remaining input variables are set to their minimum value. Default is <code>False</code>. If use_min and use_max are both <code>True</code>, both values are used.</p> <code>False</code> <code>use_max</code> <code>bool</code> <p>Use the minimum value for determing the hidden dimensions in the plot for categorical and integer parameters. In 3d-plots, only two variables can be independent. The remaining input variables are set to their minimum value. Default is <code>True</code>. If use_min and use_max are both <code>True</code>, both values are used.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 5\n    # number of points\n    fun_evals = 10\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1, -1]),\n        upper = np.array([1, 1, 1]),\n        fun_evals=fun_evals,\n        tolerance_x = np.sqrt(np.spacing(1))\n        )\n    design_control=design_control_init(init_size=ni)\n    surrogate_control=surrogate_control_init(n_theta=3)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,\n                surrogate_control=surrogate_control,)\n    S.run()\n    S.plot_important_hyperparameter_contour()\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def plot_important_hyperparameter_contour(\n    self,\n    threshold=0.0,\n    filename=None,\n    show=True,\n    max_imp=None,\n    title=\"\",\n    scale_global=False,\n    n_grid=50,\n    contour_levels=10,\n    dpi=200,\n    use_min=False,\n    use_max=True,\n    tkagg=False,\n) -&gt; None:\n    \"\"\"\n    Plot the contour of important hyperparameters.\n    Calls `plot_contour` for each pair of important hyperparameters.\n    Importance can be specified by the threshold.\n\n    Args:\n        threshold (float):\n            threshold for the importance. Not used any more in spotpython &gt;= 0.13.2.\n        filename (str):\n            filename of the plot\n        show (bool):\n            show the plot. Default is `True`.\n        max_imp (int):\n            maximum number of important hyperparameters. If there are more important hyperparameters\n            than `max_imp`, only the max_imp important ones are selected.\n        title (str):\n            title of the plots\n        scale_global (bool):\n            scale the z-axis globally. Default is `False`.\n        n_grid (int):\n            number of grid points. Default is 50.\n        contour_levels (int):\n            number of contour levels. Default is 10.\n        dpi (int):\n            dpi of the plot. Default is 200.\n        use_min (bool):\n            Use the minimum value for determing the hidden dimensions in the plot for categorical and\n            integer parameters.\n            In 3d-plots, only two variables can be independent. The remaining input variables are set\n            to their minimum value.\n            Default is `False`.\n            If use_min and use_max are both `True`, both values are used.\n        use_max (bool):\n            Use the minimum value for determing the hidden dimensions in the plot for categorical and\n            integer parameters.\n            In 3d-plots, only two variables can be independent. The remaining input variables are set\n            to their minimum value.\n            Default is `True`.\n            If use_min and use_max are both `True`, both values are used.\n\n    Returns:\n        None.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 5\n            # number of points\n            fun_evals = 10\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1, -1]),\n                upper = np.array([1, 1, 1]),\n                fun_evals=fun_evals,\n                tolerance_x = np.sqrt(np.spacing(1))\n                )\n            design_control=design_control_init(init_size=ni)\n            surrogate_control=surrogate_control_init(n_theta=3)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,\n                        surrogate_control=surrogate_control,)\n            S.run()\n            S.plot_important_hyperparameter_contour()\n\n    \"\"\"\n    impo = self.print_importance(threshold=threshold, print_screen=True)\n    indices = sort_by_kth_and_return_indices(array=impo, k=1)\n    # take the first max_imp values from the indices array\n    if max_imp is not None:\n        indices = indices[:max_imp]\n    if scale_global:\n        min_z = min(self.y)\n        max_z = max(self.y)\n    else:\n        min_z = None\n        max_z = None\n    for i in indices:\n        for j in indices:\n            if j &gt; i:\n                if filename is not None:\n                    filename_full = filename + \"_contour_\" + str(i) + \"_\" + str(j) + \".png\"\n                else:\n                    filename_full = None\n                self.plot_contour(\n                    i=i,\n                    j=j,\n                    min_z=min_z,\n                    max_z=max_z,\n                    filename=filename_full,\n                    show=show,\n                    title=title,\n                    n_grid=n_grid,\n                    contour_levels=contour_levels,\n                    dpi=dpi,\n                    use_max=use_max,\n                    use_min=use_min,\n                    tkagg=tkagg,\n                )\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.plot_model","title":"<code>plot_model(y_min=None, y_max=None)</code>","text":"<p>Plot the model fit for 1-dim objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>spot object</p> required <code>y_min</code> <code>float</code> <p>y range, lower bound.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>y range, upper bound.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    # number of initial points:\n    ni = 3\n    # number of points\n    fun_evals = 7\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1]),\n        upper = np.array([1]),\n        fun_evals=fun_evals,\n        tolerance_x = np.sqrt(np.spacing(1))\n        )\n    design_control=design_control_init(init_size=ni)\n</code></pre> <pre><code>S = spot.Spot(fun=fun,\n            fun_control=fun_control,\n            design_control=design_control\nS.run()\nS.plot_model()\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def plot_model(self, y_min=None, y_max=None) -&gt; None:\n    \"\"\"\n    Plot the model fit for 1-dim objective functions.\n\n    Args:\n        self (object):\n            spot object\n        y_min (float, optional):\n            y range, lower bound.\n        y_max (float, optional):\n            y range, upper bound.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            # number of initial points:\n            ni = 3\n            # number of points\n            fun_evals = 7\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1]),\n                upper = np.array([1]),\n                fun_evals=fun_evals,\n                tolerance_x = np.sqrt(np.spacing(1))\n                )\n            design_control=design_control_init(init_size=ni)\n\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control\n            S.run()\n            S.plot_model()\n    \"\"\"\n    if self.k == 1:\n        X_test = np.linspace(self.lower[0], self.upper[0], 100)\n        y_mo = self.fun(X=X_test.reshape(-1, 1), fun_control=self.fun_control)\n        # convert multi-objective values to single-objective values\n        y_test = self._mo2so(y_mo)\n        # Apply penalty for NA values works only on so values:\n        y_test = apply_penalty_NA(y_test, self.fun_control[\"penalty_NA\"], verbosity=self.verbosity)\n        if isinstance(self.surrogate, Kriging) and getattr(self.surrogate, \"name\", None) == \"kriging\":\n            y_hat = self.surrogate.predict(X_test[:, np.newaxis], return_val=\"y\")\n        else:\n            y_hat = self.surrogate.predict(X_test[:, np.newaxis])\n        plt.plot(X_test, y_hat, label=\"Model\")\n        plt.plot(X_test, y_test, label=\"True function\")\n        plt.scatter(self.X, self.y, edgecolor=\"b\", s=20, label=\"Samples\")\n        plt.scatter(self.X[-1], self.y[-1], edgecolor=\"r\", s=30, label=\"Last Sample\")\n        if self.noise:\n            plt.scatter(self.min_mean_X, self.min_mean_y, edgecolor=\"g\", s=30, label=\"Best Sample (mean)\")\n        else:\n            plt.scatter(self.min_X, self.min_y, edgecolor=\"g\", s=30, label=\"Best Sample\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.xlim((self.lower[0], self.upper[0]))\n        if y_min is None:\n            y_min = min([min(self.y), min(y_test)])\n        if y_max is None:\n            y_max = max([max(self.y), max(y_test)])\n        plt.ylim((y_min, y_max))\n        plt.legend(loc=\"best\")\n        # plt.title(self.surrogate.__class__.__name__ + \". \" + str(self.counter) + \": \" + str(self.min_y))\n        if self.noise:\n            plt.title(\"fun_evals: \" + str(self.counter) + \". min_y (noise): \" + str(np.round(self.min_y, 6)) + \" min_mean_y: \" + str(np.round(self.min_mean_y, 6)))\n        else:\n            plt.title(\"fun_evals: \" + str(self.counter) + \". min_y: \" + str(np.round(self.min_y, 6)))\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.plot_progress","title":"<code>plot_progress(show=True, log_x=False, log_y=False, filename='plot.png', style=['ko', 'k', 'ro-'], dpi=300, tkagg=False)</code>","text":"<p>Plot the progress of the hyperparameter tuning (optimization).</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>Show the plot.</p> <code>True</code> <code>log_x</code> <code>bool</code> <p>Use logarithmic scale for x-axis.</p> <code>False</code> <code>log_y</code> <code>bool</code> <p>Use logarithmic scale for y-axis.</p> <code>False</code> <code>filename</code> <code>str</code> <p>Filename to save the plot.</p> <code>'plot.png'</code> <code>style</code> <code>list</code> <p>Style of the plot. Default: [\u2018k\u2019, \u2018ro-\u2018], i.e., the initial points are plotted as a black line and the subsequent points as red dots connected by a line.</p> <code>['ko', 'k', 'ro-']</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 7\n    # number of points\n    fun_evals = 10\n    fun = analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1])\n        fun_evals=fun_evals,\n        tolerance_x = np.sqrt(np.spacing(1))\n        )\n    design_control=design_control_init(init_size=ni)\n    surrogate_control=surrogate_control_init(n_theta=3)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control\n                design_control=design_control,\n                surrogate_control=surrogate_control,)\n    S.run()\n    S.plot_progress(log_y=True)\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def plot_progress(self, show=True, log_x=False, log_y=False, filename=\"plot.png\", style=[\"ko\", \"k\", \"ro-\"], dpi=300, tkagg=False) -&gt; None:\n    \"\"\"Plot the progress of the hyperparameter tuning (optimization).\n\n    Args:\n        show (bool):\n            Show the plot.\n        log_x (bool):\n            Use logarithmic scale for x-axis.\n        log_y (bool):\n            Use logarithmic scale for y-axis.\n        filename (str):\n            Filename to save the plot.\n        style (list):\n            Style of the plot. Default: ['k', 'ro-'], i.e., the initial points are plotted as a black line\n            and the subsequent points as red dots connected by a line.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 7\n            # number of points\n            fun_evals = 10\n            fun = analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1])\n                fun_evals=fun_evals,\n                tolerance_x = np.sqrt(np.spacing(1))\n                )\n            design_control=design_control_init(init_size=ni)\n            surrogate_control=surrogate_control_init(n_theta=3)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control\n                        design_control=design_control,\n                        surrogate_control=surrogate_control,)\n            S.run()\n            S.plot_progress(log_y=True)\n\n    \"\"\"\n    if tkagg:\n        matplotlib.use(\"TkAgg\")\n    fig = pylab.figure(figsize=(9, 6))\n    s_y = pd.Series(self.y)\n    s_c = s_y.cummin()\n    n_init = self.design_control[\"init_size\"] * self.design_control[\"repeats\"]\n\n    ax = fig.add_subplot(211)\n    if n_init &lt;= len(s_y):\n        ax.plot(\n            range(1, n_init + 1),\n            s_y[:n_init],\n            style[0],\n            range(1, n_init + 2),\n            [s_c[:n_init].min()] * (n_init + 1),\n            style[1],\n            range(n_init + 1, len(s_c) + 1),\n            s_c[n_init:],\n            style[2],\n        )\n    else:\n        # plot only s_y values:\n        ax.plot(range(1, len(s_y) + 1), s_y, style[0])\n        logger.warning(\"Less evaluations ({len(s_y)}) than initial design points ({n_init}).\")\n    ax.set_xlabel(\"Iteration\")\n    if log_x:\n        ax.set_xscale(\"log\")\n    if log_y:\n        ax.set_yscale(\"log\")\n    if filename is not None:\n        pylab.savefig(filename, dpi=dpi, bbox_inches=\"tight\")\n    if show:\n        pylab.show()\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.print_importance","title":"<code>print_importance(threshold=0.1, print_screen=True)</code>","text":"<p>Print importance of each variable and return the results as a list.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>threshold for printing</p> <code>0.1</code> <code>print_screen</code> <code>boolean</code> <p>if <code>True</code>, values are also printed on the screen. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>output</code> <code>list</code> <p>list of results</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def print_importance(self, threshold=0.1, print_screen=True) -&gt; list:\n    \"\"\"Print importance of each variable and return the results as a list.\n\n    Args:\n        threshold (float):\n            threshold for printing\n        print_screen (boolean):\n            if `True`, values are also printed on the screen. Default is `True`.\n\n    Returns:\n        output (list):\n            list of results\n    \"\"\"\n    output = []\n    if self.surrogate.n_theta &gt; 1:\n        theta = np.power(10, self.surrogate.theta)\n        imp = 100 * theta / np.max(theta)\n        # imp = imp[imp &gt;= threshold]\n        if self.var_name is None:\n            for i in range(len(imp)):\n                if imp[i] &gt;= threshold:\n                    if print_screen:\n                        print(\"x\", i, \": \", imp[i])\n                    output.append(\"x\" + str(i) + \": \" + str(imp[i]))\n        else:\n            var_name = [self.var_name[i] for i in range(len(imp))]\n            for i in range(len(imp)):\n                if imp[i] &gt;= threshold:\n                    if print_screen:\n                        print(var_name[i] + \": \", imp[i])\n                output.append([var_name[i], imp[i]])\n    else:\n        print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n    return output\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.print_results","title":"<code>print_results(print_screen=True, dict=None)</code>","text":"Print results from the run <ol> <li>min y</li> <li>min X If <code>noise == True</code>, additionally the following values are printed:</li> <li>min mean y</li> <li>min mean X</li> </ol> <p>Parameters:</p> Name Type Description Default <code>print_screen</code> <code>bool</code> <p>print results to screen</p> <code>True</code> <p>Returns:</p> Name Type Description <code>output</code> <code>list</code> <p>list of results</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def print_results(self, print_screen=True, dict=None) -&gt; list[str]:\n    \"\"\"Print results from the run:\n        1. min y\n        2. min X\n        If `noise == True`, additionally the following values are printed:\n        3. min mean y\n        4. min mean X\n\n    Args:\n        print_screen (bool, optional):\n            print results to screen\n\n    Returns:\n        output (list):\n            list of results\n    \"\"\"\n    output = []\n    if print_screen:\n        print(f\"min y: {self.min_y}\")\n        if self.noise:\n            print(f\"min mean y: {self.min_mean_y}\")\n    if self.noise:\n        res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n    else:\n        res = self.to_all_dim(self.min_X.reshape(1, -1))\n    for i in range(res.shape[1]):\n        if self.all_var_name is None:\n            var_name = \"x\" + str(i)\n        else:\n            var_name = self.all_var_name[i]\n            var_type = self.all_var_type[i]\n            if var_type == \"factor\" and dict is not None:\n                val = get_ith_hyperparameter_name_from_fun_control(fun_control=dict, key=var_name, i=int(res[0][i]))\n            else:\n                val = res[0][i]\n        if print_screen:\n            print(var_name + \":\", val)\n        output.append([var_name, val])\n    return output\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.process_z00","title":"<code>process_z00(z00, use_min=True)</code>","text":"<p>Process each entry in the <code>z00</code> array according to the corresponding type in the <code>self.var_type</code> list. Specifically, if the type is \u201cfloat\u201d, the function will calculate the mean of the two <code>z00</code> values. If the type is not \u201cfloat\u201d, the function will retrun the maximum of the two <code>z00</code> values.</p> <p>Parameters:</p> Name Type Description Default <code>z00</code> <code>ndarray</code> <p>Array of values to process.</p> required <code>use_min</code> <code>bool</code> <p>If <code>True</code>, the minimum value is returned. If <code>False</code>, the maximum value is returned.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> <p>Processed values.</p> <p>Examples:</p> <p>from spotpython.spot import spot import numpy as np import random z00 = np.array([[1, 2, 3, 4], [5, 6, 7, 8]]) spot.var_type = [\u201cfloat\u201d, \u201cint\u201d, \u201cint\u201d, \u201cfloat\u201d] spot.process_z00(z00) [3, 6, 7, 6]</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def process_z00(self, z00, use_min=True) -&gt; list:\n    \"\"\"Process each entry in the `z00` array according to the corresponding type\n    in the `self.var_type` list.\n    Specifically, if the type is \"float\", the function will calculate the mean of the two `z00` values.\n    If the type is not \"float\", the function will retrun the maximum of the two `z00` values.\n\n    Args:\n        z00 (numpy.ndarray):\n            Array of values to process.\n        use_min (bool):\n            If `True`, the minimum value is returned. If `False`, the maximum value is returned.\n\n    Returns:\n        (list): Processed values.\n\n    Examples:\n        from spotpython.spot import spot\n        import numpy as np\n        import random\n        z00 = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n        spot.var_type = [\"float\", \"int\", \"int\", \"float\"]\n        spot.process_z00(z00)\n        [3, 6, 7, 6]\n\n    \"\"\"\n    result = []\n    for i in range(len(self.var_type)):\n        if self.var_type[i] == \"float\":\n            mean_value = np.mean(z00[:, i])\n            result.append(mean_value)\n        else:  # var_type[i] == 'int'\n            if use_min:\n                min_value = min(z00[:, i])\n                result.append(min_value)\n            else:\n                max_value = max(z00[:, i])\n                result.append(max_value)\n    return result\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.run","title":"<code>run(X_start=None)</code>","text":"<p>Run the surrogate based optimization. The optimization process is controlled by the following steps:     1. Initialize design     2. Update stats     3. Fit surrogate     4. Update design     5. Update stats     6. Update writer     7. Fit surrogate     8. Show progress if needed</p> <p>Parameters:</p> Name Type Description Default <code>X_start</code> <code>ndarray</code> <p>initial design. The initial design must have shape (n, k), where n is the number of points and k is the number of dimensions. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Spot</code> <code>Spot</code> <p>The <code>Spot</code> instance configured and updated based on the optimization process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import Spot\n    from spotpython.utils.init import (\n        fun_control_init, design_control_init\n        )\n    # number of initial points:\n    ni = 7\n    # start point X_0\n    X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    fun = Analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1]))\n    design_control=design_control_init(init_size=ni)\n    S = Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    S.run(X_start=X_start)\n        spotpython tuning: 0.0 [########--] 80.00%\n        spotpython tuning: 0.0 [#########-] 86.67%\n        spotpython tuning: 0.0 [#########-] 93.33%\n        spotpython tuning: 0.0 [##########] 100.00% Done...\n&gt;&gt;&gt; S.print_results()\n    min y: 0.0\n    x0: 0.0\n    x1: 0.0\n&gt;&gt;&gt; S.X\n    array([[ 0.0000000000000000e+00,  0.0000000000000000e+00],\n    [ 0.0000000000000000e+00,  1.0000000000000000e+00],\n    [ 1.0000000000000000e+00,  0.0000000000000000e+00],\n    [ 1.0000000000000000e+00,  1.0000000000000000e+00],\n    [-9.0924338949946959e-01, -1.5823457680063502e-01],\n    [-2.0581710650646035e-01, -4.8124908877104844e-01],\n    [ 9.4974117111856260e-01, -9.4631271618736390e-01],\n    [-1.2095571372201608e-01,  6.3835886343683867e-02],\n    [-6.6278701759800063e-01,  1.7431637339680406e-01],\n    [ 2.8200844136299108e-01,  9.3001011398034406e-01],\n    [ 4.7878811540073962e-01,  6.5321058189282999e-01],\n    [ 1.5404061268479530e-04,  4.1895410759355553e-03],\n    [-1.7027205448129213e-04,  4.7698567182254507e-03],\n    [-4.4080972128058849e-04,  5.2785168039883147e-03],\n    [ 3.7700880321788425e-03,  1.8909833144458731e-02]])\n&gt;&gt;&gt; S.y\n    array([0.0000000000000000e+00, 1.0000000000000000e+00,\n    1.0000000000000000e+00, 2.0000000000000000e+00,\n    8.5176172264376016e-01, 2.7396136677365612e-01,\n    1.7975160489355650e+00, 1.8705305067286033e-02,\n    4.6967282873066640e-01, 9.4444757310571603e-01,\n    6.5592212374576153e-01, 1.7575982937307560e-05,\n    2.2780525684937748e-05, 2.8057052860362483e-05,\n    3.7179535332164810e-04])\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def run(self, X_start: np.ndarray = None) -&gt; Spot:\n    \"\"\"\n    Run the surrogate based optimization.\n    The optimization process is controlled by the following steps:\n        1. Initialize design\n        2. Update stats\n        3. Fit surrogate\n        4. Update design\n        5. Update stats\n        6. Update writer\n        7. Fit surrogate\n        8. Show progress if needed\n\n    Args:\n        X_start (numpy.ndarray, optional):\n            initial design. The initial design must have shape (n, k), where n is the number of points and k is the number of dimensions. Defaults to None.\n\n    Returns:\n        Spot: The `Spot` instance configured and updated based on the optimization process.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import Spot\n            from spotpython.utils.init import (\n                fun_control_init, design_control_init\n                )\n            # number of initial points:\n            ni = 7\n            # start point X_0\n            X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n            fun = Analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1]))\n            design_control=design_control_init(init_size=ni)\n            S = Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,)\n            S.run(X_start=X_start)\n                spotpython tuning: 0.0 [########--] 80.00%\n                spotpython tuning: 0.0 [#########-] 86.67%\n                spotpython tuning: 0.0 [#########-] 93.33%\n                spotpython tuning: 0.0 [##########] 100.00% Done...\n        &gt;&gt;&gt; S.print_results()\n            min y: 0.0\n            x0: 0.0\n            x1: 0.0\n        &gt;&gt;&gt; S.X\n            array([[ 0.0000000000000000e+00,  0.0000000000000000e+00],\n            [ 0.0000000000000000e+00,  1.0000000000000000e+00],\n            [ 1.0000000000000000e+00,  0.0000000000000000e+00],\n            [ 1.0000000000000000e+00,  1.0000000000000000e+00],\n            [-9.0924338949946959e-01, -1.5823457680063502e-01],\n            [-2.0581710650646035e-01, -4.8124908877104844e-01],\n            [ 9.4974117111856260e-01, -9.4631271618736390e-01],\n            [-1.2095571372201608e-01,  6.3835886343683867e-02],\n            [-6.6278701759800063e-01,  1.7431637339680406e-01],\n            [ 2.8200844136299108e-01,  9.3001011398034406e-01],\n            [ 4.7878811540073962e-01,  6.5321058189282999e-01],\n            [ 1.5404061268479530e-04,  4.1895410759355553e-03],\n            [-1.7027205448129213e-04,  4.7698567182254507e-03],\n            [-4.4080972128058849e-04,  5.2785168039883147e-03],\n            [ 3.7700880321788425e-03,  1.8909833144458731e-02]])\n        &gt;&gt;&gt; S.y\n            array([0.0000000000000000e+00, 1.0000000000000000e+00,\n            1.0000000000000000e+00, 2.0000000000000000e+00,\n            8.5176172264376016e-01, 2.7396136677365612e-01,\n            1.7975160489355650e+00, 1.8705305067286033e-02,\n            4.6967282873066640e-01, 9.4444757310571603e-01,\n            6.5592212374576153e-01, 1.7575982937307560e-05,\n            2.2780525684937748e-05, 2.8057052860362483e-05,\n            3.7179535332164810e-04])\n\n    \"\"\"\n    #\n    PREFIX = self.fun_control[\"PREFIX\"]\n    filename = get_result_filename(PREFIX)\n    if os.path.exists(filename) and not self.fun_control.get(\"force_run\"):\n        # print a warning and load the result\n        print(f\"Result file {filename} exists. Loading the result.\")\n        S = load_result(filename=filename)\n        self._copy_from(S)\n        return self\n    else:\n        self.initialize_design(X_start)\n        self.update_stats()\n        self.fit_surrogate()\n        if self.verbosity &gt; 0:\n            print(\"\\n------ Starting optimization on the Surrogate for the given Budget ------\\n\")\n        timeout_start = time.time()\n        while self.should_continue(timeout_start):\n            self.update_design()\n            self.update_stats()\n            self.update_writer()\n            self.fit_surrogate()\n            self.show_progress_if_needed(timeout_start)\n\n        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n            self.spot_writer.flush()\n            self.spot_writer.close()\n        if self.fun_control.get(\"db_dict_name\") is not None:\n            self._write_db_dict()\n\n        if self.fun_control.get(\"save_result\"):\n            self.save_result(verbosity=self.verbosity)\n        return self\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.save_experiment","title":"<code>save_experiment(filename=None, path=None, overwrite=True, unpickleables='file_io', verbosity=0)</code>","text":"<p>Save the experiment to a file. If no filename is provided, the filename is generated based on the PREFIX using the <code>get_experiment_filename()</code> function. The experiment file is saved in the current working directory unless a path is provided. The file is saved in pickle format using the highest protocol. If no arguments are provided, the file is saved with the default name PREFIX + \u201c_exp.pkl\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename of the experiment file. If not provided, the filename is generated based on the PREFIX using the <code>get_experiment_filename()</code> function. Default is <code>None</code>.</p> <code>None</code> <code>path</code> <code>str</code> <p>The path to the experiment file. If not provided, the file is saved in the current working directory. Default is <code>None</code>.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If <code>True</code>, the file will be overwritten if it already exists. Default is <code>True</code>.</p> <code>True</code> <code>unpickleables</code> <code>str</code> <p>The type of unpickleable components to exclude. Default is \u201cfile_io\u201d, which excludes file I/O components like the spot_writer and logger. If set to any other value, the function will exclude the function, optimizer, surrogate, data_set, scaler, rng, and design components. Default is \u201cfile_io\u201d.</p> <code>'file_io'</code> <code>verbosity</code> <code>int</code> <p>The level of verbosity. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def save_experiment(self, filename=None, path=None, overwrite=True, unpickleables=\"file_io\", verbosity=0) -&gt; None:\n    \"\"\"\n    Save the experiment to a file.\n    If no filename is provided, the filename is generated based on the PREFIX using the\n    `get_experiment_filename()` function. The experiment file is saved in the current working directory\n    unless a path is provided. The file is saved in pickle format using the highest protocol.\n    If no arguments are provided, the file is saved with the default name PREFIX + \"_exp.pkl\".\n\n    Args:\n        filename (str):\n            The filename of the experiment file. If not provided,\n            the filename is generated based on the PREFIX using the\n            `get_experiment_filename()` function. Default is `None`.\n        path (str):\n            The path to the experiment file. If not provided, the file\n            is saved in the current working directory. Default is `None`.\n        overwrite (bool):\n            If `True`, the file will be overwritten if it already exists.\n            Default is `True`.\n        unpickleables (str):\n            The type of unpickleable components to exclude. Default is \"file_io\", which\n            excludes file I/O components like the spot_writer and logger.\n            If set to any other value, the function will exclude the function, optimizer,\n            surrogate, data_set, scaler, rng, and design components.\n            Default is \"file_io\".\n        verbosity (int):\n            The level of verbosity. Default is 0.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure we don't accidentally try to pickle unpicklable components\n    self._close_and_del_spot_writer()\n    self._remove_logger_handlers()\n\n    S = self._get_pickle_safe_spot_tuner(unpickleables=unpickleables, verbosity=verbosity)\n\n    # Determine the filename based on PREFIX if not provided\n    PREFIX = self.fun_control.get(\"PREFIX\", \"experiment\")\n    if filename is None:\n        filename = get_experiment_filename(PREFIX)\n\n    if path is not None:\n        filename = os.path.join(path, filename)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n    # Check if the file already exists\n    if filename is not None and os.path.exists(filename) and not overwrite:\n        print(f\"Error: File {filename} already exists. Use overwrite=True to overwrite the file.\")\n        return\n\n    # Serialize the experiment dictionary to the pickle file\n    if filename is not None:\n        with open(filename, \"wb\") as handle:\n            try:\n                pickle.dump(S, handle, protocol=pickle.HIGHEST_PROTOCOL)\n            except Exception as e:\n                print(f\"Error during pickling: {e}\")\n                raise e\n        print(f\"Experiment saved to {filename}\")\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.save_result","title":"<code>save_result(filename=None, path=None, overwrite=True, verbosity=0)</code>","text":"<p>Save the results to a file. If filename is not provided, the filename is generated based on the PREFIX using the <code>get_result_filename()</code> function. The results file is saved in the current working directory unless a path is provided. The file is saved in pickle format using the highest protocol. If no arguments are provided, the file is saved with the default name PREFIX + \u201c_res.pkl\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename of the results file. If not provided, the filename is generated based on the PREFIX using the <code>get_result_filename()</code> function. Default is <code>None</code>.</p> <code>None</code> <code>path</code> <code>str</code> <p>The path to the results file. If not provided, the file is saved in the current working directory. Default is <code>None</code>.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If <code>True</code>, the file will be overwritten if it already exists. Default is <code>True</code>.</p> <code>True</code> <code>verbosity</code> <code>int</code> <p>The level of verbosity. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def save_result(self, filename=None, path=None, overwrite=True, verbosity=0) -&gt; None:\n    \"\"\"\n    Save the results to a file.\n    If filename is not provided, the filename is generated based on the PREFIX using the\n    `get_result_filename()` function. The results file is saved in the current working directory\n    unless a path is provided. The file is saved in pickle format using the highest protocol.\n    If no arguments are provided, the file is saved with the default name PREFIX + \"_res.pkl\".\n\n    Args:\n        filename (str):\n            The filename of the results file. If not provided,\n            the filename is generated based on the PREFIX using the\n            `get_result_filename()` function. Default is `None`.\n        path (str):\n            The path to the results file. If not provided, the file\n            is saved in the current working directory. Default is `None`.\n        overwrite (bool):\n            If `True`, the file will be overwritten if it already exists.\n            Default is `True`.\n        verbosity (int):\n            The level of verbosity. Default is 0.\n\n    Returns:\n        None\n    \"\"\"\n    PREFIX = self.fun_control.get(\"PREFIX\", \"result\")\n    if filename is None:\n        filename = get_result_filename(PREFIX)\n    self.save_experiment(filename=filename, path=path, overwrite=overwrite, unpickleables=\"file_io\", verbosity=verbosity)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.show_progress_if_needed","title":"<code>show_progress_if_needed(timeout_start)</code>","text":"<p>Show progress bar if <code>show_progress</code> is <code>True</code>. If self.progress_file is not <code>None</code>, the progress bar is saved in the file with the name <code>self.progress_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <code>timeout_start</code> <code>float</code> <p>start time</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def show_progress_if_needed(self, timeout_start) -&gt; None:\n    \"\"\"Show progress bar if `show_progress` is `True`. If\n    self.progress_file is not `None`, the progress bar is saved\n    in the file with the name `self.progress_file`.\n\n    Args:\n        self (object): Spot object\n        timeout_start (float): start time\n\n    Returns:\n        (NoneType): None\n    \"\"\"\n    if not self.show_progress:\n        return\n    if isfinite(self.fun_evals):\n        progress_bar(progress=self.counter / self.fun_evals, y=self.min_y, filename=self.progress_file)\n    else:\n        progress_bar(progress=(time.time() - timeout_start) / (self.max_time * 60), y=self.min_y, filename=self.progress_file)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.suggest_new_X","title":"<code>suggest_new_X()</code>","text":"<p>Compute <code>n_points</code> new infill points in natural units. These diffrent points are computed by the optimizer using increasing seed. The optimizer searches in the ranges from <code>lower_j</code> to <code>upper_j</code>. The method <code>infill()</code> is used as the objective function.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>n_points</code> infill points in natural units, each of dim k</p> Note <p>This is step (S-14a) in [bart21i].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.spot import Spot\n    from spotpython.fun import Analytical\n    from spotpython.utils.init import fun_control_init\n    nn = 3\n    fun_sphere = Analytical().fun_sphere\n    fun_control = fun_control_init(\n            lower = np.array([-1, -1]),\n            upper = np.array([1, 1]),\n            n_points=nn,\n            )\n    S = Spot(\n        fun=fun_sphere,\n        fun_control=fun_control,\n        )\n    S.X = S.design.scipy_lhd(\n        S.design_control[\"init_size\"], lower=S.lower, upper=S.upper\n    )\n    print(f\"S.X: {S.X}\")\n    S.y = S.fun(S.X)\n    print(f\"S.y: {S.y}\")\n    S.fit_surrogate()\n    X0 = S.suggest_new_X()\n    print(f\"X0: {X0}\")\n    assert X0.size == S.n_points * S.k\n    assert X0.ndim == 2\n    assert X0.shape[0] == nn\n    assert X0.shape[1] == 2\n    spot_1.X: [[ 0.86352963  0.7892358 ]\n                [-0.24407197 -0.83687436]\n                [ 0.36481882  0.8375811 ]\n                [ 0.415331    0.54468512]\n                [-0.56395091 -0.77797854]\n                [-0.90259409 -0.04899292]\n                [-0.16484832  0.35724741]\n                [ 0.05170659  0.07401196]\n                [-0.78548145 -0.44638164]\n                [ 0.64017497 -0.30363301]]\n    spot_1.y: [1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764\n    0.15480068 0.00815134 0.81623768 0.502017  ]\n    X0: [[0.00154544 0.003962  ]\n        [0.00165526 0.00410847]\n        [0.00165685 0.0039177 ]]\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def suggest_new_X(self) -&gt; np.array:\n    \"\"\"\n    Compute `n_points` new infill points in natural units.\n    These diffrent points are computed by the optimizer using increasing seed.\n    The optimizer searches in the ranges from `lower_j` to `upper_j`.\n    The method `infill()` is used as the objective function.\n\n    Returns:\n        (numpy.ndarray): `n_points` infill points in natural units, each of dim k\n\n    Note:\n        This is step (S-14a) in [bart21i].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.spot import Spot\n            from spotpython.fun import Analytical\n            from spotpython.utils.init import fun_control_init\n            nn = 3\n            fun_sphere = Analytical().fun_sphere\n            fun_control = fun_control_init(\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]),\n                    n_points=nn,\n                    )\n            S = Spot(\n                fun=fun_sphere,\n                fun_control=fun_control,\n                )\n            S.X = S.design.scipy_lhd(\n                S.design_control[\"init_size\"], lower=S.lower, upper=S.upper\n            )\n            print(f\"S.X: {S.X}\")\n            S.y = S.fun(S.X)\n            print(f\"S.y: {S.y}\")\n            S.fit_surrogate()\n            X0 = S.suggest_new_X()\n            print(f\"X0: {X0}\")\n            assert X0.size == S.n_points * S.k\n            assert X0.ndim == 2\n            assert X0.shape[0] == nn\n            assert X0.shape[1] == 2\n            spot_1.X: [[ 0.86352963  0.7892358 ]\n                        [-0.24407197 -0.83687436]\n                        [ 0.36481882  0.8375811 ]\n                        [ 0.415331    0.54468512]\n                        [-0.56395091 -0.77797854]\n                        [-0.90259409 -0.04899292]\n                        [-0.16484832  0.35724741]\n                        [ 0.05170659  0.07401196]\n                        [-0.78548145 -0.44638164]\n                        [ 0.64017497 -0.30363301]]\n            spot_1.y: [1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764\n            0.15480068 0.00815134 0.81623768 0.502017  ]\n            X0: [[0.00154544 0.003962  ]\n                [0.00165526 0.00410847]\n                [0.00165685 0.0039177 ]]\n    \"\"\"\n    # (S-14a) Optimization on the surrogate:\n    new_X = np.zeros([self.n_points, self.k], dtype=float)\n    optimizer_name = self.optimizer.__name__\n    optimizers = {\n        \"dual_annealing\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n        \"differential_evolution\": lambda: self.optimizer(\n            func=self.infill,\n            bounds=self.de_bounds,\n            maxiter=self.optimizer_control[\"max_iter\"],\n            seed=self.optimizer_control[\"seed\"],\n        ),\n        \"direct\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds, eps=1e-2),\n        \"shgo\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n        \"basinhopping\": lambda: self.optimizer(func=self.infill, x0=self.min_X, minimizer_kwargs={\"method\": \"Nelder-Mead\"}),\n        \"default\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n    }\n    for i in range(self.n_points):\n        self.optimizer_control[\"seed\"] = self.optimizer_control[\"seed\"] + i\n        result = optimizers.get(optimizer_name, optimizers[\"default\"])()\n        new_X[i][:] = result.x\n    return np.unique(new_X, axis=0)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.to_all_dim","title":"<code>to_all_dim(X0)</code>","text":"<p>Expands reduced-dimensional design points back to their full-dimensional representation by reinserting fixed values for dimensions that were removed during the dimensionality reduction process. When <code>to_red_dim()</code> is called, dimensions where the lower and upper bounds are equal are removed from the design points. <code>to_all_dim()</code> reverses this process by adding back these fixed dimensions with their respective fixed values.</p> <p>Parameters:</p> Name Type Description Default <code>X0</code> <code>ndarray</code> <p>reduced dimension design points</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>full dimension design points</p> Atributes <p>self.ident (numpy.ndarray):     array of boolean values indicating fixed dimensions self.all_lower (numpy.ndarray):     backup of the original lower bounds.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n    lower = np.array([-1, -1, 0, 0])\n    upper = np.array([1, -1, 0, 5])  # Second and third dimensions are fixed\n    fun_evals = 10\n    var_type = ['float', 'int', 'float', 'int']\n    var_name = ['x1', 'x2', 'x3', 'x4']\n    spot_instance = spot.Spot(\n        fun = Analytical().fun_sphere,\n        fun_control=fun_control_init(lower=lower, upper=upper, fun_evals=fun_evals)\n    )\n    X0 = np.array([[2.5, 3.5], [4.5, 5.5]])\n    X_full_dim = spot_instance.to_all_dim(X0)\n    print(X_full_dim)\n        [[ 2.5 -1.   0.   3.5]\n        [ 4.5 -1.   0.   5.5]]\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def to_all_dim(self, X0: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Expands reduced-dimensional design points back to their full-dimensional representation\n    by reinserting fixed values for dimensions that were removed during the dimensionality\n    reduction process.\n    When `to_red_dim()` is called, dimensions where the lower and upper bounds are equal are\n    removed from the design points. `to_all_dim()` reverses this process by adding back these fixed\n    dimensions with their respective fixed values.\n\n    Args:\n        X0 (numpy.ndarray): reduced dimension design points\n\n    Returns:\n        (numpy.ndarray): full dimension design points\n\n    Atributes:\n        self.ident (numpy.ndarray):\n            array of boolean values indicating fixed dimensions\n        self.all_lower (numpy.ndarray):\n            backup of the original lower bounds.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n            lower = np.array([-1, -1, 0, 0])\n            upper = np.array([1, -1, 0, 5])  # Second and third dimensions are fixed\n            fun_evals = 10\n            var_type = ['float', 'int', 'float', 'int']\n            var_name = ['x1', 'x2', 'x3', 'x4']\n            spot_instance = spot.Spot(\n                fun = Analytical().fun_sphere,\n                fun_control=fun_control_init(lower=lower, upper=upper, fun_evals=fun_evals)\n            )\n            X0 = np.array([[2.5, 3.5], [4.5, 5.5]])\n            X_full_dim = spot_instance.to_all_dim(X0)\n            print(X_full_dim)\n                [[ 2.5 -1.   0.   3.5]\n                [ 4.5 -1.   0.   5.5]]\n    \"\"\"\n    # Number of design points (samples):\n    n = X0.shape[0]\n    # Number of dimensions:\n    k = len(self.ident)\n    # Initialize full dimension design points:\n    X = np.zeros((n, k))\n    # The following code was modified in 0.20.5:\n    # Index for navigating X0's compressed dimension\n    reduced_index = 0\n    # Iterate through each dimension, reconstructing full dimensionality\n    for i in range(k):\n        if self.ident[i]:\n            # Assign fixed dimension values using stored lower bounds\n            X[:, i] = self.all_lower[i]\n        else:\n            # Assign variable dimension values from the reduced array\n            X[:, i] = X0[:, reduced_index]\n            # Move to the next variable dimension in the compact X0 array\n            reduced_index += 1\n    return X\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.to_all_dim_if_needed","title":"<code>to_all_dim_if_needed(X)</code>","text":"<p>Conditionally expand reduced-dimensional design points back to their full-dimensional representation, if dimensionality reduction was performed. This method checks whether dimensionality reduction occurred (i.e., whether some dimensions were fixed and thus removed). If so, it uses <code>to_all_dim()</code> to restore the full-dimensional representation by reinserting the fixed dimensions. Otherwise, it returns the input design points unaltered.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D numpy array of shape (n, m), where <code>n</code> is the number of samples, and <code>m</code>             corresponds to the reduced or full number of dimensions depending on the             <code>red_dim</code> status.</p> required <p>Returns:</p> Type Description <code>array</code> <p>np.ndarray: A 2D numpy array of shape (n, k). If <code>red_dim</code> is True, <code>k</code> will be the full number         of dimensions (including both fixed and variable). If <code>red_dim</code> is False, <code>k</code> is         identical to <code>m</code>.</p> <p>Attributes:</p> Name Type Description <code>self.red_dim</code> <code>bool</code> <p>A boolean attribute indicating if dimensionality was reduced                 (True if dimensions were reduced, False otherwise).</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def to_all_dim_if_needed(self, X: np.ndarray) -&gt; np.array:\n    \"\"\"\n    Conditionally expand reduced-dimensional design points back to their full-dimensional representation,\n    if dimensionality reduction was performed.\n    This method checks whether dimensionality reduction occurred (i.e., whether some dimensions were\n    fixed and thus removed). If so, it uses `to_all_dim()` to restore the full-dimensional representation\n    by reinserting the fixed dimensions. Otherwise, it returns the input design points unaltered.\n\n    Args:\n        X (np.ndarray): A 2D numpy array of shape (n, m), where `n` is the number of samples, and `m`\n                        corresponds to the reduced or full number of dimensions depending on the\n                        `red_dim` status.\n\n    Returns:\n        np.ndarray: A 2D numpy array of shape (n, k). If `red_dim` is True, `k` will be the full number\n                    of dimensions (including both fixed and variable). If `red_dim` is False, `k` is\n                    identical to `m`.\n\n    Attributes:\n        self.red_dim (bool): A boolean attribute indicating if dimensionality was reduced\n                            (True if dimensions were reduced, False otherwise).\n    \"\"\"\n\n    if self.red_dim:\n        return self.to_all_dim(X)\n    else:\n        return X\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.to_red_dim","title":"<code>to_red_dim()</code>","text":"<p>Reduces the dimensionality of the optimization problem by removing dimensions where lower and upper bounds are equal, indicating that those variables are fixed. This function modifies the lower bounds, upper bounds, variable types, and variable names by filtering out the non-varying dimensions. If any dimension is reduced, the <code>red_dim</code> attribute is set to True, and the count of dimensions (<code>k</code>) is updated accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Attributes:</p> Name Type Description <code>self.lower</code> <code>ndarray</code> <p>lower bound</p> <code>self.upper</code> <code>ndarray</code> <p>upper bound</p> <code>self.var_type</code> <code>List[str]</code> <p>list of variable types</p> <code>self.ident</code> <code>ndarray</code> <p>array of boolean values indicating fixed dimensions</p> <code>self.red_dim</code> <code>bool</code> <p>True if dimensions are reduced, False otherwise. Checks if any dimension is fixed.</p> <code>self.all_lower</code> <code>ndarray</code> <p>backup of the original lower bounds</p> <code>self.all_upper</code> <code>ndarray</code> <p>backup of the original upper bounds</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n    lower = np.array([-1, -1, 0, 0])\n    upper = np.array([1, -1, 0, 5])  # Second and third dimensions are fixed\n    fun_evals = 10\n    var_type = ['float', 'int', 'float', 'int']\n    var_name = ['x1', 'x2', 'x3', 'x4']\n    spot_instance = spot.Spot(\n        # Assuming Spot takes fun, fun_control, design_control as arguments\n        fun = Analytical().fun_sphere,  # Replace with appropriate function\n        fun_control=fun_control_init(lower=lower, upper=upper, fun_evals=fun_evals, show_progress=True, log_level=50),\n    )\n    spot_instance.var_type = var_type\n    spot_instance.var_name = var_name\n    spot_instance.to_red_dim()\n    # Assert: Check if dimensions were reduced correctly\n    assert spot_instance.lower.size == 2, \"Expected size of lower to be 2 after reduction\"\n    assert spot_instance.upper.size == 2, \"Expected size of upper to be 2 after reduction\"\n    assert len(spot_instance.var_type) == 2, \"Expected size of var_type to be 2 after reduction\"\n    assert spot_instance.k == 2, \"Expected k to reflect the reduced dimensions\"\n    # Check remaining values\n    expected_lower = np.array([-1, 0])\n    expected_upper = np.array([1, 5])\n    expected_var_type = ['float', 'int']\n    # there are two remaining variables, they are named x1 and x2\n    expected_var_name = ['x1', 'x2']\n    np.testing.assert_array_equal(spot_instance.lower, expected_lower)\n    np.testing.assert_array_equal(spot_instance.upper, expected_upper)\n    assert spot_instance.var_type == expected_var_type\n    assert spot_instance.var_name == expected_var_name\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def to_red_dim(self) -&gt; None:\n    \"\"\"\n    Reduces the dimensionality of the optimization problem by removing dimensions\n    where lower and upper bounds are equal, indicating that those variables are fixed.\n    This function modifies the lower bounds, upper bounds, variable types, and variable names\n    by filtering out the non-varying dimensions. If any dimension is reduced, the `red_dim` attribute\n    is set to True, and the count of dimensions (`k`) is updated accordingly.\n\n    Args:\n        self (object): Spot object\n\n    Returns:\n        (NoneType): None\n\n    Attributes:\n        self.lower (numpy.ndarray):\n            lower bound\n        self.upper (numpy.ndarray):\n            upper bound\n        self.var_type (List[str]):\n            list of variable types\n        self.ident (numpy.ndarray):\n            array of boolean values indicating fixed dimensions\n        self.red_dim (bool):\n            True if dimensions are reduced, False otherwise. Checks if any dimension is fixed.\n        self.all_lower (numpy.ndarray):\n            backup of the original lower bounds\n        self.all_upper (numpy.ndarray):\n            backup of the original upper bounds\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n            lower = np.array([-1, -1, 0, 0])\n            upper = np.array([1, -1, 0, 5])  # Second and third dimensions are fixed\n            fun_evals = 10\n            var_type = ['float', 'int', 'float', 'int']\n            var_name = ['x1', 'x2', 'x3', 'x4']\n            spot_instance = spot.Spot(\n                # Assuming Spot takes fun, fun_control, design_control as arguments\n                fun = Analytical().fun_sphere,  # Replace with appropriate function\n                fun_control=fun_control_init(lower=lower, upper=upper, fun_evals=fun_evals, show_progress=True, log_level=50),\n            )\n            spot_instance.var_type = var_type\n            spot_instance.var_name = var_name\n            spot_instance.to_red_dim()\n            # Assert: Check if dimensions were reduced correctly\n            assert spot_instance.lower.size == 2, \"Expected size of lower to be 2 after reduction\"\n            assert spot_instance.upper.size == 2, \"Expected size of upper to be 2 after reduction\"\n            assert len(spot_instance.var_type) == 2, \"Expected size of var_type to be 2 after reduction\"\n            assert spot_instance.k == 2, \"Expected k to reflect the reduced dimensions\"\n            # Check remaining values\n            expected_lower = np.array([-1, 0])\n            expected_upper = np.array([1, 5])\n            expected_var_type = ['float', 'int']\n            # there are two remaining variables, they are named x1 and x2\n            expected_var_name = ['x1', 'x2']\n            np.testing.assert_array_equal(spot_instance.lower, expected_lower)\n            np.testing.assert_array_equal(spot_instance.upper, expected_upper)\n            assert spot_instance.var_type == expected_var_type\n            assert spot_instance.var_name == expected_var_name\n    \"\"\"\n    # Backup of the original values:\n    self.all_lower = self.lower\n    self.all_upper = self.upper\n    # Select only lower != upper:\n    self.ident = (self.upper - self.lower) == 0\n    # Determine if dimension is reduced:\n    self.red_dim = self.ident.any()\n    # Modifications:\n    # Modify lower and upper:\n    self.lower = self.lower[~self.ident]\n    self.upper = self.upper[~self.ident]\n    # Modify k (number of dim):\n    self.k = self.lower.size\n    # Filter out types and names corresponding to non-varying dimensions\n    if self.var_type is not None:\n        self.all_var_type = self.var_type.copy()\n        self.var_type = [vtype for vtype, fixed in zip(self.all_var_type, self.ident) if not fixed]\n\n    if self.var_name is not None:\n        self.all_var_name = self.var_name.copy()\n        self.var_name = [vname for vname, fixed in zip(self.all_var_name, self.ident) if not fixed]\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.update_design","title":"<code>update_design()</code>","text":"<p>Update design. Generate and evaluate new design points. It is basically a call to the method <code>get_new_X0()</code>. If <code>noise</code> is <code>True</code>, additionally the following steps (from <code>get_X_ocba()</code>) are performed: 1. Compute OCBA points. 2. Evaluate OCBA points. 3. Append OCBA points to the new design points.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Attributes:</p> Name Type Description <code>self.X</code> <code>ndarray</code> <p>updated design</p> <code>self.y</code> <code>ndarray</code> <p>updated design values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # 1. Without OCBA points:\n&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.utils.init import (\n        fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n        )\n    from spotpython.spot import Spot\n    # number of initial points:\n    ni = 0\n    X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n    fun = Analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1, -1]),\n        upper = np.array([1, 1])\n        )\n    design_control=design_control_init(init_size=ni)\n    S = Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control,)\n    S.initialize_design(X_start=X_start)\n    print(f\"S.X: {S.X}\")\n    print(f\"S.y: {S.y}\")\n    X_shape_before = S.X.shape\n    print(f\"X_shape_before: {X_shape_before}\")\n    print(f\"y_size_before: {S.y.size}\")\n    y_size_before = S.y.size\n    S.update_stats()\n    S.fit_surrogate()\n    S.update_design()\n    print(f\"S.X: {S.X}\")\n    print(f\"S.y: {S.y}\")\n    print(f\"S.n_points: {S.n_points}\")\n    print(f\"X_shape_after: {S.X.shape}\")\n    print(f\"y_size_after: {S.y.size}\")\n&gt;&gt;&gt; #\n&gt;&gt;&gt; # 2. Using the OCBA points:\n    import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import Spot\n    from spotpython.utils.init import fun_control_init, design_control_init\n    # number of initial points:\n    ni = 3\n    X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n    fun = Analytical().fun_sphere\n    fun_control = fun_control_init(\n            sigma=0.02,\n            lower = np.array([-1, -1]),\n            upper = np.array([1, 1]),\n            noise=True,\n            ocba_delta=1,\n        )\n    design_control=design_control_init(init_size=ni, repeats=2)\n    S = Spot(fun=fun,\n                design_control=design_control,\n                fun_control=fun_control\n    )\n    S.initialize_design(X_start=X_start)\n    print(f\"S.X: {S.X}\")\n    print(f\"S.y: {S.y}\")\n    X_shape_before = S.X.shape\n    print(f\"X_shape_before: {X_shape_before}\")\n    print(f\"y_size_before: {S.y.size}\")\n    y_size_before = S.y.size\n    S.update_stats()\n    S.fit_surrogate()\n    S.update_design()\n    print(f\"S.X: {S.X}\")\n    print(f\"S.y: {S.y}\")\n    print(f\"S.n_points: {S.n_points}\")\n    print(f\"S.ocba_delta: {S.ocba_delta}\")\n    print(f\"X_shape_after: {S.X.shape}\")\n    print(f\"y_size_after: {S.y.size}\")\n    # compare the shapes of the X and y values before and after the update_design method\n    assert X_shape_before[0] + S.ocba_delta == S.X.shape[0]\n    assert X_shape_before[1] == S.X.shape[1]\n    assert y_size_before + S.ocba_delta == S.y.size\n    Experiment saved to 000_exp.pkl\n        S.X: [[ 0.          1.        ]\n        [ 1.          0.        ]\n        [ 1.          1.        ]\n        [ 1.          1.        ]\n        [ 0.54509876 -0.36921401]\n        [ 0.54509876 -0.36921401]\n        [ 0.18642675  0.87708546]\n        [ 0.18642675  0.87708546]\n        [-0.45060393 -0.208063  ]\n        [-0.45060393 -0.208063  ]]\n        S.y: [0.98021757 0.99264427 2.02575851 2.00387949 0.45185626 0.44499372\n        0.79130456 0.81487288 0.24000221 0.23988634]\n        X_shape_before: (10, 2)\n        y_size_before: 10\n        S.X: [[ 0.          1.        ]\n        [ 1.          0.        ]\n        [ 1.          1.        ]\n        [ 1.          1.        ]\n        [ 0.54509876 -0.36921401]\n        [ 0.54509876 -0.36921401]\n        [ 0.18642675  0.87708546]\n        [ 0.18642675  0.87708546]\n        [-0.45060393 -0.208063  ]\n        [-0.45060393 -0.208063  ]\n        [-0.02292587  0.0145145 ]]\n        S.y: [ 0.98021757  0.99264427  2.02575851  2.00387949  0.45185626  0.44499372\n        0.79130456  0.81487288  0.24000221  0.23988634 -0.01904616]\n        S.n_points: 1\n        S.ocba_delta: 1\n        X_shape_after: (11, 2)\n        y_size_after: 11\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def update_design(self) -&gt; None:\n    \"\"\"\n    Update design. Generate and evaluate new design points.\n    It is basically a call to the method `get_new_X0()`.\n    If `noise` is `True`, additionally the following steps\n    (from `get_X_ocba()`) are performed:\n    1. Compute OCBA points.\n    2. Evaluate OCBA points.\n    3. Append OCBA points to the new design points.\n\n    Args:\n        self (object): Spot object\n\n    Returns:\n        (NoneType): None\n\n    Attributes:\n        self.X (numpy.ndarray): updated design\n        self.y (numpy.ndarray): updated design values\n\n    Examples:\n        &gt;&gt;&gt; # 1. Without OCBA points:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.utils.init import (\n                fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n                )\n            from spotpython.spot import Spot\n            # number of initial points:\n            ni = 0\n            X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n            fun = Analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1, -1]),\n                upper = np.array([1, 1])\n                )\n            design_control=design_control_init(init_size=ni)\n            S = Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control,)\n            S.initialize_design(X_start=X_start)\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n            X_shape_before = S.X.shape\n            print(f\"X_shape_before: {X_shape_before}\")\n            print(f\"y_size_before: {S.y.size}\")\n            y_size_before = S.y.size\n            S.update_stats()\n            S.fit_surrogate()\n            S.update_design()\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n            print(f\"S.n_points: {S.n_points}\")\n            print(f\"X_shape_after: {S.X.shape}\")\n            print(f\"y_size_after: {S.y.size}\")\n        &gt;&gt;&gt; #\n        &gt;&gt;&gt; # 2. Using the OCBA points:\n            import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import Spot\n            from spotpython.utils.init import fun_control_init, design_control_init\n            # number of initial points:\n            ni = 3\n            X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n            fun = Analytical().fun_sphere\n            fun_control = fun_control_init(\n                    sigma=0.02,\n                    lower = np.array([-1, -1]),\n                    upper = np.array([1, 1]),\n                    noise=True,\n                    ocba_delta=1,\n                )\n            design_control=design_control_init(init_size=ni, repeats=2)\n            S = Spot(fun=fun,\n                        design_control=design_control,\n                        fun_control=fun_control\n            )\n            S.initialize_design(X_start=X_start)\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n            X_shape_before = S.X.shape\n            print(f\"X_shape_before: {X_shape_before}\")\n            print(f\"y_size_before: {S.y.size}\")\n            y_size_before = S.y.size\n            S.update_stats()\n            S.fit_surrogate()\n            S.update_design()\n            print(f\"S.X: {S.X}\")\n            print(f\"S.y: {S.y}\")\n            print(f\"S.n_points: {S.n_points}\")\n            print(f\"S.ocba_delta: {S.ocba_delta}\")\n            print(f\"X_shape_after: {S.X.shape}\")\n            print(f\"y_size_after: {S.y.size}\")\n            # compare the shapes of the X and y values before and after the update_design method\n            assert X_shape_before[0] + S.ocba_delta == S.X.shape[0]\n            assert X_shape_before[1] == S.X.shape[1]\n            assert y_size_before + S.ocba_delta == S.y.size\n            Experiment saved to 000_exp.pkl\n                S.X: [[ 0.          1.        ]\n                [ 1.          0.        ]\n                [ 1.          1.        ]\n                [ 1.          1.        ]\n                [ 0.54509876 -0.36921401]\n                [ 0.54509876 -0.36921401]\n                [ 0.18642675  0.87708546]\n                [ 0.18642675  0.87708546]\n                [-0.45060393 -0.208063  ]\n                [-0.45060393 -0.208063  ]]\n                S.y: [0.98021757 0.99264427 2.02575851 2.00387949 0.45185626 0.44499372\n                0.79130456 0.81487288 0.24000221 0.23988634]\n                X_shape_before: (10, 2)\n                y_size_before: 10\n                S.X: [[ 0.          1.        ]\n                [ 1.          0.        ]\n                [ 1.          1.        ]\n                [ 1.          1.        ]\n                [ 0.54509876 -0.36921401]\n                [ 0.54509876 -0.36921401]\n                [ 0.18642675  0.87708546]\n                [ 0.18642675  0.87708546]\n                [-0.45060393 -0.208063  ]\n                [-0.45060393 -0.208063  ]\n                [-0.02292587  0.0145145 ]]\n                S.y: [ 0.98021757  0.99264427  2.02575851  2.00387949  0.45185626  0.44499372\n                0.79130456  0.81487288  0.24000221  0.23988634 -0.01904616]\n                S.n_points: 1\n                S.ocba_delta: 1\n                X_shape_after: (11, 2)\n                y_size_after: 11\n    \"\"\"\n    # OCBA (only if noise). Determination of the OCBA points depends on the\n    # old X and y values.\n    if self.noise and self.ocba_delta &gt; 0 and not np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &lt;= 2):\n        logger.warning(\"self.var_y &lt;= 0. OCBA points are not generated:\")\n        logger.warning(\"There are less than 3 points or points with no variance information.\")\n        logger.debug(\"In update_design(): self.mean_X: %s\", self.mean_X)\n        logger.debug(\"In update_design(): self.var_y: %s\", self.var_y)\n    if self.noise and self.ocba_delta &gt; 0 and np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &gt; 2):\n        X_ocba = get_ocba_X(self.mean_X, self.mean_y, self.var_y, self.ocba_delta)\n    else:\n        X_ocba = None\n    # Determine the new X0 values based on the old X and y values:\n    X0 = self.get_new_X0()\n    # Append OCBA points to the new design points:\n    if self.noise and self.ocba_delta &gt; 0 and np.all(self.var_y &gt; 0):\n        X0 = append(X_ocba, X0, axis=0)\n    X_all = self.to_all_dim_if_needed(X0)\n    logger.debug(\n        \"In update_design(): self.fun_control sigma and seed passed to fun(): %s %s\",\n        self.fun_control[\"sigma\"],\n        self.fun_control[\"seed\"],\n    )\n    # (S-18): Evaluating New Solutions:\n    y_mo = self.fun(X=X_all, fun_control=self.fun_control)\n    # Convert multi-objective values to single-objective values:\n    y0 = self._mo2so(y_mo)\n    # Apply penalty for NA values works only on so values:\n    y0 = apply_penalty_NA(y0, self.fun_control[\"penalty_NA\"], verbosity=self.verbosity)\n    X0, y0 = remove_nan(X0, y0, stop_on_zero_return=False)\n    # Append New Solutions (only if they are not nan):\n    if y0.shape[0] &gt; 0:\n        self.X = np.append(self.X, X0, axis=0)\n        self.y = np.append(self.y, y0)\n    else:\n        # otherwise, generate a random point and append it to the design\n        Xr, yr = self.generate_random_point()\n        self.X = np.append(self.X, Xr, axis=0)\n        self.y = np.append(self.y, yr)\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.update_stats","title":"<code>update_stats()</code>","text":"<p>Update the following stats: 1. <code>min_y</code>, 2. <code>min_X</code>, and 3. <code>counter</code> If <code>noise</code> is <code>True</code>, additionally the following stats are computed: 1. <code>mean_X</code>,  2. <code>mean_y</code>,  3. <code>var_y</code>, 4. <code>min_mean_X</code>(X value of the best mean y value so far), 5. <code>min_mean_y</code> (best mean y value so far), and 6. <code>min_var_y</code> (ariance of the best mean y value so far).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>Spot object</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Attributes:</p> Name Type Description <code>self.min_y</code> <code>float</code> <p>minimum y value</p> <code>self.min_X</code> <code>ndarray</code> <p>X value of the minimum y value</p> <code>self.counter</code> <code>int</code> <p>number of function evaluations</p> <code>self.mean_X</code> <code>ndarray</code> <p>mean X values</p> <code>self.mean_y</code> <code>ndarray</code> <p>mean y values</p> <code>self.var_y</code> <code>ndarray</code> <p>variance of y values</p> <code>self.min_mean_y</code> <code>float</code> <p>minimum mean y value</p> <code>self.min_mean_X</code> <code>ndarray</code> <p>X value of the minimum mean y value</p> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def update_stats(self) -&gt; None:\n    \"\"\"\n    Update the following stats:\n    1. `min_y`, 2. `min_X`, and 3. `counter`\n    If `noise` is `True`, additionally the following stats are computed:\n    1. `mean_X`,  2. `mean_y`,  3. `var_y`, 4. `min_mean_X`(X value of the best mean y value so far),\n    5. `min_mean_y` (best mean y value so far), and 6. `min_var_y` (ariance of the best mean y value so far).\n\n    Args:\n        self (object): Spot object\n\n    Returns:\n        (NoneType): None\n\n    Attributes:\n        self.min_y (float): minimum y value\n        self.min_X (numpy.ndarray): X value of the minimum y value\n        self.counter (int): number of function evaluations\n        self.mean_X (numpy.ndarray): mean X values\n        self.mean_y (numpy.ndarray): mean y values\n        self.var_y (numpy.ndarray): variance of y values\n        self.min_mean_y (float): minimum mean y value\n        self.min_mean_X (numpy.ndarray): X value of the minimum mean y value\n\n    \"\"\"\n    self.min_y = min(self.y)\n    self.min_X = self.X[argmin(self.y)]\n    self.counter = self.y.size\n    self.fun_control.update({\"counter\": self.counter})\n    # Update aggregated x and y values (if noise):\n    if self.noise:\n        Z = aggregate_mean_var(X=self.X, y=self.y)\n        self.mean_X = Z[0]\n        self.mean_y = Z[1]\n        self.var_y = Z[2]\n        # X value of the best mean y value so far:\n        self.min_mean_X = self.mean_X[argmin(self.mean_y)]\n        # best mean y value so far:\n        self.min_mean_y = self.mean_y[argmin(self.mean_y)]\n        # variance of the best mean y value so far:\n        self.min_var_y = self.var_y[argmin(self.mean_y)]\n</code></pre>"},{"location":"reference/spotpython/spot/spot/#spotpython.spot.spot.Spot.write_initial_tensorboard_log","title":"<code>write_initial_tensorboard_log()</code>","text":"<p>Writes initial design data using the spot_writer. The spot_writer is a tensorboard writer that writes the data to a tensorboard file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init\n    fun = Analytical().fun_sphere\n    fun_control = fun_control_init(\n        lower = np.array([-1]),\n        upper = np.array([1])\n        )\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                )\n    S.initialize_design()\n    S.write_initial_tensorboard_log()\n        Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_01_12_09_24_15\n        Created spot_tensorboard_path: runs/spot_logs/00_p040025_2025-01-12_09-24-15 for SummaryWriter()\n</code></pre> Source code in <code>spotpython/spot/spot.py</code> <pre><code>def write_initial_tensorboard_log(self) -&gt; None:\n    \"\"\"Writes initial design data using the spot_writer. The spot_writer\n    is a tensorboard writer that writes the data to a tensorboard file.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init\n            fun = Analytical().fun_sphere\n            fun_control = fun_control_init(\n                lower = np.array([-1]),\n                upper = np.array([1])\n                )\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        )\n            S.initialize_design()\n            S.write_initial_tensorboard_log()\n                Moving TENSORBOARD_PATH: runs/ to TENSORBOARD_PATH_OLD: runs_OLD/runs_2025_01_12_09_24_15\n                Created spot_tensorboard_path: runs/spot_logs/00_p040025_2025-01-12_09-24-15 for SummaryWriter()\n    \"\"\"\n    if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n        # range goes to init_size -1 because the last value is added by update_stats(),\n        # which always adds the last value.\n        # Changed in 0.5.9:\n        for j in range(len(self.y)):\n            X_j = self.X[j].copy()\n            y_j = self.y[j].copy()\n            config = {self.var_name[i]: X_j[i] for i in range(self.k)}\n            # var_dict = assign_values(X, get_var_name(fun_control))\n            # config = list(generate_one_config_from_var_dict(var_dict, fun_control))[0]\n            # see: https://github.com/pytorch/pytorch/issues/32651\n            # self.spot_writer.add_hparams(config, {\"spot_y\": y_j}, run_name=self.spot_tensorboard_path)\n            self.spot_writer.add_hparams(config, {\"hp_metric\": y_j})\n            self.spot_writer.flush()\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/","title":"kriging","text":""},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.Kriging","title":"<code>Kriging</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A scikit-learn compatible Kriging model class for regression tasks. Provides methods for likelihood evaluation, predictions, and hyperparameter optimization.</p> <p>Attributes:</p> Name Type Description <code>eps</code> <code>float</code> <p>A small regularization term to reduce ill-conditioning.</p> <code>penalty</code> <code>float</code> <p>The penalty value used if the correlation matrix is ill-conditioned.</p> <code>logtheta_lambda_</code> <code>ndarray</code> <p>Best-fit log(theta) parameters from fit().</p> <code>U_</code> <code>ndarray</code> <p>The Cholesky factor of the correlation matrix after fit().</p> <code>X_</code> <code>ndarray</code> <p>The training input data (n x d).</p> <code>y_</code> <code>ndarray</code> <p>The training target values (n,).</p> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>class Kriging(BaseEstimator, RegressorMixin):\n    \"\"\"\n    A scikit-learn compatible Kriging model class for regression tasks.\n    Provides methods for likelihood evaluation, predictions, and hyperparameter optimization.\n\n    Attributes:\n        eps (float): A small regularization term to reduce ill-conditioning.\n        penalty (float): The penalty value used if the correlation matrix is ill-conditioned.\n        logtheta_lambda_ (np.ndarray): Best-fit log(theta) parameters from fit().\n        U_ (np.ndarray): The Cholesky factor of the correlation matrix after fit().\n        X_ (np.ndarray): The training input data (n x d).\n        y_ (np.ndarray): The training target values (n,).\n    \"\"\"\n\n    def __init__(\n        self,\n        eps: float = None,\n        penalty: float = 1e4,\n        method=\"regression\",\n        noise: bool = False,\n        var_type: List[str] = [\"num\"],\n        name: str = \"Kriging\",\n        seed: int = 124,\n        model_optimizer=None,\n        model_fun_evals: Optional[int] = None,\n        min_theta: float = -3.0,\n        max_theta: float = 2.0,\n        n_theta: int = 1,\n        theta_init_zero: bool = False,\n        p_val: float = 2.0,\n        n_p: int = 1,\n        optim_p: bool = False,\n        min_Lambda: float = 1e-9,\n        max_Lambda: float = 1.0,\n        log_level: int = 50,\n        spot_writer=None,\n        counter=None,\n        metric_factorial=\"canberra\",\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the Kriging model.\n\n        Args:\n            eps (float, optional):\n                Small number added to the diagonal of the correlation matrix to reduce\n                ill-conditioning. Defaults to the square root of machine epsilon.\n                Only used if method is \"interpolation\". Otherwise, if method is \"regression\" or \"reinterpolation\", eps is replaced by the\n                lambda_ parameter. Defaults to None.\n            penalty (float, optional):\n                Large negative log-likelihood assigned if the correlation matrix is\n                not positive-definite. Defaults to 1e4.\n            method (str, optional):\n                The type how the model uis fitted. Can be \"interpolation\", \"regression\", or \"reinterpolation\". Defaults to \"regression\".\n        \"\"\"\n        if eps is None:\n            self.eps = self._get_eps()\n        else:\n            # check if eps is positive\n            if eps &lt;= 0:\n                raise ValueError(\"eps must be positive\")\n            self.eps = eps\n        self.penalty = penalty\n\n        self.noise = noise\n        self.var_type = var_type\n        self.name = name\n        self.seed = seed\n        self.log_level = log_level\n        self.spot_writer = spot_writer\n        self.counter = counter\n        self.metric_factorial = metric_factorial\n        self.min_theta = min_theta\n        self.max_theta = max_theta\n        self.min_Lambda = min_Lambda\n        self.max_Lambda = max_Lambda\n        self.n_theta = n_theta\n        self.p_val = p_val\n        self.n_p = n_p\n        self.optim_p = optim_p\n        self.theta_init_zero = theta_init_zero\n        self.model_optimizer = model_optimizer\n        if self.model_optimizer is None:\n            self.model_optimizer = differential_evolution\n        self.model_fun_evals = model_fun_evals\n        if self.model_fun_evals is None:\n            self.model_fun_evals = 100\n\n        # Logging information\n        self.log = {}\n        self.log[\"negLnLike\"] = []\n        self.log[\"theta\"] = []\n        self.log[\"p\"] = []\n        self.log[\"Lambda\"] = []\n\n        self.logtheta_lambda_ = None\n        self.U_ = None\n        self.X_ = None\n        self.y_ = None\n        self.negLnLike = None\n        self.Psi_ = None\n        if method not in [\"interpolation\", \"regression\", \"reinterpolation\"]:\n            raise ValueError(\"method must be one of 'interpolation', 'regression', or 'reinterpolation']\")\n        self.method = method\n        self.return_ei = False\n        self.return_std = False\n\n    def _get_eps(self) -&gt; float:\n        \"\"\"\n        Returns the square root of the machine epsilon.\n        \"\"\"\n        eps = np.finfo(float).eps\n        return np.sqrt(eps)\n\n    def get_model_params(self) -&gt; Dict[str, float]:\n        \"\"\"\n        Get the model parameters (in addition to sklearn's get_params method).\n\n        This method is NOT required for scikit-learn compatibility.\n\n        Returns:\n            dict: Parameter names not included in get_params() mapped to their values.\n        \"\"\"\n        return {\"log_theta_lambda\": self.logtheta_lambda_, \"U\": self.U_, \"X\": self.X_, \"y\": self.y_, \"negLnLike\": self.negLnLike}\n\n    def _update_log(self) -&gt; None:\n        \"\"\"\n        If spot_writer is not None, this method writes the current values of\n        negLnLike, theta, p (if optim_p is True),\n        and Lambda (if method is not \"interpolation\") to the spot_writer object.\n\n        Args:\n            self (object): The Kriging object.\n\n        Returns:\n            None\n\n        \"\"\"\n        self.log[\"negLnLike\"] = append(self.log[\"negLnLike\"], self.negLnLike)\n        self.log[\"theta\"] = append(self.log[\"theta\"], self.theta)\n        if self.optim_p:\n            self.log[\"p\"] = append(self.log[\"p\"], self.p)\n        if (self.method == \"regression\") or (self.method == \"reinterpolation\"):\n            self.log[\"Lambda\"] = append(self.log[\"Lambda\"], self.Lambda)\n        # get the length of the log\n        self.log_length = len(self.log[\"negLnLike\"])\n        if self.spot_writer is not None:\n            negLnLike = self.negLnLike.copy()\n            self.spot_writer.add_scalar(\"spot_negLnLike\", negLnLike, self.counter + self.log_length)\n            # add the self.n_theta theta values to the writer with one key \"theta\",\n            # i.e, the same key for all theta values\n            theta = self.theta.copy()\n            self.spot_writer.add_scalars(\"spot_theta\", {f\"theta_{i}\": theta[i] for i in range(self.n_theta)}, self.counter + self.log_length)\n            if (self.method == \"regression\") or (self.method == \"reinterpolation\"):\n                Lambda = self.Lambda.copy()\n                self.spot_writer.add_scalar(\"spot_Lambda\", Lambda, self.counter + self.log_length)\n            if self.optim_p:\n                p = self.p.copy()\n                self.spot_writer.add_scalars(\"spot_p\", {f\"p_{i}\": p[i] for i in range(self.n_p)}, self.counter + self.log_length)\n            self.spot_writer.flush()\n\n    def fit(self, X: np.ndarray, y: np.ndarray, bounds: Optional[List[Tuple[float, float]]] = None) -&gt; \"Kriging\":\n        \"\"\"\n        Fits the Kriging model to training data X and y. This method is compatible\n        with scikit-learn and uses differential evolution to optimize the hyperparameters\n        (log(theta)).\n\n        Args:\n            X (np.ndarray):\n                Training input data of shape (n_samples, n_features).\n            y (np.ndarray):\n                Target values of shape (n_samples,) or (n_samples, 1).\n            bounds (Optional[List[Tuple[float, float]]]):\n                Bounds for each dimension of log(theta). If None, defaults to [(-3, 2)] * n_features.\n\n        Returns:\n            Kriging:\n                The fitted Kriging model instance (self).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging\n            &gt;&gt;&gt; # Training data\n            &gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n            &gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n            &gt;&gt;&gt; # Initialize and fit the Kriging model\n            &gt;&gt;&gt; model = Kriging()\n            &gt;&gt;&gt; model.fit(X_train, y_train)\n            &gt;&gt;&gt; print(\"Fitted log(theta):\", model.logtheta_lambda_)\n        \"\"\"\n        X = np.asarray(X)\n        y = np.asarray(y).flatten()\n        self.X_ = X\n        self.y_ = y\n        self.n, self.k = X.shape\n        # Calculate and store min and max of X\n        self.min_X = np.min(self.X_, axis=0)\n        self.max_X = np.max(self.X_, axis=0)\n\n        _, aggregated_mean_y, _ = aggregate_mean_var(X=self.X_, y=self.y_)\n        self.aggregated_mean_y = np.copy(aggregated_mean_y)\n        if bounds is None:\n            if self.method == \"interpolation\":\n                bounds = [(-3.0, 2.0)] * self.k\n            else:\n                # regression and reinterpolation use lambda_ as well\n                bounds = [(-3.0, 2.0)] * self.k + [(-6.0, 0.0)]\n\n        self.logtheta_lambda_, _ = self.max_likelihood(bounds)\n\n        # store theta and Lambda in log scale\n        if (self.method == \"regression\") or (self.method == \"reinterpolation\"):\n            # case noise is True\n            self.theta = self.logtheta_lambda_[:-1]\n            self.Lambda = self.logtheta_lambda_[-1]\n        else:\n            self.theta = self.logtheta_lambda_\n            self.Lambda = None\n        # store p for future use\n        self.p = 2\n\n        # Once logtheta_lambda is found, compute the final correlation matrix\n        self.negLnLike, self.Psi_, self.U_ = self.likelihood(self.logtheta_lambda_)\n\n        # Update log with the current values\n        self._update_log()\n        return self\n\n    def predict(self, X: np.ndarray, return_std=False, return_val: str = \"y\") -&gt; np.ndarray:\n        \"\"\"\n        Predicts the Kriging response at a set of points X. This method is compatible\n        with scikit-learn and returns predictions for the input points.\n\n        Args:\n            X (np.ndarray):\n                Array of shape (n_samples, n_features) containing the points at which\n                to predict the Kriging response.\n            return_std (bool, optional):\n                If True, returns the standard deviation of the predictions as well.\n                Implememented for compatibility with scikit-learn.\n                Defaults to False.\n            return_val (str):\n                Specifies which prediction values to return.\n                It can be \"y\", \"s\", \"ei\", or \"all\".\n\n        Returns:\n            np.ndarray:\n                Predicted values of shape (n_samples,).\n            np.ndarray:\n                If self.return_std is True, returns the standard deviations of the predictions\n                of shape (n_samples,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging\n            &gt;&gt;&gt; # Training data\n            &gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n            &gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n            &gt;&gt;&gt; # Fit the Kriging model\n            &gt;&gt;&gt; model = Kriging().fit(X_train, y_train)\n            &gt;&gt;&gt; # Test data\n            &gt;&gt;&gt; X_test = np.array([[0.25, 0.25], [0.75, 0.75]])\n            &gt;&gt;&gt; # Predict responses\n            &gt;&gt;&gt; y_pred, sd, ei = model.predict(X_test)\n            &gt;&gt;&gt; print(\"Predictions:\", y_pred)\n        \"\"\"\n        self.return_std = return_std\n        X = np.atleast_2d(X)\n        if return_std:\n            # Return predictions and standard deviations\n            # Compatibility with scikit-learn\n            self.return_std = True\n            predictions, std_devs = zip(*[self._pred(x_i)[:2] for x_i in X])\n            return np.array(predictions), np.array(std_devs)\n        if return_val == \"s\":\n            # Return only standard deviations\n            self.return_std = True\n            predictions, std_devs = zip(*[self._pred(x_i)[:2] for x_i in X])\n            return np.array(std_devs)\n        elif return_val == \"all\":\n            # Return predictions, standard deviations, and expected improvements\n            self.return_std = True\n            self.return_ei = True\n            predictions, std_devs, eis = zip(*[self._pred(x_i) for x_i in X])\n            return np.array(predictions), np.array(std_devs), np.array(eis)\n        elif return_val == \"ei\":\n            # Return only neg. expected improvements\n            self.return_ei = True\n            predictions, eis = zip(*[(self._pred(x_i)[0], self._pred(x_i)[2]) for x_i in X])\n            return np.array(eis)\n        else:\n            # Return only predictions (case \"y\")\n            predictions = [self._pred(x_i)[0] for x_i in X]\n            return np.array(predictions)\n\n    def _kernel(self, X: np.ndarray, theta: np.ndarray, p: float) -&gt; np.ndarray:\n        \"\"\"\n        Computes the correlation matrix Psi using vectorized operations.\n\n        Args:\n            X (np.ndarray): Input data of shape (n_samples, n_features).\n            theta (np.ndarray): Theta parameters of shape (n_features,).\n            p (float): Power exponent.\n\n        Returns:\n            np.ndarray: The upper triangle of the correlation matrix Psi.\n        \"\"\"\n        n_samples, n_features = X.shape\n        Psi = np.zeros((n_samples, n_samples), dtype=float)\n        # Calculate all pairwise differences:\n        # X_expanded_rows will have shape (n_samples, 1, n_features)\n        # X_expanded_cols will have shape (1, n_samples, n_features)\n        # diff will have shape (n_samples, n_samples, n_features)\n        diff = np.abs(X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** p\n        # Apply theta and sum over features\n        # dist_matrix will have shape (n_samples, n_samples)\n        dist_matrix = np.sum(theta * diff, axis=2)\n        # Compute Psi using the exponential kernel\n        Psi = np.exp(-dist_matrix)\n        # Return only the upper triangle, as the matrix is symmetric\n        # and the diagonal will be handled later.\n        return np.triu(Psi, k=1)\n\n    def likelihood(self, x: np.ndarray) -&gt; Tuple[float, np.ndarray, np.ndarray]:\n        \"\"\"\n        Computes the negative of the concentrated log-likelihood for a given set\n        of log(theta) parameters using a power exponent p=1.99. Returns the\n        negative log-likelihood, the correlation matrix Psi, and its Cholesky factor U.\n\n        Args:\n            x (np.ndarray):\n                1D array of log(theta) parameters of length k. If self.method is \"regression\" or\n                \"reinterpolation\", length is k+1 and the last element of x is the log(noise) parameter.\n\n        Returns:\n            (float, np.ndarray, np.ndarray):\n                (negLnLike, Psi, U) where:\n                - negLnLike (float): The negative concentrated log-likelihood.\n                - Psi (np.ndarray): The correlation matrix.\n                - U (np.ndarray): The Cholesky factor (or None if ill-conditioned).\n        \"\"\"\n        # Extract data\n        X = self.X_\n        y = self.y_.flatten()\n\n        if (self.method == \"regression\") or (self.method == \"reinterpolation\"):\n            # case noise is True\n            theta = x[:-1]\n            # theta is in log scale, so transform it back:\n            theta = 10.0**theta\n            lambda_ = x[-1]\n            # lambda is in log scale, so transform it back:\n            lambda_ = 10.0**lambda_\n        elif self.method == \"interpolation\":\n            theta = x\n            theta = 10.0**theta\n            # use the original, untransformed eps:\n            lambda_ = self.eps\n        else:\n            raise ValueError(\"method must be one of 'interpolation', 'regression', or 'reinterpolation'\")\n\n        p = 1.99\n        n = X.shape[0]\n        one = np.ones(n)\n\n        # Build correlation matrix\n        Psi_upper_triangle = self._kernel(X, theta, p)\n        Psi = Psi_upper_triangle + Psi_upper_triangle.T + np.eye(n) + np.eye(n) * lambda_\n\n        try:\n            U = np.linalg.cholesky(Psi)\n        except LinAlgError:\n            return self.penalty, Psi, None\n\n        LnDetPsi = 2.0 * np.sum(np.log(np.abs(np.diag(U))))\n\n        temp_y = np.linalg.solve(U, y)\n        temp_one = np.linalg.solve(U, one)\n        vy = np.linalg.solve(U.T, temp_y)\n        vone = np.linalg.solve(U.T, temp_one)\n\n        mu = (one @ vy) / (one @ vone)\n        resid = y - one * mu\n        tresid = np.linalg.solve(U, resid)\n        tresid = np.linalg.solve(U.T, tresid)\n        SigmaSqr = (resid @ tresid) / n\n\n        negLnLike = (n / 2.0) * np.log(SigmaSqr) + 0.5 * LnDetPsi\n        return negLnLike, Psi, U\n\n    def _pred(self, x: np.ndarray) -&gt; float:\n        \"\"\"\n        Computes a single-point Kriging prediction using the correlation matrix\n        information. Internal helper method.\n\n        Args:\n            x (np.ndarray): 1D array of length k for the point at which to predict.\n\n        Returns:\n            float: The Kriging prediction at x.\n            float: The standard deviation of the prediction.\n            float: The NEGATIVE expected improvement at x.\n        \"\"\"\n        X = self.X_\n        y = self.y_.flatten()\n\n        if self.method == \"interpolation\":\n            theta = self.logtheta_lambda_\n            theta = 10.0**theta\n            # lambda is not transformed back:\n            lambda_ = self.eps\n        else:\n            theta = self.logtheta_lambda_[:-1]\n            theta = 10.0**theta\n            lambda_ = self.logtheta_lambda_[-1]\n            lambda_ = 10.0**lambda_\n\n        U = self.U_\n\n        p = 1.99\n        n = X.shape[0]\n        one = np.ones(n)\n\n        # Compute mu\n        y_tilde = np.linalg.solve(U, y)\n        y_tilde = np.linalg.solve(U.T, y_tilde)\n        one_tilde = np.linalg.solve(U, one)\n        one_tilde = np.linalg.solve(U.T, one_tilde)\n        mu = (one @ y_tilde) / (one @ one_tilde)\n\n        resid = y - one * mu\n        resid_tilde = np.linalg.solve(U, resid)\n        resid_tilde = np.linalg.solve(U.T, resid_tilde)\n\n        # Build psi\n        psi = np.ones(n)\n        for i in range(n):\n            dist_vec = np.abs(X[i, :] - x) ** p\n            psi[i] = np.exp(-np.sum(theta * dist_vec))\n\n        # Compute SigmaSqr and SSqr\n        if (self.method == \"interpolation\") or (self.method == \"regression\"):\n            SigmaSqr = (resid @ resid_tilde) / n\n            # Compute SSqr\n            psi_tilde = np.linalg.solve(U, psi)\n            psi_tilde = np.linalg.solve(U.T, psi_tilde)\n            # Eq. (3.1) in [forr08a] without lambda:\n            SSqr = SigmaSqr * (1 + lambda_ - psi @ psi_tilde)\n        else:\n            # method is \"reinterpolation\"\n            Psi_adjusted = self.Psi_ - np.eye(n) * lambda_ + np.eye(n) * self.eps\n            SigmaSqr = (resid @ np.linalg.solve(U.T, np.linalg.solve(U, Psi_adjusted @ resid_tilde))) / n\n            # Compute Uint (Cholesky factor of the adjusted Psi matrix)\n            Uint = np.linalg.cholesky(Psi_adjusted)\n\n            # Compute SSqr\n            psi_tilde = np.linalg.solve(Uint, psi)\n            psi_tilde = np.linalg.solve(Uint.T, psi_tilde)\n            SSqr = SigmaSqr * (1 - psi @ psi_tilde)\n\n        # Compute s\n        s = np.abs(SSqr) ** 0.5\n\n        # Final prediction\n        f = mu + psi @ resid_tilde\n\n        # Compute ExpImp\n        if self.return_ei:\n            yBest = np.min(y)\n            EITermOne = (yBest - f) * (0.5 + 0.5 * erf((1 / np.sqrt(2)) * ((yBest - f) / s)))\n            EITermTwo = s * (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * ((yBest - f) ** 2 / SSqr))\n            ExpImp = np.log10(EITermOne + EITermTwo + self.eps)\n            return float(f), float(s), float(-ExpImp)\n        else:\n            return float(f), float(s)\n\n    def max_likelihood(self, bounds: List[Tuple[float, float]]) -&gt; Tuple[np.ndarray, float]:\n        \"\"\"\n        Maximizes the Kriging likelihood function using differential evolution\n        over the range of log(theta) specified by bounds.\n\n        Args:\n            bounds (List[Tuple[float, float]]): Sequence of (low, high) bounds for log(theta).\n\n        Returns:\n            (np.ndarray, float): (best_x, best_fun) where best_x is the\n            optimal log(theta) array and best_fun is the minimized negative log-likelihood.\n        \"\"\"\n\n        def objective(logtheta_lambda):\n            neg_ln_like, _, _ = self.likelihood(logtheta_lambda)\n            return neg_ln_like\n\n        result = differential_evolution(objective, bounds)\n        return result.x, result.fun\n\n    def plot(self, show: Optional[bool] = True) -&gt; None:\n        \"\"\"\n        This function plots 1D and 2D surrogates.\n        Only for compatibility with the old Kriging implementation.\n\n        Args:\n            self (object):\n                The Kriging object.\n            show (bool):\n                If `True`, the plots are displayed.\n                If `False`, `plt.show()` should be called outside this function.\n\n        Returns:\n            None\n\n        Note:\n            * This method provides only a basic plot. For more advanced plots,\n                use the `plot_contour()` method of the `Spot` class.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n                from spotpython.fun.objectivefunctions import Analytical\n                from spotpython.spot import spot\n                from spotpython.utils.init import fun_control_init, design_control_init\n                # 1-dimensional example\n                fun = analytical().fun_sphere\n                fun_control=fun_control_init(lower = np.array([-1]),\n                                            upper = np.array([1]),\n                                            noise=False)\n                design_control=design_control_init(init_size=10)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control)\n                S.initialize_design()\n                S.update_stats()\n                S.fit_surrogate()\n                S.surrogate.plot()\n                # 2-dimensional example\n                fun = analytical().fun_sphere\n                fun_control=fun_control_init(lower = np.array([-1, -1]),\n                                            upper = np.array([1, 1]),\n                                            noise=False)\n                design_control=design_control_init(init_size=10)\n                S = spot.Spot(fun=fun,\n                            fun_control=fun_control,\n                            design_control=design_control)\n                S.initialize_design()\n                S.update_stats()\n                S.fit_surrogate()\n                S.surrogate.plot()\n        \"\"\"\n        if self.k == 1:\n            # TODO: Improve plot (add conf. interval etc.)\n            fig = pylab.figure(figsize=(9, 6))\n            n_grid = 100\n            x = linspace(self.min_X[0], self.max_X[0], num=n_grid)\n            y = self.predict(x)\n            plt.figure()\n            plt.plot(x, y, \"k\")\n            if show:\n                plt.show()\n\n        if self.k == 2:\n            fig = pylab.figure(figsize=(9, 6))\n            n_grid = 100\n            x = linspace(self.min_X[0], self.max_X[0], num=n_grid)\n            y = linspace(self.min_X[1], self.max_X[1], num=n_grid)\n            X, Y = meshgrid(x, y)\n            # Predict based on the optimized results\n            zz = array([self.predict(array([x, y]), return_val=\"all\") for x, y in zip(ravel(X), ravel(Y))])\n            zs = zz[:, 0, :]\n            zse = zz[:, 1, :]\n            Z = zs.reshape(X.shape)\n            Ze = zse.reshape(X.shape)\n\n            nat_point_X = self.X_[:, 0]\n            nat_point_Y = self.X_[:, 1]\n            contour_levels = 30\n            ax = fig.add_subplot(224)\n            # plot predicted values:\n            pylab.contourf(X, Y, Ze, contour_levels, cmap=\"jet\")\n            pylab.title(\"Error\")\n            pylab.colorbar()\n            # plot observed points:\n            pylab.plot(nat_point_X, nat_point_Y, \"ow\")\n            #\n            ax = fig.add_subplot(223)\n            # plot predicted values:\n            plt.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\")\n            plt.title(\"Surrogate\")\n            # plot observed points:\n            pylab.plot(nat_point_X, nat_point_Y, \"ow\", zorder=3)\n            pylab.colorbar()\n            #\n            ax = fig.add_subplot(221, projection=\"3d\")\n            ax.plot_surface(X, Y, Z, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n            #\n            ax = fig.add_subplot(222, projection=\"3d\")\n            ax.plot_surface(X, Y, Ze, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n            #\n            pylab.show()\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.Kriging.__init__","title":"<code>__init__(eps=None, penalty=10000.0, method='regression', noise=False, var_type=['num'], name='Kriging', seed=124, model_optimizer=None, model_fun_evals=None, min_theta=-3.0, max_theta=2.0, n_theta=1, theta_init_zero=False, p_val=2.0, n_p=1, optim_p=False, min_Lambda=1e-09, max_Lambda=1.0, log_level=50, spot_writer=None, counter=None, metric_factorial='canberra', **kwargs)</code>","text":"<p>Initializes the Kriging model.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>Small number added to the diagonal of the correlation matrix to reduce ill-conditioning. Defaults to the square root of machine epsilon. Only used if method is \u201cinterpolation\u201d. Otherwise, if method is \u201cregression\u201d or \u201creinterpolation\u201d, eps is replaced by the lambda_ parameter. Defaults to None.</p> <code>None</code> <code>penalty</code> <code>float</code> <p>Large negative log-likelihood assigned if the correlation matrix is not positive-definite. Defaults to 1e4.</p> <code>10000.0</code> <code>method</code> <code>str</code> <p>The type how the model uis fitted. Can be \u201cinterpolation\u201d, \u201cregression\u201d, or \u201creinterpolation\u201d. Defaults to \u201cregression\u201d.</p> <code>'regression'</code> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>def __init__(\n    self,\n    eps: float = None,\n    penalty: float = 1e4,\n    method=\"regression\",\n    noise: bool = False,\n    var_type: List[str] = [\"num\"],\n    name: str = \"Kriging\",\n    seed: int = 124,\n    model_optimizer=None,\n    model_fun_evals: Optional[int] = None,\n    min_theta: float = -3.0,\n    max_theta: float = 2.0,\n    n_theta: int = 1,\n    theta_init_zero: bool = False,\n    p_val: float = 2.0,\n    n_p: int = 1,\n    optim_p: bool = False,\n    min_Lambda: float = 1e-9,\n    max_Lambda: float = 1.0,\n    log_level: int = 50,\n    spot_writer=None,\n    counter=None,\n    metric_factorial=\"canberra\",\n    **kwargs,\n):\n    \"\"\"\n    Initializes the Kriging model.\n\n    Args:\n        eps (float, optional):\n            Small number added to the diagonal of the correlation matrix to reduce\n            ill-conditioning. Defaults to the square root of machine epsilon.\n            Only used if method is \"interpolation\". Otherwise, if method is \"regression\" or \"reinterpolation\", eps is replaced by the\n            lambda_ parameter. Defaults to None.\n        penalty (float, optional):\n            Large negative log-likelihood assigned if the correlation matrix is\n            not positive-definite. Defaults to 1e4.\n        method (str, optional):\n            The type how the model uis fitted. Can be \"interpolation\", \"regression\", or \"reinterpolation\". Defaults to \"regression\".\n    \"\"\"\n    if eps is None:\n        self.eps = self._get_eps()\n    else:\n        # check if eps is positive\n        if eps &lt;= 0:\n            raise ValueError(\"eps must be positive\")\n        self.eps = eps\n    self.penalty = penalty\n\n    self.noise = noise\n    self.var_type = var_type\n    self.name = name\n    self.seed = seed\n    self.log_level = log_level\n    self.spot_writer = spot_writer\n    self.counter = counter\n    self.metric_factorial = metric_factorial\n    self.min_theta = min_theta\n    self.max_theta = max_theta\n    self.min_Lambda = min_Lambda\n    self.max_Lambda = max_Lambda\n    self.n_theta = n_theta\n    self.p_val = p_val\n    self.n_p = n_p\n    self.optim_p = optim_p\n    self.theta_init_zero = theta_init_zero\n    self.model_optimizer = model_optimizer\n    if self.model_optimizer is None:\n        self.model_optimizer = differential_evolution\n    self.model_fun_evals = model_fun_evals\n    if self.model_fun_evals is None:\n        self.model_fun_evals = 100\n\n    # Logging information\n    self.log = {}\n    self.log[\"negLnLike\"] = []\n    self.log[\"theta\"] = []\n    self.log[\"p\"] = []\n    self.log[\"Lambda\"] = []\n\n    self.logtheta_lambda_ = None\n    self.U_ = None\n    self.X_ = None\n    self.y_ = None\n    self.negLnLike = None\n    self.Psi_ = None\n    if method not in [\"interpolation\", \"regression\", \"reinterpolation\"]:\n        raise ValueError(\"method must be one of 'interpolation', 'regression', or 'reinterpolation']\")\n    self.method = method\n    self.return_ei = False\n    self.return_std = False\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.Kriging.fit","title":"<code>fit(X, y, bounds=None)</code>","text":"<p>Fits the Kriging model to training data X and y. This method is compatible with scikit-learn and uses differential evolution to optimize the hyperparameters (log(theta)).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training input data of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Target values of shape (n_samples,) or (n_samples, 1).</p> required <code>bounds</code> <code>Optional[List[Tuple[float, float]]]</code> <p>Bounds for each dimension of log(theta). If None, defaults to [(-3, 2)] * n_features.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Kriging</code> <code>Kriging</code> <p>The fitted Kriging model instance (self).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging\n&gt;&gt;&gt; # Training data\n&gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt; # Initialize and fit the Kriging model\n&gt;&gt;&gt; model = Kriging()\n&gt;&gt;&gt; model.fit(X_train, y_train)\n&gt;&gt;&gt; print(\"Fitted log(theta):\", model.logtheta_lambda_)\n</code></pre> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray, bounds: Optional[List[Tuple[float, float]]] = None) -&gt; \"Kriging\":\n    \"\"\"\n    Fits the Kriging model to training data X and y. This method is compatible\n    with scikit-learn and uses differential evolution to optimize the hyperparameters\n    (log(theta)).\n\n    Args:\n        X (np.ndarray):\n            Training input data of shape (n_samples, n_features).\n        y (np.ndarray):\n            Target values of shape (n_samples,) or (n_samples, 1).\n        bounds (Optional[List[Tuple[float, float]]]):\n            Bounds for each dimension of log(theta). If None, defaults to [(-3, 2)] * n_features.\n\n    Returns:\n        Kriging:\n            The fitted Kriging model instance (self).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging\n        &gt;&gt;&gt; # Training data\n        &gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n        &gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; # Initialize and fit the Kriging model\n        &gt;&gt;&gt; model = Kriging()\n        &gt;&gt;&gt; model.fit(X_train, y_train)\n        &gt;&gt;&gt; print(\"Fitted log(theta):\", model.logtheta_lambda_)\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y).flatten()\n    self.X_ = X\n    self.y_ = y\n    self.n, self.k = X.shape\n    # Calculate and store min and max of X\n    self.min_X = np.min(self.X_, axis=0)\n    self.max_X = np.max(self.X_, axis=0)\n\n    _, aggregated_mean_y, _ = aggregate_mean_var(X=self.X_, y=self.y_)\n    self.aggregated_mean_y = np.copy(aggregated_mean_y)\n    if bounds is None:\n        if self.method == \"interpolation\":\n            bounds = [(-3.0, 2.0)] * self.k\n        else:\n            # regression and reinterpolation use lambda_ as well\n            bounds = [(-3.0, 2.0)] * self.k + [(-6.0, 0.0)]\n\n    self.logtheta_lambda_, _ = self.max_likelihood(bounds)\n\n    # store theta and Lambda in log scale\n    if (self.method == \"regression\") or (self.method == \"reinterpolation\"):\n        # case noise is True\n        self.theta = self.logtheta_lambda_[:-1]\n        self.Lambda = self.logtheta_lambda_[-1]\n    else:\n        self.theta = self.logtheta_lambda_\n        self.Lambda = None\n    # store p for future use\n    self.p = 2\n\n    # Once logtheta_lambda is found, compute the final correlation matrix\n    self.negLnLike, self.Psi_, self.U_ = self.likelihood(self.logtheta_lambda_)\n\n    # Update log with the current values\n    self._update_log()\n    return self\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.Kriging.get_model_params","title":"<code>get_model_params()</code>","text":"<p>Get the model parameters (in addition to sklearn\u2019s get_params method).</p> <p>This method is NOT required for scikit-learn compatibility.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, float]</code> <p>Parameter names not included in get_params() mapped to their values.</p> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>def get_model_params(self) -&gt; Dict[str, float]:\n    \"\"\"\n    Get the model parameters (in addition to sklearn's get_params method).\n\n    This method is NOT required for scikit-learn compatibility.\n\n    Returns:\n        dict: Parameter names not included in get_params() mapped to their values.\n    \"\"\"\n    return {\"log_theta_lambda\": self.logtheta_lambda_, \"U\": self.U_, \"X\": self.X_, \"y\": self.y_, \"negLnLike\": self.negLnLike}\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.Kriging.likelihood","title":"<code>likelihood(x)</code>","text":"<p>Computes the negative of the concentrated log-likelihood for a given set of log(theta) parameters using a power exponent p=1.99. Returns the negative log-likelihood, the correlation matrix Psi, and its Cholesky factor U.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>1D array of log(theta) parameters of length k. If self.method is \u201cregression\u201d or \u201creinterpolation\u201d, length is k+1 and the last element of x is the log(noise) parameter.</p> required <p>Returns:</p> Type Description <code>(float, ndarray, ndarray)</code> <p>(negLnLike, Psi, U) where: - negLnLike (float): The negative concentrated log-likelihood. - Psi (np.ndarray): The correlation matrix. - U (np.ndarray): The Cholesky factor (or None if ill-conditioned).</p> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>def likelihood(self, x: np.ndarray) -&gt; Tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"\n    Computes the negative of the concentrated log-likelihood for a given set\n    of log(theta) parameters using a power exponent p=1.99. Returns the\n    negative log-likelihood, the correlation matrix Psi, and its Cholesky factor U.\n\n    Args:\n        x (np.ndarray):\n            1D array of log(theta) parameters of length k. If self.method is \"regression\" or\n            \"reinterpolation\", length is k+1 and the last element of x is the log(noise) parameter.\n\n    Returns:\n        (float, np.ndarray, np.ndarray):\n            (negLnLike, Psi, U) where:\n            - negLnLike (float): The negative concentrated log-likelihood.\n            - Psi (np.ndarray): The correlation matrix.\n            - U (np.ndarray): The Cholesky factor (or None if ill-conditioned).\n    \"\"\"\n    # Extract data\n    X = self.X_\n    y = self.y_.flatten()\n\n    if (self.method == \"regression\") or (self.method == \"reinterpolation\"):\n        # case noise is True\n        theta = x[:-1]\n        # theta is in log scale, so transform it back:\n        theta = 10.0**theta\n        lambda_ = x[-1]\n        # lambda is in log scale, so transform it back:\n        lambda_ = 10.0**lambda_\n    elif self.method == \"interpolation\":\n        theta = x\n        theta = 10.0**theta\n        # use the original, untransformed eps:\n        lambda_ = self.eps\n    else:\n        raise ValueError(\"method must be one of 'interpolation', 'regression', or 'reinterpolation'\")\n\n    p = 1.99\n    n = X.shape[0]\n    one = np.ones(n)\n\n    # Build correlation matrix\n    Psi_upper_triangle = self._kernel(X, theta, p)\n    Psi = Psi_upper_triangle + Psi_upper_triangle.T + np.eye(n) + np.eye(n) * lambda_\n\n    try:\n        U = np.linalg.cholesky(Psi)\n    except LinAlgError:\n        return self.penalty, Psi, None\n\n    LnDetPsi = 2.0 * np.sum(np.log(np.abs(np.diag(U))))\n\n    temp_y = np.linalg.solve(U, y)\n    temp_one = np.linalg.solve(U, one)\n    vy = np.linalg.solve(U.T, temp_y)\n    vone = np.linalg.solve(U.T, temp_one)\n\n    mu = (one @ vy) / (one @ vone)\n    resid = y - one * mu\n    tresid = np.linalg.solve(U, resid)\n    tresid = np.linalg.solve(U.T, tresid)\n    SigmaSqr = (resid @ tresid) / n\n\n    negLnLike = (n / 2.0) * np.log(SigmaSqr) + 0.5 * LnDetPsi\n    return negLnLike, Psi, U\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.Kriging.max_likelihood","title":"<code>max_likelihood(bounds)</code>","text":"<p>Maximizes the Kriging likelihood function using differential evolution over the range of log(theta) specified by bounds.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>List[Tuple[float, float]]</code> <p>Sequence of (low, high) bounds for log(theta).</p> required <p>Returns:</p> Type Description <code>(ndarray, float)</code> <p>(best_x, best_fun) where best_x is the</p> <code>float</code> <p>optimal log(theta) array and best_fun is the minimized negative log-likelihood.</p> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>def max_likelihood(self, bounds: List[Tuple[float, float]]) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Maximizes the Kriging likelihood function using differential evolution\n    over the range of log(theta) specified by bounds.\n\n    Args:\n        bounds (List[Tuple[float, float]]): Sequence of (low, high) bounds for log(theta).\n\n    Returns:\n        (np.ndarray, float): (best_x, best_fun) where best_x is the\n        optimal log(theta) array and best_fun is the minimized negative log-likelihood.\n    \"\"\"\n\n    def objective(logtheta_lambda):\n        neg_ln_like, _, _ = self.likelihood(logtheta_lambda)\n        return neg_ln_like\n\n    result = differential_evolution(objective, bounds)\n    return result.x, result.fun\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.Kriging.plot","title":"<code>plot(show=True)</code>","text":"<p>This function plots 1D and 2D surrogates. Only for compatibility with the old Kriging implementation.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>show</code> <code>bool</code> <p>If <code>True</code>, the plots are displayed. If <code>False</code>, <code>plt.show()</code> should be called outside this function.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Note <ul> <li>This method provides only a basic plot. For more advanced plots,     use the <code>plot_contour()</code> method of the <code>Spot</code> class.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.fun.objectivefunctions import Analytical\n    from spotpython.spot import spot\n    from spotpython.utils.init import fun_control_init, design_control_init\n    # 1-dimensional example\n    fun = analytical().fun_sphere\n    fun_control=fun_control_init(lower = np.array([-1]),\n                                upper = np.array([1]),\n                                noise=False)\n    design_control=design_control_init(init_size=10)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control)\n    S.initialize_design()\n    S.update_stats()\n    S.fit_surrogate()\n    S.surrogate.plot()\n    # 2-dimensional example\n    fun = analytical().fun_sphere\n    fun_control=fun_control_init(lower = np.array([-1, -1]),\n                                upper = np.array([1, 1]),\n                                noise=False)\n    design_control=design_control_init(init_size=10)\n    S = spot.Spot(fun=fun,\n                fun_control=fun_control,\n                design_control=design_control)\n    S.initialize_design()\n    S.update_stats()\n    S.fit_surrogate()\n    S.surrogate.plot()\n</code></pre> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>def plot(self, show: Optional[bool] = True) -&gt; None:\n    \"\"\"\n    This function plots 1D and 2D surrogates.\n    Only for compatibility with the old Kriging implementation.\n\n    Args:\n        self (object):\n            The Kriging object.\n        show (bool):\n            If `True`, the plots are displayed.\n            If `False`, `plt.show()` should be called outside this function.\n\n    Returns:\n        None\n\n    Note:\n        * This method provides only a basic plot. For more advanced plots,\n            use the `plot_contour()` method of the `Spot` class.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.fun.objectivefunctions import Analytical\n            from spotpython.spot import spot\n            from spotpython.utils.init import fun_control_init, design_control_init\n            # 1-dimensional example\n            fun = analytical().fun_sphere\n            fun_control=fun_control_init(lower = np.array([-1]),\n                                        upper = np.array([1]),\n                                        noise=False)\n            design_control=design_control_init(init_size=10)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control)\n            S.initialize_design()\n            S.update_stats()\n            S.fit_surrogate()\n            S.surrogate.plot()\n            # 2-dimensional example\n            fun = analytical().fun_sphere\n            fun_control=fun_control_init(lower = np.array([-1, -1]),\n                                        upper = np.array([1, 1]),\n                                        noise=False)\n            design_control=design_control_init(init_size=10)\n            S = spot.Spot(fun=fun,\n                        fun_control=fun_control,\n                        design_control=design_control)\n            S.initialize_design()\n            S.update_stats()\n            S.fit_surrogate()\n            S.surrogate.plot()\n    \"\"\"\n    if self.k == 1:\n        # TODO: Improve plot (add conf. interval etc.)\n        fig = pylab.figure(figsize=(9, 6))\n        n_grid = 100\n        x = linspace(self.min_X[0], self.max_X[0], num=n_grid)\n        y = self.predict(x)\n        plt.figure()\n        plt.plot(x, y, \"k\")\n        if show:\n            plt.show()\n\n    if self.k == 2:\n        fig = pylab.figure(figsize=(9, 6))\n        n_grid = 100\n        x = linspace(self.min_X[0], self.max_X[0], num=n_grid)\n        y = linspace(self.min_X[1], self.max_X[1], num=n_grid)\n        X, Y = meshgrid(x, y)\n        # Predict based on the optimized results\n        zz = array([self.predict(array([x, y]), return_val=\"all\") for x, y in zip(ravel(X), ravel(Y))])\n        zs = zz[:, 0, :]\n        zse = zz[:, 1, :]\n        Z = zs.reshape(X.shape)\n        Ze = zse.reshape(X.shape)\n\n        nat_point_X = self.X_[:, 0]\n        nat_point_Y = self.X_[:, 1]\n        contour_levels = 30\n        ax = fig.add_subplot(224)\n        # plot predicted values:\n        pylab.contourf(X, Y, Ze, contour_levels, cmap=\"jet\")\n        pylab.title(\"Error\")\n        pylab.colorbar()\n        # plot observed points:\n        pylab.plot(nat_point_X, nat_point_Y, \"ow\")\n        #\n        ax = fig.add_subplot(223)\n        # plot predicted values:\n        plt.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\")\n        plt.title(\"Surrogate\")\n        # plot observed points:\n        pylab.plot(nat_point_X, nat_point_Y, \"ow\", zorder=3)\n        pylab.colorbar()\n        #\n        ax = fig.add_subplot(221, projection=\"3d\")\n        ax.plot_surface(X, Y, Z, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n        #\n        ax = fig.add_subplot(222, projection=\"3d\")\n        ax.plot_surface(X, Y, Ze, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n        #\n        pylab.show()\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.Kriging.predict","title":"<code>predict(X, return_std=False, return_val='y')</code>","text":"<p>Predicts the Kriging response at a set of points X. This method is compatible with scikit-learn and returns predictions for the input points.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Array of shape (n_samples, n_features) containing the points at which to predict the Kriging response.</p> required <code>return_std</code> <code>bool</code> <p>If True, returns the standard deviation of the predictions as well. Implememented for compatibility with scikit-learn. Defaults to False.</p> <code>False</code> <code>return_val</code> <code>str</code> <p>Specifies which prediction values to return. It can be \u201cy\u201d, \u201cs\u201d, \u201cei\u201d, or \u201call\u201d.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted values of shape (n_samples,).</p> <code>ndarray</code> <p>np.ndarray: If self.return_std is True, returns the standard deviations of the predictions of shape (n_samples,).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging\n&gt;&gt;&gt; # Training data\n&gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt; # Fit the Kriging model\n&gt;&gt;&gt; model = Kriging().fit(X_train, y_train)\n&gt;&gt;&gt; # Test data\n&gt;&gt;&gt; X_test = np.array([[0.25, 0.25], [0.75, 0.75]])\n&gt;&gt;&gt; # Predict responses\n&gt;&gt;&gt; y_pred, sd, ei = model.predict(X_test)\n&gt;&gt;&gt; print(\"Predictions:\", y_pred)\n</code></pre> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>def predict(self, X: np.ndarray, return_std=False, return_val: str = \"y\") -&gt; np.ndarray:\n    \"\"\"\n    Predicts the Kriging response at a set of points X. This method is compatible\n    with scikit-learn and returns predictions for the input points.\n\n    Args:\n        X (np.ndarray):\n            Array of shape (n_samples, n_features) containing the points at which\n            to predict the Kriging response.\n        return_std (bool, optional):\n            If True, returns the standard deviation of the predictions as well.\n            Implememented for compatibility with scikit-learn.\n            Defaults to False.\n        return_val (str):\n            Specifies which prediction values to return.\n            It can be \"y\", \"s\", \"ei\", or \"all\".\n\n    Returns:\n        np.ndarray:\n            Predicted values of shape (n_samples,).\n        np.ndarray:\n            If self.return_std is True, returns the standard deviations of the predictions\n            of shape (n_samples,).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging\n        &gt;&gt;&gt; # Training data\n        &gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n        &gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; # Fit the Kriging model\n        &gt;&gt;&gt; model = Kriging().fit(X_train, y_train)\n        &gt;&gt;&gt; # Test data\n        &gt;&gt;&gt; X_test = np.array([[0.25, 0.25], [0.75, 0.75]])\n        &gt;&gt;&gt; # Predict responses\n        &gt;&gt;&gt; y_pred, sd, ei = model.predict(X_test)\n        &gt;&gt;&gt; print(\"Predictions:\", y_pred)\n    \"\"\"\n    self.return_std = return_std\n    X = np.atleast_2d(X)\n    if return_std:\n        # Return predictions and standard deviations\n        # Compatibility with scikit-learn\n        self.return_std = True\n        predictions, std_devs = zip(*[self._pred(x_i)[:2] for x_i in X])\n        return np.array(predictions), np.array(std_devs)\n    if return_val == \"s\":\n        # Return only standard deviations\n        self.return_std = True\n        predictions, std_devs = zip(*[self._pred(x_i)[:2] for x_i in X])\n        return np.array(std_devs)\n    elif return_val == \"all\":\n        # Return predictions, standard deviations, and expected improvements\n        self.return_std = True\n        self.return_ei = True\n        predictions, std_devs, eis = zip(*[self._pred(x_i) for x_i in X])\n        return np.array(predictions), np.array(std_devs), np.array(eis)\n    elif return_val == \"ei\":\n        # Return only neg. expected improvements\n        self.return_ei = True\n        predictions, eis = zip(*[(self._pred(x_i)[0], self._pred(x_i)[2]) for x_i in X])\n        return np.array(eis)\n    else:\n        # Return only predictions (case \"y\")\n        predictions = [self._pred(x_i)[0] for x_i in X]\n        return np.array(predictions)\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.plot1d","title":"<code>plot1d(model, X, y, show=True)</code>","text":"<p>Plots the 1D Kriging surrogate model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>A fitted Kriging model.</p> required <code>X</code> <code>ndarray</code> <p>Training input data of shape (n_samples, 1).</p> required <code>y</code> <code>ndarray</code> <p>Training target values of shape (n_samples,).</p> required <code>show</code> <code>bool</code> <p>If True, displays the plot. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging\n&gt;&gt;&gt; # Training data\n&gt;&gt;&gt; X_train = np.array([[0.0], [0.5], [1.0]])\n&gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt; # Initialize and fit the Kriging model\n&gt;&gt;&gt; model = Kriging().fit(X_train, y_train)\n&gt;&gt;&gt; # Plot the 1D Kriging surrogate\n&gt;&gt;&gt; plot1d(model, X_train, y_train)\n</code></pre> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>def plot1d(model, X: np.ndarray, y: np.ndarray, show: Optional[bool] = True) -&gt; None:\n    \"\"\"\n    Plots the 1D Kriging surrogate model.\n\n    Args:\n        model (object): A fitted Kriging model.\n        X (np.ndarray): Training input data of shape (n_samples, 1).\n        y (np.ndarray): Training target values of shape (n_samples,).\n        show (bool): If True, displays the plot. Defaults to True.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging\n        &gt;&gt;&gt; # Training data\n        &gt;&gt;&gt; X_train = np.array([[0.0], [0.5], [1.0]])\n        &gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; # Initialize and fit the Kriging model\n        &gt;&gt;&gt; model = Kriging().fit(X_train, y_train)\n        &gt;&gt;&gt; # Plot the 1D Kriging surrogate\n        &gt;&gt;&gt; plot1d(model, X_train, y_train)\n    \"\"\"\n    if X.shape[1] != 1:\n        raise ValueError(\"plot1d is only supported for 1D input data.\")\n\n    _ = plt.figure(figsize=(9, 6))\n    n_grid = 100\n    x = linspace(X[:, 0].min(), X[:, 0].max(), num=n_grid).reshape(-1, 1)\n    y_pred, y_std = model.predict(x, return_std=True)\n\n    plt.plot(x, y_pred, \"k\", label=\"Prediction\")\n    plt.fill_between(\n        x.ravel(),\n        y_pred - 1.96 * y_std,\n        y_pred + 1.96 * y_std,\n        alpha=0.2,\n        label=\"95% Confidence Interval\",\n    )\n    plt.scatter(X, y, color=\"red\", label=\"Training Data\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"Prediction\")\n    plt.title(\"1D Kriging Surrogate\")\n    plt.legend()\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.plot2d","title":"<code>plot2d(model, X, y, show=True, alpha=0.8)</code>","text":"<p>Plots the 2D Kriging surrogate model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>A fitted Kriging model.</p> required <code>X</code> <code>ndarray</code> <p>Training input data of shape (n_samples, 2).</p> required <code>y</code> <code>ndarray</code> <p>Training target values of shape (n_samples,).</p> required <code>show</code> <code>bool</code> <p>If True, displays the plot. Defaults to True.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Transparency level for 3D surface plots. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging\n&gt;&gt;&gt; # Training data\n&gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt; # Initialize and fit the Kriging model\n&gt;&gt;&gt; model = Kriging().fit(X_train, y_train)\n&gt;&gt;&gt; # Plot the 2D Kriging surrogate\n&gt;&gt;&gt; plot2d(model, X_train, y_train)\n</code></pre> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>def plot2d(model, X: np.ndarray, y: np.ndarray, show: Optional[bool] = True, alpha=0.8) -&gt; None:\n    \"\"\"\n    Plots the 2D Kriging surrogate model.\n\n    Args:\n        model (object): A fitted Kriging model.\n        X (np.ndarray): Training input data of shape (n_samples, 2).\n        y (np.ndarray): Training target values of shape (n_samples,).\n        show (bool): If True, displays the plot. Defaults to True.\n        alpha (float): Transparency level for 3D surface plots. Defaults to 0.8.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging\n        &gt;&gt;&gt; # Training data\n        &gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n        &gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; # Initialize and fit the Kriging model\n        &gt;&gt;&gt; model = Kriging().fit(X_train, y_train)\n        &gt;&gt;&gt; # Plot the 2D Kriging surrogate\n        &gt;&gt;&gt; plot2d(model, X_train, y_train)\n    \"\"\"\n    if X.shape[1] != 2:\n        raise ValueError(\"plot2d is only supported for 2D input data.\")\n\n    fig = plt.figure(figsize=(12, 10))\n    n_grid = 100\n    x1 = linspace(X[:, 0].min(), X[:, 0].max(), num=n_grid)\n    x2 = linspace(X[:, 1].min(), X[:, 1].max(), num=n_grid)\n    X1, X2 = meshgrid(x1, x2)\n    grid_points = array([X1.ravel(), X2.ravel()]).T\n\n    y_pred, y_std = model.predict(grid_points, return_std=True)\n    Z_pred = y_pred.reshape(X1.shape)\n    Z_std = y_std.reshape(X1.shape)\n\n    # Plot predicted values\n    ax1 = fig.add_subplot(221, projection=\"3d\")\n    ax1.plot_surface(X1, X2, Z_pred, cmap=\"viridis\", alpha=alpha)\n    ax1.set_title(\"Prediction Surface\")\n    ax1.set_xlabel(\"X1\")\n    ax1.set_ylabel(\"X2\")\n    ax1.set_zlabel(\"Prediction\")\n\n    # Plot prediction error\n    ax2 = fig.add_subplot(222, projection=\"3d\")\n    ax2.plot_surface(X1, X2, Z_std, cmap=\"viridis\", alpha=alpha)\n    ax2.set_title(\"Prediction Error Surface\")\n    ax2.set_xlabel(\"X1\")\n    ax2.set_ylabel(\"X2\")\n    ax2.set_zlabel(\"Error\")\n\n    # Contour plot of predicted values\n    ax3 = fig.add_subplot(223)\n    contour = ax3.contourf(X1, X2, Z_pred, cmap=\"viridis\", levels=30)\n    plt.colorbar(contour, ax=ax3)\n    ax3.scatter(X[:, 0], X[:, 1], color=\"red\", label=\"Training Data\")\n    ax3.set_title(\"Prediction Contour\")\n    ax3.set_xlabel(\"X1\")\n    ax3.set_ylabel(\"X2\")\n    ax3.legend()\n\n    # Contour plot of prediction error\n    ax4 = fig.add_subplot(224)\n    contour = ax4.contourf(X1, X2, Z_std, cmap=\"viridis\", levels=30)\n    plt.colorbar(contour, ax=ax4)\n    ax4.scatter(X[:, 0], X[:, 1], color=\"red\", label=\"Training Data\")\n    ax4.set_title(\"Error Contour\")\n    ax4.set_xlabel(\"X1\")\n    ax4.set_ylabel(\"X2\")\n    ax4.legend()\n\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/surrogate/kriging/#spotpython.surrogate.kriging.plotkd","title":"<code>plotkd(model, X, y, i, j, show=True, alpha=0.8, eps=0.001, var_names=None)</code>","text":"<p>Plots the Kriging surrogate model for k-dimensional input data by varying two dimensions (i, j).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>A fitted Kriging model.</p> required <code>X</code> <code>ndarray</code> <p>Training input data of shape (n_samples, k).</p> required <code>y</code> <code>ndarray</code> <p>Training target values of shape (n_samples,).</p> required <code>i</code> <code>int</code> <p>The first dimension to vary.</p> required <code>j</code> <code>int</code> <p>The second dimension to vary.</p> required <code>show</code> <code>bool</code> <p>If True, displays the plot. Defaults to True.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Transparency level for 3D surface plots. Defaults to 0.8.</p> <code>0.8</code> <code>eps</code> <code>float</code> <p>Tolerance for considering points as \u201con the surface\u201d. Defaults to 1e-3.</p> <code>0.001</code> <code>var_names</code> <code>List[str]</code> <p>A list of three strings for axis labels. The first entry is for the x-axis, the second for the y-axis, and the third for the z-axis. If empty or None, default axis labels are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging, plotkd\n&gt;&gt;&gt; # Training data\n&gt;&gt;&gt; X_train = np.array([[0.0, 0.0, 0.0], [0.5, 0.5, 0.5], [1.0, 1.0, 1.0]])\n&gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt; # Initialize and fit the Kriging model\n&gt;&gt;&gt; model = Kriging().fit(X_train, y_train)\n&gt;&gt;&gt; # Plot the 3D Kriging surrogate\n&gt;&gt;&gt; plotkd(model, X_train, y_train, 0, 1)\n</code></pre> Source code in <code>spotpython/surrogate/kriging.py</code> <pre><code>def plotkd(\n    model,\n    X: np.ndarray,\n    y: np.ndarray,\n    i: int,\n    j: int,\n    show: Optional[bool] = True,\n    alpha=0.8,\n    eps=1e-3,\n    var_names: Optional[List[str]] = None,\n) -&gt; None:\n    \"\"\"\n    Plots the Kriging surrogate model for k-dimensional input data by varying two dimensions (i, j).\n\n    Args:\n        model (object): A fitted Kriging model.\n        X (np.ndarray): Training input data of shape (n_samples, k).\n        y (np.ndarray): Training target values of shape (n_samples,).\n        i (int): The first dimension to vary.\n        j (int): The second dimension to vary.\n        show (bool): If True, displays the plot. Defaults to True.\n        alpha (float): Transparency level for 3D surface plots. Defaults to 0.8.\n        eps (float): Tolerance for considering points as \"on the surface\". Defaults to 1e-3.\n        var_names (List[str], optional): A list of three strings for axis labels.\n            The first entry is for the x-axis, the second for the y-axis, and the third for the z-axis.\n            If empty or None, default axis labels are used.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.surrogate.kriging import Kriging, plotkd\n        &gt;&gt;&gt; # Training data\n        &gt;&gt;&gt; X_train = np.array([[0.0, 0.0, 0.0], [0.5, 0.5, 0.5], [1.0, 1.0, 1.0]])\n        &gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt; # Initialize and fit the Kriging model\n        &gt;&gt;&gt; model = Kriging().fit(X_train, y_train)\n        &gt;&gt;&gt; # Plot the 3D Kriging surrogate\n        &gt;&gt;&gt; plotkd(model, X_train, y_train, 0, 1)\n    \"\"\"\n    k = X.shape[1]\n    if i &gt;= k or j &gt;= k:\n        raise ValueError(f\"Dimensions i and j must be less than the number of features (k={k}).\")\n    if i == j:\n        raise ValueError(\"Dimensions i and j must be different.\")\n\n    # Compute the mean values for all dimensions\n    mean_values = X.mean(axis=0)\n\n    # Create a grid for the two varied dimensions\n    n_grid = 100\n    x_i = linspace(X[:, i].min(), X[:, i].max(), num=n_grid)\n    x_j = linspace(X[:, j].min(), X[:, j].max(), num=n_grid)\n    X_i, X_j = meshgrid(x_i, x_j)\n\n    # Prepare the grid points for prediction\n    grid_points = np.zeros((X_i.size, k))\n    grid_points[:, i] = X_i.ravel()\n    grid_points[:, j] = X_j.ravel()\n\n    # Set the remaining dimensions to their mean values\n    for dim in range(k):\n        if dim != i and dim != j:\n            grid_points[:, dim] = mean_values[dim]\n\n    # Predict the values and standard deviations\n    y_pred, y_std = model.predict(grid_points, return_std=True)\n    Z_pred = y_pred.reshape(X_i.shape)\n    Z_std = y_std.reshape(X_i.shape)\n\n    # Plot the results\n    fig = plt.figure(figsize=(12, 10))\n\n    # Plot predicted values\n    ax1 = fig.add_subplot(221, projection=\"3d\")\n    ax1.plot_surface(X_i, X_j, Z_pred, cmap=\"viridis\", alpha=alpha)\n    ax1.set_title(\"Prediction Surface\")\n    ax1.set_xlabel(var_names[0] if var_names else f\"Dimension {i}\")\n    ax1.set_ylabel(var_names[1] if var_names else f\"Dimension {j}\")\n    ax1.set_zlabel(var_names[2] if var_names else \"Prediction\")\n\n    # Add input points to the prediction surface\n    for idx in range(X.shape[0]):\n        x_point = X[idx, i]\n        y_point = X[idx, j]\n        z_actual = y[idx]\n        z_predicted = model.predict(X[idx].reshape(1, -1))[0]\n\n        if z_actual &gt; z_predicted + eps:\n            color = \"red\"\n        elif z_actual &lt; z_predicted - eps:\n            color = \"green\"\n        else:\n            color = \"white\"\n\n        ax1.scatter(x_point, y_point, z_actual, color=color, s=50, edgecolor=\"black\")\n\n    # Plot prediction error\n    ax2 = fig.add_subplot(222, projection=\"3d\")\n    ax2.plot_surface(X_i, X_j, Z_std, cmap=\"viridis\", alpha=alpha)\n    ax2.set_title(\"Prediction Error Surface\")\n    ax2.set_xlabel(var_names[0] if var_names else f\"Dimension {i}\")\n    ax2.set_ylabel(var_names[1] if var_names else f\"Dimension {j}\")\n    ax2.set_zlabel(var_names[2] if var_names else \"Error\")\n\n    # Add input points to the error surface\n    for idx in range(X.shape[0]):\n        x_point = X[idx, i]\n        y_point = X[idx, j]\n        z_actual = y[idx]\n        z_predicted = model.predict(X[idx].reshape(1, -1))[0]\n\n        if z_actual &gt; z_predicted + eps:\n            color = \"red\"\n        elif z_actual &lt; z_predicted - eps:\n            color = \"green\"\n        else:\n            color = \"white\"\n\n        ax2.scatter(x_point, y_point, abs(z_actual - z_predicted), color=color, s=50, edgecolor=\"black\")\n\n    # Contour plot of predicted values\n    ax3 = fig.add_subplot(223)\n    contour = ax3.contourf(X_i, X_j, Z_pred, cmap=\"viridis\", levels=30)\n    plt.colorbar(contour, ax=ax3)\n    for idx in range(X.shape[0]):\n        x_point = X[idx, i]\n        y_point = X[idx, j]\n        z_actual = y[idx]\n        z_predicted = model.predict(X[idx].reshape(1, -1))[0]\n\n        if z_actual &gt; z_predicted + eps:\n            color = \"red\"\n        elif z_actual &lt; z_predicted - eps:\n            color = \"green\"\n        else:\n            color = \"white\"\n\n        ax3.scatter(x_point, y_point, color=color, s=50, edgecolor=\"black\")\n    ax3.set_title(\"Prediction Contour\")\n    ax3.set_xlabel(var_names[0] if var_names else f\"Dimension {i}\")\n    ax3.set_ylabel(var_names[1] if var_names else f\"Dimension {j}\")\n\n    # Contour plot of prediction error\n    ax4 = fig.add_subplot(224)\n    contour = ax4.contourf(X_i, X_j, Z_std, cmap=\"viridis\", levels=30)\n    plt.colorbar(contour, ax=ax4)\n    for idx in range(X.shape[0]):\n        x_point = X[idx, i]\n        y_point = X[idx, j]\n        z_actual = y[idx]\n        z_predicted = model.predict(X[idx].reshape(1, -1))[0]\n\n        if z_actual &gt; z_predicted + eps:\n            color = \"red\"\n        elif z_actual &lt; z_predicted - eps:\n            color = \"green\"\n        else:\n            color = \"white\"\n\n        ax4.scatter(x_point, y_point, color=color, s=50, edgecolor=\"black\")\n    ax4.set_title(\"Error Contour\")\n    ax4.set_xlabel(var_names[0] if var_names else f\"Dimension {i}\")\n    ax4.set_ylabel(var_names[1] if var_names else f\"Dimension {j}\")\n\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/surrogate/functions/forr08a/","title":"forr08a","text":""},{"location":"reference/spotpython/surrogate/functions/forr08a/#spotpython.surrogate.functions.forr08a.aerofoilcd","title":"<code>aerofoilcd(X)</code>","text":"<p>Computes the drag coefficient (cd) of an aerofoil based on the shape parameter X.</p> <p>This function reads the drag coefficient data from the \u201ccd_data.csv\u201d file and uses the input X (rounded to the nearest 0.01) to return the corresponding drag coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 1D NumPy array of values in the range [0, 1] representing the shape parameters.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D NumPy array of drag coefficients (cd) corresponding to the input X.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any value in X is outside the range [0, 1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import aerofoilcd\n&gt;&gt;&gt; X = np.array([0.5, 0.75])\n&gt;&gt;&gt; aerofoilcd(X)\narray([0.029975, 0.033375])\n</code></pre> Source code in <code>spotpython/surrogate/functions/forr08a.py</code> <pre><code>def aerofoilcd(X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Computes the drag coefficient (cd) of an aerofoil based on the shape parameter X.\n\n    This function reads the drag coefficient data from the \"cd_data.csv\" file and uses\n    the input X (rounded to the nearest 0.01) to return the corresponding drag coefficients.\n\n    Args:\n        X (np.ndarray): A 1D NumPy array of values in the range [0, 1] representing the shape parameters.\n\n    Returns:\n        np.ndarray: A 1D NumPy array of drag coefficients (cd) corresponding to the input X.\n\n    Raises:\n        ValueError: If any value in X is outside the range [0, 1].\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import aerofoilcd\n        &gt;&gt;&gt; X = np.array([0.5, 0.75])\n        &gt;&gt;&gt; aerofoilcd(X)\n        array([0.029975, 0.033375])\n    \"\"\"\n    # Ensure X is a NumPy array\n    X = np.asarray(X)\n\n    # Validate the input\n    if np.any((X &lt; 0) | (X &gt; 1)):\n        raise ValueError(\"All values in X must be in the range [0, 1].\")\n\n    # The given data as a string\n    data = (\n        \"0.031745,0.031568,0.031355,0.031607,0.03132,0.031242,0.030959,0.030593,0.030347,\"\n        \"0.030153,0.030089,0.029881,0.029967,0.029686,0.029612,0.029727,0.029445,0.030188,\"\n        \"0.029907,0.029634,0.02978,0.029585,0.029301,0.029543,0.029663,0.029137,0.029611,\"\n        \"0.029395,0.02918,0.029369,0.029272,0.029384,0.029249,0.029545,0.029641,0.029975,\"\n        \"0.029801,0.029857,0.030131,0.029678,0.029451,0.029899,0.029922,0.030228,0.02979,\"\n        \"0.03004,0.030188,0.030366,0.030399,0.030193,0.030012,0.030109,0.030629,0.030551,\"\n        \"0.030721,0.031211,0.031132,0.031236,0.031379,0.031531,0.03117,0.031808,0.0318,\"\n        \"0.032141,0.032216,0.032451,0.032545,0.032836,0.032843,0.032888,0.033098,0.033271,\"\n        \"0.033478,0.03328,0.033375,0.033979,0.034197,0.034406,0.034315,0.034662,0.035125,\"\n        \"0.035306,0.035021,0.03526,0.035988,0.03579,0.036927,0.036705,0.037232,0.037563,\"\n        \"0.037501,0.037802,0.038302,0.038676,0.038898,0.03891,0.03916,0.039584,0.038509,\"\n        \"0.040168,0.039062\"\n    )\n\n    # Convert the string to a NumPy array\n    cd_data = np.fromstring(data, sep=\",\")\n\n    # Compute the indices based on X (rounded to the nearest 0.01)\n    indices = np.round(X * 100).astype(int)\n\n    # Return the corresponding drag coefficients\n    return cd_data[indices]\n</code></pre>"},{"location":"reference/spotpython/surrogate/functions/forr08a/#spotpython.surrogate.functions.forr08a.branin","title":"<code>branin(x)</code>","text":"<p>Branin\u2019s test function that takes a 2D input vector <code>x</code> in the range [0, 1] for each dimension and returns the corresponding scalar function value. The function is vectorized to handle multiple inputs.</p> The function is defined as <p>f(x) = a * (X2 - b * X1^2 + c * X1 - d)^2 + e * (1 - ff) * cos(X1) + e + 5 * x1</p> <p>where:     X1 = 15 * x1 - 5     X2 = 15 * x2</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>A 2D NumPy array of shape (n_samples, 2) where each row is a 2D input vector.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The calculated function values for the input <code>x</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>x</code> does not have exactly 2 columns or if any value in <code>x</code> is outside the range [0, 1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import branin\n&gt;&gt;&gt; # Single input\n&gt;&gt;&gt; print(branin(np.array([[0.5, 0.5]])))\n[26.63]\n&gt;&gt;&gt; # Multiple inputs\n&gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.25, 0.25], [0.5, 0.5], [0.75, 0.75], [1.0, 1.0]])\n&gt;&gt;&gt; print(branin(x))\n[308.1291, 34.0028, 26.63, 126.3879, 150.8722]\n</code></pre> Source code in <code>spotpython/surrogate/functions/forr08a.py</code> <pre><code>def branin(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Branin's test function that takes a 2D input vector `x` in the range [0, 1] for each dimension\n    and returns the corresponding scalar function value. The function is vectorized to handle\n    multiple inputs.\n\n    The function is defined as:\n        f(x) = a * (X2 - b * X1^2 + c * X1 - d)^2 + e * (1 - ff) * cos(X1) + e + 5 * x1\n    where:\n        X1 = 15 * x1 - 5\n        X2 = 15 * x2\n\n    Args:\n        x (np.ndarray): A 2D NumPy array of shape (n_samples, 2) where each row is a 2D input vector.\n\n    Returns:\n        np.ndarray: The calculated function values for the input `x`.\n\n    Raises:\n        ValueError: If `x` does not have exactly 2 columns or if any value in `x` is outside the range [0, 1].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import branin\n        &gt;&gt;&gt; # Single input\n        &gt;&gt;&gt; print(branin(np.array([[0.5, 0.5]])))\n        [26.63]\n        &gt;&gt;&gt; # Multiple inputs\n        &gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.25, 0.25], [0.5, 0.5], [0.75, 0.75], [1.0, 1.0]])\n        &gt;&gt;&gt; print(branin(x))\n        [308.1291, 34.0028, 26.63, 126.3879, 150.8722]\n    \"\"\"\n    # Ensure x is a NumPy array\n    x = np.asarray(x)\n\n    # Check if x has exactly 2 columns\n    if x.shape[1] != 2:\n        raise IndexError(\"Input to branin must have exactly 2 columns.\")\n\n    # Check if all values are within the range [0, 1]\n    if np.any((x &lt; 0) | (x &gt; 1)):\n        raise ValueError(\"Variable outside of range - use x in [0, 1] for both dimensions.\")\n\n    # Extract x1 and x2\n    x1, x2 = x[:, 0], x[:, 1]\n\n    # Transform x1 and x2 to X1 and X2\n    X1 = 15 * x1 - 5\n    X2 = 15 * x2\n\n    # Define constants\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    d = 6\n    e = 10\n    ff = 1 / (8 * np.pi)\n\n    # Compute the function values\n    f = a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e + 5 * x1\n\n    return f\n</code></pre>"},{"location":"reference/spotpython/surrogate/functions/forr08a/#spotpython.surrogate.functions.forr08a.onevar","title":"<code>onevar(x)</code>","text":"<p>One-variable test function that takes a scalar or 1D array input <code>x</code> in the range [0, 1] and returns the corresponding function values. The function is vectorized to handle multiple inputs.</p> The function is defined as <p>f(x) = ((6x - 2)^2) * np.sin((6x - 2) * 2)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>A scalar or 1D NumPy array of values in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The calculated function values for the input <code>x</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any value in <code>x</code> is outside the range [0, 1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import onevar\n&gt;&gt;&gt; # Single input\n&gt;&gt;&gt; print(onevar(np.array([0.5])))\n[0.9093]\n&gt;&gt;&gt; # Multiple inputs\n&gt;&gt;&gt; x = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n&gt;&gt;&gt; print(onevar(x))\n[3.0272, -0.2104, 0.9093,  -5.9933, 15.8297]\n</code></pre> Source code in <code>spotpython/surrogate/functions/forr08a.py</code> <pre><code>def onevar(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    One-variable test function that takes a scalar or 1D array input `x` in the range [0, 1]\n    and returns the corresponding function values. The function is vectorized to handle\n    multiple inputs.\n\n    The function is defined as:\n        f(x) = ((6x - 2)^2) * np.sin((6x - 2) * 2)\n\n    Args:\n        x (np.ndarray): A scalar or 1D NumPy array of values in the range [0, 1].\n\n    Returns:\n        np.ndarray: The calculated function values for the input `x`.\n\n    Raises:\n        ValueError: If any value in `x` is outside the range [0, 1].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import onevar\n        &gt;&gt;&gt; # Single input\n        &gt;&gt;&gt; print(onevar(np.array([0.5])))\n        [0.9093]\n        &gt;&gt;&gt; # Multiple inputs\n        &gt;&gt;&gt; x = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n        &gt;&gt;&gt; print(onevar(x))\n        [3.0272, -0.2104, 0.9093,  -5.9933, 15.8297]\n    \"\"\"\n    # Ensure x is a NumPy array\n    x = np.asarray(x)\n\n    # Check if all values are within the range [0, 1]\n    if np.any((x &lt; 0) | (x &gt; 1)):\n        raise ValueError(\"Variable outside of range - use x in [0, 1]\")\n\n    # Compute the function values\n    f = ((6 * x - 2) ** 2) * np.sin((6 * x - 2) * 2)\n\n    return f\n</code></pre>"},{"location":"reference/spotpython/torch/activation/","title":"activation","text":""},{"location":"reference/spotpython/torch/cosinewarmupcheduler/","title":"cosinewarmupcheduler","text":""},{"location":"reference/spotpython/torch/cosinewarmupcheduler/#spotpython.torch.cosinewarmupcheduler.CosineWarmupScheduler","title":"<code>CosineWarmupScheduler</code>","text":"<p>               Bases: <code>_LRScheduler</code></p> <p>Cosine annealing with warmup learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to use during training.</p> required <code>warmup</code> <code>int</code> <p>The number of warmup steps.</p> required <code>max_iters</code> <code>int</code> <p>The number of maximum iterations the model is trained for.</p> required Example <p>optimizer = torch.optim.SGD(model.parameters(), lr=0.1) scheduler = CosineWarmupScheduler(optimizer, warmup=10, max_iters=100) for epoch in range(100):     scheduler.step()     train(\u2026)</p> Source code in <code>spotpython/torch/cosinewarmupcheduler.py</code> <pre><code>class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n    \"\"\"Cosine annealing with warmup learning rate scheduler.\n\n    Args:\n        optimizer (torch.optim.Optimizer): The optimizer to use during training.\n        warmup (int): The number of warmup steps.\n        max_iters (int): The number of maximum iterations the model is trained for.\n\n    Example:\n        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n        &gt;&gt;&gt; scheduler = CosineWarmupScheduler(optimizer, warmup=10, max_iters=100)\n        &gt;&gt;&gt; for epoch in range(100):\n        &gt;&gt;&gt;     scheduler.step()\n        &gt;&gt;&gt;     train(...)\n    \"\"\"\n\n    def __init__(self, optimizer, warmup, max_iters):\n        self.warmup = warmup\n        self.max_num_iters = max_iters\n        super().__init__(optimizer)\n\n    def get_lr(self):\n        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n        return [base_lr * lr_factor for base_lr in self.base_lrs]\n\n    def get_lr_factor(self, epoch):\n        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n        if epoch &lt;= self.warmup:\n            lr_factor *= epoch * 1.0 / self.warmup\n        return lr_factor\n</code></pre>"},{"location":"reference/spotpython/torch/dataframedataset/","title":"dataframedataset","text":""},{"location":"reference/spotpython/torch/dimensions/","title":"dimensions","text":""},{"location":"reference/spotpython/torch/dimensions/#spotpython.torch.dimensions.extract_linear_dims","title":"<code>extract_linear_dims(model)</code>","text":"<p>Extracts the input and output dimensions of the Linear layers in a PyTorch model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model.</p> required <p>Returns:</p> Type Description <code>array</code> <p>np.array: Array with the input and output dimensions of the Linear layers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.torch.dimensions import extract_linear_dims\n&gt;&gt;&gt; net = NNLinearRegressor()\n&gt;&gt;&gt; result = extract_linear_dims(net)\n</code></pre> Source code in <code>spotpython/torch/dimensions.py</code> <pre><code>def extract_linear_dims(model) -&gt; np.array:\n    \"\"\"Extracts the input and output dimensions of the Linear layers in a PyTorch model.\n\n    Args:\n        model (nn.Module): PyTorch model.\n\n    Returns:\n        np.array: Array with the input and output dimensions of the Linear layers.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.torch.dimensions import extract_linear_dims\n        &gt;&gt;&gt; net = NNLinearRegressor()\n        &gt;&gt;&gt; result = extract_linear_dims(net)\n\n    \"\"\"\n    dims = []\n    for layer in model.layers:\n        if isinstance(layer, nn.Linear):\n            # Append input and output features of the Linear layer\n            dims.append(layer.in_features)\n            dims.append(layer.out_features)\n    return np.array(dims)\n</code></pre>"},{"location":"reference/spotpython/torch/mapk/","title":"mapk","text":""},{"location":"reference/spotpython/torch/mapk/#spotpython.torch.mapk.MAPK","title":"<code>MAPK</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Mean Average Precision at K (MAPK) metric.</p> <p>This class inherits from the <code>Metric</code> class of the <code>torchmetrics</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of top predictions to consider when calculating the metric.</p> <code>10</code> <code>dist_sync_on_step</code> <code>bool</code> <p>Whether to synchronize the metric states across processes during the forward pass.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>total</code> <code>Tensor</code> <p>The cumulative sum of the metric scores across all batches.</p> <code>count</code> <code>Tensor</code> <p>The number of batches processed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.torch.mapk import MAPK\n    import torch\n    mapk = MAPK(k=2)\n    target = torch.tensor([0, 1, 2, 2])\n    preds = torch.tensor(\n        [\n            [0.5, 0.2, 0.2],  # 0 is in top 2\n            [0.3, 0.4, 0.2],  # 1 is in top 2\n            [0.2, 0.4, 0.3],  # 2 is in top 2\n            [0.7, 0.2, 0.1],  # 2 isn't in top 2\n        ]\n    )\n    mapk.update(preds, target)\n    print(mapk.compute()) # tensor(0.6250)\n</code></pre> Source code in <code>spotpython/torch/mapk.py</code> <pre><code>class MAPK(torchmetrics.Metric):\n    \"\"\"\n    Mean Average Precision at K (MAPK) metric.\n\n    This class inherits from the `Metric` class of the `torchmetrics` library.\n\n    Args:\n        k (int):\n            The number of top predictions to consider when calculating the metric.\n        dist_sync_on_step (bool):\n            Whether to synchronize the metric states across processes during the forward pass.\n\n    Attributes:\n        total (torch.Tensor):\n            The cumulative sum of the metric scores across all batches.\n        count (torch.Tensor):\n            The number of batches processed.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.torch.mapk import MAPK\n            import torch\n            mapk = MAPK(k=2)\n            target = torch.tensor([0, 1, 2, 2])\n            preds = torch.tensor(\n                [\n                    [0.5, 0.2, 0.2],  # 0 is in top 2\n                    [0.3, 0.4, 0.2],  # 1 is in top 2\n                    [0.2, 0.4, 0.3],  # 2 is in top 2\n                    [0.7, 0.2, 0.1],  # 2 isn't in top 2\n                ]\n            )\n            mapk.update(preds, target)\n            print(mapk.compute()) # tensor(0.6250)\n    \"\"\"\n\n    def __init__(self, k=10, dist_sync_on_step=False):\n        super().__init__(dist_sync_on_step=dist_sync_on_step)\n        self.k = k\n        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"count\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, predicted: torch.Tensor, actual: torch.Tensor):\n        \"\"\"\n        Update the state variables with a new batch of data.\n\n        Args:\n            predicted (torch.Tensor):\n                A 2D tensor containing the predicted scores for each class.\n            actual (torch.Tensor):\n                A 1D tensor containing the ground truth labels.\n        Returns:\n            (NoneType): None\n\n        Examples:\n            &gt;&gt;&gt; from spotpython.torch.mapk import MAPK\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; mapk = MAPK(k=2)\n            &gt;&gt;&gt; target = torch.tensor([0, 1, 2, 2])\n            &gt;&gt;&gt; preds = torch.tensor(\n            ...     [\n            ...         [0.5, 0.2, 0.2],  # 0 is in top 2\n            ...         [0.3, 0.4, 0.2],  # 1 is in top 2\n            ...         [0.2, 0.4, 0.3],  # 2 is in top 2\n            ...         [0.7, 0.2, 0.1],  # 2 isn't in top 2\n            ...     ]\n            ... )\n            &gt;&gt;&gt; mapk.update(preds, target)\n            &gt;&gt;&gt; print(mapk.compute()) # tensor(0.6250)\n\n        Raises:\n            AssertionError: If the actual tensor is not 1D or the predicted tensor is not 2D.\n            AssertionError: If the number of elements in the actual and predicted tensors are not equal.\n\n        \"\"\"\n        assert len(actual.shape) == 1, \"actual must be a 1D tensor\"\n        assert len(predicted.shape) == 2, \"predicted must be a 2D tensor\"\n        assert actual.shape[0] == predicted.shape[0], \"actual and predicted must have the same number of elements\"\n\n        # Convert actual to list of lists\n        actual = actual.tolist()\n        actual = [[a] for a in actual]\n\n        # Convert predicted to list of lists of indices sorted by confidence score\n        _, predicted = predicted.topk(k=self.k, dim=1)\n        predicted = predicted.tolist()\n        # Code modified according to: \"Inplace update to inference tensor outside InferenceMode\n        # is not allowed. You can make a clone to get a normal tensor before doing inplace update.\"\n        score = np.mean([self.apk(p, a, self.k) for p, a in zip(predicted, actual)])\n        self.total = self.total + score\n        self.count = self.count + 1\n\n    def compute(self) -&gt; float:\n        \"\"\"\n        Compute the mean average precision at k.\n\n        Args:\n            self (MAPK):\n                The current instance of the class.\n\n        Returns:\n            (float):\n                The mean average precision at k.\n\n        Examples:\n            &gt;&gt;&gt; evaluator = Evaluator()\n            &gt;&gt;&gt; evaluator.total = 3.0\n            &gt;&gt;&gt; evaluator.count = 2\n            &gt;&gt;&gt; evaluator.compute()\n            1.5\n        \"\"\"\n        return self.total / self.count\n\n    @staticmethod\n    def apk(predicted: List[int], actual: List[int], k: int = 10) -&gt; float:\n        \"\"\"\n        Calculate the average precision at k for a single pair of actual and predicted labels.\n\n        Args:\n            predicted (list): A list of predicted labels.\n            actual (list): A list of ground truth labels.\n            k (int): The number of top predictions to consider.\n\n        Returns:\n            float: The average precision at k.\n\n        Examples:\n            &gt;&gt;&gt; Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3)\n            0.8888888888888888\n        \"\"\"\n        if not actual:\n            return 0.0\n\n        if len(predicted) &gt; k:\n            predicted = predicted[:k]\n\n        score = 0.0\n        num_hits = 0.0\n\n        for i, p in enumerate(predicted):\n            if p in actual and p not in predicted[:i]:\n                num_hits += 1.0\n                score += num_hits / (i + 1.0)\n\n        return score / min(len(actual), k)\n</code></pre>"},{"location":"reference/spotpython/torch/mapk/#spotpython.torch.mapk.MAPK.apk","title":"<code>apk(predicted, actual, k=10)</code>  <code>staticmethod</code>","text":"<p>Calculate the average precision at k for a single pair of actual and predicted labels.</p> <p>Parameters:</p> Name Type Description Default <code>predicted</code> <code>list</code> <p>A list of predicted labels.</p> required <code>actual</code> <code>list</code> <p>A list of ground truth labels.</p> required <code>k</code> <code>int</code> <p>The number of top predictions to consider.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The average precision at k.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3)\n0.8888888888888888\n</code></pre> Source code in <code>spotpython/torch/mapk.py</code> <pre><code>@staticmethod\ndef apk(predicted: List[int], actual: List[int], k: int = 10) -&gt; float:\n    \"\"\"\n    Calculate the average precision at k for a single pair of actual and predicted labels.\n\n    Args:\n        predicted (list): A list of predicted labels.\n        actual (list): A list of ground truth labels.\n        k (int): The number of top predictions to consider.\n\n    Returns:\n        float: The average precision at k.\n\n    Examples:\n        &gt;&gt;&gt; Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3)\n        0.8888888888888888\n    \"\"\"\n    if not actual:\n        return 0.0\n\n    if len(predicted) &gt; k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i, p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i + 1.0)\n\n    return score / min(len(actual), k)\n</code></pre>"},{"location":"reference/spotpython/torch/mapk/#spotpython.torch.mapk.MAPK.compute","title":"<code>compute()</code>","text":"<p>Compute the mean average precision at k.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>MAPK</code> <p>The current instance of the class.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean average precision at k.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; evaluator = Evaluator()\n&gt;&gt;&gt; evaluator.total = 3.0\n&gt;&gt;&gt; evaluator.count = 2\n&gt;&gt;&gt; evaluator.compute()\n1.5\n</code></pre> Source code in <code>spotpython/torch/mapk.py</code> <pre><code>def compute(self) -&gt; float:\n    \"\"\"\n    Compute the mean average precision at k.\n\n    Args:\n        self (MAPK):\n            The current instance of the class.\n\n    Returns:\n        (float):\n            The mean average precision at k.\n\n    Examples:\n        &gt;&gt;&gt; evaluator = Evaluator()\n        &gt;&gt;&gt; evaluator.total = 3.0\n        &gt;&gt;&gt; evaluator.count = 2\n        &gt;&gt;&gt; evaluator.compute()\n        1.5\n    \"\"\"\n    return self.total / self.count\n</code></pre>"},{"location":"reference/spotpython/torch/mapk/#spotpython.torch.mapk.MAPK.update","title":"<code>update(predicted, actual)</code>","text":"<p>Update the state variables with a new batch of data.</p> <p>Parameters:</p> Name Type Description Default <code>predicted</code> <code>Tensor</code> <p>A 2D tensor containing the predicted scores for each class.</p> required <code>actual</code> <code>Tensor</code> <p>A 1D tensor containing the ground truth labels.</p> required <p>Returns:     (NoneType): None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.torch.mapk import MAPK\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; mapk = MAPK(k=2)\n&gt;&gt;&gt; target = torch.tensor([0, 1, 2, 2])\n&gt;&gt;&gt; preds = torch.tensor(\n...     [\n...         [0.5, 0.2, 0.2],  # 0 is in top 2\n...         [0.3, 0.4, 0.2],  # 1 is in top 2\n...         [0.2, 0.4, 0.3],  # 2 is in top 2\n...         [0.7, 0.2, 0.1],  # 2 isn't in top 2\n...     ]\n... )\n&gt;&gt;&gt; mapk.update(preds, target)\n&gt;&gt;&gt; print(mapk.compute()) # tensor(0.6250)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the actual tensor is not 1D or the predicted tensor is not 2D.</p> <code>AssertionError</code> <p>If the number of elements in the actual and predicted tensors are not equal.</p> Source code in <code>spotpython/torch/mapk.py</code> <pre><code>def update(self, predicted: torch.Tensor, actual: torch.Tensor):\n    \"\"\"\n    Update the state variables with a new batch of data.\n\n    Args:\n        predicted (torch.Tensor):\n            A 2D tensor containing the predicted scores for each class.\n        actual (torch.Tensor):\n            A 1D tensor containing the ground truth labels.\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.torch.mapk import MAPK\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; mapk = MAPK(k=2)\n        &gt;&gt;&gt; target = torch.tensor([0, 1, 2, 2])\n        &gt;&gt;&gt; preds = torch.tensor(\n        ...     [\n        ...         [0.5, 0.2, 0.2],  # 0 is in top 2\n        ...         [0.3, 0.4, 0.2],  # 1 is in top 2\n        ...         [0.2, 0.4, 0.3],  # 2 is in top 2\n        ...         [0.7, 0.2, 0.1],  # 2 isn't in top 2\n        ...     ]\n        ... )\n        &gt;&gt;&gt; mapk.update(preds, target)\n        &gt;&gt;&gt; print(mapk.compute()) # tensor(0.6250)\n\n    Raises:\n        AssertionError: If the actual tensor is not 1D or the predicted tensor is not 2D.\n        AssertionError: If the number of elements in the actual and predicted tensors are not equal.\n\n    \"\"\"\n    assert len(actual.shape) == 1, \"actual must be a 1D tensor\"\n    assert len(predicted.shape) == 2, \"predicted must be a 2D tensor\"\n    assert actual.shape[0] == predicted.shape[0], \"actual and predicted must have the same number of elements\"\n\n    # Convert actual to list of lists\n    actual = actual.tolist()\n    actual = [[a] for a in actual]\n\n    # Convert predicted to list of lists of indices sorted by confidence score\n    _, predicted = predicted.topk(k=self.k, dim=1)\n    predicted = predicted.tolist()\n    # Code modified according to: \"Inplace update to inference tensor outside InferenceMode\n    # is not allowed. You can make a clone to get a normal tensor before doing inplace update.\"\n    score = np.mean([self.apk(p, a, self.k) for p, a in zip(predicted, actual)])\n    self.total = self.total + score\n    self.count = self.count + 1\n</code></pre>"},{"location":"reference/spotpython/torch/netcifar10/","title":"netcifar10","text":""},{"location":"reference/spotpython/torch/netcore/","title":"netcore","text":""},{"location":"reference/spotpython/torch/netfashionMNIST/","title":"netfashionMNIST","text":""},{"location":"reference/spotpython/torch/netregression/","title":"netregression","text":""},{"location":"reference/spotpython/torch/netvbdp/","title":"netvbdp","text":""},{"location":"reference/spotpython/torch/traintest/","title":"traintest","text":""},{"location":"reference/spotpython/uc/plot/","title":"plot","text":""},{"location":"reference/spotpython/uc/plot/#spotpython.uc.plot.plot_predictionintervals","title":"<code>plot_predictionintervals(y_train, y_train_pred, y_train_pred_low, y_train_pred_high, y_test, y_test_pred, y_test_pred_low, y_test_pred_high, suptitle, figsize=(10, 10))</code>","text":"<p>Plots prediction intervals for training and testing data. This function generates four subplots arranged in a 2x2 grid: 1. True vs predicted values with error bars representing prediction intervals. 2. Prediction interval width vs true values. 3. Ordered prediction interval widths for both training and testing data. 4. Histograms of the interval widths for training and testing data.</p> <p>Parameters:</p> Name Type Description Default <code>y_train</code> <code>array - like</code> <p>True values for the training set.</p> required <code>y_train_pred</code> <code>array - like</code> <p>Predicted values for the training set.</p> required <code>y_train_pred_low</code> <code>array - like</code> <p>Lower bounds of prediction intervals for the training set.</p> required <code>y_train_pred_high</code> <code>array - like</code> <p>Upper bounds of prediction intervals for the training set.</p> required <code>y_test</code> <code>array - like</code> <p>True values for the testing set.</p> required <code>y_test_pred</code> <code>array - like</code> <p>Predicted values for the testing set.</p> required <code>y_test_pred_low</code> <code>array - like</code> <p>Lower bounds of prediction intervals for the testing set.</p> required <code>y_test_pred_high</code> <code>array - like</code> <p>Upper bounds of prediction intervals for the testing set.</p> required <code>suptitle</code> <code>str</code> <p>The title for the entire figure.</p> required <code>figsize</code> <code>tuple</code> <p>Size of the figure. Default is (10, 10).</p> <code>(10, 10)</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function displays the plots but does not return any value.</p> Source code in <code>spotpython/uc/plot.py</code> <pre><code>def plot_predictionintervals(\n    y_train,\n    y_train_pred,\n    y_train_pred_low,\n    y_train_pred_high,\n    y_test,\n    y_test_pred,\n    y_test_pred_low,\n    y_test_pred_high,\n    suptitle: str,\n    figsize: tuple = (10, 10),  # Default figsize added\n) -&gt; None:\n    \"\"\"\n    Plots prediction intervals for training and testing data.\n    This function generates four subplots arranged in a 2x2 grid:\n    1. True vs predicted values with error bars representing prediction intervals.\n    2. Prediction interval width vs true values.\n    3. Ordered prediction interval widths for both training and testing data.\n    4. Histograms of the interval widths for training and testing data.\n\n    Args:\n        y_train (array-like): True values for the training set.\n        y_train_pred (array-like): Predicted values for the training set.\n        y_train_pred_low (array-like): Lower bounds of prediction intervals for the training set.\n        y_train_pred_high (array-like): Upper bounds of prediction intervals for the training set.\n        y_test (array-like): True values for the testing set.\n        y_test_pred (array-like): Predicted values for the testing set.\n        y_test_pred_low (array-like): Lower bounds of prediction intervals for the testing set.\n        y_test_pred_high (array-like): Upper bounds of prediction intervals for the testing set.\n        suptitle (str): The title for the entire figure.\n        figsize (tuple, optional): Size of the figure. Default is (10, 10).\n\n    Returns:\n        None: The function displays the plots but does not return any value.\n    \"\"\"\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)  # Use figsize parameter\n\n    ax1.errorbar(\n        x=y_train,\n        y=y_train_pred,\n        yerr=(y_train_pred - y_train_pred_low, y_train_pred_high - y_train_pred),\n        alpha=0.8,\n        label=\"train\",\n        fmt=\".\",\n    )\n    ax1.errorbar(\n        x=y_test,\n        y=y_test_pred,\n        yerr=(y_test_pred - y_test_pred_low, y_test_pred_high - y_test_pred),\n        alpha=0.8,\n        label=\"test\",\n        fmt=\".\",\n    )\n    ax1.plot(\n        [y_train.min(), y_train.max()],\n        [y_train.min(), y_train.max()],\n        color=\"gray\",\n        alpha=0.5,\n    )\n    ax1.set_xlabel(\"True values\", fontsize=12)\n    ax1.set_ylabel(\"Predicted values\", fontsize=12)\n    ax1.legend()\n    ax1.set_title(\"True vs predicted values\")\n\n    ax2.scatter(x=y_train, y=y_train_pred_high - y_train_pred_low, alpha=0.8, label=\"train\", marker=\".\")\n    ax2.scatter(x=y_test, y=y_test_pred_high - y_test_pred_low, alpha=0.8, label=\"test\", marker=\".\")\n    ax2.set_xlabel(\"True values\", fontsize=12)\n    ax2.set_ylabel(\"Interval width\", fontsize=12)\n    ax2.set_xscale(\"linear\")\n    ax2.set_ylim([0, np.max(y_test_pred_high - y_test_pred_low) * 1.1])\n    ax2.legend()\n    ax2.set_title(\"Prediction interval width vs true values\")\n\n    std_all = np.concatenate([y_train_pred_high - y_train_pred_low, y_test_pred_high - y_test_pred_low])\n    type_all = np.array([\"train\"] * len(y_train) + [\"test\"] * len(y_test))\n    x_all = np.arange(len(std_all))\n    order_all = np.argsort(std_all)\n    std_order = std_all[order_all]\n    type_order = type_all[order_all]\n    ax3.scatter(\n        x=x_all[type_order == \"train\"],\n        y=std_order[type_order == \"train\"],\n        alpha=0.8,\n        label=\"train\",\n        marker=\".\",\n    )\n    ax3.scatter(\n        x=x_all[type_order == \"test\"],\n        y=std_order[type_order == \"test\"],\n        alpha=0.8,\n        label=\"test\",\n        marker=\".\",\n    )\n    ax3.set_xlabel(\"Order\", fontsize=12)\n    ax3.set_ylabel(\"Interval width\", fontsize=12)\n    ax3.legend()\n    ax3.set_title(\"Ordered prediction interval width\")\n\n    ax4.hist(y_train_pred_high - y_train_pred_low, alpha=0.5, label=\"train\")\n    ax4.hist(y_test_pred_high - y_test_pred_low, alpha=0.5, label=\"test\")\n    ax4.set_xlabel(\"Interval width\", fontsize=12)\n    ax4.set_ylabel(\"Frequency\", fontsize=12)\n    ax4.legend()\n    ax4.set_title(\"Histogram of interval widths\")\n\n    plt.suptitle(suptitle, size=20)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for suptitle\n    plt.show()\n</code></pre>"},{"location":"reference/spotpython/utils/aggregate/","title":"aggregate","text":""},{"location":"reference/spotpython/utils/aggregate/#spotpython.utils.aggregate.aggregate_mean_var","title":"<code>aggregate_mean_var(X, y, sort=False, var_empirical=False)</code>","text":"<p>Aggregate array to mean and variance per group. Note: The empirical variance might result in nan values. Therefore, the theoretical variance is calculated by default.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array, shape <code>(n, k)</code>.</p> required <code>y</code> <code>ndarray</code> <p>values, shape <code>(n,)</code>.</p> required <code>sort</code> <code>bool</code> <p>Whether to sort the resulting DataFrame by the group keys.</p> <code>False</code> <code>var_empirical</code> <code>bool</code> <p>Whether to calculate the empirical variance. Default is False, which avoids nan values in the variance calculation.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>aggregated <code>X</code> values, shape <code>(n-m, k)</code>, if <code>m</code> duplicates in <code>X</code>.</p> <code>ndarray</code> <p>aggregated (mean per group) <code>y</code> values, shape <code>(1,)</code>, if <code>m</code> duplicates in <code>X</code>.</p> <code>ndarray</code> <p>aggregated (variance per group) <code>y</code> values, shape <code>(1,)</code>, if <code>m</code> duplicates in <code>X</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.aggregate import aggregate_mean_var\n    import numpy as np\n    X = np.array([[1, 2], [3, 4], [1, 2]])\n    y = np.array([1, 2, 3])\n    X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n    print(X_agg)\n    [[1. 2.] [3. 4.]]\n    print(y_mean)\n    [2. 2.]\n    print(y_var)\n    [1 0]\n# Empirical variance might result in nan values, see the example below\n&gt;&gt;&gt; X_agg, y_mean, y_var = aggregate_mean_var(X, y, var_empirical=True)\n    print(X_agg)\n    print(y_mean)\n    print(y_var)\n    [[1 2]\n    [3 4]]\n    [2. 2.]\n    [ 2. nan]\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1,2]])\n    y = np.array([1, 2, 3, 4, 5])\n    X_agg, y_mean, y_var = aggregate_mean_var(X, y, var_empirical=True)\n    print(X_agg)\n    print(y_mean)\n    print(y_var)\n    [[1 2]\n    [3 4]]\n    [3. 3.]\n    [4. 2.]\n&gt;&gt;&gt; X_1 = np.ones((2, 3))\n    y_1 = np.sum(X_1, axis=1)\n    y_2 = 2 * y_1\n    X_2 = np.append(X_1, 2 * X_1, axis=0)\n    X = np.append(X_2, X_1, axis=0)\n    y = np.append(y_1, y_2, axis=0)\n    y = np.append(y, y_2, axis=0)\n    print(X)\n    print(y)\n    Z = aggregate_mean_var(X, y, var_empirical=True)\n    print(Z)\n    [[1. 1. 1.]\n    [1. 1. 1.]\n    [2. 2. 2.]\n    [2. 2. 2.]\n    [1. 1. 1.]\n    [1. 1. 1.]]\n    [3. 3. 6. 6. 6. 6.]\n    (array([[1., 1., 1.],\n        [2., 2., 2.]]), array([4.5, 6. ]), array([3., 0.]))\n</code></pre> Source code in <code>spotpython/utils/aggregate.py</code> <pre><code>def aggregate_mean_var(X, y, sort=False, var_empirical=False) -&gt; (np.ndarray, np.ndarray, np.ndarray):\n    \"\"\"\n    Aggregate array to mean and variance per group.\n    Note: The empirical variance might result in nan values.\n    Therefore, the theoretical variance is calculated by default.\n\n    Args:\n        X (numpy.ndarray):\n            X array, shape `(n, k)`.\n        y (numpy.ndarray):\n            values, shape `(n,)`.\n        sort (bool):\n            Whether to sort the resulting DataFrame by the group keys.\n        var_empirical (bool):\n            Whether to calculate the empirical variance. Default is False, which\n            avoids nan values in the variance calculation.\n\n    Returns:\n        (numpy.ndarray):\n            aggregated `X` values, shape `(n-m, k)`, if `m` duplicates in `X`.\n        (numpy.ndarray):\n            aggregated (mean per group) `y` values, shape `(1,)`, if `m` duplicates in `X`.\n        (numpy.ndarray):\n            aggregated (variance per group) `y` values, shape `(1,)`, if `m` duplicates in `X`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.aggregate import aggregate_mean_var\n            import numpy as np\n            X = np.array([[1, 2], [3, 4], [1, 2]])\n            y = np.array([1, 2, 3])\n            X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n            print(X_agg)\n            [[1. 2.] [3. 4.]]\n            print(y_mean)\n            [2. 2.]\n            print(y_var)\n            [1 0]\n        # Empirical variance might result in nan values, see the example below\n        &gt;&gt;&gt; X_agg, y_mean, y_var = aggregate_mean_var(X, y, var_empirical=True)\n            print(X_agg)\n            print(y_mean)\n            print(y_var)\n            [[1 2]\n            [3 4]]\n            [2. 2.]\n            [ 2. nan]\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1,2]])\n            y = np.array([1, 2, 3, 4, 5])\n            X_agg, y_mean, y_var = aggregate_mean_var(X, y, var_empirical=True)\n            print(X_agg)\n            print(y_mean)\n            print(y_var)\n            [[1 2]\n            [3 4]]\n            [3. 3.]\n            [4. 2.]\n        &gt;&gt;&gt; X_1 = np.ones((2, 3))\n            y_1 = np.sum(X_1, axis=1)\n            y_2 = 2 * y_1\n            X_2 = np.append(X_1, 2 * X_1, axis=0)\n            X = np.append(X_2, X_1, axis=0)\n            y = np.append(y_1, y_2, axis=0)\n            y = np.append(y, y_2, axis=0)\n            print(X)\n            print(y)\n            Z = aggregate_mean_var(X, y, var_empirical=True)\n            print(Z)\n            [[1. 1. 1.]\n            [1. 1. 1.]\n            [2. 2. 2.]\n            [2. 2. 2.]\n            [1. 1. 1.]\n            [1. 1. 1.]]\n            [3. 3. 6. 6. 6. 6.]\n            (array([[1., 1., 1.],\n                [2., 2., 2.]]), array([4.5, 6. ]), array([3., 0.]))\n    \"\"\"\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        # convert X and y to numpy arrays\n        X = np.array(X)\n        y = np.array(y)\n\n    if X.ndim != 2 or y.ndim != 1:\n        raise ValueError(\"X must be a 2D array and y must be a 1D array.\")\n\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"The number of rows in X must match the length of y.\")\n\n    # Create a DataFrame from X with y as the group target\n    df = pd.DataFrame(X)\n    df[\"y\"] = y\n\n    # Define a custom function to calculate the theoretical variance\n    def theoretical_var(group):\n        n = len(group)\n        if n == 0:\n            return np.nan\n        mean = group.mean()\n        return ((group - mean) ** 2).sum() / n\n\n    if var_empirical:\n        # Group by all X columns, calculating the mean and empirical variance of y for each group\n        grouped = df.groupby(list(df.columns[:-1]), as_index=False, sort=sort).agg(y_mean=pd.NamedAgg(column=\"y\", aggfunc=\"mean\"), y_var=pd.NamedAgg(column=\"y\", aggfunc=\"var\"))\n    else:\n        # Group by all X columns, calculating the mean and theoretical variance of y for each group\n        grouped = df.groupby(list(df.columns[:-1]), as_index=False, sort=sort).agg(y_mean=pd.NamedAgg(column=\"y\", aggfunc=\"mean\"), y_var=pd.NamedAgg(column=\"y\", aggfunc=theoretical_var))\n\n    # Extract mean and variance results from the grouped DataFrame\n    y_mean = grouped[\"y_mean\"].to_numpy()\n    y_var = grouped[\"y_var\"].to_numpy()\n\n    # Extract the unique X values\n    X_agg = grouped.iloc[:, :-2].to_numpy()\n\n    return X_agg, y_mean, y_var\n</code></pre>"},{"location":"reference/spotpython/utils/aggregate/#spotpython.utils.aggregate.get_ranks","title":"<code>get_ranks(x)</code>","text":"<p>Returns a numpy array containing ranks of numbers within an input numpy array x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>numpy array</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ranks</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_ranks([2, 1])\n    [1, 0]\n&gt;&gt;&gt; get_ranks([20, 10, 100])\n    [1, 0, 2]\n</code></pre> Source code in <code>spotpython/utils/aggregate.py</code> <pre><code>def get_ranks(x):\n    \"\"\"\n    Returns a numpy array containing ranks of numbers within an input numpy array x.\n\n    Args:\n        x (numpy.ndarray): numpy array\n\n    Returns:\n        (numpy.ndarray): ranks\n\n    Examples:\n        &gt;&gt;&gt; get_ranks([2, 1])\n            [1, 0]\n        &gt;&gt;&gt; get_ranks([20, 10, 100])\n            [1, 0, 2]\n    \"\"\"\n    ts = x.argsort()\n    ranks = np.empty_like(ts)\n    ranks[ts] = np.arange(len(x))\n    return ranks\n</code></pre>"},{"location":"reference/spotpython/utils/aggregate/#spotpython.utils.aggregate.select_distant_points","title":"<code>select_distant_points(X, y, k)</code>","text":"<p>Selects k points that are distant from each other using a clustering approach.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array, shape <code>(n, k)</code>.</p> required <code>y</code> <code>ndarray</code> <p>values, shape <code>(n,)</code>.</p> required <code>k</code> <code>int</code> <p>number of points to select.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>selected <code>X</code> values, shape <code>(k, k)</code>.</p> <code>ndarray</code> <p>selected <code>y</code> values, shape <code>(k,)</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.aggregate import select_distant_points\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n    selected_points, selected_y = select_distant_points(X, y, 3)\n    print(selected_points)\n    [[ 5  6]\n    [ 9 10]\n    [ 1  2]]\n    print(selected_y)\n    [3 5 1]\n</code></pre> Source code in <code>spotpython/utils/aggregate.py</code> <pre><code>def select_distant_points(X, y, k):\n    \"\"\"\n    Selects k points that are distant from each other using a clustering approach.\n\n    Args:\n        X (numpy.ndarray): X array, shape `(n, k)`.\n        y (numpy.ndarray): values, shape `(n,)`.\n        k (int): number of points to select.\n\n    Returns:\n        (numpy.ndarray):\n            selected `X` values, shape `(k, k)`.\n        (numpy.ndarray):\n            selected `y` values, shape `(k,)`.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.aggregate import select_distant_points\n            X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n            y = np.array([1, 2, 3, 4, 5])\n            selected_points, selected_y = select_distant_points(X, y, 3)\n            print(selected_points)\n            [[ 5  6]\n            [ 9 10]\n            [ 1  2]]\n            print(selected_y)\n            [3 5 1]\n\n    \"\"\"\n    # Perform k-means clustering to find k clusters\n    kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n    # Find the closest point in X to each cluster center\n    selected_points = np.array([X[np.argmin(np.linalg.norm(X - center, axis=1))] for center in kmeans.cluster_centers_])\n    # Find indices of the selected points in the original X array\n    indices = np.array([np.where(np.all(X == point, axis=1))[0][0] for point in selected_points])\n    # Select the corresponding y values\n    selected_y = y[indices]\n    return selected_points, selected_y\n</code></pre>"},{"location":"reference/spotpython/utils/classes/","title":"classes","text":""},{"location":"reference/spotpython/utils/compare/","title":"compare","text":""},{"location":"reference/spotpython/utils/compare/#spotpython.utils.compare.check_identical_columns_and_rows","title":"<code>check_identical_columns_and_rows(df, remove=False, verbosity=1)</code>","text":"<p>Checks for exact identical columns and rows in the DataFrame.</p> Note <p>This is an efficient method for checking exact duplicates in a DataFrame. If checks with tolerance are needed, use <code>check_identical_columns_and_rows_with_tol()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>remove</code> <code>bool</code> <p>Whether to remove duplicate columns/rows.</p> <code>False</code> <code>verbosity</code> <code>int</code> <p>Level of verbosity; 0 for no output, 1 for standard messages.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the DataFrame with duplicates removed if specified,    a list of tuples indicating which columns are duplicates,    and a list of tuples indicating which rows are duplicates.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotpython.utils.compare import check_identical_columns_and_rows\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [1, 2, 3], \"C\": [4, 5, 6]})\n&gt;&gt;&gt; check_identical_columns_and_rows(df, remove=False, verbosity=1)\n        Identical columns in DataFrame:\n        [('A', 'B')]\n</code></pre> Source code in <code>spotpython/utils/compare.py</code> <pre><code>def check_identical_columns_and_rows(df, remove=False, verbosity=1) -&gt; tuple:\n    \"\"\"\n    Checks for exact identical columns and rows in the DataFrame.\n\n    Note:\n        This is an efficient method for checking exact duplicates in a DataFrame.\n        If checks with tolerance are needed, use `check_identical_columns_and_rows_with_tol()`.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to check.\n        remove (bool): Whether to remove duplicate columns/rows.\n        verbosity (int): Level of verbosity; 0 for no output, 1 for standard messages.\n\n    Returns:\n        tuple: A tuple containing the DataFrame with duplicates removed if specified,\n               a list of tuples indicating which columns are duplicates,\n               and a list of tuples indicating which rows are duplicates.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotpython.utils.compare import check_identical_columns_and_rows\n        &gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [1, 2, 3], \"C\": [4, 5, 6]})\n        &gt;&gt;&gt; check_identical_columns_and_rows(df, remove=False, verbosity=1)\n                Identical columns in DataFrame:\n                [('A', 'B')]\n    \"\"\"\n    # Check for exact identical columns\n    identical_columns = []\n    for i in range(len(df.columns)):\n        for j in range(i + 1, len(df.columns)):\n            if df.iloc[:, i].equals(df.iloc[:, j]):  # Ensure entire columns are compared\n                identical_columns.append((df.columns[i], df.columns[j]))\n\n    if identical_columns and verbosity &gt; 0:\n        print(\"Identical columns in DataFrame:\")\n        for col_pair in identical_columns:\n            print(col_pair)\n\n    if remove and identical_columns:\n        df = df.drop(columns=[col_pair[1] for col_pair in identical_columns])\n\n    # Check for exact identical rows\n    identical_rows = []\n    for i in range(len(df.index)):\n        for j in range(i + 1, len(df.index)):\n            if df.iloc[i, :].equals(df.iloc[j, :]):  # Ensure entire rows are compared\n                identical_rows.append((df.index[i], df.index[j]))\n\n    if identical_rows and verbosity &gt; 0:\n        print(\"Identical rows in DataFrame:\")\n        for row_pair in identical_rows:\n            print(row_pair)\n\n    if remove and identical_rows:\n        df = df.drop(index=[row_pair[1] for row_pair in identical_rows])\n\n    return df, identical_columns, identical_rows\n</code></pre>"},{"location":"reference/spotpython/utils/compare/#spotpython.utils.compare.check_identical_columns_and_rows_with_tol","title":"<code>check_identical_columns_and_rows_with_tol(df, tolerance, remove=False, verbosity=1)</code>","text":"<p>Checks for identical columns and rows within a given tolerance.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>tolerance</code> <code>float</code> <p>The tolerance for checking equivalence.</p> required <code>remove</code> <code>bool</code> <p>Whether to remove duplicates found within the tolerance.</p> <code>False</code> <code>verbosity</code> <code>int</code> <p>Level of verbosity; 0 for no output, 1 for standard messages.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the DataFrame with duplicates removed if specified,    a list of tuples indicating which columns are duplicates within the tolerance,    and a list of tuples indicating which rows are duplicates within the tolerance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotpython.utils.compare import check_identical_columns_and_rows_with_tol\n&gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 1, 3], \"B\": [1, 1.01, 3], \"C\": [4, 5, 6]})\n&gt;&gt;&gt; check_identical_columns_and_rows_with_tol(df, tolerance=0.05, remove=False, verbosity=1)\n    Identical columns within tolerance in DataFrame:\n    [('A', 'B')]\n</code></pre> Source code in <code>spotpython/utils/compare.py</code> <pre><code>def check_identical_columns_and_rows_with_tol(df, tolerance, remove=False, verbosity=1) -&gt; tuple:\n    \"\"\"\n    Checks for identical columns and rows within a given tolerance.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to check.\n        tolerance (float): The tolerance for checking equivalence.\n        remove (bool): Whether to remove duplicates found within the tolerance.\n        verbosity (int): Level of verbosity; 0 for no output, 1 for standard messages.\n\n    Returns:\n        tuple: A tuple containing the DataFrame with duplicates removed if specified,\n               a list of tuples indicating which columns are duplicates within the tolerance,\n               and a list of tuples indicating which rows are duplicates within the tolerance.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotpython.utils.compare import check_identical_columns_and_rows_with_tol\n        &gt;&gt;&gt; df = pd.DataFrame({\"A\": [1, 1, 3], \"B\": [1, 1.01, 3], \"C\": [4, 5, 6]})\n        &gt;&gt;&gt; check_identical_columns_and_rows_with_tol(df, tolerance=0.05, remove=False, verbosity=1)\n            Identical columns within tolerance in DataFrame:\n            [('A', 'B')]\n    \"\"\"\n\n    # Function to compare rows/columns with tolerance\n    def is_identical_with_tolerance(series1, series2, tol):\n        return np.allclose(series1, series2, atol=tol)\n\n    # Check for identical columns within tolerance\n    identical_columns = []\n    for i in range(len(df.columns)):\n        for j in range(i + 1, len(df.columns)):\n            if is_identical_with_tolerance(df.iloc[:, i], df.iloc[:, j], tolerance):\n                identical_columns.append((df.columns[i], df.columns[j]))\n\n    if identical_columns and verbosity &gt; 0:\n        print(\"Identical columns within tolerance in DataFrame:\")\n        for col_pair in identical_columns:\n            print(col_pair)\n\n    if remove and identical_columns:\n        df = df.drop(columns=[col_pair[1] for col_pair in identical_columns])\n\n    # Check for identical rows within tolerance\n    identical_rows = []\n    for i in range(len(df.index)):\n        for j in range(i + 1, len(df.index)):\n            if is_identical_with_tolerance(df.iloc[i, :], df.iloc[j, :], tolerance):\n                identical_rows.append((df.index[i], df.index[j]))\n\n    if identical_rows and verbosity &gt; 0:\n        print(\"Identical rows within tolerance in DataFrame:\")\n        for row_pair in identical_rows:\n            print(row_pair)\n\n    if remove and identical_rows:\n        df = df.drop(index=[row_pair[1] for row_pair in identical_rows])\n\n    return df, identical_columns, identical_rows\n</code></pre>"},{"location":"reference/spotpython/utils/compare/#spotpython.utils.compare.find_equal_in_lists","title":"<code>find_equal_in_lists(a, b)</code>","text":"<p>Find equal values in two lists.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>list</code> <p>list with a values</p> required <code>b</code> <code>list</code> <p>list with b values</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[int]</code> <p>list with 1 if equal, otherwise 0</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.compare import find_equal_in_lists\n    a = [1, 2, 3, 4, 5]\n    b = [1, 2, 3, 4, 5]\n    find_equal_in_lists(a, b)\n    [1, 1, 1, 1, 1]\n</code></pre> Source code in <code>spotpython/utils/compare.py</code> <pre><code>def find_equal_in_lists(a: List[int], b: List[int]) -&gt; List[int]:\n    \"\"\"Find equal values in two lists.\n\n    Args:\n        a (list): list with a values\n        b (list): list with b values\n\n    Returns:\n        list: list with 1 if equal, otherwise 0\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.compare import find_equal_in_lists\n            a = [1, 2, 3, 4, 5]\n            b = [1, 2, 3, 4, 5]\n            find_equal_in_lists(a, b)\n            [1, 1, 1, 1, 1]\n    \"\"\"\n    equal = [1 if a[i] == b[i] else 0 for i in range(len(a))]\n    return equal\n</code></pre>"},{"location":"reference/spotpython/utils/compare/#spotpython.utils.compare.selectNew","title":"<code>selectNew(A, X, tolerance=0)</code>","text":"<p>Select rows from A that are not in X.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>A array with new values</p> required <code>X</code> <code>ndarray</code> <p>X array with known values</p> required <code>tolerance</code> <code>float</code> <p>tolerance value for comparison</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array with unknown (new) values</p> <code>ndarray</code> <p>array with <code>True</code> if value is new, otherwise <code>False</code>.</p> <p>Examples:</p> <p>from spotpython.utils.compare import selectNew     import numpy as np     A = np.array([[1,2,3],[4,5,6]])     X = np.array([[1,2,3],[4,5,6]])     B, ind  = selectNew(A, X)     assert B.shape[0] == 0     assert np.equal(ind, np.array([False, False])).all() from spotpython.utils.compare import selectNew     A = np.array([[1,2,3],[4,5,7]])     X = np.array([[1,2,3],[4,5,6]])     B, ind  = selectNew(A, X)     assert B.shape[0] == 1     assert np.equal(ind, np.array([False, True])).all()</p> Source code in <code>spotpython/utils/compare.py</code> <pre><code>def selectNew(A: np.ndarray, X: np.ndarray, tolerance: float = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Select rows from A that are not in X.\n\n    Args:\n        A (numpy.ndarray): A array with new values\n        X (numpy.ndarray): X array with known values\n        tolerance (float): tolerance value for comparison\n\n    Returns:\n        (numpy.ndarray): array with unknown (new) values\n        (numpy.ndarray): array with `True` if value is new, otherwise `False`.\n\n    Examples:\n    &gt;&gt;&gt; from spotpython.utils.compare import selectNew\n        import numpy as np\n        A = np.array([[1,2,3],[4,5,6]])\n        X = np.array([[1,2,3],[4,5,6]])\n        B, ind  = selectNew(A, X)\n        assert B.shape[0] == 0\n        assert np.equal(ind, np.array([False, False])).all()\n    &gt;&gt;&gt; from spotpython.utils.compare import selectNew\n        A = np.array([[1,2,3],[4,5,7]])\n        X = np.array([[1,2,3],[4,5,6]])\n        B, ind  = selectNew(A, X)\n        assert B.shape[0] == 1\n        assert np.equal(ind, np.array([False, True])).all()\n    \"\"\"\n    B = np.abs(A[:, None] - X)\n    ind = np.any(np.all(B &lt;= tolerance, axis=2), axis=1)\n    return A[~ind], ~ind\n</code></pre>"},{"location":"reference/spotpython/utils/convert/","title":"convert","text":""},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.add_logical_columns","title":"<code>add_logical_columns(X, arity=2, operations=['and', 'or', 'xor'])</code>","text":"<p>Combines all features in a dataframe with each other using bitwise operations</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>dataframe with features</p> required <code>arity</code> <code>int</code> <p>the number of columns to combine at once</p> <code>2</code> <code>operations</code> <code>list of str</code> <p>the operations to apply. Possible values are \u2018and\u2019, \u2018or\u2019 and \u2018xor\u2019</p> <code>['and', 'or', 'xor']</code> <p>Returns:</p> Name Type Description <code>X</code> <code>DataFrame</code> <p>dataframe with new features</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n&gt;&gt;&gt; add_logical_columns(X)\n    a      b      c  a_and_b  a_and_c  b_and_c  a_or_b  a_or_c  b_or_c  a_xor_b  a_xor_c  b_xor_c\n0  True   True  False     True    False    False    True    True    True    False     True     True\n1 False   True  False    False    False    False    True   False    True     True     True    False\n2  True  False   True    False     True    False    True    True    True     True    False     True\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def add_logical_columns(X, arity=2, operations=[\"and\", \"or\", \"xor\"]):\n    \"\"\"Combines all features in a dataframe with each other using bitwise operations\n\n    Args:\n        X (pd.DataFrame): dataframe with features\n        arity (int): the number of columns to combine at once\n        operations (list of str): the operations to apply. Possible values are 'and', 'or' and 'xor'\n\n    Returns:\n        X (pd.DataFrame): dataframe with new features\n\n    Examples:\n        &gt;&gt;&gt; X = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n        &gt;&gt;&gt; add_logical_columns(X)\n            a      b      c  a_and_b  a_and_c  b_and_c  a_or_b  a_or_c  b_or_c  a_xor_b  a_xor_c  b_xor_c\n        0  True   True  False     True    False    False    True    True    True    False     True     True\n        1 False   True  False    False    False    False    True   False    True     True     True    False\n        2  True  False   True    False     True    False    True    True    True     True    False     True\n\n    \"\"\"\n    new_cols = []\n    # Iterate over all combinations of columns of the given arity\n    for cols in combinations(X.columns, arity):\n        # Create new columns for the specified operations\n        if \"and\" in operations:\n            and_col = X[list(cols)].apply(lambda x: x.all(), axis=1)\n            new_cols.append(and_col)\n        if \"or\" in operations:\n            or_col = X[list(cols)].apply(lambda x: x.any(), axis=1)\n            new_cols.append(or_col)\n        if \"xor\" in operations:\n            xor_col = X[list(cols)].apply(lambda x: x.sum() % 2 == 1, axis=1)\n            new_cols.append(xor_col)\n    # Join all the new columns at once\n    X = pd.concat([X] + new_cols, axis=1)\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.check_type","title":"<code>check_type(value)</code>","text":"<p>Check the type of the input value and return the type as a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>object</code> <p>The input value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The type of the input value as a string. Possible values are \u201cint\u201d, \u201cfloat\u201d, \u201cstr\u201d, \u201cbool\u201d, or None. Checks for numpy types as well, i.e., np.integer, np.floating, np.str_, np.bool_.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.convert import check_type\n&gt;&gt;&gt; check_type(5)\n\"int\"\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def check_type(value) -&gt; str:\n    \"\"\"Check the type of the input value and return the type as a string.\n\n    Args:\n        value (object): The input value.\n\n    Returns:\n        str:\n            The type of the input value as a string.\n            Possible values are \"int\", \"float\", \"str\", \"bool\", or None.\n            Checks for numpy types as well, i.e., np.integer, np.floating, np.str_, np.bool_.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.convert import check_type\n        &gt;&gt;&gt; check_type(5)\n        \"int\"\n\n    \"\"\"\n    if isinstance(value, (int, np.integer)):\n        return \"int\"\n    elif isinstance(value, (float, np.floating)):\n        return \"float\"\n    elif isinstance(value, (str, np.str_)):\n        return \"str\"\n    elif isinstance(value, (bool, np.bool_)):\n        return \"bool\"\n    else:\n        return None\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.class_for_name","title":"<code>class_for_name(module_name, class_name)</code>","text":"<p>Returns a class for a given module and class name.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>The name of the module.</p> required <code>class_name</code> <code>str</code> <p>The name of the class.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>The class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.convert import class_for_name\n    from scipy.optimize import rosen\n    bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n    shgo_class = class_for_name(\"scipy.optimize\", \"shgo\")\n    result = shgo_class(rosen, bounds)\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def class_for_name(module_name, class_name) -&gt; object:\n    \"\"\"Returns a class for a given module and class name.\n\n    Args:\n        module_name (str): The name of the module.\n        class_name (str): The name of the class.\n\n    Returns:\n        object: The class.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.convert import class_for_name\n            from scipy.optimize import rosen\n            bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n            shgo_class = class_for_name(\"scipy.optimize\", \"shgo\")\n            result = shgo_class(rosen, bounds)\n    \"\"\"\n    m = importlib.import_module(module_name)\n    c = getattr(m, class_name)\n    return c\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.get_Xy_from_df","title":"<code>get_Xy_from_df(df, target_column)</code>","text":"<p>Get X and y from a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataframe.</p> required <code>target_column</code> <code>str</code> <p>The name of the target column.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>The tuple (X, y).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.convert import get_Xy_from_df\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n&gt;&gt;&gt; X, y = get_Xy_from_df(df, \"c\")\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def get_Xy_from_df(df, target_column) -&gt; tuple:\n    \"\"\"Get X and y from a dataframe.\n\n    Args:\n        df (pandas.DataFrame): The input dataframe.\n        target_column (str): The name of the target column.\n\n    Returns:\n        tuple: The tuple (X, y).\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.convert import get_Xy_from_df\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n        &gt;&gt;&gt; X, y = get_Xy_from_df(df, \"c\")\n    \"\"\"\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    # convert to numpy arrays\n    X = X.to_numpy()\n    y = y.to_numpy()\n    return X, y\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.get_shape","title":"<code>get_shape(x)</code>","text":"<p>Get the shape of a numpy array <code>x</code>.</p> <p>This function returns the number of rows and columns of the input array <code>x</code>. If <code>x</code> is a 1D array (shape <code>(n,)</code>), it returns <code>(n, None)</code>. If <code>x</code> is a 2D array (shape <code>(n, k)</code>), it returns <code>(n, k)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The input numpy array.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple <code>(n, k)</code> where: - <code>n</code> is the number of rows in the array. - <code>k</code> is the number of columns in the array, or <code>None</code> if <code>x</code> is 1D.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.convert import get_shape\n&gt;&gt;&gt; x1 = np.array([1, 2, 3])\n&gt;&gt;&gt; get_shape(x1)\n(3, None)\n</code></pre> <pre><code>&gt;&gt;&gt; x2 = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; get_shape(x2)\n(3, 2)\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def get_shape(x: np.ndarray) -&gt; tuple:\n    \"\"\"\n    Get the shape of a numpy array `x`.\n\n    This function returns the number of rows and columns of the input array `x`.\n    If `x` is a 1D array (shape `(n,)`), it returns `(n, None)`.\n    If `x` is a 2D array (shape `(n, k)`), it returns `(n, k)`.\n\n    Args:\n        x (numpy.ndarray): The input numpy array.\n\n    Returns:\n        tuple: A tuple `(n, k)` where:\n            - `n` is the number of rows in the array.\n            - `k` is the number of columns in the array, or `None` if `x` is 1D.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.convert import get_shape\n        &gt;&gt;&gt; x1 = np.array([1, 2, 3])\n        &gt;&gt;&gt; get_shape(x1)\n        (3, None)\n\n        &gt;&gt;&gt; x2 = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; get_shape(x2)\n        (3, 2)\n    \"\"\"\n    if x.ndim == 1:\n        return x.shape[0], None\n    elif x.ndim == 2:\n        return x.shape[0], x.shape[1]\n    else:\n        raise ValueError(\"Input array must be 1D or 2D.\")\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.map_to_True_False","title":"<code>map_to_True_False(value)</code>","text":"<p>Map the string value to a boolean value. If the value is \u201cTrue\u201d or \u201ctrue\u201d, return True. Otherwise, return False.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string to be mapped to a boolean value.</p> required <p>Returns:     bool:         True if the value is \u201cTrue\u201d or \u201ctrue\u201d, False otherwise.</p> <p>Examples:</p> <p>from spotpython.utils.convert import map_to_True_False     map_to_True_False(\u201cTrue\u201d)     True</p> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def map_to_True_False(value):\n    \"\"\"\n    Map the string value to a boolean value.\n    If the value is \"True\" or \"true\", return True.\n    Otherwise, return False.\n\n    Args:\n        value (str):\n            The string to be mapped to a boolean value.\n    Returns:\n        bool:\n            True if the value is \"True\" or \"true\", False otherwise.\n\n    Examples:\n    &gt;&gt;&gt; from spotpython.utils.convert import map_to_True_False\n        map_to_True_False(\"True\")\n        True\n    \"\"\"\n    if value.lower() == \"true\":\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.series_to_array","title":"<code>series_to_array(series)</code>","text":"<p>Converts a pandas series to a numpy array. Args:     series (pandas.Series): The input series.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The output array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.convert import series_to_array\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; series = pd.Series([1, 2, 3])\n&gt;&gt;&gt; series_to_array(series)\narray([1, 2, 3])\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def series_to_array(series):\n    \"\"\"Converts a pandas series to a numpy array.\n    Args:\n        series (pandas.Series): The input series.\n\n    Returns:\n        (numpy.ndarray): The output array.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.convert import series_to_array\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; series = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; series_to_array(series)\n        array([1, 2, 3])\n    \"\"\"\n    if isinstance(series, np.ndarray):\n        return series\n    else:\n        return series.to_numpy()\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.set_dataset_target_type","title":"<code>set_dataset_target_type(dataset, target='y')</code>","text":"<p>Set the target column to 0 and 1 for boolean and string values.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The input dataset.</p> required <code>target</code> <code>str</code> <p>The name of the target column. Default is \u201cy\u201d.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The dataset with boolean and string target column values set to 0 and 1.</p> <p>Examples:</p> <p>from spotpython.utils.convert import set_dataset_target_type     import pandas as pd     dataset = pd.DataFrame({\u201ca\u201d: [1, 2, 3], \u201cb\u201d: [4, 5, 6], \u201cc\u201d: [7, 8, 9], \u201cy\u201d: [True, False, True]})     print(dataset)     dataset = set_dataset_target_type(dataset)     print(dataset)         a  b  c      y         0  1  4  7   True         1  2  5  8  False         2  3  6  9   True         a  b  c  y         0  1  4  7  1         1  2  5  8  0         2  3  6  9  1</p> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def set_dataset_target_type(dataset, target=\"y\") -&gt; pd.DataFrame:\n    \"\"\"Set the target column to 0 and 1 for boolean and string values.\n\n    Args:\n        dataset (pd.DataFrame): The input dataset.\n        target (str): The name of the target column. Default is \"y\".\n\n    Returns:\n        pd.DataFrame:\n            The dataset with boolean and string target column values set to 0 and 1.\n\n    Examples:\n    &gt;&gt;&gt; from spotpython.utils.convert import set_dataset_target_type\n        import pandas as pd\n        dataset = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9], \"y\": [True, False, True]})\n        print(dataset)\n        dataset = set_dataset_target_type(dataset)\n        print(dataset)\n            a  b  c      y\n            0  1  4  7   True\n            1  2  5  8  False\n            2  3  6  9   True\n            a  b  c  y\n            0  1  4  7  1\n            1  2  5  8  0\n            2  3  6  9  1\n\n\n    \"\"\"\n    val = copy.deepcopy(dataset.iloc[0, -1])\n    target_type = check_type(val)\n    if target_type == \"bool\" or target_type == \"str\":\n        # convert the target column to 0 and 1\n        dataset[target] = dataset[target].astype(int)\n    return dataset\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.set_shape","title":"<code>set_shape(x, target_shape)</code>","text":"<p>Adjust the shape of a numpy array <code>x</code> to match the target shape <code>(n, k)</code>.</p> <p>If the target shape is <code>(n, None)</code>, the array is reshaped to 1D with <code>n</code> elements. If the target shape is <code>(n, k)</code>, the array is reshaped to 2D with <code>n</code> rows and <code>k</code> columns.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The input numpy array.</p> required <code>target_shape</code> <code>tuple</code> <p>The target shape <code>(n, k)</code> where: - <code>n</code> is the number of rows. - <code>k</code> is the number of columns, or <code>None</code> for a 1D array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: The reshaped numpy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the target shape is incompatible with the size of the array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.convert import set_shape\n&gt;&gt;&gt; x = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; set_shape(x, (4, None))\n    array([1, 2, 3, 4])\n&gt;&gt;&gt; x = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; set_shape(x, (2, 2))\narray([[1, 2],\n    [3, 4]])\n&gt;&gt;&gt; x = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; set_shape(x, (4, None))\narray([1, 2, 3, 4])\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def set_shape(x: np.ndarray, target_shape: tuple) -&gt; np.ndarray:\n    \"\"\"\n    Adjust the shape of a numpy array `x` to match the target shape `(n, k)`.\n\n    If the target shape is `(n, None)`, the array is reshaped to 1D with `n` elements.\n    If the target shape is `(n, k)`, the array is reshaped to 2D with `n` rows and `k` columns.\n\n    Args:\n        x (numpy.ndarray): The input numpy array.\n        target_shape (tuple): The target shape `(n, k)` where:\n            - `n` is the number of rows.\n            - `k` is the number of columns, or `None` for a 1D array.\n\n    Returns:\n        numpy.ndarray: The reshaped numpy array.\n\n    Raises:\n        ValueError: If the target shape is incompatible with the size of the array.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.convert import set_shape\n        &gt;&gt;&gt; x = np.array([1, 2, 3, 4])\n        &gt;&gt;&gt; set_shape(x, (4, None))\n            array([1, 2, 3, 4])\n        &gt;&gt;&gt; x = np.array([1, 2, 3, 4])\n        &gt;&gt;&gt; set_shape(x, (2, 2))\n        array([[1, 2],\n            [3, 4]])\n        &gt;&gt;&gt; x = np.array([[1, 2], [3, 4]])\n        &gt;&gt;&gt; set_shape(x, (4, None))\n        array([1, 2, 3, 4])\n    \"\"\"\n    n, k = target_shape\n\n    if k is None:\n        # Reshape to 1D array with `n` elements\n        if x.size != n:\n            raise ValueError(f\"Cannot reshape array of size {x.size} to shape ({n},)\")\n        return x.reshape(n)\n    else:\n        # Reshape to 2D array with `n` rows and `k` columns\n        if x.size != n * k:\n            raise ValueError(f\"Cannot reshape array of size {x.size} to shape ({n}, {k})\")\n        return x.reshape(n, k)\n</code></pre>"},{"location":"reference/spotpython/utils/convert/#spotpython.utils.convert.sort_by_kth_and_return_indices","title":"<code>sort_by_kth_and_return_indices(array, k)</code>","text":"<p>Sorts an array of arrays based on the k-th values in descending order and returns the indices of the original array entries.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>list of lists</code> <p>The array to be sorted. Each sub-array should have at least <code>k+1</code> elements.</p> required <code>k</code> <code>int</code> <p>The index (zero-based) of the element within each sub-array to sort by.</p> required <p>Returns:</p> Type Description <code>list</code> <p>list of int: Indices of the original array entries after sorting by the k-th value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input array is empty, None, or any sub-array does not have at least <code>k+1</code> elements, or if k is out of bounds for any sub-array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.convert import sort_by_kth_and_return_indices\n    try:\n        array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n        k = 1  # Sort by the second element in each sub-array\n        indices = sort_by_kth_and_return_indices(array, k)\n        print(\"Indices of the sorted elements using the k-th value:\", indices)\n    except ValueError as error:\n        print(f\"Sorting failed due to: {error}\")\n</code></pre> Source code in <code>spotpython/utils/convert.py</code> <pre><code>def sort_by_kth_and_return_indices(array, k) -&gt; list:\n    \"\"\"Sorts an array of arrays based on the k-th values in descending order and returns\n    the indices of the original array entries.\n\n    Args:\n        array (list of lists): The array to be sorted. Each sub-array should have at least\n            `k+1` elements.\n        k (int): The index (zero-based) of the element within each sub-array to sort by.\n\n    Returns:\n        list of int: Indices of the original array entries after sorting by the k-th value.\n\n    Raises:\n        ValueError: If the input array is empty, None, or any sub-array does not have at least\n            `k+1` elements, or if k is out of bounds for any sub-array.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.convert import sort_by_kth_and_return_indices\n            try:\n                array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n                k = 1  # Sort by the second element in each sub-array\n                indices = sort_by_kth_and_return_indices(array, k)\n                print(\"Indices of the sorted elements using the k-th value:\", indices)\n            except ValueError as error:\n                print(f\"Sorting failed due to: {error}\")\n    \"\"\"\n    if not array:\n        return []\n\n    # Check for improperly structured sub-arrays and that k is within bounds\n    for item in array:\n        if not isinstance(item, list) or len(item) &lt;= k:\n            raise ValueError(\"All sub-arrays must be lists with at least k+1 elements.\")\n\n    # Enumerate the array to keep track of original indices, then sort by the k-th item\n    sorted_indices = [index for index, value in sorted(enumerate(array), key=lambda x: x[1][k], reverse=True)]\n\n    return sorted_indices\n</code></pre>"},{"location":"reference/spotpython/utils/device/","title":"device","text":""},{"location":"reference/spotpython/utils/device/#spotpython.utils.device.getDevice","title":"<code>getDevice(device=None)</code>","text":"<p>Get CPU, GPU (CUDA), or MPS device for training.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>Device for training. If None or \u201cauto\u201d, the device is selected automatically based on availability.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>device</code> <code>str</code> <p>Device for training.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested device is not recognized or available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.device import getDevice\n    getDevice()\n        'cuda:0'\n</code></pre> Source code in <code>spotpython/utils/device.py</code> <pre><code>def getDevice(device=None):\n    \"\"\"Get CPU, GPU (CUDA), or MPS device for training.\n\n    Args:\n        device (str):\n            Device for training. If None or \"auto\", the device is selected automatically based on availability.\n\n    Returns:\n        device (str):\n            Device for training.\n\n    Raises:\n        ValueError: If the requested device is not recognized or available.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.device import getDevice\n            getDevice()\n                'cuda:0'\n    \"\"\"\n    if device is None or device == \"auto\":\n        # Automatically select device\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        elif torch.backends.mps.is_available():\n            device = \"mps\"\n        return device\n\n    # Check the explicit device request\n    if device.startswith(\"cuda\"):\n        if not torch.cuda.is_available():\n            raise ValueError(\"CUDA device requested but no CUDA device is available.\")\n    elif device == \"mps\":\n        if not torch.backends.mps.is_available():\n            raise ValueError(\"MPS device requested but MPS is not available.\")\n    elif device == \"cpu\":\n        return \"cpu\"\n    else:\n        raise ValueError(f\"Unrecognized device: {device}. Valid options are 'cpu', 'cuda:x', or 'mps'.\")\n\n    return device\n</code></pre>"},{"location":"reference/spotpython/utils/eda/","title":"eda","text":""},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.compare_two_tree_models","title":"<code>compare_two_tree_models(model1, model2, headers=['Parameter', 'Default', 'Spot'])</code>","text":"<p>Compares two tree models. Args:     model1 (object):         A tree model.     model2 (object):         A tree model.     headers (list):         A list with the headers of the table.</p> <p>Returns:</p> Type Description <code>str</code> <p>A table with the comparison of the two models.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.eda import compare_two_tree_models\n&gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n&gt;&gt;&gt; fun_control = {\n...     \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n...     \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n... }\n&gt;&gt;&gt; default_values = get_default_values(fun_control)\n&gt;&gt;&gt; model1 = spot_tuner.get_model(\"rf\", default_values)\n&gt;&gt;&gt; model2 = spot_tuner.get_model(\"rf\", default_values)\n&gt;&gt;&gt; compare_two_tree_models(model1, model2)\n</code></pre> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def compare_two_tree_models(model1, model2, headers=[\"Parameter\", \"Default\", \"Spot\"]):\n    \"\"\"Compares two tree models.\n    Args:\n        model1 (object):\n            A tree model.\n        model2 (object):\n            A tree model.\n        headers (list):\n            A list with the headers of the table.\n\n    Returns:\n        (str):\n            A table with the comparison of the two models.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.eda import compare_two_tree_models\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_default_values\n        &gt;&gt;&gt; fun_control = {\n        ...     \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ... }\n        &gt;&gt;&gt; default_values = get_default_values(fun_control)\n        &gt;&gt;&gt; model1 = spot_tuner.get_model(\"rf\", default_values)\n        &gt;&gt;&gt; model2 = spot_tuner.get_model(\"rf\", default_values)\n        &gt;&gt;&gt; compare_two_tree_models(model1, model2)\n    \"\"\"\n    keys = model1.summary.keys()\n    values1 = model1.summary.values()\n    values2 = model2.summary.values()\n    tbl = []\n    for key, value1, value2 in zip(keys, values1, values2):\n        tbl.append([key, value1, value2])\n    return tabulate(tbl, headers=headers, numalign=\"right\", tablefmt=\"github\")\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.count_missing_data","title":"<code>count_missing_data(df)</code>","text":"<p>Counts the number of missing values in each column of the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data to be counted.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the number of missing values in each column.</p> Example <p>import pandas as pd df = pd.DataFrame({\u2018A\u2019: [1, 2, None], \u2018B\u2019: [4, None, 6], \u2018C\u2019: [7, 8, 9]}) count_missing_data(df)    column_name  missing_count 0           A              1 1           B              1</p> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def count_missing_data(df) -&gt; pd.DataFrame:\n    \"\"\"\n    Counts the number of missing values in each column of the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data to be counted.\n\n    Returns:\n        (pd.DataFrame): DataFrame containing the number of missing values in each column.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, None], 'B': [4, None, 6], 'C': [7, 8, 9]})\n        &gt;&gt;&gt; count_missing_data(df)\n           column_name  missing_count\n        0           A              1\n        1           B              1\n    \"\"\"\n    missing_df = df.isnull().sum(axis=0).reset_index()\n    missing_df.columns = [\"column_name\", \"missing_count\"]\n    missing_df = missing_df.loc[missing_df[\"missing_count\"] &gt; 0]\n    missing_df = missing_df.sort_values(by=\"missing_count\")\n\n    return missing_df\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.filter_highly_correlated","title":"<code>filter_highly_correlated(df, sorted=True, threshold=1 - 1e-05)</code>","text":"<p>Return a new DataFrame with only those columns that are highly correlated.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>threshold</code> <code>float</code> <p>The correlation threshold.</p> <code>1 - 1e-05</code> <code>sorted</code> <code>bool</code> <p>If True, the columns are sorted by name.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with only highly correlated columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n    df = filter_highly_correlated(df, sorted=True, threshold=0.99)\n</code></pre> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def filter_highly_correlated(df: pd.DataFrame, sorted: bool = True, threshold: float = 1 - 1e-5) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new DataFrame with only those columns that are highly correlated.\n\n    Args:\n        df (DataFrame): The input DataFrame.\n        threshold (float): The correlation threshold.\n        sorted (bool): If True, the columns are sorted by name.\n\n    Returns:\n        DataFrame: A new DataFrame with only highly correlated columns.\n\n    Examples:\n        &gt;&gt;&gt; df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n            df = filter_highly_correlated(df, sorted=True, threshold=0.99)\n\n    \"\"\"\n    corr_matrix = df.corr()\n    # Find pairs of columns with correlation greater than threshold\n    corr_pairs = corr_matrix.abs().unstack()\n    corr_pairs = corr_pairs[corr_pairs &lt; 1]  # Remove self-correlations\n    high_corr = corr_pairs[corr_pairs &gt; threshold]\n    high_corr = high_corr[high_corr &lt; 1]  # Remove self-correlations\n\n    # Get the column names of highly correlated columns\n    high_corr_cols = list(set([col[0] for col in high_corr.index]))\n\n    # Create new DataFrame with only highly correlated columns\n    new_df = df[high_corr_cols]\n    # sort the columns by name\n    if sorted:\n        new_df = new_df.sort_index(axis=1)\n\n    return new_df\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.gen_design_table","title":"<code>gen_design_table(fun_control, spot=None, tablefmt='github')</code>","text":"<p>Generates a table with the design variables and their bounds. Args:     fun_control (dict):         A dictionary with function design variables.     spot (object):         A spot object. Defaults to None. Returns:     (str):         a table with the design variables, their default values, and their bounds.         If a spot object is provided,         the table will also include the value and the importance of each hyperparameter.         Use the <code>print</code> function to display the table.</p> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def gen_design_table(fun_control: dict, spot: object = None, tablefmt=\"github\") -&gt; str:\n    \"\"\"Generates a table with the design variables and their bounds.\n    Args:\n        fun_control (dict):\n            A dictionary with function design variables.\n        spot (object):\n            A spot object. Defaults to None.\n    Returns:\n        (str):\n            a table with the design variables, their default values, and their bounds.\n            If a spot object is provided,\n            the table will also include the value and the importance of each hyperparameter.\n            Use the `print` function to display the table.\n\n    \"\"\"\n    default_values = get_default_values(fun_control)\n    defaults = list(default_values.values())\n    if spot is None:\n        tab = tabulate(\n            {\n                \"name\": get_var_name(fun_control),\n                \"type\": get_var_type(fun_control),\n                \"default\": defaults,\n                \"lower\": get_bound_values(fun_control, \"lower\", as_list=True),\n                \"upper\": get_bound_values(fun_control, \"upper\", as_list=True),\n                \"transform\": get_transform(fun_control),\n            },\n            headers=\"keys\",\n            tablefmt=tablefmt,\n        )\n    else:\n        res = spot.print_results(print_screen=False, dict=fun_control)\n        tuned = [item[1] for item in res]\n        # imp = spot.print_importance(threshold=0.0, print_screen=False)\n        # importance = [item[1] for item in imp]\n        importance = spot.get_importance()\n        stars = get_stars(importance)\n        tab = tabulate(\n            {\n                \"name\": get_var_name(fun_control),\n                \"type\": get_var_type(fun_control),\n                \"default\": defaults,\n                \"lower\": get_bound_values(fun_control, \"lower\", as_list=True),\n                \"upper\": get_bound_values(fun_control, \"upper\", as_list=True),\n                \"tuned\": tuned,\n                \"transform\": get_transform(fun_control),\n                \"importance\": importance,\n                \"stars\": stars,\n            },\n            headers=\"keys\",\n            numalign=\"right\",\n            floatfmt=(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \".2f\"),\n            tablefmt=tablefmt,\n        )\n    return tab\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.generate_config_id","title":"<code>generate_config_id(config, hash=False, timestamp=False)</code>","text":"<p>Generates a unique id for a configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary with the configuration.</p> required <code>hash</code> <code>bool</code> <p>If True, the id is hashed.</p> <code>False</code> <code>timestamp</code> <code>bool</code> <p>If True, the id is appended with a timestamp. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>A unique id for the configuration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.hyperparameters.values import get_one_config_from_X\n&gt;&gt;&gt; X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n&gt;&gt;&gt; config = get_one_config_from_X(X, fun_control)\n&gt;&gt;&gt; generate_config_id(config)\n</code></pre> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def generate_config_id(config, hash=False, timestamp=False):\n    \"\"\"Generates a unique id for a configuration.\n\n    Args:\n        config (dict):\n            A dictionary with the configuration.\n        hash (bool):\n            If True, the id is hashed.\n        timestamp (bool):\n            If True, the id is appended with a timestamp. Defaults to False.\n\n    Returns:\n        (str):\n            A unique id for the configuration.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.hyperparameters.values import get_one_config_from_X\n        &gt;&gt;&gt; X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n        &gt;&gt;&gt; config = get_one_config_from_X(X, fun_control)\n        &gt;&gt;&gt; generate_config_id(config)\n    \"\"\"\n    config_id = \"\"\n    for key in config:\n        # if config[key] is a number, round it to 4 digits after the decimal point\n        if isinstance(config[key], float):\n            config_id += str(round(config[key], 4)) + \"_\"\n        else:\n            config_id += str(config[key]) + \"_\"\n    # hash the config_id to make it shorter and unique\n    if hash:\n        config_id = str(hash(config_id)) + \"_\"\n    # remove () and , from the string\n    config_id = config_id.replace(\"(\", \"\")\n    config_id = config_id.replace(\")\", \"\")\n    config_id = config_id.replace(\",\", \"\")\n    config_id = config_id.replace(\" \", \"\")\n    config_id = config_id.replace(\":\", \"\")\n    if timestamp:\n        config_id = get_timestamp(only_int=True) + \"_\" + config_id\n    return config_id[:-1]\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.get_stars","title":"<code>get_stars(input_list)</code>","text":"<p>Converts a list of values to a list of stars, which can be used to     visualize the importance of a variable.</p> <p>Parameters:</p> Name Type Description Default <code>input_list</code> <code>list</code> <p>A list of values.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of strings.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.eda import convert_list\n&gt;&gt;&gt; get_stars([100, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n[***, '', '', '', '', '', '', '', '']\n</code></pre> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def get_stars(input_list) -&gt; list:\n    \"\"\"Converts a list of values to a list of stars, which can be used to\n        visualize the importance of a variable.\n\n    Args:\n        input_list (list): A list of values.\n\n    Returns:\n        (list):\n            A list of strings.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.eda import convert_list\n        &gt;&gt;&gt; get_stars([100, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n        [***, '', '', '', '', '', '', '', '']\n    \"\"\"\n    output_list = []\n    for value in input_list:\n        if value &gt; 95:\n            output_list.append(\"***\")\n        elif value &gt; 50:\n            output_list.append(\"**\")\n        elif value &gt; 1:\n            output_list.append(\"*\")\n        elif value &gt; 0.1:\n            output_list.append(\".\")\n        else:\n            output_list.append(\"\")\n    return output_list\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.plot_missing_data","title":"<code>plot_missing_data(df, relative=False, figsize=(7, 5), color='grey', xlabel='Missing Data', title='Missing Data')</code>","text":"<p>Plots a horizontal bar chart of the number of missing values in each column of the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data to be plotted.</p> required <code>relative</code> <code>bool</code> <p>Whether to plot relative values (percentage) or absolute values.</p> <code>False</code> <code>figsize</code> <code>tuple</code> <p>Size of the figure to be plotted.</p> <code>(7, 5)</code> <code>color</code> <code>str</code> <p>Color of the bars in the bar chart.</p> <code>'grey'</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis.</p> <code>'Missing Data'</code> <code>title</code> <code>str</code> <p>Title for the plot.</p> <code>'Missing Data'</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Example <p>import pandas as pd df = pd.DataFrame({\u2018A\u2019: [1, 2, np.nan], \u2018B\u2019: [4, np.nan, 6], \u2018C\u2019: [7, 8, 9]}) plot_missing_data(df)</p> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def plot_missing_data(df, relative=False, figsize=(7, 5), color=\"grey\", xlabel=\"Missing Data\", title=\"Missing Data\") -&gt; None:\n    \"\"\"\n    Plots a horizontal bar chart of the number of missing values in each column of the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data to be plotted.\n        relative (bool): Whether to plot relative values (percentage) or absolute values.\n        figsize (tuple): Size of the figure to be plotted.\n        color (str): Color of the bars in the bar chart.\n        xlabel (str): Label for the x-axis.\n        title (str): Title for the plot.\n\n    Returns:\n        (NoneType): None\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, np.nan], 'B': [4, np.nan, 6], 'C': [7, 8, 9]})\n        &gt;&gt;&gt; plot_missing_data(df)\n    \"\"\"\n    missing_df = count_missing_data(df)\n\n    if relative:\n        missing_df[\"missing_count\"] = missing_df[\"missing_count\"] / df.shape[0]\n        xlabel = \"Percentage of \" + xlabel\n        title = \"Percentage of \" + title\n\n    ind = np.arange(missing_df.shape[0])\n    _, ax = plt.subplots(figsize=figsize)\n    _ = ax.barh(ind, missing_df.missing_count.values, color=color)\n    ax.set_yticks(ind)\n    ax.set_yticklabels(missing_df.column_name.values, rotation=\"horizontal\")\n    ax.set_xlabel(xlabel)\n    ax.set_title(title)\n    plt.vlines(1, 0, missing_df.shape[0])\n    plt.vlines(0.97, 0, missing_df.shape[0])\n    plt.vlines(0.5, 0, missing_df.shape[0])\n    plt.show()\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.plot_sns_heatmap","title":"<code>plot_sns_heatmap(df_heat, figsize=(16, 12), cmap='vlag', vmin=-1, vmax=1, annot=True, fmt='.5f', linewidths=0.5, annot_kws={'size': 8})</code>","text":"<p>Plots a heatmap of the correlation matrix of the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df_heat</code> <code>DataFrame</code> <p>DataFrame containing the data to be plotted.</p> required <code>figsize</code> <code>tuple</code> <p>Size of the figure to be plotted.</p> <code>(16, 12)</code> <code>cmap</code> <code>str</code> <p>Color map to be used for the heatmap.</p> <code>'vlag'</code> <code>vmin</code> <code>int</code> <p>Minimum value for the color scale.</p> <code>-1</code> <code>vmax</code> <code>int</code> <p>Maximum value for the color scale.</p> <code>1</code> <code>annot</code> <code>bool</code> <p>Whether to display annotations on the heatmap.</p> <code>True</code> <code>fmt</code> <code>str</code> <p>Format string for annotations.</p> <code>'.5f'</code> <code>linewidths</code> <code>float</code> <p>Width of lines separating cells in the heatmap.</p> <code>0.5</code> <code>annot_kws</code> <code>dict</code> <p>Keyword arguments for annotations.</p> <code>{'size': 8}</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Example <p>import pandas as pd df = pd.DataFrame({\u2018A\u2019: [1, 2, 3], \u2018B\u2019: [4, 5, 6], \u2018C\u2019: [7, 8, 9]}) plot_heatmap(df)</p> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def plot_sns_heatmap(\n    df_heat,\n    figsize=(16, 12),\n    cmap=\"vlag\",\n    vmin=-1,\n    vmax=1,\n    annot=True,\n    fmt=\".5f\",\n    linewidths=0.5,\n    annot_kws={\"size\": 8},\n) -&gt; None:\n    \"\"\"\n    Plots a heatmap of the correlation matrix of the given DataFrame.\n\n    Args:\n        df_heat (pd.DataFrame): DataFrame containing the data to be plotted.\n        figsize (tuple): Size of the figure to be plotted.\n        cmap (str): Color map to be used for the heatmap.\n        vmin (int): Minimum value for the color scale.\n        vmax (int): Maximum value for the color scale.\n        annot (bool): Whether to display annotations on the heatmap.\n        fmt (str): Format string for annotations.\n        linewidths (float): Width of lines separating cells in the heatmap.\n        annot_kws (dict): Keyword arguments for annotations.\n\n    Returns:\n        (NoneType): None\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n        &gt;&gt;&gt; plot_heatmap(df)\n    \"\"\"\n    plt.figure(figsize=figsize)\n    matrix = np.triu(np.ones_like(df_heat.corr()))\n    sns.heatmap(\n        data=df_heat.corr(),\n        cmap=cmap,\n        vmin=vmin,\n        vmax=vmax,\n        annot=annot,\n        fmt=fmt,\n        linewidths=linewidths,\n        annot_kws=annot_kws,\n        mask=matrix,\n    )\n    plt.show()\n    plt.gcf().clear()\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.print_exp_table","title":"<code>print_exp_table(fun_control, tablefmt='github', print_tab=True)</code>","text":"<p>Generates a table with the design variables and their bounds.     Can be used for the experiment design, which was not run yet. Args:     fun_control (dict):         A dictionary with function design variables.     tablefmt (str):         The format of the table. Defaults to \u201cgithub\u201d.     print_tab (bool):         If True, the table is printed. Otherwise, the result code from tabulate         is returned. Defaults to True.</p> <p>Returns:</p> Type Description <code>str</code> <p>a table with the design variables, their default values, and their bounds. Use the <code>print</code> function to display the table.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n    from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n    from spotpython.fun.hyperlight import HyperLight\n    from spotpython.utils.init import fun_control_init\n    from spotpython.spot import Spot\n    from spotpython.utils.eda import print_exp_table\n    fun_control = fun_control_init(\n        PREFIX=\"print_exp_table\",\n        fun_evals=10,\n        max_time=1,\n        data_set = Diabetes(),\n        core_model_name=\"light.regression.NNLinearRegressor\",\n        hyperdict=LightHyperDict,\n        _L_in=10,\n        _L_out=1)\n    fun = HyperLight().fun\n    print_exp_table(fun_control)\n    | name           | type   | default   |   lower |   upper | transform             |\n    |----------------|--------|-----------|---------|---------|-----------------------|\n    | l1             | int    | 3         |     3   |    8    | transform_power_2_int |\n    | epochs         | int    | 4         |     4   |    9    | transform_power_2_int |\n    | batch_size     | int    | 4         |     1   |    4    | transform_power_2_int |\n    | act_fn         | factor | ReLU      |     0   |    5    | None                  |\n    | optimizer      | factor | SGD       |     0   |   11    | None                  |\n    | dropout_prob   | float  | 0.01      |     0   |    0.25 | None                  |\n    | lr_mult        | float  | 1.0       |     0.1 |   10    | None                  |\n    | patience       | int    | 2         |     2   |    6    | transform_power_2_int |\n    | batch_norm     | factor | 0         |     0   |    1    | None                  |\n    | initialization | factor | Default   |     0   |    4    | None             |\n</code></pre> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def print_exp_table(fun_control: dict, tablefmt=\"github\", print_tab=True) -&gt; str:\n    \"\"\"Generates a table with the design variables and their bounds.\n        Can be used for the experiment design, which was not run yet.\n    Args:\n        fun_control (dict):\n            A dictionary with function design variables.\n        tablefmt (str):\n            The format of the table. Defaults to \"github\".\n        print_tab (bool):\n            If True, the table is printed. Otherwise, the result code from tabulate\n            is returned. Defaults to True.\n\n    Returns:\n        (str):\n            a table with the design variables, their default values, and their bounds.\n            Use the `print` function to display the table.\n\n    Examples:\n            &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n                from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n                from spotpython.fun.hyperlight import HyperLight\n                from spotpython.utils.init import fun_control_init\n                from spotpython.spot import Spot\n                from spotpython.utils.eda import print_exp_table\n                fun_control = fun_control_init(\n                    PREFIX=\"print_exp_table\",\n                    fun_evals=10,\n                    max_time=1,\n                    data_set = Diabetes(),\n                    core_model_name=\"light.regression.NNLinearRegressor\",\n                    hyperdict=LightHyperDict,\n                    _L_in=10,\n                    _L_out=1)\n                fun = HyperLight().fun\n                print_exp_table(fun_control)\n                | name           | type   | default   |   lower |   upper | transform             |\n                |----------------|--------|-----------|---------|---------|-----------------------|\n                | l1             | int    | 3         |     3   |    8    | transform_power_2_int |\n                | epochs         | int    | 4         |     4   |    9    | transform_power_2_int |\n                | batch_size     | int    | 4         |     1   |    4    | transform_power_2_int |\n                | act_fn         | factor | ReLU      |     0   |    5    | None                  |\n                | optimizer      | factor | SGD       |     0   |   11    | None                  |\n                | dropout_prob   | float  | 0.01      |     0   |    0.25 | None                  |\n                | lr_mult        | float  | 1.0       |     0.1 |   10    | None                  |\n                | patience       | int    | 2         |     2   |    6    | transform_power_2_int |\n                | batch_norm     | factor | 0         |     0   |    1    | None                  |\n                | initialization | factor | Default   |     0   |    4    | None             |\n    \"\"\"\n    default_values = get_default_values(fun_control)\n    defaults = list(default_values.values())\n    tab = tabulate(\n        {\n            \"name\": get_var_name(fun_control),\n            \"type\": get_var_type(fun_control),\n            \"default\": defaults,\n            \"lower\": get_bound_values(fun_control, \"lower\", as_list=True),\n            \"upper\": get_bound_values(fun_control, \"upper\", as_list=True),\n            \"transform\": get_transform(fun_control),\n        },\n        headers=\"keys\",\n        tablefmt=tablefmt,\n    )\n    if print_tab:\n        print(tab)\n    else:\n        return tab\n</code></pre>"},{"location":"reference/spotpython/utils/eda/#spotpython.utils.eda.print_res_table","title":"<code>print_res_table(spot=None, tablefmt='github', print_tab=True)</code>","text":"<p>Generates a table with the design variables and their bounds, after the run was completed.</p> <p>Parameters:</p> Name Type Description Default <code>spot</code> <code>object</code> <p>A spot object. Defaults to None.</p> <code>None</code> <code>tablefmt</code> <code>str</code> <p>The format of the table. Defaults to \u201cgithub\u201d.</p> <code>'github'</code> <code>print_tab</code> <code>bool</code> <p>If True, the table is printed. Otherwise, the result code from tabulate is returned. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>a table with the design variables, their default values, their bounds, the value and the importance of each hyperparameter. Use the <code>print</code> function to display the table.</p> <p>Examples:</p> <p>from spotpython.data.diabetes import Diabetes     from spotpython.hyperdict.light_hyper_dict import LightHyperDict     from spotpython.fun.hyperlight import HyperLight     from spotpython.utils.init import fun_control_init, design_control_init     from spotpython.spot import Spot     from spotpython.utils.eda import print_res_table     from spotpython.hyperparameters.values import set_hyperparameter     fun_control = fun_control_init(         PREFIX=\u201dprint_res_table\u201d,         fun_evals=5,         max_time=1,         data_set = Diabetes(),         core_model_name=\u201dlight.regression.NNLinearRegressor\u201d,         hyperdict=LightHyperDict,         _L_in=10,         _L_out=1)     set_hyperparameter(fun_control, \u201coptimizer\u201d, [ \u201cAdadelta\u201d, \u201cAdam\u201d, \u201cAdamax\u201d])     set_hyperparameter(fun_control, \u201cl1\u201d, [1,2])     set_hyperparameter(fun_control, \u201cepochs\u201d, [2,2])     set_hyperparameter(fun_control, \u201cbatch_size\u201d, [4,11])     set_hyperparameter(fun_control, \u201cdropout_prob\u201d, [0.0, 0.025])     set_hyperparameter(fun_control, \u201cpatience\u201d, [1,2])     design_control = design_control_init(init_size=3)     fun = HyperLight().fun     S = Spot(fun=fun, fun_control=fun_control, design_control=design_control)     S.run()     print_res_table(S)     | name           | type   | default   |   lower |   upper | tuned                | transform             |   importance | stars   |     |----------------|--------|-----------|---------|---------|----------------------|-----------------------|--------------|---------|     | l1             | int    | 3         |     1.0 |     2.0 | 2.0                  | transform_power_2_int |        29.49 | *       |     | epochs         | int    | 4         |     2.0 |     2.0 | 2.0                  | transform_power_2_int |         0.00 |         |     | batch_size     | int    | 4         |     4.0 |    11.0 | 5.0                  | transform_power_2_int |         1.18 | *       |     | act_fn         | factor | ReLU      |     0.0 |     5.0 | ELU                  | None                  |         0.32 | .       |     | optimizer      | factor | SGD       |     0.0 |     2.0 | Adam                 | None                  |         0.08 |         |     | dropout_prob   | float  | 0.01      |     0.0 |   0.025 | 0.010464684336704316 | None                  |         0.27 | .       |     | lr_mult        | float  | 1.0       |     0.1 |    10.0 | 8.82569482726512     | None                  |         9.55 | *       |     | patience       | int    | 2         |     1.0 |     2.0 | 2.0                  | transform_power_2_int |       100.00 | ***     |     | batch_norm     | factor | 0         |     0.0 |     1.0 | 0                    | None                  |         0.05 |         |     | initialization | factor | Default   |     0.0 |     4.0 | kaiming_normal       | None                  |         1.07 | *       |</p> Source code in <code>spotpython/utils/eda.py</code> <pre><code>def print_res_table(spot: object = None, tablefmt=\"github\", print_tab=True) -&gt; str:\n    \"\"\"\n    Generates a table with the design variables and their bounds,\n    after the run was completed.\n\n    Args:\n        spot (object):\n            A spot object. Defaults to None.\n        tablefmt (str):\n            The format of the table. Defaults to \"github\".\n        print_tab (bool):\n            If True, the table is printed. Otherwise, the result code from tabulate\n            is returned. Defaults to True.\n\n    Returns:\n        (str):\n            a table with the design variables, their default values, their bounds,\n            the value and the importance of each hyperparameter.\n            Use the `print` function to display the table.\n\n    Examples:\n    &gt;&gt;&gt; from spotpython.data.diabetes import Diabetes\n        from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n        from spotpython.fun.hyperlight import HyperLight\n        from spotpython.utils.init import fun_control_init, design_control_init\n        from spotpython.spot import Spot\n        from spotpython.utils.eda import print_res_table\n        from spotpython.hyperparameters.values import set_hyperparameter\n        fun_control = fun_control_init(\n            PREFIX=\"print_res_table\",\n            fun_evals=5,\n            max_time=1,\n            data_set = Diabetes(),\n            core_model_name=\"light.regression.NNLinearRegressor\",\n            hyperdict=LightHyperDict,\n            _L_in=10,\n            _L_out=1)\n        set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n        set_hyperparameter(fun_control, \"l1\", [1,2])\n        set_hyperparameter(fun_control, \"epochs\", [2,2])\n        set_hyperparameter(fun_control, \"batch_size\", [4,11])\n        set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n        set_hyperparameter(fun_control, \"patience\", [1,2])\n        design_control = design_control_init(init_size=3)\n        fun = HyperLight().fun\n        S = Spot(fun=fun, fun_control=fun_control, design_control=design_control)\n        S.run()\n        print_res_table(S)\n        | name           | type   | default   |   lower |   upper | tuned                | transform             |   importance | stars   |\n        |----------------|--------|-----------|---------|---------|----------------------|-----------------------|--------------|---------|\n        | l1             | int    | 3         |     1.0 |     2.0 | 2.0                  | transform_power_2_int |        29.49 | *       |\n        | epochs         | int    | 4         |     2.0 |     2.0 | 2.0                  | transform_power_2_int |         0.00 |         |\n        | batch_size     | int    | 4         |     4.0 |    11.0 | 5.0                  | transform_power_2_int |         1.18 | *       |\n        | act_fn         | factor | ReLU      |     0.0 |     5.0 | ELU                  | None                  |         0.32 | .       |\n        | optimizer      | factor | SGD       |     0.0 |     2.0 | Adam                 | None                  |         0.08 |         |\n        | dropout_prob   | float  | 0.01      |     0.0 |   0.025 | 0.010464684336704316 | None                  |         0.27 | .       |\n        | lr_mult        | float  | 1.0       |     0.1 |    10.0 | 8.82569482726512     | None                  |         9.55 | *       |\n        | patience       | int    | 2         |     1.0 |     2.0 | 2.0                  | transform_power_2_int |       100.00 | ***     |\n        | batch_norm     | factor | 0         |     0.0 |     1.0 | 0                    | None                  |         0.05 |         |\n        | initialization | factor | Default   |     0.0 |     4.0 | kaiming_normal       | None                  |         1.07 | *       |\n    \"\"\"\n    fun_control = spot.fun_control\n    default_values = get_default_values(fun_control)\n    defaults = list(default_values.values())\n    # try spot.print_results. If it fails, issue an error message that asked to run the spot object first\n    try:\n        res = spot.print_results(print_screen=False, dict=fun_control)\n    except AttributeError as e:\n        print(f\"AttributeError: {e}. Did you run the spot object?\")\n        return\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return\n    tuned = [item[1] for item in res]\n    importance = spot.get_importance()\n    stars = get_stars(importance)\n    tab = tabulate(\n        {\n            \"name\": get_var_name(fun_control),\n            \"type\": get_var_type(fun_control),\n            \"default\": defaults,\n            \"lower\": get_bound_values(fun_control, \"lower\", as_list=True),\n            \"upper\": get_bound_values(fun_control, \"upper\", as_list=True),\n            \"tuned\": tuned,\n            \"transform\": get_transform(fun_control),\n            \"importance\": importance,\n            \"stars\": stars,\n        },\n        headers=\"keys\",\n        numalign=\"right\",\n        floatfmt=(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \".2f\"),\n        tablefmt=tablefmt,\n    )\n    if print_tab:\n        print(tab)\n    else:\n        return tab\n</code></pre>"},{"location":"reference/spotpython/utils/effects/","title":"effects","text":""},{"location":"reference/spotpython/utils/effects/#spotpython.utils.effects.plot_all_partial_dependence","title":"<code>plot_all_partial_dependence(df, df_target, model='GradientBoostingRegressor', nrows=5, ncols=6, figsize=(20, 15), title='')</code>","text":"<p>Generates Partial Dependence Plots (PDPs) for every feature in a DataFrame against a target variable, arranged in a grid.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the features.</p> required <code>df_target</code> <code>Series</code> <p>Series containing the target variable.</p> required <code>model</code> <code>str</code> <p>Name of the model class to use (e.g., \u201cGradientBoostingRegressor\u201d).                    Defaults to \u201cGradientBoostingRegressor\u201d.</p> <code>'GradientBoostingRegressor'</code> <code>nrows</code> <code>int</code> <p>Number of rows in the grid of subplots. Defaults to 5.</p> <code>5</code> <code>ncols</code> <code>int</code> <p>Number of columns in the grid of subplots. Defaults to 6.</p> <code>6</code> <code>figsize</code> <code>tuple</code> <p>Figure size (width, height) in inches. Defaults to (20, 15).</p> <code>(20, 15)</code> <code>title</code> <code>str</code> <p>Title for the subplots. Defaults to \u201c\u201d.</p> <code>''</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; form spotpython.utils.effects import plot_all_partial_dependence\n&gt;&gt;&gt; from sklearn.datasets import load_boston\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = load_boston()\n&gt;&gt;&gt; df = pd.DataFrame(data.data, columns=data.feature_names)\n&gt;&gt;&gt; df_target = pd.Series(data.target, name=\"target\")\n&gt;&gt;&gt; plot_all_partial_dependence(df, df_target, model=\"GradientBoostingRegressor\", nrows=5, ncols=6, figsize=(20, 15))\n</code></pre> Source code in <code>spotpython/utils/effects.py</code> <pre><code>def plot_all_partial_dependence(df, df_target, model=\"GradientBoostingRegressor\", nrows=5, ncols=6, figsize=(20, 15), title=\"\") -&gt; None:\n    \"\"\"\n    Generates Partial Dependence Plots (PDPs) for every feature in a DataFrame against a target variable,\n    arranged in a grid.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the features.\n        df_target (pd.Series): Series containing the target variable.\n        model (str, optional): Name of the model class to use (e.g., \"GradientBoostingRegressor\").\n                               Defaults to \"GradientBoostingRegressor\".\n        nrows (int, optional): Number of rows in the grid of subplots. Defaults to 5.\n        ncols (int, optional): Number of columns in the grid of subplots. Defaults to 6.\n        figsize (tuple, optional): Figure size (width, height) in inches. Defaults to (20, 15).\n        title (str, optional): Title for the subplots. Defaults to \"\".\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; form spotpython.utils.effects import plot_all_partial_dependence\n        &gt;&gt;&gt; from sklearn.datasets import load_boston\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = load_boston()\n        &gt;&gt;&gt; df = pd.DataFrame(data.data, columns=data.feature_names)\n        &gt;&gt;&gt; df_target = pd.Series(data.target, name=\"target\")\n        &gt;&gt;&gt; plot_all_partial_dependence(df, df_target, model=\"GradientBoostingRegressor\", nrows=5, ncols=6, figsize=(20, 15))\n\n    \"\"\"\n\n    # Separate features and target\n    X = df\n    y = df_target  # Target variable is now a Series\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Instantiate the model\n    if model == \"GradientBoostingRegressor\":\n        gb_model = GradientBoostingRegressor(random_state=42)\n    elif model == \"RandomForestRegressor\":\n        from sklearn.ensemble import RandomForestRegressor\n\n        gb_model = RandomForestRegressor(random_state=42)\n    elif model == \"DecisionTreeRegressor\":\n        from sklearn.tree import DecisionTreeRegressor\n\n        gb_model = DecisionTreeRegressor(random_state=42)\n    else:\n        raise ValueError(f\"Unsupported model: {model}\")\n\n    # Train model\n    gb_model.fit(X_train, y_train)\n\n    # Create subplots\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n    axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n\n    # Generate PDP for each feature\n    features = X.columns\n    for i, feature in enumerate(features):\n        ax = axes[i]  # Select the axis for the current feature\n        PartialDependenceDisplay.from_estimator(gb_model, X_train, [feature], ax=ax)\n        ax.set_title(title)  # Set the title of the subplot to the feature name\n\n    # Remove empty subplots if the number of features is less than nrows * ncols\n    for i in range(len(features), nrows * ncols):\n        fig.delaxes(axes[i])\n\n    plt.tight_layout()  # Adjust subplot parameters for a tight layout\n    plt.show()\n</code></pre>"},{"location":"reference/spotpython/utils/effects/#spotpython.utils.effects.randorient","title":"<code>randorient(k, p, xi, seed=None)</code>","text":"<p>Generates a random orientation of a sampling matrix. This function creates a random sampling matrix for a given number of dimensions (k), number of levels (p), and step length (xi). The resulting matrix is used for screening designs in the context of experimental design.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of dimensions.</p> required <code>p</code> <code>int</code> <p>Number of levels.</p> required <code>xi</code> <code>float</code> <p>Step length.</p> required <code>seed</code> <code>int</code> <p>Seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A random sampling matrix of shape (k+1, k).</p> Example <p>randorient(k=2, p=3, xi=0.5) array([[0. , 0. ],        [0.5, 0.5],        [1. , 1. ]])</p> Source code in <code>spotpython/utils/effects.py</code> <pre><code>def randorient(k, p, xi, seed=None) -&gt; np.ndarray:\n    \"\"\"Generates a random orientation of a sampling matrix.\n    This function creates a random sampling matrix for a given number of\n    dimensions (k), number of levels (p), and step length (xi). The\n    resulting matrix is used for screening designs in the context of\n    experimental design.\n\n    Args:\n        k (int): Number of dimensions.\n        p (int): Number of levels.\n        xi (float): Step length.\n        seed (int, optional): Seed for the random number generator.\n            Defaults to None.\n\n    Returns:\n        np.ndarray: A random sampling matrix of shape (k+1, k).\n\n    Example:\n        &gt;&gt;&gt; randorient(k=2, p=3, xi=0.5)\n        array([[0. , 0. ],\n               [0.5, 0.5],\n               [1. , 1. ]])\n    \"\"\"\n    # Initialize random number generator with the provided seed\n    if seed is not None:\n        rng = np.random.default_rng(seed)\n    else:\n        rng = np.random.default_rng()\n\n    # Step length\n    Delta = xi / (p - 1)\n\n    m = k + 1\n\n    # A truncated p-level grid in one dimension\n    xs = np.arange(0, 1 - Delta, 1 / (p - 1))\n    xsl = len(xs)\n    if xsl &lt; 1:\n        print(f\"xi = {xi}.\")\n        print(f\"p = {p}.\")\n        print(f\"Delta = {Delta}.\")\n        print(f\"p - 1 = {p - 1}.\")\n        raise ValueError(f\"The number of levels xsl is {xsl}, but it must be greater than 0.\")\n\n    # Basic sampling matrix\n    B = np.vstack((np.zeros((1, k)), np.tril(np.ones((k, k)))))\n\n    # Randomization\n\n    # Matrix with +1s and -1s on the diagonal with equal probability\n    Dstar = np.diag(2 * rng.integers(0, 2, size=k) - 1)\n\n    # Random base value\n    xstar = xs[rng.integers(0, xsl, size=k)]\n\n    # Permutation matrix\n    Pstar = np.zeros((k, k))\n    rp = rng.permutation(k)\n    for i in range(k):\n        Pstar[i, rp[i]] = 1\n\n    # A random orientation of the sampling matrix\n    Bstar = (np.ones((m, 1)) @ xstar.reshape(1, -1) + (Delta / 2) * ((2 * B - np.ones((m, k))) @ Dstar + np.ones((m, k)))) @ Pstar\n\n    return Bstar\n</code></pre>"},{"location":"reference/spotpython/utils/effects/#spotpython.utils.effects.screening_plot","title":"<code>screening_plot(X, fun, xi, p, labels, bounds=None, show=True)</code>","text":"<p>Generates a plot with elementary effect screening metrics.</p> <p>This function calculates the mean and standard deviation of the elementary effects for a given set of design variables and plots the results.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The screening plan matrix, typically structured within a [0,1]^k box.</p> required <code>fun</code> <code>object</code> <p>The objective function to evaluate at each design point in the screening plan.</p> required <code>xi</code> <code>float</code> <p>The elementary effect step length factor.</p> required <code>p</code> <code>int</code> <p>Number of discrete levels along each dimension.</p> required <code>labels</code> <code>list of str</code> <p>A list of variable names corresponding to the design variables.</p> required <code>bounds</code> <code>ndarray</code> <p>A 2xk matrix where the first row contains lower bounds and the second row contains upper bounds for each variable.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is displayed. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function generates a plot of the results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.effects import screening, screeningplan\n    from spotpython.fun.objectivefunctions import Analytical\n    fun = Analytical()\n    k = 10\n    p = 10\n    xi = 1\n    r = 25\n    X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n    # Provide real-world bounds from the wing weight docs (2 x 10).\n    value_range = np.array([\n        [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n        [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n    ])\n    labels = [\n        \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n        \"q\",   \"lambda\", \"tc\", \"N_z\",\n        \"W_dg\", \"W_p\"\n    ]\n    screening(\n        X=X,\n        fun=fun.fun_wingwt,\n        bounds=value_range,\n        xi=xi,\n        p=p,\n        labels=labels,\n        print=False,\n    )\n</code></pre> Source code in <code>spotpython/utils/effects.py</code> <pre><code>def screening_plot(X, fun, xi, p, labels, bounds=None, show=True) -&gt; None:\n    \"\"\"Generates a plot with elementary effect screening metrics.\n\n    This function calculates the mean and standard deviation of the\n    elementary effects for a given set of design variables and plots\n    the results.\n\n    Args:\n        X (np.ndarray):\n            The screening plan matrix, typically structured within a [0,1]^k box.\n        fun (object):\n            The objective function to evaluate at each design point in the screening plan.\n        xi (float):\n            The elementary effect step length factor.\n        p (int):\n            Number of discrete levels along each dimension.\n        labels (list of str):\n            A list of variable names corresponding to the design variables.\n        bounds (np.ndarray):\n            A 2xk matrix where the first row contains lower bounds and\n            the second row contains upper bounds for each variable.\n        show (bool):\n            If True, the plot is displayed. Defaults to True.\n\n    Returns:\n        None: The function generates a plot of the results.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.effects import screening, screeningplan\n            from spotpython.fun.objectivefunctions import Analytical\n            fun = Analytical()\n            k = 10\n            p = 10\n            xi = 1\n            r = 25\n            X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n            # Provide real-world bounds from the wing weight docs (2 x 10).\n            value_range = np.array([\n                [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n                [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n            ])\n            labels = [\n                \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n                \"q\",   \"lambda\", \"tc\", \"N_z\",\n                \"W_dg\", \"W_p\"\n            ]\n            screening(\n                X=X,\n                fun=fun.fun_wingwt,\n                bounds=value_range,\n                xi=xi,\n                p=p,\n                labels=labels,\n                print=False,\n            )\n    \"\"\"\n    k = X.shape[1]\n    sm, ssd = _screening(X=X, fun=fun, xi=xi, p=p, labels=labels, bounds=bounds)\n    plt.figure()\n    for i in range(k):\n        plt.text(sm[i], ssd[i], labels[i], fontsize=10)\n    plt.axis([min(sm), 1.1 * max(sm), min(ssd), 1.1 * max(ssd)])\n    plt.xlabel(\"Sample means\")\n    plt.ylabel(\"Sample standard deviations\")\n    plt.gca().tick_params(labelsize=10)\n    plt.grid(True)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/utils/effects/#spotpython.utils.effects.screening_print","title":"<code>screening_print(X, fun, xi, p, labels, bounds=None)</code>","text":"<p>Generates a DataFrame with elementary effect screening metrics.</p> <p>This function calculates the mean and standard deviation of the elementary effects for a given set of design variables and returns the results as a Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The screening plan matrix, typically structured within a [0,1]^k box.</p> required <code>fun</code> <code>object</code> <p>The objective function to evaluate at each design point in the screening plan.</p> required <code>xi</code> <code>float</code> <p>The elementary effect step length factor.</p> required <code>p</code> <code>int</code> <p>Number of discrete levels along each dimension.</p> required <code>labels</code> <code>list of str</code> <p>A list of variable names corresponding to the design variables.</p> required <code>bounds</code> <code>ndarray</code> <p>A 2xk matrix where the first row contains lower bounds and the second row contains upper bounds for each variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing three columns: - \u2018varname\u2019: The name of each variable. - \u2018mean\u2019: The mean of the elementary effects for each variable. - \u2018sd\u2019: The standard deviation of the elementary effects for each variable.</p> <code>DataFrame</code> <p>or None: If print is set to False, a plot of the results is generated instead of returning a DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.effects import screening, screeningplan\n    from spotpython.fun.objectivefunctions import Analytical\n    fun = Analytical()\n    k = 10\n    p = 10\n    xi = 1\n    r = 25\n    X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n    # Provide real-world bounds from the wing weight docs (2 x 10).\n    value_range = np.array([\n        [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n        [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n    ])\n    labels = [\n        \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n        \"q\",   \"lambda\", \"tc\", \"N_z\",\n        \"W_dg\", \"W_p\"\n    ]\n    screening(\n        X=X,\n        fun=fun.fun_wingwt,\n        bounds=value_range,\n        xi=xi,\n        p=p,\n        labels=labels,\n        print=False,\n    )\n</code></pre> Source code in <code>spotpython/utils/effects.py</code> <pre><code>def screening_print(X, fun, xi, p, labels, bounds=None) -&gt; pd.DataFrame:\n    \"\"\"Generates a DataFrame with elementary effect screening metrics.\n\n    This function calculates the mean and standard deviation of the\n    elementary effects for a given set of design variables and returns\n    the results as a Pandas DataFrame.\n\n    Args:\n        X (np.ndarray): The screening plan matrix, typically structured\n            within a [0,1]^k box.\n        fun (object): The objective function to evaluate at each\n            design point in the screening plan.\n        xi (float): The elementary effect step length factor.\n        p (int): Number of discrete levels along each dimension.\n        labels (list of str): A list of variable names corresponding to\n            the design variables.\n        bounds (np.ndarray): A 2xk matrix where the first row contains\n            lower bounds and the second row contains upper bounds for\n            each variable.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing three columns:\n            - 'varname': The name of each variable.\n            - 'mean': The mean of the elementary effects for each variable.\n            - 'sd': The standard deviation of the elementary effects for\n            each variable.\n        or None: If print is set to False, a plot of the results is\n            generated instead of returning a DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.effects import screening, screeningplan\n            from spotpython.fun.objectivefunctions import Analytical\n            fun = Analytical()\n            k = 10\n            p = 10\n            xi = 1\n            r = 25\n            X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n            # Provide real-world bounds from the wing weight docs (2 x 10).\n            value_range = np.array([\n                [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n                [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n            ])\n            labels = [\n                \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n                \"q\",   \"lambda\", \"tc\", \"N_z\",\n                \"W_dg\", \"W_p\"\n            ]\n            screening(\n                X=X,\n                fun=fun.fun_wingwt,\n                bounds=value_range,\n                xi=xi,\n                p=p,\n                labels=labels,\n                print=False,\n            )\n    \"\"\"\n    sm, ssd = _screening(X=X, fun=fun, xi=xi, p=p, labels=labels, bounds=bounds)\n    idx = np.argsort(-np.abs(sm))\n    sorted_labels = [labels[i] for i in idx]\n    sm = sm[idx]\n    ssd = ssd[idx]\n    df = pd.DataFrame({\"varname\": sorted_labels, \"mean\": sm, \"sd\": ssd})\n    return df\n</code></pre>"},{"location":"reference/spotpython/utils/file/","title":"file","text":""},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.get_experiment_filename","title":"<code>get_experiment_filename(PREFIX)</code>","text":"<p>Returns the name of the experiment file. This is the PREFIX with the suffix \u201c_exp.pkl\u201d. It is none, if PREFIX is None.</p> <p>Parameters:</p> Name Type Description Default <code>PREFIX</code> <code>str</code> <p>Prefix of the experiment.</p> required <p>Returns:</p> Name Type Description <code>filename</code> <code>str</code> <p>Name of the experiment.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import get_experiment_name\n&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n&gt;&gt;&gt; fun_control = fun_control_init(PREFIX=\"branin\")\n&gt;&gt;&gt; PREFIX = fun_control[\"PREFIX\"]\n&gt;&gt;&gt; filename = get_experiment_filename(PREFIX)\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def get_experiment_filename(PREFIX) -&gt; str:\n    \"\"\"Returns the name of the experiment file.\n    This is the PREFIX with the suffix \"_exp.pkl\".\n    It is none, if PREFIX is None.\n\n    Args:\n        PREFIX (str): Prefix of the experiment.\n\n    Returns:\n        filename (str): Name of the experiment.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import get_experiment_name\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n        &gt;&gt;&gt; fun_control = fun_control_init(PREFIX=\"branin\")\n        &gt;&gt;&gt; PREFIX = fun_control[\"PREFIX\"]\n        &gt;&gt;&gt; filename = get_experiment_filename(PREFIX)\n    \"\"\"\n    if PREFIX is None:\n        return None\n    else:\n        filename = PREFIX + \"_exp.pkl\"\n    return filename\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.get_result_filename","title":"<code>get_result_filename(PREFIX)</code>","text":"<p>Returns the name of the result file. This is the PREFIX with the suffix \u201c_res.pkl\u201d. It is none, if PREFIX is None.</p> <p>Parameters:</p> Name Type Description Default <code>PREFIX</code> <code>str</code> <p>Prefix of the experiment.</p> required <p>Returns:</p> Name Type Description <code>filename</code> <code>str</code> <p>Name of the experiment.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import get_experiment_name\n&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n&gt;&gt;&gt; fun_control = fun_control_init(PREFIX=\"branin\")\n&gt;&gt;&gt; PREFIX = fun_control[\"PREFIX\"]\n&gt;&gt;&gt; filename = get_experiment_filename(PREFIX)\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def get_result_filename(PREFIX) -&gt; str:\n    \"\"\"Returns the name of the result file.\n    This is the PREFIX with the suffix \"_res.pkl\".\n    It is none, if PREFIX is None.\n\n    Args:\n        PREFIX (str): Prefix of the experiment.\n\n    Returns:\n        filename (str): Name of the experiment.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import get_experiment_name\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n        &gt;&gt;&gt; fun_control = fun_control_init(PREFIX=\"branin\")\n        &gt;&gt;&gt; PREFIX = fun_control[\"PREFIX\"]\n        &gt;&gt;&gt; filename = get_experiment_filename(PREFIX)\n    \"\"\"\n    if PREFIX is None:\n        return None\n    else:\n        filename = PREFIX + \"_res.pkl\"\n    return filename\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_and_run_spot_python_experiment","title":"<code>load_and_run_spot_python_experiment(PREFIX=None, filename=None)</code>","text":"<p>Loads and runs a spot experiment.</p> <p>Parameters:</p> Name Type Description Default <code>PREFIX</code> <code>str</code> <p>Prefix of the experiment. Defaults to None.</p> <code>None</code> <code>filename</code> <code>str</code> <p>Name of the pickle file. Defaults to None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>spot_tuner</code> <code>Spot</code> <p>The spot tuner object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import load_and_run_spot_python_experiment\n&gt;&gt;&gt; spot_tuner = load_and_run_spot_python_experiment(filename=\"spot_branin_experiment.pickle\")\n&gt;&gt;&gt; # Or use PREFIX\n&gt;&gt;&gt; spot_tuner = load_and_run_spot_python_experiment(PREFIX=\"spot_branin_experiment\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_and_run_spot_python_experiment(PREFIX=None, filename=None) -&gt; object:\n    \"\"\"Loads and runs a spot experiment.\n\n    Args:\n        PREFIX (str, optional): Prefix of the experiment. Defaults to None.\n        filename (str): Name of the pickle file. Defaults to None\n\n    Returns:\n        spot_tuner (Spot): The spot tuner object.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import load_and_run_spot_python_experiment\n        &gt;&gt;&gt; spot_tuner = load_and_run_spot_python_experiment(filename=\"spot_branin_experiment.pickle\")\n        &gt;&gt;&gt; # Or use PREFIX\n        &gt;&gt;&gt; spot_tuner = load_and_run_spot_python_experiment(PREFIX=\"spot_branin_experiment\")\n\n    \"\"\"\n    S = load_experiment(PREFIX=PREFIX, filename=filename)\n    S.run()\n    return S\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_cifar10_data","title":"<code>load_cifar10_data(data_dir='./data')</code>","text":"<p>Loads the CIFAR10 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Directory to save the data. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <p>Returns:</p> Name Type Description <code>trainset</code> <code>CIFAR10</code> <p>Training dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import load_cifar10_data\n&gt;&gt;&gt; trainset = load_cifar10_data(data_dir=\"./data\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_cifar10_data(data_dir=\"./data\"):\n    \"\"\"Loads the CIFAR10 dataset.\n\n    Args:\n        data_dir (str, optional): Directory to save the data. Defaults to \"./data\".\n\n    Returns:\n        trainset (torchvision.datasets.CIFAR10): Training dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import load_cifar10_data\n        &gt;&gt;&gt; trainset = load_cifar10_data(data_dir=\"./data\")\n\n    \"\"\"\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n    testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n    return trainset, testset\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_core_model_from_file","title":"<code>load_core_model_from_file(coremodel, dirname='userModel')</code>","text":"<p>Loads a core model from a python file.</p> <p>Parameters:</p> Name Type Description Default <code>coremodel</code> <code>str</code> <p>Name of the core model.</p> required <code>dirname</code> <code>str</code> <p>Directory name. Defaults to \u201cuserModel\u201d.</p> <code>'userModel'</code> <p>Returns:</p> Name Type Description <code>coremodel</code> <code>object</code> <p>Core model.</p> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_core_model_from_file(coremodel, dirname=\"userModel\"):\n    \"\"\"Loads a core model from a python file.\n\n    Args:\n        coremodel (str): Name of the core model.\n        dirname (str, optional): Directory name. Defaults to \"userModel\".\n\n    Returns:\n        coremodel (object): Core model.\n\n    \"\"\"\n    sys.path.insert(0, \"./\" + dirname)\n    module = importlib.import_module(coremodel)\n    core_model = getattr(module, coremodel)\n    return core_model\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_dict_from_file","title":"<code>load_dict_from_file(coremodel, dirname='userModel')</code>","text":"<p>Loads a dictionary from a json file.</p> <p>Parameters:</p> Name Type Description Default <code>coremodel</code> <code>str</code> <p>Name of the core model.</p> required <code>dirname</code> <code>str</code> <p>Directory name. Defaults to \u201cuserModel\u201d.</p> <code>'userModel'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with the core model.</p> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_dict_from_file(coremodel, dirname=\"userModel\"):\n    \"\"\"Loads a dictionary from a json file.\n\n    Args:\n        coremodel (str): Name of the core model.\n        dirname (str, optional): Directory name. Defaults to \"userModel\".\n\n    Returns:\n        dict (dict): Dictionary with the core model.\n\n    \"\"\"\n    file_path = os.path.join(dirname, f\"{coremodel}.json\")\n    if os.path.isfile(file_path):\n        with open(file_path, \"r\") as f:\n            dict_tmp = json.load(f)\n            dict = dict_tmp[coremodel]\n    else:\n        print(f\"The file {file_path} does not exist.\")\n        dict = None\n    return dict\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_experiment","title":"<code>load_experiment(PREFIX=None, filename=None)</code>","text":"<p>Loads the experiment from a pickle file. If filename is None and PREFIX is not None, the experiment is loaded based on the PREFIX using the get_experiment_filename function. If the spot tuner object and the fun control dictionary do not exist, an error is thrown. If the design control, surrogate control, and optimizer control dictionaries do not exist, a warning is issued and <code>None</code> is assigned to the corresponding variables.</p> <p>Parameters:</p> Name Type Description Default <code>PREFIX</code> <code>str</code> <p>Prefix of the experiment. Defaults to None.</p> <code>None</code> <code>filename</code> <code>str</code> <p>Name of the pickle file. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>spot_tuner</code> <code>Spot</code> <p>The spot tuner object.</p> Notes <p>The corresponding save_experiment function is part of the class spot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import load_experiment\n&gt;&gt;&gt; spot_tuner, fun_control, design_control, _, _ = load_experiment(filename=\"RUN_0.pkl\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_experiment(PREFIX=None, filename=None):\n    \"\"\"\n    Loads the experiment from a pickle file.\n    If filename is None and PREFIX is not None, the experiment is loaded based on the PREFIX\n    using the get_experiment_filename function.\n    If the spot tuner object and the fun control dictionary do not exist, an error is thrown.\n    If the design control, surrogate control, and optimizer control dictionaries do not exist, a warning is issued\n    and `None` is assigned to the corresponding variables.\n\n    Args:\n        PREFIX (str, optional): Prefix of the experiment. Defaults to None.\n        filename (str): Name of the pickle file. Defaults to None.\n\n    Returns:\n        spot_tuner (Spot): The spot tuner object.\n\n    Notes:\n        The corresponding save_experiment function is part of the class spot.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import load_experiment\n        &gt;&gt;&gt; spot_tuner, fun_control, design_control, _, _ = load_experiment(filename=\"RUN_0.pkl\")\n\n    \"\"\"\n    filename = _handle_exp_filename(filename, PREFIX)\n    with open(filename, \"rb\") as handle:\n        spot_tuner = pickle.load(handle)\n        print(f\"Loaded experiment from {filename}\")\n    return spot_tuner\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_pickle","title":"<code>load_pickle(filename)</code>","text":"<p>Loads a pickle file.     Add .pkl to the filename.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the pickle file.</p> required <p>Returns:</p> Type Description <code>object</code> <p>Loaded object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import load_pickle\n&gt;&gt;&gt; obj = load_pickle(filename=\"obj.pkl\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_pickle(filename: str):\n    \"\"\"Loads a pickle file.\n        Add .pkl to the filename.\n\n    Args:\n        filename (str): Name of the pickle file.\n\n    Returns:\n        (object): Loaded object.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import load_pickle\n        &gt;&gt;&gt; obj = load_pickle(filename=\"obj.pkl\")\n    \"\"\"\n    filename = filename + \".pkl\"\n    with open(filename, \"rb\") as f:\n        obj = pickle.load(f)\n    return obj\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.load_result","title":"<code>load_result(PREFIX=None, filename=None)</code>","text":"<p>Loads the result from a pickle file with the name PREFIX + \u201c_res.pkl\u201d. This is the standard filename for the result file, when it is saved by the spot tuner using <code>save_result()</code>, i.e., when fun_control[\u201csave_result\u201d] is set to True. If a filename is provided, the result is loaded from this file.</p> <p>Parameters:</p> Name Type Description Default <code>PREFIX</code> <code>str</code> <p>Prefix of the experiment. Defaults to None.</p> <code>None</code> <code>filename</code> <code>str</code> <p>Name of the pickle file. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>spot_tuner</code> <code>Spot</code> <p>The spot tuner object.</p> Notes <p>The corresponding save_result function is part of the class spot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import load_result\n&gt;&gt;&gt; load_result(\"branin\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def load_result(PREFIX=None, filename=None) -&gt; tuple:\n    \"\"\"Loads the result from a pickle file with the name\n    PREFIX + \"_res.pkl\".\n    This is the standard filename for the result file,\n    when it is saved by the spot tuner using `save_result()`, i.e.,\n    when fun_control[\"save_result\"] is set to True.\n    If a filename is provided, the result is loaded from this file.\n\n    Args:\n        PREFIX (str): Prefix of the experiment. Defaults to None.\n        filename (str): Name of the pickle file. Defaults to None.\n\n    Returns:\n        spot_tuner (Spot): The spot tuner object.\n\n    Notes:\n        The corresponding save_result function is part of the class spot.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import load_result\n        &gt;&gt;&gt; load_result(\"branin\")\n\n    \"\"\"\n    filename = _handle_res_filename(filename, PREFIX)\n    spot_tuner = load_experiment(filename=filename)\n    return spot_tuner\n</code></pre>"},{"location":"reference/spotpython/utils/file/#spotpython.utils.file.save_pickle","title":"<code>save_pickle(obj, filename)</code>","text":"<p>Saves an object as a pickle file.     Add .pkl to the filename.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>Object to be saved.</p> required <code>filename</code> <code>str</code> <p>Name of the pickle file.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.file import save_pickle\n&gt;&gt;&gt; save_pickle(obj, filename=\"obj.pkl\")\n</code></pre> Source code in <code>spotpython/utils/file.py</code> <pre><code>def save_pickle(obj, filename: str):\n    \"\"\"Saves an object as a pickle file.\n        Add .pkl to the filename.\n\n    Args:\n        obj (object): Object to be saved.\n        filename (str): Name of the pickle file.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.file import save_pickle\n        &gt;&gt;&gt; save_pickle(obj, filename=\"obj.pkl\")\n    \"\"\"\n    filename = filename + \".pkl\"\n    with open(filename, \"wb\") as f:\n        pickle.dump(obj, f)\n</code></pre>"},{"location":"reference/spotpython/utils/init/","title":"init","text":""},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.X_reshape","title":"<code>X_reshape(X)</code>","text":"<p>Reshape X to 2D array.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The input array.</p> required <p>Returns:</p> Name Type Description <code>X</code> <code>array</code> <p>The reshaped input array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPy.utils.init import X_reshape\n&gt;&gt;&gt; X = np.array([1,2,3])\n&gt;&gt;&gt; X_reshape(X)\narray([[1, 2, 3]])\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def X_reshape(X) -&gt; np.array:\n    \"\"\"Reshape X to 2D array.\n\n    Args:\n        X (np.array):\n            The input array.\n\n    Returns:\n        X (np.array):\n            The reshaped input array.\n\n    Examples:\n        &gt;&gt;&gt; from spotPy.utils.init import X_reshape\n        &gt;&gt;&gt; X = np.array([1,2,3])\n        &gt;&gt;&gt; X_reshape(X)\n        array([[1, 2, 3]])\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    X = np.atleast_2d(X)\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.check_and_create_dir","title":"<code>check_and_create_dir(path)</code>","text":"<p>Check if the path exists and create it if it does not.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the directory.</p> required <p>Returns:</p> Type Description <code>noneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fromspotPy.utils.init import check_and_create_dir\n&gt;&gt;&gt; check_and_create_dir(\"data/\")\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def check_and_create_dir(path) -&gt; None:\n    \"\"\"Check if the path exists and create it if it does not.\n\n    Args:\n        path (str): Path to the directory.\n\n    Returns:\n        (noneType): None\n\n    Examples:\n        &gt;&gt;&gt; fromspotPy.utils.init import check_and_create_dir\n        &gt;&gt;&gt; check_and_create_dir(\"data/\")\n    \"\"\"\n    if not isinstance(path, str):\n        raise Exception(\"path must be a string\")\n    if not os.path.exists(path):\n        os.makedirs(path)\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.create_spot_tensorboard_path","title":"<code>create_spot_tensorboard_path(tensorboard_log, prefix)</code>","text":"<p>Creates the spot_tensorboard_path and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>tensorboard_log</code> <code>bool</code> <p>If True, the path to the folder where the tensorboard files are saved is created.</p> required <code>prefix</code> <code>str</code> <p>The prefix for the experiment name.</p> required <p>Returns:</p> Name Type Description <code>spot_tensorboard_path</code> <code>str</code> <p>The path to the folder where the tensorboard files are saved.</p> Source code in <code>spotpython/utils/init.py</code> <pre><code>def create_spot_tensorboard_path(tensorboard_log, prefix) -&gt; str:\n    \"\"\"Creates the spot_tensorboard_path and returns it.\n\n    Args:\n        tensorboard_log (bool):\n            If True, the path to the folder where the tensorboard files are saved is created.\n        prefix (str):\n            The prefix for the experiment name.\n\n    Returns:\n        spot_tensorboard_path (str):\n            The path to the folder where the tensorboard files are saved.\n    \"\"\"\n    if tensorboard_log:\n        experiment_name = get_experiment_name(prefix=prefix)\n        spot_tensorboard_path = get_spot_tensorboard_path(experiment_name)\n        os.makedirs(spot_tensorboard_path, exist_ok=True)\n        print(f\"Created spot_tensorboard_path: {spot_tensorboard_path} for SummaryWriter()\")\n    else:\n        spot_tensorboard_path = None\n    return spot_tensorboard_path\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.design_control_init","title":"<code>design_control_init(init_size=10, repeats=1)</code>","text":"<p>Initialize design_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>init_size</code> <code>int</code> <p>The initial size of the experimental design.</p> <code>10</code> <code>repeats</code> <code>int</code> <p>The number of repeats of the design.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>design_control</code> <code>dict</code> <p>A dictionary containing the information about the design of experiments.</p> Source code in <code>spotpython/utils/init.py</code> <pre><code>def design_control_init(init_size=10, repeats=1) -&gt; dict:\n    \"\"\"Initialize design_control dictionary.\n\n    Args:\n        init_size (int): The initial size of the experimental design.\n        repeats (int): The number of repeats of the design.\n\n    Returns:\n        design_control (dict):\n            A dictionary containing the information about the design of experiments.\n\n    \"\"\"\n    design_control = {\"init_size\": init_size, \"repeats\": repeats}\n    return design_control\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.fun_control_init","title":"<code>fun_control_init(_L_in=None, _L_out=None, _L_cond=None, _torchmetric=None, PREFIX=None, TENSORBOARD_CLEAN=False, accelerator='auto', collate_fn_name=None, converters=None, core_model=None, core_model_name=None, data=None, data_full_train=None, hacky=False, data_val=None, data_dir='./data', data_module=None, data_set=None, data_set_name=None, data_test=None, db_dict_name=None, design=None, device=None, devices='auto', enable_progress_bar=False, EXPERIMENT_NAME=None, eval=None, force_run=True, fun_evals=15, fun_mo2so=None, fun_repeats=1, horizon=None, hyperdict=None, infill_criterion='y', log_every_n_steps=50, log_level=50, lower=None, max_time=1, max_surrogate_points=30, metric_sklearn=None, metric_sklearn_name=None, noise=False, n_points=1, n_samples=None, num_sanity_val_steps=2, n_total=None, num_workers=0, num_nodes=1, ocba_delta=0, oml_grace_period=None, optimizer=None, penalty_NA=None, precision='32', prep_model=None, prep_model_name=None, progress_file=None, save_experiment=False, save_result=True, scaler=None, scaler_name=None, scenario=None, seed=123, show_config=False, show_models=False, show_progress=True, shuffle=None, shuffle_train=True, shuffle_val=False, shuffle_test=False, sigma=0.0, strategy='auto', surrogate=None, target_column=None, target_type=None, task=None, tensorboard_log=False, tensorboard_start=False, tensorboard_stop=False, test=None, test_seed=1234, test_size=0.4, tkagg=False, train=None, tolerance_x=0, upper=None, var_name=None, var_type=['num'], verbosity=0, weights=1.0, weight_coeff=0.0, weights_entry=None)</code>","text":"<p>Initialize fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>None</code> <code>_L_out</code> <code>int</code> <p>The number of output features.</p> <code>None</code> <code>_L_cond</code> <code>int</code> <p>The number of conditional features.</p> <code>None</code> <code>_torchmetric</code> <code>str</code> <p>The metric to be used by the Lighting Trainer. For example \u201cmean_squared_error\u201d, see https://lightning.ai/docs/torchmetrics/stable/regression/mean_squared_error.html</p> <code>None</code> <code>accelerator</code> <code>str</code> <p>The accelerator to be used by the Lighting Trainer. It can be either \u201cauto\u201d, \u201cdp\u201d, \u201cddp\u201d, \u201cddp2\u201d, \u201cddp_spawn\u201d, \u201cddp_cpu\u201d, \u201cgpu\u201d, \u201ctpu\u201d. Default is \u201cauto\u201d.</p> <code>'auto'</code> <code>collate_fn_name</code> <code>str</code> <p>The name of the collate function. Default is None.</p> <code>None</code> <code>converters</code> <code>dict</code> <p>A dictionary containing the converters. Default is None.</p> <code>None</code> <code>core_model</code> <code>object</code> <p>The core model object. Default is None.</p> <code>None</code> <code>core_model_name</code> <code>str</code> <p>The name of the core model. Default is None.</p> <code>None</code> <code>data</code> <code>object</code> <p>The data object. Default is None.</p> <code>None</code> <code>data_dir</code> <code>str</code> <p>The directory to save the data. Default is \u201c./data\u201d.</p> <code>'./data'</code> <code>data_full_train</code> <code>Dataset</code> <p>The full training dataset from which training and validation sets will be derived if data_val is None. Default is None.</p> <code>None</code> <code>data_val</code> <code>Dataset</code> <p>The validation dataset. Default is None. If not None, the training and validation sets are derived from the full training dataset (data_full_train)  and the validation dataset (data_val).</p> <code>None</code> <code>data_module</code> <code>object</code> <p>The data module object. Default is None.</p> <code>None</code> <code>data_set</code> <code>object</code> <p>The data set object. Default is None.</p> <code>None</code> <code>data_set_name</code> <code>str</code> <p>The name of the data set. Default is None.</p> <code>None</code> <code>data_test</code> <code>Dataset</code> <p>The separate test dataset that will be used for testing. Default is None.</p> <code>None</code> <code>db_dict_name</code> <code>str</code> <p>The name of the database dictionary. Default is None.</p> <code>None</code> <code>device</code> <code>str</code> <p>The device to use for the training. It can be either \u201ccpu\u201d, \u201cmps\u201d, or \u201ccuda\u201d.</p> <code>None</code> <code>devices</code> <code>str or int</code> <p>The number of devices to use for the training/validation/testing. Default is 1. Can be \u201cauto\u201d or an integer.</p> <code>'auto'</code> <code>design</code> <code>object</code> <p>The experimental design object. Default is None.</p> <code>None</code> <code>enable_progress_bar</code> <code>bool</code> <p>Whether to enable the progress bar or not.</p> <code>False</code> <code>eval</code> <code>str</code> <p>evaluation method used in sklearn taintest.py. Can be \u201ceval_test\u201d, \u201ceval_oon_score\u201d, \u201ctrain_cv\u201d or None. Default is None.</p> <code>None</code> <code>EXPERIMENT_NAME</code> <code>str</code> <p>The name of the experiment. Default is None. If None, the experiment name is generated based on the current date and time.</p> <code>None</code> <code>force_run</code> <code>bool</code> <p>Whether to force the run or not. If a result file (PREFIX+\u201d_run.pkl\u201d) exists, the run is mot performed and the result is loaded from the file. Default is False.</p> <code>True</code> <code>fun_evals</code> <code>int</code> <p>The number of function evaluations.</p> <code>15</code> <code>fun_mo2so</code> <code>object</code> <p>The multi-objective to single-objective transformation object. Default is None. If None, the first objective value is used in case of multi-objective optimization.</p> <code>None</code> <code>fun_repeats</code> <code>int</code> <p>The number of function repeats during the optimization. this value does not affect the number of the repeats in the initial design (this value can be set in the design_control). Default is 1.</p> <code>1</code> <code>horizon</code> <code>int</code> <p>The horizon of the time series data. Default is None.</p> <code>None</code> <code>hyperdict</code> <code>dict</code> <p>A dictionary containing the hyperparameters. Default is None. For example: <code>spotriver.hyperdict.river_hyper_dict import RiverHyperDict</code></p> <code>None</code> <code>infill_criterion</code> <code>str</code> <p>Can be <code>\"y\"</code>, <code>\"s\"</code>, <code>\"ei\"</code> (negative expected improvement), or <code>\"all\"</code>. Default is \u201cy\u201d.</p> <code>'y'</code> <code>log_every_n_steps</code> <code>int</code> <p>Lightning: How often to log within steps. Default: 50.</p> <code>50</code> <code>log_level</code> <code>int</code> <p>log level with the following settings: <code>NOTSET</code> (<code>0</code>), <code>DEBUG</code> (<code>10</code>: Detailed information, typically of interest only when diagnosing problems.), <code>INFO</code> (<code>20</code>: Confirmation that things are working as expected.), <code>WARNING</code> (<code>30</code>: An indication that something unexpected happened, or indicative of some problem in the near     future (e.g. \u2018disk space low\u2019). The software is still working as expected.), <code>ERROR</code> (<code>40</code>: Due to a more serious problem, the software has not been able to perform some function.), and <code>CRITICAL</code> (<code>50</code>: A serious error, indicating that the program itself may be unable to continue running.)</p> <code>50</code> <code>lower</code> <code>array</code> <p>lower bound</p> <code>None</code> <code>max_time</code> <code>int</code> <p>The maximum time in minutes.</p> <code>1</code> <code>max_surrogate_points</code> <code>int</code> <p>The maximum number of points in the surrogate model. Default is inf.</p> <code>30</code> <code>metric_sklearn</code> <code>object</code> <p>The metric object from the scikit-learn library. Default is None.</p> <code>None</code> <code>metric_sklearn_name</code> <code>str</code> <p>The name of the metric object from the scikit-learn library. Default is None.</p> <code>None</code> <code>noise</code> <code>bool</code> <p>Whether the objective function is noiy or not. Default is False. Affects the repeat of the function evaluations.</p> <code>False</code> <code>n_points</code> <code>int</code> <p>The number of infill points to be generated by the surrogate in each iteration.</p> <code>1</code> <code>num_sanity_val_steps</code> <code>int</code> <pre><code>Lightning: Sanity check runs n validation batches before starting the training routine.\nSet it to -1 to run all batches in all validation dataloaders.\nDefault: 2.\n</code></pre> <code>2</code> <code>n_samples</code> <code>int</code> <p>The number of samples in the dataset. Default is None.</p> <code>None</code> <code>n_total</code> <code>int</code> <p>The total number of samples in the dataset. Default is None.</p> <code>None</code> <code>num_nodes</code> <code>int</code> <p>The number of GPU nodes to use for the training/validation/testing. Default is 1.</p> <code>1</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for the data loading. Default is 0.</p> <code>0</code> <code>ocba_delta</code> <code>int</code> <p>The number of additional, new points (only used if noise==True) generated by the OCBA infill criterion. Default is 0.</p> <code>0</code> <code>oml_grace_period</code> <code>int</code> <p>The grace period for the OML algorithm. Default is None.</p> <code>None</code> <code>optimizer</code> <code>object</code> <p>The optimizer object used for the search on surrogate. Default is None.</p> <code>None</code> <code>penalty_NA</code> <code>float</code> <p>The penalty for NA values. Default is None. If None, the values are ignored, e.g., the initial design size used for the surrogate is reduced by the number of NA values.</p> <code>None</code> <code>precision</code> <code>str</code> <p>The precision of the data. Default is \u201c32\u201d. Can be e.g., \u201c16-mixed\u201d or \u201c16-true\u201d.</p> <code>'32'</code> <code>PREFIX</code> <code>str</code> <p>The prefix of the experiment name. If the PREFIX is not None, a spotWriter that us an instance of a SummaryWriter(), is created. Default is \u201c00\u201d.</p> <code>None</code> <code>prep_model</code> <code>object</code> <p>The preprocessing model object. Used for river. Default is None.</p> <code>None</code> <code>prep_model_name</code> <code>str</code> <p>The name of the preprocessing model. Default is None.</p> <code>None</code> <code>progress_file</code> <code>str</code> <p>The name of the progress file. Default is None.</p> <code>None</code> <code>save_experiment</code> <code>bool</code> <p>Whether to save the experiment before the run is started or not. Default is False.</p> <code>False</code> <code>save_result</code> <code>bool</code> <p>Whether to save the result after the experiment is done or not. Default is False.</p> <code>True</code> <code>scaler</code> <code>object</code> <p>The scaler object, e.g., the TorchStandard scaler from spot.utils.scaler.py. Default is None.</p> <code>None</code> <code>scaler_name</code> <code>str</code> <p>The name of the scaler object. Default is None.</p> <code>None</code> <code>scenario</code> <code>str</code> <p>The scenario to use. Default is None. Can be \u201criver\u201d, \u201csklearn\u201d, or \u201clightning\u201d.</p> <code>None</code> <code>seed</code> <code>int</code> <p>The seed to use for the random number generator. Default is 123.</p> <code>123</code> <code>sigma</code> <code>float</code> <p>The standard deviation of the noise of the objective function.</p> <code>0.0</code> <code>show_progress</code> <code>bool</code> <p>Whether to show the progress or not. Default is <code>True</code>.</p> <code>True</code> <code>show_models</code> <code>bool</code> <p>Plot model each generation. Currently only 1-dim functions are supported. Default is <code>False</code>.</p> <code>False</code> <code>show_config</code> <code>bool</code> <p>Whether to show the configuration or not. Default is <code>False</code>.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>Whether the data were shuffled or not. Default is None.</p> <code>None</code> <code>shuffle_train</code> <code>bool</code> <p>Whether the training data were shuffled or not. Default is True.</p> <code>True</code> <code>shuffle_val</code> <code>bool</code> <p>Whether the validation data were shuffled or not. Default is False.</p> <code>False</code> <code>shuffle_test</code> <code>bool</code> <p>Whether the test data were shuffled or not. Default is False.</p> <code>False</code> <code>surrogate</code> <code>object</code> <p>The surrogate model object. Default is None.</p> <code>None</code> <code>strategy</code> <code>str</code> <p>The strategy to use. Default is \u201cauto\u201d.</p> <code>'auto'</code> <code>target_column</code> <code>str</code> <p>The name of the target column. Default is None.</p> <code>None</code> <code>target_type</code> <code>str</code> <p>The type of the target column. Default is None.</p> <code>None</code> <code>task</code> <code>str</code> <p>The task to perform. It can be either \u201cclassification\u201d or \u201cregression\u201d. Default is None.</p> <code>None</code> <code>TENSORBOARD_CLEAN</code> <code>bool</code> <p>Whether to clean (delete) the tensorboard folder or not. Default is False.</p> <code>False</code> <code>tensorboard_log</code> <code>bool</code> <p>Whether to log the tensorboard or not. Starts the SummaryWriter. Default is False.</p> <code>False</code> <code>tensorboard_start</code> <code>bool</code> <p>Whether to start the tensorboard or not. Default is False.</p> <code>False</code> <code>tensorboard_stop</code> <code>bool</code> <p>Whether to stop the tensorboard or not. Default is False.</p> <code>False</code> <code>test</code> <code>object</code> <p>The test data set for spotriver. Default is None.</p> <code>None</code> <code>test_seed</code> <code>int</code> <p>The seed to use for the test set. Default is 1234.</p> <code>1234</code> <code>test_size</code> <code>float</code> <p>The size of the test set. Default is 0.4, i.e., 60% of the data is used for training and 40% for testing.</p> <code>0.4</code> <code>tkagg</code> <code>bool</code> <p>Whether to use matplotlib TkAgg or not. Default is False.</p> <code>False</code> <code>tolerance_x</code> <code>float</code> <p>tolerance for new x solutions. Minimum distance of new solutions, generated by <code>suggest_new_X</code>, to already existing solutions. If zero (which is the default), every new solution is accepted.</p> <code>0</code> <code>train</code> <code>object</code> <p>The training data set for spotriver. Default is None.</p> <code>None</code> <code>upper</code> <code>array</code> <p>upper bound</p> <code>None</code> <code>var_name</code> <code>list</code> <p>A list containing the name of the variables, e.g., [\u201cx1\u201d, \u201cx2\u201d]. Default is None.</p> <code>None</code> <code>var_type</code> <code>List[str]</code> <p>list of type information, can be either \u201cint\u201d, \u201cnum\u201d or \u201cfactor\u201d. Default is [\u201cnum\u201d].</p> <code>['num']</code> <code>verbosity</code> <code>int</code> <p>The verbosity level. Determines print output to console. Higher values result in more output. Default is 0.</p> <code>0</code> <code>weights</code> <code>float</code> <p>The weight coefficient of the objective function. Positive values mean minimization. If set to -1, scores that are better when maximized will be minimized, e.g, accuracy. Can be an array, so that different weights can be used for different (multiple) objectives. Default is 1.0.</p> <code>1.0</code> <code>weight_coeff</code> <code>float</code> <p>Determines how to weight older measures. Default is 1.0. Used in the OML algorithm eval_oml.py. Default is 0.0.</p> <code>0.0</code> <code>weights_entry</code> <code>str</code> <p>The weights entry used in the GUI. Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fun_control</code> <code>dict</code> <p>A dictionary containing the information about the core model, loss function, metrics, and the hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n    fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n    fun_control\n    {'CHECKPOINT_PATH': 'saved_models/',\n        'DATASET_PATH': 'data/',\n        'RESULTS_PATH': 'results/',\n        'TENSORBOARD_PATH': 'runs/',\n        '_L_in': 64,\n        '_L_out': 11,\n        '_L_cond': None,\n        'accelerator': \"auto\",\n        'core_model': None,\n        'core_model_name': None,\n        'data': None,\n        'data_dir': './data',\n        'db_dict_name': None,\n        'device': None,\n        'devices': \"auto\",\n        'enable_progress_bar': False,\n        'eval': None,\n        'horizon': 7,\n        'infill_criterion': 'y',\n        'k_folds': None,\n        'loss_function': None,\n        'lower': None,\n        'max_surrogate_points': 100,\n        'metric_river': None,\n        'metric_sklearn': None,\n        'metric_sklearn_name': None,\n        'metric_torch': None,\n        'metric_params': {},\n        'model_dict': {},\n        'noise': False,\n        'n_points': 1,\n        'n_samples': None,\n        'num_workers': 0,\n        'ocba_delta': 0,\n        'oml_grace_period': None,\n        'optimizer': None,\n        'path': None,\n        'prep_model': None,\n        'prep_model_name': None,\n        'save_model': False,\n        'scenario': \"lightning\",\n        'seed': 1234,\n        'show_batch_interval': 1000000,\n        'shuffle': None,\n        'sigma': 0.0,\n        'target_column': None,\n        'target_type': None,\n        'train': None,\n        'test': None,\n        'task': 'classification',\n        'tensorboard_path': None,\n        'upper': None,\n        'weights': 1.0,\n        'writer': None}\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def fun_control_init(\n    _L_in=None,\n    _L_out=None,\n    _L_cond=None,\n    _torchmetric=None,\n    PREFIX=None,\n    TENSORBOARD_CLEAN=False,\n    accelerator=\"auto\",\n    collate_fn_name=None,\n    converters=None,\n    core_model=None,\n    core_model_name=None,\n    data=None,\n    data_full_train=None,\n    hacky=False,  # !TODO: Documentation\n    data_val=None,\n    data_dir=\"./data\",\n    data_module=None,\n    data_set=None,\n    data_set_name=None,\n    data_test=None,\n    db_dict_name=None,\n    design=None,\n    device=None,\n    devices=\"auto\",\n    enable_progress_bar=False,\n    EXPERIMENT_NAME=None,\n    eval=None,\n    force_run=True,\n    fun_evals=15,\n    fun_mo2so=None,\n    fun_repeats=1,\n    horizon=None,\n    hyperdict=None,\n    infill_criterion=\"y\",\n    log_every_n_steps=50,\n    log_level=50,\n    lower=None,\n    max_time=1,\n    max_surrogate_points=30,\n    metric_sklearn=None,\n    metric_sklearn_name=None,\n    noise=False,\n    n_points=1,\n    n_samples=None,\n    num_sanity_val_steps=2,\n    n_total=None,\n    num_workers=0,\n    num_nodes=1,\n    ocba_delta=0,\n    oml_grace_period=None,\n    optimizer=None,\n    penalty_NA=None,\n    precision=\"32\",\n    prep_model=None,\n    prep_model_name=None,\n    progress_file=None,\n    save_experiment=False,\n    save_result=True,\n    scaler=None,\n    scaler_name=None,\n    scenario=None,\n    seed=123,\n    show_config=False,\n    show_models=False,\n    show_progress=True,\n    shuffle=None,\n    shuffle_train=True,\n    shuffle_val=False,\n    shuffle_test=False,\n    sigma=0.0,\n    strategy=\"auto\",\n    surrogate=None,\n    target_column=None,\n    target_type=None,\n    task=None,\n    tensorboard_log=False,\n    tensorboard_start=False,\n    tensorboard_stop=False,\n    test=None,\n    test_seed=1234,\n    test_size=0.4,\n    tkagg=False,\n    train=None,\n    tolerance_x=0,\n    upper=None,\n    var_name=None,\n    var_type=[\"num\"],\n    verbosity=0,\n    weights=1.0,\n    weight_coeff=0.0,\n    weights_entry=None,\n):\n    \"\"\"Initialize fun_control dictionary.\n\n    Args:\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output features.\n        _L_cond (int):\n            The number of conditional features.\n        _torchmetric (str):\n            The metric to be used by the Lighting Trainer.\n            For example \"mean_squared_error\",\n            see https://lightning.ai/docs/torchmetrics/stable/regression/mean_squared_error.html\n        accelerator (str):\n            The accelerator to be used by the Lighting Trainer.\n            It can be either \"auto\", \"dp\", \"ddp\", \"ddp2\", \"ddp_spawn\", \"ddp_cpu\", \"gpu\", \"tpu\".\n            Default is \"auto\".\n        collate_fn_name (str):\n            The name of the collate function. Default is None.\n        converters (dict):\n            A dictionary containing the converters. Default is None.\n        core_model (object):\n            The core model object. Default is None.\n        core_model_name (str):\n            The name of the core model. Default is None.\n        data (object):\n            The data object. Default is None.\n        data_dir (str):\n            The directory to save the data. Default is \"./data\".\n        data_full_train (torch.utils.data.Dataset, optional):\n            The full training dataset from which training and validation sets will be derived if data_val is None.\n            Default is None.\n        data_val (torch.utils.data.Dataset, optional):\n            The validation dataset. Default is None. If not None, the training and validation sets are derived from\n            the full training dataset (data_full_train)  and the validation dataset (data_val).\n        data_module (object):\n            The data module object. Default is None.\n        data_set (object):\n            The data set object. Default is None.\n        data_set_name (str):\n            The name of the data set. Default is None.\n        data_test (torch.utils.data.Dataset, optional):\n            The separate test dataset that will be used for testing. Default is None.\n        db_dict_name (str):\n            The name of the database dictionary. Default is None.\n        device (str):\n            The device to use for the training. It can be either \"cpu\", \"mps\", or \"cuda\".\n        devices (str or int):\n            The number of devices to use for the training/validation/testing.\n            Default is 1. Can be \"auto\" or an integer.\n        design (object):\n            The experimental design object. Default is None.\n        enable_progress_bar (bool):\n            Whether to enable the progress bar or not.\n        eval (str):\n            evaluation method used in sklearn taintest.py.\n            Can be \"eval_test\", \"eval_oon_score\", \"train_cv\" or None. Default is None.\n        EXPERIMENT_NAME (str):\n            The name of the experiment.\n            Default is None. If None, the experiment name is generated based on the\n            current date and time.\n        force_run (bool):\n            Whether to force the run or not. If a result file (PREFIX+\"_run.pkl\") exists, the run is mot\n            performed and the result is loaded from the file.\n            Default is False.\n        fun_evals (int):\n            The number of function evaluations.\n        fun_mo2so (object):\n            The multi-objective to single-objective transformation object. Default is None.\n            If None, the first objective value is used in case of multi-objective optimization.\n        fun_repeats (int):\n            The number of function repeats during the optimization. this value does not affect\n            the number of the repeats in the initial design (this value can be set in the\n            design_control). Default is 1.\n        horizon (int):\n            The horizon of the time series data. Default is None.\n        hyperdict (dict):\n            A dictionary containing the hyperparameters. Default is None.\n            For example: `spotriver.hyperdict.river_hyper_dict import RiverHyperDict`\n        infill_criterion (str):\n            Can be `\"y\"`, `\"s\"`, `\"ei\"` (negative expected improvement), or `\"all\"`. Default is \"y\".\n        log_every_n_steps (int):\n            Lightning: How often to log within steps. Default: 50.\n        log_level (int):\n            log level with the following settings:\n            `NOTSET` (`0`),\n            `DEBUG` (`10`: Detailed information, typically of interest only when diagnosing problems.),\n            `INFO` (`20`: Confirmation that things are working as expected.),\n            `WARNING` (`30`: An indication that something unexpected happened, or indicative of some problem in the near\n                future (e.g. \u2018disk space low\u2019). The software is still working as expected.),\n            `ERROR` (`40`: Due to a more serious problem, the software has not been able to perform some function.), and\n            `CRITICAL` (`50`: A serious error, indicating that the program itself may be unable to continue running.)\n        lower (np.array):\n            lower bound\n        max_time (int):\n            The maximum time in minutes.\n        max_surrogate_points (int):\n            The maximum number of points in the surrogate model. Default is inf.\n        metric_sklearn (object):\n            The metric object from the scikit-learn library. Default is None.\n        metric_sklearn_name (str):\n            The name of the metric object from the scikit-learn library. Default is None.\n        noise (bool):\n            Whether the objective function is noiy or not. Default is False.\n            Affects the repeat of the function evaluations.\n        n_points (int):\n            The number of infill points to be generated by the surrogate in each iteration.\n        num_sanity_val_steps (int):\n                Lightning: Sanity check runs n validation batches before starting the training routine.\n                Set it to -1 to run all batches in all validation dataloaders.\n                Default: 2.\n        n_samples (int):\n            The number of samples in the dataset. Default is None.\n        n_total (int):\n            The total number of samples in the dataset. Default is None.\n        num_nodes (int):\n            The number of GPU nodes to use for the training/validation/testing. Default is 1.\n        num_workers (int):\n            The number of workers to use for the data loading. Default is 0.\n        ocba_delta (int):\n            The number of additional, new points (only used if noise==True) generated by\n            the OCBA infill criterion. Default is 0.\n        oml_grace_period (int):\n            The grace period for the OML algorithm. Default is None.\n        optimizer (object):\n            The optimizer object used for the search on surrogate. Default is None.\n        penalty_NA (float):\n            The penalty for NA values. Default is None. If None, the values are ignored, e.g., the\n            initial design size used for the surrogate is reduced by the number of NA values.\n        precision (str):\n            The precision of the data. Default is \"32\". Can be e.g., \"16-mixed\" or \"16-true\".\n        PREFIX (str):\n            The prefix of the experiment name. If the PREFIX is not None, a spotWriter\n            that us an instance of a SummaryWriter(), is created. Default is \"00\".\n        prep_model (object):\n            The preprocessing model object. Used for river. Default is None.\n        prep_model_name (str):\n            The name of the preprocessing model. Default is None.\n        progress_file (str):\n            The name of the progress file. Default is None.\n        save_experiment (bool):\n            Whether to save the experiment before the run is started or not. Default is False.\n        save_result (bool):\n            Whether to save the result after the experiment is done or not. Default is False.\n        scaler (object):\n            The scaler object, e.g., the TorchStandard scaler from spot.utils.scaler.py.\n            Default is None.\n        scaler_name (str):\n            The name of the scaler object. Default is None.\n        scenario (str):\n            The scenario to use. Default is None. Can be \"river\", \"sklearn\", or \"lightning\".\n        seed (int):\n            The seed to use for the random number generator. Default is 123.\n        sigma (float):\n            The standard deviation of the noise of the objective function.\n        show_progress (bool):\n            Whether to show the progress or not. Default is `True`.\n        show_models (bool):\n            Plot model each generation.\n            Currently only 1-dim functions are supported. Default is `False`.\n        show_config (bool):\n            Whether to show the configuration or not. Default is `False`.\n        shuffle (bool):\n            Whether the data were shuffled or not. Default is None.\n        shuffle_train (bool):\n            Whether the training data were shuffled or not. Default is True.\n        shuffle_val (bool):\n            Whether the validation data were shuffled or not. Default is False.\n        shuffle_test (bool):\n            Whether the test data were shuffled or not. Default is False.\n        surrogate (object):\n            The surrogate model object. Default is None.\n        strategy (str):\n            The strategy to use. Default is \"auto\".\n        target_column (str):\n            The name of the target column. Default is None.\n        target_type (str):\n            The type of the target column. Default is None.\n        task (str):\n            The task to perform. It can be either \"classification\" or \"regression\".\n            Default is None.\n        TENSORBOARD_CLEAN (bool):\n            Whether to clean (delete) the tensorboard folder or not. Default is False.\n        tensorboard_log (bool):\n            Whether to log the tensorboard or not. Starts the SummaryWriter.\n            Default is False.\n        tensorboard_start (bool):\n            Whether to start the tensorboard or not. Default is False.\n        tensorboard_stop (bool):\n            Whether to stop the tensorboard or not. Default is False.\n        test (object):\n            The test data set for spotriver. Default is None.\n        test_seed (int):\n            The seed to use for the test set. Default is 1234.\n        test_size (float):\n            The size of the test set. Default is 0.4, i.e.,\n            60% of the data is used for training and 40% for testing.\n        tkagg (bool):\n            Whether to use matplotlib TkAgg or not. Default is False.\n        tolerance_x (float):\n            tolerance for new x solutions. Minimum distance of new solutions,\n            generated by `suggest_new_X`, to already existing solutions.\n            If zero (which is the default), every new solution is accepted.\n        train (object):\n            The training data set for spotriver. Default is None.\n        upper (np.array):\n            upper bound\n        var_name (list):\n            A list containing the name of the variables, e.g., [\"x1\", \"x2\"]. Default is None.\n        var_type (List[str]):\n            list of type information, can be either \"int\", \"num\" or \"factor\".\n            Default is [\"num\"].\n        verbosity (int):\n            The verbosity level. Determines print output to console. Higher values\n            result in more output. Default is 0.\n        weights (float):\n            The weight coefficient of the objective function. Positive values mean minimization.\n            If set to -1, scores that are better when maximized will be minimized, e.g, accuracy.\n            Can be an array, so that different weights can be used for different (multiple) objectives.\n            Default is 1.0.\n        weight_coeff (float):\n            Determines how to weight older measures. Default is 1.0. Used in the OML algorithm eval_oml.py.\n            Default is 0.0.\n        weights_entry (str):\n            The weights entry used in the GUI. Default is None.\n\n    Returns:\n        fun_control (dict):\n            A dictionary containing the information about the core model,\n            loss function, metrics, and the hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import fun_control_init\n            fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n            fun_control\n            {'CHECKPOINT_PATH': 'saved_models/',\n                'DATASET_PATH': 'data/',\n                'RESULTS_PATH': 'results/',\n                'TENSORBOARD_PATH': 'runs/',\n                '_L_in': 64,\n                '_L_out': 11,\n                '_L_cond': None,\n                'accelerator': \"auto\",\n                'core_model': None,\n                'core_model_name': None,\n                'data': None,\n                'data_dir': './data',\n                'db_dict_name': None,\n                'device': None,\n                'devices': \"auto\",\n                'enable_progress_bar': False,\n                'eval': None,\n                'horizon': 7,\n                'infill_criterion': 'y',\n                'k_folds': None,\n                'loss_function': None,\n                'lower': None,\n                'max_surrogate_points': 100,\n                'metric_river': None,\n                'metric_sklearn': None,\n                'metric_sklearn_name': None,\n                'metric_torch': None,\n                'metric_params': {},\n                'model_dict': {},\n                'noise': False,\n                'n_points': 1,\n                'n_samples': None,\n                'num_workers': 0,\n                'ocba_delta': 0,\n                'oml_grace_period': None,\n                'optimizer': None,\n                'path': None,\n                'prep_model': None,\n                'prep_model_name': None,\n                'save_model': False,\n                'scenario': \"lightning\",\n                'seed': 1234,\n                'show_batch_interval': 1000000,\n                'shuffle': None,\n                'sigma': 0.0,\n                'target_column': None,\n                'target_type': None,\n                'train': None,\n                'test': None,\n                'task': 'classification',\n                'tensorboard_path': None,\n                'upper': None,\n                'weights': 1.0,\n                'writer': None}\n    \"\"\"\n    # Setting the seed\n    L.seed_everything(seed)\n\n    if PREFIX is None:\n        PREFIX = _init_prefix()\n\n    CHECKPOINT_PATH, DATASET_PATH, RESULTS_PATH, TENSORBOARD_PATH = setup_paths(TENSORBOARD_CLEAN)\n    spot_tensorboard_path = create_spot_tensorboard_path(tensorboard_log, PREFIX)\n\n    if metric_sklearn is None and metric_sklearn_name is not None:\n        metric_sklearn = get_metric_sklearn(metric_sklearn_name)\n\n    fun_control = {\n        \"PREFIX\": PREFIX,\n        \"CHECKPOINT_PATH\": CHECKPOINT_PATH,\n        \"DATASET_PATH\": DATASET_PATH,\n        \"RESULTS_PATH\": RESULTS_PATH,\n        \"TENSORBOARD_PATH\": TENSORBOARD_PATH,\n        \"TENSORBOARD_CLEAN\": TENSORBOARD_CLEAN,\n        \"_L_in\": _L_in,\n        \"_L_out\": _L_out,\n        \"_L_cond\": _L_cond,\n        \"_torchmetric\": _torchmetric,\n        \"accelerator\": accelerator,\n        \"collate_fn_name\": collate_fn_name,\n        \"converters\": converters,\n        \"core_model\": core_model,\n        \"core_model_name\": core_model_name,\n        \"counter\": 0,\n        \"data\": data,\n        \"data_dir\": data_dir,\n        \"data_full_train\": data_full_train,\n        \"hacky\": hacky,\n        \"data_module\": data_module,\n        \"data_set\": data_set,\n        \"data_set_name\": data_set_name,\n        \"data_test\": data_test,\n        \"data_val\": data_val,\n        \"db_dict_name\": db_dict_name,\n        \"design\": design,\n        \"device\": device,\n        \"devices\": devices,\n        \"enable_progress_bar\": enable_progress_bar,\n        \"eval\": eval,\n        \"force_run\": force_run,\n        \"fun_evals\": fun_evals,\n        \"fun_mo2so\": fun_mo2so,\n        \"fun_repeats\": fun_repeats,\n        \"horizon\": horizon,\n        \"hyperdict\": hyperdict,\n        \"infill_criterion\": infill_criterion,\n        \"k_folds\": 3,\n        \"log_every_n_steps\": log_every_n_steps,\n        \"log_graph\": False,\n        \"log_level\": log_level,\n        \"loss_function\": None,\n        \"lower\": lower,\n        \"max_time\": max_time,\n        \"max_surrogate_points\": max_surrogate_points,\n        \"metric_river\": None,\n        \"metric_sklearn\": metric_sklearn,\n        \"metric_sklearn_name\": metric_sklearn_name,\n        \"metric_torch\": None,\n        \"metric_params\": {},\n        \"model_dict\": {},\n        \"noise\": noise,\n        \"n_points\": n_points,\n        \"n_samples\": n_samples,\n        \"n_total\": n_total,\n        \"num_nodes\": num_nodes,\n        \"num_sanity_val_steps\": num_sanity_val_steps,\n        \"num_workers\": num_workers,\n        \"ocba_delta\": ocba_delta,\n        \"oml_grace_period\": oml_grace_period,\n        \"optimizer\": optimizer,\n        \"path\": None,\n        \"penalty_NA\": penalty_NA,\n        \"precision\": precision,\n        \"prep_model\": prep_model,\n        \"prep_model_name\": prep_model_name,\n        \"progress_file\": progress_file,\n        \"save_experiment\": save_experiment,\n        \"save_result\": save_result,\n        \"save_model\": False,\n        \"scaler\": scaler,\n        \"scaler_name\": scaler_name,\n        \"scenario\": scenario,\n        \"seed\": seed,\n        \"show_batch_interval\": 1_000_000,\n        \"show_config\": show_config,\n        \"show_models\": show_models,\n        \"show_progress\": show_progress,\n        \"shuffle\": shuffle,\n        \"shuffle_train\": shuffle_train,\n        \"shuffle_val\": shuffle_val,\n        \"shuffle_test\": shuffle_test,\n        \"sigma\": sigma,\n        \"spot_tensorboard_path\": spot_tensorboard_path,\n        \"strategy\": strategy,\n        \"target_column\": target_column,\n        \"target_type\": target_type,\n        \"task\": task,\n        \"tensorboard_log\": tensorboard_log,\n        \"tensorboard_start\": tensorboard_start,\n        \"tensorboard_stop\": tensorboard_stop,\n        \"test\": test,\n        \"test_seed\": test_seed,\n        \"test_size\": test_size,\n        \"tkagg\": tkagg,\n        \"tolerance_x\": tolerance_x,\n        \"train\": train,\n        \"upper\": upper,\n        \"var_name\": var_name,\n        \"var_type\": var_type,\n        \"verbosity\": verbosity,\n        \"weights\": weights,\n        \"weight_coeff\": weight_coeff,\n        \"weights_entry\": weights_entry,\n    }\n    if hyperdict is not None and core_model_name is not None:\n        # check if hyperdict implements the methods get_scenario:\n        if hasattr(hyperdict, \"get_scenario\"):\n            scenario = hyperdict().get_scenario()\n        else:\n            scenario = None\n        if fun_control[\"hyperdict\"].__name__ == RiverHyperDict.__name__ or scenario == \"river\":\n            coremodel, core_model_instance = get_river_core_model_from_name(core_model_name)\n            if prep_model is None and prep_model_name is not None:\n                prep_model = get_river_prep_model(prep_model_name)\n        else:\n            coremodel, core_model_instance = get_core_model_from_name(core_model_name)\n            if prep_model is None and prep_model_name is not None:\n                prep_model = get_prep_model(prep_model_name)\n        fun_control.update({\"prep_model\": prep_model})\n        add_core_model_to_fun_control(\n            core_model=core_model_instance,\n            fun_control=fun_control,\n            hyper_dict=hyperdict,\n            filename=None,\n        )\n    if hyperdict is not None and core_model is not None:\n        add_core_model_to_fun_control(\n            core_model=core_model,\n            fun_control=fun_control,\n            hyper_dict=hyperdict,\n            filename=None,\n        )\n    return fun_control\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.get_experiment_name","title":"<code>get_experiment_name(prefix='00')</code>","text":"<p>Returns a unique experiment name with a given prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Prefix for the experiment name. Defaults to \u201c00\u201d.</p> <code>'00'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique experiment name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import get_experiment_name\n&gt;&gt;&gt; get_experiment_name(prefix=\"00\")\n00_ubuntu_2021-08-31_14-30-00\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def get_experiment_name(prefix: str = \"00\") -&gt; str:\n    \"\"\"Returns a unique experiment name with a given prefix.\n\n    Args:\n        prefix (str, optional): Prefix for the experiment name. Defaults to \"00\".\n\n    Returns:\n        str: Unique experiment name.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import get_experiment_name\n        &gt;&gt;&gt; get_experiment_name(prefix=\"00\")\n        00_ubuntu_2021-08-31_14-30-00\n    \"\"\"\n    start_time = datetime.datetime.now(tzlocal())\n    HOSTNAME = socket.gethostname().split(\".\")[0]\n    experiment_name = prefix + \"_\" + HOSTNAME + \"_\" + str(start_time).split(\".\", 1)[0].replace(\" \", \"_\")\n    experiment_name = experiment_name.replace(\":\", \"-\")\n    return experiment_name\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.get_feature_names","title":"<code>get_feature_names(fun_control)</code>","text":"<p>Get the feature names from the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The function control dictionary. Must contain a \u201cdata_set\u201d key.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of feature names.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If \u201cdata_set\u201d is not in fun_control.</p> <code>ValueError</code> <p>If \u201cdata_set\u201d is None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import get_feature_names\n    get_feature_names(fun_control)\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def get_feature_names(fun_control: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"\n    Get the feature names from the fun_control dictionary.\n\n    Args:\n        fun_control (dict): The function control dictionary. Must contain a \"data_set\" key.\n\n    Returns:\n        List[str]: List of feature names.\n\n    Raises:\n        ValueError: If \"data_set\" is not in fun_control.\n        ValueError: If \"data_set\" is None.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import get_feature_names\n            get_feature_names(fun_control)\n    \"\"\"\n    data_set = fun_control.get(\"data_set\")\n\n    if data_set is None:\n        raise ValueError(\"'data_set' key not found or is None in 'fun_control'\")\n\n    return data_set.names\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.get_spot_tensorboard_path","title":"<code>get_spot_tensorboard_path(experiment_name)</code>","text":"<p>Get the path to the spot tensorboard files.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>The name of the experiment.</p> required <p>Returns:</p> Name Type Description <code>spot_tensorboard_path</code> <code>str</code> <p>The path to the folder where the spot tensorboard files are saved.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import get_spot_tensorboard_path\n&gt;&gt;&gt; get_spot_tensorboard_path(\"00_ubuntu_2021-08-31_14-30-00\")\nruns/spot_logs/00_ubuntu_2021-08-31_14-30-00\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def get_spot_tensorboard_path(experiment_name) -&gt; str:\n    \"\"\"Get the path to the spot tensorboard files.\n\n    Args:\n        experiment_name (str): The name of the experiment.\n\n    Returns:\n        spot_tensorboard_path (str): The path to the folder where the spot tensorboard files are saved.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import get_spot_tensorboard_path\n        &gt;&gt;&gt; get_spot_tensorboard_path(\"00_ubuntu_2021-08-31_14-30-00\")\n        runs/spot_logs/00_ubuntu_2021-08-31_14-30-00\n\n    \"\"\"\n    spot_tensorboard_path = os.environ.get(\"PATH_TENSORBOARD\", \"runs/spot_logs/\")\n    spot_tensorboard_path = os.path.join(spot_tensorboard_path, experiment_name)\n    return spot_tensorboard_path\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.get_tensorboard_path","title":"<code>get_tensorboard_path(fun_control)</code>","text":"<p>Get the path to the tensorboard files.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> required <p>Returns:</p> Name Type Description <code>tensorboard_path</code> <code>str</code> <p>The path to the folder where the tensorboard files are saved.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import get_tensorboard_path\n&gt;&gt;&gt; get_tensorboard_path(fun_control)\nruns/\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def get_tensorboard_path(fun_control) -&gt; str:\n    \"\"\"Get the path to the tensorboard files.\n\n    Args:\n        fun_control (dict): The function control dictionary.\n\n    Returns:\n        tensorboard_path (str): The path to the folder where the tensorboard files are saved.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import get_tensorboard_path\n        &gt;&gt;&gt; get_tensorboard_path(fun_control)\n        runs/\n    \"\"\"\n    return fun_control[\"TENSORBOARD_PATH\"]\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.optimizer_control_init","title":"<code>optimizer_control_init(max_iter=1000, seed=125)</code>","text":"<p>Initialize optimizer_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>max_iter</code> <code>int</code> <p>The maximum number of iterations. This will be used for the optimization of the surrogate model. Default is 1000.</p> <code>1000</code> <code>seed</code> <code>int</code> <p>The seed to use for the random number generator. Default is 125.</p> <code>125</code> Notes <ul> <li>Differential evaluation uses <code>maxiter = 1000</code> and sets the number of function evaluations to   (maxiter + 1) * popsize * N, which results in 1000 * 15 * k,   because the default popsize is 15 and N is the number of parameters. This is already sufficient   for many situations. For example, for k=2 these are 30 000 iterations.   Therefore we set this value to 1000.</li> <li>This value will be passed to the surrogate model in the <code>Spot</code> class.</li> </ul> <p>Returns:</p> Name Type Description <code>optimizer_control</code> <code>dict</code> <p>A dictionary containing the information about the optimizer.</p> Source code in <code>spotpython/utils/init.py</code> <pre><code>def optimizer_control_init(\n    max_iter=1000,\n    seed=125,\n) -&gt; dict:\n    \"\"\"Initialize optimizer_control dictionary.\n\n    Args:\n        max_iter (int):\n            The maximum number of iterations. This will be used for the\n            optimization of the surrogate model. Default is 1000.\n        seed (int):\n            The seed to use for the random number generator.\n            Default is 125.\n\n    Notes:\n        * Differential evaluation uses `maxiter = 1000` and sets the number of function evaluations to\n          (maxiter + 1) * popsize * N, which results in 1000 * 15 * k,\n          because the default popsize is 15 and N is the number of parameters. This is already sufficient\n          for many situations. For example, for k=2 these are 30 000 iterations.\n          Therefore we set this value to 1000.\n        * This value will be passed to the surrogate model in the `Spot` class.\n\n    Returns:\n        optimizer_control (dict):\n            A dictionary containing the information about the optimizer.\n\n    \"\"\"\n    optimizer_control = {\"max_iter\": max_iter, \"seed\": seed}\n    return optimizer_control\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.setup_paths","title":"<code>setup_paths(tensorboard_clean)</code>","text":"<p>Setup paths for checkpoints, datasets, results, and tensorboard files. This function also handles cleaning the tensorboard path if specified.</p> <p>Parameters:</p> Name Type Description Default <code>tensorboard_clean</code> <code>bool</code> <p>If True, move the existing tensorboard folder to a timestamped backup folder to avoid overwriting old tensorboard files.</p> required <p>Returns:</p> Name Type Description <code>CHECKPOINT_PATH</code> <code>str</code> <p>The path to the folder where the pretrained models are saved.</p> <code>DATASET_PATH</code> <code>str</code> <p>The path to the folder where the datasets are/should be downloaded.</p> <code>RESULTS_PATH</code> <code>str</code> <p>The path to the folder where the results (plots, csv, etc.) are saved.</p> <code>TENSORBOARD_PATH</code> <code>str</code> <p>The path to the folder where the tensorboard files are saved.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.init import setup_paths\n&gt;&gt;&gt; setup_paths(tensorboard_clean=True)\n('runs/saved_models/', 'data/', 'results/', 'runs/')\n</code></pre> Source code in <code>spotpython/utils/init.py</code> <pre><code>def setup_paths(tensorboard_clean) -&gt; tuple:\n    \"\"\"\n    Setup paths for checkpoints, datasets, results, and tensorboard files.\n    This function also handles cleaning the tensorboard path if specified.\n\n    Args:\n        tensorboard_clean (bool):\n            If True, move the existing tensorboard folder to a timestamped backup\n            folder to avoid overwriting old tensorboard files.\n\n    Returns:\n        CHECKPOINT_PATH (str):\n            The path to the folder where the pretrained models are saved.\n        DATASET_PATH (str):\n            The path to the folder where the datasets are/should be downloaded.\n        RESULTS_PATH (str):\n            The path to the folder where the results (plots, csv, etc.) are saved.\n        TENSORBOARD_PATH (str):\n            The path to the folder where the tensorboard files are saved.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.init import setup_paths\n        &gt;&gt;&gt; setup_paths(tensorboard_clean=True)\n        ('runs/saved_models/', 'data/', 'results/', 'runs/')\n\n    \"\"\"\n    # Path to the folder where the pretrained models are saved\n    CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"runs/saved_models/\")\n    os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n\n    # Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n    DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n    os.makedirs(DATASET_PATH, exist_ok=True)\n\n    # Path to the folder where the results (plots, csv, etc.) are saved\n    RESULTS_PATH = os.environ.get(\"PATH_RESULTS\", \"results/\")\n    os.makedirs(RESULTS_PATH, exist_ok=True)\n\n    # Path to the folder where the tensorboard files are saved\n    TENSORBOARD_PATH = os.environ.get(\"PATH_TENSORBOARD\", \"runs/\")\n    if tensorboard_clean:\n        # if the folder \"runs\" exists, move it to \"runs_Y_M_D_H_M_S\" to avoid overwriting old tensorboard files\n        if os.path.exists(TENSORBOARD_PATH):\n            now = datetime.datetime.now()\n            os.makedirs(\"runs_OLD\", exist_ok=True)\n            # use [:-1] to remove \"/\" from the end of the path\n            TENSORBOARD_PATH_OLD = \"runs_OLD/\" + TENSORBOARD_PATH[:-1] + \"_\" + now.strftime(\"%Y_%m_%d_%H_%M_%S\") + \"_\" + \"0\"\n            print(f\"Moving TENSORBOARD_PATH: {TENSORBOARD_PATH} to TENSORBOARD_PATH_OLD: {TENSORBOARD_PATH_OLD}\")\n            # if TENSORBOARD_PATH_OLD already exists, change the name increasing the number at the end\n            while os.path.exists(TENSORBOARD_PATH_OLD):\n                TENSORBOARD_PATH_OLD = copy.deepcopy(TENSORBOARD_PATH_OLD[:-1] + str(int(TENSORBOARD_PATH_OLD[-1]) + 1))\n            os.rename(TENSORBOARD_PATH[:-1], TENSORBOARD_PATH_OLD)\n\n    os.makedirs(TENSORBOARD_PATH, exist_ok=True)\n\n    # Ensure the figures folder exists\n    if not os.path.exists(\"./figures\"):\n        os.makedirs(\"./figures\")\n    return CHECKPOINT_PATH, DATASET_PATH, RESULTS_PATH, TENSORBOARD_PATH\n</code></pre>"},{"location":"reference/spotpython/utils/init/#spotpython.utils.init.surrogate_control_init","title":"<code>surrogate_control_init(log_level=50, method='regression', model_optimizer=differential_evolution, model_fun_evals=10000, min_theta=-3.0, max_theta=2.0, n_theta='anisotropic', p_val=2.0, n_p=1, optim_p=False, min_Lambda=1e-09, max_Lambda=1, seed=124, theta_init_zero=False, var_type=None, metric_factorial='canberra')</code>","text":"<p>Initialize surrogate_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>model_optimizer</code> <code>object</code> <p>The optimizer object used for the search on surrogate. Default is differential_evolution.</p> <code>differential_evolution</code> <code>model_fun_evals</code> <code>int</code> <p>The number of function evaluations. This will be used for the optimization of the surrogate model. Default is 1000.</p> <code>10000</code> <code>min_theta</code> <code>float</code> <p>The minimum value of theta. Note that the base10-logarithm is used.  Default is -3.</p> <code>-3.0</code> <code>max_theta</code> <code>float</code> <p>The maximum value of theta. Note that the base10-logarithm is used. Default is 3.</p> <code>2.0</code> <code>method</code> <code>str</code> <p>The method to be used for the surrogate model. Default is \u201cregression\u201d. Can be one of [\u201cregression\u201d, \u201cinterpolation\u201d, \u201creinterpolation\u201d]. Note: Will also be set in the Spot class, if None.</p> <code>'regression'</code> <code>n_theta</code> <code>int</code> <p>The number of theta values. If larger than 1 or set to the string \u201canisotropic\u201d, then the k theta values are used, where k is the problem dimension. This is handled in spot.py. Default is \u201canisotropic\u201d.</p> <code>'anisotropic'</code> <code>p_val</code> <code>float</code> <pre><code>p value. Used as an initial value if optim_p = True. Otherwise as a constant. Defaults to 2.0.\n</code></pre> <code>2.0</code> <code>n_p</code> <code>int</code> <p>The number of p values. Number of p values to be used. Default is 1.</p> <code>1</code> <code>optim_p</code> <code>bool</code> <p>Whether to optimize p or not.</p> <code>False</code> <code>min_Lambda</code> <code>float</code> <p>The minimum value of lambda. Default is 1e-9.</p> <code>1e-09</code> <code>max_Lambda</code> <code>float</code> <p>The maximum value of lambda. Default is 1.</p> <code>1</code> <code>seed</code> <code>int</code> <p>The seed to use for the random number generator.</p> <code>124</code> <code>theta_init_zero</code> <code>bool</code> <p>Whether to initialize theta with zero or not. If False, theta is set to n/(100 * k). Default is False.</p> <code>False</code> <code>var_type</code> <code>list</code> <p>A list containing the type of the variables. Default is None. Note: Will be set in the Spot class.</p> <code>None</code> <code>metric_factorial</code> <code>str</code> <p>The metric to be used for the factorial design. Default is \u201ccanberra\u201d.</p> <code>'canberra'</code> <p>Returns:</p> Name Type Description <code>surrogate_control</code> <code>dict</code> <p>A dictionary containing the information about the surrogate model.</p> Note <ul> <li>The surrogate_control dictionary is used in the Spot class. The following values   are updated in the Spot class if they are None in the surrogate_control dictionary:<ul> <li><code>method</code>: If the surrogate model dictionary is passed to the Spot class,   and the <code>method</code> value is <code>None</code>, then the method value is set in the   Spot class based on the value of <code>method</code> in the Spot class fun_control dictionary.</li> <li><code>var_type</code>: The <code>var_type</code> value is set in the Spot class based on the value    of <code>var_type</code> in the Spot class fun_control dictionary and the dimension of the problem.    If the Kriging model is used as a surrogate in the Spot class, the setting from     surrogate_control_init() is overwritten.</li> <li><code>n_theta</code>: If self.surrogate_control[\u201cn_theta\u201d] &gt; 1,    use k theta values, where k is the problem dimension specified in the Spot class.    The problem dimension is set in the Spot class based on the    length of the lower bounds.</li> </ul> </li> <li>This value <code>model_fun_evals</code> will used for the optimization of the surrogate model, e.g., theta values.   Differential evaluation uses <code>maxiter = 1000</code> and sets the number of function evaluations to   (maxiter + 1) * popsize * N, which results in 1000 * 15 * k,   because the default popsize is 15 and N is the number of parameters. This is already sufficient   for many situations. For example, for k=2 these are 30 000 iterations.   Therefore we set this value to 1000.</li> </ul> Source code in <code>spotpython/utils/init.py</code> <pre><code>def surrogate_control_init(\n    log_level: int = 50,\n    method=\"regression\",\n    model_optimizer=differential_evolution,\n    model_fun_evals=10000,\n    min_theta=-3.0,\n    max_theta=2.0,\n    n_theta=\"anisotropic\",\n    p_val=2.0,\n    n_p=1,\n    optim_p=False,\n    min_Lambda=1e-9,\n    max_Lambda=1,\n    seed=124,\n    theta_init_zero=False,\n    var_type=None,\n    metric_factorial=\"canberra\",\n) -&gt; dict:\n    \"\"\"Initialize surrogate_control dictionary.\n\n    Args:\n        model_optimizer (object):\n            The optimizer object used for the search on surrogate.\n            Default is differential_evolution.\n        model_fun_evals (int):\n            The number of function evaluations. This will be used for the\n            optimization of the surrogate model. Default is 1000.\n        min_theta (float):\n            The minimum value of theta. Note that the base10-logarithm is used.\n             Default is -3.\n        max_theta (float): The maximum value of theta. Note that the base10-logarithm is used.\n            Default is 3.\n        method (str):\n            The method to be used for the surrogate model. Default is \"regression\".\n            Can be one of [\"regression\", \"interpolation\", \"reinterpolation\"].\n            Note: Will also be set in the Spot class, if None.\n        n_theta (int):\n            The number of theta values. If larger than 1 or set to the string \"anisotropic\",\n            then the k theta values are used, where k is the problem dimension.\n            This is handled in spot.py. Default is \"anisotropic\".\n        p_val (float):\n                p value. Used as an initial value if optim_p = True. Otherwise as a constant. Defaults to 2.0.\n        n_p (int):\n            The number of p values. Number of p values to be used. Default is 1.\n        optim_p (bool):\n            Whether to optimize p or not.\n        min_Lambda (float):\n            The minimum value of lambda. Default is 1e-9.\n        max_Lambda (float):\n            The maximum value of lambda. Default is 1.\n        seed (int):\n            The seed to use for the random number generator.\n        theta_init_zero (bool):\n            Whether to initialize theta with zero or not. If False, theta is\n            set to n/(100 * k). Default is False.\n        var_type (list):\n            A list containing the type of the variables. Default is None.\n            Note: Will be set in the Spot class.\n        metric_factorial (str):\n            The metric to be used for the factorial design. Default is \"canberra\".\n\n    Returns:\n        surrogate_control (dict):\n            A dictionary containing the information about the surrogate model.\n\n    Note:\n        * The surrogate_control dictionary is used in the Spot class. The following values\n          are updated in the Spot class if they are None in the surrogate_control dictionary:\n            * `method`: If the surrogate model dictionary is passed to the Spot class,\n              and the `method` value is `None`, then the method value is set in the\n              Spot class based on the value of `method` in the Spot class fun_control dictionary.\n            * `var_type`: The `var_type` value is set in the Spot class based on the value\n               of `var_type` in the Spot class fun_control dictionary and the dimension of the problem.\n               If the Kriging model is used as a surrogate in the Spot class, the setting from\n                surrogate_control_init() is overwritten.\n            * `n_theta`: If self.surrogate_control[\"n_theta\"] &gt; 1,\n               use k theta values, where k is the problem dimension specified in the Spot class.\n               The problem dimension is set in the Spot class based on the\n               length of the lower bounds.\n        * This value `model_fun_evals` will used for the optimization of the surrogate model, e.g., theta values.\n          Differential evaluation uses `maxiter = 1000` and sets the number of function evaluations to\n          (maxiter + 1) * popsize * N, which results in 1000 * 15 * k,\n          because the default popsize is 15 and N is the number of parameters. This is already sufficient\n          for many situations. For example, for k=2 these are 30 000 iterations.\n          Therefore we set this value to 1000.\n\n    \"\"\"\n    surrogate_control = {\n        \"log_level\": log_level,\n        \"method\": method,\n        \"model_optimizer\": model_optimizer,\n        \"model_fun_evals\": model_fun_evals,\n        \"min_theta\": min_theta,\n        \"max_theta\": max_theta,\n        \"n_theta\": n_theta,\n        \"p_val\": p_val,\n        \"n_p\": n_p,\n        \"optim_p\": optim_p,\n        \"min_Lambda\": min_Lambda,\n        \"max_Lambda\": max_Lambda,\n        \"seed\": seed,\n        \"theta_init_zero\": theta_init_zero,\n        \"var_type\": var_type,\n        \"metric_factorial\": metric_factorial,\n    }\n    return surrogate_control\n</code></pre>"},{"location":"reference/spotpython/utils/linalg/","title":"linalg","text":""},{"location":"reference/spotpython/utils/linalg/#spotpython.utils.linalg.matrix_inversion_dispatcher","title":"<code>matrix_inversion_dispatcher(K, method='inv')</code>","text":"<p>Returns the inverse of K using one of three methods: \u2018inv\u2019 -&gt; direct numpy.linalg.inv(K), \u2018chol\u2019 -&gt; Cholesky factorization (then forms K^-1), \u2018direct\u2019 -&gt; Cholesky factorization with repeated solves (still forms K^-1).</p> <p>Parameters:</p> Name Type Description Default <code>K</code> <code>ndarray</code> <p>The matrix to invert.</p> required <code>method</code> <code>str</code> <p>The inversion method to use.</p> <code>'inv'</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The inverse of K.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If method is not \u2018inv\u2019, \u2018chol\u2019, or \u2018direct\u2019.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.linalg import matrix_inversion_dispatcher\n&gt;&gt;&gt; K = np.array([[1.0, 0.5], [0.5, 1.0]])\n&gt;&gt;&gt; Ki = matrix_inversion_dispatcher(K, method=\"inv\")\n&gt;&gt;&gt; print(Ki)\n[[ 1.33333333 -0.66666667]\n [-0.66666667  1.33333333]]\n</code></pre> Source code in <code>spotpython/utils/linalg.py</code> <pre><code>def matrix_inversion_dispatcher(K: np.ndarray, method: str = \"inv\") -&gt; np.ndarray:\n    \"\"\"\n    Returns the inverse of K using one of three methods:\n    'inv' -&gt; direct numpy.linalg.inv(K),\n    'chol' -&gt; Cholesky factorization (then forms K^-1),\n    'direct' -&gt; Cholesky factorization with repeated solves (still forms K^-1).\n\n    Args:\n        K (ndarray): The matrix to invert.\n        method (str): The inversion method to use.\n\n    Returns:\n        ndarray: The inverse of K.\n\n    Raises:\n        ValueError: If method is not 'inv', 'chol', or 'direct'.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.linalg import matrix_inversion_dispatcher\n        &gt;&gt;&gt; K = np.array([[1.0, 0.5], [0.5, 1.0]])\n        &gt;&gt;&gt; Ki = matrix_inversion_dispatcher(K, method=\"inv\")\n        &gt;&gt;&gt; print(Ki)\n        [[ 1.33333333 -0.66666667]\n         [-0.66666667  1.33333333]]\n    \"\"\"\n    n = K.shape[0]\n    if method == \"inv\":\n        return inv(K)\n\n    L, success = try_cholesky(K.copy())\n    if not success:\n        # If Cholesky fails repeatedly, we can still do a naive inverse as fallback\n        return inv(K)\n\n    if method == \"chol\":\n        # Build K^-1 from L\n        Id = np.eye(n)\n        tmp = linalg.solve_triangular(L, Id, lower=True)\n        Ki = linalg.solve_triangular(L.T, tmp, lower=False)\n        return Ki\n\n    elif method == \"direct\":\n        # Build K^-1 by repeated solves on identity columns\n        Ki = np.zeros((n, n))\n        for i in range(n):\n            e_i = np.zeros(n)\n            e_i[i] = 1.0\n            # Solve K * x = e_i using the Cholesky factor\n            tmp = linalg.solve_triangular(L, e_i, lower=True)\n            x = linalg.solve_triangular(L.T, tmp, lower=False)\n            Ki[:, i] = x\n        return Ki\n\n    else:\n        raise ValueError(\"method must be 'inv', 'chol', or 'direct'.\")\n</code></pre>"},{"location":"reference/spotpython/utils/linalg/#spotpython.utils.linalg.try_cholesky","title":"<code>try_cholesky(Kmat, max_attempts=3)</code>","text":"<p>Attempt Cholesky on Kmat multiple times, adding jitter at each step.</p> Source code in <code>spotpython/utils/linalg.py</code> <pre><code>def try_cholesky(Kmat, max_attempts=3):\n    \"\"\"Attempt Cholesky on Kmat multiple times, adding jitter at each step.\"\"\"\n    jitter_scale = 1e-8\n    for _attempt in range(max_attempts):\n        try:\n            L_ = linalg.cholesky(Kmat, lower=True)\n            return L_, True\n        except linalg.LinAlgError:\n            jitter = jitter_scale * np.trace(Kmat) / len(Kmat)\n            Kmat += np.eye(len(Kmat)) * jitter\n            jitter_scale *= 10.0\n    return None, False\n</code></pre>"},{"location":"reference/spotpython/utils/math/","title":"math","text":""},{"location":"reference/spotpython/utils/math/#spotpython.utils.math.generate_div2_list","title":"<code>generate_div2_list(n, n_min)</code>","text":"<p>Generate a list of numbers from n to n_min (inclusive) by dividing n by 2 until the result is less than n_min. This function starts with n and keeps dividing it by 2 until n_min is reached. The number of times each value is added to the list is determined by n // current.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number to start with.</p> required <code>n_min</code> <code>int</code> <p>The minimum number to stop at.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of numbers from n to n_min (inclusive).</p> <p>Examples:</p> <p>from spotpython.utils.math import generate_div2_list generate_div2_list(10, 1) [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] generate_div2_list(10, 2) [10, 5, 5, 2, 2, 2, 2, 2]</p> Source code in <code>spotpython/utils/math.py</code> <pre><code>def generate_div2_list(n, n_min) -&gt; list:\n    \"\"\"\n    Generate a list of numbers from n to n_min (inclusive) by dividing n by 2\n    until the result is less than n_min.\n    This function starts with n and keeps dividing it by 2 until n_min is reached.\n    The number of times each value is added to the list is determined by n // current.\n\n    Args:\n        n (int): The number to start with.\n        n_min (int): The minimum number to stop at.\n\n    Returns:\n        list: A list of numbers from n to n_min (inclusive).\n\n    Examples:\n        from spotpython.utils.math import generate_div2_list\n        generate_div2_list(10, 1)\n        [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n        generate_div2_list(10, 2)\n        [10, 5, 5, 2, 2, 2, 2, 2]\n    \"\"\"\n    result = []\n    current = n\n    while current &gt;= n_min:\n        result.extend([current] * (n // current))\n        current = current // 2\n    return result\n</code></pre>"},{"location":"reference/spotpython/utils/metrics/","title":"metrics","text":""},{"location":"reference/spotpython/utils/metrics/#spotpython.utils.metrics.apk","title":"<code>apk(actual, predicted, k=10)</code>","text":"<p>Computes the average precision at k. This function computes the average precision at k between two lists of items.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>list</code> <p>A list of elements that are to be predicted (order doesn\u2019t matter)</p> required <code>predicted</code> <code>list</code> <p>A list of predicted elements (order does matter)</p> required <code>k</code> <code>int</code> <p>The maximum number of predicted elements</p> <code>10</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average precision at k over the input lists</p> Source code in <code>spotpython/utils/metrics.py</code> <pre><code>def apk(actual, predicted, k=10):\n    \"\"\"\n    Computes the average precision at k.\n    This function computes the average precision at k between two lists of\n    items.\n\n    Args:\n        actual (list): A list of elements that are to be predicted (order doesn't matter)\n        predicted (list): A list of predicted elements (order does matter)\n        k (int): The maximum number of predicted elements\n\n    Returns:\n        score (float): The average precision at k over the input lists\n    \"\"\"\n    if len(predicted) &gt; k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i, p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i + 1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n</code></pre>"},{"location":"reference/spotpython/utils/metrics/#spotpython.utils.metrics.get_metric_sign","title":"<code>get_metric_sign(metric_name)</code>","text":"<p>Returns the sign of a metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The name of the metric. Can be one of the following:     - \u201caccuracy_score\u201d     - \u201ccohen_kappa_score\u201d     - \u201cf1_score\u201d     - \u201chamming_loss\u201d     - \u201chinge_loss\u201d     -\u201cjaccard_score\u201d     - \u201cmatthews_corrcoef\u201d     - \u201cprecision_score\u201d     - \u201crecall_score\u201d     - \u201croc_auc_score\u201d     - \u201czero_one_loss\u201d</p> required <p>Returns:</p> Name Type Description <code>sign</code> <code>float</code> <p>The sign of the metric. -1 for max, +1 for min.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the metric is not found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.metrics import get_metric_sign\n&gt;&gt;&gt; get_metric_sign(\"accuracy_score\")\n-1\n&gt;&gt;&gt; get_metric_sign(\"hamming_loss\")\n+1\n</code></pre> Source code in <code>spotpython/utils/metrics.py</code> <pre><code>def get_metric_sign(metric_name):\n    \"\"\"Returns the sign of a metric.\n\n    Args:\n        metric_name (str):\n            The name of the metric. Can be one of the following:\n                - \"accuracy_score\"\n                - \"cohen_kappa_score\"\n                - \"f1_score\"\n                - \"hamming_loss\"\n                - \"hinge_loss\"\n                -\"jaccard_score\"\n                - \"matthews_corrcoef\"\n                - \"precision_score\"\n                - \"recall_score\"\n                - \"roc_auc_score\"\n                - \"zero_one_loss\"\n\n    Returns:\n        sign (float): The sign of the metric. -1 for max, +1 for min.\n\n    Raises:\n        ValueError: If the metric is not found.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.metrics import get_metric_sign\n        &gt;&gt;&gt; get_metric_sign(\"accuracy_score\")\n        -1\n        &gt;&gt;&gt; get_metric_sign(\"hamming_loss\")\n        +1\n\n    \"\"\"\n    if metric_name in [\n        \"accuracy_score\",\n        \"cohen_kappa_score\",\n        \"f1_score\",\n        \"jaccard_score\",\n        \"matthews_corrcoef\",\n        \"precision_score\",\n        \"recall_score\",\n        \"roc_auc_score\",\n        \"explained_variance_score\",\n        \"r2_score\",\n        \"d2_absolute_error_score\",\n        \"d2_pinball_score\",\n        \"d2_tweedie_score\",\n    ]:\n        return -1\n    elif metric_name in [\n        \"hamming_loss\",\n        \"hinge_loss\",\n        \"zero_one_loss\",\n        \"max_error\",\n        \"mean_absolute_error\",\n        \"mean_squared_error\",\n        \"root_mean_squared_error\",\n        \"mean_squared_log_error\",\n        \"root_mean_squared_log_error\",\n        \"median_absolute_error\",\n        \"mean_poisson_deviance\",\n        \"mean_gamma_deviance\",\n        \"mean_absolute_percentage_error\",\n    ]:\n        return +1\n    else:\n        raise ValueError(f\"Metric '{metric_name}' not found.\")\n</code></pre>"},{"location":"reference/spotpython/utils/metrics/#spotpython.utils.metrics.mapk","title":"<code>mapk(actual, predicted, k=10)</code>","text":"<p>Computes the mean average precision at k. This function computes the mean average precision at k between two lists of lists of items.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>list</code> <p>A list of lists of elements that are to be predicted (order doesn\u2019t matter in the lists)</p> required <code>predicted</code> <code>list</code> <p>A list of lists of predicted elements (order matters in the lists)</p> required <code>k</code> <code>int</code> <p>The maximum number of predicted elements</p> <code>10</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The mean average precision at k over the input lists</p> Source code in <code>spotpython/utils/metrics.py</code> <pre><code>def mapk(actual, predicted, k=10):\n    \"\"\"\n    Computes the mean average precision at k.\n    This function computes the mean average precision at k between two lists\n    of lists of items.\n\n    Args:\n        actual (list): A list of lists of elements that are to be predicted\n            (order doesn't matter in the lists)\n        predicted (list): A list of lists of predicted elements\n            (order matters in the lists)\n        k (int): The maximum number of predicted elements\n\n    Returns:\n        score (float): The mean average precision at k over the input lists\n    \"\"\"\n    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n</code></pre>"},{"location":"reference/spotpython/utils/metrics/#spotpython.utils.metrics.mapk_score","title":"<code>mapk_score(y_true, y_pred, k=3)</code>","text":"<p>Wrapper for mapk func using numpy arrays</p> <p>Args:         y_true (np.array): array of true values         y_pred (np.array): array of predicted values         k (int): number of predictions</p> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>mean average precision at k</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y_true = np.array([0, 1, 2, 2])\n&gt;&gt;&gt; y_pred = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n         [0.3, 0.4, 0.2],  # 1 is in top 2\n         [0.2, 0.4, 0.3],  # 2 is in top 2\n         [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n&gt;&gt;&gt; mapk_score(y_true, y_pred, k=1)\n0.25\n&gt;&gt;&gt; mapk_score(y_true, y_pred, k=2)\n0.375\n&gt;&gt;&gt; mapk_score(y_true, y_pred, k=3)\n0.4583333333333333\n&gt;&gt;&gt; mapk_score(y_true, y_pred, k=4)\n0.4583333333333333\n</code></pre> Source code in <code>spotpython/utils/metrics.py</code> <pre><code>def mapk_score(y_true, y_pred, k=3):\n    \"\"\"Wrapper for mapk func using numpy arrays\n\n     Args:\n            y_true (np.array): array of true values\n            y_pred (np.array): array of predicted values\n            k (int): number of predictions\n\n    Returns:\n            score (float): mean average precision at k\n\n    Examples:\n            &gt;&gt;&gt; y_true = np.array([0, 1, 2, 2])\n            &gt;&gt;&gt; y_pred = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n                     [0.3, 0.4, 0.2],  # 1 is in top 2\n                     [0.2, 0.4, 0.3],  # 2 is in top 2\n                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=1)\n            0.25\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=2)\n            0.375\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=3)\n            0.4583333333333333\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=4)\n            0.4583333333333333\n    \"\"\"\n    y_true = series_to_array(y_true)\n    sorted_prediction_ids = np.argsort(-y_pred, axis=1)\n    top_k_prediction_ids = sorted_prediction_ids[:, :k]\n    score = mapk(y_true.reshape(-1, 1), top_k_prediction_ids, k=k)\n    return score\n</code></pre>"},{"location":"reference/spotpython/utils/metrics/#spotpython.utils.metrics.mapk_scorer","title":"<code>mapk_scorer(estimator, X, y)</code>","text":"<p>Scorer for mean average precision at k. This function computes the mean average precision at k between two lists of lists of items.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>sklearn estimator</code> <p>The estimator to be used for prediction.</p> required <code>X</code> <code>array-like of shape (n_samples, n_features</code> <p>The input samples.</p> required <code>y</code> <code>array-like of shape (n_samples,</code> <p>The target values.</p> required <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The mean average precision at k over the input lists</p> Source code in <code>spotpython/utils/metrics.py</code> <pre><code>def mapk_scorer(estimator, X, y):\n    \"\"\"\n    Scorer for mean average precision at k.\n    This function computes the mean average precision at k between two lists\n    of lists of items.\n\n    Args:\n        estimator (sklearn estimator): The estimator to be used for prediction.\n        X (array-like of shape (n_samples, n_features)): The input samples.\n        y (array-like of shape (n_samples,)): The target values.\n\n    Returns:\n        score (float): The mean average precision at k over the input lists\n    \"\"\"\n    y_pred = estimator.predict_proba(X)\n    score = mapk_score(y, y_pred, k=3)\n    return score\n</code></pre>"},{"location":"reference/spotpython/utils/misc/","title":"misc","text":""},{"location":"reference/spotpython/utils/misc/#spotpython.utils.misc.get_regressor","title":"<code>get_regressor(name)</code>","text":"<p>Returns a scikit-learn regressor based on the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the regressor. Supported names are: \u201clinear\u201d, \u201cpolynomial\u201d, \u201crandom_forest\u201d, and \u201ckriging\u201d.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>A scikit-learn regressor object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown regressor name is provided.</p> Example <p>from spotpython.utils.misc import get_regressor regressor = get_regressor(\u201clinear\u201d) print(type(regressor))  Source code in <code>spotpython/utils/misc.py</code> <pre><code>def get_regressor(name) -&gt; object:\n    \"\"\"\n    Returns a scikit-learn regressor based on the given name.\n\n    Args:\n        name (str): The name of the regressor.\n            Supported names are: \"linear\", \"polynomial\", \"random_forest\", and \"kriging\".\n\n    Returns:\n        object: A scikit-learn regressor object.\n\n    Raises:\n        ValueError: If an unknown regressor name is provided.\n\n    Example:\n        &gt;&gt;&gt; from spotpython.utils.misc import get_regressor\n        &gt;&gt;&gt; regressor = get_regressor(\"linear\")\n        &gt;&gt;&gt; print(type(regressor))\n        &lt;class 'sklearn.linear_model._base.LinearRegression'&gt;\n    \"\"\"\n    if name == \"linear\":\n        mdl = LinearRegression()\n    elif name == \"polynomial\":\n        degree_polyn = 2\n        mdl = Pipeline([(\"poly\", PolynomialFeatures(degree=degree_polyn)), (\"linear\", LinearRegression())])\n    elif name == \"random_forest\":\n        mdl = RandomForestRegressor()\n    # elif name == \"xgboost\":\n    #     mdl = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, random_state=42)\n    elif name == \"kriging\":\n        mdl = Kriging()\n    else:\n        raise ValueError(f\"Unknown regressor {name}\")\n    return mdl\n</code></pre>"},{"location":"reference/spotpython/utils/numpy2json/","title":"numpy2json","text":""},{"location":"reference/spotpython/utils/numpy2json/#spotpython.utils.numpy2json.NumpyEncoder","title":"<code>NumpyEncoder</code>","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>JSONEncoder subclass that knows how to encode numpy arrays.</p> Note <p>Taken from: https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable</p> Source code in <code>spotpython/utils/numpy2json.py</code> <pre><code>class NumpyEncoder(json.JSONEncoder):\n    \"\"\"\n    JSONEncoder subclass that knows how to encode numpy arrays.\n\n    Note:\n        Taken from:\n        https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable\n\n    \"\"\"\n\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.bool_):\n            return int(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, np.float64):\n            if np.isnan(obj):\n                return \"NaN\"\n            elif np.isinf(obj):\n                return \"Inf\"\n            else:\n                return float(obj)\n        return json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"reference/spotpython/utils/optimize/","title":"optimize","text":""},{"location":"reference/spotpython/utils/optimize/#spotpython.utils.optimize.run_minimize_with_restarts","title":"<code>run_minimize_with_restarts(objective, gradient, x0, bounds, n_restarts_optimizer=5, method='L-BFGS-B', maxit=100, verb=0, random_state=None)</code>","text":"<p>Runs multiple restarts of the minimize() function and returns the best found result.</p> <p>Parameters:</p> Name Type Description Default <code>objective</code> <code>callable</code> <p>The objective function to minimize.</p> required <code>gradient</code> <code>callable</code> <p>The gradient of the objective.</p> required <code>x0</code> <code>ndarray</code> <p>Initial guess for the optimizer.</p> required <code>bounds</code> <code>list</code> <p>List of (min, max) pairs for each element in x0.</p> required <code>n_restarts_optimizer</code> <code>int</code> <p>Number of random-restart attempts.</p> <code>5</code> <code>method</code> <code>str</code> <p>Optimization method. Default \u201cL-BFGS-B\u201d.</p> <code>'L-BFGS-B'</code> <code>maxit</code> <code>int</code> <p>Max iterations.</p> <code>100</code> <code>verb</code> <code>int</code> <p>Verbosity level.</p> <code>0</code> <code>random_state</code> <code>int</code> <p>Seed for the random-number generator to ensure reproducibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>OptimizeResult</code> <code>object</code> <p>The best optimization result among all restarts,</p> <code>minimize</code> <p>represented as a <code>OptimizeResult</code> object. Important attributes are:</p> <code>minimize</code> <p><code>x</code> the solution array, <code>success</code> a Boolean flag indicating if the optimizer</p> <code>minimize</code> <p>exited successfully and <code>message</code> which describes the cause of the termination.</p> Source code in <code>spotpython/utils/optimize.py</code> <pre><code>def run_minimize_with_restarts(objective, gradient, x0, bounds, n_restarts_optimizer=5, method=\"L-BFGS-B\", maxit=100, verb=0, random_state=None) -&gt; \"minimize\":\n    \"\"\"\n    Runs multiple restarts of the minimize() function and returns the best found result.\n\n    Args:\n        objective (callable): The objective function to minimize.\n        gradient (callable): The gradient of the objective.\n        x0 (np.ndarray): Initial guess for the optimizer.\n        bounds (list): List of (min, max) pairs for each element in x0.\n        n_restarts_optimizer (int): Number of random-restart attempts.\n        method (str): Optimization method. Default \"L-BFGS-B\".\n        maxit (int): Max iterations.\n        verb (int): Verbosity level.\n        random_state (int, optional): Seed for the random-number generator to ensure reproducibility.\n\n    Returns:\n        OptimizeResult (object): The best optimization result among all restarts,\n        represented as a ``OptimizeResult`` object. Important attributes are:\n        ``x`` the solution array, ``success`` a Boolean flag indicating if the optimizer\n        exited successfully and ``message`` which describes the cause of the termination.\n    \"\"\"\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    best_result = None\n    best_fun = float(\"inf\")\n\n    for _ in range(n_restarts_optimizer):\n        # Create a random starting point within bounds\n        x0_rand = []\n        for (lb, ub), init_val in zip(bounds, x0):\n            if lb == -np.inf or ub == np.inf:\n                # If unbounded, keep the same initial guess\n                x0_rand.append(init_val)\n            else:\n                x0_rand.append(np.random.uniform(lb, ub))\n        x0_rand = np.array(x0_rand)\n\n        result = minimize(\n            fun=objective,\n            x0=x0_rand,\n            method=method,\n            jac=gradient,\n            bounds=bounds,\n            options={\"maxiter\": maxit, \"disp\": verb &gt; 0},\n        )\n        if result.fun &lt; best_fun:\n            best_fun = result.fun\n            best_result = result\n\n    return best_result\n</code></pre>"},{"location":"reference/spotpython/utils/parallel/","title":"parallel","text":""},{"location":"reference/spotpython/utils/parallel/#spotpython.utils.parallel.evaluate_row","title":"<code>evaluate_row(row, objective_function, fun_control)</code>","text":"<p>Evaluates a single row using the provided objective function.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>array - like</code> <p>The input data for the row to be evaluated.</p> required <code>objective_function</code> <code>callable</code> <p>A function that computes the objective value. It should accept a NumPy array and an additional control parameter.</p> required <code>fun_control</code> <code>any</code> <p>Additional control parameter to be passed to the objective function.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the objective function applied to the row.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.parallel import evaluate_row\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; def sample_objective(row, control):\n...     return row + control.get('offset', 0)\n&gt;&gt;&gt; row = [1, 2, 3]\n&gt;&gt;&gt; fun_control = {'offset': 10}\n&gt;&gt;&gt; evaluate_row(row, sample_objective, fun_control)\n    array([11, 12, 13])\n</code></pre> Source code in <code>spotpython/utils/parallel.py</code> <pre><code>def evaluate_row(row: Union[np.ndarray, list], objective_function: Callable[[np.ndarray, Any], Any], fun_control: Any) -&gt; Any:\n    \"\"\"\n    Evaluates a single row using the provided objective function.\n\n    Args:\n        row (array-like): The input data for the row to be evaluated.\n        objective_function (callable): A function that computes the objective value.\n            It should accept a NumPy array and an additional control parameter.\n        fun_control (any): Additional control parameter to be passed to the objective function.\n\n    Returns:\n        The result of the objective function applied to the row.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.parallel import evaluate_row\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; def sample_objective(row, control):\n        ...     return row + control.get('offset', 0)\n        &gt;&gt;&gt; row = [1, 2, 3]\n        &gt;&gt;&gt; fun_control = {'offset': 10}\n        &gt;&gt;&gt; evaluate_row(row, sample_objective, fun_control)\n            array([11, 12, 13])\n    \"\"\"\n    if fun_control is not None:\n        if \"seed\" in fun_control:\n            seed = fun_control[\"seed\"]\n            set_all_seeds(seed)\n    return objective_function(np.array([row]), fun_control)\n</code></pre>"},{"location":"reference/spotpython/utils/parallel/#spotpython.utils.parallel.make_parallel","title":"<code>make_parallel(obj_func, num_cores, method='mp')</code>","text":"<p>Creates a parallelized wrapper function for the given objective function.</p> <p>Parameters:</p> Name Type Description Default <code>obj_func</code> <code>callable</code> <p>The objective function to be parallelized. It should accept the same arguments as the wrapper function.</p> required <code>num_cores</code> <code>int</code> <p>The number of cores to use for parallel processing.</p> required <code>method</code> <code>str</code> <p>The parallelization method to use. Defaults to \u2018mp\u2019 (multiprocessing). Other methods may be supported depending on the implementation of <code>parallel_objective_function</code>.</p> <code>'mp'</code> <p>Returns:</p> Name Type Description <code>callable</code> <code>Callable</code> <p>A wrapper function that executes the objective function</p> <code>Callable</code> <p>in parallel using the specified number of cores and method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.parallel import make_parallel\n&gt;&gt;&gt; def sample_function(x):\n...     return x ** 2\n...\n&gt;&gt;&gt; parallel_func = make_parallel(sample_function, num_cores=4, method='mp')\n&gt;&gt;&gt; result = parallel_func([1, 2, 3, 4])\n&gt;&gt;&gt; print(result)\n[1, 4, 9, 16]\n</code></pre> Source code in <code>spotpython/utils/parallel.py</code> <pre><code>def make_parallel(obj_func, num_cores, method=\"mp\") -&gt; Callable:\n    \"\"\"\n    Creates a parallelized wrapper function for the given objective function.\n\n    Args:\n        obj_func (callable): The objective function to be parallelized.\n            It should accept the same arguments as the wrapper function.\n        num_cores (int): The number of cores to use for parallel processing.\n        method (str, optional): The parallelization method to use.\n            Defaults to 'mp' (multiprocessing). Other methods may be supported\n            depending on the implementation of `parallel_objective_function`.\n\n    Returns:\n        callable: A wrapper function that executes the objective function\n        in parallel using the specified number of cores and method.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.parallel import make_parallel\n        &gt;&gt;&gt; def sample_function(x):\n        ...     return x ** 2\n        ...\n        &gt;&gt;&gt; parallel_func = make_parallel(sample_function, num_cores=4, method='mp')\n        &gt;&gt;&gt; result = parallel_func([1, 2, 3, 4])\n        &gt;&gt;&gt; print(result)\n        [1, 4, 9, 16]\n    \"\"\"\n    global parallel_wrap\n\n    def parallel_wrap(X, fun_control=None):\n        return parallel_objective_function(obj_func, X, num_cores, fun_control, method)\n\n    return parallel_wrap\n</code></pre>"},{"location":"reference/spotpython/utils/parallel/#spotpython.utils.parallel.parallel_objective_function","title":"<code>parallel_objective_function(objective_function, X, num_cores, fun_control, method)</code>","text":"<p>Executes an objective function in parallel using either multiprocessing or joblib.</p> <p>Parameters:</p> Name Type Description Default <code>objective_function</code> <code>callable</code> <p>The function to be evaluated for each row in <code>X</code>.</p> required <code>X</code> <code>iterable</code> <p>The input data, where each element represents a row to be processed.</p> required <code>num_cores</code> <code>int</code> <p>The number of CPU cores to use for parallel processing.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary of shared control parameters for the objective function.</p> required <code>method</code> <code>str</code> <p>The parallelization method to use. Options are: - \u2018mp\u2019: Use Python\u2019s multiprocessing module. - \u2018joblib\u2019: Use the joblib library.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: A flattened array of results obtained by applying the objective function to each row in <code>X</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported <code>method</code> is provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.parallel import parallel_objective_function\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; def sample_objective(row, control):\n...     return sum(row) + control.get('offset', 0)\n&gt;&gt;&gt; X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n&gt;&gt;&gt; fun_control = {'offset': 10}\n&gt;&gt;&gt; parallel_objective_function(sample_objective, X, num_cores=2, fun_control=fun_control, method='mp')\narray([16, 25, 34])\n&gt;&gt;&gt; parallel_objective_function(sample_objective, X, num_cores=2, fun_control=fun_control, method='joblib')\narray([16, 25, 34])\n</code></pre> Source code in <code>spotpython/utils/parallel.py</code> <pre><code>def parallel_objective_function(objective_function, X, num_cores, fun_control, method) -&gt; np.ndarray:\n    \"\"\"\n    Executes an objective function in parallel using either multiprocessing or joblib.\n\n    Args:\n        objective_function (callable): The function to be evaluated for each row in `X`.\n        X (iterable): The input data, where each element represents a row to be processed.\n        num_cores (int): The number of CPU cores to use for parallel processing.\n        fun_control (dict): A dictionary of shared control parameters for the objective function.\n        method (str): The parallelization method to use. Options are:\n            - 'mp': Use Python's multiprocessing module.\n            - 'joblib': Use the joblib library.\n\n    Returns:\n        numpy.ndarray: A flattened array of results obtained by applying the objective function to each row in `X`.\n\n    Raises:\n        ValueError: If an unsupported `method` is provided.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.parallel import parallel_objective_function\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; def sample_objective(row, control):\n        ...     return sum(row) + control.get('offset', 0)\n        &gt;&gt;&gt; X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n        &gt;&gt;&gt; fun_control = {'offset': 10}\n        &gt;&gt;&gt; parallel_objective_function(sample_objective, X, num_cores=2, fun_control=fun_control, method='mp')\n        array([16, 25, 34])\n        &gt;&gt;&gt; parallel_objective_function(sample_objective, X, num_cores=2, fun_control=fun_control, method='joblib')\n        array([16, 25, 34])\n    \"\"\"\n    if method == \"mp\":\n        with Pool(processes=num_cores) as pool:\n            results = pool.starmap(evaluate_row, [(row, objective_function, fun_control) for row in X])\n    elif method == \"joblib\":\n        results = Parallel(n_jobs=num_cores)(delayed(evaluate_row)(row, objective_function, fun_control) for row in X)\n\n    return np.array(results).flatten()\n</code></pre>"},{"location":"reference/spotpython/utils/pca/","title":"pca","text":""},{"location":"reference/spotpython/utils/pca/#spotpython.utils.pca.pca_analysis","title":"<code>pca_analysis(df, df_name='', k=10, scaler=StandardScaler(), max_scree=None, figsize=(12, 6))</code>","text":"<p>Perform PCA analysis on a DataFrame with specified scaling.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input data frame to perform PCA on.</p> required <code>df_name</code> <code>str</code> <p>The name of the data frame.</p> <code>''</code> <code>k</code> <code>int</code> <p>The number of top features to select based on their influence on PC1.</p> <code>10</code> <code>scaler</code> <code>obj</code> <p>An instance of a Scaler from sklearn (e.g., StandardScaler()).</p> <code>StandardScaler()</code> <code>max_scree</code> <code>int</code> <p>The maximum number of principal components to plot in the scree plot. Default is None, which means all components will be plotted.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>The size of the figure for the plots (width, height).</p> <code>(12, 6)</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>Two pd.Index objects containing the names of the top k features most influential on PC1 and PC2, respectively.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotpython.utils import pca_analysis\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"A\": [1, 2, 3],\n...     \"B\": [1, 2, 3],\n...     \"C\": [4, 5, 6]\n... })\n&gt;&gt;&gt; pca_analysis(df)\n</code></pre> Source code in <code>spotpython/utils/pca.py</code> <pre><code>def pca_analysis(\n    df,\n    df_name=\"\",\n    k=10,\n    scaler=StandardScaler(),\n    max_scree=None,\n    figsize=(12, 6),\n) -&gt; tuple:\n    \"\"\"\n    Perform PCA analysis on a DataFrame with specified scaling.\n\n    Args:\n        df (pd.DataFrame):\n            The input data frame to perform PCA on.\n        df_name (str):\n            The name of the data frame.\n        k (int):\n            The number of top features to select based on their influence on PC1.\n        scaler (obj):\n            An instance of a Scaler from sklearn (e.g., StandardScaler()).\n        max_scree (int):\n            The maximum number of principal components to plot in the scree plot. Default is None, which means all components will be plotted.\n        figsize (tuple):\n            The size of the figure for the plots (width, height).\n\n    Returns:\n        tuple: Two pd.Index objects containing the names of the top k features most influential on PC1 and PC2, respectively.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotpython.utils import pca_analysis\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"A\": [1, 2, 3],\n        ...     \"B\": [1, 2, 3],\n        ...     \"C\": [4, 5, 6]\n        ... })\n        &gt;&gt;&gt; pca_analysis(df)\n    \"\"\"\n    # Scale the data\n    scaled_data = scaler.fit_transform(df)\n    feature_names = df.columns\n    sample_names = df.index\n\n    # Perform PCA\n    pca = PCA()\n    pca.fit(scaled_data)\n    pca_data = pca.transform(scaled_data)\n\n    # Scree plot\n    per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n    full_labels = [\"PC\" + str(x) for x in range(1, len(per_var) + 1)]\n\n    # Limit the number of PCs in the scree plot\n    if max_scree is not None:\n        per_var = per_var[:max_scree]\n        scree_labels = full_labels[:max_scree]\n    else:\n        scree_labels = full_labels\n\n    plt.figure(figsize=figsize)  # Set the figure size for the scree plot\n    plt.bar(x=range(1, len(per_var) + 1), height=per_var, tick_label=scree_labels)\n    plt.ylabel(\"Percentage of Explained Variance\")\n    plt.xlabel(\"Principal Component\")\n    plt.title(f\"Scree Plot. {df_name}\")\n    plt.show()\n\n    # PCA plot\n    plt.figure(figsize=figsize)  # Set the figure size for the PCA plot\n    pca_df = pd.DataFrame(pca_data, index=sample_names, columns=full_labels)\n\n    plt.scatter(pca_df.PC1, pca_df.PC2)\n    plt.title(f\"PCA Graph. {df_name}\")\n    plt.xlabel(\"PC1 - {0}%\".format(per_var[0]))\n    plt.ylabel(\"PC2 - {0}%\".format(per_var[1]))\n\n    for sample in pca_df.index:\n        plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]))\n\n    plt.show()\n\n    # Determine top k features influencing PC1 and PC2\n    loading_scores_pc1 = pd.Series(pca.components_[0], index=feature_names)\n    loading_scores_pc2 = pd.Series(pca.components_[1], index=feature_names)\n\n    sorted_loading_scores_pc1 = loading_scores_pc1.abs().sort_values(ascending=False)\n    sorted_loading_scores_pc2 = loading_scores_pc2.abs().sort_values(ascending=False)\n\n    top_k_features_pc1 = sorted_loading_scores_pc1.head(k).index\n    top_k_features_pc2 = sorted_loading_scores_pc2.head(k).index\n\n    return top_k_features_pc1, top_k_features_pc2\n</code></pre>"},{"location":"reference/spotpython/utils/preprocess/","title":"preprocess","text":""},{"location":"reference/spotpython/utils/preprocess/#spotpython.utils.preprocess.generic_preprocess_df","title":"<code>generic_preprocess_df(df, target, imputer_num=SimpleImputer(strategy='mean'), imputer_cat=SimpleImputer(strategy='most_frequent'), encoder_cat=OneHotEncoder(categories='auto', drop=None, handle_unknown='ignore', sparse_output=False), scaler_num=RobustScaler(), test_size=0.2, random_state=42, shuffle=True, n_jobs=None)</code>","text":"<p>Preprocesses a DataFrame by handling numerical and categorical features, splitting the data into training and testing sets, and applying transformations. Supports single or multiple target columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to preprocess.</p> required <code>target</code> <code>Union[str, List[str]]</code> <p>The name(s) of the target column(s) to predict. Can be a single string or a list of strings.</p> required <code>imputer_num</code> <code>SimpleImputer</code> <p>Imputer for numerical columns. Defaults to <code>SimpleImputer(strategy=\"mean\")</code>.</p> <code>SimpleImputer(strategy='mean')</code> <code>imputer_cat</code> <code>SimpleImputer</code> <p>Imputer for categorical columns. Defaults to <code>SimpleImputer(strategy=\"most_frequent\")</code>.</p> <code>SimpleImputer(strategy='most_frequent')</code> <code>encoder_cat</code> <code>OneHotEncoder</code> <p>Encoder for categorical columns. Defaults to <code>OneHotEncoder(categories=\"auto\", drop=None, handle_unknown=\"ignore\")</code>.</p> <code>OneHotEncoder(categories='auto', drop=None, handle_unknown='ignore', sparse_output=False)</code> <code>scaler_num</code> <code>RobustScaler</code> <p>Scaler for numerical columns. Defaults to <code>RobustScaler()</code>.</p> <code>RobustScaler()</code> <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the test split. Defaults to 0.2.</p> <code>0.2</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting. Defaults to True.</p> <code>True</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel for the <code>ColumnTransformer</code>. Defaults to None (1 job).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, DataFrame, DataFrame]</code> <p>Tuple[np.ndarray, np.ndarray, pd.DataFrame, pd.DataFrame]: A tuple containing: - X_train (np.ndarray): Transformed training feature set. - X_test (np.ndarray): Transformed testing feature set. - y_train (pd.DataFrame): Training target values. - y_test (pd.DataFrame): Testing target values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the target column(s) are not found in the DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.preprocess import generic_preprocess_df\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.impute import SimpleImputer\n&gt;&gt;&gt; from sklearn.preprocessing import OneHotEncoder, RobustScaler\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"age\": [25, 30, np.nan, 35],\n...     \"gender\": [\"M\", \"F\", \"M\", \"F\"],\n...     \"income\": [50000, 60000, 55000, np.nan],\n...     \"target1\": [1, 0, 1, 0],\n...     \"target2\": [0, 1, 0, 1]\n... })\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = generic_preprocess_df(\n...     df,\n...     target=[\"target1\", \"target2\"],\n...     imputer_num=SimpleImputer(strategy=\"mean\"),\n...     imputer_cat=SimpleImputer(strategy=\"most_frequent\"),\n...     encoder_cat=OneHotEncoder(),\n...     scaler_num=RobustScaler(),\n...     test_size=0.25,\n...     random_state=42\n... )\n</code></pre> Source code in <code>spotpython/utils/preprocess.py</code> <pre><code>def generic_preprocess_df(\n    df: pd.DataFrame,\n    target: Union[str, List[str]],\n    imputer_num=SimpleImputer(strategy=\"mean\"),\n    imputer_cat=SimpleImputer(strategy=\"most_frequent\"),\n    encoder_cat=OneHotEncoder(categories=\"auto\", drop=None, handle_unknown=\"ignore\", sparse_output=False),\n    scaler_num=RobustScaler(),\n    test_size=0.2,\n    random_state=42,\n    shuffle=True,\n    n_jobs=None,\n) -&gt; Tuple[np.ndarray, np.ndarray, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Preprocesses a DataFrame by handling numerical and categorical features,\n    splitting the data into training and testing sets, and applying transformations.\n    Supports single or multiple target columns.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to preprocess.\n        target (Union[str, List[str]]): The name(s) of the target column(s) to predict.\n            Can be a single string or a list of strings.\n        imputer_num (SimpleImputer, optional): Imputer for numerical columns.\n            Defaults to `SimpleImputer(strategy=\"mean\")`.\n        imputer_cat (SimpleImputer, optional): Imputer for categorical columns.\n            Defaults to `SimpleImputer(strategy=\"most_frequent\")`.\n        encoder_cat (OneHotEncoder, optional): Encoder for categorical columns.\n            Defaults to `OneHotEncoder(categories=\"auto\", drop=None, handle_unknown=\"ignore\")`.\n        scaler_num (RobustScaler, optional): Scaler for numerical columns.\n            Defaults to `RobustScaler()`.\n        test_size (float, optional): Proportion of the dataset to include in the test split.\n            Defaults to 0.2.\n        random_state (int, optional): Random seed for reproducibility. Defaults to 42.\n        shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n        n_jobs (int, optional): Number of jobs to run in parallel for the `ColumnTransformer`.\n            Defaults to None (1 job).\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, pd.DataFrame, pd.DataFrame]:\n            A tuple containing:\n            - X_train (np.ndarray): Transformed training feature set.\n            - X_test (np.ndarray): Transformed testing feature set.\n            - y_train (pd.DataFrame): Training target values.\n            - y_test (pd.DataFrame): Testing target values.\n\n    Raises:\n        ValueError: If any of the target column(s) are not found in the DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.preprocess import generic_preprocess_df\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.impute import SimpleImputer\n        &gt;&gt;&gt; from sklearn.preprocessing import OneHotEncoder, RobustScaler\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"age\": [25, 30, np.nan, 35],\n        ...     \"gender\": [\"M\", \"F\", \"M\", \"F\"],\n        ...     \"income\": [50000, 60000, 55000, np.nan],\n        ...     \"target1\": [1, 0, 1, 0],\n        ...     \"target2\": [0, 1, 0, 1]\n        ... })\n        &gt;&gt;&gt; X_train, X_test, y_train, y_test = generic_preprocess_df(\n        ...     df,\n        ...     target=[\"target1\", \"target2\"],\n        ...     imputer_num=SimpleImputer(strategy=\"mean\"),\n        ...     imputer_cat=SimpleImputer(strategy=\"most_frequent\"),\n        ...     encoder_cat=OneHotEncoder(),\n        ...     scaler_num=RobustScaler(),\n        ...     test_size=0.25,\n        ...     random_state=42\n        ... )\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n\n    if isinstance(target, str):\n        target = [target]  # Convert to list for consistent handling\n\n    for t in target:\n        if t not in df.columns:\n            raise ValueError(f\"Target column '{t}' not found in the DataFrame.\")\n\n    X = df.drop(target, axis=1)\n    y = df[target]\n\n    num_cols = get_num_cols(X)\n    cat_cols = get_cat_cols(X)\n    X[cat_cols] = X[cat_cols].astype(str)\n\n    numerical_transformer = Pipeline(steps=[(\"imputer\", imputer_num), (\"scaler\", scaler_num)])\n    categorical_transformer = Pipeline(steps=[(\"imputer\", imputer_cat), (\"encoder\", encoder_cat)])\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"numerical\", numerical_transformer, num_cols),\n            (\"categorical\", categorical_transformer, cat_cols),\n        ],\n        remainder=\"drop\",\n        sparse_threshold=0,\n        n_jobs=n_jobs,\n    )\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=shuffle)\n\n    X_train = preprocessor.fit_transform(X_train)\n    X_test = preprocessor.transform(X_test)\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"reference/spotpython/utils/preprocess/#spotpython.utils.preprocess.get_cat_cols","title":"<code>get_cat_cols(df)</code>","text":"<p>Identifies categorical columns in a DataFrame.</p> <p>This function selects columns with object data types (e.g., strings) or columns with all NaN values from the given DataFrame and returns their names as a list.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of column names corresponding to categorical columns.</p> Example <p>import pandas as pd import numpy as np df = pd.DataFrame({ \u2026     \u201cage\u201d: [25, 30, np.nan, 35], \u2026     \u201cgender\u201d: [\u201cM\u201d, \u201cF\u201d, \u201cM\u201d, \u201cF\u201d], \u2026     \u201cincome\u201d: [50000, 60000, 55000, np.nan] \u2026 }) get_cat_cols(df) [\u2018gender\u2019]</p> Source code in <code>spotpython/utils/preprocess.py</code> <pre><code>def get_cat_cols(df: pd.DataFrame) -&gt; list:\n    \"\"\"\n    Identifies categorical columns in a DataFrame.\n\n    This function selects columns with object data types (e.g., strings)\n    or columns with all NaN values from the given DataFrame and returns their names as a list.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        list: A list of column names corresponding to categorical columns.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"age\": [25, 30, np.nan, 35],\n        ...     \"gender\": [\"M\", \"F\", \"M\", \"F\"],\n        ...     \"income\": [50000, 60000, 55000, np.nan]\n        ... })\n        &gt;&gt;&gt; get_cat_cols(df)\n        ['gender']\n    \"\"\"\n    return df.select_dtypes(include=[\"object\"]).columns.tolist() + [col for col in df.columns if df[col].isna().all()]\n</code></pre>"},{"location":"reference/spotpython/utils/preprocess/#spotpython.utils.preprocess.get_num_cols","title":"<code>get_num_cols(df)</code>","text":"<p>Identifies numerical columns in a DataFrame.</p> <p>This function selects columns with numerical data types (e.g., int, float) from the given DataFrame and returns their names as a list.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of column names corresponding to numerical columns.</p> Example <p>import pandas as pd import numpy as np df = pd.DataFrame({ \u2026     \u201cage\u201d: [25, 30, np.nan, 35], \u2026     \u201cgender\u201d: [\u201cM\u201d, \u201cF\u201d, \u201cM\u201d, \u201cF\u201d], \u2026     \u201cincome\u201d: [50000, 60000, 55000, np.nan] \u2026 }) get_num_cols(df) [\u2018age\u2019, \u2018income\u2019]</p> Source code in <code>spotpython/utils/preprocess.py</code> <pre><code>def get_num_cols(df: pd.DataFrame) -&gt; list:\n    \"\"\"\n    Identifies numerical columns in a DataFrame.\n\n    This function selects columns with numerical data types (e.g., int, float)\n    from the given DataFrame and returns their names as a list.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        list: A list of column names corresponding to numerical columns.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"age\": [25, 30, np.nan, 35],\n        ...     \"gender\": [\"M\", \"F\", \"M\", \"F\"],\n        ...     \"income\": [50000, 60000, 55000, np.nan]\n        ... })\n        &gt;&gt;&gt; get_num_cols(df)\n        ['age', 'income']\n    \"\"\"\n    return df.select_dtypes(include=[np.number]).columns.tolist()\n</code></pre>"},{"location":"reference/spotpython/utils/progress/","title":"progress","text":""},{"location":"reference/spotpython/utils/progress/#spotpython.utils.progress.progress_bar","title":"<code>progress_bar(progress, bar_length=10, message='spotpython tuning:', y=None, filename=None)</code>","text":"<p>Displays or updates a console progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>progress</code> <code>float</code> <p>a float between 0 and 1. Any int will be converted to a float. A value under 0 represents a halt. A value at 1 or bigger represents 100%.</p> required <code>bar_length</code> <code>int</code> <p>length of the progress bar</p> <code>10</code> <code>message</code> <code>str</code> <p>message text to display</p> <code>'spotpython tuning:'</code> <code>filename</code> <code>str</code> <p>If not None, write the progress bar to filename.</p> <code>None</code> Source code in <code>spotpython/utils/progress.py</code> <pre><code>def progress_bar(progress: float, bar_length: int = 10, message: str = \"spotpython tuning:\", y=None, filename=None) -&gt; None:\n    \"\"\"\n    Displays or updates a console progress bar.\n\n    Args:\n        progress (float):\n            a float between 0 and 1. Any int will be converted to a float.\n            A value under 0 represents a halt.\n            A value at 1 or bigger represents 100%.\n        bar_length (int):\n            length of the progress bar\n        message (str):\n            message text to display\n        filename (str):\n            If not None, write the progress bar to filename.\n    \"\"\"\n    if filename is not None:\n        # open the file in append mode\n        file = open(filename, \"a\")\n    status = \"\"\n    if y is not None:\n        message = f\"{message} {y}\"\n    if progress &lt; 0:\n        progress = 0\n        status = \"Halt...\\r\\n\"\n    elif progress &gt;= 1:\n        progress = 1\n        status = \"Done...\\r\\n\"\n    block = int(round(bar_length * progress))\n    text = f\"{message} [{'#' * block + '-' * (bar_length - block)}] {progress * 100:.2f}% {status}\\r\\n\"\n    if filename is not None:\n        file.write(text)\n        file.flush()\n    stdout.write(text)\n    stdout.flush()\n    if filename is not None:\n        file.close()\n</code></pre>"},{"location":"reference/spotpython/utils/repair/","title":"repair","text":""},{"location":"reference/spotpython/utils/repair/#spotpython.utils.repair.apply_penalty_NA","title":"<code>apply_penalty_NA(y, penalty_NA, sd=0.1, stop_on_zero_return=False, verbosity=0)</code>","text":"<p>Replaces NaN values in y with a penalty value of penalty_NA and issues a warning if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>y array</p> required <code>penalty_NA</code> <code>float</code> <p>penalty value to replace NaN values in y</p> required <code>sd</code> <code>float</code> <p>standard deviation for the random noise added to penalty_NA. Default is 0.1.</p> <code>0.1</code> <code>stop_on_zero_return</code> <code>bool</code> <p>whether to stop if the returned dimension is less than 1. Default is False.</p> <code>False</code> <code>verbosity</code> <code>int</code> <p>verbosity level. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: y array with NaN values replaced by penalty value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.repair import apply_penalty_NA\n&gt;&gt;&gt; y = np.array([1, np.nan, 2])\n&gt;&gt;&gt; y_cleaned = apply_penalty_NA(y, 0)\n&gt;&gt;&gt; print(y_cleaned)\n[1. 0. 2.]\n</code></pre> Source code in <code>spotpython/utils/repair.py</code> <pre><code>def apply_penalty_NA(y: np.ndarray, penalty_NA: float, sd=0.1, stop_on_zero_return: bool = False, verbosity=0) -&gt; np.ndarray:\n    \"\"\"\n    Replaces NaN values in y with a penalty value of penalty_NA and issues a warning if necessary.\n\n    Args:\n        y (numpy.ndarray): y array\n        penalty_NA (float): penalty value to replace NaN values in y\n        sd (float): standard deviation for the random noise added to penalty_NA. Default is 0.1.\n        stop_on_zero_return (bool): whether to stop if the returned dimension is less than 1. Default is False.\n        verbosity (int): verbosity level. Default is 0.\n\n    Returns:\n        numpy.ndarray: y array with NaN values replaced by penalty value\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.repair import apply_penalty_NA\n        &gt;&gt;&gt; y = np.array([1, np.nan, 2])\n        &gt;&gt;&gt; y_cleaned = apply_penalty_NA(y, 0)\n        &gt;&gt;&gt; print(y_cleaned)\n        [1. 0. 2.]\n    \"\"\"\n    if not isinstance(y, np.ndarray):\n        raise TypeError(\"Input y must be a numpy array.\")\n\n    if not isinstance(penalty_NA, (int, float)):\n        return y\n\n    if not isinstance(sd, (int, float)):\n        raise TypeError(\"sd must be a numeric value.\")\n\n    if not isinstance(stop_on_zero_return, bool):\n        raise TypeError(\"stop_on_zero_return must be a boolean value.\")\n\n    original_dim = y.shape[0]\n    nan_ind = ~np.isfinite(y)\n    nan_dim = np.sum(nan_ind)\n\n    random_values = np.random.normal(0, sd, y.shape)\n    penalty_values = penalty_NA + random_values\n\n    y_cleaned = y.copy()\n    y_cleaned[nan_ind] = penalty_values[nan_ind]\n\n    if nan_dim &gt; 1:\n        warnings.warn(f\"\\n!!! The dimension of the returned y array is {y_cleaned.shape[0]}, \" f\"which is smaller than the original dimension {original_dim}.\")\n        warnings.warn(\"\\n!!! Check whether continuing with the reduced dimension is useful.\")\n        if verbosity &gt; 0:\n            print(f\"y before penalty: {y}. y after penalty: {y_cleaned}\")\n\n    if (original_dim - nan_dim) &lt; 1 and stop_on_zero_return:\n        raise ValueError(\"!!!! The dimension of the returned y array is less than 1. Check the input data.\")\n    return y_cleaned\n</code></pre>"},{"location":"reference/spotpython/utils/repair/#spotpython.utils.repair.remove_nan","title":"<code>remove_nan(X, y, stop_on_zero_return=False)</code>","text":"<p>Remove rows from X and y where y contains NaN values and issue a warning     if the dimension of the returned y array is smaller than the dimension of the original y array.     Handles both 1D (shape <code>(n,)</code>) and 2D (shape <code>(n, m)</code>) y arrays.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array</p> required <code>y</code> <code>ndarray</code> <p>y array (can be 1D or 2D)</p> required <code>stop_on_zero_return</code> <code>bool</code> <p>whether to stop if the returned dimension is less than 1. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[numpy.ndarray, numpy.ndarray]: X and y arrays with rows containing NaN values in y removed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.repair import remove_nan\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; y = np.array([1, np.nan, 2])\n&gt;&gt;&gt; X_cleaned, y_cleaned = remove_nan(X, y)\n&gt;&gt;&gt; print(X_cleaned, y_cleaned)\n[[1 2]\n [5 6]] [1. 2.]\n</code></pre> <pre><code>&gt;&gt;&gt; y = np.array([[1, 2], [np.nan, 4], [5, np.nan]])\n&gt;&gt;&gt; X_cleaned, y_cleaned = remove_nan(X, y)\n&gt;&gt;&gt; print(X_cleaned, y_cleaned)\n[[1 2]] [[1. 2.]]\n</code></pre> Source code in <code>spotpython/utils/repair.py</code> <pre><code>def remove_nan(X: np.ndarray, y: np.ndarray, stop_on_zero_return: bool = False) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Remove rows from X and y where y contains NaN values and issue a warning\n        if the dimension of the returned y array is smaller than the dimension of the original y array.\n        Handles both 1D (shape `(n,)`) and 2D (shape `(n, m)`) y arrays.\n\n    Args:\n        X (numpy.ndarray):\n            X array\n        y (numpy.ndarray):\n            y array (can be 1D or 2D)\n        stop_on_zero_return (bool):\n            whether to stop if the returned dimension is less than 1.\n            Default is False.\n\n    Returns:\n        Tuple[numpy.ndarray, numpy.ndarray]:\n            X and y arrays with rows containing NaN values in y removed.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.repair import remove_nan\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; y = np.array([1, np.nan, 2])\n        &gt;&gt;&gt; X_cleaned, y_cleaned = remove_nan(X, y)\n        &gt;&gt;&gt; print(X_cleaned, y_cleaned)\n        [[1 2]\n         [5 6]] [1. 2.]\n\n        &gt;&gt;&gt; y = np.array([[1, 2], [np.nan, 4], [5, np.nan]])\n        &gt;&gt;&gt; X_cleaned, y_cleaned = remove_nan(X, y)\n        &gt;&gt;&gt; print(X_cleaned, y_cleaned)\n        [[1 2]] [[1. 2.]]\n    \"\"\"\n    # Get the original dimension of the y array\n    original_dim = y.shape[0]\n\n    # Identify rows where all elements in y are finite\n    if y.ndim == 1:\n        ind = np.isfinite(y)\n    elif y.ndim == 2:\n        ind = np.all(np.isfinite(y), axis=0)\n    else:\n        raise ValueError(\"y must be a 1D or 2D array.\")\n\n    # Update X and y by removing rows with NaN in y\n    X_cleaned = X[ind, :]\n    y_cleaned = y[ind, :] if y.ndim == 2 else y[ind]\n\n    # Check if dimensions have been reduced\n    returned_dim = y_cleaned.shape[0]\n    if returned_dim &lt; original_dim:\n        warnings.warn(f\"\\n!!! The dimension of the returned y array is {y_cleaned.shape[0]}, \" f\"which is smaller than the original dimension {original_dim}.\")\n        warnings.warn(\"\\n!!! Check whether to continue with the reduced dimension is useful.\")\n\n    # Throw an error if the returned dimension is smaller than one\n    if returned_dim &lt; 1 and stop_on_zero_return:\n        raise ValueError(\"!!!! The dimension of the returned y array is less than 1. Check the input data.\")\n\n    return X_cleaned, y_cleaned\n</code></pre>"},{"location":"reference/spotpython/utils/repair/#spotpython.utils.repair.repair_non_numeric","title":"<code>repair_non_numeric(X, var_type)</code>","text":"<p>Round non-numeric values to integers. This applies to all variables except for \u201cnum\u201d and \u201cfloat\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array</p> required <code>var_type</code> <code>list</code> <p>list with type information</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: X array with non-numeric values rounded to integers</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.array([[1.2, 2.3], [3.4, 4.5]])\n&gt;&gt;&gt; var_type = [\"num\", \"factor\"]\n&gt;&gt;&gt; repair_non_numeric(X, var_type)\narray([[1., 2.],\n       [3., 4.]])\n</code></pre> Source code in <code>spotpython/utils/repair.py</code> <pre><code>def repair_non_numeric(X: np.ndarray, var_type: List[str]) -&gt; np.ndarray:\n    \"\"\"\n    Round non-numeric values to integers.\n    This applies to all variables except for \"num\" and \"float\".\n\n    Args:\n        X (numpy.ndarray): X array\n        var_type (list): list with type information\n\n    Returns:\n        numpy.ndarray: X array with non-numeric values rounded to integers\n\n    Examples:\n        &gt;&gt;&gt; X = np.array([[1.2, 2.3], [3.4, 4.5]])\n        &gt;&gt;&gt; var_type = [\"num\", \"factor\"]\n        &gt;&gt;&gt; repair_non_numeric(X, var_type)\n        array([[1., 2.],\n               [3., 4.]])\n    \"\"\"\n    mask = np.isin(var_type, [\"num\", \"float\"], invert=True)\n    X[:, mask] = np.around(X[:, mask])\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/","title":"sampling","text":""},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.bestlh","title":"<code>bestlh(n, k, population, iterations, p=1, plot=False, verbosity=0, edges=0, q_list=[1, 2, 5, 10, 20, 50, 100])</code>","text":"<p>Generates an optimized Latin hypercube by evolving the Morris-Mitchell criterion across multiple exponents (q values) and selecting the best plan.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points required in the Latin hypercube.</p> required <code>k</code> <code>int</code> <p>Number of design variables (dimensions).</p> required <code>population</code> <code>int</code> <p>Number of offspring in each generation of the evolutionary search.</p> required <code>iterations</code> <code>int</code> <p>Number of generations for the evolutionary search.</p> required <code>p</code> <code>int</code> <p>The distance norm to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2). Defaults to 1 (faster than 2).</p> <code>1</code> <code>plot</code> <code>bool</code> <p>If True, a scatter plot of the optimized plan in the first two dimensions will be displayed. Only if k&gt;=2.  Defaults to False.</p> <code>False</code> <code>verbosity</code> <code>int</code> <p>Verbosity level. 0 is silent, 1 prints the best q value found. Defaults to 0.</p> <code>0</code> <code>edges</code> <code>int</code> <p>If 1, places centers of the extreme bins at the domain edges ([0,1]). Otherwise, bins are fully contained within the domain, i.e. midpoints. Defaults to 0.</p> <code>0</code> <code>q_list</code> <code>list</code> <p>A list of q values to optimize. Defaults to [1, 2, 5, 10, 20, 50, 100]. These values are used to evaluate the space-fillingness of the Latin hypercube. The best plan is selected based on the lowest mmphi value.</p> <code>[1, 2, 5, 10, 20, 50, 100]</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D array of shape (n, k) representing an optimized Latin hypercube.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.sampling import bestlh\n    bestlh(n=5, k=2, population=5, iterations=10)\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def bestlh(n: int, k: int, population: int, iterations: int, p=1, plot=False, verbosity=0, edges=0, q_list=[1, 2, 5, 10, 20, 50, 100]) -&gt; np.ndarray:\n    \"\"\"\n    Generates an optimized Latin hypercube by evolving the Morris-Mitchell\n    criterion across multiple exponents (q values) and selecting the best plan.\n\n    Args:\n        n (int):\n            Number of points required in the Latin hypercube.\n        k (int):\n            Number of design variables (dimensions).\n        population (int):\n            Number of offspring in each generation of the evolutionary search.\n        iterations (int):\n            Number of generations for the evolutionary search.\n        p (int, optional):\n            The distance norm to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2).\n            Defaults to 1 (faster than 2).\n        plot (bool, optional):\n            If True, a scatter plot of the optimized plan in the first two dimensions\n            will be displayed. Only if k&gt;=2.  Defaults to False.\n        verbosity (int, optional):\n            Verbosity level. 0 is silent, 1 prints the best q value found. Defaults to 0.\n        edges (int, optional):\n            If 1, places centers of the extreme bins at the domain edges ([0,1]).\n            Otherwise, bins are fully contained within the domain, i.e. midpoints.\n            Defaults to 0.\n        q_list (list, optional):\n            A list of q values to optimize. Defaults to [1, 2, 5, 10, 20, 50, 100].\n            These values are used to evaluate the space-fillingness of the Latin\n            hypercube. The best plan is selected based on the lowest mmphi value.\n\n    Returns:\n        np.ndarray:\n            A 2D array of shape (n, k) representing an optimized Latin hypercube.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.sampling import bestlh\n            bestlh(n=5, k=2, population=5, iterations=10)\n    \"\"\"\n    if n &lt; 2:\n        raise ValueError(\"Latin hypercubes require at least 2 points\")\n    if k &lt; 2:\n        raise ValueError(\"Latin hypercubes are not defined for dim k &lt; 2\")\n\n    # A list of exponents (q) to optimize\n\n    # Start with a random Latin hypercube\n    X_start = rlh(n, k, edges=edges)\n\n    # Allocate a 3D array to store the results for each q\n    # (shape: (n, k, number_of_q_values))\n    X3D = np.zeros((n, k, len(q_list)))\n\n    # Evolve the plan for each q in q_list\n    for i, q_val in enumerate(q_list):\n        if verbosity &gt; 0:\n            print(f\"Now optimizing for q={q_val}...\")\n        X3D[:, :, i] = mmlhs(X_start, population, iterations, q_val)\n\n    # Sort the set of evolved plans according to the Morris-Mitchell criterion\n    index_order = mmsort(X3D, p=p)\n\n    # index_order is a 1-based array of plan indices; the first element is the best\n    best_idx = index_order[0] - 1\n    if verbosity &gt; 0:\n        print(f\"Best lh found using q={q_list[best_idx]}...\")\n\n    # The best plan in 3D array order\n    X = X3D[:, :, best_idx]\n\n    # Plot the first two dimensions\n    if plot and (k &gt;= 2):\n        plt.scatter(X[:, 0], X[:, 1], c=\"r\", marker=\"o\")\n        plt.title(f\"Morris-Mitchell optimum plan found using q={q_list[best_idx]}\")\n        plt.xlabel(\"x_1\")\n        plt.ylabel(\"x_2\")\n        plt.grid(True)\n        plt.show()\n\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.fullfactorial","title":"<code>fullfactorial(q, Edges=1)</code>","text":"<p>Generates a full factorial sampling plan in the unit cube.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>list or ndarray</code> <p>A list or array containing the number of points along each dimension (k-vector).</p> required <code>Edges</code> <code>int</code> <p>Determines spacing of points. If <code>Edges=1</code>, points are equally spaced from edge to edge (default). Otherwise, points will be in the centers of n = q[0]q[1]\u2026*q[k-1] bins filling the unit cube.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Full factorial sampling plan as an array of shape (n, k), where n is the total number of points and k is the number of dimensions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any dimension in <code>q</code> is less than 2.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.sampling import fullfactorial\n&gt;&gt;&gt; q = [3, 2]\n&gt;&gt;&gt; X = fullfactorial(q, Edges=0)\n&gt;&gt;&gt; print(X)\n        [[0.         0.        ]\n        [0.         0.75      ]\n        [0.41666667 0.        ]\n        [0.41666667 0.75      ]\n        [0.83333333 0.        ]\n        [0.83333333 0.75      ]]\n&gt;&gt;&gt; X = fullfactorial(q, Edges=1)\n&gt;&gt;&gt; print(X)\n        [[0.  0. ]\n        [0.  1. ]\n        [0.5 0. ]\n        [0.5 1. ]\n        [1.  0. ]\n        [1.  1. ]]\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def fullfactorial(q, Edges=1) -&gt; np.ndarray:\n    \"\"\"Generates a full factorial sampling plan in the unit cube.\n\n    Args:\n        q (list or np.ndarray):\n            A list or array containing the number of points along each dimension (k-vector).\n        Edges (int, optional):\n            Determines spacing of points. If `Edges=1`, points are equally spaced from edge to edge (default).\n            Otherwise, points will be in the centers of n = q[0]*q[1]*...*q[k-1] bins filling the unit cube.\n\n    Returns:\n        (np.ndarray): Full factorial sampling plan as an array of shape (n, k), where n is the total number of points and k is the number of dimensions.\n\n    Raises:\n        ValueError: If any dimension in `q` is less than 2.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.sampling import fullfactorial\n        &gt;&gt;&gt; q = [3, 2]\n        &gt;&gt;&gt; X = fullfactorial(q, Edges=0)\n        &gt;&gt;&gt; print(X)\n                [[0.         0.        ]\n                [0.         0.75      ]\n                [0.41666667 0.        ]\n                [0.41666667 0.75      ]\n                [0.83333333 0.        ]\n                [0.83333333 0.75      ]]\n        &gt;&gt;&gt; X = fullfactorial(q, Edges=1)\n        &gt;&gt;&gt; print(X)\n                [[0.  0. ]\n                [0.  1. ]\n                [0.5 0. ]\n                [0.5 1. ]\n                [1.  0. ]\n                [1.  1. ]]\n\n    \"\"\"\n    q = np.array(q)\n    if np.min(q) &lt; 2:\n        raise ValueError(\"You must have at least two points per dimension.\")\n\n    # Total number of points in the sampling plan\n    n = np.prod(q)\n\n    # Number of dimensions\n    k = len(q)\n\n    # Pre-allocate memory for the sampling plan\n    X = np.zeros((n, k))\n\n    # Additional phantom element\n    q = np.append(q, 1)\n\n    for j in range(k):\n        if Edges == 1:\n            one_d_slice = np.linspace(0, 1, q[j])\n        else:\n            one_d_slice = np.linspace(1 / (2 * q[j]), 1, q[j]) - 1 / (2 * q[j])\n\n        column = np.array([])\n\n        while len(column) &lt; n:\n            for ll in range(q[j]):\n                column = np.append(column, np.ones(np.prod(q[j + 1 : k])) * one_d_slice[ll])\n\n        X[:, j] = column\n\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.jd","title":"<code>jd(X, p=1.0)</code>","text":"<p>Computes and counts the distinct p-norm distances between all pairs of points in X. It returns: 1) A list of distinct distances (sorted), and 2) A corresponding multiplicity array that indicates how often each distance occurs.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array of shape (n, d) representing n points in d-dimensional space.</p> required <code>p</code> <code>float</code> <p>The distance norm to use. p=1 uses the Manhattan (L1) norm, while p=2 uses the Euclidean (L2) norm. Defaults to 1.0 (Manhattan norm).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>A tuple (J, distinct_d), where: - distinct_d is a 1D float array of unique, sorted distances between points. - J is a 1D integer array that provides the multiplicity (occurrence count)   of each distance in distinct_d.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.sampling import jd\n&gt;&gt;&gt; # A small 3-point set in 2D\n&gt;&gt;&gt; X = np.array([[0.0, 0.0],\n...               [1.0, 1.0],\n...               [2.0, 2.0]])\n&gt;&gt;&gt; J, distinct_d = jd(X, p=2.0)\n&gt;&gt;&gt; print(\"Distinct distances:\", distinct_d)\n&gt;&gt;&gt; print(\"Occurrences:\", J)\n# Possible output (using Euclidean norm):\n# Distinct distances: [1.41421356 2.82842712]\n# Occurrences: [1 1]\n# Explanation: Distances are sqrt(2) between consecutive points and 2*sqrt(2) for the farthest pair.\n    Distinct distances: [1.41421356 2.82842712]\n    Occurrences: [2 1]\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def jd(X: np.ndarray, p: float = 1.0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Computes and counts the distinct p-norm distances between all pairs of points in X.\n    It returns:\n    1) A list of distinct distances (sorted), and\n    2) A corresponding multiplicity array that indicates how often each distance occurs.\n\n    Args:\n        X (np.ndarray):\n            A 2D array of shape (n, d) representing n points in d-dimensional space.\n        p (float, optional):\n            The distance norm to use. p=1 uses the Manhattan (L1) norm, while p=2 uses the\n            Euclidean (L2) norm. Defaults to 1.0 (Manhattan norm).\n\n    Returns:\n        (np.ndarray, np.ndarray):\n            A tuple (J, distinct_d), where:\n            - distinct_d is a 1D float array of unique, sorted distances between points.\n            - J is a 1D integer array that provides the multiplicity (occurrence count)\n              of each distance in distinct_d.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.sampling import jd\n        &gt;&gt;&gt; # A small 3-point set in 2D\n        &gt;&gt;&gt; X = np.array([[0.0, 0.0],\n        ...               [1.0, 1.0],\n        ...               [2.0, 2.0]])\n        &gt;&gt;&gt; J, distinct_d = jd(X, p=2.0)\n        &gt;&gt;&gt; print(\"Distinct distances:\", distinct_d)\n        &gt;&gt;&gt; print(\"Occurrences:\", J)\n        # Possible output (using Euclidean norm):\n        # Distinct distances: [1.41421356 2.82842712]\n        # Occurrences: [1 1]\n        # Explanation: Distances are sqrt(2) between consecutive points and 2*sqrt(2) for the farthest pair.\n            Distinct distances: [1.41421356 2.82842712]\n            Occurrences: [2 1]\n    \"\"\"\n    n = X.shape[0]\n\n    # Allocate enough space for all pairwise distances\n    # (n*(n-1))/2 pairs for an n-point set\n    pair_count = n * (n - 1) // 2\n    d = np.zeros(pair_count, dtype=float)\n\n    # Fill the distance array\n    idx = 0\n    for i in range(n - 1):\n        for j in range(i + 1, n):\n            # Compute the p-norm distance\n            d[idx] = np.linalg.norm(X[i] - X[j], ord=p)\n            idx += 1\n\n    # Find unique distances and their multiplicities\n    distinct_d = np.unique(d)\n    J = np.zeros_like(distinct_d, dtype=int)\n    for i, val in enumerate(distinct_d):\n        J[i] = np.sum(d == val)\n\n    return J, distinct_d\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.mm","title":"<code>mm(X1, X2, p=1.0)</code>","text":"<p>Determines which of two sampling plans has better space-filling properties according to the Morris-Mitchell criterion.</p> <p>Parameters:</p> Name Type Description Default <code>X1</code> <code>ndarray</code> <p>A 2D array representing the first sampling plan.</p> required <code>X2</code> <code>ndarray</code> <p>A 2D array representing the second sampling plan.</p> required <code>p</code> <code>float</code> <p>The distance metric. p=1 uses Manhattan (L1) distance, while p=2 uses Euclidean (L2). Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <ul> <li>0 if both plans are identical or equally space-filling</li> <li>1 if X1 is more space-filling</li> <li>2 if X2 is more space-filling</li> </ul> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.sampling import mm\n&gt;&gt;&gt; # Create two 3-point sampling plans in 2D\n&gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n...                [0.5, 0.5],\n...                [0.0, 1.0]])\n&gt;&gt;&gt; X2 = np.array([[0.1, 0.1],\n...                [0.4, 0.6],\n...                [0.1, 0.9]])\n&gt;&gt;&gt; # Compare which plan has better space-filling (Morris-Mitchell)\n&gt;&gt;&gt; better = mm(X1, X2, p=2.0)\n&gt;&gt;&gt; print(better)\n# Prints either 0, 1, or 2 depending on which plan is more space-filling.\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def mm(X1: np.ndarray, X2: np.ndarray, p: Optional[float] = 1.0) -&gt; int:\n    \"\"\"\n    Determines which of two sampling plans has better space-filling properties\n    according to the Morris-Mitchell criterion.\n\n    Args:\n        X1 (np.ndarray): A 2D array representing the first sampling plan.\n        X2 (np.ndarray): A 2D array representing the second sampling plan.\n        p (float, optional): The distance metric. p=1 uses Manhattan (L1) distance,\n            while p=2 uses Euclidean (L2). Defaults to 1.0.\n\n    Returns:\n        int:\n            - 0 if both plans are identical or equally space-filling\n            - 1 if X1 is more space-filling\n            - 2 if X2 is more space-filling\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.sampling import mm\n        &gt;&gt;&gt; # Create two 3-point sampling plans in 2D\n        &gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n        ...                [0.5, 0.5],\n        ...                [0.0, 1.0]])\n        &gt;&gt;&gt; X2 = np.array([[0.1, 0.1],\n        ...                [0.4, 0.6],\n        ...                [0.1, 0.9]])\n        &gt;&gt;&gt; # Compare which plan has better space-filling (Morris-Mitchell)\n        &gt;&gt;&gt; better = mm(X1, X2, p=2.0)\n        &gt;&gt;&gt; print(better)\n        # Prints either 0, 1, or 2 depending on which plan is more space-filling.\n    \"\"\"\n    # Quick check if the sorted sets of points are identical\n    # (mimicking MATLAB's sortrows check)\n    X1_sorted = X1[np.lexsort(np.rot90(X1))]\n    X2_sorted = X2[np.lexsort(np.rot90(X2))]\n    if np.array_equal(X1_sorted, X2_sorted):\n        return 0  # Identical sampling plans\n\n    # Compute distance multiplicities for each plan\n    J1, d1 = jd(X1, p)\n    J2, d2 = jd(X2, p)\n    m1, m2 = len(d1), len(d2)\n\n    # Construct V1 and V2: alternate distance and negative multiplicity\n    V1 = np.zeros(2 * m1)\n    V1[0::2] = d1\n    V1[1::2] = -J1\n\n    V2 = np.zeros(2 * m2)\n    V2[0::2] = d2\n    V2[1::2] = -J2\n\n    # Trim the longer vector to match the size of the shorter\n    m = min(m1, m2)\n    V1 = V1[:m]\n    V2 = V2[:m]\n\n    # Compare element-by-element:\n    # c[i] = 1 if V1[i] &gt; V2[i], 2 if V1[i] &lt; V2[i], 0 otherwise.\n    c = (V1 &gt; V2).astype(int) + 2 * (V1 &lt; V2).astype(int)\n\n    if np.sum(c) == 0:\n        # Equally space-filling\n        return 0\n    else:\n        # The first non-zero entry indicates which plan is better\n        idx = np.argmax(c != 0)\n        return c[idx]\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.mmlhs","title":"<code>mmlhs(X_start, population, iterations, q=2.0, plot=False)</code>","text":"<p>Performs an evolutionary search (using perturbations) to find a Morris-Mitchell optimal Latin hypercube, starting from an initial plan X_start.</p> This function does the following <ol> <li>Initializes a \u201cbest\u201d Latin hypercube (X_best) from the provided X_start.</li> <li>Iteratively perturbs X_best to create offspring.</li> <li>Evaluates the space-fillingness of each offspring via the Morris-Mitchell    metric (using mmphi).</li> <li>Updates the best plan whenever a better offspring is found.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>X_start</code> <code>ndarray</code> <p>A 2D array of shape (n, k) providing the initial Latin hypercube (n points in k dimensions).</p> required <code>population</code> <code>int</code> <p>Number of offspring to create in each generation.</p> required <code>iterations</code> <code>int</code> <p>Total number of generations to run the evolutionary search.</p> required <code>q</code> <code>float</code> <p>The exponent used by the Morris-Mitchell space-filling criterion. Defaults to 2.0.</p> <code>2.0</code> <code>plot</code> <code>bool</code> <p>If True, a simple scatter plot of the first two dimensions will be displayed at each iteration. Only if k &gt;= 2. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D array representing the most space-filling Latin hypercube found after all iterations, of the same shape as X_start.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.sampling import mmlhs\n&gt;&gt;&gt; # Suppose we have an initial 4x2 plan\n&gt;&gt;&gt; X_start = np.array([\n...     [0, 0],\n...     [1, 3],\n...     [2, 1],\n...     [3, 2]\n... ])\n&gt;&gt;&gt; # Search for a more space-filling plan\n&gt;&gt;&gt; X_opt = mmlhs(X_start, population=5, iterations=10, q=2)\n&gt;&gt;&gt; print(\"Optimized plan:\")\n&gt;&gt;&gt; print(X_opt)\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def mmlhs(X_start: np.ndarray, population: int, iterations: int, q: Optional[float] = 2.0, plot=False) -&gt; np.ndarray:\n    \"\"\"\n    Performs an evolutionary search (using perturbations) to find a Morris-Mitchell\n    optimal Latin hypercube, starting from an initial plan X_start.\n\n    This function does the following:\n      1. Initializes a \"best\" Latin hypercube (X_best) from the provided X_start.\n      2. Iteratively perturbs X_best to create offspring.\n      3. Evaluates the space-fillingness of each offspring via the Morris-Mitchell\n         metric (using mmphi).\n      4. Updates the best plan whenever a better offspring is found.\n\n    Args:\n        X_start (np.ndarray):\n            A 2D array of shape (n, k) providing the initial Latin hypercube\n            (n points in k dimensions).\n        population (int):\n            Number of offspring to create in each generation.\n        iterations (int):\n            Total number of generations to run the evolutionary search.\n        q (float, optional):\n            The exponent used by the Morris-Mitchell space-filling criterion.\n            Defaults to 2.0.\n        plot (bool, optional):\n            If True, a simple scatter plot of the first two dimensions will be\n            displayed at each iteration. Only if k &gt;= 2. Defaults to False.\n\n    Returns:\n        np.ndarray:\n            A 2D array representing the most space-filling Latin hypercube found\n            after all iterations, of the same shape as X_start.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.sampling import mmlhs\n        &gt;&gt;&gt; # Suppose we have an initial 4x2 plan\n        &gt;&gt;&gt; X_start = np.array([\n        ...     [0, 0],\n        ...     [1, 3],\n        ...     [2, 1],\n        ...     [3, 2]\n        ... ])\n        &gt;&gt;&gt; # Search for a more space-filling plan\n        &gt;&gt;&gt; X_opt = mmlhs(X_start, population=5, iterations=10, q=2)\n        &gt;&gt;&gt; print(\"Optimized plan:\")\n        &gt;&gt;&gt; print(X_opt)\n    \"\"\"\n    n = X_start.shape[0]\n    if n &lt; 2:\n        raise ValueError(\"Latin hypercubes require at least 2 points\")\n    k = X_start.shape[1]\n    if k &lt; 2:\n        raise ValueError(\"Latin hypercubes are not defined for dim k &lt; 2\")\n\n    # Initialize best plan and its metric\n    X_best = X_start.copy()\n    Phi_best = mmphi(X_best, q=q)\n\n    # After 85% of iterations, reduce the mutation rate to 1\n    leveloff = int(np.floor(0.85 * iterations))\n\n    for it in range(1, iterations + 1):\n        # Decrease number of mutations over time\n        if it &lt; leveloff:\n            mutations = int(round(1 + (0.5 * n - 1) * (leveloff - it) / (leveloff - 1)))\n        else:\n            mutations = 1\n\n        X_improved = X_best.copy()\n        Phi_improved = Phi_best\n\n        # Create offspring, evaluate, and keep the best\n        for _ in range(population):\n            X_try = perturb(X_best.copy(), mutations)\n            Phi_try = mmphi(X_try, q=q)\n\n            if Phi_try &lt; Phi_improved:\n                X_improved = X_try\n                Phi_improved = Phi_try\n\n        # Update the global best if we found a better plan\n        if Phi_improved &lt; Phi_best:\n            X_best = X_improved\n            Phi_best = Phi_improved\n\n        # Simple visualization of the first two dimensions\n        if plot and (X_best.shape[1] &gt;= 2):\n            plt.clf()\n            plt.scatter(X_best[:, 0], X_best[:, 1], marker=\"o\")\n            plt.grid(True)\n            plt.title(f\"Iteration {it} - Current Best Plan\")\n            plt.pause(0.01)\n\n    return X_best\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.mmphi","title":"<code>mmphi(X, q=2.0, p=1.0, verbosity=0)</code>","text":"<p>Calculates the Morris-Mitchell sampling plan quality criterion.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array representing the sampling plan, where each row is a point in d-dimensional space (shape: (n, d)).</p> required <code>q</code> <code>float</code> <p>Exponent used in the computation of the metric. Defaults to 2.0.</p> <code>2.0</code> <code>p</code> <code>float</code> <p>The distance norm to use. For example, p=1 is Manhattan (L1), p=2 is Euclidean (L2). Defaults to 1.0.</p> <code>1.0</code> <code>verbosity</code> <code>int</code> <p>If set to 1, prints additional information about the computation. Defaults to 0 (no additional output).</p> <code>0</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The space-fillingness metric Phiq. Larger values typically indicate a more space-filling plan according to the Morris-Mitchell criterion.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.sampling import mmphi\n&gt;&gt;&gt; # Simple 3-point sampling plan in 2D\n&gt;&gt;&gt; X = np.array([\n...     [0.0, 0.0],\n...     [0.5, 0.5],\n...     [1.0, 1.0]\n... ])\n&gt;&gt;&gt; # Calculate the space-fillingness metric with q=2, using Euclidean distances (p=2)\n&gt;&gt;&gt; quality = mmphi(X, q=2, p=2)\n&gt;&gt;&gt; print(quality)\n# This value indicates how well points are spread out, with smaller being better.\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def mmphi(X: np.ndarray, q: Optional[float] = 2.0, p: Optional[float] = 1.0, verbosity=0) -&gt; float:\n    \"\"\"\n    Calculates the Morris-Mitchell sampling plan quality criterion.\n\n    Args:\n        X (np.ndarray):\n            A 2D array representing the sampling plan, where each row is a point in\n            d-dimensional space (shape: (n, d)).\n        q (float, optional):\n            Exponent used in the computation of the metric. Defaults to 2.0.\n        p (float, optional):\n            The distance norm to use. For example, p=1 is Manhattan (L1),\n            p=2 is Euclidean (L2). Defaults to 1.0.\n        verbosity (int, optional):\n            If set to 1, prints additional information about the computation.\n            Defaults to 0 (no additional output).\n\n    Returns:\n        float:\n            The space-fillingness metric Phiq. Larger values typically indicate a more\n            space-filling plan according to the Morris-Mitchell criterion.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.sampling import mmphi\n        &gt;&gt;&gt; # Simple 3-point sampling plan in 2D\n        &gt;&gt;&gt; X = np.array([\n        ...     [0.0, 0.0],\n        ...     [0.5, 0.5],\n        ...     [1.0, 1.0]\n        ... ])\n        &gt;&gt;&gt; # Calculate the space-fillingness metric with q=2, using Euclidean distances (p=2)\n        &gt;&gt;&gt; quality = mmphi(X, q=2, p=2)\n        &gt;&gt;&gt; print(quality)\n        # This value indicates how well points are spread out, with smaller being better.\n    \"\"\"\n    # check that X has unique rows\n    if X.shape[0] != len(np.unique(X, axis=0)):\n        # issue a warning if there are duplicate rows\n        print(\"Warning: X contains duplicate rows. This may affect the space-fillingness metric.\")\n        # make X unique\n        X = np.unique(X, axis=0)\n    # Compute the distance multiplicities: J, and unique distances: d\n    J, d = jd(X, p)\n    print(f\"J: {J}, d: {d}\") if verbosity &gt; 0 else None\n\n    # Summation of J[i] * d[i]^(-q), then raised to 1/q\n    # This follows the Morris-Mitchell definition.\n    Phiq = np.sum(J * (d ** (-q))) ** (1.0 / q)\n    return Phiq\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.mmphi_intensive","title":"<code>mmphi_intensive(X, q=2.0, p=2.0)</code>","text":"<p>Calculates a size-invariant Morris-Mitchell criterion.</p> <p>This \u201cintensive\u201d version of the criterion allows for the comparison of sampling plans with different sample sizes by normalizing for the number of point pairs. A smaller value indicates a better (more space-filling) design.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array representing the sampling plan (shape: (n, d)).</p> required <code>q</code> <code>float</code> <p>The exponent used in the computation of the metric. Defaults to 2.0.</p> <code>2.0</code> <code>p</code> <code>float</code> <p>The distance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean). Defaults to 2.0.</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The size-invariant space-fillingness metric. Smaller is better.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.sampling import mmphi_intensive\n&gt;&gt;&gt; # Create a simple 3-point sampling plan in 2D\n&gt;&gt;&gt; X = np.array([\n...     [0.0, 0.0],\n...     [0.5, 0.5],\n...     [1.0, 1.0]\n... ])\n&gt;&gt;&gt; # Calculate the intensive space-fillingness metric with q=2, using Euclidean distances (p=2)\n&gt;&gt;&gt; quality = mmphi_intensive(X, q=2, p=2)\n&gt;&gt;&gt; print(quality)\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def mmphi_intensive(X: np.ndarray, q: Optional[float] = 2.0, p: Optional[float] = 2.0) -&gt; float:\n    \"\"\"\n    Calculates a size-invariant Morris-Mitchell criterion.\n\n    This \"intensive\" version of the criterion allows for the comparison of\n    sampling plans with different sample sizes by normalizing for the number\n    of point pairs. A smaller value indicates a better (more space-filling)\n    design.\n\n    Args:\n        X (np.ndarray):\n            A 2D array representing the sampling plan (shape: (n, d)).\n        q (float, optional):\n            The exponent used in the computation of the metric. Defaults to 2.0.\n        p (float, optional):\n            The distance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean).\n            Defaults to 2.0.\n\n    Returns:\n        float:\n            The size-invariant space-fillingness metric. Smaller is better.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.sampling import mmphi_intensive\n        &gt;&gt;&gt; # Create a simple 3-point sampling plan in 2D\n        &gt;&gt;&gt; X = np.array([\n        ...     [0.0, 0.0],\n        ...     [0.5, 0.5],\n        ...     [1.0, 1.0]\n        ... ])\n        &gt;&gt;&gt; # Calculate the intensive space-fillingness metric with q=2, using Euclidean distances (p=2)\n        &gt;&gt;&gt; quality = mmphi_intensive(X, q=2, p=2)\n        &gt;&gt;&gt; print(quality)\n    \"\"\"\n    # Ensure there are no duplicate points\n    if X.shape[0] != len(np.unique(X, axis=0)):\n        X = np.unique(X, axis=0)\n\n    n_points = X.shape[0]\n\n    # The criterion is not well-defined for fewer than 2 points.\n    if n_points &lt; 2:\n        return np.inf\n\n    # Get the unique distances and their multiplicities\n    J, d = jd(X, p=p)\n\n    # If all points are identical, the design is infinitely bad.\n    if d.size == 0:\n        return np.inf\n\n    # Calculate the number of unique pairs of points\n    M = n_points * (n_points - 1) / 2\n\n    try:\n        # Calculate the sum term of the original mmphi\n        sum_term = np.sum(J * (d ** (-q)))\n        # Normalize the sum by M before taking the final root\n        intensive_phiq = (sum_term / M) ** (1.0 / q)\n    except ZeroDivisionError:\n        return np.inf\n    except FloatingPointError:\n        return np.inf\n    except Exception:\n        return np.inf\n\n    return intensive_phiq\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.mmphi_intensive_update","title":"<code>mmphi_intensive_update(X, new_point, J, d, q=2.0, p=2.0)</code>","text":"<p>Updates the Morris-Mitchell intensive criterion for n+1 points by adding a new point to the design.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Existing sampling plan (shape: (n, d)).</p> required <code>new_point</code> <code>ndarray</code> <p>New point to add (shape: (d,)).</p> required <code>J</code> <code>ndarray</code> <p>Multiplicities of distances for the existing design.</p> required <code>d</code> <code>ndarray</code> <p>Unique distances for the existing design.</p> required <code>q</code> <code>float</code> <p>Exponent used in the computation of the metric. Defaults to 2.0.</p> <code>2.0</code> <code>p</code> <code>float</code> <p>Distance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean). Defaults to 2.0.</p> <code>2.0</code> <p>Returns:</p> Type Description <code>tuple[float, ndarray, ndarray]</code> <p>tuple[float, np.ndarray, np.ndarray]: Updated intensive_phiq, updated_J, updated_d.</p> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def mmphi_intensive_update(X: np.ndarray, new_point: np.ndarray, J: np.ndarray, d: np.ndarray, q: float = 2.0, p: float = 2.0) -&gt; tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"\n    Updates the Morris-Mitchell intensive criterion for n+1 points by adding a new point to the design.\n\n    Args:\n        X (np.ndarray): Existing sampling plan (shape: (n, d)).\n        new_point (np.ndarray): New point to add (shape: (d,)).\n        J (np.ndarray): Multiplicities of distances for the existing design.\n        d (np.ndarray): Unique distances for the existing design.\n        q (float): Exponent used in the computation of the metric. Defaults to 2.0.\n        p (float): Distance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean). Defaults to 2.0.\n\n    Returns:\n        tuple[float, np.ndarray, np.ndarray]: Updated intensive_phiq, updated_J, updated_d.\n    \"\"\"\n    n_points = X.shape[0]\n    if n_points &lt; 1:\n        raise ValueError(\"The existing design must contain at least one point.\")\n\n    # Compute distances between the new point and all existing points\n    new_distances = np.array([np.linalg.norm(new_point - X[i], ord=p) for i in range(n_points)])\n\n    # Combine old distances and new distances into a single list\n    all_distances = []\n    for dist, count in zip(d, J):\n        all_distances.extend([dist] * count)\n    all_distances.extend(new_distances)\n\n    # Find unique distances and their counts\n    updated_d, updated_J = np.unique(all_distances, return_counts=True)\n\n    # Calculate the number of unique pairs of points\n    M = (n_points + 1) * n_points / 2\n\n    # Compute the updated intensive_phiq\n    sum_term = np.sum(updated_J * (updated_d ** (-q)))\n    intensive_phiq = (sum_term / M) ** (1.0 / q)\n\n    return intensive_phiq, updated_J, updated_d\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.mmsort","title":"<code>mmsort(X3D, p=1.0)</code>","text":"<p>Ranks multiple sampling plans stored in a 3D array according to the Morris-Mitchell criterion, using a simple bubble sort.</p> <p>Parameters:</p> Name Type Description Default <code>X3D</code> <code>ndarray</code> <p>A 3D NumPy array of shape (n, d, m), where m is the number of sampling plans, and each plan is an (n, d) matrix of points.</p> required <code>p</code> <code>float</code> <p>The distance metric to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2). Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D integer array of length m that holds the plan indices in ascending order of space-filling quality. The first index in the returned array corresponds to the most space-filling plan.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.sampling import mmsort\n&gt;&gt;&gt; # Suppose we have two 3-point sampling plans in 2D, stored in X3D:\n&gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n...                [0.5, 0.5],\n...                [1.0, 1.0]])\n&gt;&gt;&gt; X2 = np.array([[0.2, 0.2],\n...                [0.6, 0.4],\n...                [0.9, 0.9]])\n&gt;&gt;&gt; # Stack them along the third dimension: shape will be (3, 2, 2)\n&gt;&gt;&gt; X3D = np.stack([X1, X2], axis=2)\n&gt;&gt;&gt; # Sort them using the Morris-Mitchell criterion with p=2\n&gt;&gt;&gt; ranking = mmsort(X3D, p=2.0)\n&gt;&gt;&gt; print(ranking)\n# It might print [2 1] or [1 2], depending on which plan is more space-filling.\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def mmsort(X3D: np.ndarray, p: Optional[float] = 1.0) -&gt; np.ndarray:\n    \"\"\"\n    Ranks multiple sampling plans stored in a 3D array according to the\n    Morris-Mitchell criterion, using a simple bubble sort.\n\n    Args:\n        X3D (np.ndarray):\n            A 3D NumPy array of shape (n, d, m), where m is the number of\n            sampling plans, and each plan is an (n, d) matrix of points.\n        p (float, optional):\n            The distance metric to use. p=1 for Manhattan (L1), p=2 for\n            Euclidean (L2). Defaults to 1.0.\n\n    Returns:\n        np.ndarray:\n            A 1D integer array of length m that holds the plan indices in\n            ascending order of space-filling quality. The first index in the\n            returned array corresponds to the most space-filling plan.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.sampling import mmsort\n        &gt;&gt;&gt; # Suppose we have two 3-point sampling plans in 2D, stored in X3D:\n        &gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n        ...                [0.5, 0.5],\n        ...                [1.0, 1.0]])\n        &gt;&gt;&gt; X2 = np.array([[0.2, 0.2],\n        ...                [0.6, 0.4],\n        ...                [0.9, 0.9]])\n        &gt;&gt;&gt; # Stack them along the third dimension: shape will be (3, 2, 2)\n        &gt;&gt;&gt; X3D = np.stack([X1, X2], axis=2)\n        &gt;&gt;&gt; # Sort them using the Morris-Mitchell criterion with p=2\n        &gt;&gt;&gt; ranking = mmsort(X3D, p=2.0)\n        &gt;&gt;&gt; print(ranking)\n        # It might print [2 1] or [1 2], depending on which plan is more space-filling.\n    \"\"\"\n    # Number of plans (m)\n    m = X3D.shape[2]\n\n    # Create index array (1-based to match original MATLAB convention)\n    Index = np.arange(1, m + 1)\n\n    swap_flag = True\n    while swap_flag:\n        swap_flag = False\n        i = 0\n        while i &lt; m - 1:\n            # Compare plan at Index[i] vs. Index[i+1] using mm()\n            # Note: subtract 1 from each index to convert to 0-based array indexing\n            if mm(X3D[:, :, Index[i] - 1], X3D[:, :, Index[i + 1] - 1], p) == 2:\n                # Swap indices if the second plan is more space-filling\n                Index[i], Index[i + 1] = Index[i + 1], Index[i]\n                swap_flag = True\n            i += 1\n\n    return Index\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.perturb","title":"<code>perturb(X, PertNum=1)</code>","text":"<p>Performs a specified number of random element swaps on a sampling plan. If the plan is a Latin hypercube, the result remains a valid Latin hypercube.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array (sampling plan) of shape (n, k), where each row is a point and each column is a dimension.</p> required <code>PertNum</code> <code>int</code> <p>The number of element swaps (perturbations) to perform. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The perturbed sampling plan, identical in shape to the input, with one or more random column swaps executed.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.sampling import perturb\n&gt;&gt;&gt; # Create a simple 4x2 sampling plan\n&gt;&gt;&gt; X_original = np.array([\n...     [1, 3],\n...     [2, 4],\n...     [3, 1],\n...     [4, 2]\n... ])\n&gt;&gt;&gt; # Perturb it once\n&gt;&gt;&gt; X_perturbed = perturb(X_original, PertNum=1)\n&gt;&gt;&gt; print(X_perturbed)\n# The output may differ due to random swaps, but each column is still a permutation of [1,2,3,4].\n    [[1 3]\n    [2 2]\n    [3 1]\n    [4 4]]\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def perturb(X: np.ndarray, PertNum: Optional[int] = 1) -&gt; np.ndarray:\n    \"\"\"\n    Performs a specified number of random element swaps on a sampling plan.\n    If the plan is a Latin hypercube, the result remains a valid Latin hypercube.\n\n    Args:\n        X (np.ndarray):\n            A 2D array (sampling plan) of shape (n, k), where each row is a point\n            and each column is a dimension.\n        PertNum (int, optional):\n            The number of element swaps (perturbations) to perform. Defaults to 1.\n\n    Returns:\n        np.ndarray:\n            The perturbed sampling plan, identical in shape to the input, with\n            one or more random column swaps executed.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.sampling import perturb\n        &gt;&gt;&gt; # Create a simple 4x2 sampling plan\n        &gt;&gt;&gt; X_original = np.array([\n        ...     [1, 3],\n        ...     [2, 4],\n        ...     [3, 1],\n        ...     [4, 2]\n        ... ])\n        &gt;&gt;&gt; # Perturb it once\n        &gt;&gt;&gt; X_perturbed = perturb(X_original, PertNum=1)\n        &gt;&gt;&gt; print(X_perturbed)\n        # The output may differ due to random swaps, but each column is still a permutation of [1,2,3,4].\n            [[1 3]\n            [2 2]\n            [3 1]\n            [4 4]]\n    \"\"\"\n    # Get dimensions of the plan\n    n, k = X.shape\n    if n &lt; 2 or k &lt; 2:\n        raise ValueError(\"Latin hypercubes require at least 2 points and 2 dimensions\")\n\n    for _ in range(PertNum):\n        # Pick a random column\n        col = int(np.floor(np.random.rand() * k))\n\n        # Pick two distinct row indices\n        el1, el2 = 0, 0\n        while el1 == el2:\n            el1 = int(np.floor(np.random.rand() * n))\n            el2 = int(np.floor(np.random.rand() * n))\n\n        # Swap the two selected elements in the chosen column\n        X[el1, col], X[el2, col] = X[el2, col], X[el1, col]\n\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.phisort","title":"<code>phisort(X3D, q=2.0, p=1.0)</code>","text":"<p>Ranks multiple sampling plans stored in a 3D array by the Morris-Mitchell numerical quality metric (mmphi). Uses a simple bubble-sort: sampling plans with smaller mmphi values are placed first in the index array.</p> <p>Parameters:</p> Name Type Description Default <code>X3D</code> <code>ndarray</code> <p>A 3D array of shape (n, d, m), where m is the number of sampling plans.</p> required <code>q</code> <code>float</code> <p>Exponent for the mmphi metric. Defaults to 2.0.</p> <code>2.0</code> <code>p</code> <code>float</code> <p>Distance norm for mmphi. p=1 is Manhattan; p=2 is Euclidean. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D integer array of length m, giving the plan indices in ascending order of mmphi. The first index in the returned array corresponds to the numerically lowest mmphi value.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.sampling import phisort\n    X1 = bestlh(n=5, k=2, population=5, iterations=10)\n    X2 = bestlh(n=5, k=2, population=15, iterations=20)\n    X3 = bestlh(n=5, k=2, population=25, iterations=30)\n    # Map X1 and X2 so that X3D has the two sampling plans in X3D[:, :, 0] and X3D[:, :, 1]\n    X3D = np.array([X1, X2])\n    print(phisort(X3D))\n    X3D = np.array([X3, X2])\n    print(phisort(X3D))\n        [2 1]\n        [1 2]\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def phisort(X3D: np.ndarray, q: Optional[float] = 2.0, p: Optional[float] = 1.0) -&gt; np.ndarray:\n    \"\"\"\n    Ranks multiple sampling plans stored in a 3D array by the Morris-Mitchell\n    numerical quality metric (mmphi). Uses a simple bubble-sort:\n    sampling plans with smaller mmphi values are placed first in the index array.\n\n    Args:\n        X3D (np.ndarray):\n            A 3D array of shape (n, d, m), where m is the number of sampling plans.\n        q (float, optional):\n            Exponent for the mmphi metric. Defaults to 2.0.\n        p (float, optional):\n            Distance norm for mmphi. p=1 is Manhattan; p=2 is Euclidean. Defaults to 1.0.\n\n    Returns:\n        np.ndarray:\n            A 1D integer array of length m, giving the plan indices in ascending\n            order of mmphi. The first index in the returned array corresponds\n            to the numerically lowest mmphi value.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.sampling import phisort\n            X1 = bestlh(n=5, k=2, population=5, iterations=10)\n            X2 = bestlh(n=5, k=2, population=15, iterations=20)\n            X3 = bestlh(n=5, k=2, population=25, iterations=30)\n            # Map X1 and X2 so that X3D has the two sampling plans in X3D[:, :, 0] and X3D[:, :, 1]\n            X3D = np.array([X1, X2])\n            print(phisort(X3D))\n            X3D = np.array([X3, X2])\n            print(phisort(X3D))\n                [2 1]\n                [1 2]\n    \"\"\"\n    # Number of 2D sampling plans\n    m = X3D.shape[2]\n\n    # Create a 1-based index array\n    Index = np.arange(1, m + 1)\n\n    # Bubble-sort: plan with lower mmphi() climbs toward the front\n    swap_flag = True\n    while swap_flag:\n        swap_flag = False\n        for i in range(m - 1):\n            # Retrieve mmphi values for consecutive plans\n            val_i = mmphi(X3D[:, :, Index[i] - 1], q=q, p=p)\n            val_j = mmphi(X3D[:, :, Index[i + 1] - 1], q=q, p=p)\n\n            # Swap if the left plan's mmphi is larger (i.e. 'worse')\n            if val_i &gt; val_j:\n                Index[i], Index[i + 1] = Index[i + 1], Index[i]\n                swap_flag = True\n\n    return Index\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.rlh","title":"<code>rlh(n, k, edges=0)</code>","text":"<p>Generates a random Latin hypercube within the [0,1]^k hypercube.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Desired number of points.</p> required <code>k</code> <code>int</code> <p>Number of design variables (dimensions).</p> required <code>edges</code> <code>int</code> <p>If 1, places centers of the extreme bins at the domain edges ([0,1]). Otherwise, bins are fully contained within the domain, i.e. midpoints. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A Latin hypercube sampling plan of n points in k dimensions,         with each coordinate in the range [0,1].</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.utils.sampling import rlh\n&gt;&gt;&gt; # Generate a 2D Latin hypercube with 5 points and edges=0\n&gt;&gt;&gt; X = rlh(n=5, k=2, edges=0)\n&gt;&gt;&gt; print(X)\n# Example output (values vary due to randomness):\n# [[0.1  0.5 ]\n#  [0.7  0.1 ]\n#  [0.9  0.7 ]\n#  [0.3  0.9 ]\n#  [0.5  0.3 ]]\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def rlh(n: int, k: int, edges: int = 0) -&gt; np.ndarray:\n    \"\"\"\n    Generates a random Latin hypercube within the [0,1]^k hypercube.\n\n    Args:\n        n (int): Desired number of points.\n        k (int): Number of design variables (dimensions).\n        edges (int, optional):\n            If 1, places centers of the extreme bins at the domain edges ([0,1]).\n            Otherwise, bins are fully contained within the domain, i.e. midpoints.\n            Defaults to 0.\n\n    Returns:\n        np.ndarray: A Latin hypercube sampling plan of n points in k dimensions,\n                    with each coordinate in the range [0,1].\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.utils.sampling import rlh\n        &gt;&gt;&gt; # Generate a 2D Latin hypercube with 5 points and edges=0\n        &gt;&gt;&gt; X = rlh(n=5, k=2, edges=0)\n        &gt;&gt;&gt; print(X)\n        # Example output (values vary due to randomness):\n        # [[0.1  0.5 ]\n        #  [0.7  0.1 ]\n        #  [0.9  0.7 ]\n        #  [0.3  0.9 ]\n        #  [0.5  0.3 ]]\n\n    \"\"\"\n    # Initialize array\n    X = np.zeros((n, k), dtype=float)\n\n    # Fill with random permutations\n    for i in range(k):\n        X[:, i] = np.random.permutation(n)\n\n    # Adjust normalization based on the edges flag\n    if edges == 1:\n        # [X=0..n-1] -&gt; [0..1]\n        X = X / (n - 1)\n    else:\n        # Points at true midpoints\n        # [X=0..n-1] -&gt; [0.5/n..(n-0.5)/n]\n        X = (X + 0.5) / n\n\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/sampling/#spotpython.utils.sampling.subset","title":"<code>subset(X, ns)</code>","text":"<p>Returns a space-filling subset of a given size from a sampling plan, along with the remainder. It repeatedly attempts to substitute each point in the subset with a point from the remainder if doing so improves the Morris-Mitchell metric.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array representing the original sampling plan, of shape (n, d).</p> required <code>ns</code> <code>int</code> <p>The size of the desired subset.</p> required <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>A tuple (Xs, Xr) where: - Xs is the chosen subset of size ns, with space-filling properties. - Xr is the remainder (X \\ Xs).</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.sampling import subset, bestlh\n    X = bestlh(n=5, k=3, population=5, iterations=10)\n    Xs, Xr = subset(X, ns=2)\n    print(Xs)\n    print(Xr)\n        [[0.25 0.   0.5 ]\n        [0.5  0.75 0.  ]]\n        [[1.   0.25 0.25]\n        [0.   1.   0.75]\n        [0.75 0.5  1.  ]]\n</code></pre> Source code in <code>spotpython/utils/sampling.py</code> <pre><code>def subset(X: np.ndarray, ns: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns a space-filling subset of a given size from a sampling plan, along with\n    the remainder. It repeatedly attempts to substitute each point in the subset\n    with a point from the remainder if doing so improves the Morris-Mitchell metric.\n\n    Args:\n        X (np.ndarray):\n            A 2D array representing the original sampling plan, of shape (n, d).\n        ns (int):\n            The size of the desired subset.\n\n    Returns:\n        (np.ndarray, np.ndarray):\n            A tuple (Xs, Xr) where:\n            - Xs is the chosen subset of size ns, with space-filling properties.\n            - Xr is the remainder (X \\\\ Xs).\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.sampling import subset, bestlh\n            X = bestlh(n=5, k=3, population=5, iterations=10)\n            Xs, Xr = subset(X, ns=2)\n            print(Xs)\n            print(Xr)\n                [[0.25 0.   0.5 ]\n                [0.5  0.75 0.  ]]\n                [[1.   0.25 0.25]\n                [0.   1.   0.75]\n                [0.75 0.5  1.  ]]\n    \"\"\"\n    # Number of total points\n    n = X.shape[0]\n\n    # Morris-Mitchell parameters\n    p = 1\n    q = 5\n\n    # Create a random permutation of row indices\n    r = np.random.permutation(n)\n\n    # Initial subset and remainder\n    Xs = X[r[:ns], :].copy()\n    Xr = X[r[ns:], :].copy()\n\n    # Attempt to improve space-filling by swapping points\n    for j in range(ns):\n        orig_crit = mmphi(Xs, q=q, p=p)\n        orig_point = Xs[j, :].copy()\n\n        # Track best substitution index and metric\n        bestsub = 0\n        bestsubcrit = np.inf\n\n        # Try replacing Xs[j] with each candidate in Xr\n        for i in range(n - ns):\n            Xs[j, :] = Xr[i, :]\n            crit = mmphi(Xs, q=q, p=p)\n            if crit &lt; bestsubcrit:\n                bestsubcrit = crit\n                bestsub = i\n\n        # If a better subset is found, swap permanently\n        if bestsubcrit &lt; orig_crit:\n            Xs[j, :] = Xr[bestsub, :].copy()\n            Xr[bestsub, :] = orig_point\n        else:\n            Xs[j, :] = orig_point\n\n    return Xs, Xr\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/","title":"scaler","text":""},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchMinMaxScaler","title":"<code>TorchMinMaxScaler</code>","text":"<p>A class for scaling data using min-max normalization with PyTorch tensors. This scaler calculates the minimum and maximum values in the dataset to scale the data within a given range.</p> <p>Attributes:</p> Name Type Description <code>min</code> <code>Tensor</code> <p>The minimum values computed over the fitted data.</p> <code>max</code> <code>Tensor</code> <p>The maximum values computed over the fitted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotpython.utils.scaler import TorchMinMaxScaler\n&gt;&gt;&gt; scaler = TorchMinMaxScaler()\n# Given a tensor\n&gt;&gt;&gt; tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n# Fit and transform the tensor using the scaler\n&gt;&gt;&gt; scaled_tensor = scaler.fit_transform(tensor)\n&gt;&gt;&gt; print(scaled_tensor)\n# The output will be a tensor with values scaled between 0 and 1.\n</code></pre> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>class TorchMinMaxScaler:\n    \"\"\"\n    A class for scaling data using min-max normalization with PyTorch tensors.\n    This scaler calculates the minimum and maximum values in the dataset to scale the data within a given range.\n\n    Attributes:\n        min (torch.Tensor): The minimum values computed over the fitted data.\n        max (torch.Tensor): The maximum values computed over the fitted data.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from spotpython.utils.scaler import TorchMinMaxScaler\n        &gt;&gt;&gt; scaler = TorchMinMaxScaler()\n        # Given a tensor\n        &gt;&gt;&gt; tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n        # Fit and transform the tensor using the scaler\n        &gt;&gt;&gt; scaled_tensor = scaler.fit_transform(tensor)\n        &gt;&gt;&gt; print(scaled_tensor)\n        # The output will be a tensor with values scaled between 0 and 1.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the TorchMinMaxScaler class without any predefined min and max.\n        \"\"\"\n        self.min = None\n        self.max = None\n\n    def fit(self, x: torch.Tensor) -&gt; None:\n        \"\"\"\n        Compute the minimum and maximum value of the input tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise TypeError(\"Input should be a torch tensor\")\n        self.min = x.min(dim=0, keepdim=True).values\n        self.max = x.max(dim=0, keepdim=True).values\n\n    def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Scale the input tensor using the computed minimum and maximum values.\n\n        Args:\n            x (torch.Tensor): The input tensor to be scaled.\n\n        Returns:\n            torch.Tensor: The scaled tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n            RuntimeError: If the scaler has not been fitted before transforming data.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise TypeError(\"Input should be a torch tensor\")\n        if self.min is None or self.max is None:\n            raise RuntimeError(\"Must fit scaler before transforming data\")\n        x = (x - self.min) / (self.max - self.min + 1e-7)\n        return x\n\n    def fit_transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Fit the scaler to the input tensor and then scale the tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The scaled tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n        \"\"\"\n        self.fit(x)\n        return self.transform(x)\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchMinMaxScaler.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the TorchMinMaxScaler class without any predefined min and max.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the TorchMinMaxScaler class without any predefined min and max.\n    \"\"\"\n    self.min = None\n    self.max = None\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchMinMaxScaler.fit","title":"<code>fit(x)</code>","text":"<p>Compute the minimum and maximum value of the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def fit(self, x: torch.Tensor) -&gt; None:\n    \"\"\"\n    Compute the minimum and maximum value of the input tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n    \"\"\"\n    if not torch.is_tensor(x):\n        raise TypeError(\"Input should be a torch tensor\")\n    self.min = x.min(dim=0, keepdim=True).values\n    self.max = x.max(dim=0, keepdim=True).values\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchMinMaxScaler.fit_transform","title":"<code>fit_transform(x)</code>","text":"<p>Fit the scaler to the input tensor and then scale the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The scaled tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def fit_transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Fit the scaler to the input tensor and then scale the tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The scaled tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n    \"\"\"\n    self.fit(x)\n    return self.transform(x)\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchMinMaxScaler.transform","title":"<code>transform(x)</code>","text":"<p>Scale the input tensor using the computed minimum and maximum values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to be scaled.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The scaled tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> <code>RuntimeError</code> <p>If the scaler has not been fitted before transforming data.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Scale the input tensor using the computed minimum and maximum values.\n\n    Args:\n        x (torch.Tensor): The input tensor to be scaled.\n\n    Returns:\n        torch.Tensor: The scaled tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n        RuntimeError: If the scaler has not been fitted before transforming data.\n    \"\"\"\n    if not torch.is_tensor(x):\n        raise TypeError(\"Input should be a torch tensor\")\n    if self.min is None or self.max is None:\n        raise RuntimeError(\"Must fit scaler before transforming data\")\n    x = (x - self.min) / (self.max - self.min + 1e-7)\n    return x\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchStandardScaler","title":"<code>TorchStandardScaler</code>","text":"<p>A class for scaling data using standardization with torch tensors. This scaler computes the mean and standard deviation on a dataset so that it can later be used to scale the data using the computed mean and standard deviation.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>Tensor</code> <p>The mean value computed over the fitted data.</p> <code>std</code> <code>Tensor</code> <p>The standard deviation computed over the fitted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotpython.utils.scaler import TorchStandardScaler\n# Create a sample tensor\n&gt;&gt;&gt; tensor = torch.rand((10, 3))  # Random tensor with shape (10, 3)\n&gt;&gt;&gt; scaler = TorchStandardScaler()\n# Fit the scaler to the data\n&gt;&gt;&gt; scaler.fit(tensor)\n# Transform the data using the fitted scaler\n&gt;&gt;&gt; transformed_tensor = scaler.transform(tensor)\n&gt;&gt;&gt; print(transformed_tensor)\n# Using fit_transform method to fit and transform in one step\n&gt;&gt;&gt; another_tensor = torch.rand((10, 3))\n&gt;&gt;&gt; scaled_tensor = scaler.fit_transform(another_tensor)\n&gt;&gt;&gt; print(scaled_tensor)\n</code></pre> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>class TorchStandardScaler:\n    \"\"\"\n    A class for scaling data using standardization with torch tensors.\n    This scaler computes the mean and standard deviation on a dataset so that\n    it can later be used to scale the data using the computed mean and standard deviation.\n\n    Attributes:\n        mean (torch.Tensor): The mean value computed over the fitted data.\n        std (torch.Tensor): The standard deviation computed over the fitted data.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from spotpython.utils.scaler import TorchStandardScaler\n        # Create a sample tensor\n        &gt;&gt;&gt; tensor = torch.rand((10, 3))  # Random tensor with shape (10, 3)\n        &gt;&gt;&gt; scaler = TorchStandardScaler()\n        # Fit the scaler to the data\n        &gt;&gt;&gt; scaler.fit(tensor)\n        # Transform the data using the fitted scaler\n        &gt;&gt;&gt; transformed_tensor = scaler.transform(tensor)\n        &gt;&gt;&gt; print(transformed_tensor)\n        # Using fit_transform method to fit and transform in one step\n        &gt;&gt;&gt; another_tensor = torch.rand((10, 3))\n        &gt;&gt;&gt; scaled_tensor = scaler.fit_transform(another_tensor)\n        &gt;&gt;&gt; print(scaled_tensor)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the TorchStandardScaler class without any pre-defined mean and std.\n        \"\"\"\n        self.mean = None\n        self.std = None\n\n    def fit(self, x: torch.Tensor) -&gt; None:\n        \"\"\"\n        Compute the mean and standard deviation of the input tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise TypeError(\"Input should be a torch tensor\")\n        self.mean = x.mean(dim=0, keepdim=True)\n        self.std = x.std(dim=0, unbiased=False, keepdim=True)\n\n    def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Scale the input tensor using the computed mean and standard deviation.\n\n        Args:\n            x (torch.Tensor): The input tensor to be transformed, expected shape [n_samples, n_features]\n\n        Returns:\n            torch.Tensor: The scaled tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n            RuntimeError: If the scaler has not been fitted before transforming data.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise TypeError(\"Input should be a torch tensor\")\n        if self.mean is None or self.std is None:\n            raise RuntimeError(\"Must fit scaler before transforming data\")\n        x = (x - self.mean) / (self.std + 1e-7)\n        return x\n\n    def fit_transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Fit the scaler to the input tensor and then scale the tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n        Returns:\n            torch.Tensor: The scaled tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n        \"\"\"\n        self.fit(x)\n        return self.transform(x)\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchStandardScaler.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the TorchStandardScaler class without any pre-defined mean and std.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the TorchStandardScaler class without any pre-defined mean and std.\n    \"\"\"\n    self.mean = None\n    self.std = None\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchStandardScaler.fit","title":"<code>fit(x)</code>","text":"<p>Compute the mean and standard deviation of the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor, expected shape [n_samples, n_features]</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def fit(self, x: torch.Tensor) -&gt; None:\n    \"\"\"\n    Compute the mean and standard deviation of the input tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n    \"\"\"\n    if not torch.is_tensor(x):\n        raise TypeError(\"Input should be a torch tensor\")\n    self.mean = x.mean(dim=0, keepdim=True)\n    self.std = x.std(dim=0, unbiased=False, keepdim=True)\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchStandardScaler.fit_transform","title":"<code>fit_transform(x)</code>","text":"<p>Fit the scaler to the input tensor and then scale the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor, expected shape [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The scaled tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def fit_transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Fit the scaler to the input tensor and then scale the tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n    Returns:\n        torch.Tensor: The scaled tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n    \"\"\"\n    self.fit(x)\n    return self.transform(x)\n</code></pre>"},{"location":"reference/spotpython/utils/scaler/#spotpython.utils.scaler.TorchStandardScaler.transform","title":"<code>transform(x)</code>","text":"<p>Scale the input tensor using the computed mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to be transformed, expected shape [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The scaled tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> <code>RuntimeError</code> <p>If the scaler has not been fitted before transforming data.</p> Source code in <code>spotpython/utils/scaler.py</code> <pre><code>def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Scale the input tensor using the computed mean and standard deviation.\n\n    Args:\n        x (torch.Tensor): The input tensor to be transformed, expected shape [n_samples, n_features]\n\n    Returns:\n        torch.Tensor: The scaled tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n        RuntimeError: If the scaler has not been fitted before transforming data.\n    \"\"\"\n    if not torch.is_tensor(x):\n        raise TypeError(\"Input should be a torch tensor\")\n    if self.mean is None or self.std is None:\n        raise RuntimeError(\"Must fit scaler before transforming data\")\n    x = (x - self.mean) / (self.std + 1e-7)\n    return x\n</code></pre>"},{"location":"reference/spotpython/utils/seed/","title":"seed","text":""},{"location":"reference/spotpython/utils/seed/#spotpython.utils.seed.set_all_seeds","title":"<code>set_all_seeds(seed)</code>","text":"<p>Set the seed for all relevant random number generators to ensure reproducibility. This function sets the seed for Python\u2019s built-in <code>random</code> module, NumPy, and PyTorch\u2019s CPU and GPU (CUDA) random number generators. It also configures PyTorch\u2019s settings to improve the reproducibility of experiments, which is crucial when debugging or comparing model performances.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The seed value to be set for all random number generators.</p> required Example <p>from spotpython.utils.seed import set_all_seeds set_all_seeds(42)</p> Notes <ul> <li>Setting <code>torch.backends.cudnn.deterministic</code> to <code>True</code> can make computations   more reproducible but at the potential cost of performance.</li> <li>Additional considerations may be necessary for complete reproducibility   in distributed or multi-threaded setups.</li> </ul> Source code in <code>spotpython/utils/seed.py</code> <pre><code>def set_all_seeds(seed: int):\n    \"\"\"Set the seed for all relevant random number generators to ensure reproducibility.\n    This function sets the seed for Python's built-in `random` module, NumPy,\n    and PyTorch's CPU and GPU (CUDA) random number generators. It also configures\n    PyTorch's settings to improve the reproducibility of experiments, which is\n    crucial when debugging or comparing model performances.\n\n    Args:\n        seed (int): The seed value to be set for all random number generators.\n\n    Example:\n        &gt;&gt;&gt; from spotpython.utils.seed import set_all_seeds\n        &gt;&gt;&gt; set_all_seeds(42)\n        &gt;&gt;&gt; # Proceed with model initialization or data processing to ensure results can be reproduced\n        &gt;&gt;&gt; model = SomeModel()  # Replace with actual model\n        &gt;&gt;&gt; train_model(model)   # Replace with actual training function\n\n    Notes:\n        - Setting `torch.backends.cudnn.deterministic` to `True` can make computations\n          more reproducible but at the potential cost of performance.\n        - Additional considerations may be necessary for complete reproducibility\n          in distributed or multi-threaded setups.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True  # Improvements for reproducibility\n        torch.backends.cudnn.benchmark = False\n\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # Ensuring hash-based functions use a consistent seed\n</code></pre>"},{"location":"reference/spotpython/utils/seed/#spotpython.utils.seed.set_all_seeds--proceed-with-model-initialization-or-data-processing-to-ensure-results-can-be-reproduced","title":"Proceed with model initialization or data processing to ensure results can be reproduced","text":"<p>model = SomeModel()  # Replace with actual model train_model(model)   # Replace with actual training function</p>"},{"location":"reference/spotpython/utils/split/","title":"split","text":""},{"location":"reference/spotpython/utils/split/#spotpython.utils.split.calculate_data_split","title":"<code>calculate_data_split(test_size, full_size, verbosity=0, stage=None)</code>","text":"<p>Calculates the split sizes for training, validation, and test datasets. Returns a tuple containing the sizes (full_train_size, val_size, train_size, test_size), where full_train_size is the size of the full dataset minus the test set.</p> Note <p>The first return value is full_train_size, i.e., the size of the full dataset minus the test set.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>float or int</code> <p>The size of the test set. Can be a float for proportion or an int for absolute number of test samples.</p> required <code>full_size</code> <code>int</code> <p>The size of the full dataset.</p> required <code>verbosity</code> <code>int</code> <p>The level of verbosity for debug output. Defaults to 0.</p> <code>0</code> <code>stage</code> <code>str</code> <p>The stage of setup, for debug output if needed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the sizes (full_train_size, val_size, train_size, test_size).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.split import calculate_data_split\n    # Using proportion for test size\n    calculate_data_split(0.2, 1000)\n        (0.8, 0.16, 0.64, 0.2)\n    # Using absolute number for test size\n    calculate_data_split(200, 1000)\n        (800, 160, 640, 200)\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sizes are not correct, i.e., full_size != train_size + val_size + test_size.</p> Source code in <code>spotpython/utils/split.py</code> <pre><code>def calculate_data_split(test_size, full_size, verbosity=0, stage=None) -&gt; tuple:\n    \"\"\"\n    Calculates the split sizes for training, validation, and test datasets.\n    Returns a tuple containing the sizes (full_train_size, val_size, train_size, test_size),\n    where full_train_size is the size of the full dataset minus the test set.\n\n    Note:\n        The first return value is full_train_size, i.e.,\n        the size of the full dataset minus the test set.\n\n    Args:\n        test_size (float or int):\n            The size of the test set.\n            Can be a float for proportion or an int for absolute number of test samples.\n        full_size (int):\n            The size of the full dataset.\n        verbosity (int, optional):\n            The level of verbosity for debug output. Defaults to 0.\n        stage (str, optional):\n            The stage of setup, for debug output if needed.\n\n    Returns:\n        tuple: A tuple containing the sizes (full_train_size, val_size, train_size, test_size).\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.split import calculate_data_split\n            # Using proportion for test size\n            calculate_data_split(0.2, 1000)\n                (0.8, 0.16, 0.64, 0.2)\n            # Using absolute number for test size\n            calculate_data_split(200, 1000)\n                (800, 160, 640, 200)\n\n    Raises:\n        ValueError: If the sizes are not correct, i.e., full_size != train_size + val_size + test_size.\n    \"\"\"\n    if isinstance(test_size, float):\n        full_train_size = round(1.0 - test_size, 2)\n        val_size = round(full_train_size * test_size, 2)\n        train_size = 1.0 - test_size - val_size\n        # check if the sizes are correct, i.e., 1.0 = train_size + val_size + test_size\n        if full_train_size + test_size != 1.0:\n            raise ValueError(f\"full_size ({full_size}) != full_train_size ({full_train_size}) + test_size ({test_size})\")\n    else:\n        # test_size is considered an int, training size calculation directly based on it\n        # everything is calculated as an int\n        # return values are also ints\n        # check if test_size does not exceed full_size\n        if test_size &gt; full_size:\n            raise ValueError(f\"test_size ({test_size}) &gt; full_size ({full_size})\")\n        full_train_size = full_size - test_size\n        val_size = int(full_train_size * test_size / full_size)\n        train_size = full_train_size - val_size\n        # check if the sizes are correct, i.e., full_size = train_size + val_size + test_size\n        if train_size + val_size + test_size != full_size:\n            raise ValueError(f\"full_size ({full_size}) != full_train_size ({full_train_size}) + test_size ({test_size})\")\n\n    if verbosity &gt; 0:\n        print(f\"stage: {stage}\")\n    if verbosity &gt; 1:\n        print(f\"full_sizefull_train_size: {full_train_size}\")\n        print(f\"full_sizeval_size: {val_size}\")\n        print(f\"full_sizetrain_size: {train_size}\")\n        print(f\"full_sizetest_size: {test_size}\")\n\n    return full_train_size, val_size, train_size, test_size\n</code></pre>"},{"location":"reference/spotpython/utils/split/#spotpython.utils.split.compute_lengths_from_fractions","title":"<code>compute_lengths_from_fractions(fractions, dataset_length)</code>","text":"<p>Compute lengths of dataset splits from given fractions.</p> <p>Given a list of fractions that sum up to 1, compute the lengths of each corresponding partition of a dataset with a specified length. Each length is determined as <code>floor(frac * dataset_length)</code>. Any remaining items (due to flooring) are distributed among the partitions in a round-robin fashion.</p> <p>Parameters:</p> Name Type Description Default <code>fractions</code> <code>List[float]</code> <p>A list of fractions that should sum to 1.</p> required <code>dataset_length</code> <code>int</code> <p>The length of the dataset.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: A list of lengths corresponding to each fraction.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the fractions do not sum to 1.</p> <code>ValueError</code> <p>If any fraction is outside the range [0, 1].</p> <code>ValueError</code> <p>If the sum of computed lengths does not equal the dataset length.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.split import compute_lengths_from_fractions\n&gt;&gt;&gt; dataset_length = 5\n&gt;&gt;&gt; fractions = [0.2, 0.3, 0.5]\n&gt;&gt;&gt; compute_lengths_from_fractions(fractions, dataset_length)\n[1, 1, 3]\n</code></pre> <p>In this example, \u2018dataset_length\u2019 is 5 and the \u2018fractions\u2019 specify the desired size distribution. The function calculates partitions of lengths [1, 1, 3] based on the given fractions.</p> Source code in <code>spotpython/utils/split.py</code> <pre><code>def compute_lengths_from_fractions(fractions: List[float], dataset_length: int) -&gt; List[int]:\n    \"\"\"Compute lengths of dataset splits from given fractions.\n\n    Given a list of fractions that sum up to 1, compute the lengths of each\n    corresponding partition of a dataset with a specified length. Each length is\n    determined as `floor(frac * dataset_length)`. Any remaining items (due to flooring)\n    are distributed among the partitions in a round-robin fashion.\n\n    Args:\n        fractions (List[float]): A list of fractions that should sum to 1.\n        dataset_length (int): The length of the dataset.\n\n    Returns:\n        List[int]: A list of lengths corresponding to each fraction.\n\n    Raises:\n        ValueError: If the fractions do not sum to 1.\n        ValueError: If any fraction is outside the range [0, 1].\n        ValueError: If the sum of computed lengths does not equal the dataset length.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.split import compute_lengths_from_fractions\n        &gt;&gt;&gt; dataset_length = 5\n        &gt;&gt;&gt; fractions = [0.2, 0.3, 0.5]\n        &gt;&gt;&gt; compute_lengths_from_fractions(fractions, dataset_length)\n        [1, 1, 3]\n\n        In this example, 'dataset_length' is 5 and the 'fractions' specify the\n        desired size distribution. The function calculates partitions of lengths\n        [1, 1, 3] based on the given fractions.\n\n    \"\"\"\n    if not math.isclose(sum(fractions), 1) or sum(fractions) &gt; 1:\n        raise ValueError(\"Fractions must sum up to 1.\")\n\n    subset_lengths: List[int] = []\n    for i, frac in enumerate(fractions):\n        if frac &lt; 0 or frac &gt; 1:\n            raise ValueError(f\"Fraction at index {i} is not between 0 and 1\")\n        n_items_in_split = int(math.floor(dataset_length * frac))\n        subset_lengths.append(n_items_in_split)\n\n    remainder = dataset_length - sum(subset_lengths)\n\n    # Add 1 to all the lengths in a round-robin fashion until the remainder is 0\n    for i in range(remainder):\n        idx_to_add_at = i % len(subset_lengths)\n        subset_lengths[idx_to_add_at] += 1\n\n    lengths = subset_lengths\n    for i, length in enumerate(lengths):\n        if length == 0:\n            warnings.warn(f\"Length of split at index {i} is 0. \" f\"This might result in an empty dataset.\")\n\n    if sum(lengths) != dataset_length:\n        raise ValueError(\"Sum of computed lengths does not equal the input dataset length!\")\n\n    return lengths\n</code></pre>"},{"location":"reference/spotpython/utils/stats/","title":"stats","text":""},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.compute_coefficients_table","title":"<code>compute_coefficients_table(model, X_encoded, y, vif_table=None)</code>","text":"Compute a coefficients table containing <ol> <li>Variable name</li> <li>Zero-order correlation</li> <li>Partial correlation</li> <li>Semipartial (part) correlation</li> <li>Tolerance (1 / VIF)</li> <li>VIF</li> </ol> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>RegressionResultsWrapper</code> <p>A fitted OLS model from statsmodels.</p> required <code>X_encoded</code> <code>DataFrame</code> <p>The DataFrame used to fit the model, including \u2018const\u2019.</p> required <code>y</code> <code>Series</code> <p>Dependent variable used in fitting the model.</p> required <code>vif_table</code> <code>DataFrame</code> <p>A DataFrame with columns [\u201cfeature\u201d, \u201cVIF\u201d] for each column in X_encoded (typ. from statsmodels.stats.outliers_influence.variance_inflation_factor). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame with columns: - \u201cVariable\u201d - \u201cZero-Order r\u201d - \u201cPartial r\u201d - \u201cSemipartial r\u201d - \u201cTolerance\u201d - \u201cVIF\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import compute_coefficients_table\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; X = sm.add_constant(data)\n&gt;&gt;&gt; model = sm.OLS(y, X).fit()\n&gt;&gt;&gt; vif_table = pd.DataFrame({\n...     'feature': ['x1', 'x2', 'x3'],\n...     'VIF': [1, 2, 3]\n... })\n&gt;&gt;&gt; compute_coefficients_table(model, data, y, vif_table)\n   Variable  Zero-Order r  Partial r  Semipartial r  Tolerance  VIF\n0       x1           0.0        0.0            0.0        1.0  1.0\n1       x2           0.0        0.0            0.0        0.5  2.0\n2       x3           0.0        0.0            0.0        0.333333  3.0\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def compute_coefficients_table(model, X_encoded, y, vif_table=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute a coefficients table containing:\n      1. Variable name\n      2. Zero-order correlation\n      3. Partial correlation\n      4. Semipartial (part) correlation\n      5. Tolerance (1 / VIF)\n      6. VIF\n\n    Args:\n        model (statsmodels.regression.linear_model.RegressionResultsWrapper):\n            A fitted OLS model from statsmodels.\n        X_encoded (pd.DataFrame):\n            The DataFrame used to fit the model, including 'const'.\n        y (pd.Series):\n            Dependent variable used in fitting the model.\n        vif_table (pd.DataFrame):\n            A DataFrame with columns [\"feature\", \"VIF\"] for each column in X_encoded\n            (typ. from statsmodels.stats.outliers_influence.variance_inflation_factor).\n            Default is None.\n\n    Returns:\n        pd.DataFrame with columns:\n            - \"Variable\"\n            - \"Zero-Order r\"\n            - \"Partial r\"\n            - \"Semipartial r\"\n            - \"Tolerance\"\n            - \"VIF\"\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import compute_coefficients_table\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import statsmodels.api as sm\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'x1': [1, 2, 3, 4, 5],\n        ...     'x2': [2, 4, 6, 8, 10],\n        ...     'x3': [1, 3, 5, 7, 9]\n        ... })\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; X = sm.add_constant(data)\n        &gt;&gt;&gt; model = sm.OLS(y, X).fit()\n        &gt;&gt;&gt; vif_table = pd.DataFrame({\n        ...     'feature': ['x1', 'x2', 'x3'],\n        ...     'VIF': [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; compute_coefficients_table(model, data, y, vif_table)\n           Variable  Zero-Order r  Partial r  Semipartial r  Tolerance  VIF\n        0       x1           0.0        0.0            0.0        1.0  1.0\n        1       x2           0.0        0.0            0.0        0.5  2.0\n        2       x3           0.0        0.0            0.0        0.333333  3.0\n\n    \"\"\"\n\n    # Full-model R^2 and residual df\n    r2_full = model.rsquared\n\n    # We want to iterate over each predictor except the intercept\n    predictors = [col for col in X_encoded.columns if col != \"const\"]\n\n    results = []\n\n    for var in predictors:\n        # -------------------------------------------------------------------\n        # 1) Zero-order correlation: Pearson correlation of var with y\n        # -------------------------------------------------------------------\n        zero_order_r = X_encoded[var].corr(y)\n\n        # -------------------------------------------------------------------\n        # 2) Partial Correlation &amp; 3) Semipartial Correlation\n        #    We compare a 'full' model vs. a 'reduced' model (without var)\n        # -------------------------------------------------------------------\n        X_reduced = X_encoded.drop(columns=[var])\n        reduced_model = sm.OLS(y, X_reduced).fit()\n        r2_reduced = reduced_model.rsquared\n\n        # The difference in R^2 contributed by this predictor\n        delta_r2 = r2_full - r2_reduced\n\n        # Determine sign from the unstandardized coefficient in the full model\n        coeff_sign = np.sign(model.params.get(var, 0.0))\n\n        # If numeric issues occur (e.g., delta_r2 &lt; 0), set correlations to NaN\n        if delta_r2 &lt;= 0.0 or (1 - r2_reduced) &lt;= 0.0:\n            partial_r = np.nan\n            semipartial_r = np.nan\n        else:\n            # partial correlation\n            # partial_r\u00b2 = (R\u00b2_full - R\u00b2_reduced) / (1 - R\u00b2_reduced)\n            partial_r = coeff_sign * np.sqrt(delta_r2 / (1 - r2_reduced))\n\n            # semipartial correlation (also called part correlation)\n            # semipartial_r\u00b2 = (R\u00b2_full - R\u00b2_reduced)\n            # By definition, semipartial_r = sqrt( delta_r2 ), but we treat R\u00b2 as a fraction\n            # Because the base R\u00b2 is SSR / TSS, so:\n            semipartial_r = coeff_sign * np.sqrt(delta_r2)\n\n        # -------------------------------------------------------------------\n        # 4) Tolerance &amp; 5) VIF\n        # -------------------------------------------------------------------\n        if vif_table is None:\n            vif_table = vif(X_encoded)\n            # results.append({\"Variable\": var, \"Zero-Order r\": zero_order_r, \"Partial r\": partial_r, \"Semipartial r\": semipartial_r})\n        # Get the VIF for this predictor\n        vif_row = vif_table.loc[vif_table[\"feature\"] == var, \"VIF\"]\n        if len(vif_row) == 0:\n            var_vif = np.nan\n        else:\n            var_vif = vif_row.iloc[0]\n        if var_vif &lt;= 0 or np.isnan(var_vif):\n            tolerance = np.nan\n        else:\n            tolerance = 1.0 / var_vif\n        # Collect results\n        results.append({\"Variable\": var, \"Zero-Order r\": zero_order_r, \"Partial r\": partial_r, \"Semipartial r\": semipartial_r, \"Tolerance\": tolerance, \"VIF\": var_vif})\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.compute_standardized_betas","title":"<code>compute_standardized_betas(model, X_encoded, y)</code>","text":"<p>Computes standardized (beta) coefficients for a fitted statsmodels OLS model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>RegressionResultsWrapper</code> <p>The fitted OLS model.</p> required <code>X_encoded</code> <code>DataFrame</code> <p>The design matrix of independent variables.</p> required <code>y</code> <code>Series</code> <p>The dependent variable.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame containing the standardized beta coefficients.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import compute_standardized_betas\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; X = sm.add_constant(data)\n&gt;&gt;&gt; model = sm.OLS(y, X).fit()\n&gt;&gt;&gt; compute_standardized_betas(model, data, y)\n   Variable  Standardized Beta\n0     const           0.000000\n1       x1           0.000000\n2       x2           0.000000\n3       x3           0.000000\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def compute_standardized_betas(model, X_encoded, y) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes standardized (beta) coefficients for a fitted statsmodels OLS model.\n\n    Args:\n        model (statsmodels.regression.linear_model.RegressionResultsWrapper): The fitted OLS model.\n        X_encoded (pandas.DataFrame): The design matrix of independent variables.\n        y (pandas.Series): The dependent variable.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the standardized beta coefficients.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import compute_standardized_betas\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import statsmodels.api as sm\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'x1': [1, 2, 3, 4, 5],\n        ...     'x2': [2, 4, 6, 8, 10],\n        ...     'x3': [1, 3, 5, 7, 9]\n        ... })\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; X = sm.add_constant(data)\n        &gt;&gt;&gt; model = sm.OLS(y, X).fit()\n        &gt;&gt;&gt; compute_standardized_betas(model, data, y)\n           Variable  Standardized Beta\n        0     const           0.000000\n        1       x1           0.000000\n        2       x2           0.000000\n        3       x3           0.000000\n\n    \"\"\"\n    coeffs_unstd = model.params\n    std_X = X_encoded.drop(columns=[\"const\"], errors=\"ignore\").std()\n    std_y = y.std()\n    beta_std = coeffs_unstd.drop(\"const\", errors=\"ignore\") * (std_X / std_y)\n    beta_std_df = pd.DataFrame({\"Variable\": beta_std.index, \"Standardized Beta\": beta_std.values})\n    return beta_std_df\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.condition_index","title":"<code>condition_index(df)</code>","text":"<p>Calculates the Condition Index for a DataFrame to assess multicollinearity.</p> <p>The Condition Index is computed based on the eigenvalues of the covariance matrix of the standardized data. High condition indices suggest potential multicollinearity issues.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame containing the independent variables.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame with the following columns: - \u2018Index\u2019: The index of the eigenvalue. - \u2018Eigenvalue\u2019: The eigenvalue of the covariance matrix. - \u2018Condition Index\u2019: The Condition Index for the eigenvalue.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import condition_index\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; condition_index(data)\n   Index  Eigenvalue  Condition Index\n0      0    1.140000         1.000000\n1      1    0.000000              inf\n2      2    0.002857        20.000000\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def condition_index(df) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculates the Condition Index for a DataFrame to assess multicollinearity.\n\n    The Condition Index is computed based on the eigenvalues of the covariance matrix\n    of the standardized data. High condition indices suggest potential multicollinearity issues.\n\n    Args:\n        df (pandas.DataFrame): A DataFrame containing the independent variables.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with the following columns:\n            - 'Index': The index of the eigenvalue.\n            - 'Eigenvalue': The eigenvalue of the covariance matrix.\n            - 'Condition Index': The Condition Index for the eigenvalue.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import condition_index\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'x1': [1, 2, 3, 4, 5],\n        ...     'x2': [2, 4, 6, 8, 10],\n        ...     'x3': [1, 3, 5, 7, 9]\n        ... })\n        &gt;&gt;&gt; condition_index(data)\n           Index  Eigenvalue  Condition Index\n        0      0    1.140000         1.000000\n        1      1    0.000000              inf\n        2      2    0.002857        20.000000\n    \"\"\"\n    # Standardize the data\n    X = df.values\n    X_centered = X - np.mean(X, axis=0)\n\n    # Compute the covariance matrix\n    covariance_matrix = np.cov(X_centered, rowvar=False)\n\n    # Compute the eigenvalues of the covariance matrix\n    eigenvalues, _ = np.linalg.eigh(covariance_matrix)\n\n    # Handle division by zero for eigenvalues\n    max_eigenvalue = max(eigenvalues)\n    condition_indices = np.array([np.sqrt(max_eigenvalue / ev) if ev &gt; 0 else np.inf for ev in eigenvalues])\n\n    # Create a DataFrame for the results\n    condition_index_df = pd.DataFrame({\"Index\": range(len(eigenvalues)), \"Eigenvalue\": eigenvalues, \"Condition Index\": condition_indices})\n\n    return condition_index_df\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.cov_to_cor","title":"<code>cov_to_cor(covariance_matrix)</code>","text":"<p>Convert a covariance matrix to a correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>covariance_matrix</code> <code>ndarray</code> <p>A square matrix of covariance values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: A corresponding square matrix of correlation coefficients.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import cov_to_cor\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; cov_matrix = np.array([[1, 0.8], [0.8, 1]])\n&gt;&gt;&gt; cov_to_cor(cov_matrix)\narray([[1. , 0.8],\n       [0.8, 1. ]])\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def cov_to_cor(covariance_matrix) -&gt; np.ndarray:\n    \"\"\"Convert a covariance matrix to a correlation matrix.\n\n    Args:\n        covariance_matrix (numpy.ndarray): A square matrix of covariance values.\n\n    Returns:\n        numpy.ndarray: A corresponding square matrix of correlation coefficients.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import cov_to_cor\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; cov_matrix = np.array([[1, 0.8], [0.8, 1]])\n        &gt;&gt;&gt; cov_to_cor(cov_matrix)\n        array([[1. , 0.8],\n               [0.8, 1. ]])\n    \"\"\"\n    d = np.sqrt(np.diag(covariance_matrix))\n    return covariance_matrix / np.outer(d, d)\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.fit_all_lm","title":"<code>fit_all_lm(basic, xlist, data, remove_na=True)</code>","text":"<p>Fit a linear regression model for all possible combinations of independent variables.</p> <p>Parameters:</p> Name Type Description Default <code>basic</code> <code>str</code> <p>The basic model formula.</p> required <code>xlist</code> <code>list</code> <p>A list of independent variables.</p> required <code>data</code> <code>DataFrame</code> <p>The data frame containing the variables.</p> required <code>remove_na</code> <code>bool</code> <p>Whether to remove missing values from the data frame.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the estimated coefficients, confidence intervals, p-values, AIC values, sample size, and the basic model formula.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import fit_all_lm\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'y': [1, 2, 3],\n&gt;&gt;&gt;     'x1': [4, 5, 6],\n&gt;&gt;&gt;     'x2': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n{'estimate':   variables  estimate  conf_low  conf_high    p         aic  n\n0    basic  1.000000  1.000000   1.000000  0.0  0.000000  3\n1       x2  1.000000  1.000000   1.000000  0.0  0.000000  3}\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def fit_all_lm(basic, xlist, data, remove_na=True) -&gt; dict:\n    \"\"\"Fit a linear regression model for all possible combinations of independent variables.\n\n    Args:\n        basic (str): The basic model formula.\n        xlist (list): A list of independent variables.\n        data (pandas.DataFrame): The data frame containing the variables.\n        remove_na (bool): Whether to remove missing values from the data frame.\n\n    Returns:\n        dict: A dictionary containing the estimated coefficients, confidence intervals,\n            p-values, AIC values, sample size, and the basic model formula.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import fit_all_lm\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        &gt;&gt;&gt;     'y': [1, 2, 3],\n        &gt;&gt;&gt;     'x1': [4, 5, 6],\n        &gt;&gt;&gt;     'x2': [7, 8, 9]\n        &gt;&gt;&gt; })\n        &gt;&gt;&gt; fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n        {'estimate':   variables  estimate  conf_low  conf_high    p         aic  n\n        0    basic  1.000000  1.000000   1.000000  0.0  0.000000  3\n        1       x2  1.000000  1.000000   1.000000  0.0  0.000000  3}\n    \"\"\"\n    # Prepare the data frame\n    data = copy.deepcopy(data)\n    data_cols = get_all_vars_from_formula(basic) + xlist\n    # make sure that no duplicates are present in the data_cols\n    data_cols = list(set(data_cols))\n    data = data[data_cols]\n    if remove_na:\n        data = data.dropna()\n    print(f\"The basic model is: {basic}\")\n    print(f\"The following features will be used for fitting the basic model: {data.columns}\")\n    mod_0 = ols(basic, data=data).fit()\n    p = mod_0.pvalues.iloc[1]\n    print(f\"p-values: {p}\")\n    estimate = mod_0.params.iloc[1]\n    print(f\"estimate: {estimate}\")\n    conf_int = mod_0.conf_int().iloc[1]\n    print(f\"conf_int: {conf_int}\")\n    aic_value = mod_0.aic\n    print(f\"aic: {aic_value}\")\n    n = len(mod_0.resid)\n    df_0 = pd.DataFrame([[\"basic\", estimate, conf_int[0], conf_int[1], p, aic_value, n]], columns=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\"])\n\n    # All combinations model\n    comb_lst = list(itertools.chain.from_iterable(itertools.combinations(xlist, r) for r in range(1, len(xlist) + 1)))\n    n_comb = len(comb_lst)\n    # if more than 100 combinations, exit\n    if n_comb &gt; 100:\n        print(f\"Number of combinations is {n_comb}. Exiting.\")\n        return None\n    print(f\"Combinations: {comb_lst}\")\n    models = [ols(f\"{basic} + {' + '.join(comb)}\", data=data).fit() for comb in comb_lst]\n\n    df_list = []\n    for i, model in enumerate(models):\n        p = model.pvalues.iloc[1]\n        estimate = model.params.iloc[1]\n        conf_int = model.conf_int().iloc[1]\n        aic_value = model.aic\n        n = len(model.resid)\n        comb_str = \", \".join(comb_lst[i])\n        df_list.append([comb_str, estimate, conf_int[0], conf_int[1], p, aic_value, n])\n\n    df_coef = pd.DataFrame(df_list, columns=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\"])\n    estimates = pd.concat([df_0, df_coef], ignore_index=True)\n    return {\"estimate\": estimates, \"xlist\": xlist, \"fun\": \"all_lm\", \"basic\": basic, \"family\": \"lm\"}\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.get_all_vars_from_formula","title":"<code>get_all_vars_from_formula(formula)</code>","text":"<p>Utility function to extract variables from a formula.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>A formula.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of variables.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import get_all_vars_from_formula\n    get_all_vars_from_formula(\"y ~ x1 + x2\")\n        ['y', 'x1', 'x2']\n    get_all_vars_from_formula(\"y ~ \")\n        ['y']\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def get_all_vars_from_formula(formula) -&gt; list:\n    \"\"\"Utility function to extract variables from a formula.\n\n    Args:\n        formula (str): A formula.\n\n    Returns:\n        list: A list of variables.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import get_all_vars_from_formula\n            get_all_vars_from_formula(\"y ~ x1 + x2\")\n                ['y', 'x1', 'x2']\n            get_all_vars_from_formula(\"y ~ \")\n                ['y']\n    \"\"\"\n    # Split the formula into the dependent and independent variables\n    dependent, independent = formula.split(\"~\")\n    # Strip whitespace and split the independent variables by '+'\n    independent_vars = independent.strip().split(\"+\") if independent.strip() else []\n    # Combine the dependent variable with the independent variables\n    return [dependent.strip()] + [var.strip() for var in independent_vars]\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.get_combinations","title":"<code>get_combinations(ind_list, type='indices')</code>","text":"<p>Generates all possible combinations of two targets from a list of target indices. Order is not important.</p> <p>Parameters:</p> Name Type Description Default <code>ind_list</code> <code>list</code> <p>A list of target indices.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of tuples, where each tuple contains a combination of two target indices.  The order of the targets within a tuple is not important, and each combination  appears only once.</p> <code>type</code> <code>str</code> <p>The type of output, either \u2018values\u2019 or \u2018indices\u2019. Default is \u2018indices\u2019.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import get_combinations\n&gt;&gt;&gt; ind_list = [0, 10, 20, 30]\n&gt;&gt;&gt; combinations = get_combinations(ind_list)\n&gt;&gt;&gt; combinations = get_combinations(ind_list, type='indices')\n    [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n&gt;&gt;&gt; print(combinations, type='values')\n    [(0, 10), (0, 20), (0, 30), (1, 20), (1, 30), (2, 30)]\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def get_combinations(ind_list: list, type=\"indices\") -&gt; list:\n    \"\"\"\n    Generates all possible combinations of two targets from a list of target indices. Order is not important.\n\n    Args:\n        ind_list (list): A list of target indices.\n\n    Returns:\n        list: A list of tuples, where each tuple contains a combination of two target indices.\n             The order of the targets within a tuple is not important, and each combination\n             appears only once.\n        type (str): The type of output, either 'values' or 'indices'. Default is 'indices'.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import get_combinations\n        &gt;&gt;&gt; ind_list = [0, 10, 20, 30]\n        &gt;&gt;&gt; combinations = get_combinations(ind_list)\n        &gt;&gt;&gt; combinations = get_combinations(ind_list, type='indices')\n            [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n        &gt;&gt;&gt; print(combinations, type='values')\n            [(0, 10), (0, 20), (0, 30), (1, 20), (1, 30), (2, 30)]\n    \"\"\"\n    # check that ind_list is a list\n    if not isinstance(ind_list, list):\n        raise ValueError(\"ind_list must be a list.\")\n    m = len(ind_list)\n    if type == \"values\":\n        combinations = [(ind_list[i], ind_list[j]) for i in range(m) for j in range(i + 1, m)]\n    elif type == \"indices\":\n        combinations = [(i, j) for i in range(m) for j in range(i + 1, m)]\n    else:\n        raise ValueError(\"type must be either 'values' or 'indices'.\")\n    return combinations\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.partial_correlation","title":"<code>partial_correlation(x, method='pearson')</code>","text":"<p>Calculate the partial correlation matrix for a given data set.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame or ndarray</code> <p>The data matrix with variables as columns.</p> required <code>method</code> <code>str</code> <p>Correlation method, one of \u2018pearson\u2019, \u2018kendall\u2019, or \u2018spearman\u2019.</p> <code>'pearson'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the partial correlation estimates, p-values, statistics, sample size (n), number of given parameters (gp), and method used.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input is not a matrix-like structure or not numeric.</p> References <ol> <li>Kim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients. Commun Stat Appl Methods 22, 6 (Nov 2015), 665\u2013674.</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import partial_correlation\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'A': [1, 2, 3],\n&gt;&gt;&gt;     'B': [4, 5, 6],\n&gt;&gt;&gt;     'C': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; partial_correlation(data, method='pearson')\n{'estimate': array([[ 1. , -1. ,  1. ],\n                    [-1. ,  1. , -1. ],\n                    [ 1. , -1. ,  1. ]]),\n'p_value': array([[0. , 0. , 0. ],\n                  [0. , 0. , 0. ],\n                  [0. , 0. , 0. ]]), ...\n}\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def partial_correlation(x, method=\"pearson\") -&gt; dict:\n    \"\"\"Calculate the partial correlation matrix for a given data set.\n\n    Args:\n        x (pandas.DataFrame or numpy.ndarray): The data matrix with variables as columns.\n        method (str): Correlation method, one of 'pearson', 'kendall', or 'spearman'.\n\n    Returns:\n        dict: A dictionary containing the partial correlation estimates, p-values,\n            statistics, sample size (n), number of given parameters (gp), and method used.\n\n    Raises:\n        ValueError: If input is not a matrix-like structure or not numeric.\n\n    References:\n        1. Kim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients.\n        Commun Stat Appl Methods 22, 6 (Nov 2015), 665\u2013674.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import partial_correlation\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        &gt;&gt;&gt;     'A': [1, 2, 3],\n        &gt;&gt;&gt;     'B': [4, 5, 6],\n        &gt;&gt;&gt;     'C': [7, 8, 9]\n        &gt;&gt;&gt; })\n        &gt;&gt;&gt; partial_correlation(data, method='pearson')\n        {'estimate': array([[ 1. , -1. ,  1. ],\n                            [-1. ,  1. , -1. ],\n                            [ 1. , -1. ,  1. ]]),\n        'p_value': array([[0. , 0. , 0. ],\n                          [0. , 0. , 0. ],\n                          [0. , 0. , 0. ]]), ...\n        }\n    \"\"\"\n    eps = 1e-6\n    if isinstance(x, pd.DataFrame):\n        x = x.to_numpy()\n    if not isinstance(x, np.ndarray):\n        raise ValueError(\"Supply a matrix-like 'x'\")\n    if not np.issubdtype(x.dtype, np.number):\n        raise ValueError(\"'x' must be numeric\")\n\n    n = x.shape[0]\n    gp = x.shape[1] - 2\n    cvx = np.cov(x, rowvar=False, bias=True)\n\n    try:\n        if np.linalg.det(cvx) &lt; np.finfo(float).eps:\n            icvx = pinv(cvx)\n        else:\n            icvx = inv(cvx)\n    except LinAlgError:\n        icvx = pinv(cvx)\n\n    p_cor = -cov_to_cor(icvx)\n    np.fill_diagonal(p_cor, 1)\n\n    if method == \"kendall\":\n        denominator = np.sqrt(2 * (2 * (n - gp) + 5) / (9 * (n - gp) * (n - 1 - gp)))\n        statistic = p_cor / denominator\n        p_value = 2 * norm.cdf(-np.abs(statistic))\n    else:\n        factor = np.sqrt((n - 2 - gp) / (1 + eps - p_cor**2))\n        statistic = p_cor * factor\n        p_value = 2 * t.cdf(-np.abs(statistic), df=n - 2 - gp)\n\n    np.fill_diagonal(statistic, 0)\n    np.fill_diagonal(p_value, 0)\n\n    return {\"estimate\": p_cor, \"p_value\": p_value, \"statistic\": statistic, \"n\": n, \"gp\": gp, \"method\": method}\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.partial_correlation_test","title":"<code>partial_correlation_test(x, y, z, method='pearson')</code>","text":"<p>The partial correlation coefficient between x and y given z.     x and y should be arrays (vectors) of the same length, and z should be a data frame (matrix).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>The first variable as a 1-dimensional array or list.</p> required <code>y</code> <code>array - like</code> <p>The second variable as a 1-dimensional array or list.</p> required <code>z</code> <code>DataFrame</code> <p>A data frame containing other conditional variables.</p> required <code>method</code> <code>str</code> <p>Correlation method, one of \u2018pearson\u2019, \u2018kendall\u2019, or \u2018spearman\u2019.</p> <code>'pearson'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the partial correlation estimate, p-value, statistic, sample size (n), number of given parameters (gp), and method used.</p> References <ol> <li>Kim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients. Commun Stat Appl Methods 22, 6 (Nov 2015), 665\u2013674.</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import pairwise_partial_correlation\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; x = [1, 2, 3]\n&gt;&gt;&gt; y = [4, 5, 6]\n&gt;&gt;&gt; z = pd.DataFrame({'C': [7, 8, 9]})\n&gt;&gt;&gt; pairwise_partial_correlation(x, y, z)\n{'estimate': -1.0, 'p_value': 0.0, 'statistic': -inf, 'n': 3, 'gp': 1, 'method': 'pearson'}\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def partial_correlation_test(x, y, z, method=\"pearson\") -&gt; dict:\n    \"\"\"The partial correlation coefficient between x and y given z.\n        x and y should be arrays (vectors) of the same length, and z should be a data frame (matrix).\n\n    Args:\n        x (array-like): The first variable as a 1-dimensional array or list.\n        y (array-like): The second variable as a 1-dimensional array or list.\n        z (pandas.DataFrame): A data frame containing other conditional variables.\n        method (str): Correlation method, one of 'pearson', 'kendall', or 'spearman'.\n\n    Returns:\n        dict: A dictionary with the partial correlation estimate, p-value, statistic,\n            sample size (n), number of given parameters (gp), and method used.\n\n    References:\n        1. Kim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients.\n        Commun Stat Appl Methods 22, 6 (Nov 2015), 665\u2013674.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import pairwise_partial_correlation\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; x = [1, 2, 3]\n        &gt;&gt;&gt; y = [4, 5, 6]\n        &gt;&gt;&gt; z = pd.DataFrame({'C': [7, 8, 9]})\n        &gt;&gt;&gt; pairwise_partial_correlation(x, y, z)\n        {'estimate': -1.0, 'p_value': 0.0, 'statistic': -inf, 'n': 3, 'gp': 1, 'method': 'pearson'}\n    \"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n    z = pd.DataFrame(z)\n\n    xyz = pd.concat([pd.Series(x), pd.Series(y), z], axis=1)\n\n    pcor_result = partial_correlation(xyz, method=method)\n\n    return {\n        \"estimate\": pcor_result[\"estimate\"][0, 1],\n        \"p_value\": pcor_result[\"p_value\"][0, 1],\n        \"statistic\": pcor_result[\"statistic\"][0, 1],\n        \"n\": pcor_result[\"n\"],\n        \"gp\": pcor_result[\"gp\"],\n        \"method\": method,\n    }\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.plot_coeff_vs_pvals","title":"<code>plot_coeff_vs_pvals(data, xlabels=None, xlim=(0, 1), xlab='p-value', ylim=None, ylab=None, xscale_log=True, yscale_log=False, title=None, show=True, y_scaler=1.1)</code>","text":"<p>Plot the coefficient estimates from fit_all_lm against the corresponding p-values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary containing the estimated coefficients, p-values, and other information. Generated by the fit_all_lm function.</p> required <code>xlabels</code> <code>list</code> <p>A list of x-axis labels.</p> <code>None</code> <code>xlim</code> <code>tuple</code> <p>A tuple of the x-axis limits.</p> <code>(0, 1)</code> <code>xlab</code> <code>str</code> <p>The x-axis label.</p> <code>'p-value'</code> <code>ylim</code> <code>tuple</code> <p>A tuple of the y-axis limits.</p> <code>None</code> <code>ylab</code> <code>str</code> <p>The y-axis label.</p> <code>None</code> <code>xscale_log</code> <code>bool</code> <p>Whether to use a log scale on the x-axis.</p> <code>True</code> <code>yscale_log</code> <code>bool</code> <p>Whether to use a log scale on the y-axis.</p> <code>False</code> <code>title</code> <code>str</code> <p>The plot title.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot.</p> <code>True</code> <code>y_scaler</code> <code>float</code> <p>A scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.</p> <code>1.1</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>Based on the R package \u2018allestimates\u2019 by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates</li> </ul> References <p>Wang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies. The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import plot_coeff_vs_pvals, fit_all_lm\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'y': [1, 2, 3],\n&gt;&gt;&gt;     'x1': [4, 5, 6],\n&gt;&gt;&gt;     'x2': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; estimates = fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n&gt;&gt;&gt; plot_coeff_vs_pvals(estimates)\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def plot_coeff_vs_pvals(data, xlabels=None, xlim=(0, 1), xlab=\"p-value\", ylim=None, ylab=None, xscale_log=True, yscale_log=False, title=None, show=True, y_scaler=1.1) -&gt; None:\n    \"\"\"Plot the coefficient estimates from fit_all_lm against the corresponding p-values.\n\n    Args:\n        data (dict):\n            A dictionary containing the estimated coefficients, p-values, and other information.\n            Generated by the fit_all_lm function.\n        xlabels (list):\n            A list of x-axis labels.\n        xlim (tuple):\n            A tuple of the x-axis limits.\n        xlab (str):\n            The x-axis label.\n        ylim (tuple):\n            A tuple of the y-axis limits.\n        ylab (str):\n            The y-axis label.\n        xscale_log (bool):\n            Whether to use a log scale on the x-axis.\n        yscale_log (bool):\n            Whether to use a log scale on the y-axis.\n        title (str):\n            The plot title.\n        show (bool):\n            Whether to display the plot.\n        y_scaler (float):\n            A scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.\n\n    Returns:\n        None\n\n    Notes:\n        * Based on the R package 'allestimates' by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates\n\n    References:\n        Wang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies. The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import plot_coeff_vs_pvals, fit_all_lm\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        &gt;&gt;&gt;     'y': [1, 2, 3],\n        &gt;&gt;&gt;     'x1': [4, 5, 6],\n        &gt;&gt;&gt;     'x2': [7, 8, 9]\n        &gt;&gt;&gt; })\n        &gt;&gt;&gt; estimates = fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n        &gt;&gt;&gt; plot_coeff_vs_pvals(estimates)\n    \"\"\"\n    data = copy.deepcopy(data)\n    if xlabels is None:\n        xlabels = [0, 0.001, 0.01, 0.05, 0.2, 0.5, 1]\n    xbreaks = np.power(xlabels, np.log(0.5) / np.log(0.05))\n\n    result_df = data[\"estimate\"]\n    if ylab is None:\n        ylab = \"Coefficient\" if data[\"fun\"] == \"all_lm\" else \"Effect estimates\"\n    hline = 0 if data[\"fun\"] == \"all_lm\" else 1\n\n    result_df[\"p_value\"] = np.power(result_df[\"p\"], np.log(0.5) / np.log(0.05))\n    if ylim is None:\n        maxv = max(result_df[\"estimate\"].max(), abs(result_df[\"estimate\"].min()))\n        maxv = maxv * y_scaler\n        ylim = (-maxv, maxv) if data[\"fun\"] == \"all_lm\" else (1 / maxv, maxv)\n\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=result_df, x=\"p_value\", y=\"estimate\")\n    if xscale_log:\n        plt.xscale(\"log\")\n    if yscale_log:\n        plt.yscale(\"log\")\n    plt.xticks(ticks=xbreaks, labels=xlabels)\n    plt.axvline(x=0.5, linestyle=\"--\")\n    plt.axhline(y=hline, linestyle=\"--\")\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n    plt.xlabel(xlab)\n    plt.ylabel(ylab)\n    if title:\n        plt.title(title)\n    plt.grid(True)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.plot_coeff_vs_pvals_by_included","title":"<code>plot_coeff_vs_pvals_by_included(data, xlabels=None, xlim=(0, 1), xlab='P value', ylim=None, ylab=None, yscale_log=False, title=None, grid=True, ncol=2, show=True, y_scaler=1.1)</code>","text":"<p>Generates a panel of scatter plots with effect estimates of all possible models against p-values. Uses a dictionry generated by the fit_all_lm function. Each plot includes effect estimates from all models including a specific variable.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary, generated by the fit_all_lm function, containing the following keys: - estimate (pd.DataFrame): A DataFrame containing the estimates. - xlist (list): A list of variables. - fun (str): The function name. - family (str): The family of the model.</p> required <code>xlabels</code> <code>list</code> <p>A list of x-axis labels.</p> <code>None</code> <code>xlim</code> <code>tuple</code> <p>The x-axis limits.</p> <code>(0, 1)</code> <code>xlab</code> <code>str</code> <p>The x-axis label.</p> <code>'P value'</code> <code>ylim</code> <code>tuple</code> <p>The y-axis limits.</p> <code>None</code> <code>ylab</code> <code>str</code> <p>The y-axis label.</p> <code>None</code> <code>yscale_log</code> <code>bool</code> <p>Whether to scale y-axis to log10. Default is False.</p> <code>False</code> <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>None</code> <code>grid</code> <code>bool</code> <p>Whether to display gridlines. Default is True.</p> <code>True</code> <code>ncol</code> <code>int</code> <p>Number of columns in the plot grid. Default is 2.</p> <code>2</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Default is True.</p> <code>True</code> <code>y_scaler</code> <code>float</code> <p>A scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.</p> <code>1.1</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>Based on the R package \u2018allestimates\u2019 by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates</li> </ul> References <p>Wang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies. The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203</p> <p>Examples:</p> <p>data = {     \u201cestimate\u201d: pd.DataFrame({         \u201cvariables\u201d: [\u201cCrude\u201d, \u201cAL\u201d, \u201cAM\u201d, \u201cAN\u201d, \u201cAO\u201d],         \u201cestimate\u201d: [0.5, 0.6, 0.7, 0.8, 0.9],         \u201cconf_low\u201d: [0.1, 0.2, 0.3, 0.4, 0.5],         \u201cconf_high\u201d: [0.9, 1.0, 1.1, 1.2, 1.3],         \u201cp\u201d: [0.01, 0.02, 0.03, 0.04, 0.05],         \u201caic\u201d: [100, 200, 300, 400, 500],         \u201cn\u201d: [10, 20, 30, 40, 50]     }),     \u201cxlist\u201d: [\u201cAL\u201d, \u201cAM\u201d, \u201cAN\u201d, \u201cAO\u201d],     \u201cfun\u201d: \u201call_lm\u201d } plot_coeff_vs_pvals_by_included(data)</p> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def plot_coeff_vs_pvals_by_included(data, xlabels=None, xlim=(0, 1), xlab=\"P value\", ylim=None, ylab=None, yscale_log=False, title=None, grid=True, ncol=2, show=True, y_scaler=1.1) -&gt; None:\n    \"\"\"\n    Generates a panel of scatter plots with effect estimates of all possible models against p-values.\n    Uses a dictionry generated by the fit_all_lm function.\n    Each plot includes effect estimates from all models including a specific variable.\n\n    Args:\n        data (dict): A dictionary, generated by the fit_all_lm function, containing the following keys:\n            - estimate (pd.DataFrame): A DataFrame containing the estimates.\n            - xlist (list): A list of variables.\n            - fun (str): The function name.\n            - family (str): The family of the model.\n        xlabels (list): A list of x-axis labels.\n        xlim (tuple): The x-axis limits.\n        xlab (str): The x-axis label.\n        ylim (tuple): The y-axis limits.\n        ylab (str): The y-axis label.\n        yscale_log (bool): Whether to scale y-axis to log10. Default is False.\n        title (str): The title of the plot.\n        grid (bool): Whether to display gridlines. Default is True.\n        ncol (int): Number of columns in the plot grid. Default is 2.\n        show (bool): Whether to display the plot. Default is True.\n        y_scaler (float): A scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.\n\n    Returns:\n        None\n\n    Notes:\n        * Based on the R package 'allestimates' by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates\n\n    References:\n        Wang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies. The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203\n\n\n    Examples:\n        data = {\n            \"estimate\": pd.DataFrame({\n                \"variables\": [\"Crude\", \"AL\", \"AM\", \"AN\", \"AO\"],\n                \"estimate\": [0.5, 0.6, 0.7, 0.8, 0.9],\n                \"conf_low\": [0.1, 0.2, 0.3, 0.4, 0.5],\n                \"conf_high\": [0.9, 1.0, 1.1, 1.2, 1.3],\n                \"p\": [0.01, 0.02, 0.03, 0.04, 0.05],\n                \"aic\": [100, 200, 300, 400, 500],\n                \"n\": [10, 20, 30, 40, 50]\n            }),\n            \"xlist\": [\"AL\", \"AM\", \"AN\", \"AO\"],\n            \"fun\": \"all_lm\"\n        }\n        plot_coeff_vs_pvals_by_included(data)\n    \"\"\"\n    if xlabels is None:\n        xlabels = [0, 0.001, 0.01, 0.05, 0.2, 0.5, 1]\n    xbreaks = np.power(xlabels, np.log(0.5) / np.log(0.05))\n\n    result_df = data[\"estimate\"]\n    if ylab is None:\n        ylab = {\"all_lm\": \"Coefficient\", \"poisson\": \"Rate ratio\", \"binomial\": \"Odds ratio\"}.get(data.get(\"fun\"), \"Effect estimates\")\n\n    hline = 0 if data[\"fun\"] == \"all_lm\" else 1\n\n    result_df[\"p_value\"] = np.power(result_df[\"p\"], np.log(0.5) / np.log(0.05))\n    if ylim is None:\n        maxv = max(result_df[\"estimate\"].max(), abs(result_df[\"estimate\"].min()))\n        maxv = maxv * y_scaler\n        if data[\"fun\"] == \"all_lm\":\n            ylim = (-maxv, maxv)\n        else:\n            ylim = (1 / maxv, maxv)\n\n    # Create a DataFrame to mark inclusion of variables\n    mark_df = pd.DataFrame({x: result_df[\"variables\"].str.contains(x).astype(int) for x in data[\"xlist\"]})\n    df_scatter = pd.concat([result_df, mark_df], axis=1)\n\n    # Melt the DataFrame for plotting\n    df_long = df_scatter.melt(id_vars=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\", \"p_value\"], value_vars=data[\"xlist\"], var_name=\"variable\", value_name=\"inclusion\")\n    df_long[\"inclusion\"] = df_long[\"inclusion\"].apply(lambda x: \"Included\" if x &gt; 0 else \"Not included\")\n\n    # Plotting\n    g = sns.FacetGrid(df_long, col=\"variable\", hue=\"inclusion\", palette={\"Included\": \"blue\", \"Not included\": \"orange\"}, col_wrap=ncol, height=4, sharex=False, sharey=False)\n    g.map(sns.scatterplot, \"p_value\", \"estimate\")\n    g.add_legend()\n    for ax in g.axes.flat:\n        ax.set_xticks(xbreaks)\n        ax.set_xticklabels(xlabels)\n        ax.set_xlim(xlim)\n        ax.set_ylim(ylim)\n        ax.axvline(x=0.5, linestyle=\"--\", linewidth=1.5, color=\"black\")  # Black dashed vertical line\n        ax.axhline(y=hline, linestyle=\"--\", linewidth=1.5, color=\"black\")  # Black dashed horizontal line\n        if grid:\n            ax.grid(True)\n    if yscale_log:\n        g.set(yscale=\"log\")\n    g.set_axis_labels(xlab, ylab)\n    g.set_titles(\"{col_name}\")\n    if title:\n        plt.subplots_adjust(top=0.9)\n        g.figure.suptitle(title)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.preprocess_df_for_ols","title":"<code>preprocess_df_for_ols(df, independent_var_columns, target_col)</code>","text":"<p>Preprocesses a df for fiitting an OLS regression model using the specified target column and predictors.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the data.</p> required <code>independent_var_columns</code> <code>list of str</code> <p>List of names for predictor columns.</p> required <code>target_col</code> <code>str</code> <p>Name of the target/dependent variable column.</p> required <p>Returns:</p> Name Type Description <code>X_encoded</code> <code>DataFrame</code> <p>Encoded predictors with a constant term.</p> <code>y</code> <code>Series</code> <p>Target variable.</p> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def preprocess_df_for_ols(df, independent_var_columns, target_col) -&gt; tuple:\n    \"\"\"\n    Preprocesses a df for fiitting an OLS regression model using the specified target column and predictors.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        independent_var_columns (list of str): List of names for predictor columns.\n        target_col (str): Name of the target/dependent variable column.\n\n    Returns:\n        X_encoded (pd.DataFrame): Encoded predictors with a constant term.\n        y (pd.Series): Target variable.\n\n    \"\"\"\n    # Ensure the target column is numeric and 1D\n    y = pd.to_numeric(df[target_col], errors=\"coerce\").fillna(0).squeeze()\n    if y.ndim != 1:\n        raise ValueError(f\"Target column '{target_col}' must be 1-dimensional.\")\n\n    # Ensure predictors are numeric\n    X = df[independent_var_columns].apply(pd.to_numeric, errors=\"coerce\")\n    # Impute missing values\n    X = X.fillna(X.median())\n\n    # Identify categorical columns (replace with your actual categorical list if needed)\n    categorical_cols = [\"type\"]\n    encoder = OneHotEncoder(drop=\"first\", sparse_output=False)\n    X_categorical_encoded = encoder.fit_transform(df[categorical_cols])\n\n    # Convert encoded data into a DataFrame\n    X_categorical_encoded_df = pd.DataFrame(X_categorical_encoded, columns=encoder.get_feature_names_out(categorical_cols), index=df.index)  # Ensure alignment with the original DataFrame\n\n    # Combine numeric and categorical (encoded) parts\n    X_encoded = pd.concat([X, X_categorical_encoded_df], axis=1)\n\n    # Add a constant term\n    X_encoded = sm.add_constant(X_encoded)\n\n    # Ensure alignment between X_encoded and y\n    if X_encoded.shape[0] != y.shape[0]:\n        raise ValueError(f\"Mismatch in rows: predictors (X_encoded) have {X_encoded.shape[0]} rows, \" f\"but target (y) has {y.shape[0]} rows.\")\n\n    return X_encoded, y\n</code></pre>"},{"location":"reference/spotpython/utils/stats/#spotpython.utils.stats.vif","title":"<code>vif(X, sorted=True)</code>","text":"<p>Calculates the Variance Inflation Factor (VIF) for each feature in a DataFrame.</p> <p>VIF measures the multicollinearity among independent variables within a regression model. High VIF values indicate high multicollinearity, which can cause issues with model interpretation and stability.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>A DataFrame containing the independent variables.</p> required <code>sorted</code> <code>bool</code> <p>Whether to sort the output DataFrame by VIF values.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame with two columns: - \u201cfeature\u201d: The name of the feature. - \u201cVIF\u201d: The Variance Inflation Factor for the feature.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import vif\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; vif(data)\n   feature          VIF\n0      x1  1260.000000\n1      x2         0.000000\n2      x3   630.000000\n</code></pre> Source code in <code>spotpython/utils/stats.py</code> <pre><code>def vif(X, sorted=True) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculates the Variance Inflation Factor (VIF) for each feature in a DataFrame.\n\n    VIF measures the multicollinearity among independent variables within a regression model.\n    High VIF values indicate high multicollinearity, which can cause issues with model\n    interpretation and stability.\n\n    Args:\n        X (pandas.DataFrame): A DataFrame containing the independent variables.\n        sorted (bool): Whether to sort the output DataFrame by VIF values.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with two columns:\n            - \"feature\": The name of the feature.\n            - \"VIF\": The Variance Inflation Factor for the feature.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import vif\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'x1': [1, 2, 3, 4, 5],\n        ...     'x2': [2, 4, 6, 8, 10],\n        ...     'x3': [1, 3, 5, 7, 9]\n        ... })\n        &gt;&gt;&gt; vif(data)\n           feature          VIF\n        0      x1  1260.000000\n        1      x2         0.000000\n        2      x3   630.000000\n    \"\"\"\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = X.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    if sorted:\n        vif_data = vif_data.sort_values(by=\"VIF\", ascending=False).reset_index(drop=True)\n    return vif_data\n</code></pre>"},{"location":"reference/spotpython/utils/tensorboard/","title":"tensorboard","text":""},{"location":"reference/spotpython/utils/tensorboard/#spotpython.utils.tensorboard.start_tensorboard","title":"<code>start_tensorboard()</code>","text":"<p>Starts a tensorboard server in the background.</p> <p>Returns:</p> Name Type Description <code>process</code> <code>Popen</code> <p>The process of the tensorboard server.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.tensorboard import start_tensorboard\n&gt;&gt;&gt; process = start_tensorboard()\n</code></pre> Source code in <code>spotpython/utils/tensorboard.py</code> <pre><code>def start_tensorboard() -&gt; subprocess.Popen:\n    \"\"\"Starts a tensorboard server in the background.\n\n    Returns:\n        process: The process of the tensorboard server.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.tensorboard import start_tensorboard\n        &gt;&gt;&gt; process = start_tensorboard()\n\n    \"\"\"\n    cmd = [\"tensorboard\", \"--logdir=./runs\"]\n    process = subprocess.Popen(cmd)\n    return process\n</code></pre>"},{"location":"reference/spotpython/utils/tensorboard/#spotpython.utils.tensorboard.stop_tensorboard","title":"<code>stop_tensorboard(process)</code>","text":"<p>Stops a tensorboard server if the process exists.</p> <p>Parameters:</p> Name Type Description Default <code>process</code> <code>Popen</code> <p>The process of the tensorboard server.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.tensorboard import start_tensorboard, stop_tensorboard\n&gt;&gt;&gt; process = start_tensorboard()\n&gt;&gt;&gt; stop_tensorboard(process)\n</code></pre> Source code in <code>spotpython/utils/tensorboard.py</code> <pre><code>def stop_tensorboard(process) -&gt; None:\n    \"\"\"\n    Stops a tensorboard server if the process exists.\n\n    Args:\n        process (subprocess.Popen): The process of the tensorboard server.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.tensorboard import start_tensorboard, stop_tensorboard\n        &gt;&gt;&gt; process = start_tensorboard()\n        &gt;&gt;&gt; stop_tensorboard(process)\n    \"\"\"\n    if process is not None and process.poll() is None:\n        process.terminate()\n        process.wait()  # Ensure the process has terminated\n    else:\n        print(\"No active tensorboard process found or the process is already terminated.\")\n</code></pre>"},{"location":"reference/spotpython/utils/time/","title":"time","text":""},{"location":"reference/spotpython/utils/time/#spotpython.utils.time.get_timestamp","title":"<code>get_timestamp(only_int=True)</code>","text":"<p>Returns a timestamp as a string.</p> <p>Parameters:</p> Name Type Description Default <code>only_int</code> <code>bool</code> <p>if True, the timestamp is returned as an integer.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the timestamp as a string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.time import get_timestamp\n&gt;&gt;&gt; get_timestamp()\n'2021-06-28 14:51:54.500000'\n&gt;&gt;&gt; get_timestamp(only_int=True)\n'20210628145154500000'\n</code></pre> Source code in <code>spotpython/utils/time.py</code> <pre><code>def get_timestamp(only_int=True) -&gt; str:\n    \"\"\"\n    Returns a timestamp as a string.\n\n    Args:\n        only_int (bool): if True, the timestamp is returned as an integer.\n\n    Returns:\n        str: the timestamp as a string.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.time import get_timestamp\n        &gt;&gt;&gt; get_timestamp()\n        '2021-06-28 14:51:54.500000'\n        &gt;&gt;&gt; get_timestamp(only_int=True)\n        '20210628145154500000'\n    \"\"\"\n    dt = datetime.datetime.now().isoformat(sep=\" \", timespec=\"microseconds\")\n    if only_int:\n        # remove - . : and space\n        dt = dt.replace(\"-\", \"\")\n        dt = dt.replace(\".\", \"\")\n        dt = dt.replace(\":\", \"\")\n        dt = dt.replace(\" \", \"\")\n    return dt\n</code></pre>"},{"location":"reference/spotpython/utils/transform/","title":"transform","text":""},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.cod_to_nat_X","title":"<code>cod_to_nat_X(cod_X, cod_type, min_X=None, max_X=None, mean_X=None, std_X=None)</code>","text":"<p>Compute natural X-values from coded units based on the setting of the <code>cod_type</code> attribute. If <code>cod_type</code> is \u201cnorm\u201d, the values are de-normalized from [0,1]. If <code>cod_type</code> is \u201cstd\u201d, the values are de-standardized. Otherwise, the values are not modified.</p> <p>Parameters:</p> Name Type Description Default <code>cod_X</code> <code>array</code> <p>The coded X-values.</p> required <code>cod_type</code> <code>str</code> <p>The type of coding (\u201cnorm\u201d, \u201cstd\u201d, or other).</p> required <code>min_X</code> <code>array</code> <p>The minimum values of X. Defaults to None.</p> <code>None</code> <code>max_X</code> <code>array</code> <p>The maximum values of X. Defaults to None.</p> <code>None</code> <code>mean_X</code> <code>array</code> <p>The mean values of X. Defaults to None.</p> <code>None</code> <code>std_X</code> <code>array</code> <p>The standard deviation of X. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X</code> <code>array</code> <p>The natural (physical or real world) X-values.</p> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def cod_to_nat_X(cod_X, cod_type, min_X=None, max_X=None, mean_X=None, std_X=None) -&gt; np.ndarray:\n    \"\"\"\n    Compute natural X-values from coded units based on the\n    setting of the `cod_type` attribute. If `cod_type` is \"norm\", the values are\n    de-normalized from [0,1]. If `cod_type` is \"std\", the values are de-standardized.\n    Otherwise, the values are not modified.\n\n    Args:\n        cod_X (np.array):\n            The coded X-values.\n        cod_type (str):\n            The type of coding (\"norm\", \"std\", or other).\n        min_X (np.array):\n            The minimum values of X. Defaults to None.\n        max_X (np.array):\n            The maximum values of X. Defaults to None.\n        mean_X (np.array):\n            The mean values of X. Defaults to None.\n        std_X (np.array):\n            The standard deviation of X. Defaults to None.\n\n    Returns:\n        X (np.array): The natural (physical or real world) X-values.\n    \"\"\"\n    X_copy = copy.deepcopy(cod_X)\n    # k is the number of columns in X, i.e., the dimension of the input space.\n    k = cod_X.shape[1]\n    if cod_type == \"norm\":\n        # De-normalize X from [0,1] column-wise.\n        for i in range(k):\n            X_copy[:, i] = X_copy[:, i] * (max_X[i] - min_X[i]) + min_X[i]\n        X = X_copy\n    elif cod_type == \"std\":\n        # De-standardize X column-wise.\n        for i in range(k):\n            X_copy[:, i] = X_copy[:, i] * std_X[i] + mean_X[i]\n        X = X_copy\n    else:\n        X = X_copy\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.cod_to_nat_y","title":"<code>cod_to_nat_y(cod_y, cod_type, min_y=None, max_y=None, mean_y=None, std_y=None)</code>","text":"<p>Compute natural y-values from coded units based on the setting of the <code>cod_type</code> attribute. If <code>cod_type</code> is \u201cnorm\u201d, the values are de-normalized from [0,1]. If <code>cod_type</code> is \u201cstd\u201d, the values are de-standardized. Otherwise, the values are not modified.</p> <p>Parameters:</p> Name Type Description Default <code>cod_y</code> <code>array</code> <p>The coded y-values.</p> required <code>cod_type</code> <code>str</code> <p>The type of coding (\u201cnorm\u201d, \u201cstd\u201d, or other).</p> required <code>min_y</code> <code>array</code> <p>The minimum values of y. Defaults to None.</p> <code>None</code> <code>max_y</code> <code>array</code> <p>The maximum values of y. Defaults to None.</p> <code>None</code> <code>mean_y</code> <code>array</code> <p>The mean values of y. Defaults to None.</p> <code>None</code> <code>std_y</code> <code>array</code> <p>The standard deviation of y. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y</code> <code>array</code> <p>The natural (physical or real world) y-values.</p> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def cod_to_nat_y(cod_y, cod_type, min_y=None, max_y=None, mean_y=None, std_y=None) -&gt; np.ndarray:\n    \"\"\"\n    Compute natural y-values from coded units based on the\n    setting of the `cod_type` attribute. If `cod_type` is \"norm\", the values are\n    de-normalized from [0,1]. If `cod_type` is \"std\", the values are de-standardized.\n    Otherwise, the values are not modified.\n\n    Args:\n        cod_y (np.array):\n            The coded y-values.\n        cod_type (str):\n            The type of coding (\"norm\", \"std\", or other).\n        min_y (np.array):\n            The minimum values of y. Defaults to None.\n        max_y (np.array):\n            The maximum values of y. Defaults to None.\n        mean_y (np.array):\n            The mean values of y. Defaults to None.\n        std_y (np.array):\n            The standard deviation of y. Defaults to None.\n\n    Returns:\n        y (np.array): The natural (physical or real world) y-values.\n    \"\"\"\n    y_copy = copy.deepcopy(cod_y)\n    if cod_type == \"norm\":\n        y = y_copy * (max_y - min_y) + min_y\n    elif cod_type == \"std\":\n        y = y_copy * std_y + mean_y\n    else:\n        y = y_copy\n    return y\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.nat_to_cod_X","title":"<code>nat_to_cod_X(X, cod_type)</code>","text":"<p>Compute coded X-values from natural (physical or real world) units based on the setting of the <code>cod_type</code> attribute. If <code>cod_type</code> is \u201cnorm\u201d, the values are normalized to [0,1]. If <code>cod_type</code> is \u201cstd\u201d, the values are standardized. Otherwise, the values are not modified.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The input array.</p> required <code>cod_type</code> <code>str</code> <p>The type of coding (\u201cnorm\u201d, \u201cstd\u201d, or other).</p> required <p>Returns:</p> Name Type Description <code>cod_X</code> <code>array</code> <p>The coded X-values.</p> <code>min_X</code> <code>array</code> <p>The minimum values of X.</p> <code>max_X</code> <code>array</code> <p>The maximum values of X.</p> <code>mean_X</code> <code>array</code> <p>The mean values of X.</p> <code>std_X</code> <code>array</code> <p>The standard deviation of X.</p> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def nat_to_cod_X(X, cod_type):\n    \"\"\"\n    Compute coded X-values from natural (physical or real world) units based on the\n    setting of the `cod_type` attribute. If `cod_type` is \"norm\", the values are\n    normalized to [0,1]. If `cod_type` is \"std\", the values are standardized.\n    Otherwise, the values are not modified.\n\n    Args:\n        X (np.array): The input array.\n        cod_type (str): The type of coding (\"norm\", \"std\", or other).\n\n    Returns:\n        cod_X (np.array): The coded X-values.\n        min_X (np.array): The minimum values of X.\n        max_X (np.array): The maximum values of X.\n        mean_X (np.array): The mean values of X.\n        std_X (np.array): The standard deviation of X.\n    \"\"\"\n    min_X = np.min(X, axis=0)\n    max_X = np.max(X, axis=0)\n    mean_X = np.mean(X, axis=0)\n    # make std_X array similar to mean_X array\n    std_X = np.zeros_like(mean_X)\n    X_copy = copy.deepcopy(X)\n    # k is the number of columns in X, i.e., the dimension of the input space.\n    k = X.shape[1]\n    if cod_type == \"norm\":\n        # Normalize X to [0,1] column-wise. If the range is zero, set the value to 0.5.\n        for i in range(k):\n            if max_X[i] - min_X[i] == 0:\n                X_copy[:, i] = 0.5\n            else:\n                X_copy[:, i] = (X_copy[:, i] - min_X[i]) / (max_X[i] - min_X[i])\n        cod_X = X_copy\n    elif cod_type == \"std\":\n        # Standardize X column-wise. If the standard deviation is zero, do not divide.\n        for i in range(k):\n            if max_X[i] - min_X[i] == 0:\n                X_copy[:, i] = 0\n            else:\n                std_X[i] = np.std(X_copy[:, i], ddof=1)\n                X_copy[:, i] = (X_copy[:, i] - mean_X[i]) / std_X[i]\n        cod_X = X_copy\n    else:\n        cod_X = X_copy\n    return cod_X, min_X, max_X, mean_X, std_X\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.nat_to_cod_y","title":"<code>nat_to_cod_y(y, cod_type)</code>","text":"<p>Compute coded y-values from natural (physical or real world) units based on the setting of the <code>cod_type</code> attribute. If <code>cod_type</code> is \u201cnorm\u201d, the values are normalized to [0,1]. If <code>cod_type</code> is \u201cstd\u201d, the values are standardized. Otherwise, the values are not modified.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array</code> <p>The input array.</p> required <code>cod_type</code> <code>str</code> <p>The type of coding (\u201cnorm\u201d, \u201cstd\u201d, or other).</p> required <p>Returns:</p> Name Type Description <code>cod_y</code> <code>array</code> <p>The coded y-values.</p> <code>min_y</code> <code>array</code> <p>The minimum values of y.</p> <code>max_y</code> <code>array</code> <p>The maximum values of y.</p> <code>mean_y</code> <code>array</code> <p>The mean values of y.</p> <code>std_y</code> <code>array</code> <p>The standard deviation of y.</p> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def nat_to_cod_y(y, cod_type) -&gt; np.ndarray:\n    \"\"\"\n    Compute coded y-values from natural (physical or real world) units based on the\n    setting of the `cod_type` attribute. If `cod_type` is \"norm\", the values are\n    normalized to [0,1]. If `cod_type` is \"std\", the values are standardized.\n    Otherwise, the values are not modified.\n\n    Args:\n        y (np.array): The input array.\n        cod_type (str): The type of coding (\"norm\", \"std\", or other).\n\n    Returns:\n        cod_y (np.array):\n            The coded y-values.\n        min_y (np.array):\n            The minimum values of y.\n        max_y (np.array):\n            The maximum values of y.\n        mean_y (np.array):\n            The mean values of y.\n        std_y (np.array):\n            The standard deviation of y.\n    \"\"\"\n    mean_y = np.mean(y)\n    std_y = None\n    min_y = min(y)\n    max_y = max(y)\n    y_copy = copy.deepcopy(y)\n    if cod_type == \"norm\":\n        if (max_y - min_y) != 0:\n            cod_y = (y_copy - min_y) / (max_y - min_y)\n        else:\n            cod_y = 0.5 * np.ones_like(y_copy)\n    elif cod_type == \"std\":\n        if (max_y - min_y) != 0:\n            std_y = np.std(y, ddof=1)\n            cod_y = (y_copy - mean_y) / std_y\n        else:\n            cod_y = np.zeros_like(y_copy)\n    else:\n        cod_y = y_copy\n    return cod_y, min_y, max_y, mean_y, std_y\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.scale","title":"<code>scale(X, lower, upper)</code>","text":"<p>Sample scaling from unit hypercube to different bounds. Converts a sample from <code>[0, 1)</code> to <code>[a, b)</code>. The following transformation is used: <code>(b - a) * X + a</code></p> Note <p>equal lower and upper bounds are feasible.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>Sample to scale.</p> required <code>lower</code> <code>array</code> <p>lower bound of transformed data.</p> required <code>upper</code> <code>array</code> <p>upper bounds of transformed data.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Scaled sample.</p> <p>Examples:</p> <p>Transform three samples in the unit hypercube to (lower, upper) bounds:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.stats import qmc\n&gt;&gt;&gt; from spotpython.utils.transform import scale\n&gt;&gt;&gt; lower = np.array([6, 0])\n&gt;&gt;&gt; upper = np.array([6, 5])\n&gt;&gt;&gt; sample = np.array([[0.5 , 0.75],\n&gt;&gt;&gt;             [0.5 , 0.5],\n&gt;&gt;&gt;             [0.75, 0.25]])\n&gt;&gt;&gt; scale(sample, lower, upper)\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def scale(X: np.ndarray, lower: np.ndarray, upper: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Sample scaling from unit hypercube to different bounds. Converts a sample from `[0, 1)` to `[a, b)`.\n    The following transformation is used:\n    `(b - a) * X + a`\n\n    Note:\n        equal lower and upper bounds are feasible.\n\n    Args:\n        X (array):\n            Sample to scale.\n        lower (array):\n            lower bound of transformed data.\n        upper (array):\n            upper bounds of transformed data.\n\n    Returns:\n        (array):\n            Scaled sample.\n\n    Examples:\n        Transform three samples in the unit hypercube to (lower, upper) bounds:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from scipy.stats import qmc\n        &gt;&gt;&gt; from spotpython.utils.transform import scale\n        &gt;&gt;&gt; lower = np.array([6, 0])\n        &gt;&gt;&gt; upper = np.array([6, 5])\n        &gt;&gt;&gt; sample = np.array([[0.5 , 0.75],\n        &gt;&gt;&gt;             [0.5 , 0.5],\n        &gt;&gt;&gt;             [0.75, 0.25]])\n        &gt;&gt;&gt; scale(sample, lower, upper)\n\n    \"\"\"\n    # Checking that X is within (0,1) interval\n    if (X.max() &gt; 1.0) or (X.min() &lt; 0.0):\n        raise ValueError(\"Sample is not in unit hypercube\")\n    # Vectorized scaling operation\n    X = (upper - lower) * X + lower\n    # Handling case where lower == upper\n    X[:, lower == upper] = lower[lower == upper]\n    return X\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_hyper_parameter_values","title":"<code>transform_hyper_parameter_values(fun_control, hyper_parameter_values)</code>","text":"<p>Transform the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \u201cint\u201d, or \u201cfloat\u201d or \u201cnum\u201d. Let fun_control = {\u201ccore_model_hyper_dict\u201d:{ \u201cleaf_prediction\u201d: { \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d], \u201ctype\u201d: \u201cfactor\u201d, \u201cdefault\u201d: \u201cmean\u201d, \u201ccore_model_parameter_type\u201d: \u201cstr\u201d}, \u201cmax_depth\u201d: { \u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 20, \u201ctransform\u201d: \u201ctransform_power_2\u201d, \u201clower\u201d: 2, \u201cupper\u201d: 20}}} and v = {\u2018max_depth\u2019: 20,\u2019leaf_prediction\u2019: \u2018mean\u2019} and def transform_power_2(x): return 2**x. The function takes fun_control and v as input and returns a dictionary with the same structure as v. The function transforms the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \u201cint\u201d, or \u201cfloat\u201d or \u201cnum\u201d. For example, transform_hyper_parameter_values(fun_control, v) returns  {\u2018max_depth\u2019: 1048576, \u2018leaf_prediction\u2019: \u2018mean\u2019}.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>A dictionary containing the information about the core model and the hyperparameters.</p> required <code>hyper_parameter_values</code> <code>dict</code> <p>A dictionary containing the values of the hyperparameters.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the values of the hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_hyper_parameter_values\n    fun_control = {\n        \"core_model_hyper_dict\": {\n            \"leaf_prediction\": {\n                    \"type\": \"factor\",\n                    \"transform\": \"None\",\n                    \"default\": \"mean\",\n                    \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                    \"core_model_parameter_type\": \"str\"\n                                },\n            \"max_depth\": {\n                    \"type\": \"int\",\n                    \"default\": 20,\n                    \"transform\": \"transform_power_2\",\n                    \"lower\": 2,\n                    \"upper\": 20}\n                }\n        }\n    hyper_parameter_values = {\n            'max_depth': 2,\n            'leaf_prediction': 'mean'}\n    transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n        {'max_depth': 4, 'leaf_prediction': 'mean'}\n    fun_control = {\n        \"core_model_hyper_dict\": {\n            \"l1\": {\n                \"type\": \"int\",\n                \"default\": 3,\n                \"transform\": \"transform_power_2_int\",\n                \"lower\": 3,\n                \"upper\": 8\n            },\n            \"epochs\": {\n                \"type\": \"int\",\n                \"default\": 4,\n                \"transform\": \"transform_power_2_int\",\n                \"lower\": 4,\n                \"upper\": 9\n            },\n            \"batch_size\": {\n                \"type\": \"int\",\n                \"default\": 4,\n                \"transform\": \"transform_power_2_int\",\n                \"lower\": 1,\n                \"upper\": 4\n            },\n            \"act_fn\": {\n                \"levels\": [\n                    \"Sigmoid\",\n                    \"Tanh\",\n                    \"ReLU\",\n                    \"LeakyReLU\",\n                    \"ELU\",\n                    \"Swish\"\n                ],\n                \"type\": \"factor\",\n                \"default\": \"ReLU\",\n                \"transform\": \"None\",\n                \"class_name\": \"spotpython.torch.activation\",\n                \"core_model_parameter_type\": \"instance()\",\n                \"lower\": 0,\n                \"upper\": 5\n            },\n            \"optimizer\": {\n                \"levels\": [\n                    \"Adadelta\",\n                    \"Adagrad\",\n                    \"Adam\",\n                    \"AdamW\",\n                    \"SparseAdam\",\n                    \"Adamax\",\n                    \"ASGD\",\n                    \"NAdam\",\n                    \"RAdam\",\n                    \"RMSprop\",\n                    \"Rprop\",\n                    \"SGD\"\n                ],\n                \"type\": \"factor\",\n                \"default\": \"SGD\",\n                \"transform\": \"None\",\n                \"class_name\": \"torch.optim\",\n                \"core_model_parameter_type\": \"str\",\n                \"lower\": 0,\n                \"upper\": 11\n            },\n            \"dropout_prob\": {\n                \"type\": \"float\",\n                \"default\": 0.01,\n                \"transform\": \"None\",\n                \"lower\": 0.0,\n                \"upper\": 0.25\n            },\n            \"lr_mult\": {\n                \"type\": \"float\",\n                \"default\": 1.0,\n                \"transform\": \"None\",\n                \"lower\": 0.1,\n                \"upper\": 10.0\n            },\n            \"patience\": {\n                \"type\": \"int\",\n                \"default\": 2,\n                \"transform\": \"transform_power_2_int\",\n                \"lower\": 2,\n                \"upper\": 6\n            },\n            \"batch_norm\": {\n                \"levels\": [\n                    0,\n                    1\n                ],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"bool\",\n                \"lower\": 0,\n                \"upper\": 1\n            },\n            \"initialization\": {\n                \"levels\": [\n                    \"Default\",\n                    \"kaiming_uniform\",\n                    \"kaiming_normal\",\n                    \"xavier_uniform\",\n                    \"xavier_normal\"\n                ],\n                \"type\": \"factor\",\n                \"default\": \"Default\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"str\",\n                \"lower\": 0,\n                \"upper\": 4\n            }\n        }\n    }\n    hyper_parameter_values = {\n            'l1': 2,\n            'epochs': 3,\n            'batch_size': 4,\n            'act_fn': 'ReLU',\n            'optimizer': 'SGD',\n            'dropout_prob': 0.01,\n            'lr_mult': 1.0,\n            'patience': 3,\n            'batch_norm': 0,\n            'initialization': 'Default',\n        }\n    transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n        {'l1': 4,\n        'epochs': 8,\n        'batch_size': 16,\n        'act_fn': 'ReLU',\n        'optimizer': 'SGD',\n        'dropout_prob': 0.01,\n        'lr_mult': 1.0,\n        'patience': 8,\n        'batch_norm': 0,\n        'initialization': 'Default'}\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_hyper_parameter_values(fun_control, hyper_parameter_values):\n    \"\"\"\n    Transform the values of the hyperparameters according to the transform function specified in fun_control\n    if the hyperparameter is of type \"int\", or \"float\" or \"num\".\n    Let fun_control = {\"core_model_hyper_dict\":{ \"leaf_prediction\":\n    { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"},\n    \"max_depth\": { \"type\": \"int\", \"default\": 20, \"transform\": \"transform_power_2\", \"lower\": 2, \"upper\": 20}}}\n    and v = {'max_depth': 20,'leaf_prediction': 'mean'} and def transform_power_2(x): return 2**x.\n    The function takes fun_control and v as input and returns a dictionary with the same structure as v.\n    The function transforms the values of the hyperparameters according to the transform function\n    specified in fun_control if the hyperparameter is of type \"int\", or \"float\" or \"num\".\n    For example, transform_hyper_parameter_values(fun_control, v) returns\n     {'max_depth': 1048576, 'leaf_prediction': 'mean'}.\n\n    Args:\n        fun_control (dict):\n            A dictionary containing the information about the core model and the hyperparameters.\n        hyper_parameter_values (dict):\n            A dictionary containing the values of the hyperparameters.\n\n    Returns:\n        (dict):\n            A dictionary containing the values of the hyperparameters.\n\n    Examples:\n            &gt;&gt;&gt; from spotpython.utils.transform import transform_hyper_parameter_values\n                fun_control = {\n                    \"core_model_hyper_dict\": {\n                        \"leaf_prediction\": {\n                                \"type\": \"factor\",\n                                \"transform\": \"None\",\n                                \"default\": \"mean\",\n                                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                                \"core_model_parameter_type\": \"str\"\n                                            },\n                        \"max_depth\": {\n                                \"type\": \"int\",\n                                \"default\": 20,\n                                \"transform\": \"transform_power_2\",\n                                \"lower\": 2,\n                                \"upper\": 20}\n                            }\n                    }\n                hyper_parameter_values = {\n                        'max_depth': 2,\n                        'leaf_prediction': 'mean'}\n                transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n                    {'max_depth': 4, 'leaf_prediction': 'mean'}\n                fun_control = {\n                    \"core_model_hyper_dict\": {\n                        \"l1\": {\n                            \"type\": \"int\",\n                            \"default\": 3,\n                            \"transform\": \"transform_power_2_int\",\n                            \"lower\": 3,\n                            \"upper\": 8\n                        },\n                        \"epochs\": {\n                            \"type\": \"int\",\n                            \"default\": 4,\n                            \"transform\": \"transform_power_2_int\",\n                            \"lower\": 4,\n                            \"upper\": 9\n                        },\n                        \"batch_size\": {\n                            \"type\": \"int\",\n                            \"default\": 4,\n                            \"transform\": \"transform_power_2_int\",\n                            \"lower\": 1,\n                            \"upper\": 4\n                        },\n                        \"act_fn\": {\n                            \"levels\": [\n                                \"Sigmoid\",\n                                \"Tanh\",\n                                \"ReLU\",\n                                \"LeakyReLU\",\n                                \"ELU\",\n                                \"Swish\"\n                            ],\n                            \"type\": \"factor\",\n                            \"default\": \"ReLU\",\n                            \"transform\": \"None\",\n                            \"class_name\": \"spotpython.torch.activation\",\n                            \"core_model_parameter_type\": \"instance()\",\n                            \"lower\": 0,\n                            \"upper\": 5\n                        },\n                        \"optimizer\": {\n                            \"levels\": [\n                                \"Adadelta\",\n                                \"Adagrad\",\n                                \"Adam\",\n                                \"AdamW\",\n                                \"SparseAdam\",\n                                \"Adamax\",\n                                \"ASGD\",\n                                \"NAdam\",\n                                \"RAdam\",\n                                \"RMSprop\",\n                                \"Rprop\",\n                                \"SGD\"\n                            ],\n                            \"type\": \"factor\",\n                            \"default\": \"SGD\",\n                            \"transform\": \"None\",\n                            \"class_name\": \"torch.optim\",\n                            \"core_model_parameter_type\": \"str\",\n                            \"lower\": 0,\n                            \"upper\": 11\n                        },\n                        \"dropout_prob\": {\n                            \"type\": \"float\",\n                            \"default\": 0.01,\n                            \"transform\": \"None\",\n                            \"lower\": 0.0,\n                            \"upper\": 0.25\n                        },\n                        \"lr_mult\": {\n                            \"type\": \"float\",\n                            \"default\": 1.0,\n                            \"transform\": \"None\",\n                            \"lower\": 0.1,\n                            \"upper\": 10.0\n                        },\n                        \"patience\": {\n                            \"type\": \"int\",\n                            \"default\": 2,\n                            \"transform\": \"transform_power_2_int\",\n                            \"lower\": 2,\n                            \"upper\": 6\n                        },\n                        \"batch_norm\": {\n                            \"levels\": [\n                                0,\n                                1\n                            ],\n                            \"type\": \"factor\",\n                            \"default\": 0,\n                            \"transform\": \"None\",\n                            \"core_model_parameter_type\": \"bool\",\n                            \"lower\": 0,\n                            \"upper\": 1\n                        },\n                        \"initialization\": {\n                            \"levels\": [\n                                \"Default\",\n                                \"kaiming_uniform\",\n                                \"kaiming_normal\",\n                                \"xavier_uniform\",\n                                \"xavier_normal\"\n                            ],\n                            \"type\": \"factor\",\n                            \"default\": \"Default\",\n                            \"transform\": \"None\",\n                            \"core_model_parameter_type\": \"str\",\n                            \"lower\": 0,\n                            \"upper\": 4\n                        }\n                    }\n                }\n                hyper_parameter_values = {\n                        'l1': 2,\n                        'epochs': 3,\n                        'batch_size': 4,\n                        'act_fn': 'ReLU',\n                        'optimizer': 'SGD',\n                        'dropout_prob': 0.01,\n                        'lr_mult': 1.0,\n                        'patience': 3,\n                        'batch_norm': 0,\n                        'initialization': 'Default',\n                    }\n                transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n                    {'l1': 4,\n                    'epochs': 8,\n                    'batch_size': 16,\n                    'act_fn': 'ReLU',\n                    'optimizer': 'SGD',\n                    'dropout_prob': 0.01,\n                    'lr_mult': 1.0,\n                    'patience': 8,\n                    'batch_norm': 0,\n                    'initialization': 'Default'}\n    \"\"\"\n    hyper_parameter_values = copy.deepcopy(hyper_parameter_values)\n    for key, value in hyper_parameter_values.items():\n        if fun_control[\"core_model_hyper_dict\"][key][\"type\"] in [\"int\", \"float\", \"num\", \"factor\"] and fun_control[\"core_model_hyper_dict\"][key][\"transform\"] != \"None\":\n            hyper_parameter_values[key] = eval(fun_control[\"core_model_hyper_dict\"][key][\"transform\"])(value)\n    return hyper_parameter_values\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_multby2_int","title":"<code>transform_multby2_int(x)</code>","text":"<p>Transformations for hyperparameters of type int.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>input, will be multiplied by 2</p> required <p>Returns:</p> Type Description <code>int</code> <p>The result of multiplying x by 2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_multby2_int\n&gt;&gt;&gt; transform_multby2_int(3)\n6\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_multby2_int(x: int) -&gt; int:\n    \"\"\"Transformations for hyperparameters of type int.\n\n    Args:\n        x (int):\n            input, will be multiplied by 2\n\n    Returns:\n        (int):\n            The result of multiplying x by 2.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_multby2_int\n        &gt;&gt;&gt; transform_multby2_int(3)\n        6\n    \"\"\"\n    return int(2 * x)\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_none_to_None","title":"<code>transform_none_to_None(x)</code>","text":"<p>Transformations for hyperparameters of type None. If the input is \u201cnone\u201d, the output is None.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>The string to transform.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The transformed string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_none_to_None\n&gt;&gt;&gt; transform_none_to_None(\"none\")\nNone\n</code></pre> Note <p>Needed for sklearn.linear_model.LogisticRegression</p> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_none_to_None(x) -&gt; str:\n    \"\"\"\n    Transformations for hyperparameters of type None.\n    If the input is \"none\", the output is None.\n\n    Args:\n        x (str): The string to transform.\n\n    Returns:\n        (str): The transformed string.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_none_to_None\n        &gt;&gt;&gt; transform_none_to_None(\"none\")\n        None\n\n    Note:\n        Needed for sklearn.linear_model.LogisticRegression\n    \"\"\"\n    if x == \"none\":\n        return None\n    else:\n        return x\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_power","title":"<code>transform_power(base, x, as_int=False)</code>","text":"<p>Raises a given base to the power of x.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>int</code> <p>The base to raise to the power of x.</p> required <code>x</code> <code>int</code> <p>The exponent.</p> required <code>as_int</code> <code>bool</code> <p>If True, returns the result as an integer.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The result of raising the base to the power of x.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_power\n&gt;&gt;&gt; transform_power(2, 3)\n8\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_power(base: int, x: int, as_int: bool = False) -&gt; float:\n    \"\"\"\n    Raises a given base to the power of x.\n\n    Args:\n        base (int):\n            The base to raise to the power of x.\n        x (int):\n            The exponent.\n        as_int (bool):\n            If True, returns the result as an integer.\n\n    Returns:\n        (float):\n            The result of raising the base to the power of x.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_power\n        &gt;&gt;&gt; transform_power(2, 3)\n        8\n    \"\"\"\n    result = base**x\n    if as_int:\n        result = int(result)\n    return result\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_power_10","title":"<code>transform_power_10(x)</code>","text":"<p>Transformations for hyperparameters of type float.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The exponent.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The result of raising 10 to the power of x.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_power_10\n&gt;&gt;&gt; transform_power_10(3)\n1000\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_power_10(x) -&gt; float:\n    \"\"\"Transformations for hyperparameters of type float.\n\n    Args:\n        x (float): The exponent.\n\n    Returns:\n        (float): The result of raising 10 to the power of x.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_power_10\n        &gt;&gt;&gt; transform_power_10(3)\n        1000\n    \"\"\"\n    return 10**x\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_power_10_int","title":"<code>transform_power_10_int(x)</code>","text":"<p>Transformations for hyperparameters of type int. Args:     x (int): The exponent.</p> <p>Returns:</p> Type Description <code>int</code> <p>The result of raising 10 to the power of x.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_power_10_int\n&gt;&gt;&gt; transform_power_10_int(3)\n1000\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_power_10_int(x: int) -&gt; int:\n    \"\"\"Transformations for hyperparameters of type int.\n    Args:\n        x (int): The exponent.\n\n    Returns:\n        (int): The result of raising 10 to the power of x.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_power_10_int\n        &gt;&gt;&gt; transform_power_10_int(3)\n        1000\n    \"\"\"\n    return int(10**x)\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_power_2","title":"<code>transform_power_2(x)</code>","text":"<p>Transformations for hyperparameters of type float.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The exponent.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The result of raising 2 to the power of x.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_power_2\n&gt;&gt;&gt; transform_power_2(3)\n8\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_power_2(x) -&gt; float:\n    \"\"\"Transformations for hyperparameters of type float.\n\n    Args:\n        x (float): The exponent.\n\n    Returns:\n        (float): The result of raising 2 to the power of x.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_power_2\n        &gt;&gt;&gt; transform_power_2(3)\n        8\n    \"\"\"\n    return 2**x\n</code></pre>"},{"location":"reference/spotpython/utils/transform/#spotpython.utils.transform.transform_power_2_int","title":"<code>transform_power_2_int(x)</code>","text":"<p>Transformations for hyperparameters of type int.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>The exponent.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The result of raising 2 to the power of x.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.transform import transform_power_2_int\n&gt;&gt;&gt; transform_power_2_int(3)\n8\n</code></pre> Source code in <code>spotpython/utils/transform.py</code> <pre><code>def transform_power_2_int(x: int) -&gt; int:\n    \"\"\"Transformations for hyperparameters of type int.\n\n    Args:\n        x (int): The exponent.\n\n    Returns:\n        (int): The result of raising 2 to the power of x.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.transform import transform_power_2_int\n        &gt;&gt;&gt; transform_power_2_int(3)\n        8\n    \"\"\"\n    return int(2**x)\n</code></pre>"}]}