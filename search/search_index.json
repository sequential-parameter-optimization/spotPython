{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"spotPython \u00b6 Surrogate Model Based Optimization and Hyperparameter Tuning in Python \u00b6 Documentation for spotPython see Hyperparameter Tuning Cookbook , a guide for scikit-learn, PyTorch, river, and spotPython. News and updates related to spotPython see SPOTSeven","title":"Home"},{"location":"#spotpython","text":"","title":"spotPython"},{"location":"#surrogate-model-based-optimization-and-hyperparameter-tuning-in-python","text":"Documentation for spotPython see Hyperparameter Tuning Cookbook , a guide for scikit-learn, PyTorch, river, and spotPython. News and updates related to spotPython see SPOTSeven","title":"Surrogate Model Based Optimization and Hyperparameter Tuning in Python"},{"location":"about/","text":"Contact/Privacy Policy \u00b6 Address \u00b6 Prof. Dr. Thomas Bartz-Beielstein TH K\u00f6ln Raum 1.519 Steinm\u00fcllerallee 6 51643 Gummersbach +49 (0)2261 8196 6391 thomas.bartz-beielstein [at] th-koeln.de www.spotseven.de Privacy Policy \u00b6 We are very delighted that you have shown interest in our enterprise. Data protection is of a particularly high priority for the management of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. The use of the Internet pages of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is possible without any indication of personal data; however, if a data subject wants to use special enterprise services via our website, processing of personal data could become necessary. If the processing of personal data is necessary and there is no statutory basis for such processing, we generally obtain consent from the data subject. The processing of personal data, such as the name, address, e-mail address, or telephone number of a data subject shall always be in line with the General Data Protection Regulation (GDPR), and in accordance with the country-specific data protection regulations applicable to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. By means of this data protection declaration, our enterprise would like to inform the general public of the nature, scope, and purpose of the personal data we collect, use and process. Furthermore, data subjects are informed, by means of this data protection declaration, of the rights to which they are entitled. As the controller, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab has implemented numerous technical and organizational measures to ensure the most complete protection of personal data processed through this website. However, Internet-based data transmissions may in principle have security gaps, so absolute protection may not be guaranteed. For this reason, every data subject is free to transfer personal data to us via alternative means, e.g. by telephone. Definitions The data protection declaration of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is based on the terms used by the European legislator for the adoption of the General Data Protection Regulation (GDPR). Our data protection declaration should be legible and understandable for the general public, as well as our customers and business partners. To ensure this, we would like to first explain the terminology used. In this data protection declaration, we use, inter alia, the following terms: a) Personal data Personal data means any information relating to an identified or identifiable natural person (\u201cdata subject\u201d). An identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. b) Data subject Data subject is any identified or identifiable natural person, whose personal data is processed by the controller responsible for the processing. c) Processing Processing is any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction. d) Restriction of processing Restriction of processing is the marking of stored personal data with the aim of limiting their processing in the future. e) Profiling Profiling means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person\u2019s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements. f) Pseudonymisation Pseudonymisation is the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person. g) Controller or controller responsible for the processing Controller or controller responsible for the processing is the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law. h) Processor Processor is a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller. i) Recipient Recipient is a natural or legal person, public authority, agency or another body, to which the personal data are disclosed, whether a third party or not. However, public authorities which may receive personal data in the framework of a particular inquiry in accordance with Union or Member State law shall not be regarded as recipients; the processing of those data by those public authorities shall be in compliance with the applicable data protection rules according to the purposes of the processing. j) Third party Third party is a natural or legal person, public authority, agency or body other than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data. k) Consent Consent of the data subject is any freely given, specific, informed and unambiguous indication of the data subject\u2019s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her. Name and Address of the controller Controller for the purposes of the General Data Protection Regulation (GDPR), other data protection laws applicable in Member states of the European Union and other provisions related to data protection is: TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab Steinm\u00fcllerallee 1 51643 Gummersbach Deutschland Phone: +49 2261 81966391 Email: thomas.bartz-beielstein@th-koeln.de Website: www.spotseven.de Collection of general data and information The website of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab collects a series of general data and information when a data subject or automated system calls up the website. This general data and information are stored in the server log files. Collected may be (1) the browser types and versions used, (2) the operating system used by the accessing system, (3) the website from which an accessing system reaches our website (so-called referrers), (4) the sub-websites, (5) the date and time of access to the Internet site, (6) an Internet protocol address (IP address), (7) the Internet service provider of the accessing system, and (8) any other similar data and information that may be used in the event of attacks on our information technology systems. When using these general data and information, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab does not draw any conclusions about the data subject. Rather, this information is needed to (1) deliver the content of our website correctly, (2) optimize the content of our website as well as its advertisement, (3) ensure the long-term viability of our information technology systems and website technology, and (4) provide law enforcement authorities with the information necessary for criminal prosecution in case of a cyber-attack. Therefore, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab analyzes anonymously collected data and information statistically, with the aim of increasing the data protection and data security of our enterprise, and to ensure an optimal level of protection for the personal data we process. The anonymous data of the server log files are stored separately from all personal data provided by a data subject. Comments function in the blog on the website The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab offers users the possibility to leave individual comments on individual blog contributions on a blog, which is on the website of the controller. A blog is a web-based, publicly-accessible portal, through which one or more people called bloggers or web-bloggers may post articles or write down thoughts in so-called blogposts. Blogposts may usually be commented by third parties. If a data subject leaves a comment on the blog published on this website, the comments made by the data subject are also stored and published, as well as information on the date of the commentary and on the user\u2019s (pseudonym) chosen by the data subject. In addition, the IP address assigned by the Internet service provider (ISP) to the data subject is also logged. This storage of the IP address takes place for security reasons, and in case the data subject violates the rights of third parties, or posts illegal content through a given comment. The storage of these personal data is, therefore, in the own interest of the data controller, so that he can exculpate in the event of an infringement. This collected personal data will not be passed to third parties, unless such a transfer is required by law or serves the aim of the defense of the data controller. Routine erasure and blocking of personal data The data controller shall process and store the personal data of the data subject only for the period necessary to achieve the purpose of storage, or as far as this is granted by the European legislator or other legislators in laws or regulations to which the controller is subject to. If the storage purpose is not applicable, or if a storage period prescribed by the European legislator or another competent legislator expires, the personal data are routinely blocked or erased in accordance with legal requirements. Rights of the data subject a) Right of confirmation Each data subject shall have the right granted by the European legislator to obtain from the controller the confirmation as to whether or not personal data concerning him or her are being processed. If a data subject wishes to avail himself of this right of confirmation, he or she may, at any time, contact our Data Protection Officer or another employee of the controller. b) Right of access Each data subject shall have the right granted by the European legislator to obtain from the controller free information about his or her personal data stored at any time and a copy of this information. Furthermore, the European directives and regulations grant the data subject access to the following information: the purposes of the processing; the categories of personal data concerned; the recipients or categories of recipients to whom the personal data have been or will be disclosed, in particular recipients in third countries or international organisations; where possible, the envisaged period for which the personal data will be stored, or, if not possible, the criteria used to determine that period; the existence of the right to request from the controller rectification or erasure of personal data, or restriction of processing of personal data concerning the data subject, or to object to such processing; the existence of the right to lodge a complaint with a supervisory authority; where the personal data are not collected from the data subject, any available information as to their source; the existence of automated decision-making, including profiling, referred to in Article 22(1) and (4) of the GDPR and, at least in those cases, meaningful information about the logic involved, as well as the significance and envisaged consequences of such processing for the data subject. Furthermore, the data subject shall have a right to obtain information as to whether personal data are transferred to a third country or to an international organisation. Where this is the case, the data subject shall have the right to be informed of the appropriate safeguards relating to the transfer. If a data subject wishes to avail himself of this right of access, he or she may at any time contact our Data Protection Officer or another employee of the controller. c) Right to rectification Each data subject shall have the right granted by the European legislator to obtain from the controller without undue delay the rectification of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement. If a data subject wishes to exercise this right to rectification, he or she may, at any time, contact our Data Protection Officer or another employee of the controller. d) Right to erasure (Right to be forgotten) Each data subject shall have the right granted by the European legislator to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies, as long as the processing is not necessary: The personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed. The data subject withdraws consent to which the processing is based according to point (a) of Article 6(1) of the GDPR, or point (a) of Article 9(2) of the GDPR, and where there is no other legal ground for the processing. The data subject objects to the processing pursuant to Article 21(1) of the GDPR and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2) of the GDPR. The personal data have been unlawfully processed. The personal data must be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject. The personal data have been collected in relation to the offer of information society services referred to in Article 8(1) of the GDPR. If one of the aforementioned reasons applies, and a data subject wishes to request the erasure of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee shall promptly ensure that the erasure request is complied with immediately. Where the controller has made personal data public and is obliged pursuant to Article 17(1) to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform other controllers processing the personal data that the data subject has requested erasure by such controllers of any links to, or copy or replication of, those personal data, as far as processing is not required. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the necessary measures in individual cases. e) Right of restriction of processing Each data subject shall have the right granted by the European legislator to obtain from the controller restriction of processing where one of the following applies: The accuracy of the personal data is contested by the data subject, for a period enabling the controller to verify the accuracy of the personal data. The processing is unlawful and the data subject opposes the erasure of the personal data and requests instead the restriction of their use instead. The controller no longer needs the personal data for the purposes of the processing, but they are required by the data subject for the establishment, exercise or defence of legal claims. The data subject has objected to processing pursuant to Article 21(1) of the GDPR pending the verification whether the legitimate grounds of the controller override those of the data subject. If one of the aforementioned conditions is met, and a data subject wishes to request the restriction of the processing of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the restriction of the processing. f) Right to data portability Each data subject shall have the right granted by the European legislator, to receive the personal data concerning him or her, which was provided to a controller, in a structured, commonly used and machine-readable format. He or she shall have the right to transmit those data to another controller without hindrance from the controller to which the personal data have been provided, as long as the processing is based on consent pursuant to point (a) of Article 6(1) of the GDPR or point (a) of Article 9(2) of the GDPR, or on a contract pursuant to point (b) of Article 6(1) of the GDPR, and the processing is carried out by automated means, as long as the processing is not necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller. Furthermore, in exercising his or her right to data portability pursuant to Article 20(1) of the GDPR, the data subject shall have the right to have personal data transmitted directly from one controller to another, where technically feasible and when doing so does not adversely affect the rights and freedoms of others. In order to assert the right to data portability, the data subject may at any time contact the Data Protection Officer designated by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee. g) Right to object Each data subject shall have the right granted by the European legislator to object, on grounds relating to his or her particular situation, at any time, to processing of personal data concerning him or her, which is based on point (e) or (f) of Article 6(1) of the GDPR. This also applies to profiling based on these provisions. The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall no longer process the personal data in the event of the objection, unless we can demonstrate compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject, or for the establishment, exercise or defence of legal claims. If the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab processes personal data for direct marketing purposes, the data subject shall have the right to object at any time to processing of personal data concerning him or her for such marketing. This applies to profiling to the extent that it is related to such direct marketing. If the data subject objects to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab to the processing for direct marketing purposes, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab will no longer process the personal data for these purposes. In addition, the data subject has the right, on grounds relating to his or her particular situation, to object to processing of personal data concerning him or her by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab for scientific or historical research purposes, or for statistical purposes pursuant to Article 89(1) of the GDPR, unless the processing is necessary for the performance of a task carried out for reasons of public interest. In order to exercise the right to object, the data subject may directly contact the Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee. In addition, the data subject is free in the context of the use of information society services, and notwithstanding Directive 2002/58/EC, to use his or her right to object by automated means using technical specifications. h) Automated individual decision-making, including profiling Each data subject shall have the right granted by the European legislator not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her, or similarly significantly affects him or her, as long as the decision (1) is not is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) is not authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, or (3) is not based on the data subject\u2019s explicit consent. If the decision (1) is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) it is based on the data subject\u2019s explicit consent, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall implement suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and contest the decision. If the data subject wishes to exercise the rights concerning automated individual decision-making, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller. i) Right to withdraw data protection consent Each data subject shall have the right granted by the European legislator to withdraw his or her consent to processing of his or her personal data at any time. f the data subject wishes to exercise the right to withdraw the consent, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller. Data protection provisions about the application and use of Facebook On this website, the controller has integrated components of the enterprise Facebook. Facebook is a social network. A social network is a place for social meetings on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Facebook allows social network users to include the creation of private profiles, upload photos, and network through friend requests. The operating company of Facebook is Facebook, Inc., 1 Hacker Way, Menlo Park, CA 94025, United States. If a person lives outside of the United States or Canada, the controller is the Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Ireland. With each call-up to one of the individual pages of this Internet website, which is operated by the controller and into which a Facebook component (Facebook plug-ins) was integrated, the web browser on the information technology system of the data subject is automatically prompted to download display of the corresponding Facebook component from Facebook through the Facebook component. An overview of all the Facebook Plug-ins may be accessed under https://developers.facebook.com/docs/plugins/. During the course of this technical procedure, Facebook is made aware of what specific sub-site of our website was visited by the data subject. If the data subject is logged in at the same time on Facebook, Facebook detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-site of our Internet page was visited by the data subject. This information is collected through the Facebook component and associated with the respective Facebook account of the data subject. If the data subject clicks on one of the Facebook buttons integrated into our website, e.g. the \u201cLike\u201d button, or if the data subject submits a comment, then Facebook matches this information with the personal Facebook user account of the data subject and stores the personal data. Facebook always receives, through the Facebook component, information about a visit to our website by the data subject, whenever the data subject is logged in at the same time on Facebook during the time of the call-up to our website. This occurs regardless of whether the data subject clicks on the Facebook component or not. If such a transmission of information to Facebook is not desirable for the data subject, then he or she may prevent this by logging off from their Facebook account before a call-up to our website is made. The data protection guideline published by Facebook, which is available at https://facebook.com/about/privacy/, provides information about the collection, processing and use of personal data by Facebook. In addition, it is explained there what setting options Facebook offers to protect the privacy of the data subject. In addition, different configuration options are made available to allow the elimination of data transmission to Facebook, e.g. the Facebook blocker of the provider Webgraph, which may be obtained under http://webgraph.com/resources/facebookblocker/. These applications may be used by the data subject to eliminate a data transmission to Facebook. Data protection provisions about the application and use of Google+ On this website, the controller has integrated the Google+ button as a component. Google+ is a so-called social network. A social network is a social meeting place on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Google+ allows users of the social network to include the creation of private profiles, upload photos and network through friend requests. The operating company of Google+ is Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES. With each call-up to one of the individual pages of this website, which is operated by the controller and on which a Google+ button has been integrated, the Internet browser on the information technology system of the data subject automatically downloads a display of the corresponding Google+ button of Google through the respective Google+ button component. During the course of this technical procedure, Google is made aware of what specific sub-page of our website was visited by the data subject. More detailed information about Google+ is available under https://developers.google.com/+/. If the data subject is logged in at the same time to Google+, Google recognizes with each call-up to our website by the data subject and for the entire duration of his or her stay on our Internet site, which specific sub-pages of our Internet page were visited by the data subject. This information is collected through the Google+ button and Google matches this with the respective Google+ account associated with the data subject. If the data subject clicks on the Google+ button integrated on our website and thus gives a Google+ 1 recommendation, then Google assigns this information to the personal Google+ user account of the data subject and stores the personal data. Google stores the Google+ 1 recommendation of the data subject, making it publicly available in accordance with the terms and conditions accepted by the data subject in this regard. Subsequently, a Google+ 1 recommendation given by the data subject on this website together with other personal data, such as the Google+ account name used by the data subject and the stored photo, is stored and processed on other Google services, such as search-engine results of the Google search engine, the Google account of the data subject or in other places, e.g. on Internet pages, or in relation to advertisements. Google is also able to link the visit to this website with other personal data stored on Google. Google further records this personal information with the purpose of improving or optimizing the various Google services. Through the Google+ button, Google receives information that the data subject visited our website, if the data subject at the time of the call-up to our website is logged in to Google+. This occurs regardless of whether the data subject clicks or doesn\u2019t click on the Google+ button. If the data subject does not wish to transmit personal data to Google, he or she may prevent such transmission by logging out of his Google+ account before calling up our website. Further information and the data protection provisions of Google may be retrieved under https://www.google.com/intl/en/policies/privacy/. More references from Google about the Google+ 1 button may be obtained under https://developers.google.com/+/web/buttons-policy. Data protection provisions about the application and use of Jetpack for WordPress On this website, the controller has integrated Jetpack. Jetpack is a WordPress plug-in, which provides additional features to the operator of a website based on WordPress. Jetpack allows the Internet site operator, inter alia, an overview of the visitors of the site. By displaying related posts and publications, or the ability to share content on the page, it is also possible to increase visitor numbers. In addition, security features are integrated into Jetpack, so a Jetpack-using site is better protected against brute-force attacks. Jetpack also optimizes and accelerates the loading of images on the website. The operating company of Jetpack Plug-Ins for WordPress is the Automattic Inc., 132 Hawthorne Street, San Francisco, CA 94107, UNITED STATES. The operating enterprise uses the tracking technology created by Quantcast Inc., 201 Third Street, San Francisco, CA 94103, UNITED STATES. Jetpack sets a cookie on the information technology system used by the data subject. The definition of cookies is explained above. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Jetpack component was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to submit data through the Jetpack component for analysis purposes to Automattic. During the course of this technical procedure Automattic receives data that is used to create an overview of website visits. The data obtained in this way serves the analysis of the behaviour of the data subject, which has access to the Internet page of the controller and is analyzed with the aim to optimize the website. The data collected through the Jetpack component is not used to identify the data subject without a prior obtaining of a separate express consent of the data subject. The data comes also to the notice of Quantcast. Quantcast uses the data for the same purposes as Automattic. The data subject can, as stated above, prevent the setting of cookies through our website at any time by means of a corresponding adjustment of the web browser used and thus permanently deny the setting of cookies. Such an adjustment to the Internet browser used would also prevent Automattic/Quantcast from setting a cookie on the information technology system of the data subject. In addition, cookies already in use by Automattic/Quantcast may be deleted at any time via a web browser or other software programs. In addition, the data subject has the possibility of objecting to a collection of data relating to a use of this Internet site that are generated by the Jetpack cookie as well as the processing of these data by Automattic/Quantcast and the chance to preclude any such. For this purpose, the data subject must press the \u2018opt-out\u2019 button under the link https://www.quantcast.com/opt-out/ which sets an opt-out cookie. The opt-out cookie set with this purpose is placed on the information technology system used by the data subject. If the cookies are deleted on the system of the data subject, then the data subject must call up the link again and set a new opt-out cookie. With the setting of the opt-out cookie, however, the possibility exists that the websites of the controller are not fully usable anymore by the data subject. The applicable data protection provisions of Automattic may be accessed under https://automattic.com/privacy/. The applicable data protection provisions of Quantcast can be accessed under https://www.quantcast.com/privacy/. Data protection provisions about the application and use of LinkedIn The controller has integrated components of the LinkedIn Corporation on this website. LinkedIn is a web-based social network that enables users with existing business contacts to connect and to make new business contacts. Over 400 million registered people in more than 200 countries use LinkedIn. Thus, LinkedIn is currently the largest platform for business contacts and one of the most visited websites in the world. The operating company of LinkedIn is LinkedIn Corporation, 2029 Stierlin Court Mountain View, CA 94043, UNITED STATES. For privacy matters outside of the UNITED STATES LinkedIn Ireland, Privacy Policy Issues, Wilton Plaza, Wilton Place, Dublin 2, Ireland, is responsible. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a LinkedIn component (LinkedIn plug-in) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to the download of a display of the corresponding LinkedIn component of LinkedIn. Further information about the LinkedIn plug-in may be accessed under https://developer.linkedin.com/plugins. During the course of this technical procedure, LinkedIn gains knowledge of what specific sub-page of our website was visited by the data subject. If the data subject is logged in at the same time on LinkedIn, LinkedIn detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-page of our Internet page was visited by the data subject. This information is collected through the LinkedIn component and associated with the respective LinkedIn account of the data subject. If the data subject clicks on one of the LinkedIn buttons integrated on our website, then LinkedIn assigns this information to the personal LinkedIn user account of the data subject and stores the personal data. LinkedIn receives information via the LinkedIn component that the data subject has visited our website, provided that the data subject is logged in at LinkedIn at the time of the call-up to our website. This occurs regardless of whether the person clicks on the LinkedIn button or not. If such a transmission of information to LinkedIn is not desirable for the data subject, then he or she may prevent this by logging off from their LinkedIn account before a call-up to our website is made. LinkedIn provides under https://www.linkedin.com/psettings/guest-controls the possibility to unsubscribe from e-mail messages, SMS messages and targeted ads, as well as the ability to manage ad settings. LinkedIn also uses affiliates such as Eire, Google Analytics, BlueKai, DoubleClick, Nielsen, Comscore, Eloqua, and Lotame. The setting of such cookies may be denied under https://www.linkedin.com/legal/cookie-policy. The applicable privacy policy for LinkedIn is available under https://www.linkedin.com/legal/privacy-policy. The LinkedIn Cookie Policy is available under https://www.linkedin.com/legal/cookie-policy. Data protection provisions about the application and use of Twitter On this website, the controller has integrated components of Twitter. Twitter is a multilingual, publicly-accessible microblogging service on which users may publish and spread so-called \u2018tweets,\u2019 e.g. short messages, which are limited to 140 characters. These short messages are available for everyone, including those who are not logged on to Twitter. The tweets are also displayed to so-called followers of the respective user. Followers are other Twitter users who follow a user\u2019s tweets. Furthermore, Twitter allows you to address a wide audience via hashtags, links or retweets. The operating company of Twitter is Twitter, Inc., 1355 Market Street, Suite 900, San Francisco, CA 94103, UNITED STATES. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Twitter component (Twitter button) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding Twitter component of Twitter. Further information about the Twitter buttons is available under https://about.twitter.com/de/resources/buttons. During the course of this technical procedure, Twitter gains knowledge of what specific sub-page of our website was visited by the data subject. The purpose of the integration of the Twitter component is a retransmission of the contents of this website to allow our users to introduce this web page to the digital world and increase our visitor numbers. If the data subject is logged in at the same time on Twitter, Twitter detects with every call-up to our website by the data subject and for the entire duration of their stay on our Internet site which specific sub-page of our Internet page was visited by the data subject. This information is collected through the Twitter component and associated with the respective Twitter account of the data subject. If the data subject clicks on one of the Twitter buttons integrated on our website, then Twitter assigns this information to the personal Twitter user account of the data subject and stores the personal data. Twitter receives information via the Twitter component that the data subject has visited our website, provided that the data subject is logged in on Twitter at the time of the call-up to our website. This occurs regardless of whether the person clicks on the Twitter component or not. If such a transmission of information to Twitter is not desirable for the data subject, then he or she may prevent this by logging off from their Twitter account before a call-up to our website is made. The applicable data protection provisions of Twitter may be accessed under https://twitter.com/privacy?lang=en. Data protection provisions about the application and use of YouTube On this website, the controller has integrated components of YouTube. YouTube is an Internet video portal that enables video publishers to set video clips and other users free of charge, which also provides free viewing, review and commenting on them. YouTube allows you to publish all kinds of videos, so you can access both full movies and TV broadcasts, as well as music videos, trailers, and videos made by users via the Internet portal. The operating company of YouTube is YouTube, LLC, 901 Cherry Ave., San Bruno, CA 94066, UNITED STATES. The YouTube, LLC is a subsidiary of Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a YouTube component (YouTube video) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding YouTube component. Further information about YouTube may be obtained under https://www.youtube.com/yt/about/en/. During the course of this technical procedure, YouTube and Google gain knowledge of what specific sub-page of our website was visited by the data subject. If the data subject is logged in on YouTube, YouTube recognizes with each call-up to a sub-page that contains a YouTube video, which specific sub-page of our Internet site was visited by the data subject. This information is collected by YouTube and Google and assigned to the respective YouTube account of the data subject. YouTube and Google will receive information through the YouTube component that the data subject has visited our website, if the data subject at the time of the call to our website is logged in on YouTube; this occurs regardless of whether the person clicks on a YouTube video or not. If such a transmission of this information to YouTube and Google is not desirable for the data subject, the delivery may be prevented if the data subject logs off from their own YouTube account before a call-up to our website is made. YouTube\u2019s data protection provisions, available at https://www.google.com/intl/en/policies/privacy/, provide information about the collection, processing and use of personal data by YouTube and Google. Legal basis for the processing Art. 6(1) lit. a GDPR serves as the legal basis for processing operations for which we obtain consent for a specific processing purpose. If the processing of personal data is necessary for the performance of a contract to which the data subject is party, as is the case, for example, when processing operations are necessary for the supply of goods or to provide any other service, the processing is based on Article 6(1) lit. b GDPR. The same applies to such processing operations which are necessary for carrying out pre-contractual measures, for example in the case of inquiries concerning our products or services. Is our company subject to a legal obligation by which processing of personal data is required, such as for the fulfillment of tax obligations, the processing is based on Art. 6(1) lit. c GDPR. In rare cases, the processing of personal data may be necessary to protect the vital interests of the data subject or of another natural person. This would be the case, for example, if a visitor were injured in our company and his name, age, health insurance data or other vital information would have to be passed on to a doctor, hospital or other third party. Then the processing would be based on Art. 6(1) lit. d GDPR. Finally, processing operations could be based on Article 6(1) lit. f GDPR. This legal basis is used for processing operations which are not covered by any of the abovementioned legal grounds, if processing is necessary for the purposes of the legitimate interests pursued by our company or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data. Such processing operations are particularly permissible because they have been specifically mentioned by the European legislator. He considered that a legitimate interest could be assumed if the data subject is a client of the controller (Recital 47 Sentence 2 GDPR). The legitimate interests pursued by the controller or by a third party Where the processing of personal data is based on Article 6(1) lit. f GDPR our legitimate interest is to carry out our business in favor of the well-being of all our employees and the shareholders. Period for which the personal data will be stored The criteria used to determine the period of storage of personal data is the respective statutory retention period. After expiration of that period, the corresponding data is routinely deleted, as long as it is no longer necessary for the fulfillment of the contract or the initiation of a contract. Provision of personal data as statutory or contractual requirement; Requirement necessary to enter into a contract; Obligation of the data subject to provide the personal data; possible consequences of failure to provide such data We clarify that the provision of personal data is partly required by law (e.g. tax regulations) or can also result from contractual provisions (e.g. information on the contractual partner). Sometimes it may be necessary to conclude a contract that the data subject provides us with personal data, which must subsequently be processed by us. The data subject is, for example, obliged to provide us with personal data when our company signs a contract with him or her. The non-provision of the personal data would have the consequence that the contract with the data subject could not be concluded. Before personal data is provided by the data subject, the data subject must contact our Data Protection Officer. Our Data Protection Officer clarifies to the data subject whether the provision of the personal data is required by law or contract or is necessary for the conclusion of the contract, whether there is an obligation to provide the personal data and the consequences of non-provision of the personal data. Existence of automated decision-making As a responsible company, we do not use automatic decision-making or profiling. This Privacy Policy has been generated by the Privacy Policy Generator of the External Data Protection Officers that was developed in cooperation with RC GmbH, which sells used notebooks and the Media Law Lawyers from WBS-LAW.","title":"About"},{"location":"about/#contactprivacy-policy","text":"","title":"Contact/Privacy Policy"},{"location":"about/#address","text":"Prof. Dr. Thomas Bartz-Beielstein TH K\u00f6ln Raum 1.519 Steinm\u00fcllerallee 6 51643 Gummersbach +49 (0)2261 8196 6391 thomas.bartz-beielstein [at] th-koeln.de www.spotseven.de","title":"Address"},{"location":"about/#privacy-policy","text":"We are very delighted that you have shown interest in our enterprise. Data protection is of a particularly high priority for the management of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. The use of the Internet pages of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is possible without any indication of personal data; however, if a data subject wants to use special enterprise services via our website, processing of personal data could become necessary. If the processing of personal data is necessary and there is no statutory basis for such processing, we generally obtain consent from the data subject. The processing of personal data, such as the name, address, e-mail address, or telephone number of a data subject shall always be in line with the General Data Protection Regulation (GDPR), and in accordance with the country-specific data protection regulations applicable to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. By means of this data protection declaration, our enterprise would like to inform the general public of the nature, scope, and purpose of the personal data we collect, use and process. Furthermore, data subjects are informed, by means of this data protection declaration, of the rights to which they are entitled. As the controller, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab has implemented numerous technical and organizational measures to ensure the most complete protection of personal data processed through this website. However, Internet-based data transmissions may in principle have security gaps, so absolute protection may not be guaranteed. For this reason, every data subject is free to transfer personal data to us via alternative means, e.g. by telephone. Definitions The data protection declaration of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is based on the terms used by the European legislator for the adoption of the General Data Protection Regulation (GDPR). Our data protection declaration should be legible and understandable for the general public, as well as our customers and business partners. To ensure this, we would like to first explain the terminology used. In this data protection declaration, we use, inter alia, the following terms: a) Personal data Personal data means any information relating to an identified or identifiable natural person (\u201cdata subject\u201d). An identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. b) Data subject Data subject is any identified or identifiable natural person, whose personal data is processed by the controller responsible for the processing. c) Processing Processing is any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction. d) Restriction of processing Restriction of processing is the marking of stored personal data with the aim of limiting their processing in the future. e) Profiling Profiling means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person\u2019s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements. f) Pseudonymisation Pseudonymisation is the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person. g) Controller or controller responsible for the processing Controller or controller responsible for the processing is the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law. h) Processor Processor is a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller. i) Recipient Recipient is a natural or legal person, public authority, agency or another body, to which the personal data are disclosed, whether a third party or not. However, public authorities which may receive personal data in the framework of a particular inquiry in accordance with Union or Member State law shall not be regarded as recipients; the processing of those data by those public authorities shall be in compliance with the applicable data protection rules according to the purposes of the processing. j) Third party Third party is a natural or legal person, public authority, agency or body other than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data. k) Consent Consent of the data subject is any freely given, specific, informed and unambiguous indication of the data subject\u2019s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her. Name and Address of the controller Controller for the purposes of the General Data Protection Regulation (GDPR), other data protection laws applicable in Member states of the European Union and other provisions related to data protection is: TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab Steinm\u00fcllerallee 1 51643 Gummersbach Deutschland Phone: +49 2261 81966391 Email: thomas.bartz-beielstein@th-koeln.de Website: www.spotseven.de Collection of general data and information The website of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab collects a series of general data and information when a data subject or automated system calls up the website. This general data and information are stored in the server log files. Collected may be (1) the browser types and versions used, (2) the operating system used by the accessing system, (3) the website from which an accessing system reaches our website (so-called referrers), (4) the sub-websites, (5) the date and time of access to the Internet site, (6) an Internet protocol address (IP address), (7) the Internet service provider of the accessing system, and (8) any other similar data and information that may be used in the event of attacks on our information technology systems. When using these general data and information, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab does not draw any conclusions about the data subject. Rather, this information is needed to (1) deliver the content of our website correctly, (2) optimize the content of our website as well as its advertisement, (3) ensure the long-term viability of our information technology systems and website technology, and (4) provide law enforcement authorities with the information necessary for criminal prosecution in case of a cyber-attack. Therefore, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab analyzes anonymously collected data and information statistically, with the aim of increasing the data protection and data security of our enterprise, and to ensure an optimal level of protection for the personal data we process. The anonymous data of the server log files are stored separately from all personal data provided by a data subject. Comments function in the blog on the website The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab offers users the possibility to leave individual comments on individual blog contributions on a blog, which is on the website of the controller. A blog is a web-based, publicly-accessible portal, through which one or more people called bloggers or web-bloggers may post articles or write down thoughts in so-called blogposts. Blogposts may usually be commented by third parties. If a data subject leaves a comment on the blog published on this website, the comments made by the data subject are also stored and published, as well as information on the date of the commentary and on the user\u2019s (pseudonym) chosen by the data subject. In addition, the IP address assigned by the Internet service provider (ISP) to the data subject is also logged. This storage of the IP address takes place for security reasons, and in case the data subject violates the rights of third parties, or posts illegal content through a given comment. The storage of these personal data is, therefore, in the own interest of the data controller, so that he can exculpate in the event of an infringement. This collected personal data will not be passed to third parties, unless such a transfer is required by law or serves the aim of the defense of the data controller. Routine erasure and blocking of personal data The data controller shall process and store the personal data of the data subject only for the period necessary to achieve the purpose of storage, or as far as this is granted by the European legislator or other legislators in laws or regulations to which the controller is subject to. If the storage purpose is not applicable, or if a storage period prescribed by the European legislator or another competent legislator expires, the personal data are routinely blocked or erased in accordance with legal requirements. Rights of the data subject a) Right of confirmation Each data subject shall have the right granted by the European legislator to obtain from the controller the confirmation as to whether or not personal data concerning him or her are being processed. If a data subject wishes to avail himself of this right of confirmation, he or she may, at any time, contact our Data Protection Officer or another employee of the controller. b) Right of access Each data subject shall have the right granted by the European legislator to obtain from the controller free information about his or her personal data stored at any time and a copy of this information. Furthermore, the European directives and regulations grant the data subject access to the following information: the purposes of the processing; the categories of personal data concerned; the recipients or categories of recipients to whom the personal data have been or will be disclosed, in particular recipients in third countries or international organisations; where possible, the envisaged period for which the personal data will be stored, or, if not possible, the criteria used to determine that period; the existence of the right to request from the controller rectification or erasure of personal data, or restriction of processing of personal data concerning the data subject, or to object to such processing; the existence of the right to lodge a complaint with a supervisory authority; where the personal data are not collected from the data subject, any available information as to their source; the existence of automated decision-making, including profiling, referred to in Article 22(1) and (4) of the GDPR and, at least in those cases, meaningful information about the logic involved, as well as the significance and envisaged consequences of such processing for the data subject. Furthermore, the data subject shall have a right to obtain information as to whether personal data are transferred to a third country or to an international organisation. Where this is the case, the data subject shall have the right to be informed of the appropriate safeguards relating to the transfer. If a data subject wishes to avail himself of this right of access, he or she may at any time contact our Data Protection Officer or another employee of the controller. c) Right to rectification Each data subject shall have the right granted by the European legislator to obtain from the controller without undue delay the rectification of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement. If a data subject wishes to exercise this right to rectification, he or she may, at any time, contact our Data Protection Officer or another employee of the controller. d) Right to erasure (Right to be forgotten) Each data subject shall have the right granted by the European legislator to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies, as long as the processing is not necessary: The personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed. The data subject withdraws consent to which the processing is based according to point (a) of Article 6(1) of the GDPR, or point (a) of Article 9(2) of the GDPR, and where there is no other legal ground for the processing. The data subject objects to the processing pursuant to Article 21(1) of the GDPR and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2) of the GDPR. The personal data have been unlawfully processed. The personal data must be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject. The personal data have been collected in relation to the offer of information society services referred to in Article 8(1) of the GDPR. If one of the aforementioned reasons applies, and a data subject wishes to request the erasure of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee shall promptly ensure that the erasure request is complied with immediately. Where the controller has made personal data public and is obliged pursuant to Article 17(1) to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform other controllers processing the personal data that the data subject has requested erasure by such controllers of any links to, or copy or replication of, those personal data, as far as processing is not required. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the necessary measures in individual cases. e) Right of restriction of processing Each data subject shall have the right granted by the European legislator to obtain from the controller restriction of processing where one of the following applies: The accuracy of the personal data is contested by the data subject, for a period enabling the controller to verify the accuracy of the personal data. The processing is unlawful and the data subject opposes the erasure of the personal data and requests instead the restriction of their use instead. The controller no longer needs the personal data for the purposes of the processing, but they are required by the data subject for the establishment, exercise or defence of legal claims. The data subject has objected to processing pursuant to Article 21(1) of the GDPR pending the verification whether the legitimate grounds of the controller override those of the data subject. If one of the aforementioned conditions is met, and a data subject wishes to request the restriction of the processing of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the restriction of the processing. f) Right to data portability Each data subject shall have the right granted by the European legislator, to receive the personal data concerning him or her, which was provided to a controller, in a structured, commonly used and machine-readable format. He or she shall have the right to transmit those data to another controller without hindrance from the controller to which the personal data have been provided, as long as the processing is based on consent pursuant to point (a) of Article 6(1) of the GDPR or point (a) of Article 9(2) of the GDPR, or on a contract pursuant to point (b) of Article 6(1) of the GDPR, and the processing is carried out by automated means, as long as the processing is not necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller. Furthermore, in exercising his or her right to data portability pursuant to Article 20(1) of the GDPR, the data subject shall have the right to have personal data transmitted directly from one controller to another, where technically feasible and when doing so does not adversely affect the rights and freedoms of others. In order to assert the right to data portability, the data subject may at any time contact the Data Protection Officer designated by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee. g) Right to object Each data subject shall have the right granted by the European legislator to object, on grounds relating to his or her particular situation, at any time, to processing of personal data concerning him or her, which is based on point (e) or (f) of Article 6(1) of the GDPR. This also applies to profiling based on these provisions. The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall no longer process the personal data in the event of the objection, unless we can demonstrate compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject, or for the establishment, exercise or defence of legal claims. If the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab processes personal data for direct marketing purposes, the data subject shall have the right to object at any time to processing of personal data concerning him or her for such marketing. This applies to profiling to the extent that it is related to such direct marketing. If the data subject objects to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab to the processing for direct marketing purposes, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab will no longer process the personal data for these purposes. In addition, the data subject has the right, on grounds relating to his or her particular situation, to object to processing of personal data concerning him or her by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab for scientific or historical research purposes, or for statistical purposes pursuant to Article 89(1) of the GDPR, unless the processing is necessary for the performance of a task carried out for reasons of public interest. In order to exercise the right to object, the data subject may directly contact the Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee. In addition, the data subject is free in the context of the use of information society services, and notwithstanding Directive 2002/58/EC, to use his or her right to object by automated means using technical specifications. h) Automated individual decision-making, including profiling Each data subject shall have the right granted by the European legislator not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her, or similarly significantly affects him or her, as long as the decision (1) is not is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) is not authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, or (3) is not based on the data subject\u2019s explicit consent. If the decision (1) is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) it is based on the data subject\u2019s explicit consent, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall implement suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and contest the decision. If the data subject wishes to exercise the rights concerning automated individual decision-making, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller. i) Right to withdraw data protection consent Each data subject shall have the right granted by the European legislator to withdraw his or her consent to processing of his or her personal data at any time. f the data subject wishes to exercise the right to withdraw the consent, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller. Data protection provisions about the application and use of Facebook On this website, the controller has integrated components of the enterprise Facebook. Facebook is a social network. A social network is a place for social meetings on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Facebook allows social network users to include the creation of private profiles, upload photos, and network through friend requests. The operating company of Facebook is Facebook, Inc., 1 Hacker Way, Menlo Park, CA 94025, United States. If a person lives outside of the United States or Canada, the controller is the Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Ireland. With each call-up to one of the individual pages of this Internet website, which is operated by the controller and into which a Facebook component (Facebook plug-ins) was integrated, the web browser on the information technology system of the data subject is automatically prompted to download display of the corresponding Facebook component from Facebook through the Facebook component. An overview of all the Facebook Plug-ins may be accessed under https://developers.facebook.com/docs/plugins/. During the course of this technical procedure, Facebook is made aware of what specific sub-site of our website was visited by the data subject. If the data subject is logged in at the same time on Facebook, Facebook detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-site of our Internet page was visited by the data subject. This information is collected through the Facebook component and associated with the respective Facebook account of the data subject. If the data subject clicks on one of the Facebook buttons integrated into our website, e.g. the \u201cLike\u201d button, or if the data subject submits a comment, then Facebook matches this information with the personal Facebook user account of the data subject and stores the personal data. Facebook always receives, through the Facebook component, information about a visit to our website by the data subject, whenever the data subject is logged in at the same time on Facebook during the time of the call-up to our website. This occurs regardless of whether the data subject clicks on the Facebook component or not. If such a transmission of information to Facebook is not desirable for the data subject, then he or she may prevent this by logging off from their Facebook account before a call-up to our website is made. The data protection guideline published by Facebook, which is available at https://facebook.com/about/privacy/, provides information about the collection, processing and use of personal data by Facebook. In addition, it is explained there what setting options Facebook offers to protect the privacy of the data subject. In addition, different configuration options are made available to allow the elimination of data transmission to Facebook, e.g. the Facebook blocker of the provider Webgraph, which may be obtained under http://webgraph.com/resources/facebookblocker/. These applications may be used by the data subject to eliminate a data transmission to Facebook. Data protection provisions about the application and use of Google+ On this website, the controller has integrated the Google+ button as a component. Google+ is a so-called social network. A social network is a social meeting place on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Google+ allows users of the social network to include the creation of private profiles, upload photos and network through friend requests. The operating company of Google+ is Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES. With each call-up to one of the individual pages of this website, which is operated by the controller and on which a Google+ button has been integrated, the Internet browser on the information technology system of the data subject automatically downloads a display of the corresponding Google+ button of Google through the respective Google+ button component. During the course of this technical procedure, Google is made aware of what specific sub-page of our website was visited by the data subject. More detailed information about Google+ is available under https://developers.google.com/+/. If the data subject is logged in at the same time to Google+, Google recognizes with each call-up to our website by the data subject and for the entire duration of his or her stay on our Internet site, which specific sub-pages of our Internet page were visited by the data subject. This information is collected through the Google+ button and Google matches this with the respective Google+ account associated with the data subject. If the data subject clicks on the Google+ button integrated on our website and thus gives a Google+ 1 recommendation, then Google assigns this information to the personal Google+ user account of the data subject and stores the personal data. Google stores the Google+ 1 recommendation of the data subject, making it publicly available in accordance with the terms and conditions accepted by the data subject in this regard. Subsequently, a Google+ 1 recommendation given by the data subject on this website together with other personal data, such as the Google+ account name used by the data subject and the stored photo, is stored and processed on other Google services, such as search-engine results of the Google search engine, the Google account of the data subject or in other places, e.g. on Internet pages, or in relation to advertisements. Google is also able to link the visit to this website with other personal data stored on Google. Google further records this personal information with the purpose of improving or optimizing the various Google services. Through the Google+ button, Google receives information that the data subject visited our website, if the data subject at the time of the call-up to our website is logged in to Google+. This occurs regardless of whether the data subject clicks or doesn\u2019t click on the Google+ button. If the data subject does not wish to transmit personal data to Google, he or she may prevent such transmission by logging out of his Google+ account before calling up our website. Further information and the data protection provisions of Google may be retrieved under https://www.google.com/intl/en/policies/privacy/. More references from Google about the Google+ 1 button may be obtained under https://developers.google.com/+/web/buttons-policy. Data protection provisions about the application and use of Jetpack for WordPress On this website, the controller has integrated Jetpack. Jetpack is a WordPress plug-in, which provides additional features to the operator of a website based on WordPress. Jetpack allows the Internet site operator, inter alia, an overview of the visitors of the site. By displaying related posts and publications, or the ability to share content on the page, it is also possible to increase visitor numbers. In addition, security features are integrated into Jetpack, so a Jetpack-using site is better protected against brute-force attacks. Jetpack also optimizes and accelerates the loading of images on the website. The operating company of Jetpack Plug-Ins for WordPress is the Automattic Inc., 132 Hawthorne Street, San Francisco, CA 94107, UNITED STATES. The operating enterprise uses the tracking technology created by Quantcast Inc., 201 Third Street, San Francisco, CA 94103, UNITED STATES. Jetpack sets a cookie on the information technology system used by the data subject. The definition of cookies is explained above. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Jetpack component was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to submit data through the Jetpack component for analysis purposes to Automattic. During the course of this technical procedure Automattic receives data that is used to create an overview of website visits. The data obtained in this way serves the analysis of the behaviour of the data subject, which has access to the Internet page of the controller and is analyzed with the aim to optimize the website. The data collected through the Jetpack component is not used to identify the data subject without a prior obtaining of a separate express consent of the data subject. The data comes also to the notice of Quantcast. Quantcast uses the data for the same purposes as Automattic. The data subject can, as stated above, prevent the setting of cookies through our website at any time by means of a corresponding adjustment of the web browser used and thus permanently deny the setting of cookies. Such an adjustment to the Internet browser used would also prevent Automattic/Quantcast from setting a cookie on the information technology system of the data subject. In addition, cookies already in use by Automattic/Quantcast may be deleted at any time via a web browser or other software programs. In addition, the data subject has the possibility of objecting to a collection of data relating to a use of this Internet site that are generated by the Jetpack cookie as well as the processing of these data by Automattic/Quantcast and the chance to preclude any such. For this purpose, the data subject must press the \u2018opt-out\u2019 button under the link https://www.quantcast.com/opt-out/ which sets an opt-out cookie. The opt-out cookie set with this purpose is placed on the information technology system used by the data subject. If the cookies are deleted on the system of the data subject, then the data subject must call up the link again and set a new opt-out cookie. With the setting of the opt-out cookie, however, the possibility exists that the websites of the controller are not fully usable anymore by the data subject. The applicable data protection provisions of Automattic may be accessed under https://automattic.com/privacy/. The applicable data protection provisions of Quantcast can be accessed under https://www.quantcast.com/privacy/. Data protection provisions about the application and use of LinkedIn The controller has integrated components of the LinkedIn Corporation on this website. LinkedIn is a web-based social network that enables users with existing business contacts to connect and to make new business contacts. Over 400 million registered people in more than 200 countries use LinkedIn. Thus, LinkedIn is currently the largest platform for business contacts and one of the most visited websites in the world. The operating company of LinkedIn is LinkedIn Corporation, 2029 Stierlin Court Mountain View, CA 94043, UNITED STATES. For privacy matters outside of the UNITED STATES LinkedIn Ireland, Privacy Policy Issues, Wilton Plaza, Wilton Place, Dublin 2, Ireland, is responsible. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a LinkedIn component (LinkedIn plug-in) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to the download of a display of the corresponding LinkedIn component of LinkedIn. Further information about the LinkedIn plug-in may be accessed under https://developer.linkedin.com/plugins. During the course of this technical procedure, LinkedIn gains knowledge of what specific sub-page of our website was visited by the data subject. If the data subject is logged in at the same time on LinkedIn, LinkedIn detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-page of our Internet page was visited by the data subject. This information is collected through the LinkedIn component and associated with the respective LinkedIn account of the data subject. If the data subject clicks on one of the LinkedIn buttons integrated on our website, then LinkedIn assigns this information to the personal LinkedIn user account of the data subject and stores the personal data. LinkedIn receives information via the LinkedIn component that the data subject has visited our website, provided that the data subject is logged in at LinkedIn at the time of the call-up to our website. This occurs regardless of whether the person clicks on the LinkedIn button or not. If such a transmission of information to LinkedIn is not desirable for the data subject, then he or she may prevent this by logging off from their LinkedIn account before a call-up to our website is made. LinkedIn provides under https://www.linkedin.com/psettings/guest-controls the possibility to unsubscribe from e-mail messages, SMS messages and targeted ads, as well as the ability to manage ad settings. LinkedIn also uses affiliates such as Eire, Google Analytics, BlueKai, DoubleClick, Nielsen, Comscore, Eloqua, and Lotame. The setting of such cookies may be denied under https://www.linkedin.com/legal/cookie-policy. The applicable privacy policy for LinkedIn is available under https://www.linkedin.com/legal/privacy-policy. The LinkedIn Cookie Policy is available under https://www.linkedin.com/legal/cookie-policy. Data protection provisions about the application and use of Twitter On this website, the controller has integrated components of Twitter. Twitter is a multilingual, publicly-accessible microblogging service on which users may publish and spread so-called \u2018tweets,\u2019 e.g. short messages, which are limited to 140 characters. These short messages are available for everyone, including those who are not logged on to Twitter. The tweets are also displayed to so-called followers of the respective user. Followers are other Twitter users who follow a user\u2019s tweets. Furthermore, Twitter allows you to address a wide audience via hashtags, links or retweets. The operating company of Twitter is Twitter, Inc., 1355 Market Street, Suite 900, San Francisco, CA 94103, UNITED STATES. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Twitter component (Twitter button) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding Twitter component of Twitter. Further information about the Twitter buttons is available under https://about.twitter.com/de/resources/buttons. During the course of this technical procedure, Twitter gains knowledge of what specific sub-page of our website was visited by the data subject. The purpose of the integration of the Twitter component is a retransmission of the contents of this website to allow our users to introduce this web page to the digital world and increase our visitor numbers. If the data subject is logged in at the same time on Twitter, Twitter detects with every call-up to our website by the data subject and for the entire duration of their stay on our Internet site which specific sub-page of our Internet page was visited by the data subject. This information is collected through the Twitter component and associated with the respective Twitter account of the data subject. If the data subject clicks on one of the Twitter buttons integrated on our website, then Twitter assigns this information to the personal Twitter user account of the data subject and stores the personal data. Twitter receives information via the Twitter component that the data subject has visited our website, provided that the data subject is logged in on Twitter at the time of the call-up to our website. This occurs regardless of whether the person clicks on the Twitter component or not. If such a transmission of information to Twitter is not desirable for the data subject, then he or she may prevent this by logging off from their Twitter account before a call-up to our website is made. The applicable data protection provisions of Twitter may be accessed under https://twitter.com/privacy?lang=en. Data protection provisions about the application and use of YouTube On this website, the controller has integrated components of YouTube. YouTube is an Internet video portal that enables video publishers to set video clips and other users free of charge, which also provides free viewing, review and commenting on them. YouTube allows you to publish all kinds of videos, so you can access both full movies and TV broadcasts, as well as music videos, trailers, and videos made by users via the Internet portal. The operating company of YouTube is YouTube, LLC, 901 Cherry Ave., San Bruno, CA 94066, UNITED STATES. The YouTube, LLC is a subsidiary of Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a YouTube component (YouTube video) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding YouTube component. Further information about YouTube may be obtained under https://www.youtube.com/yt/about/en/. During the course of this technical procedure, YouTube and Google gain knowledge of what specific sub-page of our website was visited by the data subject. If the data subject is logged in on YouTube, YouTube recognizes with each call-up to a sub-page that contains a YouTube video, which specific sub-page of our Internet site was visited by the data subject. This information is collected by YouTube and Google and assigned to the respective YouTube account of the data subject. YouTube and Google will receive information through the YouTube component that the data subject has visited our website, if the data subject at the time of the call to our website is logged in on YouTube; this occurs regardless of whether the person clicks on a YouTube video or not. If such a transmission of this information to YouTube and Google is not desirable for the data subject, the delivery may be prevented if the data subject logs off from their own YouTube account before a call-up to our website is made. YouTube\u2019s data protection provisions, available at https://www.google.com/intl/en/policies/privacy/, provide information about the collection, processing and use of personal data by YouTube and Google. Legal basis for the processing Art. 6(1) lit. a GDPR serves as the legal basis for processing operations for which we obtain consent for a specific processing purpose. If the processing of personal data is necessary for the performance of a contract to which the data subject is party, as is the case, for example, when processing operations are necessary for the supply of goods or to provide any other service, the processing is based on Article 6(1) lit. b GDPR. The same applies to such processing operations which are necessary for carrying out pre-contractual measures, for example in the case of inquiries concerning our products or services. Is our company subject to a legal obligation by which processing of personal data is required, such as for the fulfillment of tax obligations, the processing is based on Art. 6(1) lit. c GDPR. In rare cases, the processing of personal data may be necessary to protect the vital interests of the data subject or of another natural person. This would be the case, for example, if a visitor were injured in our company and his name, age, health insurance data or other vital information would have to be passed on to a doctor, hospital or other third party. Then the processing would be based on Art. 6(1) lit. d GDPR. Finally, processing operations could be based on Article 6(1) lit. f GDPR. This legal basis is used for processing operations which are not covered by any of the abovementioned legal grounds, if processing is necessary for the purposes of the legitimate interests pursued by our company or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data. Such processing operations are particularly permissible because they have been specifically mentioned by the European legislator. He considered that a legitimate interest could be assumed if the data subject is a client of the controller (Recital 47 Sentence 2 GDPR). The legitimate interests pursued by the controller or by a third party Where the processing of personal data is based on Article 6(1) lit. f GDPR our legitimate interest is to carry out our business in favor of the well-being of all our employees and the shareholders. Period for which the personal data will be stored The criteria used to determine the period of storage of personal data is the respective statutory retention period. After expiration of that period, the corresponding data is routinely deleted, as long as it is no longer necessary for the fulfillment of the contract or the initiation of a contract. Provision of personal data as statutory or contractual requirement; Requirement necessary to enter into a contract; Obligation of the data subject to provide the personal data; possible consequences of failure to provide such data We clarify that the provision of personal data is partly required by law (e.g. tax regulations) or can also result from contractual provisions (e.g. information on the contractual partner). Sometimes it may be necessary to conclude a contract that the data subject provides us with personal data, which must subsequently be processed by us. The data subject is, for example, obliged to provide us with personal data when our company signs a contract with him or her. The non-provision of the personal data would have the consequence that the contract with the data subject could not be concluded. Before personal data is provided by the data subject, the data subject must contact our Data Protection Officer. Our Data Protection Officer clarifies to the data subject whether the provision of the personal data is required by law or contract or is necessary for the conclusion of the contract, whether there is an obligation to provide the personal data and the consequences of non-provision of the personal data. Existence of automated decision-making As a responsible company, we do not use automatic decision-making or profiling. This Privacy Policy has been generated by the Privacy Policy Generator of the External Data Protection Officers that was developed in cooperation with RC GmbH, which sells used notebooks and the Media Law Lawyers from WBS-LAW.","title":"Privacy Policy"},{"location":"download/","text":"Install spotPython \u00b6 pip install spotPython","title":"Download"},{"location":"download/#install-spotpython","text":"pip install spotPython","title":"Install spotPython"},{"location":"examples/","text":"SPOT Examples \u00b6 Simple spotPython run \u00b6 import numpy as np from math import inf from spotPython.fun.objectivefunctions import analytical from spotPython.spot import spot # number of initial points: ni = 7 # number of points n = 10 fun = analytical().fun_sphere lower = np.array([-1]) upper = np.array([1]) design_control={\"init_size\": ni} spot_1 = spot.Spot(fun=fun, lower = lower, upper= upper, fun_evals = n, show_progress=True, design_control=design_control,) spot_1.run() Further Examples \u00b6 Examples can be found in the Hyperparameter Tuning Cookbook, e.g., Documentation of the Sequential Parameter Optimization .","title":"Examples"},{"location":"examples/#spot-examples","text":"","title":"SPOT Examples"},{"location":"examples/#simple-spotpython-run","text":"import numpy as np from math import inf from spotPython.fun.objectivefunctions import analytical from spotPython.spot import spot # number of initial points: ni = 7 # number of points n = 10 fun = analytical().fun_sphere lower = np.array([-1]) upper = np.array([1]) design_control={\"init_size\": ni} spot_1 = spot.Spot(fun=fun, lower = lower, upper= upper, fun_evals = n, show_progress=True, design_control=design_control,) spot_1.run()","title":"Simple spotPython run"},{"location":"examples/#further-examples","text":"Examples can be found in the Hyperparameter Tuning Cookbook, e.g., Documentation of the Sequential Parameter Optimization .","title":"Further Examples"},{"location":"hyperparameter-tuning-cookbook/","text":"Hyperparameter Tuning Cookbook \u00b6 The following is a cookbook of hyperparameter tuning recipes. It is not meant to be exhaustive, but instead act as a place to capture a number of the common patterns used in hyperparameter tuning. Hyperparameter Tuning Cookbook","title":"Documentation"},{"location":"hyperparameter-tuning-cookbook/#hyperparameter-tuning-cookbook","text":"The following is a cookbook of hyperparameter tuning recipes. It is not meant to be exhaustive, but instead act as a place to capture a number of the common patterns used in hyperparameter tuning. Hyperparameter Tuning Cookbook","title":"Hyperparameter Tuning Cookbook"},{"location":"reference/SUMMARY/","text":"spotPython budget ocba build kriging surrogates data base light_hyper_dict sklearn_hyper_dict torch_hyper_dict torchdata vbdp design designs factorial spacefilling fun hyperlight hypersklearn hypertorch objectivefunctions hyperparameters categorical optimizer values light cifar10datamodule crossvalidationdatamodule csvdatamodule csvdataset litmodel mnistdatamodule netlightbase netlinearbase traintest traintest_NEW traintest_OLD utils plot contour validation sklearn traintest spot spot torch activation dataframedataset initialization mapk netcifar10 netcore netfashionMNIST netregression netvbdp traintest utils aggregate classes compare convert device eda file init metrics progress repair transform","title":"SUMMARY"},{"location":"reference/spotPython/budget/ocba/","text":"OCBA: Optimal Computing Budget Allocation get_ocba ( means , vars , delta ) \u00b6 Optimal Computer Budget Allocation (OCBA) This function calculates the budget recommendations for a given set of means, variances, and incremental budget using the OCBA algorithm. References [1]: Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization: An Optimal Computer Budget Allocation, pp. 49 and pp. 215 [2]: C.S.M Currie and T. Monks: How to choose the best setup for a system. A tutorial for the Simulation Workshop 2021, see: sw21_tutorial.ipynb and sim-tools Parameters: Name Type Description Default means numpy . array An array of means. required vars numpy . array An array of variances. required delta int The incremental budget. required Returns: Type Description numpy . array An array of budget recommendations. Note The implementation is based on the pseudo-code in the Chen et al. (p. 49), see [1]. Examples: From the Chen et al. book (p. 49): >>> mean_y = np . array ([ 1 , 2 , 3 , 4 , 5 ]) var_y = np.array([1,1,9,9,4]) get_ocba(mean_y, var_y, 50) [11 9 19 9 2] Source code in spotPython/budget/ocba.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_ocba ( means , vars , delta ) -> int32 : \"\"\" Optimal Computer Budget Allocation (OCBA) This function calculates the budget recommendations for a given set of means, variances, and incremental budget using the OCBA algorithm. References: [1]: Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization: An Optimal Computer Budget Allocation, pp. 49 and pp. 215 [2]: C.S.M Currie and T. Monks: How to choose the best setup for a system. A tutorial for the Simulation Workshop 2021, see: [sw21_tutorial.ipynb](https://colab.research.google.com/github/TomMonks/sim-tools/blob/master/examples/sw21_tutorial.ipynb) and [sim-tools](https://github.com/TomMonks/sim-tools) Args: means (numpy.array): An array of means. vars (numpy.array): An array of variances. delta (int): The incremental budget. Returns: (numpy.array): An array of budget recommendations. Note: The implementation is based on the pseudo-code in the Chen et al. (p. 49), see [1]. Examples: From the Chen et al. book (p. 49): >>> mean_y = np.array([1,2,3,4,5]) var_y = np.array([1,1,9,9,4]) get_ocba(mean_y, var_y, 50) [11 9 19 9 2] \"\"\" n_designs = means . shape [ 0 ] allocations = zeros ( n_designs , int32 ) ratios = zeros ( n_designs , float64 ) budget = delta ranks = get_ranks ( means ) best , second_best = argpartition ( ranks , 2 )[: 2 ] ratios [ second_best ] = 1.0 select = [ i for i in range ( n_designs ) if i not in [ best , second_best ]] temp = ( means [ best ] - means [ second_best ]) / ( means [ best ] - means [ select ]) ratios [ select ] = square ( temp ) * ( vars [ select ] / vars [ second_best ]) select = [ i for i in range ( n_designs ) if i not in [ best ]] temp = ( square ( ratios [ select ]) / vars [ select ]) . sum () ratios [ best ] = sqrt ( vars [ best ] * temp ) more_runs = full ( n_designs , True , dtype = bool ) add_budget = zeros ( n_designs , dtype = float ) more_alloc = True while more_alloc : more_alloc = False ratio_s = ( more_runs * ratios ) . sum () add_budget [ more_runs ] = ( budget / ratio_s ) * ratios [ more_runs ] add_budget = around ( add_budget ) . astype ( int ) mask = add_budget < allocations add_budget [ mask ] = allocations [ mask ] more_runs [ mask ] = 0 if mask . sum () > 0 : more_alloc = True if more_alloc : budget = allocations . sum () + delta budget -= ( add_budget * ~ more_runs ) . sum () t_budget = add_budget . sum () add_budget [ best ] += allocations . sum () + delta - t_budget return add_budget - allocations get_ocba_X ( X , means , vars , delta ) \u00b6 This function calculates the OCBA allocation and repeats the input array X along the specified axis. Parameters: Name Type Description Default X numpy . ndarray Input array to be repeated. required means list List of means for each alternative. required vars list List of variances for each alternative. required delta float Indifference zone parameter. required Returns: Type Description numpy . ndarray Repeated array of X along the specified axis based on the OCBA allocation. Examples: >>> X = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ]]) means = [1,2,3] vars = [1,1,1] delta = 50 get_ocba_X(X, means, vars, delta) array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [4, 5, 6], [4, 5, 6], [4, 5, 6]]) Source code in spotPython/budget/ocba.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def get_ocba_X ( X , means , vars , delta ) -> float64 : \"\"\" This function calculates the OCBA allocation and repeats the input array X along the specified axis. Args: X (numpy.ndarray): Input array to be repeated. means (list): List of means for each alternative. vars (list): List of variances for each alternative. delta (float): Indifference zone parameter. Returns: (numpy.ndarray): Repeated array of X along the specified axis based on the OCBA allocation. Examples: >>> X = np.array([[1,2,3],[4,5,6]]) means = [1,2,3] vars = [1,1,1] delta = 50 get_ocba_X(X, means, vars, delta) array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [4, 5, 6], [4, 5, 6], [4, 5, 6]]) \"\"\" o = get_ocba ( means = means , vars = vars , delta = delta ) return repeat ( X , o , axis = 0 )","title":"ocba"},{"location":"reference/spotPython/budget/ocba/#spotPython.budget.ocba.get_ocba","text":"Optimal Computer Budget Allocation (OCBA) This function calculates the budget recommendations for a given set of means, variances, and incremental budget using the OCBA algorithm. References [1]: Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization: An Optimal Computer Budget Allocation, pp. 49 and pp. 215 [2]: C.S.M Currie and T. Monks: How to choose the best setup for a system. A tutorial for the Simulation Workshop 2021, see: sw21_tutorial.ipynb and sim-tools Parameters: Name Type Description Default means numpy . array An array of means. required vars numpy . array An array of variances. required delta int The incremental budget. required Returns: Type Description numpy . array An array of budget recommendations. Note The implementation is based on the pseudo-code in the Chen et al. (p. 49), see [1]. Examples: From the Chen et al. book (p. 49): >>> mean_y = np . array ([ 1 , 2 , 3 , 4 , 5 ]) var_y = np.array([1,1,9,9,4]) get_ocba(mean_y, var_y, 50) [11 9 19 9 2] Source code in spotPython/budget/ocba.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_ocba ( means , vars , delta ) -> int32 : \"\"\" Optimal Computer Budget Allocation (OCBA) This function calculates the budget recommendations for a given set of means, variances, and incremental budget using the OCBA algorithm. References: [1]: Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization: An Optimal Computer Budget Allocation, pp. 49 and pp. 215 [2]: C.S.M Currie and T. Monks: How to choose the best setup for a system. A tutorial for the Simulation Workshop 2021, see: [sw21_tutorial.ipynb](https://colab.research.google.com/github/TomMonks/sim-tools/blob/master/examples/sw21_tutorial.ipynb) and [sim-tools](https://github.com/TomMonks/sim-tools) Args: means (numpy.array): An array of means. vars (numpy.array): An array of variances. delta (int): The incremental budget. Returns: (numpy.array): An array of budget recommendations. Note: The implementation is based on the pseudo-code in the Chen et al. (p. 49), see [1]. Examples: From the Chen et al. book (p. 49): >>> mean_y = np.array([1,2,3,4,5]) var_y = np.array([1,1,9,9,4]) get_ocba(mean_y, var_y, 50) [11 9 19 9 2] \"\"\" n_designs = means . shape [ 0 ] allocations = zeros ( n_designs , int32 ) ratios = zeros ( n_designs , float64 ) budget = delta ranks = get_ranks ( means ) best , second_best = argpartition ( ranks , 2 )[: 2 ] ratios [ second_best ] = 1.0 select = [ i for i in range ( n_designs ) if i not in [ best , second_best ]] temp = ( means [ best ] - means [ second_best ]) / ( means [ best ] - means [ select ]) ratios [ select ] = square ( temp ) * ( vars [ select ] / vars [ second_best ]) select = [ i for i in range ( n_designs ) if i not in [ best ]] temp = ( square ( ratios [ select ]) / vars [ select ]) . sum () ratios [ best ] = sqrt ( vars [ best ] * temp ) more_runs = full ( n_designs , True , dtype = bool ) add_budget = zeros ( n_designs , dtype = float ) more_alloc = True while more_alloc : more_alloc = False ratio_s = ( more_runs * ratios ) . sum () add_budget [ more_runs ] = ( budget / ratio_s ) * ratios [ more_runs ] add_budget = around ( add_budget ) . astype ( int ) mask = add_budget < allocations add_budget [ mask ] = allocations [ mask ] more_runs [ mask ] = 0 if mask . sum () > 0 : more_alloc = True if more_alloc : budget = allocations . sum () + delta budget -= ( add_budget * ~ more_runs ) . sum () t_budget = add_budget . sum () add_budget [ best ] += allocations . sum () + delta - t_budget return add_budget - allocations","title":"get_ocba()"},{"location":"reference/spotPython/budget/ocba/#spotPython.budget.ocba.get_ocba_X","text":"This function calculates the OCBA allocation and repeats the input array X along the specified axis. Parameters: Name Type Description Default X numpy . ndarray Input array to be repeated. required means list List of means for each alternative. required vars list List of variances for each alternative. required delta float Indifference zone parameter. required Returns: Type Description numpy . ndarray Repeated array of X along the specified axis based on the OCBA allocation. Examples: >>> X = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ]]) means = [1,2,3] vars = [1,1,1] delta = 50 get_ocba_X(X, means, vars, delta) array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [4, 5, 6], [4, 5, 6], [4, 5, 6]]) Source code in spotPython/budget/ocba.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def get_ocba_X ( X , means , vars , delta ) -> float64 : \"\"\" This function calculates the OCBA allocation and repeats the input array X along the specified axis. Args: X (numpy.ndarray): Input array to be repeated. means (list): List of means for each alternative. vars (list): List of variances for each alternative. delta (float): Indifference zone parameter. Returns: (numpy.ndarray): Repeated array of X along the specified axis based on the OCBA allocation. Examples: >>> X = np.array([[1,2,3],[4,5,6]]) means = [1,2,3] vars = [1,1,1] delta = 50 get_ocba_X(X, means, vars, delta) array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [4, 5, 6], [4, 5, 6], [4, 5, 6]]) \"\"\" o = get_ocba ( means = means , vars = vars , delta = delta ) return repeat ( X , o , axis = 0 )","title":"get_ocba_X()"},{"location":"reference/spotPython/build/kriging/","text":"Kriging \u00b6 Bases: surrogates Kriging surrogate. Attributes: Name Type Description nat_range_X list List of X natural ranges. nat_range_y list List of y nat ranges. noise bool noisy objective function. Default: False. If True , regression kriging will be used. var_type str variable type. Can be either \"num \u201d (numerical) of \"factor\" (factor). num_mask array array of bool variables. True represent numerical (float) variables. factor_mask array array of factor variables. True represents factor (unordered) variables. int_mask array array of integer variables. True represents integers (ordered) variables. ordered_mask array array of ordered variables. True represents integers or float (ordered) variables. Set of veriables which an order relation, i.e., they are either num (float) or int. name str Surrogate name seed int Random seed. use_cod_y bool Use coded y values. sigma float Kriging sigma. gen method Design generator, e.g., spotPython.design.spacefilling.spacefilling. min_theta float min log10 theta value. Defaults: -6. max_theta float max log10 theta value. Defaults: 3. min_p float min p value. Default: 1. max_p float max p value. Default: 2. Source code in spotPython/build/kriging.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 class Kriging ( surrogates ): \"\"\"Kriging surrogate. Attributes: nat_range_X (list): List of X natural ranges. nat_range_y (list): List of y nat ranges. noise (bool): noisy objective function. Default: False. If `True`, regression kriging will be used. var_type (str): variable type. Can be either `\"num`\" (numerical) of `\"factor\"` (factor). num_mask (array): array of bool variables. `True` represent numerical (float) variables. factor_mask (array): array of factor variables. `True` represents factor (unordered) variables. int_mask (array): array of integer variables. `True` represents integers (ordered) variables. ordered_mask (array): array of ordered variables. `True` represents integers or float (ordered) variables. Set of veriables which an order relation, i.e., they are either num (float) or int. name (str): Surrogate name seed (int): Random seed. use_cod_y (bool): Use coded y values. sigma (float): Kriging sigma. gen (method): Design generator, e.g., spotPython.design.spacefilling.spacefilling. min_theta (float): min log10 theta value. Defaults: -6. max_theta (float): max log10 theta value. Defaults: 3. min_p (float): min p value. Default: 1. max_p (float): max p value. Default: 2. \"\"\" def __init__ ( self : object , noise : bool = False , cod_type : Optional [ str ] = \"norm\" , var_type : List [ str ] = [ \"num\" ], use_cod_y : bool = False , name : str = \"kriging\" , seed : int = 124 , model_optimizer = None , model_fun_evals : Optional [ int ] = None , min_theta : float = - 3 , max_theta : float = 2 , n_theta : int = 1 , n_p : int = 1 , optim_p : bool = False , log_level : int = 50 , spot_writer = None , counter = None , ** kwargs ): \"\"\" Initialize the Kriging surrogate. Args: noise (bool): Use regression instead of interpolation kriging. Defaults to False. cod_type (Optional[str]): Normalize or standardize X and values. Can be None, \"norm\", or \"std\". Defaults to \"norm\". var_type (List[str]): Variable type. Can be either \"num\" (numerical) or \"factor\" (factor). Defaults to [\"num\"]. use_cod_y (bool): Use coded y values (instead of natural one). Defaults to False. name (str): Surrogate name. Defaults to \"kriging\". seed (int): Random seed. Defaults to 124. model_optimizer : Optimizer on the surrogate. If None, differential_evolution is selected. model_fun_evals (Optional[int]): Number of iterations used by the optimizer on the surrogate. min_theta (float): Min log10 theta value. Defaults to -3. max_theta (float): Max log10 theta value. Defaults to 2. n_theta (int): Number of theta values. Defaults to 1. n_p (int): Number of p values. Defaults to 1. optim_p (bool): Determines whether p should be optimized. log_level (int): Logging level, e.g., 20 is \"INFO\". Defaults to 50 (\"CRITICAL\"). spot_writer : Spot writer. counter : Counter. Examples: Surrogate of the x*sin(x) function, see [1]. >>> from spotPython.build.kriging import Kriging import numpy as np import matplotlib.pyplot as plt rng = np.random.RandomState(1) X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1) y = np.squeeze(X * np.sin(X)) training_indices = rng.choice(arange(y.size), size=6, replace=False) X_train, y_train = X[training_indices], y[training_indices] S = Kriging(name='kriging', seed=124) S.fit(X_train, y_train) mean_prediction, std_prediction = S.predict(X) plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\") plt.scatter(X_train, y_train, label=\"Observations\") plt.plot(X, mean_prediction, label=\"Mean prediction\") plt.fill_between( X.ravel(), mean_prediction - 1.96 * std_prediction, mean_prediction + 1.96 * std_prediction, alpha=0.5, label=r\"95% confidence interval\", ) plt.legend() plt.xlabel(\"$x$\") plt.ylabel(\"$f(x)$\") _ = plt.title(\"Gaussian process regression on noise-free dataset\") plt.show() References: [[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)] scikit-learn: Gaussian Processes regression: basic introductory example \"\"\" super () . __init__ ( name , seed , log_level ) self . noise = noise self . var_type = var_type self . cod_type = cod_type self . use_cod_y = use_cod_y self . name = name self . seed = seed self . log_level = log_level self . spot_writer = spot_writer self . counter = counter self . sigma = 0 self . eps = sqrt ( spacing ( 1 )) self . min_theta = min_theta self . max_theta = max_theta self . min_p = 1 self . max_p = 2 self . min_Lambda = 1e-9 self . max_Lambda = 1. self . n_theta = n_theta self . n_p = n_p self . optim_p = optim_p # Psi matrix condition: self . cnd_Psi = 0 self . inf_Psi = False self . model_optimizer = model_optimizer if self . model_optimizer is None : self . model_optimizer = differential_evolution self . model_fun_evals = model_fun_evals # differential evaluation uses maxiter = 1000 # and sets the number of function evaluations to # (maxiter + 1) * popsize * N, which results in # 1000 * 15 * k, because the default popsize is 15 and # N is the number of parameters. This seems to be quite large: # for k=2 these are 30 000 iterations. Therefore we set this value to # 100 if self . model_fun_evals is None : self . model_fun_evals = 100 # Logging information self . log [ \"negLnLike\" ] = [] self . log [ \"theta\" ] = [] self . log [ \"p\" ] = [] self . log [ \"Lambda\" ] = [] # Logger logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) def exp_imp ( self , y0 : float , s0 : float ) -> float : \"\"\" Calculates the expected improvement for a given function value and error in coded units. Args: self (object): The Kriging object. y0 (float): The function value in coded units. s0 (float): The error value. Returns: float: The expected improvement value. Examples: >>> from spotPython.build.kriging import Kriging >>> S = Kriging(name='kriging', seed=124) >>> S.cod_y = [0.0, 0.0, 0.0, 0.0, 0.0] >>> S.mean_cod_y = [0.0, 0.0, 0.0, 0.0, 0.0] >>> S.exp_imp(1.0, 2.0) 0.0 \"\"\" # y_min = min(self.cod_y) y_min = min ( self . mean_cod_y ) if s0 <= 0.0 : EI = 0.0 elif s0 > 0.0 : EI_one = ( y_min - y0 ) * ( 0.5 + 0.5 * erf (( 1.0 / sqrt ( 2.0 )) * (( y_min - y0 ) / s0 )) ) EI_two = ( s0 * ( 1.0 / sqrt ( 2.0 * pi ))) * ( exp ( - ( 1.0 / 2.0 ) * (( y_min - y0 ) ** 2.0 / s0 ** 2.0 )) ) EI = EI_one + EI_two return EI def set_de_bounds ( self ) -> None : \"\"\" Determine search bounds for model_optimizer, e.g., differential evolution. This method sets the attribute `de_bounds` of the object to a list of lists, where each inner list represents the lower and upper bounds for a parameter being optimized. The number of inner lists is determined by the number of parameters being optimized (`n_theta` and `n_p`), as well as whether noise is being considered (`noise`). Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.set_de_bounds() >>> print(obj.de_bounds) [[min_theta, max_theta], [min_theta, max_theta], ..., [min_p, max_p], [min_Lambda, max_Lambda]] Returns: None \"\"\" de_bounds = [[ self . min_theta , self . max_theta ] for _ in range ( self . n_theta )] if self . optim_p : de_bounds += [[ self . min_p , self . max_p ] for _ in range ( self . n_p )] if self . noise : de_bounds . append ([ self . min_Lambda , self . max_Lambda ]) else : if self . noise : de_bounds . append ([ self . min_Lambda , self . max_Lambda ]) self . de_bounds = de_bounds def extract_from_bounds ( self , new_theta_p_Lambda : np . ndarray ) -> None : \"\"\" Extract `theta`, `p`, and `Lambda` from bounds. The kriging object stores `theta` as an array, `p` as an array, and `Lambda` as a float. Args: self (object): The Kriging object. new_theta_p_Lambda (np.ndarray): 1d-array with theta, p, and Lambda values. Order is important. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.extract_from_bounds(np.array([1, 2, 3])) >>> print(obj.theta) [1] >>> print(obj.p) [2] >>> print(obj.Lambda) 3 Returns: None \"\"\" self . theta = new_theta_p_Lambda [: self . n_theta ] if self . optim_p : self . p = new_theta_p_Lambda [ self . n_theta : self . n_theta + self . n_p ] if self . noise : self . Lambda = new_theta_p_Lambda [ self . n_theta + self . n_p ] else : if self . noise : self . Lambda = new_theta_p_Lambda [ self . n_theta ] def optimize_model ( self ) -> Union [ List [ float ], Tuple [ float ]]: \"\"\" Optimize the model using the specified model_optimizer. This method uses the specified model_optimizer to optimize the likelihood function (`fun_likelihood`) with respect to the model parameters. The optimization is performed within the bounds specified by the attribute `de_bounds`. The result of the optimization is returned as a list or tuple of optimized parameter values. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> result = obj.optimize_model() >>> print(result) [optimized_theta, optimized_p, optimized_Lambda] Returns: result[\"x\"] (Union[List[float], Tuple[float]]): A list or tuple of optimized parameter values. \"\"\" if self . model_optimizer . __name__ == 'dual_annealing' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) elif self . model_optimizer . __name__ == 'differential_evolution' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds , maxiter = self . model_fun_evals , seed = self . seed ) elif self . model_optimizer . __name__ == 'direct' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds , # maxfun=self.model_fun_evals, eps = 1e-2 ) elif self . model_optimizer . __name__ == 'shgo' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) elif self . model_optimizer . __name__ == 'basinhopping' : result = self . model_optimizer ( func = self . fun_likelihood , x0 = mean ( self . de_bounds , axis = 1 )) else : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) return result [ \"x\" ] def update_log ( self ) -> None : \"\"\" Update the log with the current values of negLnLike, theta, p, and Lambda. This method appends the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to their respective lists in the log dictionary. It also updates the log_length attribute with the current length of the negLnLike list in the log. If spot_writer is not None, this method also writes the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to the spot_writer object. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.update_log() >>> print(obj.log) {'negLnLike': [0.5], 'theta': [0.1], 'p': [0.2], 'Lambda': [0.3]} \"\"\" self . log [ \"negLnLike\" ] = append ( self . log [ \"negLnLike\" ], self . negLnLike ) self . log [ \"theta\" ] = append ( self . log [ \"theta\" ], self . theta ) if self . optim_p : self . log [ \"p\" ] = append ( self . log [ \"p\" ], self . p ) if self . noise : self . log [ \"Lambda\" ] = append ( self . log [ \"Lambda\" ], self . Lambda ) # get the length of the log self . log_length = len ( self . log [ \"negLnLike\" ]) if self . spot_writer is not None : writer = self . spot_writer negLnLike = self . negLnLike . copy () writer . add_scalar ( \"spot_negLnLike\" , negLnLike , self . counter + self . log_length ) # add the self.n_theta theta values to the writer with one key \"theta\", # i.e, the same key for all theta values theta = self . theta . copy () writer . add_scalars ( \"spot_theta\" , { f \"theta_ { i } \" : theta [ i ] for i in range ( self . n_theta )}, self . counter + self . log_length ) if self . noise : Lambda = self . Lambda . copy () writer . add_scalar ( \"spot_Lambda\" , Lambda , self . counter + self . log_length ) if self . optim_p : p = self . p . copy () writer . add_scalars ( \"spot_p\" , { f \"p_ { i } \" : p [ i ] for i in range ( self . n_p )}, self . counter + self . log_length ) writer . flush () def fit ( self , nat_X : np . ndarray , nat_y : np . ndarray ) -> object : \"\"\" Fits the hyperparameters (`theta`, `p`, `Lambda`) of the Kriging model. The function computes the following internal values: 1. `theta`, `p`, and `Lambda` values via optimization of the function `fun_likelihood()`. 2. Correlation matrix `Psi` via `rebuildPsi()`. Args: self (object): The Kriging object. nat_X (np.ndarray): Sample points. nat_y (np.ndarray): Function values. Returns: object: Fitted estimator. Attributes: theta (np.ndarray): Kriging theta values. Shape (k,). p (np.ndarray): Kriging p values. Shape (k,). LnDetPsi (np.float64): Determinant Psi matrix. Psi (np.matrix): Correlation matrix Psi. Shape (n,n). psi (np.ndarray): psi vector. Shape (n,). one (np.ndarray): vector of ones. Shape (n,). mu (np.float64): Kriging expected mean value mu. U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n). SigmaSqr (np.float64): Sigma squared value. Lambda (float): lambda noise value. Examples: >>> from spotPython.build.kriging import Kriging >>> nat_X = np.array([[1, 2], [3, 4]]) >>> nat_y = np.array([1, 2]) >>> surrogate = Kriging() >>> surrogate.fit(nat_X, nat_y) \"\"\" self . initialize_variables ( nat_X , nat_y ) self . set_variable_types () self . nat_to_cod_init () self . set_theta_values () self . initialize_matrices () # build_Psi() and build_U() are called in fun_likelihood self . set_de_bounds () # Finally, set new theta and p values and update the surrogate again # for new_theta_p_Lambda in de_results[\"x\"]: new_theta_p_Lambda = self . optimize_model () self . extract_from_bounds ( new_theta_p_Lambda ) self . build_Psi () self . build_U () # TODO: check if the following line is necessary! self . likelihood () self . update_log () def initialize_variables ( self , nat_X : np . ndarray , nat_y : np . ndarray ) -> None : \"\"\" Initialize variables for the class instance. This method takes in the independent and dependent variable data as input and initializes the class instance variables. It creates deep copies of the input data and stores them in the instance variables `nat_X` and `nat_y`. It also calculates the number of observations `n` and the number of independent variables `k` from the shape of `nat_X`. Finally, it creates empty arrays with the same shape as `nat_X` and `nat_y` and stores them in the instance variables `cod_X` and `cod_y`. Args: self (object): The Kriging object. nat_X (np.ndarray): The independent variable data. nat_y (np.ndarray): The dependent variable data. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> surrogate = Kriging() >>> nat_X = np.array([[1, 2], [3, 4]]) >>> nat_y = np.array([1, 2]) >>> surrogate.initialize_variables(nat_X, nat_y) >>> surrogate.nat_X array([[1, 2], [3, 4]]) >>> surrogate.nat_y array([1, 2]) \"\"\" self . nat_X = copy . deepcopy ( nat_X ) self . nat_y = copy . deepcopy ( nat_y ) self . n = self . nat_X . shape [ 0 ] self . k = self . nat_X . shape [ 1 ] self . cod_X = np . empty_like ( self . nat_X ) self . cod_y = np . empty_like ( self . nat_y ) def set_variable_types ( self ) -> None : \"\"\" Set the variable types for the class instance. This method sets the variable types for the class instance based on the `var_type` attribute. If the length of `var_type` is less than `k`, all variable types are forced to 'num' and a warning is logged. The method then creates masks for each variable type ('num', 'factor', 'int', 'float') using numpy arrays. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.var_type = [\"num\", \"factor\"] >>> instance = MyClass() >>> instance.set_variable_types() >>> instance.num_mask array([ True, False]) Returns: None \"\"\" # assume all variable types are \"num\" if \"num\" is # specified once: if len ( self . var_type ) < self . k : self . var_type = self . var_type * self . k logger . warning ( \"Warning: All variable types forced to 'num'.\" ) self . num_mask = np . array ( list ( map ( lambda x : x == \"num\" , self . var_type ))) self . factor_mask = np . array ( list ( map ( lambda x : x == \"factor\" , self . var_type ))) self . int_mask = np . array ( list ( map ( lambda x : x == \"int\" , self . var_type ))) self . ordered_mask = np . array ( list ( map ( lambda x : x == \"int\" or x == \"num\" or x == \"float\" , self . var_type ))) def set_theta_values ( self ) -> None : \"\"\" Set the theta values for the class instance. This method sets the theta values for the class instance based on the `n_theta` and `k` attributes. If `n_theta` is greater than `k`, `n_theta` is set to `k` and a warning is logged. The method then initializes the `theta` attribute as a list of zeros with length `n_theta`. The `x0_theta` attribute is also initialized as a list of ones with length `n_theta`, multiplied by `n / (100 * k)`. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_theta = 3 >>> self.k = 2 >>> instance = MyClass() >>> instance.set_theta_values() >>> instance.theta array([0., 0., 0.]) \"\"\" if self . n_theta > self . k : self . n_theta = self . k logger . warning ( \"More theta values than dimensions. `n_theta` set to `k`.\" ) self . theta : List [ float ] = zeros ( self . n_theta ) # TODO: Currently not used: self . x0_theta : List [ float ] = ones (( self . n_theta ,)) * self . n / ( 100 * self . k ) def initialize_matrices ( self ) -> None : \"\"\" Initialize the matrices for the class instance. This method initializes several matrices and attributes for the class instance. The `p` attribute is initialized as a list of ones with length `n_p`, multiplied by 2.0. The `pen_val` attribute is initialized as the natural logarithm of the variance of `nat_y`, multiplied by `n`, plus 1e4. The `negLnLike`, `LnDetPsi`, `mu`, `U`, `SigmaSqr`, and `Lambda` attributes are all set to None. The `gen` attribute is initialized using the `spacefilling` function with arguments `k` and `seed`. The `Psi` attribute is initialized as a zero matrix with shape `(n, n)` and dtype `float64`. The `psi` attribute is initialized as a zero matrix with shape `(n, 1)`. The `one` attribute is initialized as a list of ones with length `n`. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> instance.initialize_matrices() Returns: None \"\"\" self . p = ones ( self . n_p ) * 2.0 self . pen_val = self . n * log ( var ( self . nat_y )) + 1e4 self . negLnLike = None self . gen = spacefilling ( k = self . k , seed = self . seed ) self . LnDetPsi = None self . Psi = zeros (( self . n , self . n ), dtype = float64 ) self . psi = zeros (( self . n , 1 )) self . one = ones ( self . n ) self . mu = None self . U = None self . SigmaSqr = None self . Lambda = None def fun_likelihood ( self , new_theta_p_Lambda : np . ndarray ) -> float : \"\"\" Compute log likelihood for a set of hyperparameters (theta, p, Lambda). This method computes the log likelihood for a set of hyperparameters (theta, p, Lambda) by performing the following steps: 1. Extracts the hyperparameters from the input array using `extract_from_bounds()`. 2. Checks if any element in `10^theta` is equal to 0. If so, logs a warning and returns the penalty value (`pen_val`). 3. Builds the `Psi` matrix using `build_Psi()`. 4. Checks if `Psi` is ill-conditioned or infinite. If so, logs a warning and returns the penalty value (`pen_val`). 5. Builds the `U` matrix using `build_U()`. If an exception occurs, logs an error and returns the penalty value (`pen_val`). 6. Computes the negative log likelihood using `likelihood()`. 7. Returns the computed negative log likelihood (`negLnLike`). Args: self (object): The Kriging object. new_theta_p_Lambda (np.ndarray): An array containing the `theta`, `p`, and `Lambda` values. Returns: float: The negative log likelihood of the surface at the specified hyperparameters. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> negLnLike = instance.fun_likelihood(new_theta_p_Lambda) >>> print(negLnLike) \"\"\" self . extract_from_bounds ( new_theta_p_Lambda ) if self . __is_any__ ( power ( 10.0 , self . theta ), 0 ): logger . warning ( \"Failure in fun_likelihood: 10^theta == 0. Setting negLnLike to %s \" , self . pen_val ) return self . pen_val self . build_Psi () if ( self . inf_Psi or self . cnd_Psi > 1e9 ): logger . warning ( \"Failure in fun_likelihood: Psi is ill conditioned: %s \" , self . cnd_Psi ) logger . warning ( \"Setting negLnLike to: %s \" , self . pen_val ) return self . pen_val try : self . build_U () except Exception as error : penalty_value = self . pen_val print ( \"Error in fun_likelihood(). Call to build_U() failed.\" ) print ( \"error= %s , type(error)= %s \" % ( error , type ( error ))) print ( \"Setting negLnLike to %.2f .\" % self . pen_val ) return penalty_value self . likelihood () return self . negLnLike def __is_any__ ( self , x : Union [ np . ndarray , Any ], v : Any ) -> bool : \"\"\" Check if any element in `x` is equal to `v`. This method checks if any element in the input array `x` is equal to the value `v`. If `x` is not an instance of `ndarray`, it is first converted to a numpy array using the `array()` function. Args: self (object): The Kriging object. x (np.ndarray or array-like): The input array to check for the presence of value `v`. v (scalar): The value to check for in the input array `x`. Returns: bool: True if any element in `x` is equal to `v`, False otherwise. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> result = instance.__is_any__(x, v) >>> print(result) \"\"\" if not isinstance ( x , ndarray ): x = array ([ x ]) return any ( x == v ) def build_Psi ( self ) -> None : \"\"\" Constructs a new (n x n) correlation matrix Psi to reflect new data or a change in hyperparameters. This method uses `theta`, `p`, and coded `X` values to construct the correlation matrix as described in [Forr08a, p.57]. Args: self (object): The Kriging object. Returns: None Raises: LinAlgError: If building Psi fails. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_Psi() \"\"\" self . Psi = zeros (( self . n , self . n ), dtype = float64 ) theta = power ( 10.0 , self . theta ) if self . n_theta == 1 : theta = theta * ones ( self . k ) try : D = zeros (( self . n , self . n )) if self . ordered_mask . any (): X_ordered = self . cod_X [:, self . ordered_mask ] D = squareform ( pdist ( X_ordered , metric = 'sqeuclidean' , out = None , w = theta [ self . ordered_mask ])) if self . factor_mask . any (): X_factor = self . cod_X [:, self . factor_mask ] D = ( D + squareform ( pdist ( X_factor , metric = 'hamming' , out = None , w = theta [ self . factor_mask ]))) self . Psi = exp ( - D ) except LinAlgError as err : print ( f \"Building Psi failed: \\n { self . Psi } . { err =} , { type ( err ) =} \" ) if self . noise : self . Psi [ diag_indices_from ( self . Psi )] += self . Lambda else : self . Psi [ diag_indices_from ( self . Psi )] += self . eps if ( isinf ( self . Psi )) . any (): self . inf_Psi = True self . cnd_Psi = cond ( self . Psi ) def build_U ( self , scipy : bool = True ) -> None : \"\"\" Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57]. This method uses either `scipy_cholesky` or numpy's `cholesky` to perform the Cholesky factorization of Psi. Args: self (object): The Kriging object. scipy (bool): If True, use `scipy_cholesky`. If False, use numpy's `cholesky`. Defaults to True. Returns: None Raises: LinAlgError: If Cholesky factorization fails for Psi. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_U() \"\"\" try : self . U = scipy_cholesky ( self . Psi , lower = True ) if scipy else cholesky ( self . Psi ) self . U = self . U . T except LinAlgError as err : print ( f \"build_U() Cholesky failed for Psi: \\n { self . Psi } . { err =} , { type ( err ) =} \" ) def likelihood ( self ) -> None : \"\"\" Calculates the negative of the concentrated log-likelihood. This method implements equation (2.32) in [Forr08a] to calculate the negative of the concentrated log-likelihood. It also modifies `mu`, `SigmaSqr`, `LnDetPsi`, and `negLnLike`. Note: `build_Psi` and `build_U` should be called first. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_Psi() >>> obj.build_U() >>> obj.likelihood() \"\"\" # (2.20) in [Forr08a]: U_T_inv_one = solve ( self . U . T , self . one ) U_T_inv_cod_y = solve ( self . U . T , self . cod_y ) mu = self . one . T . dot ( solve ( self . U , U_T_inv_cod_y )) / self . one . T . dot ( solve ( self . U , U_T_inv_one )) self . mu = mu # (2.31) in [Forr08a] cod_y_minus_mu = self . cod_y - self . one . dot ( self . mu ) self . SigmaSqr = cod_y_minus_mu . T . dot ( solve ( self . U , solve ( self . U . T , cod_y_minus_mu ))) / self . n # (2.32) in [Forr08a] self . LnDetPsi = 2.0 * sum ( log ( abs ( diag ( self . U )))) self . negLnLike = - 1.0 * ( - ( self . n / 2.0 ) * log ( self . SigmaSqr ) - 0.5 * self . LnDetPsi ) def plot ( self , show : Optional [ bool ] = True ) -> None : \"\"\" This function plots 1D and 2D surrogates. Args: self (object): The Kriging object. show (bool): If `True`, the plots are displayed. If `False`, `plt.show()` should be called outside this function. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> plot(show=True) \"\"\" if self . k == 1 : # TODO: Improve plot (add conf. interval etc.) fig = pylab . figure ( figsize = ( 9 , 6 )) # t1 = array(arange(0.0, 1.0, 0.01)) # y1 = array([self.predict(array([x]), return_val=\"y\") for x in t1]) # plt.figure() # plt.plot(t1, y1, \"k\") # if show: # plt.show() # n_grid = 100 x = linspace ( self . nat_range_X [ 0 ][ 0 ], self . nat_range_X [ 0 ][ 1 ], num = n_grid ) y = self . predict ( x ) plt . figure () plt . plot ( x , y , \"k\" ) if show : plt . show () if self . k == 2 : fig = pylab . figure ( figsize = ( 9 , 6 )) n_grid = 100 x = linspace ( self . nat_range_X [ 0 ][ 0 ], self . nat_range_X [ 0 ][ 1 ], num = n_grid ) y = linspace ( self . nat_range_X [ 1 ][ 0 ], self . nat_range_X [ 1 ][ 1 ], num = n_grid ) X , Y = meshgrid ( x , y ) # Predict based on the optimized results zz = array ( [ self . predict ( array ([ x , y ]), return_val = \"all\" ) for x , y in zip ( ravel ( X ), ravel ( Y ))] ) zs = zz [:, 0 , :] zse = zz [:, 1 , :] Z = zs . reshape ( X . shape ) Ze = zse . reshape ( X . shape ) if self . cod_type == \"norm\" : nat_point_X = ( self . cod_X [:, 0 ] * ( self . nat_range_X [ 0 ][ 1 ] - self . nat_range_X [ 0 ][ 0 ]) ) + self . nat_range_X [ 0 ][ 0 ] nat_point_Y = ( self . cod_X [:, 1 ] * ( self . nat_range_X [ 1 ][ 1 ] - self . nat_range_X [ 1 ][ 0 ]) ) + self . nat_range_X [ 1 ][ 0 ] elif self . cod_type == \"std\" : nat_point_X = self . cod_X [:, 0 ] * self . nat_std_X [ 0 ] + self . nat_mean_X [ 0 ] nat_point_Y = self . cod_X [:, 1 ] * self . nat_std_X [ 1 ] + self . nat_mean_X [ 1 ] else : nat_point_X = self . cod_X [:, 0 ] nat_point_Y = self . cod_X [:, 1 ] contour_levels = 30 ax = fig . add_subplot ( 224 ) # plot predicted values: pylab . contourf ( X , Y , Ze , contour_levels , cmap = \"jet\" ) pylab . title ( \"Error\" ) pylab . colorbar () # plot observed points: pylab . plot ( nat_point_X , nat_point_Y , \"ow\" ) # ax = fig . add_subplot ( 223 ) # plot predicted values: plt . contourf ( X , Y , Z , contour_levels , zorder = 1 , cmap = \"jet\" ) plt . title ( \"Surrogate\" ) # plot observed points: pylab . plot ( nat_point_X , nat_point_Y , \"ow\" , zorder = 3 ) pylab . colorbar () # ax = fig . add_subplot ( 221 , projection = \"3d\" ) ax . plot_surface ( X , Y , Z , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" ) # ax = fig . add_subplot ( 222 , projection = \"3d\" ) ax . plot_surface ( X , Y , Ze , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" ) # pylab . show () def predict ( self , nat_X : ndarray , nat : bool = True , return_val : str = \"y\" ) -> Union [ float , Tuple [ float , float , float ]]: \"\"\" This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X. Args: self (object): The Kriging object. nat_X (ndarray): Design variable to evaluate in natural units. nat (bool): argument `nat_X` is in natural range. Default: `True`. If set to `False`, `nat_X` will not be normalized (which might be useful if already normalized y values are used). return_val (str): whether `y`, `s`, neg. `ei` (negative expected improvement), or all three values are returned. Default is (for compatibility with sklearn) \"y\". To return `s`, select \"s\", to return neg. `ei`, select \"ei\". To return the tuple `(y, s, ei)`, select \"all\". Returns: float: The predicted value in natural units if return_val is \"y\". float: predicted error if return_val is \"s\". float: expected improvement if return_val is \"ei\". Tuple[float, float, float]: The predicted value in natural units, predicted error and expected improvement if return_val is \"all\". Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> k.predict(array([[0.3, 0.3]])) array([0.09]) \"\"\" # Check for the shape and the type of the Input if isinstance ( nat_X , ndarray ): try : X = nat_X . reshape ( - 1 , self . nat_X . shape [ 1 ]) X = repair_non_numeric ( X , self . var_type ) except Exception : raise TypeError ( \"13.1: Input to predict was not convertible to the size of X\" ) else : raise TypeError ( f \"type of the given input is an { type ( nat_X ) } instead of an ndarray\" ) n = X . shape [ 0 ] y = empty ( n , dtype = float ) s = empty ( n , dtype = float ) ei = empty ( n , dtype = float ) for i in range ( n ): if nat : x = self . nat_to_cod_x ( X [ i , :]) else : x = X [ i , :] y [ i ], s [ i ], ei [ i ] = self . predict_coded ( x ) if return_val == \"y\" : return y elif return_val == \"s\" : return s elif return_val == \"ei\" : return - 1.0 * ei else : return y , s , - 1.0 * ei def build_psi_vec ( self , cod_x : ndarray ) -> None : \"\"\" Build the psi vector. Needed by `predict_cod`, `predict_err_coded`, `regression_predict_coded`. Modifies `self.psi`. Args: self (object): The Kriging object. cod_x (ndarray): point to calculate psi Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> build_psi_vec(cod_x) \"\"\" self . psi = zeros (( self . n )) # theta = self.theta # TODO: theta = power ( 10.0 , self . theta ) if self . n_theta == 1 : theta = theta * ones ( self . k ) try : D = zeros (( self . n )) if self . ordered_mask . any (): X_ordered = self . cod_X [:, self . ordered_mask ] x_ordered = cod_x [ self . ordered_mask ] D = cdist ( x_ordered . reshape ( - 1 , sum ( self . ordered_mask )), X_ordered . reshape ( - 1 , sum ( self . ordered_mask )), metric = 'sqeuclidean' , out = None , w = theta [ self . ordered_mask ]) if self . factor_mask . any (): X_factor = self . cod_X [:, self . factor_mask ] x_factor = cod_x [ self . factor_mask ] D = ( D + cdist ( x_factor . reshape ( - 1 , sum ( self . factor_mask )), X_factor . reshape ( - 1 , sum ( self . factor_mask )), metric = 'hamming' , out = None , w = theta [ self . factor_mask ])) self . psi = exp ( - D ) . T except LinAlgError as err : print ( f \"Building psi failed: \\n { self . psi } . { err =} , { type ( err ) =} \" ) def predict_coded ( self , cod_x : np . ndarray ) -> Tuple [ float , float , float ]: \"\"\" Kriging prediction of one point in the coded units as described in (2.20) in [Forr08a]. The error is returned as well. Args: self (object): The Kriging object. cod_x (np.ndarray): Point in coded units to make prediction at. Returns: f (float): Predicted value in coded units. SSqr (float): Predicted error. EI (float): Expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> k.predict_coded(cod_x) (0.09, 0.0, 0.0) Note: `self.mu` and `self.SigmaSqr` are computed in `likelihood`, not here. See also [Forr08a, p.60]. \"\"\" self . build_psi_vec ( cod_x ) U_T_inv = solve ( self . U . T , self . cod_y - self . one . dot ( self . mu )) f = self . mu + self . psi . T . dot ( solve ( self . U , U_T_inv )) if self . noise : Lambda = self . Lambda else : Lambda = 0.0 # Error in [Forr08a, p.87]: SSqr = self . SigmaSqr * ( 1 + Lambda - self . psi . T . dot ( solve ( self . U , solve ( self . U . T , self . psi )))) SSqr = power ( abs ( SSqr [ 0 ]), 0.5 )[ 0 ] EI = self . exp_imp ( y0 = f [ 0 ], s0 = SSqr ) return f [ 0 ], SSqr , EI def weighted_exp_imp ( self , cod_x : np . ndarray , w : float ) -> float : \"\"\" Weighted expected improvement. Args: self (object): The Kriging object. cod_x (np.ndarray): A coded design vector. w (float): Weight. Returns: EI (float): Weighted expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> w = 0.5 >>> k.weighted_exp_imp(cod_x, w) 0.0 References: [Sobester et al. 2005]. \"\"\" y0 , s0 = self . predict_coded ( cod_x ) y_min = min ( self . cod_y ) if s0 <= 0.0 : EI = 0.0 else : y_min_y0 = y_min - y0 EI_one = w * ( y_min_y0 * ( 0.5 + 0.5 * erf (( 1.0 / sqrt ( 2.0 )) * ( y_min_y0 / s0 ))) ) EI_two = ( ( 1.0 - w ) * ( s0 * ( 1.0 / sqrt ( 2.0 * pi ))) * ( exp ( - ( 1.0 / 2.0 ) * (( y_min_y0 ) ** 2.0 / s0 ** 2.0 ))) ) EI = EI_one + EI_two return EI def calculate_mean_MSE ( self , n_samples : int = 200 , points : Optional [ np . ndarray ] = None ) -> Tuple [ float , float ]: \"\"\" Calculates the mean MSE metric of the model by evaluating MSE at a number of points. Args: self (object): The Kriging object. n_samples (int): Number of points to sample the mean squared error at. Ignored if the points argument is specified. points (np.ndarray): An array of points to sample the model at. Returns: mean_MSE (float): The mean value of MSE. std_MSE (float): The standard deviation of the MSE points. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> n_samples = 200 >>> mean_MSE, std_MSE = k.calculate_mean_MSE(n_samples) >>> print(f\"Mean MSE: {mean_MSE}, Standard deviation of MSE: {std_MSE}\") \"\"\" if points is None : points = self . gen . lhd ( n_samples ) values = [ self . predict ( cod_X = point , nat = True , return_val = \"s\" ) for point in points ] return mean ( values ), std ( values ) def cod_to_nat_x ( self , cod_X : np . ndarray ) -> np . ndarray : \"\"\" Converts an array representing one point in normalized (coded) units to natural (physical or real world) units. Args: self (object): The Kriging object. cod_X (np.ndarray): An array representing one point (self.k long) in normalized (coded) units. Returns: X (np.ndarray): An array of natural (physical or real world) units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_X = array([0.3, 0.3]) >>> nat_X = k.cod_to_nat_x(cod_X) >>> print(f\"Natural units: {nat_X}\") \"\"\" X = copy . deepcopy ( cod_X ) if self . cod_type == \"norm\" : for i in range ( self . k ): X [ i ] = ( X [ i ] * float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ]) ) + self . nat_range_X [ i ][ 0 ] return X elif self . cod_type == \"std\" : for i in range ( self . k ): X [ i ] = X [ i ] * self . nat_std_X [ i ] + self . nat_mean_X [ i ] return X else : return cod_X def cod_to_nat_y ( self , cod_y : np . ndarray ) -> np . ndarray : \"\"\" Converts a normalized array of coded (model) units in the range of [0,1] to an array of observed values in real-world units. Args: self (object): The Kriging object. cod_y (np.ndarray): A normalized array of coded (model) units in the range of [0,1]. Returns: y (np.ndarray): An array of observed values in real-world units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_y = array([0.5, 0.5]) >>> nat_y = k.cod_to_nat_y(cod_y) >>> print(f\"Real-world units: {nat_y}\") \"\"\" return ( cod_y * ( self . nat_range_y [ 1 ] - self . nat_range_y [ 0 ]) + self . nat_range_y [ 0 ] if self . cod_type == \"norm\" else cod_y * self . nat_std_y + self . nat_mean_y if self . cod_type == \"std\" else cod_y ) def nat_to_cod_x ( self , nat_X : np . ndarray ) -> np . ndarray : \"\"\" Normalizes one point (row) of nat_X array to [0,1]. The internal nat_range_X values are not updated. Args: self (object): The Kriging object. nat_X (np.ndarray): An array representing one point (self.k long) in natural (physical or real world) units. Returns: X (np.ndarray): An array of coded values in the range of [0,1] for each dimension. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> nat_X = array([5.0, 5.0]) >>> cod_X = k.nat_to_cod_x(nat_X) >>> print(f\"Coded values: {cod_X}\") \"\"\" X = copy . deepcopy ( nat_X ) if self . cod_type == \"norm\" : for i in range ( self . k ): # TODO: Check Implementation of range correction if range == 0: # rangex <- xmax - xmin # rangey <- ymax - ymin # xmin[rangex == 0] <- xmin[rangex == 0] - 0.5 # xmax[rangex == 0] <- xmax[rangex == 0] + 0.5 # rangex[rangex == 0] <- 1 # logger.debug(f\"self.nat_range_X[{i}]:\\n {self.nat_range_X[i]}\") # logger.debug(f\"X[{i}]:\\n {X[i]}\") rangex = float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ]) if rangex == 0 : self . nat_range_X [ i ][ 0 ] = self . nat_range_X [ i ][ 0 ] - 0.5 self . nat_range_X [ i ][ 1 ] = self . nat_range_X [ i ][ 1 ] + 0.5 X [ i ] = ( X [ i ] - self . nat_range_X [ i ][ 0 ]) / float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ] ) return X elif self . cod_type == \"std\" : for i in range ( self . k ): X [ i ] = ( X [ i ] - self . nat_mean_X [ i ]) / self . nat_std_X [ i ] return X else : return nat_X def nat_to_cod_y ( self , nat_y : np . ndarray ) -> np . ndarray : \"\"\" Normalizes natural y values to [0,1]. Args: self (object): The Kriging object. nat_y (np.ndarray): An array of observed values in natural (real-world) units. Returns: y (np.ndarray): A normalized array of coded (model) units in the range of [0,1]. Examples: >>> from spotPython.build.kriging import Kriging >>> import numpy as np >>> kriging = Kriging() >>> nat_y = np.array([5.0, 5.0]) >>> cod_y = kriging.nat_to_cod_y(nat_y) >>> print(f\"Coded values: {cod_y}\") \"\"\" return ( ( nat_y - self . nat_range_y [ 0 ]) / ( self . nat_range_y [ 1 ] - self . nat_range_y [ 0 ]) if self . use_cod_y and self . cod_type == \"norm\" else ( nat_y - self . nat_mean_y ) / self . nat_std_y if self . use_cod_y and self . cod_type == \"std\" else nat_y ) def nat_to_cod_init ( self ) -> None : \"\"\" Determines max and min of each dimension and normalizes that axis to a range of [0,1]. Called when 1) surrogate is initialized and 2) new points arrive, i.e., suggested by the surrogate as infill points. This method calls `nat_to_cod_x` and `nat_to_cod_y` and updates the ranges `nat_range_X` and `nat_range_y`. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> kriging = Kriging() >>> kriging.nat_to_cod_init() \"\"\" self . nat_range_X = [] self . nat_range_y = [] for i in range ( self . k ): self . nat_range_X . append ([ min ( self . nat_X [:, i ]), max ( self . nat_X [:, i ])]) self . nat_range_y . append ( min ( self . nat_y )) self . nat_range_y . append ( max ( self . nat_y )) self . nat_mean_X = mean ( self . nat_X , axis = 0 ) self . nat_std_X = std ( self . nat_X , axis = 0 ) self . nat_mean_y = mean ( self . nat_y ) self . nat_std_y = std ( self . nat_y ) Z = aggregate_mean_var ( X = self . nat_X , y = self . nat_y ) mu = Z [ 1 ] self . mean_cod_y = empty_like ( mu ) for i in range ( self . n ): self . cod_X [ i ] = self . nat_to_cod_x ( self . nat_X [ i ]) for i in range ( self . n ): self . cod_y [ i ] = self . nat_to_cod_y ( self . nat_y [ i ]) for i in range ( mu . shape [ 0 ]): self . mean_cod_y [ i ] = self . nat_to_cod_y ( mu [ i ]) __init__ ( noise = False , cod_type = 'norm' , var_type = [ 'num' ], use_cod_y = False , name = 'kriging' , seed = 124 , model_optimizer = None , model_fun_evals = None , min_theta =- 3 , max_theta = 2 , n_theta = 1 , n_p = 1 , optim_p = False , log_level = 50 , spot_writer = None , counter = None , ** kwargs ) \u00b6 Initialize the Kriging surrogate. Parameters: Name Type Description Default noise bool Use regression instead of interpolation kriging. Defaults to False. False cod_type Optional [ str ] Normalize or standardize X and values. Can be None, \u201cnorm\u201d, or \u201cstd\u201d. Defaults to \u201cnorm\u201d. 'norm' var_type List [ str ] Variable type. Can be either \u201cnum\u201d (numerical) or \u201cfactor\u201d (factor). Defaults to [\u201cnum\u201d]. ['num'] use_cod_y bool Use coded y values (instead of natural one). Defaults to False. False name str Surrogate name. Defaults to \u201ckriging\u201d. 'kriging' seed int Random seed. Defaults to 124. 124 model_optimizer Optimizer on the surrogate. If None, differential_evolution is selected. None model_fun_evals Optional [ int ] Number of iterations used by the optimizer on the surrogate. None min_theta float Min log10 theta value. Defaults to -3. -3 max_theta float Max log10 theta value. Defaults to 2. 2 n_theta int Number of theta values. Defaults to 1. 1 n_p int Number of p values. Defaults to 1. 1 optim_p bool Determines whether p should be optimized. False log_level int Logging level, e.g., 20 is \u201cINFO\u201d. Defaults to 50 (\u201cCRITICAL\u201d). 50 spot_writer Spot writer. None counter Counter. None Examples: Surrogate of the x*sin(x) function, see [1]. >>> from spotPython.build.kriging import Kriging import numpy as np import matplotlib.pyplot as plt rng = np.random.RandomState(1) X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1) y = np.squeeze(X * np.sin(X)) training_indices = rng.choice(arange(y.size), size=6, replace=False) X_train, y_train = X[training_indices], y[training_indices] S = Kriging(name='kriging', seed=124) S.fit(X_train, y_train) mean_prediction, std_prediction = S.predict(X) plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\") plt.scatter(X_train, y_train, label=\"Observations\") plt.plot(X, mean_prediction, label=\"Mean prediction\") plt.fill_between( X.ravel(), mean_prediction - 1.96 * std_prediction, mean_prediction + 1.96 * std_prediction, alpha=0.5, label=r\"95% confidence interval\", ) plt.legend() plt.xlabel(\"$x$\") plt.ylabel(\"$f(x)$\") _ = plt.title(\"Gaussian process regression on noise-free dataset\") plt.show() References [ 1 ] scikit-learn: Gaussian Processes regression: basic introductory example Source code in spotPython/build/kriging.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def __init__ ( self : object , noise : bool = False , cod_type : Optional [ str ] = \"norm\" , var_type : List [ str ] = [ \"num\" ], use_cod_y : bool = False , name : str = \"kriging\" , seed : int = 124 , model_optimizer = None , model_fun_evals : Optional [ int ] = None , min_theta : float = - 3 , max_theta : float = 2 , n_theta : int = 1 , n_p : int = 1 , optim_p : bool = False , log_level : int = 50 , spot_writer = None , counter = None , ** kwargs ): \"\"\" Initialize the Kriging surrogate. Args: noise (bool): Use regression instead of interpolation kriging. Defaults to False. cod_type (Optional[str]): Normalize or standardize X and values. Can be None, \"norm\", or \"std\". Defaults to \"norm\". var_type (List[str]): Variable type. Can be either \"num\" (numerical) or \"factor\" (factor). Defaults to [\"num\"]. use_cod_y (bool): Use coded y values (instead of natural one). Defaults to False. name (str): Surrogate name. Defaults to \"kriging\". seed (int): Random seed. Defaults to 124. model_optimizer : Optimizer on the surrogate. If None, differential_evolution is selected. model_fun_evals (Optional[int]): Number of iterations used by the optimizer on the surrogate. min_theta (float): Min log10 theta value. Defaults to -3. max_theta (float): Max log10 theta value. Defaults to 2. n_theta (int): Number of theta values. Defaults to 1. n_p (int): Number of p values. Defaults to 1. optim_p (bool): Determines whether p should be optimized. log_level (int): Logging level, e.g., 20 is \"INFO\". Defaults to 50 (\"CRITICAL\"). spot_writer : Spot writer. counter : Counter. Examples: Surrogate of the x*sin(x) function, see [1]. >>> from spotPython.build.kriging import Kriging import numpy as np import matplotlib.pyplot as plt rng = np.random.RandomState(1) X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1) y = np.squeeze(X * np.sin(X)) training_indices = rng.choice(arange(y.size), size=6, replace=False) X_train, y_train = X[training_indices], y[training_indices] S = Kriging(name='kriging', seed=124) S.fit(X_train, y_train) mean_prediction, std_prediction = S.predict(X) plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\") plt.scatter(X_train, y_train, label=\"Observations\") plt.plot(X, mean_prediction, label=\"Mean prediction\") plt.fill_between( X.ravel(), mean_prediction - 1.96 * std_prediction, mean_prediction + 1.96 * std_prediction, alpha=0.5, label=r\"95% confidence interval\", ) plt.legend() plt.xlabel(\"$x$\") plt.ylabel(\"$f(x)$\") _ = plt.title(\"Gaussian process regression on noise-free dataset\") plt.show() References: [[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)] scikit-learn: Gaussian Processes regression: basic introductory example \"\"\" super () . __init__ ( name , seed , log_level ) self . noise = noise self . var_type = var_type self . cod_type = cod_type self . use_cod_y = use_cod_y self . name = name self . seed = seed self . log_level = log_level self . spot_writer = spot_writer self . counter = counter self . sigma = 0 self . eps = sqrt ( spacing ( 1 )) self . min_theta = min_theta self . max_theta = max_theta self . min_p = 1 self . max_p = 2 self . min_Lambda = 1e-9 self . max_Lambda = 1. self . n_theta = n_theta self . n_p = n_p self . optim_p = optim_p # Psi matrix condition: self . cnd_Psi = 0 self . inf_Psi = False self . model_optimizer = model_optimizer if self . model_optimizer is None : self . model_optimizer = differential_evolution self . model_fun_evals = model_fun_evals # differential evaluation uses maxiter = 1000 # and sets the number of function evaluations to # (maxiter + 1) * popsize * N, which results in # 1000 * 15 * k, because the default popsize is 15 and # N is the number of parameters. This seems to be quite large: # for k=2 these are 30 000 iterations. Therefore we set this value to # 100 if self . model_fun_evals is None : self . model_fun_evals = 100 # Logging information self . log [ \"negLnLike\" ] = [] self . log [ \"theta\" ] = [] self . log [ \"p\" ] = [] self . log [ \"Lambda\" ] = [] # Logger logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) __is_any__ ( x , v ) \u00b6 Check if any element in x is equal to v . This method checks if any element in the input array x is equal to the value v . If x is not an instance of ndarray , it is first converted to a numpy array using the array() function. Parameters: Name Type Description Default self object The Kriging object. required x np.ndarray or array-like The input array to check for the presence of value v . required v scalar The value to check for in the input array x . required Returns: Name Type Description bool bool True if any element in x is equal to v , False otherwise. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> instance = MyClass () >>> result = instance . __is_any__ ( x , v ) >>> print ( result ) Source code in spotPython/build/kriging.py 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 def __is_any__ ( self , x : Union [ np . ndarray , Any ], v : Any ) -> bool : \"\"\" Check if any element in `x` is equal to `v`. This method checks if any element in the input array `x` is equal to the value `v`. If `x` is not an instance of `ndarray`, it is first converted to a numpy array using the `array()` function. Args: self (object): The Kriging object. x (np.ndarray or array-like): The input array to check for the presence of value `v`. v (scalar): The value to check for in the input array `x`. Returns: bool: True if any element in `x` is equal to `v`, False otherwise. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> result = instance.__is_any__(x, v) >>> print(result) \"\"\" if not isinstance ( x , ndarray ): x = array ([ x ]) return any ( x == v ) build_Psi () \u00b6 Constructs a new (n x n) correlation matrix Psi to reflect new data or a change in hyperparameters. This method uses theta , p , and coded X values to construct the correlation matrix as described in [Forr08a, p.57]. Parameters: Name Type Description Default self object The Kriging object. required Returns: Type Description None None Raises: Type Description LinAlgError If building Psi fails. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> obj = MyClass () >>> obj . build_Psi () Source code in spotPython/build/kriging.py 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 def build_Psi ( self ) -> None : \"\"\" Constructs a new (n x n) correlation matrix Psi to reflect new data or a change in hyperparameters. This method uses `theta`, `p`, and coded `X` values to construct the correlation matrix as described in [Forr08a, p.57]. Args: self (object): The Kriging object. Returns: None Raises: LinAlgError: If building Psi fails. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_Psi() \"\"\" self . Psi = zeros (( self . n , self . n ), dtype = float64 ) theta = power ( 10.0 , self . theta ) if self . n_theta == 1 : theta = theta * ones ( self . k ) try : D = zeros (( self . n , self . n )) if self . ordered_mask . any (): X_ordered = self . cod_X [:, self . ordered_mask ] D = squareform ( pdist ( X_ordered , metric = 'sqeuclidean' , out = None , w = theta [ self . ordered_mask ])) if self . factor_mask . any (): X_factor = self . cod_X [:, self . factor_mask ] D = ( D + squareform ( pdist ( X_factor , metric = 'hamming' , out = None , w = theta [ self . factor_mask ]))) self . Psi = exp ( - D ) except LinAlgError as err : print ( f \"Building Psi failed: \\n { self . Psi } . { err =} , { type ( err ) =} \" ) if self . noise : self . Psi [ diag_indices_from ( self . Psi )] += self . Lambda else : self . Psi [ diag_indices_from ( self . Psi )] += self . eps if ( isinf ( self . Psi )) . any (): self . inf_Psi = True self . cnd_Psi = cond ( self . Psi ) build_U ( scipy = True ) \u00b6 Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57]. This method uses either scipy_cholesky or numpy\u2019s cholesky to perform the Cholesky factorization of Psi. Parameters: Name Type Description Default self object The Kriging object. required scipy bool If True, use scipy_cholesky . If False, use numpy\u2019s cholesky . Defaults to True. True Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> obj = MyClass () >>> obj . build_U () Source code in spotPython/build/kriging.py 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 def build_U ( self , scipy : bool = True ) -> None : \"\"\" Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57]. This method uses either `scipy_cholesky` or numpy's `cholesky` to perform the Cholesky factorization of Psi. Args: self (object): The Kriging object. scipy (bool): If True, use `scipy_cholesky`. If False, use numpy's `cholesky`. Defaults to True. Returns: None Raises: LinAlgError: If Cholesky factorization fails for Psi. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_U() \"\"\" try : self . U = scipy_cholesky ( self . Psi , lower = True ) if scipy else cholesky ( self . Psi ) self . U = self . U . T except LinAlgError as err : print ( f \"build_U() Cholesky failed for Psi: \\n { self . Psi } . { err =} , { type ( err ) =} \" ) build_psi_vec ( cod_x ) \u00b6 Build the psi vector. Needed by predict_cod , predict_err_coded , regression_predict_coded . Modifies self.psi . Parameters: Name Type Description Default self object The Kriging object. required cod_x ndarray point to calculate psi required Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> cod_x = array ([ 0.3 , 0.3 ]) >>> build_psi_vec ( cod_x ) Source code in spotPython/build/kriging.py 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 def build_psi_vec ( self , cod_x : ndarray ) -> None : \"\"\" Build the psi vector. Needed by `predict_cod`, `predict_err_coded`, `regression_predict_coded`. Modifies `self.psi`. Args: self (object): The Kriging object. cod_x (ndarray): point to calculate psi Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> build_psi_vec(cod_x) \"\"\" self . psi = zeros (( self . n )) # theta = self.theta # TODO: theta = power ( 10.0 , self . theta ) if self . n_theta == 1 : theta = theta * ones ( self . k ) try : D = zeros (( self . n )) if self . ordered_mask . any (): X_ordered = self . cod_X [:, self . ordered_mask ] x_ordered = cod_x [ self . ordered_mask ] D = cdist ( x_ordered . reshape ( - 1 , sum ( self . ordered_mask )), X_ordered . reshape ( - 1 , sum ( self . ordered_mask )), metric = 'sqeuclidean' , out = None , w = theta [ self . ordered_mask ]) if self . factor_mask . any (): X_factor = self . cod_X [:, self . factor_mask ] x_factor = cod_x [ self . factor_mask ] D = ( D + cdist ( x_factor . reshape ( - 1 , sum ( self . factor_mask )), X_factor . reshape ( - 1 , sum ( self . factor_mask )), metric = 'hamming' , out = None , w = theta [ self . factor_mask ])) self . psi = exp ( - D ) . T except LinAlgError as err : print ( f \"Building psi failed: \\n { self . psi } . { err =} , { type ( err ) =} \" ) calculate_mean_MSE ( n_samples = 200 , points = None ) \u00b6 Calculates the mean MSE metric of the model by evaluating MSE at a number of points. Parameters: Name Type Description Default self object The Kriging object. required n_samples int Number of points to sample the mean squared error at. Ignored if the points argument is specified. 200 points np . ndarray An array of points to sample the model at. None Returns: Name Type Description mean_MSE float The mean value of MSE. std_MSE float The standard deviation of the MSE points. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> n_samples = 200 >>> mean_MSE , std_MSE = k . calculate_mean_MSE ( n_samples ) >>> print ( f \"Mean MSE: { mean_MSE } , Standard deviation of MSE: { std_MSE } \" ) Source code in spotPython/build/kriging.py 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 def calculate_mean_MSE ( self , n_samples : int = 200 , points : Optional [ np . ndarray ] = None ) -> Tuple [ float , float ]: \"\"\" Calculates the mean MSE metric of the model by evaluating MSE at a number of points. Args: self (object): The Kriging object. n_samples (int): Number of points to sample the mean squared error at. Ignored if the points argument is specified. points (np.ndarray): An array of points to sample the model at. Returns: mean_MSE (float): The mean value of MSE. std_MSE (float): The standard deviation of the MSE points. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> n_samples = 200 >>> mean_MSE, std_MSE = k.calculate_mean_MSE(n_samples) >>> print(f\"Mean MSE: {mean_MSE}, Standard deviation of MSE: {std_MSE}\") \"\"\" if points is None : points = self . gen . lhd ( n_samples ) values = [ self . predict ( cod_X = point , nat = True , return_val = \"s\" ) for point in points ] return mean ( values ), std ( values ) cod_to_nat_x ( cod_X ) \u00b6 Converts an array representing one point in normalized (coded) units to natural (physical or real world) units. Parameters: Name Type Description Default self object The Kriging object. required cod_X np . ndarray An array representing one point (self.k long) in normalized (coded) units. required Returns: Name Type Description X np . ndarray An array of natural (physical or real world) units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> cod_X = array ([ 0.3 , 0.3 ]) >>> nat_X = k . cod_to_nat_x ( cod_X ) >>> print ( f \"Natural units: { nat_X } \" ) Source code in spotPython/build/kriging.py 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 def cod_to_nat_x ( self , cod_X : np . ndarray ) -> np . ndarray : \"\"\" Converts an array representing one point in normalized (coded) units to natural (physical or real world) units. Args: self (object): The Kriging object. cod_X (np.ndarray): An array representing one point (self.k long) in normalized (coded) units. Returns: X (np.ndarray): An array of natural (physical or real world) units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_X = array([0.3, 0.3]) >>> nat_X = k.cod_to_nat_x(cod_X) >>> print(f\"Natural units: {nat_X}\") \"\"\" X = copy . deepcopy ( cod_X ) if self . cod_type == \"norm\" : for i in range ( self . k ): X [ i ] = ( X [ i ] * float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ]) ) + self . nat_range_X [ i ][ 0 ] return X elif self . cod_type == \"std\" : for i in range ( self . k ): X [ i ] = X [ i ] * self . nat_std_X [ i ] + self . nat_mean_X [ i ] return X else : return cod_X cod_to_nat_y ( cod_y ) \u00b6 Converts a normalized array of coded (model) units in the range of [0,1] to an array of observed values in real-world units. Parameters: Name Type Description Default self object The Kriging object. required cod_y np . ndarray A normalized array of coded (model) units in the range of [0,1]. required Returns: Name Type Description y np . ndarray An array of observed values in real-world units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> cod_y = array ([ 0.5 , 0.5 ]) >>> nat_y = k . cod_to_nat_y ( cod_y ) >>> print ( f \"Real-world units: { nat_y } \" ) Source code in spotPython/build/kriging.py 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 def cod_to_nat_y ( self , cod_y : np . ndarray ) -> np . ndarray : \"\"\" Converts a normalized array of coded (model) units in the range of [0,1] to an array of observed values in real-world units. Args: self (object): The Kriging object. cod_y (np.ndarray): A normalized array of coded (model) units in the range of [0,1]. Returns: y (np.ndarray): An array of observed values in real-world units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_y = array([0.5, 0.5]) >>> nat_y = k.cod_to_nat_y(cod_y) >>> print(f\"Real-world units: {nat_y}\") \"\"\" return ( cod_y * ( self . nat_range_y [ 1 ] - self . nat_range_y [ 0 ]) + self . nat_range_y [ 0 ] if self . cod_type == \"norm\" else cod_y * self . nat_std_y + self . nat_mean_y if self . cod_type == \"std\" else cod_y ) exp_imp ( y0 , s0 ) \u00b6 Calculates the expected improvement for a given function value and error in coded units. Parameters: Name Type Description Default self object The Kriging object. required y0 float The function value in coded units. required s0 float The error value. required Returns: Name Type Description float float The expected improvement value. Examples: >>> from spotPython.build.kriging import Kriging >>> S = Kriging ( name = 'kriging' , seed = 124 ) >>> S . cod_y = [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] >>> S . mean_cod_y = [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] >>> S . exp_imp ( 1.0 , 2.0 ) 0.0 Source code in spotPython/build/kriging.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def exp_imp ( self , y0 : float , s0 : float ) -> float : \"\"\" Calculates the expected improvement for a given function value and error in coded units. Args: self (object): The Kriging object. y0 (float): The function value in coded units. s0 (float): The error value. Returns: float: The expected improvement value. Examples: >>> from spotPython.build.kriging import Kriging >>> S = Kriging(name='kriging', seed=124) >>> S.cod_y = [0.0, 0.0, 0.0, 0.0, 0.0] >>> S.mean_cod_y = [0.0, 0.0, 0.0, 0.0, 0.0] >>> S.exp_imp(1.0, 2.0) 0.0 \"\"\" # y_min = min(self.cod_y) y_min = min ( self . mean_cod_y ) if s0 <= 0.0 : EI = 0.0 elif s0 > 0.0 : EI_one = ( y_min - y0 ) * ( 0.5 + 0.5 * erf (( 1.0 / sqrt ( 2.0 )) * (( y_min - y0 ) / s0 )) ) EI_two = ( s0 * ( 1.0 / sqrt ( 2.0 * pi ))) * ( exp ( - ( 1.0 / 2.0 ) * (( y_min - y0 ) ** 2.0 / s0 ** 2.0 )) ) EI = EI_one + EI_two return EI extract_from_bounds ( new_theta_p_Lambda ) \u00b6 Extract theta , p , and Lambda from bounds. The kriging object stores theta as an array, p as an array, and Lambda as a float. Parameters: Name Type Description Default self object The Kriging object. required new_theta_p_Lambda np . ndarray 1d-array with theta, p, and Lambda values. Order is important. required Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging ( name = 'kriging' , seed = 124 ) >>> obj = MyClass () >>> obj . extract_from_bounds ( np . array ([ 1 , 2 , 3 ])) >>> print ( obj . theta ) [1] >>> print ( obj . p ) [2] >>> print ( obj . Lambda ) 3 Returns: Type Description None None Source code in spotPython/build/kriging.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 def extract_from_bounds ( self , new_theta_p_Lambda : np . ndarray ) -> None : \"\"\" Extract `theta`, `p`, and `Lambda` from bounds. The kriging object stores `theta` as an array, `p` as an array, and `Lambda` as a float. Args: self (object): The Kriging object. new_theta_p_Lambda (np.ndarray): 1d-array with theta, p, and Lambda values. Order is important. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.extract_from_bounds(np.array([1, 2, 3])) >>> print(obj.theta) [1] >>> print(obj.p) [2] >>> print(obj.Lambda) 3 Returns: None \"\"\" self . theta = new_theta_p_Lambda [: self . n_theta ] if self . optim_p : self . p = new_theta_p_Lambda [ self . n_theta : self . n_theta + self . n_p ] if self . noise : self . Lambda = new_theta_p_Lambda [ self . n_theta + self . n_p ] else : if self . noise : self . Lambda = new_theta_p_Lambda [ self . n_theta ] fit ( nat_X , nat_y ) \u00b6 Fits the hyperparameters ( theta , p , Lambda ) of the Kriging model. The function computes the following internal values: 1. theta , p , and Lambda values via optimization of the function fun_likelihood() . 2. Correlation matrix Psi via rebuildPsi() . Parameters: Name Type Description Default self object The Kriging object. required nat_X np . ndarray Sample points. required nat_y np . ndarray Function values. required Returns: Name Type Description object object Fitted estimator. Attributes: Name Type Description theta np . ndarray Kriging theta values. Shape (k,). p np . ndarray Kriging p values. Shape (k,). LnDetPsi np . float64 Determinant Psi matrix. Psi np . matrix Correlation matrix Psi. Shape (n,n). psi np . ndarray psi vector. Shape (n,). one np . ndarray vector of ones. Shape (n,). mu np . float64 Kriging expected mean value mu. U np . matrix Kriging U matrix, Cholesky decomposition. Shape (n,n). SigmaSqr np . float64 Sigma squared value. Lambda float lambda noise value. Examples: >>> from spotPython.build.kriging import Kriging >>> nat_X = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> nat_y = np . array ([ 1 , 2 ]) >>> surrogate = Kriging () >>> surrogate . fit ( nat_X , nat_y ) Source code in spotPython/build/kriging.py 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def fit ( self , nat_X : np . ndarray , nat_y : np . ndarray ) -> object : \"\"\" Fits the hyperparameters (`theta`, `p`, `Lambda`) of the Kriging model. The function computes the following internal values: 1. `theta`, `p`, and `Lambda` values via optimization of the function `fun_likelihood()`. 2. Correlation matrix `Psi` via `rebuildPsi()`. Args: self (object): The Kriging object. nat_X (np.ndarray): Sample points. nat_y (np.ndarray): Function values. Returns: object: Fitted estimator. Attributes: theta (np.ndarray): Kriging theta values. Shape (k,). p (np.ndarray): Kriging p values. Shape (k,). LnDetPsi (np.float64): Determinant Psi matrix. Psi (np.matrix): Correlation matrix Psi. Shape (n,n). psi (np.ndarray): psi vector. Shape (n,). one (np.ndarray): vector of ones. Shape (n,). mu (np.float64): Kriging expected mean value mu. U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n). SigmaSqr (np.float64): Sigma squared value. Lambda (float): lambda noise value. Examples: >>> from spotPython.build.kriging import Kriging >>> nat_X = np.array([[1, 2], [3, 4]]) >>> nat_y = np.array([1, 2]) >>> surrogate = Kriging() >>> surrogate.fit(nat_X, nat_y) \"\"\" self . initialize_variables ( nat_X , nat_y ) self . set_variable_types () self . nat_to_cod_init () self . set_theta_values () self . initialize_matrices () # build_Psi() and build_U() are called in fun_likelihood self . set_de_bounds () # Finally, set new theta and p values and update the surrogate again # for new_theta_p_Lambda in de_results[\"x\"]: new_theta_p_Lambda = self . optimize_model () self . extract_from_bounds ( new_theta_p_Lambda ) self . build_Psi () self . build_U () # TODO: check if the following line is necessary! self . likelihood () self . update_log () fun_likelihood ( new_theta_p_Lambda ) \u00b6 Compute log likelihood for a set of hyperparameters (theta, p, Lambda). This method computes the log likelihood for a set of hyperparameters (theta, p, Lambda) by performing the following steps: 1. Extracts the hyperparameters from the input array using extract_from_bounds() . 2. Checks if any element in 10^theta is equal to 0. If so, logs a warning and returns the penalty value ( pen_val ). 3. Builds the Psi matrix using build_Psi() . 4. Checks if Psi is ill-conditioned or infinite. If so, logs a warning and returns the penalty value ( pen_val ). 5. Builds the U matrix using build_U() . If an exception occurs, logs an error and returns the penalty value ( pen_val ). 6. Computes the negative log likelihood using likelihood() . 7. Returns the computed negative log likelihood ( negLnLike ). Parameters: Name Type Description Default self object The Kriging object. required new_theta_p_Lambda np . ndarray An array containing the theta , p , and Lambda values. required Returns: Name Type Description float float The negative log likelihood of the surface at the specified hyperparameters. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> instance = MyClass () >>> negLnLike = instance . fun_likelihood ( new_theta_p_Lambda ) >>> print ( negLnLike ) Source code in spotPython/build/kriging.py 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 def fun_likelihood ( self , new_theta_p_Lambda : np . ndarray ) -> float : \"\"\" Compute log likelihood for a set of hyperparameters (theta, p, Lambda). This method computes the log likelihood for a set of hyperparameters (theta, p, Lambda) by performing the following steps: 1. Extracts the hyperparameters from the input array using `extract_from_bounds()`. 2. Checks if any element in `10^theta` is equal to 0. If so, logs a warning and returns the penalty value (`pen_val`). 3. Builds the `Psi` matrix using `build_Psi()`. 4. Checks if `Psi` is ill-conditioned or infinite. If so, logs a warning and returns the penalty value (`pen_val`). 5. Builds the `U` matrix using `build_U()`. If an exception occurs, logs an error and returns the penalty value (`pen_val`). 6. Computes the negative log likelihood using `likelihood()`. 7. Returns the computed negative log likelihood (`negLnLike`). Args: self (object): The Kriging object. new_theta_p_Lambda (np.ndarray): An array containing the `theta`, `p`, and `Lambda` values. Returns: float: The negative log likelihood of the surface at the specified hyperparameters. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> negLnLike = instance.fun_likelihood(new_theta_p_Lambda) >>> print(negLnLike) \"\"\" self . extract_from_bounds ( new_theta_p_Lambda ) if self . __is_any__ ( power ( 10.0 , self . theta ), 0 ): logger . warning ( \"Failure in fun_likelihood: 10^theta == 0. Setting negLnLike to %s \" , self . pen_val ) return self . pen_val self . build_Psi () if ( self . inf_Psi or self . cnd_Psi > 1e9 ): logger . warning ( \"Failure in fun_likelihood: Psi is ill conditioned: %s \" , self . cnd_Psi ) logger . warning ( \"Setting negLnLike to: %s \" , self . pen_val ) return self . pen_val try : self . build_U () except Exception as error : penalty_value = self . pen_val print ( \"Error in fun_likelihood(). Call to build_U() failed.\" ) print ( \"error= %s , type(error)= %s \" % ( error , type ( error ))) print ( \"Setting negLnLike to %.2f .\" % self . pen_val ) return penalty_value self . likelihood () return self . negLnLike initialize_matrices () \u00b6 Initialize the matrices for the class instance. This method initializes several matrices and attributes for the class instance. The p attribute is initialized as a list of ones with length n_p , multiplied by 2.0. The pen_val attribute is initialized as the natural logarithm of the variance of nat_y , multiplied by n , plus 1e4. The negLnLike , LnDetPsi , mu , U , SigmaSqr , and Lambda attributes are all set to None. The gen attribute is initialized using the spacefilling function with arguments k and seed . The Psi attribute is initialized as a zero matrix with shape (n, n) and dtype float64 . The psi attribute is initialized as a zero matrix with shape (n, 1) . The one attribute is initialized as a list of ones with length n . Parameters: Name Type Description Default self object The Kriging object. required Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> instance = MyClass () >>> instance . initialize_matrices () Returns: Type Description None None Source code in spotPython/build/kriging.py 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 def initialize_matrices ( self ) -> None : \"\"\" Initialize the matrices for the class instance. This method initializes several matrices and attributes for the class instance. The `p` attribute is initialized as a list of ones with length `n_p`, multiplied by 2.0. The `pen_val` attribute is initialized as the natural logarithm of the variance of `nat_y`, multiplied by `n`, plus 1e4. The `negLnLike`, `LnDetPsi`, `mu`, `U`, `SigmaSqr`, and `Lambda` attributes are all set to None. The `gen` attribute is initialized using the `spacefilling` function with arguments `k` and `seed`. The `Psi` attribute is initialized as a zero matrix with shape `(n, n)` and dtype `float64`. The `psi` attribute is initialized as a zero matrix with shape `(n, 1)`. The `one` attribute is initialized as a list of ones with length `n`. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> instance.initialize_matrices() Returns: None \"\"\" self . p = ones ( self . n_p ) * 2.0 self . pen_val = self . n * log ( var ( self . nat_y )) + 1e4 self . negLnLike = None self . gen = spacefilling ( k = self . k , seed = self . seed ) self . LnDetPsi = None self . Psi = zeros (( self . n , self . n ), dtype = float64 ) self . psi = zeros (( self . n , 1 )) self . one = ones ( self . n ) self . mu = None self . U = None self . SigmaSqr = None self . Lambda = None initialize_variables ( nat_X , nat_y ) \u00b6 Initialize variables for the class instance. This method takes in the independent and dependent variable data as input and initializes the class instance variables. It creates deep copies of the input data and stores them in the instance variables nat_X and nat_y . It also calculates the number of observations n and the number of independent variables k from the shape of nat_X . Finally, it creates empty arrays with the same shape as nat_X and nat_y and stores them in the instance variables cod_X and cod_y . Parameters: Name Type Description Default self object The Kriging object. required nat_X np . ndarray The independent variable data. required nat_y np . ndarray The dependent variable data. required Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> surrogate = Kriging () >>> nat_X = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> nat_y = np . array ([ 1 , 2 ]) >>> surrogate . initialize_variables ( nat_X , nat_y ) >>> surrogate . nat_X array([[1, 2], [3, 4]]) >>> surrogate . nat_y array([1, 2]) Source code in spotPython/build/kriging.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 def initialize_variables ( self , nat_X : np . ndarray , nat_y : np . ndarray ) -> None : \"\"\" Initialize variables for the class instance. This method takes in the independent and dependent variable data as input and initializes the class instance variables. It creates deep copies of the input data and stores them in the instance variables `nat_X` and `nat_y`. It also calculates the number of observations `n` and the number of independent variables `k` from the shape of `nat_X`. Finally, it creates empty arrays with the same shape as `nat_X` and `nat_y` and stores them in the instance variables `cod_X` and `cod_y`. Args: self (object): The Kriging object. nat_X (np.ndarray): The independent variable data. nat_y (np.ndarray): The dependent variable data. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> surrogate = Kriging() >>> nat_X = np.array([[1, 2], [3, 4]]) >>> nat_y = np.array([1, 2]) >>> surrogate.initialize_variables(nat_X, nat_y) >>> surrogate.nat_X array([[1, 2], [3, 4]]) >>> surrogate.nat_y array([1, 2]) \"\"\" self . nat_X = copy . deepcopy ( nat_X ) self . nat_y = copy . deepcopy ( nat_y ) self . n = self . nat_X . shape [ 0 ] self . k = self . nat_X . shape [ 1 ] self . cod_X = np . empty_like ( self . nat_X ) self . cod_y = np . empty_like ( self . nat_y ) likelihood () \u00b6 Calculates the negative of the concentrated log-likelihood. This method implements equation (2.32) in [Forr08a] to calculate the negative of the concentrated log-likelihood. It also modifies mu , SigmaSqr , LnDetPsi , and negLnLike . Note build_Psi and build_U should be called first. Parameters: Name Type Description Default self object The Kriging object. required Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> obj = MyClass () >>> obj . build_Psi () >>> obj . build_U () >>> obj . likelihood () Source code in spotPython/build/kriging.py 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 def likelihood ( self ) -> None : \"\"\" Calculates the negative of the concentrated log-likelihood. This method implements equation (2.32) in [Forr08a] to calculate the negative of the concentrated log-likelihood. It also modifies `mu`, `SigmaSqr`, `LnDetPsi`, and `negLnLike`. Note: `build_Psi` and `build_U` should be called first. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_Psi() >>> obj.build_U() >>> obj.likelihood() \"\"\" # (2.20) in [Forr08a]: U_T_inv_one = solve ( self . U . T , self . one ) U_T_inv_cod_y = solve ( self . U . T , self . cod_y ) mu = self . one . T . dot ( solve ( self . U , U_T_inv_cod_y )) / self . one . T . dot ( solve ( self . U , U_T_inv_one )) self . mu = mu # (2.31) in [Forr08a] cod_y_minus_mu = self . cod_y - self . one . dot ( self . mu ) self . SigmaSqr = cod_y_minus_mu . T . dot ( solve ( self . U , solve ( self . U . T , cod_y_minus_mu ))) / self . n # (2.32) in [Forr08a] self . LnDetPsi = 2.0 * sum ( log ( abs ( diag ( self . U )))) self . negLnLike = - 1.0 * ( - ( self . n / 2.0 ) * log ( self . SigmaSqr ) - 0.5 * self . LnDetPsi ) nat_to_cod_init () \u00b6 Determines max and min of each dimension and normalizes that axis to a range of [0,1]. Called when 1) surrogate is initialized and 2) new points arrive, i.e., suggested by the surrogate as infill points. This method calls nat_to_cod_x and nat_to_cod_y and updates the ranges nat_range_X and nat_range_y . Parameters: Name Type Description Default self object The Kriging object. required Examples: >>> from spotPython.build.kriging import Kriging >>> kriging = Kriging () >>> kriging . nat_to_cod_init () Source code in spotPython/build/kriging.py 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 def nat_to_cod_init ( self ) -> None : \"\"\" Determines max and min of each dimension and normalizes that axis to a range of [0,1]. Called when 1) surrogate is initialized and 2) new points arrive, i.e., suggested by the surrogate as infill points. This method calls `nat_to_cod_x` and `nat_to_cod_y` and updates the ranges `nat_range_X` and `nat_range_y`. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> kriging = Kriging() >>> kriging.nat_to_cod_init() \"\"\" self . nat_range_X = [] self . nat_range_y = [] for i in range ( self . k ): self . nat_range_X . append ([ min ( self . nat_X [:, i ]), max ( self . nat_X [:, i ])]) self . nat_range_y . append ( min ( self . nat_y )) self . nat_range_y . append ( max ( self . nat_y )) self . nat_mean_X = mean ( self . nat_X , axis = 0 ) self . nat_std_X = std ( self . nat_X , axis = 0 ) self . nat_mean_y = mean ( self . nat_y ) self . nat_std_y = std ( self . nat_y ) Z = aggregate_mean_var ( X = self . nat_X , y = self . nat_y ) mu = Z [ 1 ] self . mean_cod_y = empty_like ( mu ) for i in range ( self . n ): self . cod_X [ i ] = self . nat_to_cod_x ( self . nat_X [ i ]) for i in range ( self . n ): self . cod_y [ i ] = self . nat_to_cod_y ( self . nat_y [ i ]) for i in range ( mu . shape [ 0 ]): self . mean_cod_y [ i ] = self . nat_to_cod_y ( mu [ i ]) nat_to_cod_x ( nat_X ) \u00b6 Normalizes one point (row) of nat_X array to [0,1]. The internal nat_range_X values are not updated. Parameters: Name Type Description Default self object The Kriging object. required nat_X np . ndarray An array representing one point (self.k long) in natural (physical or real world) units. required Returns: Name Type Description X np . ndarray An array of coded values in the range of [0,1] for each dimension. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> nat_X = array ([ 5.0 , 5.0 ]) >>> cod_X = k . nat_to_cod_x ( nat_X ) >>> print ( f \"Coded values: { cod_X } \" ) Source code in spotPython/build/kriging.py 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 def nat_to_cod_x ( self , nat_X : np . ndarray ) -> np . ndarray : \"\"\" Normalizes one point (row) of nat_X array to [0,1]. The internal nat_range_X values are not updated. Args: self (object): The Kriging object. nat_X (np.ndarray): An array representing one point (self.k long) in natural (physical or real world) units. Returns: X (np.ndarray): An array of coded values in the range of [0,1] for each dimension. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> nat_X = array([5.0, 5.0]) >>> cod_X = k.nat_to_cod_x(nat_X) >>> print(f\"Coded values: {cod_X}\") \"\"\" X = copy . deepcopy ( nat_X ) if self . cod_type == \"norm\" : for i in range ( self . k ): # TODO: Check Implementation of range correction if range == 0: # rangex <- xmax - xmin # rangey <- ymax - ymin # xmin[rangex == 0] <- xmin[rangex == 0] - 0.5 # xmax[rangex == 0] <- xmax[rangex == 0] + 0.5 # rangex[rangex == 0] <- 1 # logger.debug(f\"self.nat_range_X[{i}]:\\n {self.nat_range_X[i]}\") # logger.debug(f\"X[{i}]:\\n {X[i]}\") rangex = float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ]) if rangex == 0 : self . nat_range_X [ i ][ 0 ] = self . nat_range_X [ i ][ 0 ] - 0.5 self . nat_range_X [ i ][ 1 ] = self . nat_range_X [ i ][ 1 ] + 0.5 X [ i ] = ( X [ i ] - self . nat_range_X [ i ][ 0 ]) / float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ] ) return X elif self . cod_type == \"std\" : for i in range ( self . k ): X [ i ] = ( X [ i ] - self . nat_mean_X [ i ]) / self . nat_std_X [ i ] return X else : return nat_X nat_to_cod_y ( nat_y ) \u00b6 Normalizes natural y values to [0,1]. Parameters: Name Type Description Default self object The Kriging object. required nat_y np . ndarray An array of observed values in natural (real-world) units. required Returns: Name Type Description y np . ndarray A normalized array of coded (model) units in the range of [0,1]. Examples: >>> from spotPython.build.kriging import Kriging >>> import numpy as np >>> kriging = Kriging () >>> nat_y = np . array ([ 5.0 , 5.0 ]) >>> cod_y = kriging . nat_to_cod_y ( nat_y ) >>> print ( f \"Coded values: { cod_y } \" ) Source code in spotPython/build/kriging.py 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 def nat_to_cod_y ( self , nat_y : np . ndarray ) -> np . ndarray : \"\"\" Normalizes natural y values to [0,1]. Args: self (object): The Kriging object. nat_y (np.ndarray): An array of observed values in natural (real-world) units. Returns: y (np.ndarray): A normalized array of coded (model) units in the range of [0,1]. Examples: >>> from spotPython.build.kriging import Kriging >>> import numpy as np >>> kriging = Kriging() >>> nat_y = np.array([5.0, 5.0]) >>> cod_y = kriging.nat_to_cod_y(nat_y) >>> print(f\"Coded values: {cod_y}\") \"\"\" return ( ( nat_y - self . nat_range_y [ 0 ]) / ( self . nat_range_y [ 1 ] - self . nat_range_y [ 0 ]) if self . use_cod_y and self . cod_type == \"norm\" else ( nat_y - self . nat_mean_y ) / self . nat_std_y if self . use_cod_y and self . cod_type == \"std\" else nat_y ) optimize_model () \u00b6 Optimize the model using the specified model_optimizer. This method uses the specified model_optimizer to optimize the likelihood function ( fun_likelihood ) with respect to the model parameters. The optimization is performed within the bounds specified by the attribute de_bounds . The result of the optimization is returned as a list or tuple of optimized parameter values. Parameters: Name Type Description Default self object The Kriging object. required Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging ( name = 'kriging' , seed = 124 ) >>> obj = MyClass () >>> result = obj . optimize_model () >>> print ( result ) [optimized_theta, optimized_p, optimized_Lambda] Returns: Type Description Union [ List [ float ], Tuple [ float ]] result[\u201cx\u201d] (Union[List[float], Tuple[float]]): A list or tuple of optimized parameter values. Source code in spotPython/build/kriging.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def optimize_model ( self ) -> Union [ List [ float ], Tuple [ float ]]: \"\"\" Optimize the model using the specified model_optimizer. This method uses the specified model_optimizer to optimize the likelihood function (`fun_likelihood`) with respect to the model parameters. The optimization is performed within the bounds specified by the attribute `de_bounds`. The result of the optimization is returned as a list or tuple of optimized parameter values. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> result = obj.optimize_model() >>> print(result) [optimized_theta, optimized_p, optimized_Lambda] Returns: result[\"x\"] (Union[List[float], Tuple[float]]): A list or tuple of optimized parameter values. \"\"\" if self . model_optimizer . __name__ == 'dual_annealing' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) elif self . model_optimizer . __name__ == 'differential_evolution' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds , maxiter = self . model_fun_evals , seed = self . seed ) elif self . model_optimizer . __name__ == 'direct' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds , # maxfun=self.model_fun_evals, eps = 1e-2 ) elif self . model_optimizer . __name__ == 'shgo' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) elif self . model_optimizer . __name__ == 'basinhopping' : result = self . model_optimizer ( func = self . fun_likelihood , x0 = mean ( self . de_bounds , axis = 1 )) else : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) return result [ \"x\" ] plot ( show = True ) \u00b6 This function plots 1D and 2D surrogates. Parameters: Name Type Description Default self object The Kriging object. required show bool If True , the plots are displayed. If False , plt.show() should be called outside this function. True Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> plot ( show = True ) Source code in spotPython/build/kriging.py 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 def plot ( self , show : Optional [ bool ] = True ) -> None : \"\"\" This function plots 1D and 2D surrogates. Args: self (object): The Kriging object. show (bool): If `True`, the plots are displayed. If `False`, `plt.show()` should be called outside this function. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> plot(show=True) \"\"\" if self . k == 1 : # TODO: Improve plot (add conf. interval etc.) fig = pylab . figure ( figsize = ( 9 , 6 )) # t1 = array(arange(0.0, 1.0, 0.01)) # y1 = array([self.predict(array([x]), return_val=\"y\") for x in t1]) # plt.figure() # plt.plot(t1, y1, \"k\") # if show: # plt.show() # n_grid = 100 x = linspace ( self . nat_range_X [ 0 ][ 0 ], self . nat_range_X [ 0 ][ 1 ], num = n_grid ) y = self . predict ( x ) plt . figure () plt . plot ( x , y , \"k\" ) if show : plt . show () if self . k == 2 : fig = pylab . figure ( figsize = ( 9 , 6 )) n_grid = 100 x = linspace ( self . nat_range_X [ 0 ][ 0 ], self . nat_range_X [ 0 ][ 1 ], num = n_grid ) y = linspace ( self . nat_range_X [ 1 ][ 0 ], self . nat_range_X [ 1 ][ 1 ], num = n_grid ) X , Y = meshgrid ( x , y ) # Predict based on the optimized results zz = array ( [ self . predict ( array ([ x , y ]), return_val = \"all\" ) for x , y in zip ( ravel ( X ), ravel ( Y ))] ) zs = zz [:, 0 , :] zse = zz [:, 1 , :] Z = zs . reshape ( X . shape ) Ze = zse . reshape ( X . shape ) if self . cod_type == \"norm\" : nat_point_X = ( self . cod_X [:, 0 ] * ( self . nat_range_X [ 0 ][ 1 ] - self . nat_range_X [ 0 ][ 0 ]) ) + self . nat_range_X [ 0 ][ 0 ] nat_point_Y = ( self . cod_X [:, 1 ] * ( self . nat_range_X [ 1 ][ 1 ] - self . nat_range_X [ 1 ][ 0 ]) ) + self . nat_range_X [ 1 ][ 0 ] elif self . cod_type == \"std\" : nat_point_X = self . cod_X [:, 0 ] * self . nat_std_X [ 0 ] + self . nat_mean_X [ 0 ] nat_point_Y = self . cod_X [:, 1 ] * self . nat_std_X [ 1 ] + self . nat_mean_X [ 1 ] else : nat_point_X = self . cod_X [:, 0 ] nat_point_Y = self . cod_X [:, 1 ] contour_levels = 30 ax = fig . add_subplot ( 224 ) # plot predicted values: pylab . contourf ( X , Y , Ze , contour_levels , cmap = \"jet\" ) pylab . title ( \"Error\" ) pylab . colorbar () # plot observed points: pylab . plot ( nat_point_X , nat_point_Y , \"ow\" ) # ax = fig . add_subplot ( 223 ) # plot predicted values: plt . contourf ( X , Y , Z , contour_levels , zorder = 1 , cmap = \"jet\" ) plt . title ( \"Surrogate\" ) # plot observed points: pylab . plot ( nat_point_X , nat_point_Y , \"ow\" , zorder = 3 ) pylab . colorbar () # ax = fig . add_subplot ( 221 , projection = \"3d\" ) ax . plot_surface ( X , Y , Z , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" ) # ax = fig . add_subplot ( 222 , projection = \"3d\" ) ax . plot_surface ( X , Y , Ze , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" ) # pylab . show () predict ( nat_X , nat = True , return_val = 'y' ) \u00b6 This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X. Parameters: Name Type Description Default self object The Kriging object. required nat_X ndarray Design variable to evaluate in natural units. required nat bool argument nat_X is in natural range. Default: True . If set to False , nat_X will not be normalized (which might be useful if already normalized y values are used). True return_val str whether y , s , neg. ei (negative expected improvement), or all three values are returned. Default is (for compatibility with sklearn) \u201cy\u201d. To return s , select \u201cs\u201d, to return neg. ei , select \u201cei\u201d. To return the tuple (y, s, ei) , select \u201call\u201d. 'y' Returns: Name Type Description float Union [ float , Tuple [ float , float , float ]] The predicted value in natural units if return_val is \u201cy\u201d. float Union [ float , Tuple [ float , float , float ]] predicted error if return_val is \u201cs\u201d. float Union [ float , Tuple [ float , float , float ]] expected improvement if return_val is \u201cei\u201d. Union [ float , Tuple [ float , float , float ]] Tuple[float, float, float]: The predicted value in natural units, predicted error and expected improvement if return_val is \u201call\u201d. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> k . predict ( array ([[ 0.3 , 0.3 ]])) array([0.09]) Source code in spotPython/build/kriging.py 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 def predict ( self , nat_X : ndarray , nat : bool = True , return_val : str = \"y\" ) -> Union [ float , Tuple [ float , float , float ]]: \"\"\" This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X. Args: self (object): The Kriging object. nat_X (ndarray): Design variable to evaluate in natural units. nat (bool): argument `nat_X` is in natural range. Default: `True`. If set to `False`, `nat_X` will not be normalized (which might be useful if already normalized y values are used). return_val (str): whether `y`, `s`, neg. `ei` (negative expected improvement), or all three values are returned. Default is (for compatibility with sklearn) \"y\". To return `s`, select \"s\", to return neg. `ei`, select \"ei\". To return the tuple `(y, s, ei)`, select \"all\". Returns: float: The predicted value in natural units if return_val is \"y\". float: predicted error if return_val is \"s\". float: expected improvement if return_val is \"ei\". Tuple[float, float, float]: The predicted value in natural units, predicted error and expected improvement if return_val is \"all\". Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> k.predict(array([[0.3, 0.3]])) array([0.09]) \"\"\" # Check for the shape and the type of the Input if isinstance ( nat_X , ndarray ): try : X = nat_X . reshape ( - 1 , self . nat_X . shape [ 1 ]) X = repair_non_numeric ( X , self . var_type ) except Exception : raise TypeError ( \"13.1: Input to predict was not convertible to the size of X\" ) else : raise TypeError ( f \"type of the given input is an { type ( nat_X ) } instead of an ndarray\" ) n = X . shape [ 0 ] y = empty ( n , dtype = float ) s = empty ( n , dtype = float ) ei = empty ( n , dtype = float ) for i in range ( n ): if nat : x = self . nat_to_cod_x ( X [ i , :]) else : x = X [ i , :] y [ i ], s [ i ], ei [ i ] = self . predict_coded ( x ) if return_val == \"y\" : return y elif return_val == \"s\" : return s elif return_val == \"ei\" : return - 1.0 * ei else : return y , s , - 1.0 * ei predict_coded ( cod_x ) \u00b6 Kriging prediction of one point in the coded units as described in (2.20) in [Forr08a]. The error is returned as well. Parameters: Name Type Description Default self object The Kriging object. required cod_x np . ndarray Point in coded units to make prediction at. required Returns: Name Type Description f float Predicted value in coded units. SSqr float Predicted error. EI float Expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> cod_x = array ([ 0.3 , 0.3 ]) >>> k . predict_coded ( cod_x ) (0.09, 0.0, 0.0) Note self.mu and self.SigmaSqr are computed in likelihood , not here. See also [Forr08a, p.60]. Source code in spotPython/build/kriging.py 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 def predict_coded ( self , cod_x : np . ndarray ) -> Tuple [ float , float , float ]: \"\"\" Kriging prediction of one point in the coded units as described in (2.20) in [Forr08a]. The error is returned as well. Args: self (object): The Kriging object. cod_x (np.ndarray): Point in coded units to make prediction at. Returns: f (float): Predicted value in coded units. SSqr (float): Predicted error. EI (float): Expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> k.predict_coded(cod_x) (0.09, 0.0, 0.0) Note: `self.mu` and `self.SigmaSqr` are computed in `likelihood`, not here. See also [Forr08a, p.60]. \"\"\" self . build_psi_vec ( cod_x ) U_T_inv = solve ( self . U . T , self . cod_y - self . one . dot ( self . mu )) f = self . mu + self . psi . T . dot ( solve ( self . U , U_T_inv )) if self . noise : Lambda = self . Lambda else : Lambda = 0.0 # Error in [Forr08a, p.87]: SSqr = self . SigmaSqr * ( 1 + Lambda - self . psi . T . dot ( solve ( self . U , solve ( self . U . T , self . psi )))) SSqr = power ( abs ( SSqr [ 0 ]), 0.5 )[ 0 ] EI = self . exp_imp ( y0 = f [ 0 ], s0 = SSqr ) return f [ 0 ], SSqr , EI set_de_bounds () \u00b6 Determine search bounds for model_optimizer, e.g., differential evolution. This method sets the attribute de_bounds of the object to a list of lists, where each inner list represents the lower and upper bounds for a parameter being optimized. The number of inner lists is determined by the number of parameters being optimized ( n_theta and n_p ), as well as whether noise is being considered ( noise ). Parameters: Name Type Description Default self object The Kriging object. required Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging ( name = 'kriging' , seed = 124 ) >>> obj = MyClass () >>> obj . set_de_bounds () >>> print ( obj . de_bounds ) [[min_theta, max_theta], [min_theta, max_theta], ..., [min_p, max_p], [min_Lambda, max_Lambda]] Returns: Type Description None None Source code in spotPython/build/kriging.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def set_de_bounds ( self ) -> None : \"\"\" Determine search bounds for model_optimizer, e.g., differential evolution. This method sets the attribute `de_bounds` of the object to a list of lists, where each inner list represents the lower and upper bounds for a parameter being optimized. The number of inner lists is determined by the number of parameters being optimized (`n_theta` and `n_p`), as well as whether noise is being considered (`noise`). Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.set_de_bounds() >>> print(obj.de_bounds) [[min_theta, max_theta], [min_theta, max_theta], ..., [min_p, max_p], [min_Lambda, max_Lambda]] Returns: None \"\"\" de_bounds = [[ self . min_theta , self . max_theta ] for _ in range ( self . n_theta )] if self . optim_p : de_bounds += [[ self . min_p , self . max_p ] for _ in range ( self . n_p )] if self . noise : de_bounds . append ([ self . min_Lambda , self . max_Lambda ]) else : if self . noise : de_bounds . append ([ self . min_Lambda , self . max_Lambda ]) self . de_bounds = de_bounds set_theta_values () \u00b6 Set the theta values for the class instance. This method sets the theta values for the class instance based on the n_theta and k attributes. If n_theta is greater than k , n_theta is set to k and a warning is logged. The method then initializes the theta attribute as a list of zeros with length n_theta . The x0_theta attribute is also initialized as a list of ones with length n_theta , multiplied by n / (100 * k) . Parameters: Name Type Description Default self object The Kriging object. required Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_theta = 3 >>> self . k = 2 >>> instance = MyClass () >>> instance . set_theta_values () >>> instance . theta array([0., 0., 0.]) Source code in spotPython/build/kriging.py 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 def set_theta_values ( self ) -> None : \"\"\" Set the theta values for the class instance. This method sets the theta values for the class instance based on the `n_theta` and `k` attributes. If `n_theta` is greater than `k`, `n_theta` is set to `k` and a warning is logged. The method then initializes the `theta` attribute as a list of zeros with length `n_theta`. The `x0_theta` attribute is also initialized as a list of ones with length `n_theta`, multiplied by `n / (100 * k)`. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_theta = 3 >>> self.k = 2 >>> instance = MyClass() >>> instance.set_theta_values() >>> instance.theta array([0., 0., 0.]) \"\"\" if self . n_theta > self . k : self . n_theta = self . k logger . warning ( \"More theta values than dimensions. `n_theta` set to `k`.\" ) self . theta : List [ float ] = zeros ( self . n_theta ) # TODO: Currently not used: self . x0_theta : List [ float ] = ones (( self . n_theta ,)) * self . n / ( 100 * self . k ) set_variable_types () \u00b6 Set the variable types for the class instance. This method sets the variable types for the class instance based on the var_type attribute. If the length of var_type is less than k , all variable types are forced to \u2018num\u2019 and a warning is logged. The method then creates masks for each variable type (\u2018num\u2019, \u2018factor\u2019, \u2018int\u2019, \u2018float\u2019) using numpy arrays. Parameters: Name Type Description Default self object The Kriging object. required Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . var_type = [ \"num\" , \"factor\" ] >>> instance = MyClass () >>> instance . set_variable_types () >>> instance . num_mask array([ True, False]) Returns: Type Description None None Source code in spotPython/build/kriging.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 def set_variable_types ( self ) -> None : \"\"\" Set the variable types for the class instance. This method sets the variable types for the class instance based on the `var_type` attribute. If the length of `var_type` is less than `k`, all variable types are forced to 'num' and a warning is logged. The method then creates masks for each variable type ('num', 'factor', 'int', 'float') using numpy arrays. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.var_type = [\"num\", \"factor\"] >>> instance = MyClass() >>> instance.set_variable_types() >>> instance.num_mask array([ True, False]) Returns: None \"\"\" # assume all variable types are \"num\" if \"num\" is # specified once: if len ( self . var_type ) < self . k : self . var_type = self . var_type * self . k logger . warning ( \"Warning: All variable types forced to 'num'.\" ) self . num_mask = np . array ( list ( map ( lambda x : x == \"num\" , self . var_type ))) self . factor_mask = np . array ( list ( map ( lambda x : x == \"factor\" , self . var_type ))) self . int_mask = np . array ( list ( map ( lambda x : x == \"int\" , self . var_type ))) self . ordered_mask = np . array ( list ( map ( lambda x : x == \"int\" or x == \"num\" or x == \"float\" , self . var_type ))) update_log () \u00b6 Update the log with the current values of negLnLike, theta, p, and Lambda. This method appends the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to their respective lists in the log dictionary. It also updates the log_length attribute with the current length of the negLnLike list in the log. If spot_writer is not None, this method also writes the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to the spot_writer object. Parameters: Name Type Description Default self object The Kriging object. required Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging ( name = 'kriging' , seed = 124 ) >>> obj = MyClass () >>> obj . update_log () >>> print ( obj . log ) {'negLnLike': [0.5], 'theta': [0.1], 'p': [0.2], 'Lambda': [0.3]} Source code in spotPython/build/kriging.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def update_log ( self ) -> None : \"\"\" Update the log with the current values of negLnLike, theta, p, and Lambda. This method appends the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to their respective lists in the log dictionary. It also updates the log_length attribute with the current length of the negLnLike list in the log. If spot_writer is not None, this method also writes the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to the spot_writer object. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.update_log() >>> print(obj.log) {'negLnLike': [0.5], 'theta': [0.1], 'p': [0.2], 'Lambda': [0.3]} \"\"\" self . log [ \"negLnLike\" ] = append ( self . log [ \"negLnLike\" ], self . negLnLike ) self . log [ \"theta\" ] = append ( self . log [ \"theta\" ], self . theta ) if self . optim_p : self . log [ \"p\" ] = append ( self . log [ \"p\" ], self . p ) if self . noise : self . log [ \"Lambda\" ] = append ( self . log [ \"Lambda\" ], self . Lambda ) # get the length of the log self . log_length = len ( self . log [ \"negLnLike\" ]) if self . spot_writer is not None : writer = self . spot_writer negLnLike = self . negLnLike . copy () writer . add_scalar ( \"spot_negLnLike\" , negLnLike , self . counter + self . log_length ) # add the self.n_theta theta values to the writer with one key \"theta\", # i.e, the same key for all theta values theta = self . theta . copy () writer . add_scalars ( \"spot_theta\" , { f \"theta_ { i } \" : theta [ i ] for i in range ( self . n_theta )}, self . counter + self . log_length ) if self . noise : Lambda = self . Lambda . copy () writer . add_scalar ( \"spot_Lambda\" , Lambda , self . counter + self . log_length ) if self . optim_p : p = self . p . copy () writer . add_scalars ( \"spot_p\" , { f \"p_ { i } \" : p [ i ] for i in range ( self . n_p )}, self . counter + self . log_length ) writer . flush () weighted_exp_imp ( cod_x , w ) \u00b6 Weighted expected improvement. Parameters: Name Type Description Default self object The Kriging object. required cod_x np . ndarray A coded design vector. required w float Weight. required Returns: Name Type Description EI float Weighted expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> cod_x = array ([ 0.3 , 0.3 ]) >>> w = 0.5 >>> k . weighted_exp_imp ( cod_x , w ) 0.0 References [Sobester et al. 2005]. Source code in spotPython/build/kriging.py 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 def weighted_exp_imp ( self , cod_x : np . ndarray , w : float ) -> float : \"\"\" Weighted expected improvement. Args: self (object): The Kriging object. cod_x (np.ndarray): A coded design vector. w (float): Weight. Returns: EI (float): Weighted expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> w = 0.5 >>> k.weighted_exp_imp(cod_x, w) 0.0 References: [Sobester et al. 2005]. \"\"\" y0 , s0 = self . predict_coded ( cod_x ) y_min = min ( self . cod_y ) if s0 <= 0.0 : EI = 0.0 else : y_min_y0 = y_min - y0 EI_one = w * ( y_min_y0 * ( 0.5 + 0.5 * erf (( 1.0 / sqrt ( 2.0 )) * ( y_min_y0 / s0 ))) ) EI_two = ( ( 1.0 - w ) * ( s0 * ( 1.0 / sqrt ( 2.0 * pi ))) * ( exp ( - ( 1.0 / 2.0 ) * (( y_min_y0 ) ** 2.0 / s0 ** 2.0 ))) ) EI = EI_one + EI_two return EI","title":"kriging"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging","text":"Bases: surrogates Kriging surrogate. Attributes: Name Type Description nat_range_X list List of X natural ranges. nat_range_y list List of y nat ranges. noise bool noisy objective function. Default: False. If True , regression kriging will be used. var_type str variable type. Can be either \"num \u201d (numerical) of \"factor\" (factor). num_mask array array of bool variables. True represent numerical (float) variables. factor_mask array array of factor variables. True represents factor (unordered) variables. int_mask array array of integer variables. True represents integers (ordered) variables. ordered_mask array array of ordered variables. True represents integers or float (ordered) variables. Set of veriables which an order relation, i.e., they are either num (float) or int. name str Surrogate name seed int Random seed. use_cod_y bool Use coded y values. sigma float Kriging sigma. gen method Design generator, e.g., spotPython.design.spacefilling.spacefilling. min_theta float min log10 theta value. Defaults: -6. max_theta float max log10 theta value. Defaults: 3. min_p float min p value. Default: 1. max_p float max p value. Default: 2. Source code in spotPython/build/kriging.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 class Kriging ( surrogates ): \"\"\"Kriging surrogate. Attributes: nat_range_X (list): List of X natural ranges. nat_range_y (list): List of y nat ranges. noise (bool): noisy objective function. Default: False. If `True`, regression kriging will be used. var_type (str): variable type. Can be either `\"num`\" (numerical) of `\"factor\"` (factor). num_mask (array): array of bool variables. `True` represent numerical (float) variables. factor_mask (array): array of factor variables. `True` represents factor (unordered) variables. int_mask (array): array of integer variables. `True` represents integers (ordered) variables. ordered_mask (array): array of ordered variables. `True` represents integers or float (ordered) variables. Set of veriables which an order relation, i.e., they are either num (float) or int. name (str): Surrogate name seed (int): Random seed. use_cod_y (bool): Use coded y values. sigma (float): Kriging sigma. gen (method): Design generator, e.g., spotPython.design.spacefilling.spacefilling. min_theta (float): min log10 theta value. Defaults: -6. max_theta (float): max log10 theta value. Defaults: 3. min_p (float): min p value. Default: 1. max_p (float): max p value. Default: 2. \"\"\" def __init__ ( self : object , noise : bool = False , cod_type : Optional [ str ] = \"norm\" , var_type : List [ str ] = [ \"num\" ], use_cod_y : bool = False , name : str = \"kriging\" , seed : int = 124 , model_optimizer = None , model_fun_evals : Optional [ int ] = None , min_theta : float = - 3 , max_theta : float = 2 , n_theta : int = 1 , n_p : int = 1 , optim_p : bool = False , log_level : int = 50 , spot_writer = None , counter = None , ** kwargs ): \"\"\" Initialize the Kriging surrogate. Args: noise (bool): Use regression instead of interpolation kriging. Defaults to False. cod_type (Optional[str]): Normalize or standardize X and values. Can be None, \"norm\", or \"std\". Defaults to \"norm\". var_type (List[str]): Variable type. Can be either \"num\" (numerical) or \"factor\" (factor). Defaults to [\"num\"]. use_cod_y (bool): Use coded y values (instead of natural one). Defaults to False. name (str): Surrogate name. Defaults to \"kriging\". seed (int): Random seed. Defaults to 124. model_optimizer : Optimizer on the surrogate. If None, differential_evolution is selected. model_fun_evals (Optional[int]): Number of iterations used by the optimizer on the surrogate. min_theta (float): Min log10 theta value. Defaults to -3. max_theta (float): Max log10 theta value. Defaults to 2. n_theta (int): Number of theta values. Defaults to 1. n_p (int): Number of p values. Defaults to 1. optim_p (bool): Determines whether p should be optimized. log_level (int): Logging level, e.g., 20 is \"INFO\". Defaults to 50 (\"CRITICAL\"). spot_writer : Spot writer. counter : Counter. Examples: Surrogate of the x*sin(x) function, see [1]. >>> from spotPython.build.kriging import Kriging import numpy as np import matplotlib.pyplot as plt rng = np.random.RandomState(1) X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1) y = np.squeeze(X * np.sin(X)) training_indices = rng.choice(arange(y.size), size=6, replace=False) X_train, y_train = X[training_indices], y[training_indices] S = Kriging(name='kriging', seed=124) S.fit(X_train, y_train) mean_prediction, std_prediction = S.predict(X) plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\") plt.scatter(X_train, y_train, label=\"Observations\") plt.plot(X, mean_prediction, label=\"Mean prediction\") plt.fill_between( X.ravel(), mean_prediction - 1.96 * std_prediction, mean_prediction + 1.96 * std_prediction, alpha=0.5, label=r\"95% confidence interval\", ) plt.legend() plt.xlabel(\"$x$\") plt.ylabel(\"$f(x)$\") _ = plt.title(\"Gaussian process regression on noise-free dataset\") plt.show() References: [[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)] scikit-learn: Gaussian Processes regression: basic introductory example \"\"\" super () . __init__ ( name , seed , log_level ) self . noise = noise self . var_type = var_type self . cod_type = cod_type self . use_cod_y = use_cod_y self . name = name self . seed = seed self . log_level = log_level self . spot_writer = spot_writer self . counter = counter self . sigma = 0 self . eps = sqrt ( spacing ( 1 )) self . min_theta = min_theta self . max_theta = max_theta self . min_p = 1 self . max_p = 2 self . min_Lambda = 1e-9 self . max_Lambda = 1. self . n_theta = n_theta self . n_p = n_p self . optim_p = optim_p # Psi matrix condition: self . cnd_Psi = 0 self . inf_Psi = False self . model_optimizer = model_optimizer if self . model_optimizer is None : self . model_optimizer = differential_evolution self . model_fun_evals = model_fun_evals # differential evaluation uses maxiter = 1000 # and sets the number of function evaluations to # (maxiter + 1) * popsize * N, which results in # 1000 * 15 * k, because the default popsize is 15 and # N is the number of parameters. This seems to be quite large: # for k=2 these are 30 000 iterations. Therefore we set this value to # 100 if self . model_fun_evals is None : self . model_fun_evals = 100 # Logging information self . log [ \"negLnLike\" ] = [] self . log [ \"theta\" ] = [] self . log [ \"p\" ] = [] self . log [ \"Lambda\" ] = [] # Logger logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) def exp_imp ( self , y0 : float , s0 : float ) -> float : \"\"\" Calculates the expected improvement for a given function value and error in coded units. Args: self (object): The Kriging object. y0 (float): The function value in coded units. s0 (float): The error value. Returns: float: The expected improvement value. Examples: >>> from spotPython.build.kriging import Kriging >>> S = Kriging(name='kriging', seed=124) >>> S.cod_y = [0.0, 0.0, 0.0, 0.0, 0.0] >>> S.mean_cod_y = [0.0, 0.0, 0.0, 0.0, 0.0] >>> S.exp_imp(1.0, 2.0) 0.0 \"\"\" # y_min = min(self.cod_y) y_min = min ( self . mean_cod_y ) if s0 <= 0.0 : EI = 0.0 elif s0 > 0.0 : EI_one = ( y_min - y0 ) * ( 0.5 + 0.5 * erf (( 1.0 / sqrt ( 2.0 )) * (( y_min - y0 ) / s0 )) ) EI_two = ( s0 * ( 1.0 / sqrt ( 2.0 * pi ))) * ( exp ( - ( 1.0 / 2.0 ) * (( y_min - y0 ) ** 2.0 / s0 ** 2.0 )) ) EI = EI_one + EI_two return EI def set_de_bounds ( self ) -> None : \"\"\" Determine search bounds for model_optimizer, e.g., differential evolution. This method sets the attribute `de_bounds` of the object to a list of lists, where each inner list represents the lower and upper bounds for a parameter being optimized. The number of inner lists is determined by the number of parameters being optimized (`n_theta` and `n_p`), as well as whether noise is being considered (`noise`). Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.set_de_bounds() >>> print(obj.de_bounds) [[min_theta, max_theta], [min_theta, max_theta], ..., [min_p, max_p], [min_Lambda, max_Lambda]] Returns: None \"\"\" de_bounds = [[ self . min_theta , self . max_theta ] for _ in range ( self . n_theta )] if self . optim_p : de_bounds += [[ self . min_p , self . max_p ] for _ in range ( self . n_p )] if self . noise : de_bounds . append ([ self . min_Lambda , self . max_Lambda ]) else : if self . noise : de_bounds . append ([ self . min_Lambda , self . max_Lambda ]) self . de_bounds = de_bounds def extract_from_bounds ( self , new_theta_p_Lambda : np . ndarray ) -> None : \"\"\" Extract `theta`, `p`, and `Lambda` from bounds. The kriging object stores `theta` as an array, `p` as an array, and `Lambda` as a float. Args: self (object): The Kriging object. new_theta_p_Lambda (np.ndarray): 1d-array with theta, p, and Lambda values. Order is important. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.extract_from_bounds(np.array([1, 2, 3])) >>> print(obj.theta) [1] >>> print(obj.p) [2] >>> print(obj.Lambda) 3 Returns: None \"\"\" self . theta = new_theta_p_Lambda [: self . n_theta ] if self . optim_p : self . p = new_theta_p_Lambda [ self . n_theta : self . n_theta + self . n_p ] if self . noise : self . Lambda = new_theta_p_Lambda [ self . n_theta + self . n_p ] else : if self . noise : self . Lambda = new_theta_p_Lambda [ self . n_theta ] def optimize_model ( self ) -> Union [ List [ float ], Tuple [ float ]]: \"\"\" Optimize the model using the specified model_optimizer. This method uses the specified model_optimizer to optimize the likelihood function (`fun_likelihood`) with respect to the model parameters. The optimization is performed within the bounds specified by the attribute `de_bounds`. The result of the optimization is returned as a list or tuple of optimized parameter values. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> result = obj.optimize_model() >>> print(result) [optimized_theta, optimized_p, optimized_Lambda] Returns: result[\"x\"] (Union[List[float], Tuple[float]]): A list or tuple of optimized parameter values. \"\"\" if self . model_optimizer . __name__ == 'dual_annealing' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) elif self . model_optimizer . __name__ == 'differential_evolution' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds , maxiter = self . model_fun_evals , seed = self . seed ) elif self . model_optimizer . __name__ == 'direct' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds , # maxfun=self.model_fun_evals, eps = 1e-2 ) elif self . model_optimizer . __name__ == 'shgo' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) elif self . model_optimizer . __name__ == 'basinhopping' : result = self . model_optimizer ( func = self . fun_likelihood , x0 = mean ( self . de_bounds , axis = 1 )) else : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) return result [ \"x\" ] def update_log ( self ) -> None : \"\"\" Update the log with the current values of negLnLike, theta, p, and Lambda. This method appends the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to their respective lists in the log dictionary. It also updates the log_length attribute with the current length of the negLnLike list in the log. If spot_writer is not None, this method also writes the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to the spot_writer object. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.update_log() >>> print(obj.log) {'negLnLike': [0.5], 'theta': [0.1], 'p': [0.2], 'Lambda': [0.3]} \"\"\" self . log [ \"negLnLike\" ] = append ( self . log [ \"negLnLike\" ], self . negLnLike ) self . log [ \"theta\" ] = append ( self . log [ \"theta\" ], self . theta ) if self . optim_p : self . log [ \"p\" ] = append ( self . log [ \"p\" ], self . p ) if self . noise : self . log [ \"Lambda\" ] = append ( self . log [ \"Lambda\" ], self . Lambda ) # get the length of the log self . log_length = len ( self . log [ \"negLnLike\" ]) if self . spot_writer is not None : writer = self . spot_writer negLnLike = self . negLnLike . copy () writer . add_scalar ( \"spot_negLnLike\" , negLnLike , self . counter + self . log_length ) # add the self.n_theta theta values to the writer with one key \"theta\", # i.e, the same key for all theta values theta = self . theta . copy () writer . add_scalars ( \"spot_theta\" , { f \"theta_ { i } \" : theta [ i ] for i in range ( self . n_theta )}, self . counter + self . log_length ) if self . noise : Lambda = self . Lambda . copy () writer . add_scalar ( \"spot_Lambda\" , Lambda , self . counter + self . log_length ) if self . optim_p : p = self . p . copy () writer . add_scalars ( \"spot_p\" , { f \"p_ { i } \" : p [ i ] for i in range ( self . n_p )}, self . counter + self . log_length ) writer . flush () def fit ( self , nat_X : np . ndarray , nat_y : np . ndarray ) -> object : \"\"\" Fits the hyperparameters (`theta`, `p`, `Lambda`) of the Kriging model. The function computes the following internal values: 1. `theta`, `p`, and `Lambda` values via optimization of the function `fun_likelihood()`. 2. Correlation matrix `Psi` via `rebuildPsi()`. Args: self (object): The Kriging object. nat_X (np.ndarray): Sample points. nat_y (np.ndarray): Function values. Returns: object: Fitted estimator. Attributes: theta (np.ndarray): Kriging theta values. Shape (k,). p (np.ndarray): Kriging p values. Shape (k,). LnDetPsi (np.float64): Determinant Psi matrix. Psi (np.matrix): Correlation matrix Psi. Shape (n,n). psi (np.ndarray): psi vector. Shape (n,). one (np.ndarray): vector of ones. Shape (n,). mu (np.float64): Kriging expected mean value mu. U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n). SigmaSqr (np.float64): Sigma squared value. Lambda (float): lambda noise value. Examples: >>> from spotPython.build.kriging import Kriging >>> nat_X = np.array([[1, 2], [3, 4]]) >>> nat_y = np.array([1, 2]) >>> surrogate = Kriging() >>> surrogate.fit(nat_X, nat_y) \"\"\" self . initialize_variables ( nat_X , nat_y ) self . set_variable_types () self . nat_to_cod_init () self . set_theta_values () self . initialize_matrices () # build_Psi() and build_U() are called in fun_likelihood self . set_de_bounds () # Finally, set new theta and p values and update the surrogate again # for new_theta_p_Lambda in de_results[\"x\"]: new_theta_p_Lambda = self . optimize_model () self . extract_from_bounds ( new_theta_p_Lambda ) self . build_Psi () self . build_U () # TODO: check if the following line is necessary! self . likelihood () self . update_log () def initialize_variables ( self , nat_X : np . ndarray , nat_y : np . ndarray ) -> None : \"\"\" Initialize variables for the class instance. This method takes in the independent and dependent variable data as input and initializes the class instance variables. It creates deep copies of the input data and stores them in the instance variables `nat_X` and `nat_y`. It also calculates the number of observations `n` and the number of independent variables `k` from the shape of `nat_X`. Finally, it creates empty arrays with the same shape as `nat_X` and `nat_y` and stores them in the instance variables `cod_X` and `cod_y`. Args: self (object): The Kriging object. nat_X (np.ndarray): The independent variable data. nat_y (np.ndarray): The dependent variable data. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> surrogate = Kriging() >>> nat_X = np.array([[1, 2], [3, 4]]) >>> nat_y = np.array([1, 2]) >>> surrogate.initialize_variables(nat_X, nat_y) >>> surrogate.nat_X array([[1, 2], [3, 4]]) >>> surrogate.nat_y array([1, 2]) \"\"\" self . nat_X = copy . deepcopy ( nat_X ) self . nat_y = copy . deepcopy ( nat_y ) self . n = self . nat_X . shape [ 0 ] self . k = self . nat_X . shape [ 1 ] self . cod_X = np . empty_like ( self . nat_X ) self . cod_y = np . empty_like ( self . nat_y ) def set_variable_types ( self ) -> None : \"\"\" Set the variable types for the class instance. This method sets the variable types for the class instance based on the `var_type` attribute. If the length of `var_type` is less than `k`, all variable types are forced to 'num' and a warning is logged. The method then creates masks for each variable type ('num', 'factor', 'int', 'float') using numpy arrays. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.var_type = [\"num\", \"factor\"] >>> instance = MyClass() >>> instance.set_variable_types() >>> instance.num_mask array([ True, False]) Returns: None \"\"\" # assume all variable types are \"num\" if \"num\" is # specified once: if len ( self . var_type ) < self . k : self . var_type = self . var_type * self . k logger . warning ( \"Warning: All variable types forced to 'num'.\" ) self . num_mask = np . array ( list ( map ( lambda x : x == \"num\" , self . var_type ))) self . factor_mask = np . array ( list ( map ( lambda x : x == \"factor\" , self . var_type ))) self . int_mask = np . array ( list ( map ( lambda x : x == \"int\" , self . var_type ))) self . ordered_mask = np . array ( list ( map ( lambda x : x == \"int\" or x == \"num\" or x == \"float\" , self . var_type ))) def set_theta_values ( self ) -> None : \"\"\" Set the theta values for the class instance. This method sets the theta values for the class instance based on the `n_theta` and `k` attributes. If `n_theta` is greater than `k`, `n_theta` is set to `k` and a warning is logged. The method then initializes the `theta` attribute as a list of zeros with length `n_theta`. The `x0_theta` attribute is also initialized as a list of ones with length `n_theta`, multiplied by `n / (100 * k)`. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_theta = 3 >>> self.k = 2 >>> instance = MyClass() >>> instance.set_theta_values() >>> instance.theta array([0., 0., 0.]) \"\"\" if self . n_theta > self . k : self . n_theta = self . k logger . warning ( \"More theta values than dimensions. `n_theta` set to `k`.\" ) self . theta : List [ float ] = zeros ( self . n_theta ) # TODO: Currently not used: self . x0_theta : List [ float ] = ones (( self . n_theta ,)) * self . n / ( 100 * self . k ) def initialize_matrices ( self ) -> None : \"\"\" Initialize the matrices for the class instance. This method initializes several matrices and attributes for the class instance. The `p` attribute is initialized as a list of ones with length `n_p`, multiplied by 2.0. The `pen_val` attribute is initialized as the natural logarithm of the variance of `nat_y`, multiplied by `n`, plus 1e4. The `negLnLike`, `LnDetPsi`, `mu`, `U`, `SigmaSqr`, and `Lambda` attributes are all set to None. The `gen` attribute is initialized using the `spacefilling` function with arguments `k` and `seed`. The `Psi` attribute is initialized as a zero matrix with shape `(n, n)` and dtype `float64`. The `psi` attribute is initialized as a zero matrix with shape `(n, 1)`. The `one` attribute is initialized as a list of ones with length `n`. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> instance.initialize_matrices() Returns: None \"\"\" self . p = ones ( self . n_p ) * 2.0 self . pen_val = self . n * log ( var ( self . nat_y )) + 1e4 self . negLnLike = None self . gen = spacefilling ( k = self . k , seed = self . seed ) self . LnDetPsi = None self . Psi = zeros (( self . n , self . n ), dtype = float64 ) self . psi = zeros (( self . n , 1 )) self . one = ones ( self . n ) self . mu = None self . U = None self . SigmaSqr = None self . Lambda = None def fun_likelihood ( self , new_theta_p_Lambda : np . ndarray ) -> float : \"\"\" Compute log likelihood for a set of hyperparameters (theta, p, Lambda). This method computes the log likelihood for a set of hyperparameters (theta, p, Lambda) by performing the following steps: 1. Extracts the hyperparameters from the input array using `extract_from_bounds()`. 2. Checks if any element in `10^theta` is equal to 0. If so, logs a warning and returns the penalty value (`pen_val`). 3. Builds the `Psi` matrix using `build_Psi()`. 4. Checks if `Psi` is ill-conditioned or infinite. If so, logs a warning and returns the penalty value (`pen_val`). 5. Builds the `U` matrix using `build_U()`. If an exception occurs, logs an error and returns the penalty value (`pen_val`). 6. Computes the negative log likelihood using `likelihood()`. 7. Returns the computed negative log likelihood (`negLnLike`). Args: self (object): The Kriging object. new_theta_p_Lambda (np.ndarray): An array containing the `theta`, `p`, and `Lambda` values. Returns: float: The negative log likelihood of the surface at the specified hyperparameters. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> negLnLike = instance.fun_likelihood(new_theta_p_Lambda) >>> print(negLnLike) \"\"\" self . extract_from_bounds ( new_theta_p_Lambda ) if self . __is_any__ ( power ( 10.0 , self . theta ), 0 ): logger . warning ( \"Failure in fun_likelihood: 10^theta == 0. Setting negLnLike to %s \" , self . pen_val ) return self . pen_val self . build_Psi () if ( self . inf_Psi or self . cnd_Psi > 1e9 ): logger . warning ( \"Failure in fun_likelihood: Psi is ill conditioned: %s \" , self . cnd_Psi ) logger . warning ( \"Setting negLnLike to: %s \" , self . pen_val ) return self . pen_val try : self . build_U () except Exception as error : penalty_value = self . pen_val print ( \"Error in fun_likelihood(). Call to build_U() failed.\" ) print ( \"error= %s , type(error)= %s \" % ( error , type ( error ))) print ( \"Setting negLnLike to %.2f .\" % self . pen_val ) return penalty_value self . likelihood () return self . negLnLike def __is_any__ ( self , x : Union [ np . ndarray , Any ], v : Any ) -> bool : \"\"\" Check if any element in `x` is equal to `v`. This method checks if any element in the input array `x` is equal to the value `v`. If `x` is not an instance of `ndarray`, it is first converted to a numpy array using the `array()` function. Args: self (object): The Kriging object. x (np.ndarray or array-like): The input array to check for the presence of value `v`. v (scalar): The value to check for in the input array `x`. Returns: bool: True if any element in `x` is equal to `v`, False otherwise. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> result = instance.__is_any__(x, v) >>> print(result) \"\"\" if not isinstance ( x , ndarray ): x = array ([ x ]) return any ( x == v ) def build_Psi ( self ) -> None : \"\"\" Constructs a new (n x n) correlation matrix Psi to reflect new data or a change in hyperparameters. This method uses `theta`, `p`, and coded `X` values to construct the correlation matrix as described in [Forr08a, p.57]. Args: self (object): The Kriging object. Returns: None Raises: LinAlgError: If building Psi fails. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_Psi() \"\"\" self . Psi = zeros (( self . n , self . n ), dtype = float64 ) theta = power ( 10.0 , self . theta ) if self . n_theta == 1 : theta = theta * ones ( self . k ) try : D = zeros (( self . n , self . n )) if self . ordered_mask . any (): X_ordered = self . cod_X [:, self . ordered_mask ] D = squareform ( pdist ( X_ordered , metric = 'sqeuclidean' , out = None , w = theta [ self . ordered_mask ])) if self . factor_mask . any (): X_factor = self . cod_X [:, self . factor_mask ] D = ( D + squareform ( pdist ( X_factor , metric = 'hamming' , out = None , w = theta [ self . factor_mask ]))) self . Psi = exp ( - D ) except LinAlgError as err : print ( f \"Building Psi failed: \\n { self . Psi } . { err =} , { type ( err ) =} \" ) if self . noise : self . Psi [ diag_indices_from ( self . Psi )] += self . Lambda else : self . Psi [ diag_indices_from ( self . Psi )] += self . eps if ( isinf ( self . Psi )) . any (): self . inf_Psi = True self . cnd_Psi = cond ( self . Psi ) def build_U ( self , scipy : bool = True ) -> None : \"\"\" Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57]. This method uses either `scipy_cholesky` or numpy's `cholesky` to perform the Cholesky factorization of Psi. Args: self (object): The Kriging object. scipy (bool): If True, use `scipy_cholesky`. If False, use numpy's `cholesky`. Defaults to True. Returns: None Raises: LinAlgError: If Cholesky factorization fails for Psi. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_U() \"\"\" try : self . U = scipy_cholesky ( self . Psi , lower = True ) if scipy else cholesky ( self . Psi ) self . U = self . U . T except LinAlgError as err : print ( f \"build_U() Cholesky failed for Psi: \\n { self . Psi } . { err =} , { type ( err ) =} \" ) def likelihood ( self ) -> None : \"\"\" Calculates the negative of the concentrated log-likelihood. This method implements equation (2.32) in [Forr08a] to calculate the negative of the concentrated log-likelihood. It also modifies `mu`, `SigmaSqr`, `LnDetPsi`, and `negLnLike`. Note: `build_Psi` and `build_U` should be called first. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_Psi() >>> obj.build_U() >>> obj.likelihood() \"\"\" # (2.20) in [Forr08a]: U_T_inv_one = solve ( self . U . T , self . one ) U_T_inv_cod_y = solve ( self . U . T , self . cod_y ) mu = self . one . T . dot ( solve ( self . U , U_T_inv_cod_y )) / self . one . T . dot ( solve ( self . U , U_T_inv_one )) self . mu = mu # (2.31) in [Forr08a] cod_y_minus_mu = self . cod_y - self . one . dot ( self . mu ) self . SigmaSqr = cod_y_minus_mu . T . dot ( solve ( self . U , solve ( self . U . T , cod_y_minus_mu ))) / self . n # (2.32) in [Forr08a] self . LnDetPsi = 2.0 * sum ( log ( abs ( diag ( self . U )))) self . negLnLike = - 1.0 * ( - ( self . n / 2.0 ) * log ( self . SigmaSqr ) - 0.5 * self . LnDetPsi ) def plot ( self , show : Optional [ bool ] = True ) -> None : \"\"\" This function plots 1D and 2D surrogates. Args: self (object): The Kriging object. show (bool): If `True`, the plots are displayed. If `False`, `plt.show()` should be called outside this function. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> plot(show=True) \"\"\" if self . k == 1 : # TODO: Improve plot (add conf. interval etc.) fig = pylab . figure ( figsize = ( 9 , 6 )) # t1 = array(arange(0.0, 1.0, 0.01)) # y1 = array([self.predict(array([x]), return_val=\"y\") for x in t1]) # plt.figure() # plt.plot(t1, y1, \"k\") # if show: # plt.show() # n_grid = 100 x = linspace ( self . nat_range_X [ 0 ][ 0 ], self . nat_range_X [ 0 ][ 1 ], num = n_grid ) y = self . predict ( x ) plt . figure () plt . plot ( x , y , \"k\" ) if show : plt . show () if self . k == 2 : fig = pylab . figure ( figsize = ( 9 , 6 )) n_grid = 100 x = linspace ( self . nat_range_X [ 0 ][ 0 ], self . nat_range_X [ 0 ][ 1 ], num = n_grid ) y = linspace ( self . nat_range_X [ 1 ][ 0 ], self . nat_range_X [ 1 ][ 1 ], num = n_grid ) X , Y = meshgrid ( x , y ) # Predict based on the optimized results zz = array ( [ self . predict ( array ([ x , y ]), return_val = \"all\" ) for x , y in zip ( ravel ( X ), ravel ( Y ))] ) zs = zz [:, 0 , :] zse = zz [:, 1 , :] Z = zs . reshape ( X . shape ) Ze = zse . reshape ( X . shape ) if self . cod_type == \"norm\" : nat_point_X = ( self . cod_X [:, 0 ] * ( self . nat_range_X [ 0 ][ 1 ] - self . nat_range_X [ 0 ][ 0 ]) ) + self . nat_range_X [ 0 ][ 0 ] nat_point_Y = ( self . cod_X [:, 1 ] * ( self . nat_range_X [ 1 ][ 1 ] - self . nat_range_X [ 1 ][ 0 ]) ) + self . nat_range_X [ 1 ][ 0 ] elif self . cod_type == \"std\" : nat_point_X = self . cod_X [:, 0 ] * self . nat_std_X [ 0 ] + self . nat_mean_X [ 0 ] nat_point_Y = self . cod_X [:, 1 ] * self . nat_std_X [ 1 ] + self . nat_mean_X [ 1 ] else : nat_point_X = self . cod_X [:, 0 ] nat_point_Y = self . cod_X [:, 1 ] contour_levels = 30 ax = fig . add_subplot ( 224 ) # plot predicted values: pylab . contourf ( X , Y , Ze , contour_levels , cmap = \"jet\" ) pylab . title ( \"Error\" ) pylab . colorbar () # plot observed points: pylab . plot ( nat_point_X , nat_point_Y , \"ow\" ) # ax = fig . add_subplot ( 223 ) # plot predicted values: plt . contourf ( X , Y , Z , contour_levels , zorder = 1 , cmap = \"jet\" ) plt . title ( \"Surrogate\" ) # plot observed points: pylab . plot ( nat_point_X , nat_point_Y , \"ow\" , zorder = 3 ) pylab . colorbar () # ax = fig . add_subplot ( 221 , projection = \"3d\" ) ax . plot_surface ( X , Y , Z , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" ) # ax = fig . add_subplot ( 222 , projection = \"3d\" ) ax . plot_surface ( X , Y , Ze , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" ) # pylab . show () def predict ( self , nat_X : ndarray , nat : bool = True , return_val : str = \"y\" ) -> Union [ float , Tuple [ float , float , float ]]: \"\"\" This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X. Args: self (object): The Kriging object. nat_X (ndarray): Design variable to evaluate in natural units. nat (bool): argument `nat_X` is in natural range. Default: `True`. If set to `False`, `nat_X` will not be normalized (which might be useful if already normalized y values are used). return_val (str): whether `y`, `s`, neg. `ei` (negative expected improvement), or all three values are returned. Default is (for compatibility with sklearn) \"y\". To return `s`, select \"s\", to return neg. `ei`, select \"ei\". To return the tuple `(y, s, ei)`, select \"all\". Returns: float: The predicted value in natural units if return_val is \"y\". float: predicted error if return_val is \"s\". float: expected improvement if return_val is \"ei\". Tuple[float, float, float]: The predicted value in natural units, predicted error and expected improvement if return_val is \"all\". Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> k.predict(array([[0.3, 0.3]])) array([0.09]) \"\"\" # Check for the shape and the type of the Input if isinstance ( nat_X , ndarray ): try : X = nat_X . reshape ( - 1 , self . nat_X . shape [ 1 ]) X = repair_non_numeric ( X , self . var_type ) except Exception : raise TypeError ( \"13.1: Input to predict was not convertible to the size of X\" ) else : raise TypeError ( f \"type of the given input is an { type ( nat_X ) } instead of an ndarray\" ) n = X . shape [ 0 ] y = empty ( n , dtype = float ) s = empty ( n , dtype = float ) ei = empty ( n , dtype = float ) for i in range ( n ): if nat : x = self . nat_to_cod_x ( X [ i , :]) else : x = X [ i , :] y [ i ], s [ i ], ei [ i ] = self . predict_coded ( x ) if return_val == \"y\" : return y elif return_val == \"s\" : return s elif return_val == \"ei\" : return - 1.0 * ei else : return y , s , - 1.0 * ei def build_psi_vec ( self , cod_x : ndarray ) -> None : \"\"\" Build the psi vector. Needed by `predict_cod`, `predict_err_coded`, `regression_predict_coded`. Modifies `self.psi`. Args: self (object): The Kriging object. cod_x (ndarray): point to calculate psi Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> build_psi_vec(cod_x) \"\"\" self . psi = zeros (( self . n )) # theta = self.theta # TODO: theta = power ( 10.0 , self . theta ) if self . n_theta == 1 : theta = theta * ones ( self . k ) try : D = zeros (( self . n )) if self . ordered_mask . any (): X_ordered = self . cod_X [:, self . ordered_mask ] x_ordered = cod_x [ self . ordered_mask ] D = cdist ( x_ordered . reshape ( - 1 , sum ( self . ordered_mask )), X_ordered . reshape ( - 1 , sum ( self . ordered_mask )), metric = 'sqeuclidean' , out = None , w = theta [ self . ordered_mask ]) if self . factor_mask . any (): X_factor = self . cod_X [:, self . factor_mask ] x_factor = cod_x [ self . factor_mask ] D = ( D + cdist ( x_factor . reshape ( - 1 , sum ( self . factor_mask )), X_factor . reshape ( - 1 , sum ( self . factor_mask )), metric = 'hamming' , out = None , w = theta [ self . factor_mask ])) self . psi = exp ( - D ) . T except LinAlgError as err : print ( f \"Building psi failed: \\n { self . psi } . { err =} , { type ( err ) =} \" ) def predict_coded ( self , cod_x : np . ndarray ) -> Tuple [ float , float , float ]: \"\"\" Kriging prediction of one point in the coded units as described in (2.20) in [Forr08a]. The error is returned as well. Args: self (object): The Kriging object. cod_x (np.ndarray): Point in coded units to make prediction at. Returns: f (float): Predicted value in coded units. SSqr (float): Predicted error. EI (float): Expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> k.predict_coded(cod_x) (0.09, 0.0, 0.0) Note: `self.mu` and `self.SigmaSqr` are computed in `likelihood`, not here. See also [Forr08a, p.60]. \"\"\" self . build_psi_vec ( cod_x ) U_T_inv = solve ( self . U . T , self . cod_y - self . one . dot ( self . mu )) f = self . mu + self . psi . T . dot ( solve ( self . U , U_T_inv )) if self . noise : Lambda = self . Lambda else : Lambda = 0.0 # Error in [Forr08a, p.87]: SSqr = self . SigmaSqr * ( 1 + Lambda - self . psi . T . dot ( solve ( self . U , solve ( self . U . T , self . psi )))) SSqr = power ( abs ( SSqr [ 0 ]), 0.5 )[ 0 ] EI = self . exp_imp ( y0 = f [ 0 ], s0 = SSqr ) return f [ 0 ], SSqr , EI def weighted_exp_imp ( self , cod_x : np . ndarray , w : float ) -> float : \"\"\" Weighted expected improvement. Args: self (object): The Kriging object. cod_x (np.ndarray): A coded design vector. w (float): Weight. Returns: EI (float): Weighted expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> w = 0.5 >>> k.weighted_exp_imp(cod_x, w) 0.0 References: [Sobester et al. 2005]. \"\"\" y0 , s0 = self . predict_coded ( cod_x ) y_min = min ( self . cod_y ) if s0 <= 0.0 : EI = 0.0 else : y_min_y0 = y_min - y0 EI_one = w * ( y_min_y0 * ( 0.5 + 0.5 * erf (( 1.0 / sqrt ( 2.0 )) * ( y_min_y0 / s0 ))) ) EI_two = ( ( 1.0 - w ) * ( s0 * ( 1.0 / sqrt ( 2.0 * pi ))) * ( exp ( - ( 1.0 / 2.0 ) * (( y_min_y0 ) ** 2.0 / s0 ** 2.0 ))) ) EI = EI_one + EI_two return EI def calculate_mean_MSE ( self , n_samples : int = 200 , points : Optional [ np . ndarray ] = None ) -> Tuple [ float , float ]: \"\"\" Calculates the mean MSE metric of the model by evaluating MSE at a number of points. Args: self (object): The Kriging object. n_samples (int): Number of points to sample the mean squared error at. Ignored if the points argument is specified. points (np.ndarray): An array of points to sample the model at. Returns: mean_MSE (float): The mean value of MSE. std_MSE (float): The standard deviation of the MSE points. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> n_samples = 200 >>> mean_MSE, std_MSE = k.calculate_mean_MSE(n_samples) >>> print(f\"Mean MSE: {mean_MSE}, Standard deviation of MSE: {std_MSE}\") \"\"\" if points is None : points = self . gen . lhd ( n_samples ) values = [ self . predict ( cod_X = point , nat = True , return_val = \"s\" ) for point in points ] return mean ( values ), std ( values ) def cod_to_nat_x ( self , cod_X : np . ndarray ) -> np . ndarray : \"\"\" Converts an array representing one point in normalized (coded) units to natural (physical or real world) units. Args: self (object): The Kriging object. cod_X (np.ndarray): An array representing one point (self.k long) in normalized (coded) units. Returns: X (np.ndarray): An array of natural (physical or real world) units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_X = array([0.3, 0.3]) >>> nat_X = k.cod_to_nat_x(cod_X) >>> print(f\"Natural units: {nat_X}\") \"\"\" X = copy . deepcopy ( cod_X ) if self . cod_type == \"norm\" : for i in range ( self . k ): X [ i ] = ( X [ i ] * float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ]) ) + self . nat_range_X [ i ][ 0 ] return X elif self . cod_type == \"std\" : for i in range ( self . k ): X [ i ] = X [ i ] * self . nat_std_X [ i ] + self . nat_mean_X [ i ] return X else : return cod_X def cod_to_nat_y ( self , cod_y : np . ndarray ) -> np . ndarray : \"\"\" Converts a normalized array of coded (model) units in the range of [0,1] to an array of observed values in real-world units. Args: self (object): The Kriging object. cod_y (np.ndarray): A normalized array of coded (model) units in the range of [0,1]. Returns: y (np.ndarray): An array of observed values in real-world units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_y = array([0.5, 0.5]) >>> nat_y = k.cod_to_nat_y(cod_y) >>> print(f\"Real-world units: {nat_y}\") \"\"\" return ( cod_y * ( self . nat_range_y [ 1 ] - self . nat_range_y [ 0 ]) + self . nat_range_y [ 0 ] if self . cod_type == \"norm\" else cod_y * self . nat_std_y + self . nat_mean_y if self . cod_type == \"std\" else cod_y ) def nat_to_cod_x ( self , nat_X : np . ndarray ) -> np . ndarray : \"\"\" Normalizes one point (row) of nat_X array to [0,1]. The internal nat_range_X values are not updated. Args: self (object): The Kriging object. nat_X (np.ndarray): An array representing one point (self.k long) in natural (physical or real world) units. Returns: X (np.ndarray): An array of coded values in the range of [0,1] for each dimension. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> nat_X = array([5.0, 5.0]) >>> cod_X = k.nat_to_cod_x(nat_X) >>> print(f\"Coded values: {cod_X}\") \"\"\" X = copy . deepcopy ( nat_X ) if self . cod_type == \"norm\" : for i in range ( self . k ): # TODO: Check Implementation of range correction if range == 0: # rangex <- xmax - xmin # rangey <- ymax - ymin # xmin[rangex == 0] <- xmin[rangex == 0] - 0.5 # xmax[rangex == 0] <- xmax[rangex == 0] + 0.5 # rangex[rangex == 0] <- 1 # logger.debug(f\"self.nat_range_X[{i}]:\\n {self.nat_range_X[i]}\") # logger.debug(f\"X[{i}]:\\n {X[i]}\") rangex = float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ]) if rangex == 0 : self . nat_range_X [ i ][ 0 ] = self . nat_range_X [ i ][ 0 ] - 0.5 self . nat_range_X [ i ][ 1 ] = self . nat_range_X [ i ][ 1 ] + 0.5 X [ i ] = ( X [ i ] - self . nat_range_X [ i ][ 0 ]) / float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ] ) return X elif self . cod_type == \"std\" : for i in range ( self . k ): X [ i ] = ( X [ i ] - self . nat_mean_X [ i ]) / self . nat_std_X [ i ] return X else : return nat_X def nat_to_cod_y ( self , nat_y : np . ndarray ) -> np . ndarray : \"\"\" Normalizes natural y values to [0,1]. Args: self (object): The Kriging object. nat_y (np.ndarray): An array of observed values in natural (real-world) units. Returns: y (np.ndarray): A normalized array of coded (model) units in the range of [0,1]. Examples: >>> from spotPython.build.kriging import Kriging >>> import numpy as np >>> kriging = Kriging() >>> nat_y = np.array([5.0, 5.0]) >>> cod_y = kriging.nat_to_cod_y(nat_y) >>> print(f\"Coded values: {cod_y}\") \"\"\" return ( ( nat_y - self . nat_range_y [ 0 ]) / ( self . nat_range_y [ 1 ] - self . nat_range_y [ 0 ]) if self . use_cod_y and self . cod_type == \"norm\" else ( nat_y - self . nat_mean_y ) / self . nat_std_y if self . use_cod_y and self . cod_type == \"std\" else nat_y ) def nat_to_cod_init ( self ) -> None : \"\"\" Determines max and min of each dimension and normalizes that axis to a range of [0,1]. Called when 1) surrogate is initialized and 2) new points arrive, i.e., suggested by the surrogate as infill points. This method calls `nat_to_cod_x` and `nat_to_cod_y` and updates the ranges `nat_range_X` and `nat_range_y`. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> kriging = Kriging() >>> kriging.nat_to_cod_init() \"\"\" self . nat_range_X = [] self . nat_range_y = [] for i in range ( self . k ): self . nat_range_X . append ([ min ( self . nat_X [:, i ]), max ( self . nat_X [:, i ])]) self . nat_range_y . append ( min ( self . nat_y )) self . nat_range_y . append ( max ( self . nat_y )) self . nat_mean_X = mean ( self . nat_X , axis = 0 ) self . nat_std_X = std ( self . nat_X , axis = 0 ) self . nat_mean_y = mean ( self . nat_y ) self . nat_std_y = std ( self . nat_y ) Z = aggregate_mean_var ( X = self . nat_X , y = self . nat_y ) mu = Z [ 1 ] self . mean_cod_y = empty_like ( mu ) for i in range ( self . n ): self . cod_X [ i ] = self . nat_to_cod_x ( self . nat_X [ i ]) for i in range ( self . n ): self . cod_y [ i ] = self . nat_to_cod_y ( self . nat_y [ i ]) for i in range ( mu . shape [ 0 ]): self . mean_cod_y [ i ] = self . nat_to_cod_y ( mu [ i ])","title":"Kriging"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.__init__","text":"Initialize the Kriging surrogate. Parameters: Name Type Description Default noise bool Use regression instead of interpolation kriging. Defaults to False. False cod_type Optional [ str ] Normalize or standardize X and values. Can be None, \u201cnorm\u201d, or \u201cstd\u201d. Defaults to \u201cnorm\u201d. 'norm' var_type List [ str ] Variable type. Can be either \u201cnum\u201d (numerical) or \u201cfactor\u201d (factor). Defaults to [\u201cnum\u201d]. ['num'] use_cod_y bool Use coded y values (instead of natural one). Defaults to False. False name str Surrogate name. Defaults to \u201ckriging\u201d. 'kriging' seed int Random seed. Defaults to 124. 124 model_optimizer Optimizer on the surrogate. If None, differential_evolution is selected. None model_fun_evals Optional [ int ] Number of iterations used by the optimizer on the surrogate. None min_theta float Min log10 theta value. Defaults to -3. -3 max_theta float Max log10 theta value. Defaults to 2. 2 n_theta int Number of theta values. Defaults to 1. 1 n_p int Number of p values. Defaults to 1. 1 optim_p bool Determines whether p should be optimized. False log_level int Logging level, e.g., 20 is \u201cINFO\u201d. Defaults to 50 (\u201cCRITICAL\u201d). 50 spot_writer Spot writer. None counter Counter. None Examples: Surrogate of the x*sin(x) function, see [1]. >>> from spotPython.build.kriging import Kriging import numpy as np import matplotlib.pyplot as plt rng = np.random.RandomState(1) X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1) y = np.squeeze(X * np.sin(X)) training_indices = rng.choice(arange(y.size), size=6, replace=False) X_train, y_train = X[training_indices], y[training_indices] S = Kriging(name='kriging', seed=124) S.fit(X_train, y_train) mean_prediction, std_prediction = S.predict(X) plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\") plt.scatter(X_train, y_train, label=\"Observations\") plt.plot(X, mean_prediction, label=\"Mean prediction\") plt.fill_between( X.ravel(), mean_prediction - 1.96 * std_prediction, mean_prediction + 1.96 * std_prediction, alpha=0.5, label=r\"95% confidence interval\", ) plt.legend() plt.xlabel(\"$x$\") plt.ylabel(\"$f(x)$\") _ = plt.title(\"Gaussian process regression on noise-free dataset\") plt.show() References [ 1 ] scikit-learn: Gaussian Processes regression: basic introductory example Source code in spotPython/build/kriging.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def __init__ ( self : object , noise : bool = False , cod_type : Optional [ str ] = \"norm\" , var_type : List [ str ] = [ \"num\" ], use_cod_y : bool = False , name : str = \"kriging\" , seed : int = 124 , model_optimizer = None , model_fun_evals : Optional [ int ] = None , min_theta : float = - 3 , max_theta : float = 2 , n_theta : int = 1 , n_p : int = 1 , optim_p : bool = False , log_level : int = 50 , spot_writer = None , counter = None , ** kwargs ): \"\"\" Initialize the Kriging surrogate. Args: noise (bool): Use regression instead of interpolation kriging. Defaults to False. cod_type (Optional[str]): Normalize or standardize X and values. Can be None, \"norm\", or \"std\". Defaults to \"norm\". var_type (List[str]): Variable type. Can be either \"num\" (numerical) or \"factor\" (factor). Defaults to [\"num\"]. use_cod_y (bool): Use coded y values (instead of natural one). Defaults to False. name (str): Surrogate name. Defaults to \"kriging\". seed (int): Random seed. Defaults to 124. model_optimizer : Optimizer on the surrogate. If None, differential_evolution is selected. model_fun_evals (Optional[int]): Number of iterations used by the optimizer on the surrogate. min_theta (float): Min log10 theta value. Defaults to -3. max_theta (float): Max log10 theta value. Defaults to 2. n_theta (int): Number of theta values. Defaults to 1. n_p (int): Number of p values. Defaults to 1. optim_p (bool): Determines whether p should be optimized. log_level (int): Logging level, e.g., 20 is \"INFO\". Defaults to 50 (\"CRITICAL\"). spot_writer : Spot writer. counter : Counter. Examples: Surrogate of the x*sin(x) function, see [1]. >>> from spotPython.build.kriging import Kriging import numpy as np import matplotlib.pyplot as plt rng = np.random.RandomState(1) X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1) y = np.squeeze(X * np.sin(X)) training_indices = rng.choice(arange(y.size), size=6, replace=False) X_train, y_train = X[training_indices], y[training_indices] S = Kriging(name='kriging', seed=124) S.fit(X_train, y_train) mean_prediction, std_prediction = S.predict(X) plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\") plt.scatter(X_train, y_train, label=\"Observations\") plt.plot(X, mean_prediction, label=\"Mean prediction\") plt.fill_between( X.ravel(), mean_prediction - 1.96 * std_prediction, mean_prediction + 1.96 * std_prediction, alpha=0.5, label=r\"95% confidence interval\", ) plt.legend() plt.xlabel(\"$x$\") plt.ylabel(\"$f(x)$\") _ = plt.title(\"Gaussian process regression on noise-free dataset\") plt.show() References: [[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)] scikit-learn: Gaussian Processes regression: basic introductory example \"\"\" super () . __init__ ( name , seed , log_level ) self . noise = noise self . var_type = var_type self . cod_type = cod_type self . use_cod_y = use_cod_y self . name = name self . seed = seed self . log_level = log_level self . spot_writer = spot_writer self . counter = counter self . sigma = 0 self . eps = sqrt ( spacing ( 1 )) self . min_theta = min_theta self . max_theta = max_theta self . min_p = 1 self . max_p = 2 self . min_Lambda = 1e-9 self . max_Lambda = 1. self . n_theta = n_theta self . n_p = n_p self . optim_p = optim_p # Psi matrix condition: self . cnd_Psi = 0 self . inf_Psi = False self . model_optimizer = model_optimizer if self . model_optimizer is None : self . model_optimizer = differential_evolution self . model_fun_evals = model_fun_evals # differential evaluation uses maxiter = 1000 # and sets the number of function evaluations to # (maxiter + 1) * popsize * N, which results in # 1000 * 15 * k, because the default popsize is 15 and # N is the number of parameters. This seems to be quite large: # for k=2 these are 30 000 iterations. Therefore we set this value to # 100 if self . model_fun_evals is None : self . model_fun_evals = 100 # Logging information self . log [ \"negLnLike\" ] = [] self . log [ \"theta\" ] = [] self . log [ \"p\" ] = [] self . log [ \"Lambda\" ] = [] # Logger logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" )","title":"__init__()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.__is_any__","text":"Check if any element in x is equal to v . This method checks if any element in the input array x is equal to the value v . If x is not an instance of ndarray , it is first converted to a numpy array using the array() function. Parameters: Name Type Description Default self object The Kriging object. required x np.ndarray or array-like The input array to check for the presence of value v . required v scalar The value to check for in the input array x . required Returns: Name Type Description bool bool True if any element in x is equal to v , False otherwise. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> instance = MyClass () >>> result = instance . __is_any__ ( x , v ) >>> print ( result ) Source code in spotPython/build/kriging.py 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 def __is_any__ ( self , x : Union [ np . ndarray , Any ], v : Any ) -> bool : \"\"\" Check if any element in `x` is equal to `v`. This method checks if any element in the input array `x` is equal to the value `v`. If `x` is not an instance of `ndarray`, it is first converted to a numpy array using the `array()` function. Args: self (object): The Kriging object. x (np.ndarray or array-like): The input array to check for the presence of value `v`. v (scalar): The value to check for in the input array `x`. Returns: bool: True if any element in `x` is equal to `v`, False otherwise. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> result = instance.__is_any__(x, v) >>> print(result) \"\"\" if not isinstance ( x , ndarray ): x = array ([ x ]) return any ( x == v )","title":"__is_any__()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.build_Psi","text":"Constructs a new (n x n) correlation matrix Psi to reflect new data or a change in hyperparameters. This method uses theta , p , and coded X values to construct the correlation matrix as described in [Forr08a, p.57]. Parameters: Name Type Description Default self object The Kriging object. required Returns: Type Description None None Raises: Type Description LinAlgError If building Psi fails. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> obj = MyClass () >>> obj . build_Psi () Source code in spotPython/build/kriging.py 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 def build_Psi ( self ) -> None : \"\"\" Constructs a new (n x n) correlation matrix Psi to reflect new data or a change in hyperparameters. This method uses `theta`, `p`, and coded `X` values to construct the correlation matrix as described in [Forr08a, p.57]. Args: self (object): The Kriging object. Returns: None Raises: LinAlgError: If building Psi fails. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_Psi() \"\"\" self . Psi = zeros (( self . n , self . n ), dtype = float64 ) theta = power ( 10.0 , self . theta ) if self . n_theta == 1 : theta = theta * ones ( self . k ) try : D = zeros (( self . n , self . n )) if self . ordered_mask . any (): X_ordered = self . cod_X [:, self . ordered_mask ] D = squareform ( pdist ( X_ordered , metric = 'sqeuclidean' , out = None , w = theta [ self . ordered_mask ])) if self . factor_mask . any (): X_factor = self . cod_X [:, self . factor_mask ] D = ( D + squareform ( pdist ( X_factor , metric = 'hamming' , out = None , w = theta [ self . factor_mask ]))) self . Psi = exp ( - D ) except LinAlgError as err : print ( f \"Building Psi failed: \\n { self . Psi } . { err =} , { type ( err ) =} \" ) if self . noise : self . Psi [ diag_indices_from ( self . Psi )] += self . Lambda else : self . Psi [ diag_indices_from ( self . Psi )] += self . eps if ( isinf ( self . Psi )) . any (): self . inf_Psi = True self . cnd_Psi = cond ( self . Psi )","title":"build_Psi()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.build_U","text":"Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57]. This method uses either scipy_cholesky or numpy\u2019s cholesky to perform the Cholesky factorization of Psi. Parameters: Name Type Description Default self object The Kriging object. required scipy bool If True, use scipy_cholesky . If False, use numpy\u2019s cholesky . Defaults to True. True Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> obj = MyClass () >>> obj . build_U () Source code in spotPython/build/kriging.py 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 def build_U ( self , scipy : bool = True ) -> None : \"\"\" Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57]. This method uses either `scipy_cholesky` or numpy's `cholesky` to perform the Cholesky factorization of Psi. Args: self (object): The Kriging object. scipy (bool): If True, use `scipy_cholesky`. If False, use numpy's `cholesky`. Defaults to True. Returns: None Raises: LinAlgError: If Cholesky factorization fails for Psi. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_U() \"\"\" try : self . U = scipy_cholesky ( self . Psi , lower = True ) if scipy else cholesky ( self . Psi ) self . U = self . U . T except LinAlgError as err : print ( f \"build_U() Cholesky failed for Psi: \\n { self . Psi } . { err =} , { type ( err ) =} \" )","title":"build_U()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.build_psi_vec","text":"Build the psi vector. Needed by predict_cod , predict_err_coded , regression_predict_coded . Modifies self.psi . Parameters: Name Type Description Default self object The Kriging object. required cod_x ndarray point to calculate psi required Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> cod_x = array ([ 0.3 , 0.3 ]) >>> build_psi_vec ( cod_x ) Source code in spotPython/build/kriging.py 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 def build_psi_vec ( self , cod_x : ndarray ) -> None : \"\"\" Build the psi vector. Needed by `predict_cod`, `predict_err_coded`, `regression_predict_coded`. Modifies `self.psi`. Args: self (object): The Kriging object. cod_x (ndarray): point to calculate psi Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> build_psi_vec(cod_x) \"\"\" self . psi = zeros (( self . n )) # theta = self.theta # TODO: theta = power ( 10.0 , self . theta ) if self . n_theta == 1 : theta = theta * ones ( self . k ) try : D = zeros (( self . n )) if self . ordered_mask . any (): X_ordered = self . cod_X [:, self . ordered_mask ] x_ordered = cod_x [ self . ordered_mask ] D = cdist ( x_ordered . reshape ( - 1 , sum ( self . ordered_mask )), X_ordered . reshape ( - 1 , sum ( self . ordered_mask )), metric = 'sqeuclidean' , out = None , w = theta [ self . ordered_mask ]) if self . factor_mask . any (): X_factor = self . cod_X [:, self . factor_mask ] x_factor = cod_x [ self . factor_mask ] D = ( D + cdist ( x_factor . reshape ( - 1 , sum ( self . factor_mask )), X_factor . reshape ( - 1 , sum ( self . factor_mask )), metric = 'hamming' , out = None , w = theta [ self . factor_mask ])) self . psi = exp ( - D ) . T except LinAlgError as err : print ( f \"Building psi failed: \\n { self . psi } . { err =} , { type ( err ) =} \" )","title":"build_psi_vec()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.calculate_mean_MSE","text":"Calculates the mean MSE metric of the model by evaluating MSE at a number of points. Parameters: Name Type Description Default self object The Kriging object. required n_samples int Number of points to sample the mean squared error at. Ignored if the points argument is specified. 200 points np . ndarray An array of points to sample the model at. None Returns: Name Type Description mean_MSE float The mean value of MSE. std_MSE float The standard deviation of the MSE points. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> n_samples = 200 >>> mean_MSE , std_MSE = k . calculate_mean_MSE ( n_samples ) >>> print ( f \"Mean MSE: { mean_MSE } , Standard deviation of MSE: { std_MSE } \" ) Source code in spotPython/build/kriging.py 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 def calculate_mean_MSE ( self , n_samples : int = 200 , points : Optional [ np . ndarray ] = None ) -> Tuple [ float , float ]: \"\"\" Calculates the mean MSE metric of the model by evaluating MSE at a number of points. Args: self (object): The Kriging object. n_samples (int): Number of points to sample the mean squared error at. Ignored if the points argument is specified. points (np.ndarray): An array of points to sample the model at. Returns: mean_MSE (float): The mean value of MSE. std_MSE (float): The standard deviation of the MSE points. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> n_samples = 200 >>> mean_MSE, std_MSE = k.calculate_mean_MSE(n_samples) >>> print(f\"Mean MSE: {mean_MSE}, Standard deviation of MSE: {std_MSE}\") \"\"\" if points is None : points = self . gen . lhd ( n_samples ) values = [ self . predict ( cod_X = point , nat = True , return_val = \"s\" ) for point in points ] return mean ( values ), std ( values )","title":"calculate_mean_MSE()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.cod_to_nat_x","text":"Converts an array representing one point in normalized (coded) units to natural (physical or real world) units. Parameters: Name Type Description Default self object The Kriging object. required cod_X np . ndarray An array representing one point (self.k long) in normalized (coded) units. required Returns: Name Type Description X np . ndarray An array of natural (physical or real world) units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> cod_X = array ([ 0.3 , 0.3 ]) >>> nat_X = k . cod_to_nat_x ( cod_X ) >>> print ( f \"Natural units: { nat_X } \" ) Source code in spotPython/build/kriging.py 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 def cod_to_nat_x ( self , cod_X : np . ndarray ) -> np . ndarray : \"\"\" Converts an array representing one point in normalized (coded) units to natural (physical or real world) units. Args: self (object): The Kriging object. cod_X (np.ndarray): An array representing one point (self.k long) in normalized (coded) units. Returns: X (np.ndarray): An array of natural (physical or real world) units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_X = array([0.3, 0.3]) >>> nat_X = k.cod_to_nat_x(cod_X) >>> print(f\"Natural units: {nat_X}\") \"\"\" X = copy . deepcopy ( cod_X ) if self . cod_type == \"norm\" : for i in range ( self . k ): X [ i ] = ( X [ i ] * float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ]) ) + self . nat_range_X [ i ][ 0 ] return X elif self . cod_type == \"std\" : for i in range ( self . k ): X [ i ] = X [ i ] * self . nat_std_X [ i ] + self . nat_mean_X [ i ] return X else : return cod_X","title":"cod_to_nat_x()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.cod_to_nat_y","text":"Converts a normalized array of coded (model) units in the range of [0,1] to an array of observed values in real-world units. Parameters: Name Type Description Default self object The Kriging object. required cod_y np . ndarray A normalized array of coded (model) units in the range of [0,1]. required Returns: Name Type Description y np . ndarray An array of observed values in real-world units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> cod_y = array ([ 0.5 , 0.5 ]) >>> nat_y = k . cod_to_nat_y ( cod_y ) >>> print ( f \"Real-world units: { nat_y } \" ) Source code in spotPython/build/kriging.py 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 def cod_to_nat_y ( self , cod_y : np . ndarray ) -> np . ndarray : \"\"\" Converts a normalized array of coded (model) units in the range of [0,1] to an array of observed values in real-world units. Args: self (object): The Kriging object. cod_y (np.ndarray): A normalized array of coded (model) units in the range of [0,1]. Returns: y (np.ndarray): An array of observed values in real-world units. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_y = array([0.5, 0.5]) >>> nat_y = k.cod_to_nat_y(cod_y) >>> print(f\"Real-world units: {nat_y}\") \"\"\" return ( cod_y * ( self . nat_range_y [ 1 ] - self . nat_range_y [ 0 ]) + self . nat_range_y [ 0 ] if self . cod_type == \"norm\" else cod_y * self . nat_std_y + self . nat_mean_y if self . cod_type == \"std\" else cod_y )","title":"cod_to_nat_y()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.exp_imp","text":"Calculates the expected improvement for a given function value and error in coded units. Parameters: Name Type Description Default self object The Kriging object. required y0 float The function value in coded units. required s0 float The error value. required Returns: Name Type Description float float The expected improvement value. Examples: >>> from spotPython.build.kriging import Kriging >>> S = Kriging ( name = 'kriging' , seed = 124 ) >>> S . cod_y = [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] >>> S . mean_cod_y = [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] >>> S . exp_imp ( 1.0 , 2.0 ) 0.0 Source code in spotPython/build/kriging.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def exp_imp ( self , y0 : float , s0 : float ) -> float : \"\"\" Calculates the expected improvement for a given function value and error in coded units. Args: self (object): The Kriging object. y0 (float): The function value in coded units. s0 (float): The error value. Returns: float: The expected improvement value. Examples: >>> from spotPython.build.kriging import Kriging >>> S = Kriging(name='kriging', seed=124) >>> S.cod_y = [0.0, 0.0, 0.0, 0.0, 0.0] >>> S.mean_cod_y = [0.0, 0.0, 0.0, 0.0, 0.0] >>> S.exp_imp(1.0, 2.0) 0.0 \"\"\" # y_min = min(self.cod_y) y_min = min ( self . mean_cod_y ) if s0 <= 0.0 : EI = 0.0 elif s0 > 0.0 : EI_one = ( y_min - y0 ) * ( 0.5 + 0.5 * erf (( 1.0 / sqrt ( 2.0 )) * (( y_min - y0 ) / s0 )) ) EI_two = ( s0 * ( 1.0 / sqrt ( 2.0 * pi ))) * ( exp ( - ( 1.0 / 2.0 ) * (( y_min - y0 ) ** 2.0 / s0 ** 2.0 )) ) EI = EI_one + EI_two return EI","title":"exp_imp()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.extract_from_bounds","text":"Extract theta , p , and Lambda from bounds. The kriging object stores theta as an array, p as an array, and Lambda as a float. Parameters: Name Type Description Default self object The Kriging object. required new_theta_p_Lambda np . ndarray 1d-array with theta, p, and Lambda values. Order is important. required Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging ( name = 'kriging' , seed = 124 ) >>> obj = MyClass () >>> obj . extract_from_bounds ( np . array ([ 1 , 2 , 3 ])) >>> print ( obj . theta ) [1] >>> print ( obj . p ) [2] >>> print ( obj . Lambda ) 3 Returns: Type Description None None Source code in spotPython/build/kriging.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 def extract_from_bounds ( self , new_theta_p_Lambda : np . ndarray ) -> None : \"\"\" Extract `theta`, `p`, and `Lambda` from bounds. The kriging object stores `theta` as an array, `p` as an array, and `Lambda` as a float. Args: self (object): The Kriging object. new_theta_p_Lambda (np.ndarray): 1d-array with theta, p, and Lambda values. Order is important. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.extract_from_bounds(np.array([1, 2, 3])) >>> print(obj.theta) [1] >>> print(obj.p) [2] >>> print(obj.Lambda) 3 Returns: None \"\"\" self . theta = new_theta_p_Lambda [: self . n_theta ] if self . optim_p : self . p = new_theta_p_Lambda [ self . n_theta : self . n_theta + self . n_p ] if self . noise : self . Lambda = new_theta_p_Lambda [ self . n_theta + self . n_p ] else : if self . noise : self . Lambda = new_theta_p_Lambda [ self . n_theta ]","title":"extract_from_bounds()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.fit","text":"Fits the hyperparameters ( theta , p , Lambda ) of the Kriging model. The function computes the following internal values: 1. theta , p , and Lambda values via optimization of the function fun_likelihood() . 2. Correlation matrix Psi via rebuildPsi() . Parameters: Name Type Description Default self object The Kriging object. required nat_X np . ndarray Sample points. required nat_y np . ndarray Function values. required Returns: Name Type Description object object Fitted estimator. Attributes: Name Type Description theta np . ndarray Kriging theta values. Shape (k,). p np . ndarray Kriging p values. Shape (k,). LnDetPsi np . float64 Determinant Psi matrix. Psi np . matrix Correlation matrix Psi. Shape (n,n). psi np . ndarray psi vector. Shape (n,). one np . ndarray vector of ones. Shape (n,). mu np . float64 Kriging expected mean value mu. U np . matrix Kriging U matrix, Cholesky decomposition. Shape (n,n). SigmaSqr np . float64 Sigma squared value. Lambda float lambda noise value. Examples: >>> from spotPython.build.kriging import Kriging >>> nat_X = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> nat_y = np . array ([ 1 , 2 ]) >>> surrogate = Kriging () >>> surrogate . fit ( nat_X , nat_y ) Source code in spotPython/build/kriging.py 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def fit ( self , nat_X : np . ndarray , nat_y : np . ndarray ) -> object : \"\"\" Fits the hyperparameters (`theta`, `p`, `Lambda`) of the Kriging model. The function computes the following internal values: 1. `theta`, `p`, and `Lambda` values via optimization of the function `fun_likelihood()`. 2. Correlation matrix `Psi` via `rebuildPsi()`. Args: self (object): The Kriging object. nat_X (np.ndarray): Sample points. nat_y (np.ndarray): Function values. Returns: object: Fitted estimator. Attributes: theta (np.ndarray): Kriging theta values. Shape (k,). p (np.ndarray): Kriging p values. Shape (k,). LnDetPsi (np.float64): Determinant Psi matrix. Psi (np.matrix): Correlation matrix Psi. Shape (n,n). psi (np.ndarray): psi vector. Shape (n,). one (np.ndarray): vector of ones. Shape (n,). mu (np.float64): Kriging expected mean value mu. U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n). SigmaSqr (np.float64): Sigma squared value. Lambda (float): lambda noise value. Examples: >>> from spotPython.build.kriging import Kriging >>> nat_X = np.array([[1, 2], [3, 4]]) >>> nat_y = np.array([1, 2]) >>> surrogate = Kriging() >>> surrogate.fit(nat_X, nat_y) \"\"\" self . initialize_variables ( nat_X , nat_y ) self . set_variable_types () self . nat_to_cod_init () self . set_theta_values () self . initialize_matrices () # build_Psi() and build_U() are called in fun_likelihood self . set_de_bounds () # Finally, set new theta and p values and update the surrogate again # for new_theta_p_Lambda in de_results[\"x\"]: new_theta_p_Lambda = self . optimize_model () self . extract_from_bounds ( new_theta_p_Lambda ) self . build_Psi () self . build_U () # TODO: check if the following line is necessary! self . likelihood () self . update_log ()","title":"fit()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.fun_likelihood","text":"Compute log likelihood for a set of hyperparameters (theta, p, Lambda). This method computes the log likelihood for a set of hyperparameters (theta, p, Lambda) by performing the following steps: 1. Extracts the hyperparameters from the input array using extract_from_bounds() . 2. Checks if any element in 10^theta is equal to 0. If so, logs a warning and returns the penalty value ( pen_val ). 3. Builds the Psi matrix using build_Psi() . 4. Checks if Psi is ill-conditioned or infinite. If so, logs a warning and returns the penalty value ( pen_val ). 5. Builds the U matrix using build_U() . If an exception occurs, logs an error and returns the penalty value ( pen_val ). 6. Computes the negative log likelihood using likelihood() . 7. Returns the computed negative log likelihood ( negLnLike ). Parameters: Name Type Description Default self object The Kriging object. required new_theta_p_Lambda np . ndarray An array containing the theta , p , and Lambda values. required Returns: Name Type Description float float The negative log likelihood of the surface at the specified hyperparameters. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> instance = MyClass () >>> negLnLike = instance . fun_likelihood ( new_theta_p_Lambda ) >>> print ( negLnLike ) Source code in spotPython/build/kriging.py 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 def fun_likelihood ( self , new_theta_p_Lambda : np . ndarray ) -> float : \"\"\" Compute log likelihood for a set of hyperparameters (theta, p, Lambda). This method computes the log likelihood for a set of hyperparameters (theta, p, Lambda) by performing the following steps: 1. Extracts the hyperparameters from the input array using `extract_from_bounds()`. 2. Checks if any element in `10^theta` is equal to 0. If so, logs a warning and returns the penalty value (`pen_val`). 3. Builds the `Psi` matrix using `build_Psi()`. 4. Checks if `Psi` is ill-conditioned or infinite. If so, logs a warning and returns the penalty value (`pen_val`). 5. Builds the `U` matrix using `build_U()`. If an exception occurs, logs an error and returns the penalty value (`pen_val`). 6. Computes the negative log likelihood using `likelihood()`. 7. Returns the computed negative log likelihood (`negLnLike`). Args: self (object): The Kriging object. new_theta_p_Lambda (np.ndarray): An array containing the `theta`, `p`, and `Lambda` values. Returns: float: The negative log likelihood of the surface at the specified hyperparameters. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> negLnLike = instance.fun_likelihood(new_theta_p_Lambda) >>> print(negLnLike) \"\"\" self . extract_from_bounds ( new_theta_p_Lambda ) if self . __is_any__ ( power ( 10.0 , self . theta ), 0 ): logger . warning ( \"Failure in fun_likelihood: 10^theta == 0. Setting negLnLike to %s \" , self . pen_val ) return self . pen_val self . build_Psi () if ( self . inf_Psi or self . cnd_Psi > 1e9 ): logger . warning ( \"Failure in fun_likelihood: Psi is ill conditioned: %s \" , self . cnd_Psi ) logger . warning ( \"Setting negLnLike to: %s \" , self . pen_val ) return self . pen_val try : self . build_U () except Exception as error : penalty_value = self . pen_val print ( \"Error in fun_likelihood(). Call to build_U() failed.\" ) print ( \"error= %s , type(error)= %s \" % ( error , type ( error ))) print ( \"Setting negLnLike to %.2f .\" % self . pen_val ) return penalty_value self . likelihood () return self . negLnLike","title":"fun_likelihood()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.initialize_matrices","text":"Initialize the matrices for the class instance. This method initializes several matrices and attributes for the class instance. The p attribute is initialized as a list of ones with length n_p , multiplied by 2.0. The pen_val attribute is initialized as the natural logarithm of the variance of nat_y , multiplied by n , plus 1e4. The negLnLike , LnDetPsi , mu , U , SigmaSqr , and Lambda attributes are all set to None. The gen attribute is initialized using the spacefilling function with arguments k and seed . The Psi attribute is initialized as a zero matrix with shape (n, n) and dtype float64 . The psi attribute is initialized as a zero matrix with shape (n, 1) . The one attribute is initialized as a list of ones with length n . Parameters: Name Type Description Default self object The Kriging object. required Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> instance = MyClass () >>> instance . initialize_matrices () Returns: Type Description None None Source code in spotPython/build/kriging.py 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 def initialize_matrices ( self ) -> None : \"\"\" Initialize the matrices for the class instance. This method initializes several matrices and attributes for the class instance. The `p` attribute is initialized as a list of ones with length `n_p`, multiplied by 2.0. The `pen_val` attribute is initialized as the natural logarithm of the variance of `nat_y`, multiplied by `n`, plus 1e4. The `negLnLike`, `LnDetPsi`, `mu`, `U`, `SigmaSqr`, and `Lambda` attributes are all set to None. The `gen` attribute is initialized using the `spacefilling` function with arguments `k` and `seed`. The `Psi` attribute is initialized as a zero matrix with shape `(n, n)` and dtype `float64`. The `psi` attribute is initialized as a zero matrix with shape `(n, 1)`. The `one` attribute is initialized as a list of ones with length `n`. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> instance = MyClass() >>> instance.initialize_matrices() Returns: None \"\"\" self . p = ones ( self . n_p ) * 2.0 self . pen_val = self . n * log ( var ( self . nat_y )) + 1e4 self . negLnLike = None self . gen = spacefilling ( k = self . k , seed = self . seed ) self . LnDetPsi = None self . Psi = zeros (( self . n , self . n ), dtype = float64 ) self . psi = zeros (( self . n , 1 )) self . one = ones ( self . n ) self . mu = None self . U = None self . SigmaSqr = None self . Lambda = None","title":"initialize_matrices()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.initialize_variables","text":"Initialize variables for the class instance. This method takes in the independent and dependent variable data as input and initializes the class instance variables. It creates deep copies of the input data and stores them in the instance variables nat_X and nat_y . It also calculates the number of observations n and the number of independent variables k from the shape of nat_X . Finally, it creates empty arrays with the same shape as nat_X and nat_y and stores them in the instance variables cod_X and cod_y . Parameters: Name Type Description Default self object The Kriging object. required nat_X np . ndarray The independent variable data. required nat_y np . ndarray The dependent variable data. required Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> surrogate = Kriging () >>> nat_X = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> nat_y = np . array ([ 1 , 2 ]) >>> surrogate . initialize_variables ( nat_X , nat_y ) >>> surrogate . nat_X array([[1, 2], [3, 4]]) >>> surrogate . nat_y array([1, 2]) Source code in spotPython/build/kriging.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 def initialize_variables ( self , nat_X : np . ndarray , nat_y : np . ndarray ) -> None : \"\"\" Initialize variables for the class instance. This method takes in the independent and dependent variable data as input and initializes the class instance variables. It creates deep copies of the input data and stores them in the instance variables `nat_X` and `nat_y`. It also calculates the number of observations `n` and the number of independent variables `k` from the shape of `nat_X`. Finally, it creates empty arrays with the same shape as `nat_X` and `nat_y` and stores them in the instance variables `cod_X` and `cod_y`. Args: self (object): The Kriging object. nat_X (np.ndarray): The independent variable data. nat_y (np.ndarray): The dependent variable data. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> surrogate = Kriging() >>> nat_X = np.array([[1, 2], [3, 4]]) >>> nat_y = np.array([1, 2]) >>> surrogate.initialize_variables(nat_X, nat_y) >>> surrogate.nat_X array([[1, 2], [3, 4]]) >>> surrogate.nat_y array([1, 2]) \"\"\" self . nat_X = copy . deepcopy ( nat_X ) self . nat_y = copy . deepcopy ( nat_y ) self . n = self . nat_X . shape [ 0 ] self . k = self . nat_X . shape [ 1 ] self . cod_X = np . empty_like ( self . nat_X ) self . cod_y = np . empty_like ( self . nat_y )","title":"initialize_variables()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.likelihood","text":"Calculates the negative of the concentrated log-likelihood. This method implements equation (2.32) in [Forr08a] to calculate the negative of the concentrated log-likelihood. It also modifies mu , SigmaSqr , LnDetPsi , and negLnLike . Note build_Psi and build_U should be called first. Parameters: Name Type Description Default self object The Kriging object. required Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> obj = MyClass () >>> obj . build_Psi () >>> obj . build_U () >>> obj . likelihood () Source code in spotPython/build/kriging.py 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 def likelihood ( self ) -> None : \"\"\" Calculates the negative of the concentrated log-likelihood. This method implements equation (2.32) in [Forr08a] to calculate the negative of the concentrated log-likelihood. It also modifies `mu`, `SigmaSqr`, `LnDetPsi`, and `negLnLike`. Note: `build_Psi` and `build_U` should be called first. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> obj = MyClass() >>> obj.build_Psi() >>> obj.build_U() >>> obj.likelihood() \"\"\" # (2.20) in [Forr08a]: U_T_inv_one = solve ( self . U . T , self . one ) U_T_inv_cod_y = solve ( self . U . T , self . cod_y ) mu = self . one . T . dot ( solve ( self . U , U_T_inv_cod_y )) / self . one . T . dot ( solve ( self . U , U_T_inv_one )) self . mu = mu # (2.31) in [Forr08a] cod_y_minus_mu = self . cod_y - self . one . dot ( self . mu ) self . SigmaSqr = cod_y_minus_mu . T . dot ( solve ( self . U , solve ( self . U . T , cod_y_minus_mu ))) / self . n # (2.32) in [Forr08a] self . LnDetPsi = 2.0 * sum ( log ( abs ( diag ( self . U )))) self . negLnLike = - 1.0 * ( - ( self . n / 2.0 ) * log ( self . SigmaSqr ) - 0.5 * self . LnDetPsi )","title":"likelihood()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.nat_to_cod_init","text":"Determines max and min of each dimension and normalizes that axis to a range of [0,1]. Called when 1) surrogate is initialized and 2) new points arrive, i.e., suggested by the surrogate as infill points. This method calls nat_to_cod_x and nat_to_cod_y and updates the ranges nat_range_X and nat_range_y . Parameters: Name Type Description Default self object The Kriging object. required Examples: >>> from spotPython.build.kriging import Kriging >>> kriging = Kriging () >>> kriging . nat_to_cod_init () Source code in spotPython/build/kriging.py 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 def nat_to_cod_init ( self ) -> None : \"\"\" Determines max and min of each dimension and normalizes that axis to a range of [0,1]. Called when 1) surrogate is initialized and 2) new points arrive, i.e., suggested by the surrogate as infill points. This method calls `nat_to_cod_x` and `nat_to_cod_y` and updates the ranges `nat_range_X` and `nat_range_y`. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> kriging = Kriging() >>> kriging.nat_to_cod_init() \"\"\" self . nat_range_X = [] self . nat_range_y = [] for i in range ( self . k ): self . nat_range_X . append ([ min ( self . nat_X [:, i ]), max ( self . nat_X [:, i ])]) self . nat_range_y . append ( min ( self . nat_y )) self . nat_range_y . append ( max ( self . nat_y )) self . nat_mean_X = mean ( self . nat_X , axis = 0 ) self . nat_std_X = std ( self . nat_X , axis = 0 ) self . nat_mean_y = mean ( self . nat_y ) self . nat_std_y = std ( self . nat_y ) Z = aggregate_mean_var ( X = self . nat_X , y = self . nat_y ) mu = Z [ 1 ] self . mean_cod_y = empty_like ( mu ) for i in range ( self . n ): self . cod_X [ i ] = self . nat_to_cod_x ( self . nat_X [ i ]) for i in range ( self . n ): self . cod_y [ i ] = self . nat_to_cod_y ( self . nat_y [ i ]) for i in range ( mu . shape [ 0 ]): self . mean_cod_y [ i ] = self . nat_to_cod_y ( mu [ i ])","title":"nat_to_cod_init()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.nat_to_cod_x","text":"Normalizes one point (row) of nat_X array to [0,1]. The internal nat_range_X values are not updated. Parameters: Name Type Description Default self object The Kriging object. required nat_X np . ndarray An array representing one point (self.k long) in natural (physical or real world) units. required Returns: Name Type Description X np . ndarray An array of coded values in the range of [0,1] for each dimension. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> nat_X = array ([ 5.0 , 5.0 ]) >>> cod_X = k . nat_to_cod_x ( nat_X ) >>> print ( f \"Coded values: { cod_X } \" ) Source code in spotPython/build/kriging.py 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 def nat_to_cod_x ( self , nat_X : np . ndarray ) -> np . ndarray : \"\"\" Normalizes one point (row) of nat_X array to [0,1]. The internal nat_range_X values are not updated. Args: self (object): The Kriging object. nat_X (np.ndarray): An array representing one point (self.k long) in natural (physical or real world) units. Returns: X (np.ndarray): An array of coded values in the range of [0,1] for each dimension. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> nat_X = array([5.0, 5.0]) >>> cod_X = k.nat_to_cod_x(nat_X) >>> print(f\"Coded values: {cod_X}\") \"\"\" X = copy . deepcopy ( nat_X ) if self . cod_type == \"norm\" : for i in range ( self . k ): # TODO: Check Implementation of range correction if range == 0: # rangex <- xmax - xmin # rangey <- ymax - ymin # xmin[rangex == 0] <- xmin[rangex == 0] - 0.5 # xmax[rangex == 0] <- xmax[rangex == 0] + 0.5 # rangex[rangex == 0] <- 1 # logger.debug(f\"self.nat_range_X[{i}]:\\n {self.nat_range_X[i]}\") # logger.debug(f\"X[{i}]:\\n {X[i]}\") rangex = float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ]) if rangex == 0 : self . nat_range_X [ i ][ 0 ] = self . nat_range_X [ i ][ 0 ] - 0.5 self . nat_range_X [ i ][ 1 ] = self . nat_range_X [ i ][ 1 ] + 0.5 X [ i ] = ( X [ i ] - self . nat_range_X [ i ][ 0 ]) / float ( self . nat_range_X [ i ][ 1 ] - self . nat_range_X [ i ][ 0 ] ) return X elif self . cod_type == \"std\" : for i in range ( self . k ): X [ i ] = ( X [ i ] - self . nat_mean_X [ i ]) / self . nat_std_X [ i ] return X else : return nat_X","title":"nat_to_cod_x()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.nat_to_cod_y","text":"Normalizes natural y values to [0,1]. Parameters: Name Type Description Default self object The Kriging object. required nat_y np . ndarray An array of observed values in natural (real-world) units. required Returns: Name Type Description y np . ndarray A normalized array of coded (model) units in the range of [0,1]. Examples: >>> from spotPython.build.kriging import Kriging >>> import numpy as np >>> kriging = Kriging () >>> nat_y = np . array ([ 5.0 , 5.0 ]) >>> cod_y = kriging . nat_to_cod_y ( nat_y ) >>> print ( f \"Coded values: { cod_y } \" ) Source code in spotPython/build/kriging.py 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 def nat_to_cod_y ( self , nat_y : np . ndarray ) -> np . ndarray : \"\"\" Normalizes natural y values to [0,1]. Args: self (object): The Kriging object. nat_y (np.ndarray): An array of observed values in natural (real-world) units. Returns: y (np.ndarray): A normalized array of coded (model) units in the range of [0,1]. Examples: >>> from spotPython.build.kriging import Kriging >>> import numpy as np >>> kriging = Kriging() >>> nat_y = np.array([5.0, 5.0]) >>> cod_y = kriging.nat_to_cod_y(nat_y) >>> print(f\"Coded values: {cod_y}\") \"\"\" return ( ( nat_y - self . nat_range_y [ 0 ]) / ( self . nat_range_y [ 1 ] - self . nat_range_y [ 0 ]) if self . use_cod_y and self . cod_type == \"norm\" else ( nat_y - self . nat_mean_y ) / self . nat_std_y if self . use_cod_y and self . cod_type == \"std\" else nat_y )","title":"nat_to_cod_y()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.optimize_model","text":"Optimize the model using the specified model_optimizer. This method uses the specified model_optimizer to optimize the likelihood function ( fun_likelihood ) with respect to the model parameters. The optimization is performed within the bounds specified by the attribute de_bounds . The result of the optimization is returned as a list or tuple of optimized parameter values. Parameters: Name Type Description Default self object The Kriging object. required Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging ( name = 'kriging' , seed = 124 ) >>> obj = MyClass () >>> result = obj . optimize_model () >>> print ( result ) [optimized_theta, optimized_p, optimized_Lambda] Returns: Type Description Union [ List [ float ], Tuple [ float ]] result[\u201cx\u201d] (Union[List[float], Tuple[float]]): A list or tuple of optimized parameter values. Source code in spotPython/build/kriging.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def optimize_model ( self ) -> Union [ List [ float ], Tuple [ float ]]: \"\"\" Optimize the model using the specified model_optimizer. This method uses the specified model_optimizer to optimize the likelihood function (`fun_likelihood`) with respect to the model parameters. The optimization is performed within the bounds specified by the attribute `de_bounds`. The result of the optimization is returned as a list or tuple of optimized parameter values. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> result = obj.optimize_model() >>> print(result) [optimized_theta, optimized_p, optimized_Lambda] Returns: result[\"x\"] (Union[List[float], Tuple[float]]): A list or tuple of optimized parameter values. \"\"\" if self . model_optimizer . __name__ == 'dual_annealing' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) elif self . model_optimizer . __name__ == 'differential_evolution' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds , maxiter = self . model_fun_evals , seed = self . seed ) elif self . model_optimizer . __name__ == 'direct' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds , # maxfun=self.model_fun_evals, eps = 1e-2 ) elif self . model_optimizer . __name__ == 'shgo' : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) elif self . model_optimizer . __name__ == 'basinhopping' : result = self . model_optimizer ( func = self . fun_likelihood , x0 = mean ( self . de_bounds , axis = 1 )) else : result = self . model_optimizer ( func = self . fun_likelihood , bounds = self . de_bounds ) return result [ \"x\" ]","title":"optimize_model()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.plot","text":"This function plots 1D and 2D surrogates. Parameters: Name Type Description Default self object The Kriging object. required show bool If True , the plots are displayed. If False , plt.show() should be called outside this function. True Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_p = 2 >>> self . n = 3 >>> self . nat_y = np . array ([ 1 , 2 , 3 ]) >>> self . k = 2 >>> self . seed = 1 >>> plot ( show = True ) Source code in spotPython/build/kriging.py 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 def plot ( self , show : Optional [ bool ] = True ) -> None : \"\"\" This function plots 1D and 2D surrogates. Args: self (object): The Kriging object. show (bool): If `True`, the plots are displayed. If `False`, `plt.show()` should be called outside this function. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_p = 2 >>> self.n = 3 >>> self.nat_y = np.array([1, 2, 3]) >>> self.k = 2 >>> self.seed = 1 >>> plot(show=True) \"\"\" if self . k == 1 : # TODO: Improve plot (add conf. interval etc.) fig = pylab . figure ( figsize = ( 9 , 6 )) # t1 = array(arange(0.0, 1.0, 0.01)) # y1 = array([self.predict(array([x]), return_val=\"y\") for x in t1]) # plt.figure() # plt.plot(t1, y1, \"k\") # if show: # plt.show() # n_grid = 100 x = linspace ( self . nat_range_X [ 0 ][ 0 ], self . nat_range_X [ 0 ][ 1 ], num = n_grid ) y = self . predict ( x ) plt . figure () plt . plot ( x , y , \"k\" ) if show : plt . show () if self . k == 2 : fig = pylab . figure ( figsize = ( 9 , 6 )) n_grid = 100 x = linspace ( self . nat_range_X [ 0 ][ 0 ], self . nat_range_X [ 0 ][ 1 ], num = n_grid ) y = linspace ( self . nat_range_X [ 1 ][ 0 ], self . nat_range_X [ 1 ][ 1 ], num = n_grid ) X , Y = meshgrid ( x , y ) # Predict based on the optimized results zz = array ( [ self . predict ( array ([ x , y ]), return_val = \"all\" ) for x , y in zip ( ravel ( X ), ravel ( Y ))] ) zs = zz [:, 0 , :] zse = zz [:, 1 , :] Z = zs . reshape ( X . shape ) Ze = zse . reshape ( X . shape ) if self . cod_type == \"norm\" : nat_point_X = ( self . cod_X [:, 0 ] * ( self . nat_range_X [ 0 ][ 1 ] - self . nat_range_X [ 0 ][ 0 ]) ) + self . nat_range_X [ 0 ][ 0 ] nat_point_Y = ( self . cod_X [:, 1 ] * ( self . nat_range_X [ 1 ][ 1 ] - self . nat_range_X [ 1 ][ 0 ]) ) + self . nat_range_X [ 1 ][ 0 ] elif self . cod_type == \"std\" : nat_point_X = self . cod_X [:, 0 ] * self . nat_std_X [ 0 ] + self . nat_mean_X [ 0 ] nat_point_Y = self . cod_X [:, 1 ] * self . nat_std_X [ 1 ] + self . nat_mean_X [ 1 ] else : nat_point_X = self . cod_X [:, 0 ] nat_point_Y = self . cod_X [:, 1 ] contour_levels = 30 ax = fig . add_subplot ( 224 ) # plot predicted values: pylab . contourf ( X , Y , Ze , contour_levels , cmap = \"jet\" ) pylab . title ( \"Error\" ) pylab . colorbar () # plot observed points: pylab . plot ( nat_point_X , nat_point_Y , \"ow\" ) # ax = fig . add_subplot ( 223 ) # plot predicted values: plt . contourf ( X , Y , Z , contour_levels , zorder = 1 , cmap = \"jet\" ) plt . title ( \"Surrogate\" ) # plot observed points: pylab . plot ( nat_point_X , nat_point_Y , \"ow\" , zorder = 3 ) pylab . colorbar () # ax = fig . add_subplot ( 221 , projection = \"3d\" ) ax . plot_surface ( X , Y , Z , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" ) # ax = fig . add_subplot ( 222 , projection = \"3d\" ) ax . plot_surface ( X , Y , Ze , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" ) # pylab . show ()","title":"plot()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.predict","text":"This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X. Parameters: Name Type Description Default self object The Kriging object. required nat_X ndarray Design variable to evaluate in natural units. required nat bool argument nat_X is in natural range. Default: True . If set to False , nat_X will not be normalized (which might be useful if already normalized y values are used). True return_val str whether y , s , neg. ei (negative expected improvement), or all three values are returned. Default is (for compatibility with sklearn) \u201cy\u201d. To return s , select \u201cs\u201d, to return neg. ei , select \u201cei\u201d. To return the tuple (y, s, ei) , select \u201call\u201d. 'y' Returns: Name Type Description float Union [ float , Tuple [ float , float , float ]] The predicted value in natural units if return_val is \u201cy\u201d. float Union [ float , Tuple [ float , float , float ]] predicted error if return_val is \u201cs\u201d. float Union [ float , Tuple [ float , float , float ]] expected improvement if return_val is \u201cei\u201d. Union [ float , Tuple [ float , float , float ]] Tuple[float, float, float]: The predicted value in natural units, predicted error and expected improvement if return_val is \u201call\u201d. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> k . predict ( array ([[ 0.3 , 0.3 ]])) array([0.09]) Source code in spotPython/build/kriging.py 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 def predict ( self , nat_X : ndarray , nat : bool = True , return_val : str = \"y\" ) -> Union [ float , Tuple [ float , float , float ]]: \"\"\" This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X. Args: self (object): The Kriging object. nat_X (ndarray): Design variable to evaluate in natural units. nat (bool): argument `nat_X` is in natural range. Default: `True`. If set to `False`, `nat_X` will not be normalized (which might be useful if already normalized y values are used). return_val (str): whether `y`, `s`, neg. `ei` (negative expected improvement), or all three values are returned. Default is (for compatibility with sklearn) \"y\". To return `s`, select \"s\", to return neg. `ei`, select \"ei\". To return the tuple `(y, s, ei)`, select \"all\". Returns: float: The predicted value in natural units if return_val is \"y\". float: predicted error if return_val is \"s\". float: expected improvement if return_val is \"ei\". Tuple[float, float, float]: The predicted value in natural units, predicted error and expected improvement if return_val is \"all\". Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> k.predict(array([[0.3, 0.3]])) array([0.09]) \"\"\" # Check for the shape and the type of the Input if isinstance ( nat_X , ndarray ): try : X = nat_X . reshape ( - 1 , self . nat_X . shape [ 1 ]) X = repair_non_numeric ( X , self . var_type ) except Exception : raise TypeError ( \"13.1: Input to predict was not convertible to the size of X\" ) else : raise TypeError ( f \"type of the given input is an { type ( nat_X ) } instead of an ndarray\" ) n = X . shape [ 0 ] y = empty ( n , dtype = float ) s = empty ( n , dtype = float ) ei = empty ( n , dtype = float ) for i in range ( n ): if nat : x = self . nat_to_cod_x ( X [ i , :]) else : x = X [ i , :] y [ i ], s [ i ], ei [ i ] = self . predict_coded ( x ) if return_val == \"y\" : return y elif return_val == \"s\" : return s elif return_val == \"ei\" : return - 1.0 * ei else : return y , s , - 1.0 * ei","title":"predict()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.predict_coded","text":"Kriging prediction of one point in the coded units as described in (2.20) in [Forr08a]. The error is returned as well. Parameters: Name Type Description Default self object The Kriging object. required cod_x np . ndarray Point in coded units to make prediction at. required Returns: Name Type Description f float Predicted value in coded units. SSqr float Predicted error. EI float Expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> cod_x = array ([ 0.3 , 0.3 ]) >>> k . predict_coded ( cod_x ) (0.09, 0.0, 0.0) Note self.mu and self.SigmaSqr are computed in likelihood , not here. See also [Forr08a, p.60]. Source code in spotPython/build/kriging.py 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 def predict_coded ( self , cod_x : np . ndarray ) -> Tuple [ float , float , float ]: \"\"\" Kriging prediction of one point in the coded units as described in (2.20) in [Forr08a]. The error is returned as well. Args: self (object): The Kriging object. cod_x (np.ndarray): Point in coded units to make prediction at. Returns: f (float): Predicted value in coded units. SSqr (float): Predicted error. EI (float): Expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> k.predict_coded(cod_x) (0.09, 0.0, 0.0) Note: `self.mu` and `self.SigmaSqr` are computed in `likelihood`, not here. See also [Forr08a, p.60]. \"\"\" self . build_psi_vec ( cod_x ) U_T_inv = solve ( self . U . T , self . cod_y - self . one . dot ( self . mu )) f = self . mu + self . psi . T . dot ( solve ( self . U , U_T_inv )) if self . noise : Lambda = self . Lambda else : Lambda = 0.0 # Error in [Forr08a, p.87]: SSqr = self . SigmaSqr * ( 1 + Lambda - self . psi . T . dot ( solve ( self . U , solve ( self . U . T , self . psi )))) SSqr = power ( abs ( SSqr [ 0 ]), 0.5 )[ 0 ] EI = self . exp_imp ( y0 = f [ 0 ], s0 = SSqr ) return f [ 0 ], SSqr , EI","title":"predict_coded()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.set_de_bounds","text":"Determine search bounds for model_optimizer, e.g., differential evolution. This method sets the attribute de_bounds of the object to a list of lists, where each inner list represents the lower and upper bounds for a parameter being optimized. The number of inner lists is determined by the number of parameters being optimized ( n_theta and n_p ), as well as whether noise is being considered ( noise ). Parameters: Name Type Description Default self object The Kriging object. required Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging ( name = 'kriging' , seed = 124 ) >>> obj = MyClass () >>> obj . set_de_bounds () >>> print ( obj . de_bounds ) [[min_theta, max_theta], [min_theta, max_theta], ..., [min_p, max_p], [min_Lambda, max_Lambda]] Returns: Type Description None None Source code in spotPython/build/kriging.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def set_de_bounds ( self ) -> None : \"\"\" Determine search bounds for model_optimizer, e.g., differential evolution. This method sets the attribute `de_bounds` of the object to a list of lists, where each inner list represents the lower and upper bounds for a parameter being optimized. The number of inner lists is determined by the number of parameters being optimized (`n_theta` and `n_p`), as well as whether noise is being considered (`noise`). Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.set_de_bounds() >>> print(obj.de_bounds) [[min_theta, max_theta], [min_theta, max_theta], ..., [min_p, max_p], [min_Lambda, max_Lambda]] Returns: None \"\"\" de_bounds = [[ self . min_theta , self . max_theta ] for _ in range ( self . n_theta )] if self . optim_p : de_bounds += [[ self . min_p , self . max_p ] for _ in range ( self . n_p )] if self . noise : de_bounds . append ([ self . min_Lambda , self . max_Lambda ]) else : if self . noise : de_bounds . append ([ self . min_Lambda , self . max_Lambda ]) self . de_bounds = de_bounds","title":"set_de_bounds()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.set_theta_values","text":"Set the theta values for the class instance. This method sets the theta values for the class instance based on the n_theta and k attributes. If n_theta is greater than k , n_theta is set to k and a warning is logged. The method then initializes the theta attribute as a list of zeros with length n_theta . The x0_theta attribute is also initialized as a list of ones with length n_theta , multiplied by n / (100 * k) . Parameters: Name Type Description Default self object The Kriging object. required Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . n_theta = 3 >>> self . k = 2 >>> instance = MyClass () >>> instance . set_theta_values () >>> instance . theta array([0., 0., 0.]) Source code in spotPython/build/kriging.py 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 def set_theta_values ( self ) -> None : \"\"\" Set the theta values for the class instance. This method sets the theta values for the class instance based on the `n_theta` and `k` attributes. If `n_theta` is greater than `k`, `n_theta` is set to `k` and a warning is logged. The method then initializes the `theta` attribute as a list of zeros with length `n_theta`. The `x0_theta` attribute is also initialized as a list of ones with length `n_theta`, multiplied by `n / (100 * k)`. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.n_theta = 3 >>> self.k = 2 >>> instance = MyClass() >>> instance.set_theta_values() >>> instance.theta array([0., 0., 0.]) \"\"\" if self . n_theta > self . k : self . n_theta = self . k logger . warning ( \"More theta values than dimensions. `n_theta` set to `k`.\" ) self . theta : List [ float ] = zeros ( self . n_theta ) # TODO: Currently not used: self . x0_theta : List [ float ] = ones (( self . n_theta ,)) * self . n / ( 100 * self . k )","title":"set_theta_values()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.set_variable_types","text":"Set the variable types for the class instance. This method sets the variable types for the class instance based on the var_type attribute. If the length of var_type is less than k , all variable types are forced to \u2018num\u2019 and a warning is logged. The method then creates masks for each variable type (\u2018num\u2019, \u2018factor\u2019, \u2018int\u2019, \u2018float\u2019) using numpy arrays. Parameters: Name Type Description Default self object The Kriging object. required Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass ( Kriging ): >>> def __init__ ( self ): >>> super () . __init__ () >>> self . var_type = [ \"num\" , \"factor\" ] >>> instance = MyClass () >>> instance . set_variable_types () >>> instance . num_mask array([ True, False]) Returns: Type Description None None Source code in spotPython/build/kriging.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 def set_variable_types ( self ) -> None : \"\"\" Set the variable types for the class instance. This method sets the variable types for the class instance based on the `var_type` attribute. If the length of `var_type` is less than `k`, all variable types are forced to 'num' and a warning is logged. The method then creates masks for each variable type ('num', 'factor', 'int', 'float') using numpy arrays. Args: self (object): The Kriging object. Examples: >>> from spotPython.build.kriging import Kriging >>> class MyClass(Kriging): >>> def __init__(self): >>> super().__init__() >>> self.var_type = [\"num\", \"factor\"] >>> instance = MyClass() >>> instance.set_variable_types() >>> instance.num_mask array([ True, False]) Returns: None \"\"\" # assume all variable types are \"num\" if \"num\" is # specified once: if len ( self . var_type ) < self . k : self . var_type = self . var_type * self . k logger . warning ( \"Warning: All variable types forced to 'num'.\" ) self . num_mask = np . array ( list ( map ( lambda x : x == \"num\" , self . var_type ))) self . factor_mask = np . array ( list ( map ( lambda x : x == \"factor\" , self . var_type ))) self . int_mask = np . array ( list ( map ( lambda x : x == \"int\" , self . var_type ))) self . ordered_mask = np . array ( list ( map ( lambda x : x == \"int\" or x == \"num\" or x == \"float\" , self . var_type )))","title":"set_variable_types()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.update_log","text":"Update the log with the current values of negLnLike, theta, p, and Lambda. This method appends the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to their respective lists in the log dictionary. It also updates the log_length attribute with the current length of the negLnLike list in the log. If spot_writer is not None, this method also writes the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to the spot_writer object. Parameters: Name Type Description Default self object The Kriging object. required Returns: Type Description None None Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging ( name = 'kriging' , seed = 124 ) >>> obj = MyClass () >>> obj . update_log () >>> print ( obj . log ) {'negLnLike': [0.5], 'theta': [0.1], 'p': [0.2], 'Lambda': [0.3]} Source code in spotPython/build/kriging.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def update_log ( self ) -> None : \"\"\" Update the log with the current values of negLnLike, theta, p, and Lambda. This method appends the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to their respective lists in the log dictionary. It also updates the log_length attribute with the current length of the negLnLike list in the log. If spot_writer is not None, this method also writes the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to the spot_writer object. Args: self (object): The Kriging object. Returns: None Examples: >>> from spotPython.build.kriging import Kriging >>> MyClass = Kriging(name='kriging', seed=124) >>> obj = MyClass() >>> obj.update_log() >>> print(obj.log) {'negLnLike': [0.5], 'theta': [0.1], 'p': [0.2], 'Lambda': [0.3]} \"\"\" self . log [ \"negLnLike\" ] = append ( self . log [ \"negLnLike\" ], self . negLnLike ) self . log [ \"theta\" ] = append ( self . log [ \"theta\" ], self . theta ) if self . optim_p : self . log [ \"p\" ] = append ( self . log [ \"p\" ], self . p ) if self . noise : self . log [ \"Lambda\" ] = append ( self . log [ \"Lambda\" ], self . Lambda ) # get the length of the log self . log_length = len ( self . log [ \"negLnLike\" ]) if self . spot_writer is not None : writer = self . spot_writer negLnLike = self . negLnLike . copy () writer . add_scalar ( \"spot_negLnLike\" , negLnLike , self . counter + self . log_length ) # add the self.n_theta theta values to the writer with one key \"theta\", # i.e, the same key for all theta values theta = self . theta . copy () writer . add_scalars ( \"spot_theta\" , { f \"theta_ { i } \" : theta [ i ] for i in range ( self . n_theta )}, self . counter + self . log_length ) if self . noise : Lambda = self . Lambda . copy () writer . add_scalar ( \"spot_Lambda\" , Lambda , self . counter + self . log_length ) if self . optim_p : p = self . p . copy () writer . add_scalars ( \"spot_p\" , { f \"p_ { i } \" : p [ i ] for i in range ( self . n_p )}, self . counter + self . log_length ) writer . flush ()","title":"update_log()"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.weighted_exp_imp","text":"Weighted expected improvement. Parameters: Name Type Description Default self object The Kriging object. required cod_x np . ndarray A coded design vector. required w float Weight. required Returns: Name Type Description EI float Weighted expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array ([[ 0.0 , 0.0 ], [ 0.1 , 0.1 ], [ 0.2 , 0.2 ]]) >>> y = array ([ 0.0 , 0.01 , 0.04 ]) >>> k = Kriging ( X , y ) >>> cod_x = array ([ 0.3 , 0.3 ]) >>> w = 0.5 >>> k . weighted_exp_imp ( cod_x , w ) 0.0 References [Sobester et al. 2005]. Source code in spotPython/build/kriging.py 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 def weighted_exp_imp ( self , cod_x : np . ndarray , w : float ) -> float : \"\"\" Weighted expected improvement. Args: self (object): The Kriging object. cod_x (np.ndarray): A coded design vector. w (float): Weight. Returns: EI (float): Weighted expected improvement. Examples: >>> from spotPython.build.kriging import Kriging >>> from numpy import array >>> X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]]) >>> y = array([0.0, 0.01, 0.04]) >>> k = Kriging(X, y) >>> cod_x = array([0.3, 0.3]) >>> w = 0.5 >>> k.weighted_exp_imp(cod_x, w) 0.0 References: [Sobester et al. 2005]. \"\"\" y0 , s0 = self . predict_coded ( cod_x ) y_min = min ( self . cod_y ) if s0 <= 0.0 : EI = 0.0 else : y_min_y0 = y_min - y0 EI_one = w * ( y_min_y0 * ( 0.5 + 0.5 * erf (( 1.0 / sqrt ( 2.0 )) * ( y_min_y0 / s0 ))) ) EI_two = ( ( 1.0 - w ) * ( s0 * ( 1.0 / sqrt ( 2.0 * pi ))) * ( exp ( - ( 1.0 / 2.0 ) * (( y_min_y0 ) ** 2.0 / s0 ** 2.0 ))) ) EI = EI_one + EI_two return EI","title":"weighted_exp_imp()"},{"location":"reference/spotPython/build/surrogates/","text":"surrogates \u00b6 Super class for all surrogate model classes (e.g., Kriging) Source code in spotPython/build/surrogates.py 4 5 6 7 8 9 10 11 12 13 class surrogates : \"\"\" Super class for all surrogate model classes (e.g., Kriging) \"\"\" def __init__ ( self , name = \"\" , seed = 123 , verbosity = 0 ): self . name = name self . seed = seed self . rng = default_rng ( self . seed ) self . log = {} self . verbosity = verbosity","title":"surrogates"},{"location":"reference/spotPython/build/surrogates/#spotPython.build.surrogates.surrogates","text":"Super class for all surrogate model classes (e.g., Kriging) Source code in spotPython/build/surrogates.py 4 5 6 7 8 9 10 11 12 13 class surrogates : \"\"\" Super class for all surrogate model classes (e.g., Kriging) \"\"\" def __init__ ( self , name = \"\" , seed = 123 , verbosity = 0 ): self . name = name self . seed = seed self . rng = default_rng ( self . seed ) self . log = {} self . verbosity = verbosity","title":"surrogates"},{"location":"reference/spotPython/data/","text":"Datasets. This module contains a collection of datasets for multiple tasks: classification, regression, etc. The data corresponds to popular datasets and are conveniently wrapped to easily iterate over the data in a stream fashion. All datasets have fixed size.","title":"data"},{"location":"reference/spotPython/data/base/","text":"Config \u00b6 Bases: abc . ABC Base class for all configurations. All configurations inherit from this class, be they stored in a file or generated on the fly. Attributes: Name Type Description desc str The description from the docstring. _repr_content dict The items that are displayed in the repr method. Source code in spotPython/data/base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 class Config ( abc . ABC ): \"\"\"Base class for all configurations. All configurations inherit from this class, be they stored in a file or generated on the fly. Attributes: desc (str): The description from the docstring. _repr_content (dict): The items that are displayed in the __repr__ method. \"\"\" def __init__ ( self ): \"\"\"Initialize a Config object.\"\"\" pass @property def desc ( self ) -> str : \"\"\"Return the description from the docstring. Returns: str: The description from the docstring. Examples: >>> class MyConfig(Config): ... '''My configuration class.''' ... pass >>> MyConfig().desc 'My configuration class.' \"\"\" desc = re . split ( pattern = r \"\\w+\\n\\s {4} \\-{3,}\" , string = self . __doc__ , maxsplit = 0 )[ 0 ] return inspect . cleandoc ( desc ) @property def _repr_content ( self ) -> dict : \"\"\"The items that are displayed in the __repr__ method. This property can be overridden in order to modify the output of the __repr__ method. Returns: dict: A dictionary containing the items to be displayed in the __repr__ method. Examples: >>> class MyConfig(Config): ... '''My configuration class.''' ... pass >>> MyConfig()._repr_content {'Name': 'MyConfig'} \"\"\" content = {} content [ \"Name\" ] = self . __class__ . __name__ return content __init__ () \u00b6 Initialize a Config object. Source code in spotPython/data/base.py 69 70 71 def __init__ ( self ): \"\"\"Initialize a Config object.\"\"\" pass desc () property \u00b6 Return the description from the docstring. Returns: Name Type Description str str The description from the docstring. Examples: >>> class MyConfig ( Config ): ... '''My configuration class.''' ... pass >>> MyConfig () . desc 'My configuration class.' Source code in spotPython/data/base.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 @property def desc ( self ) -> str : \"\"\"Return the description from the docstring. Returns: str: The description from the docstring. Examples: >>> class MyConfig(Config): ... '''My configuration class.''' ... pass >>> MyConfig().desc 'My configuration class.' \"\"\" desc = re . split ( pattern = r \"\\w+\\n\\s {4} \\-{3,}\" , string = self . __doc__ , maxsplit = 0 )[ 0 ] return inspect . cleandoc ( desc ) Dataset \u00b6 Bases: abc . ABC Base class for all datasets. All datasets inherit from this class, be they stored in a file or generated on the fly. Parameters: Name Type Description Default task str Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d required n_features int Number of features in the dataset. required n_samples int Number of samples in the dataset. None n_classes int Number of classes in the dataset, only applies to classification datasets. None n_outputs int Number of outputs the target is made of, only applies to multi-output datasets. None sparse bool Whether the dataset is sparse or not. False Attributes: Name Type Description desc str The description from the docstring. _repr_content dict The items that are displayed in the repr method. Source code in spotPython/data/base.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 class Dataset ( abc . ABC ): \"\"\"Base class for all datasets. All datasets inherit from this class, be they stored in a file or generated on the fly. Args: task (str): Type of task the dataset is meant for. Should be one of: - \"Regression\" - \"Binary classification\" - \"Multi-class classification\" - \"Multi-output binary classification\" - \"Multi-output regression\" n_features (int): Number of features in the dataset. n_samples (int, optional): Number of samples in the dataset. n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets. n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets. sparse (bool, optional): Whether the dataset is sparse or not. Attributes: desc (str): The description from the docstring. _repr_content (dict): The items that are displayed in the __repr__ method. \"\"\" def __init__ ( self , task : str , n_features : int , n_samples : Optional [ int ] = None , n_classes : Optional [ int ] = None , n_outputs : Optional [ int ] = None , sparse : bool = False , ): \"\"\"Initialize a Dataset object. Args: task (str): Type of task the dataset is meant for. Should be one of: - \"Regression\" - \"Binary classification\" - \"Multi-class classification\" - \"Multi-output binary classification\" - \"Multi-output regression\" n_features (int): Number of features in the dataset. n_samples (int, optional): Number of samples in the dataset. Defaults to None. n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets. Defaults to None. n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets. Defaults to None. sparse (bool, optional): Whether the dataset is sparse or not. Defaults to False. \"\"\" self . task = task self . n_features = n_features self . n_samples = n_samples self . n_outputs = n_outputs self . n_classes = n_classes self . sparse = sparse @abc . abstractmethod def __iter__ ( self ): \"\"\"Abstract method for iterating over samples in the dataset.\"\"\" raise NotImplementedError def take ( self , k : int ) -> itertools . islice : \"\"\"Iterate over the k samples. Args: k (int): The number of samples to iterate over. Returns: itertools.islice: An iterator over the first k samples in the dataset. Examples: >>> class MyDataset(Dataset): ... def __init__(self): ... super().__init__('Regression', 10) ... def __iter__(self): ... yield from range(10) >>> list(MyDataset().take(5)) [0, 1, 2, 3, 4] \"\"\" return itertools . islice ( self , k ) @property def desc ( self ) -> str : \"\"\"Return the description from the docstring. Returns: str: The description from the docstring. Examples: >>> class MyDataset(Dataset): ... '''My dataset class.''' ... def __init__(self): ... super().__init__('Regression', 10) ... def __iter__(self): ... yield from range(10) >>> MyDataset().desc 'My dataset class.' \"\"\" desc = re . split ( pattern = r \"\\w+\\n\\s {4} \\-{3,}\" , string = self . __doc__ , maxsplit = 0 )[ 0 ] return inspect . cleandoc ( desc ) @property def _repr_content ( self ) -> dict : \"\"\"The items that are displayed in the __repr__ method. This property can be overridden in order to modify the output of the __repr__ method. Returns: dict: A dictionary containing the items to be displayed in the __repr__ method. \"\"\" content = {} content [ \"Name\" ] = self . __class__ . __name__ content [ \"Task\" ] = self . task if isinstance ( self , SyntheticDataset ) and self . n_samples is None : content [ \"Samples\" ] = \"\u221e\" elif self . n_samples : content [ \"Samples\" ] = f \" { self . n_samples : , } \" if self . n_features : content [ \"Features\" ] = f \" { self . n_features : , } \" if self . n_outputs : content [ \"Outputs\" ] = f \" { self . n_outputs : , } \" if self . n_classes : content [ \"Classes\" ] = f \" { self . n_classes : , } \" content [ \"Sparse\" ] = str ( self . sparse ) return content def __repr__ ( self ): l_len = max ( map ( len , self . _repr_content . keys ())) r_len = max ( map ( len , self . _repr_content . values ())) out = f \" { self . desc } \\n\\n \" + \" \\n \" . join ( k . rjust ( l_len ) + \" \" + v . ljust ( r_len ) for k , v in self . _repr_content . items () ) if \"Parameters \\n ----------\" in self . __doc__ : params = re . split ( r \"\\w+\\n\\s {4} \\-{3,}\" , re . split ( \"Parameters \\n ----------\" , self . __doc__ )[ 1 ], )[ 0 ] . rstrip () out += f \" \\n\\n Parameters \\n ---------- { params } \" return out __init__ ( task , n_features , n_samples = None , n_classes = None , n_outputs = None , sparse = False ) \u00b6 Initialize a Dataset object. Parameters: Name Type Description Default task str Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d required n_features int Number of features in the dataset. required n_samples int Number of samples in the dataset. Defaults to None. None n_classes int Number of classes in the dataset, only applies to classification datasets. Defaults to None. None n_outputs int Number of outputs the target is made of, only applies to multi-output datasets. Defaults to None. None sparse bool Whether the dataset is sparse or not. Defaults to False. False Source code in spotPython/data/base.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def __init__ ( self , task : str , n_features : int , n_samples : Optional [ int ] = None , n_classes : Optional [ int ] = None , n_outputs : Optional [ int ] = None , sparse : bool = False , ): \"\"\"Initialize a Dataset object. Args: task (str): Type of task the dataset is meant for. Should be one of: - \"Regression\" - \"Binary classification\" - \"Multi-class classification\" - \"Multi-output binary classification\" - \"Multi-output regression\" n_features (int): Number of features in the dataset. n_samples (int, optional): Number of samples in the dataset. Defaults to None. n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets. Defaults to None. n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets. Defaults to None. sparse (bool, optional): Whether the dataset is sparse or not. Defaults to False. \"\"\" self . task = task self . n_features = n_features self . n_samples = n_samples self . n_outputs = n_outputs self . n_classes = n_classes self . sparse = sparse __iter__ () abstractmethod \u00b6 Abstract method for iterating over samples in the dataset. Source code in spotPython/data/base.py 167 168 169 170 @abc . abstractmethod def __iter__ ( self ): \"\"\"Abstract method for iterating over samples in the dataset.\"\"\" raise NotImplementedError desc () property \u00b6 Return the description from the docstring. Returns: Name Type Description str str The description from the docstring. Examples: >>> class MyDataset ( Dataset ): ... '''My dataset class.''' ... def __init__ ( self ): ... super () . __init__ ( 'Regression' , 10 ) ... def __iter__ ( self ): ... yield from range ( 10 ) >>> MyDataset () . desc 'My dataset class.' Source code in spotPython/data/base.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 @property def desc ( self ) -> str : \"\"\"Return the description from the docstring. Returns: str: The description from the docstring. Examples: >>> class MyDataset(Dataset): ... '''My dataset class.''' ... def __init__(self): ... super().__init__('Regression', 10) ... def __iter__(self): ... yield from range(10) >>> MyDataset().desc 'My dataset class.' \"\"\" desc = re . split ( pattern = r \"\\w+\\n\\s {4} \\-{3,}\" , string = self . __doc__ , maxsplit = 0 )[ 0 ] return inspect . cleandoc ( desc ) take ( k ) \u00b6 Iterate over the k samples. Parameters: Name Type Description Default k int The number of samples to iterate over. required Returns: Type Description itertools . islice itertools.islice: An iterator over the first k samples in the dataset. Examples: >>> class MyDataset ( Dataset ): ... def __init__ ( self ): ... super () . __init__ ( 'Regression' , 10 ) ... def __iter__ ( self ): ... yield from range ( 10 ) >>> list ( MyDataset () . take ( 5 )) [0, 1, 2, 3, 4] Source code in spotPython/data/base.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def take ( self , k : int ) -> itertools . islice : \"\"\"Iterate over the k samples. Args: k (int): The number of samples to iterate over. Returns: itertools.islice: An iterator over the first k samples in the dataset. Examples: >>> class MyDataset(Dataset): ... def __init__(self): ... super().__init__('Regression', 10) ... def __iter__(self): ... yield from range(10) >>> list(MyDataset().take(5)) [0, 1, 2, 3, 4] \"\"\" return itertools . islice ( self , k ) FileConfig \u00b6 Bases: Config Base class for configurations that are stored in a local file. Parameters: Name Type Description Default filename str The file\u2019s name. required directory Optional [ str ] The directory where the file is contained. Defaults to the location of the datasets module. None desc dict Extra config parameters to pass as keyword arguments. {} Returns: Type Description FileConfig A FileConfig object. Examples: >>> config = FileConfig ( filename = \"config.json\" , directory = \"/path/to/directory\" ) Source code in spotPython/data/base.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 class FileConfig ( Config ): \"\"\"Base class for configurations that are stored in a local file. Args: filename (str): The file's name. directory (Optional[str]): The directory where the file is contained. Defaults to the location of the `datasets` module. desc (dict): Extra config parameters to pass as keyword arguments. Returns: (FileConfig): A FileConfig object. Examples: >>> config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\") \"\"\" def __init__ ( self , filename : str , directory : Optional [ str ] = None , ** desc ): super () . __init__ ( ** desc ) self . filename = filename self . directory = directory @property def path ( self ) -> pathlib . Path : \"\"\"The path to the configuration file. Returns: pathlib.Path: The path to the configuration file. Examples: >>> config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\") >>> config.path PosixPath('/path/to/directory/config.json') \"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename ) @property def _repr_content ( self ) -> dict : \"\"\"The content of the string representation of the FileConfig object. Returns: dict: A dictionary containing the content of the string representation of the FileConfig object. Examples: >>> config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\") >>> config._repr_content {'Path': '/path/to/directory/config.json'} \"\"\" content = super () . _repr_content content [ \"Path\" ] = str ( self . path ) return content path () property \u00b6 The path to the configuration file. Returns: Type Description pathlib . Path pathlib.Path: The path to the configuration file. Examples: >>> config = FileConfig ( filename = \"config.json\" , directory = \"/path/to/directory\" ) >>> config . path PosixPath('/path/to/directory/config.json') Source code in spotPython/data/base.py 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 @property def path ( self ) -> pathlib . Path : \"\"\"The path to the configuration file. Returns: pathlib.Path: The path to the configuration file. Examples: >>> config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\") >>> config.path PosixPath('/path/to/directory/config.json') \"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename ) FileDataset \u00b6 Bases: Dataset Base class for datasets that are stored in a local file. Small datasets that are part of the spotRiver package inherit from this class. Parameters: Name Type Description Default filename str The file\u2019s name. required directory Optional [ str ] The directory where the file is contained. Defaults to the location of the datasets module. None desc dict Extra dataset parameters to pass as keyword arguments. {} Returns: Type Description FileDataset A FileDataset object. Examples: >>> dataset = FileDataset ( filename = \"dataset.csv\" , directory = \"/path/to/directory\" ) Source code in spotPython/data/base.py 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 class FileDataset ( Dataset ): \"\"\"Base class for datasets that are stored in a local file. Small datasets that are part of the spotRiver package inherit from this class. Args: filename (str): The file's name. directory (Optional[str]): The directory where the file is contained. Defaults to the location of the `datasets` module. desc (dict): Extra dataset parameters to pass as keyword arguments. Returns: (FileDataset): A FileDataset object. Examples: >>> dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\") \"\"\" def __init__ ( self , filename : str , directory : Optional [ str ] = None , ** desc ): super () . __init__ ( ** desc ) self . filename = filename self . directory = directory @property def path ( self ) -> pathlib . Path : \"\"\"The path to the dataset file. Returns: pathlib.Path: The path to the dataset file. Examples: >>> dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\") >>> dataset.path PosixPath('/path/to/directory/dataset.csv') \"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename ) @property def _repr_content ( self ) -> dict : \"\"\"The content of the string representation of the FileDataset object. Returns: dict: A dictionary containing the content of the string representation of the FileDataset object. Examples: >>> dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\") >>> dataset._repr_content {'Path': '/path/to/directory/dataset.csv'} \"\"\" content = super () . _repr_content content [ \"Path\" ] = str ( self . path ) return content path () property \u00b6 The path to the dataset file. Returns: Type Description pathlib . Path pathlib.Path: The path to the dataset file. Examples: >>> dataset = FileDataset ( filename = \"dataset.csv\" , directory = \"/path/to/directory\" ) >>> dataset . path PosixPath('/path/to/directory/dataset.csv') Source code in spotPython/data/base.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 @property def path ( self ) -> pathlib . Path : \"\"\"The path to the dataset file. Returns: pathlib.Path: The path to the dataset file. Examples: >>> dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\") >>> dataset.path PosixPath('/path/to/directory/dataset.csv') \"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename ) GenericFileDataset \u00b6 Bases: Dataset Base class for datasets that are stored in a local file. Small datasets that are part of the spotRiver package inherit from this class. Parameters: Name Type Description Default filename str The file\u2019s name. required target str The name of the target variable. required converters dict A dictionary specifying how to convert the columns of the dataset. Defaults to None. None parse_dates list A list of columns to parse as dates. Defaults to None. None directory str The directory where the file is contained. Defaults to the location of the datasets module. None desc dict Extra dataset parameters to pass as keyword arguments. {} Examples: >>> from river.datasets import Iris >>> dataset = Iris () >>> for x , y in dataset : ... print ( x , y ) ... break ({'sepal_length': 5.1, 'sepal_width': 3.5, 'petal_length': 1.4, 'petal_width': 0.2}, 'setosa') Source code in spotPython/data/base.py 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 class GenericFileDataset ( Dataset ): \"\"\"Base class for datasets that are stored in a local file. Small datasets that are part of the spotRiver package inherit from this class. Args: filename (str): The file's name. target (str): The name of the target variable. converters (dict): A dictionary specifying how to convert the columns of the dataset. Defaults to None. parse_dates (list): A list of columns to parse as dates. Defaults to None. directory (str): The directory where the file is contained. Defaults to the location of the `datasets` module. desc (dict): Extra dataset parameters to pass as keyword arguments. Examples: >>> from river.datasets import Iris >>> dataset = Iris() >>> for x, y in dataset: ... print(x, y) ... break ({'sepal_length': 5.1, 'sepal_width': 3.5, 'petal_length': 1.4, 'petal_width': 0.2}, 'setosa') \"\"\" def __init__ ( self , filename : str , target : str , converters : dict = None , parse_dates : list = None , directory : str = None , ** desc : dict , ): super () . __init__ ( ** desc ) self . filename = filename self . directory = directory self . target = target self . converters = converters self . parse_dates = parse_dates @property def path ( self ) -> pathlib . Path : \"\"\"Returns the path where the dataset is stored.\"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename ) @property def _repr_content ( self ): content = super () . _repr_content content [ \"Path\" ] = str ( self . path ) return content path () property \u00b6 Returns the path where the dataset is stored. Source code in spotPython/data/base.py 658 659 660 661 662 663 @property def path ( self ) -> pathlib . Path : \"\"\"Returns the path where the dataset is stored.\"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename ) RemoteDataset \u00b6 Bases: FileDataset Base class for datasets that are stored in a remote file. Medium and large datasets that are not part of the river package inherit from this class. The filename doesn\u2019t have to be provided if unpack is False. Indeed in the latter case the filename will be inferred from the URL. Parameters: Name Type Description Default url str The URL the dataset is located at. required size int The expected download size. required unpack bool Whether to unpack the download or not. Defaults to True. True filename str An optional name to given to the file if the file is unpacked. Defaults to None. None desc dict Extra dataset parameters to pass as keyword arguments. {} Examples: >>> from river.datasets import AirlinePassengers >>> dataset = AirlinePassengers () >>> for x , y in dataset : ... print ( x , y ) ... break ({'month': datetime.datetime(1949, 1, 1, 0, 0)}, 112) Source code in spotPython/data/base.py 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 class RemoteDataset ( FileDataset ): \"\"\"Base class for datasets that are stored in a remote file. Medium and large datasets that are not part of the river package inherit from this class. The filename doesn't have to be provided if unpack is False. Indeed in the latter case the filename will be inferred from the URL. Args: url (str): The URL the dataset is located at. size (int): The expected download size. unpack (bool): Whether to unpack the download or not. Defaults to True. filename (str): An optional name to given to the file if the file is unpacked. Defaults to None. desc (dict): Extra dataset parameters to pass as keyword arguments. Examples: >>> from river.datasets import AirlinePassengers >>> dataset = AirlinePassengers() >>> for x, y in dataset: ... print(x, y) ... break ({'month': datetime.datetime(1949, 1, 1, 0, 0)}, 112) \"\"\" def __init__ ( self , url : str , size : int , unpack : bool = True , filename : str = None , ** desc : dict ): if filename is None : filename = path . basename ( url ) super () . __init__ ( filename = filename , ** desc ) self . url = url self . size = size self . unpack = unpack @property def path ( self ) -> pathlib . Path : \"\"\"Returns the path where the dataset is stored.\"\"\" return pathlib . Path ( get_data_home (), self . __class__ . __name__ , self . filename ) def download ( self , force : bool = False , verbose : bool = True ) -> None : \"\"\"Downloads the dataset. Args: force (bool): Whether to force the download even if the data is already downloaded. Defaults to False. verbose (bool): Whether to display information about the download. Defaults to True. \"\"\" if not force and self . is_downloaded : return # Determine where to download the archive directory = self . path . parent directory . mkdir ( parents = True , exist_ok = True ) archive_path = directory . joinpath ( path . basename ( self . url )) with request . urlopen ( self . url ) as r : # Notify the user if verbose : meta = r . info () try : n_bytes = int ( meta [ \"Content-Length\" ]) msg = f \"Downloading { self . url } ( { n_bytes } )\" except KeyError : msg = f \"Downloading { self . url } \" print ( msg ) # Now dump the contents of the requests with open ( archive_path , \"wb\" ) as f : shutil . copyfileobj ( r , f ) if not self . unpack : return if verbose : print ( f \"Uncompressing into { directory } \" ) if archive_path . suffix . endswith ( \"zip\" ): with zipfile . ZipFile ( archive_path , \"r\" ) as zf : zf . extractall ( directory ) elif archive_path . suffix . endswith (( \"gz\" , \"tar\" )): mode = \"r:\" if archive_path . suffix . endswith ( \"tar\" ) else \"r:gz\" tar = tarfile . open ( archive_path , mode ) tar . extractall ( directory ) tar . close () else : raise RuntimeError ( f \"Unhandled extension type: { archive_path . suffix } \" ) # Delete the archive file now that it has been uncompressed archive_path . unlink () @abc . abstractmethod def _iter ( self ): pass @property def is_downloaded ( self ) -> bool : \"\"\"Indicate whether or not the data has been correctly downloaded.\"\"\" if self . path . exists (): if self . path . is_file (): return self . path . stat () . st_size == self . size return sum ( f . stat () . st_size for f in self . path . glob ( \"**/*\" ) if f . is_file ()) return False def __iter__ ( self ): \"\"\"Iterates over the samples of a dataset.\"\"\" if not self . is_downloaded : self . download ( verbose = True ) if not self . is_downloaded : raise RuntimeError ( \"Something went wrong during the download\" ) yield from self . _iter () @property def _repr_content ( self ): content = super () . _repr_content content [ \"URL\" ] = self . url content [ \"Size\" ] = self . size content [ \"Downloaded\" ] = str ( self . is_downloaded ) return content __iter__ () \u00b6 Iterates over the samples of a dataset. Source code in spotPython/data/base.py 594 595 596 597 598 599 600 def __iter__ ( self ): \"\"\"Iterates over the samples of a dataset.\"\"\" if not self . is_downloaded : self . download ( verbose = True ) if not self . is_downloaded : raise RuntimeError ( \"Something went wrong during the download\" ) yield from self . _iter () download ( force = False , verbose = True ) \u00b6 Downloads the dataset. Parameters: Name Type Description Default force bool Whether to force the download even if the data is already downloaded. Defaults to False. False verbose bool Whether to display information about the download. Defaults to True. True Source code in spotPython/data/base.py 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 def download ( self , force : bool = False , verbose : bool = True ) -> None : \"\"\"Downloads the dataset. Args: force (bool): Whether to force the download even if the data is already downloaded. Defaults to False. verbose (bool): Whether to display information about the download. Defaults to True. \"\"\" if not force and self . is_downloaded : return # Determine where to download the archive directory = self . path . parent directory . mkdir ( parents = True , exist_ok = True ) archive_path = directory . joinpath ( path . basename ( self . url )) with request . urlopen ( self . url ) as r : # Notify the user if verbose : meta = r . info () try : n_bytes = int ( meta [ \"Content-Length\" ]) msg = f \"Downloading { self . url } ( { n_bytes } )\" except KeyError : msg = f \"Downloading { self . url } \" print ( msg ) # Now dump the contents of the requests with open ( archive_path , \"wb\" ) as f : shutil . copyfileobj ( r , f ) if not self . unpack : return if verbose : print ( f \"Uncompressing into { directory } \" ) if archive_path . suffix . endswith ( \"zip\" ): with zipfile . ZipFile ( archive_path , \"r\" ) as zf : zf . extractall ( directory ) elif archive_path . suffix . endswith (( \"gz\" , \"tar\" )): mode = \"r:\" if archive_path . suffix . endswith ( \"tar\" ) else \"r:gz\" tar = tarfile . open ( archive_path , mode ) tar . extractall ( directory ) tar . close () else : raise RuntimeError ( f \"Unhandled extension type: { archive_path . suffix } \" ) # Delete the archive file now that it has been uncompressed archive_path . unlink () is_downloaded () property \u00b6 Indicate whether or not the data has been correctly downloaded. Source code in spotPython/data/base.py 584 585 586 587 588 589 590 591 592 @property def is_downloaded ( self ) -> bool : \"\"\"Indicate whether or not the data has been correctly downloaded.\"\"\" if self . path . exists (): if self . path . is_file (): return self . path . stat () . st_size == self . size return sum ( f . stat () . st_size for f in self . path . glob ( \"**/*\" ) if f . is_file ()) return False path () property \u00b6 Returns the path where the dataset is stored. Source code in spotPython/data/base.py 519 520 521 522 @property def path ( self ) -> pathlib . Path : \"\"\"Returns the path where the dataset is stored.\"\"\" return pathlib . Path ( get_data_home (), self . __class__ . __name__ , self . filename ) SyntheticDataset \u00b6 Bases: Dataset A synthetic dataset. Parameters: Name Type Description Default task str Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d required n_features int Number of features in the dataset. required n_samples int Number of samples in the dataset. required n_classes int Number of classes in the dataset, only applies to classification datasets. None n_outputs int Number of outputs the target is made of, only applies to multi-output datasets. None sparse bool Whether the dataset is sparse or not. False Returns: Type Description SyntheticDataset A synthetic dataset object. Examples: >>> from sklearn.datasets import make_classification >>> X , y = make_classification ( n_features = 4 , random_state = 0 ) >>> dataset = SyntheticDataset ( task = \"Binary classification\" , n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) Source code in spotPython/data/base.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 class SyntheticDataset ( Dataset ): \"\"\"A synthetic dataset. Args: task (str): Type of task the dataset is meant for. Should be one of: - \"Regression\" - \"Binary classification\" - \"Multi-class classification\" - \"Multi-output binary classification\" - \"Multi-output regression\" n_features (int): Number of features in the dataset. n_samples (int): Number of samples in the dataset. n_classes (int): Number of classes in the dataset, only applies to classification datasets. n_outputs (int): Number of outputs the target is made of, only applies to multi-output datasets. sparse (bool): Whether the dataset is sparse or not. Returns: (SyntheticDataset): A synthetic dataset object. Examples: >>> from sklearn.datasets import make_classification >>> X, y = make_classification(n_features=4, random_state=0) >>> dataset = SyntheticDataset(task=\"Binary classification\", n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) \"\"\" def __init__ ( self , task : str , n_features : int , n_samples : int , n_classes : Union [ int , None ] = None , n_outputs : Union [ int , None ] = None , sparse : bool = False , ): pass def __repr__ ( self ) -> str : \"\"\"String representation of the SyntheticDataset object. Returns: str: A string representation of the SyntheticDataset object. Examples: >>> from sklearn.datasets import make_classification >>> X, y = make_classification(n_features=4, random_state=0) >>> dataset = SyntheticDataset(task=\"Binary classification\", n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) >>> print(dataset) Synthetic data generator Configuration ------------- task Binary classification n_features 4 n_samples 100 n_classes 2 n_outputs 1 sparse False \"\"\" l_len_prop = max ( map ( len , self . _repr_content . keys ())) r_len_prop = max ( map ( len , self . _repr_content . values ())) params = self . _get_params () l_len_config = max ( map ( len , params . keys ())) r_len_config = max ( map ( len , map ( str , params . values ()))) out = ( \"Synthetic data generator \\n\\n \" + \" \\n \" . join ( k . rjust ( l_len_prop ) + \" \" + v . ljust ( r_len_prop ) for k , v in self . _repr_content . items ()) + \" \\n\\n Configuration \\n ------------- \\n \" + \" \\n \" . join ( k . rjust ( l_len_config ) + \" \" + str ( v ) . ljust ( r_len_config ) for k , v in params . items ()) ) return out def _get_params ( self ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the parameters that were used during initialization. Returns: dict: A dictionary containing the parameters that were used during initialization. Examples: >>> from sklearn.datasets import make_classification >>> X, y = make_classification(n_features=4, random_state=0) >>> dataset = SyntheticDataset(task=\"Binary classification\", n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) >>> dataset._get_params() {'task': 'Binary classification', 'n_features': 4, 'n_samples': 100, 'n_classes': 2, 'n_outputs': 1, 'sparse': False} \"\"\" return { name : getattr ( self , name ) for name , param in inspect . signature ( self . __init__ ) . parameters . items () # type: ignore if param . kind != param . VAR_KEYWORD } __repr__ () \u00b6 String representation of the SyntheticDataset object. Returns: Name Type Description str str A string representation of the SyntheticDataset object. Examples: >>> from sklearn.datasets import make_classification >>> X , y = make_classification ( n_features = 4 , random_state = 0 ) >>> dataset = SyntheticDataset ( task = \"Binary classification\" , n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) >>> print ( dataset ) Synthetic data generator Configuration \u00b6 task Binary classification n_features 4 n_samples 100 n_classes 2 n_outputs 1 sparse False Source code in spotPython/data/base.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def __repr__ ( self ) -> str : \"\"\"String representation of the SyntheticDataset object. Returns: str: A string representation of the SyntheticDataset object. Examples: >>> from sklearn.datasets import make_classification >>> X, y = make_classification(n_features=4, random_state=0) >>> dataset = SyntheticDataset(task=\"Binary classification\", n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) >>> print(dataset) Synthetic data generator Configuration ------------- task Binary classification n_features 4 n_samples 100 n_classes 2 n_outputs 1 sparse False \"\"\" l_len_prop = max ( map ( len , self . _repr_content . keys ())) r_len_prop = max ( map ( len , self . _repr_content . values ())) params = self . _get_params () l_len_config = max ( map ( len , params . keys ())) r_len_config = max ( map ( len , map ( str , params . values ()))) out = ( \"Synthetic data generator \\n\\n \" + \" \\n \" . join ( k . rjust ( l_len_prop ) + \" \" + v . ljust ( r_len_prop ) for k , v in self . _repr_content . items ()) + \" \\n\\n Configuration \\n ------------- \\n \" + \" \\n \" . join ( k . rjust ( l_len_config ) + \" \" + str ( v ) . ljust ( r_len_config ) for k , v in params . items ()) ) return out get_data_home ( data_home = None ) \u00b6 Return the location where remote datasets are to be stored. By default the data directory is set to a folder named \u2018spotriver_data\u2019 in the user home folder. Alternatively, it can be set by the \u2018SPOTRIVER_DATA\u2019 environment variable or programmatically by giving an explicit folder path. The \u2018~\u2019 symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created. Parameters: Name Type Description Default data_home str or pathlib.Path The path to spotriver data directory. If None , the default path is ~/spotriver_data . None Returns: Name Type Description data_home pathlib . Path The path to the spotriver data directory. Examples: >>> from pathlib import Path >>> get_data_home () PosixPath('/home/user/spotriver_data') >>> get_data_home ( Path ( '/tmp/spotriver_data' )) PosixPath('/tmp/spotriver_data') Source code in spotPython/data/base.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def get_data_home ( data_home : Optional [ Union [ str , Path ]] = None ) -> Path : \"\"\"Return the location where remote datasets are to be stored. By default the data directory is set to a folder named 'spotriver_data' in the user home folder. Alternatively, it can be set by the 'SPOTRIVER_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created. Args: data_home (str or pathlib.Path, optional): The path to spotriver data directory. If `None`, the default path is `~/spotriver_data`. Returns: data_home (pathlib.Path): The path to the spotriver data directory. Examples: >>> from pathlib import Path >>> get_data_home() PosixPath('/home/user/spotriver_data') >>> get_data_home(Path('/tmp/spotriver_data')) PosixPath('/tmp/spotriver_data') \"\"\" if data_home is None : data_home = environ . get ( \"SPOTRIVER_DATA\" , Path . home () / \"spotriver_data\" ) # Ensure data_home is a Path() object pointing to an absolute path data_home = Path ( data_home ) . absolute () # Create data directory if it does not exists. data_home . mkdir ( parents = True , exist_ok = True ) return data_home","title":"base"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Config","text":"Bases: abc . ABC Base class for all configurations. All configurations inherit from this class, be they stored in a file or generated on the fly. Attributes: Name Type Description desc str The description from the docstring. _repr_content dict The items that are displayed in the repr method. Source code in spotPython/data/base.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 class Config ( abc . ABC ): \"\"\"Base class for all configurations. All configurations inherit from this class, be they stored in a file or generated on the fly. Attributes: desc (str): The description from the docstring. _repr_content (dict): The items that are displayed in the __repr__ method. \"\"\" def __init__ ( self ): \"\"\"Initialize a Config object.\"\"\" pass @property def desc ( self ) -> str : \"\"\"Return the description from the docstring. Returns: str: The description from the docstring. Examples: >>> class MyConfig(Config): ... '''My configuration class.''' ... pass >>> MyConfig().desc 'My configuration class.' \"\"\" desc = re . split ( pattern = r \"\\w+\\n\\s {4} \\-{3,}\" , string = self . __doc__ , maxsplit = 0 )[ 0 ] return inspect . cleandoc ( desc ) @property def _repr_content ( self ) -> dict : \"\"\"The items that are displayed in the __repr__ method. This property can be overridden in order to modify the output of the __repr__ method. Returns: dict: A dictionary containing the items to be displayed in the __repr__ method. Examples: >>> class MyConfig(Config): ... '''My configuration class.''' ... pass >>> MyConfig()._repr_content {'Name': 'MyConfig'} \"\"\" content = {} content [ \"Name\" ] = self . __class__ . __name__ return content","title":"Config"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Config.__init__","text":"Initialize a Config object. Source code in spotPython/data/base.py 69 70 71 def __init__ ( self ): \"\"\"Initialize a Config object.\"\"\" pass","title":"__init__()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Config.desc","text":"Return the description from the docstring. Returns: Name Type Description str str The description from the docstring. Examples: >>> class MyConfig ( Config ): ... '''My configuration class.''' ... pass >>> MyConfig () . desc 'My configuration class.' Source code in spotPython/data/base.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 @property def desc ( self ) -> str : \"\"\"Return the description from the docstring. Returns: str: The description from the docstring. Examples: >>> class MyConfig(Config): ... '''My configuration class.''' ... pass >>> MyConfig().desc 'My configuration class.' \"\"\" desc = re . split ( pattern = r \"\\w+\\n\\s {4} \\-{3,}\" , string = self . __doc__ , maxsplit = 0 )[ 0 ] return inspect . cleandoc ( desc )","title":"desc()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Dataset","text":"Bases: abc . ABC Base class for all datasets. All datasets inherit from this class, be they stored in a file or generated on the fly. Parameters: Name Type Description Default task str Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d required n_features int Number of features in the dataset. required n_samples int Number of samples in the dataset. None n_classes int Number of classes in the dataset, only applies to classification datasets. None n_outputs int Number of outputs the target is made of, only applies to multi-output datasets. None sparse bool Whether the dataset is sparse or not. False Attributes: Name Type Description desc str The description from the docstring. _repr_content dict The items that are displayed in the repr method. Source code in spotPython/data/base.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 class Dataset ( abc . ABC ): \"\"\"Base class for all datasets. All datasets inherit from this class, be they stored in a file or generated on the fly. Args: task (str): Type of task the dataset is meant for. Should be one of: - \"Regression\" - \"Binary classification\" - \"Multi-class classification\" - \"Multi-output binary classification\" - \"Multi-output regression\" n_features (int): Number of features in the dataset. n_samples (int, optional): Number of samples in the dataset. n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets. n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets. sparse (bool, optional): Whether the dataset is sparse or not. Attributes: desc (str): The description from the docstring. _repr_content (dict): The items that are displayed in the __repr__ method. \"\"\" def __init__ ( self , task : str , n_features : int , n_samples : Optional [ int ] = None , n_classes : Optional [ int ] = None , n_outputs : Optional [ int ] = None , sparse : bool = False , ): \"\"\"Initialize a Dataset object. Args: task (str): Type of task the dataset is meant for. Should be one of: - \"Regression\" - \"Binary classification\" - \"Multi-class classification\" - \"Multi-output binary classification\" - \"Multi-output regression\" n_features (int): Number of features in the dataset. n_samples (int, optional): Number of samples in the dataset. Defaults to None. n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets. Defaults to None. n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets. Defaults to None. sparse (bool, optional): Whether the dataset is sparse or not. Defaults to False. \"\"\" self . task = task self . n_features = n_features self . n_samples = n_samples self . n_outputs = n_outputs self . n_classes = n_classes self . sparse = sparse @abc . abstractmethod def __iter__ ( self ): \"\"\"Abstract method for iterating over samples in the dataset.\"\"\" raise NotImplementedError def take ( self , k : int ) -> itertools . islice : \"\"\"Iterate over the k samples. Args: k (int): The number of samples to iterate over. Returns: itertools.islice: An iterator over the first k samples in the dataset. Examples: >>> class MyDataset(Dataset): ... def __init__(self): ... super().__init__('Regression', 10) ... def __iter__(self): ... yield from range(10) >>> list(MyDataset().take(5)) [0, 1, 2, 3, 4] \"\"\" return itertools . islice ( self , k ) @property def desc ( self ) -> str : \"\"\"Return the description from the docstring. Returns: str: The description from the docstring. Examples: >>> class MyDataset(Dataset): ... '''My dataset class.''' ... def __init__(self): ... super().__init__('Regression', 10) ... def __iter__(self): ... yield from range(10) >>> MyDataset().desc 'My dataset class.' \"\"\" desc = re . split ( pattern = r \"\\w+\\n\\s {4} \\-{3,}\" , string = self . __doc__ , maxsplit = 0 )[ 0 ] return inspect . cleandoc ( desc ) @property def _repr_content ( self ) -> dict : \"\"\"The items that are displayed in the __repr__ method. This property can be overridden in order to modify the output of the __repr__ method. Returns: dict: A dictionary containing the items to be displayed in the __repr__ method. \"\"\" content = {} content [ \"Name\" ] = self . __class__ . __name__ content [ \"Task\" ] = self . task if isinstance ( self , SyntheticDataset ) and self . n_samples is None : content [ \"Samples\" ] = \"\u221e\" elif self . n_samples : content [ \"Samples\" ] = f \" { self . n_samples : , } \" if self . n_features : content [ \"Features\" ] = f \" { self . n_features : , } \" if self . n_outputs : content [ \"Outputs\" ] = f \" { self . n_outputs : , } \" if self . n_classes : content [ \"Classes\" ] = f \" { self . n_classes : , } \" content [ \"Sparse\" ] = str ( self . sparse ) return content def __repr__ ( self ): l_len = max ( map ( len , self . _repr_content . keys ())) r_len = max ( map ( len , self . _repr_content . values ())) out = f \" { self . desc } \\n\\n \" + \" \\n \" . join ( k . rjust ( l_len ) + \" \" + v . ljust ( r_len ) for k , v in self . _repr_content . items () ) if \"Parameters \\n ----------\" in self . __doc__ : params = re . split ( r \"\\w+\\n\\s {4} \\-{3,}\" , re . split ( \"Parameters \\n ----------\" , self . __doc__ )[ 1 ], )[ 0 ] . rstrip () out += f \" \\n\\n Parameters \\n ---------- { params } \" return out","title":"Dataset"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Dataset.__init__","text":"Initialize a Dataset object. Parameters: Name Type Description Default task str Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d required n_features int Number of features in the dataset. required n_samples int Number of samples in the dataset. Defaults to None. None n_classes int Number of classes in the dataset, only applies to classification datasets. Defaults to None. None n_outputs int Number of outputs the target is made of, only applies to multi-output datasets. Defaults to None. None sparse bool Whether the dataset is sparse or not. Defaults to False. False Source code in spotPython/data/base.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def __init__ ( self , task : str , n_features : int , n_samples : Optional [ int ] = None , n_classes : Optional [ int ] = None , n_outputs : Optional [ int ] = None , sparse : bool = False , ): \"\"\"Initialize a Dataset object. Args: task (str): Type of task the dataset is meant for. Should be one of: - \"Regression\" - \"Binary classification\" - \"Multi-class classification\" - \"Multi-output binary classification\" - \"Multi-output regression\" n_features (int): Number of features in the dataset. n_samples (int, optional): Number of samples in the dataset. Defaults to None. n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets. Defaults to None. n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets. Defaults to None. sparse (bool, optional): Whether the dataset is sparse or not. Defaults to False. \"\"\" self . task = task self . n_features = n_features self . n_samples = n_samples self . n_outputs = n_outputs self . n_classes = n_classes self . sparse = sparse","title":"__init__()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Dataset.__iter__","text":"Abstract method for iterating over samples in the dataset. Source code in spotPython/data/base.py 167 168 169 170 @abc . abstractmethod def __iter__ ( self ): \"\"\"Abstract method for iterating over samples in the dataset.\"\"\" raise NotImplementedError","title":"__iter__()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Dataset.desc","text":"Return the description from the docstring. Returns: Name Type Description str str The description from the docstring. Examples: >>> class MyDataset ( Dataset ): ... '''My dataset class.''' ... def __init__ ( self ): ... super () . __init__ ( 'Regression' , 10 ) ... def __iter__ ( self ): ... yield from range ( 10 ) >>> MyDataset () . desc 'My dataset class.' Source code in spotPython/data/base.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 @property def desc ( self ) -> str : \"\"\"Return the description from the docstring. Returns: str: The description from the docstring. Examples: >>> class MyDataset(Dataset): ... '''My dataset class.''' ... def __init__(self): ... super().__init__('Regression', 10) ... def __iter__(self): ... yield from range(10) >>> MyDataset().desc 'My dataset class.' \"\"\" desc = re . split ( pattern = r \"\\w+\\n\\s {4} \\-{3,}\" , string = self . __doc__ , maxsplit = 0 )[ 0 ] return inspect . cleandoc ( desc )","title":"desc()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Dataset.take","text":"Iterate over the k samples. Parameters: Name Type Description Default k int The number of samples to iterate over. required Returns: Type Description itertools . islice itertools.islice: An iterator over the first k samples in the dataset. Examples: >>> class MyDataset ( Dataset ): ... def __init__ ( self ): ... super () . __init__ ( 'Regression' , 10 ) ... def __iter__ ( self ): ... yield from range ( 10 ) >>> list ( MyDataset () . take ( 5 )) [0, 1, 2, 3, 4] Source code in spotPython/data/base.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def take ( self , k : int ) -> itertools . islice : \"\"\"Iterate over the k samples. Args: k (int): The number of samples to iterate over. Returns: itertools.islice: An iterator over the first k samples in the dataset. Examples: >>> class MyDataset(Dataset): ... def __init__(self): ... super().__init__('Regression', 10) ... def __iter__(self): ... yield from range(10) >>> list(MyDataset().take(5)) [0, 1, 2, 3, 4] \"\"\" return itertools . islice ( self , k )","title":"take()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.FileConfig","text":"Bases: Config Base class for configurations that are stored in a local file. Parameters: Name Type Description Default filename str The file\u2019s name. required directory Optional [ str ] The directory where the file is contained. Defaults to the location of the datasets module. None desc dict Extra config parameters to pass as keyword arguments. {} Returns: Type Description FileConfig A FileConfig object. Examples: >>> config = FileConfig ( filename = \"config.json\" , directory = \"/path/to/directory\" ) Source code in spotPython/data/base.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 class FileConfig ( Config ): \"\"\"Base class for configurations that are stored in a local file. Args: filename (str): The file's name. directory (Optional[str]): The directory where the file is contained. Defaults to the location of the `datasets` module. desc (dict): Extra config parameters to pass as keyword arguments. Returns: (FileConfig): A FileConfig object. Examples: >>> config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\") \"\"\" def __init__ ( self , filename : str , directory : Optional [ str ] = None , ** desc ): super () . __init__ ( ** desc ) self . filename = filename self . directory = directory @property def path ( self ) -> pathlib . Path : \"\"\"The path to the configuration file. Returns: pathlib.Path: The path to the configuration file. Examples: >>> config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\") >>> config.path PosixPath('/path/to/directory/config.json') \"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename ) @property def _repr_content ( self ) -> dict : \"\"\"The content of the string representation of the FileConfig object. Returns: dict: A dictionary containing the content of the string representation of the FileConfig object. Examples: >>> config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\") >>> config._repr_content {'Path': '/path/to/directory/config.json'} \"\"\" content = super () . _repr_content content [ \"Path\" ] = str ( self . path ) return content","title":"FileConfig"},{"location":"reference/spotPython/data/base/#spotPython.data.base.FileConfig.path","text":"The path to the configuration file. Returns: Type Description pathlib . Path pathlib.Path: The path to the configuration file. Examples: >>> config = FileConfig ( filename = \"config.json\" , directory = \"/path/to/directory\" ) >>> config . path PosixPath('/path/to/directory/config.json') Source code in spotPython/data/base.py 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 @property def path ( self ) -> pathlib . Path : \"\"\"The path to the configuration file. Returns: pathlib.Path: The path to the configuration file. Examples: >>> config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\") >>> config.path PosixPath('/path/to/directory/config.json') \"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename )","title":"path()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.FileDataset","text":"Bases: Dataset Base class for datasets that are stored in a local file. Small datasets that are part of the spotRiver package inherit from this class. Parameters: Name Type Description Default filename str The file\u2019s name. required directory Optional [ str ] The directory where the file is contained. Defaults to the location of the datasets module. None desc dict Extra dataset parameters to pass as keyword arguments. {} Returns: Type Description FileDataset A FileDataset object. Examples: >>> dataset = FileDataset ( filename = \"dataset.csv\" , directory = \"/path/to/directory\" ) Source code in spotPython/data/base.py 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 class FileDataset ( Dataset ): \"\"\"Base class for datasets that are stored in a local file. Small datasets that are part of the spotRiver package inherit from this class. Args: filename (str): The file's name. directory (Optional[str]): The directory where the file is contained. Defaults to the location of the `datasets` module. desc (dict): Extra dataset parameters to pass as keyword arguments. Returns: (FileDataset): A FileDataset object. Examples: >>> dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\") \"\"\" def __init__ ( self , filename : str , directory : Optional [ str ] = None , ** desc ): super () . __init__ ( ** desc ) self . filename = filename self . directory = directory @property def path ( self ) -> pathlib . Path : \"\"\"The path to the dataset file. Returns: pathlib.Path: The path to the dataset file. Examples: >>> dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\") >>> dataset.path PosixPath('/path/to/directory/dataset.csv') \"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename ) @property def _repr_content ( self ) -> dict : \"\"\"The content of the string representation of the FileDataset object. Returns: dict: A dictionary containing the content of the string representation of the FileDataset object. Examples: >>> dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\") >>> dataset._repr_content {'Path': '/path/to/directory/dataset.csv'} \"\"\" content = super () . _repr_content content [ \"Path\" ] = str ( self . path ) return content","title":"FileDataset"},{"location":"reference/spotPython/data/base/#spotPython.data.base.FileDataset.path","text":"The path to the dataset file. Returns: Type Description pathlib . Path pathlib.Path: The path to the dataset file. Examples: >>> dataset = FileDataset ( filename = \"dataset.csv\" , directory = \"/path/to/directory\" ) >>> dataset . path PosixPath('/path/to/directory/dataset.csv') Source code in spotPython/data/base.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 @property def path ( self ) -> pathlib . Path : \"\"\"The path to the dataset file. Returns: pathlib.Path: The path to the dataset file. Examples: >>> dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\") >>> dataset.path PosixPath('/path/to/directory/dataset.csv') \"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename )","title":"path()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.GenericFileDataset","text":"Bases: Dataset Base class for datasets that are stored in a local file. Small datasets that are part of the spotRiver package inherit from this class. Parameters: Name Type Description Default filename str The file\u2019s name. required target str The name of the target variable. required converters dict A dictionary specifying how to convert the columns of the dataset. Defaults to None. None parse_dates list A list of columns to parse as dates. Defaults to None. None directory str The directory where the file is contained. Defaults to the location of the datasets module. None desc dict Extra dataset parameters to pass as keyword arguments. {} Examples: >>> from river.datasets import Iris >>> dataset = Iris () >>> for x , y in dataset : ... print ( x , y ) ... break ({'sepal_length': 5.1, 'sepal_width': 3.5, 'petal_length': 1.4, 'petal_width': 0.2}, 'setosa') Source code in spotPython/data/base.py 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 class GenericFileDataset ( Dataset ): \"\"\"Base class for datasets that are stored in a local file. Small datasets that are part of the spotRiver package inherit from this class. Args: filename (str): The file's name. target (str): The name of the target variable. converters (dict): A dictionary specifying how to convert the columns of the dataset. Defaults to None. parse_dates (list): A list of columns to parse as dates. Defaults to None. directory (str): The directory where the file is contained. Defaults to the location of the `datasets` module. desc (dict): Extra dataset parameters to pass as keyword arguments. Examples: >>> from river.datasets import Iris >>> dataset = Iris() >>> for x, y in dataset: ... print(x, y) ... break ({'sepal_length': 5.1, 'sepal_width': 3.5, 'petal_length': 1.4, 'petal_width': 0.2}, 'setosa') \"\"\" def __init__ ( self , filename : str , target : str , converters : dict = None , parse_dates : list = None , directory : str = None , ** desc : dict , ): super () . __init__ ( ** desc ) self . filename = filename self . directory = directory self . target = target self . converters = converters self . parse_dates = parse_dates @property def path ( self ) -> pathlib . Path : \"\"\"Returns the path where the dataset is stored.\"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename ) @property def _repr_content ( self ): content = super () . _repr_content content [ \"Path\" ] = str ( self . path ) return content","title":"GenericFileDataset"},{"location":"reference/spotPython/data/base/#spotPython.data.base.GenericFileDataset.path","text":"Returns the path where the dataset is stored. Source code in spotPython/data/base.py 658 659 660 661 662 663 @property def path ( self ) -> pathlib . Path : \"\"\"Returns the path where the dataset is stored.\"\"\" if self . directory : return pathlib . Path ( self . directory ) . joinpath ( self . filename ) return pathlib . Path ( __file__ ) . parent . joinpath ( self . filename )","title":"path()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.RemoteDataset","text":"Bases: FileDataset Base class for datasets that are stored in a remote file. Medium and large datasets that are not part of the river package inherit from this class. The filename doesn\u2019t have to be provided if unpack is False. Indeed in the latter case the filename will be inferred from the URL. Parameters: Name Type Description Default url str The URL the dataset is located at. required size int The expected download size. required unpack bool Whether to unpack the download or not. Defaults to True. True filename str An optional name to given to the file if the file is unpacked. Defaults to None. None desc dict Extra dataset parameters to pass as keyword arguments. {} Examples: >>> from river.datasets import AirlinePassengers >>> dataset = AirlinePassengers () >>> for x , y in dataset : ... print ( x , y ) ... break ({'month': datetime.datetime(1949, 1, 1, 0, 0)}, 112) Source code in spotPython/data/base.py 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 class RemoteDataset ( FileDataset ): \"\"\"Base class for datasets that are stored in a remote file. Medium and large datasets that are not part of the river package inherit from this class. The filename doesn't have to be provided if unpack is False. Indeed in the latter case the filename will be inferred from the URL. Args: url (str): The URL the dataset is located at. size (int): The expected download size. unpack (bool): Whether to unpack the download or not. Defaults to True. filename (str): An optional name to given to the file if the file is unpacked. Defaults to None. desc (dict): Extra dataset parameters to pass as keyword arguments. Examples: >>> from river.datasets import AirlinePassengers >>> dataset = AirlinePassengers() >>> for x, y in dataset: ... print(x, y) ... break ({'month': datetime.datetime(1949, 1, 1, 0, 0)}, 112) \"\"\" def __init__ ( self , url : str , size : int , unpack : bool = True , filename : str = None , ** desc : dict ): if filename is None : filename = path . basename ( url ) super () . __init__ ( filename = filename , ** desc ) self . url = url self . size = size self . unpack = unpack @property def path ( self ) -> pathlib . Path : \"\"\"Returns the path where the dataset is stored.\"\"\" return pathlib . Path ( get_data_home (), self . __class__ . __name__ , self . filename ) def download ( self , force : bool = False , verbose : bool = True ) -> None : \"\"\"Downloads the dataset. Args: force (bool): Whether to force the download even if the data is already downloaded. Defaults to False. verbose (bool): Whether to display information about the download. Defaults to True. \"\"\" if not force and self . is_downloaded : return # Determine where to download the archive directory = self . path . parent directory . mkdir ( parents = True , exist_ok = True ) archive_path = directory . joinpath ( path . basename ( self . url )) with request . urlopen ( self . url ) as r : # Notify the user if verbose : meta = r . info () try : n_bytes = int ( meta [ \"Content-Length\" ]) msg = f \"Downloading { self . url } ( { n_bytes } )\" except KeyError : msg = f \"Downloading { self . url } \" print ( msg ) # Now dump the contents of the requests with open ( archive_path , \"wb\" ) as f : shutil . copyfileobj ( r , f ) if not self . unpack : return if verbose : print ( f \"Uncompressing into { directory } \" ) if archive_path . suffix . endswith ( \"zip\" ): with zipfile . ZipFile ( archive_path , \"r\" ) as zf : zf . extractall ( directory ) elif archive_path . suffix . endswith (( \"gz\" , \"tar\" )): mode = \"r:\" if archive_path . suffix . endswith ( \"tar\" ) else \"r:gz\" tar = tarfile . open ( archive_path , mode ) tar . extractall ( directory ) tar . close () else : raise RuntimeError ( f \"Unhandled extension type: { archive_path . suffix } \" ) # Delete the archive file now that it has been uncompressed archive_path . unlink () @abc . abstractmethod def _iter ( self ): pass @property def is_downloaded ( self ) -> bool : \"\"\"Indicate whether or not the data has been correctly downloaded.\"\"\" if self . path . exists (): if self . path . is_file (): return self . path . stat () . st_size == self . size return sum ( f . stat () . st_size for f in self . path . glob ( \"**/*\" ) if f . is_file ()) return False def __iter__ ( self ): \"\"\"Iterates over the samples of a dataset.\"\"\" if not self . is_downloaded : self . download ( verbose = True ) if not self . is_downloaded : raise RuntimeError ( \"Something went wrong during the download\" ) yield from self . _iter () @property def _repr_content ( self ): content = super () . _repr_content content [ \"URL\" ] = self . url content [ \"Size\" ] = self . size content [ \"Downloaded\" ] = str ( self . is_downloaded ) return content","title":"RemoteDataset"},{"location":"reference/spotPython/data/base/#spotPython.data.base.RemoteDataset.__iter__","text":"Iterates over the samples of a dataset. Source code in spotPython/data/base.py 594 595 596 597 598 599 600 def __iter__ ( self ): \"\"\"Iterates over the samples of a dataset.\"\"\" if not self . is_downloaded : self . download ( verbose = True ) if not self . is_downloaded : raise RuntimeError ( \"Something went wrong during the download\" ) yield from self . _iter ()","title":"__iter__()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.RemoteDataset.download","text":"Downloads the dataset. Parameters: Name Type Description Default force bool Whether to force the download even if the data is already downloaded. Defaults to False. False verbose bool Whether to display information about the download. Defaults to True. True Source code in spotPython/data/base.py 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 def download ( self , force : bool = False , verbose : bool = True ) -> None : \"\"\"Downloads the dataset. Args: force (bool): Whether to force the download even if the data is already downloaded. Defaults to False. verbose (bool): Whether to display information about the download. Defaults to True. \"\"\" if not force and self . is_downloaded : return # Determine where to download the archive directory = self . path . parent directory . mkdir ( parents = True , exist_ok = True ) archive_path = directory . joinpath ( path . basename ( self . url )) with request . urlopen ( self . url ) as r : # Notify the user if verbose : meta = r . info () try : n_bytes = int ( meta [ \"Content-Length\" ]) msg = f \"Downloading { self . url } ( { n_bytes } )\" except KeyError : msg = f \"Downloading { self . url } \" print ( msg ) # Now dump the contents of the requests with open ( archive_path , \"wb\" ) as f : shutil . copyfileobj ( r , f ) if not self . unpack : return if verbose : print ( f \"Uncompressing into { directory } \" ) if archive_path . suffix . endswith ( \"zip\" ): with zipfile . ZipFile ( archive_path , \"r\" ) as zf : zf . extractall ( directory ) elif archive_path . suffix . endswith (( \"gz\" , \"tar\" )): mode = \"r:\" if archive_path . suffix . endswith ( \"tar\" ) else \"r:gz\" tar = tarfile . open ( archive_path , mode ) tar . extractall ( directory ) tar . close () else : raise RuntimeError ( f \"Unhandled extension type: { archive_path . suffix } \" ) # Delete the archive file now that it has been uncompressed archive_path . unlink ()","title":"download()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.RemoteDataset.is_downloaded","text":"Indicate whether or not the data has been correctly downloaded. Source code in spotPython/data/base.py 584 585 586 587 588 589 590 591 592 @property def is_downloaded ( self ) -> bool : \"\"\"Indicate whether or not the data has been correctly downloaded.\"\"\" if self . path . exists (): if self . path . is_file (): return self . path . stat () . st_size == self . size return sum ( f . stat () . st_size for f in self . path . glob ( \"**/*\" ) if f . is_file ()) return False","title":"is_downloaded()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.RemoteDataset.path","text":"Returns the path where the dataset is stored. Source code in spotPython/data/base.py 519 520 521 522 @property def path ( self ) -> pathlib . Path : \"\"\"Returns the path where the dataset is stored.\"\"\" return pathlib . Path ( get_data_home (), self . __class__ . __name__ , self . filename )","title":"path()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.SyntheticDataset","text":"Bases: Dataset A synthetic dataset. Parameters: Name Type Description Default task str Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d required n_features int Number of features in the dataset. required n_samples int Number of samples in the dataset. required n_classes int Number of classes in the dataset, only applies to classification datasets. None n_outputs int Number of outputs the target is made of, only applies to multi-output datasets. None sparse bool Whether the dataset is sparse or not. False Returns: Type Description SyntheticDataset A synthetic dataset object. Examples: >>> from sklearn.datasets import make_classification >>> X , y = make_classification ( n_features = 4 , random_state = 0 ) >>> dataset = SyntheticDataset ( task = \"Binary classification\" , n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) Source code in spotPython/data/base.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 class SyntheticDataset ( Dataset ): \"\"\"A synthetic dataset. Args: task (str): Type of task the dataset is meant for. Should be one of: - \"Regression\" - \"Binary classification\" - \"Multi-class classification\" - \"Multi-output binary classification\" - \"Multi-output regression\" n_features (int): Number of features in the dataset. n_samples (int): Number of samples in the dataset. n_classes (int): Number of classes in the dataset, only applies to classification datasets. n_outputs (int): Number of outputs the target is made of, only applies to multi-output datasets. sparse (bool): Whether the dataset is sparse or not. Returns: (SyntheticDataset): A synthetic dataset object. Examples: >>> from sklearn.datasets import make_classification >>> X, y = make_classification(n_features=4, random_state=0) >>> dataset = SyntheticDataset(task=\"Binary classification\", n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) \"\"\" def __init__ ( self , task : str , n_features : int , n_samples : int , n_classes : Union [ int , None ] = None , n_outputs : Union [ int , None ] = None , sparse : bool = False , ): pass def __repr__ ( self ) -> str : \"\"\"String representation of the SyntheticDataset object. Returns: str: A string representation of the SyntheticDataset object. Examples: >>> from sklearn.datasets import make_classification >>> X, y = make_classification(n_features=4, random_state=0) >>> dataset = SyntheticDataset(task=\"Binary classification\", n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) >>> print(dataset) Synthetic data generator Configuration ------------- task Binary classification n_features 4 n_samples 100 n_classes 2 n_outputs 1 sparse False \"\"\" l_len_prop = max ( map ( len , self . _repr_content . keys ())) r_len_prop = max ( map ( len , self . _repr_content . values ())) params = self . _get_params () l_len_config = max ( map ( len , params . keys ())) r_len_config = max ( map ( len , map ( str , params . values ()))) out = ( \"Synthetic data generator \\n\\n \" + \" \\n \" . join ( k . rjust ( l_len_prop ) + \" \" + v . ljust ( r_len_prop ) for k , v in self . _repr_content . items ()) + \" \\n\\n Configuration \\n ------------- \\n \" + \" \\n \" . join ( k . rjust ( l_len_config ) + \" \" + str ( v ) . ljust ( r_len_config ) for k , v in params . items ()) ) return out def _get_params ( self ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the parameters that were used during initialization. Returns: dict: A dictionary containing the parameters that were used during initialization. Examples: >>> from sklearn.datasets import make_classification >>> X, y = make_classification(n_features=4, random_state=0) >>> dataset = SyntheticDataset(task=\"Binary classification\", n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) >>> dataset._get_params() {'task': 'Binary classification', 'n_features': 4, 'n_samples': 100, 'n_classes': 2, 'n_outputs': 1, 'sparse': False} \"\"\" return { name : getattr ( self , name ) for name , param in inspect . signature ( self . __init__ ) . parameters . items () # type: ignore if param . kind != param . VAR_KEYWORD }","title":"SyntheticDataset"},{"location":"reference/spotPython/data/base/#spotPython.data.base.SyntheticDataset.__repr__","text":"String representation of the SyntheticDataset object. Returns: Name Type Description str str A string representation of the SyntheticDataset object. Examples: >>> from sklearn.datasets import make_classification >>> X , y = make_classification ( n_features = 4 , random_state = 0 ) >>> dataset = SyntheticDataset ( task = \"Binary classification\" , n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) >>> print ( dataset ) Synthetic data generator","title":"__repr__()"},{"location":"reference/spotPython/data/base/#spotPython.data.base.SyntheticDataset.__repr__--configuration","text":"task Binary classification n_features 4 n_samples 100 n_classes 2 n_outputs 1 sparse False Source code in spotPython/data/base.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def __repr__ ( self ) -> str : \"\"\"String representation of the SyntheticDataset object. Returns: str: A string representation of the SyntheticDataset object. Examples: >>> from sklearn.datasets import make_classification >>> X, y = make_classification(n_features=4, random_state=0) >>> dataset = SyntheticDataset(task=\"Binary classification\", n_features=4, n_samples=100, n_classes=2, n_outputs=1, sparse=False) >>> print(dataset) Synthetic data generator Configuration ------------- task Binary classification n_features 4 n_samples 100 n_classes 2 n_outputs 1 sparse False \"\"\" l_len_prop = max ( map ( len , self . _repr_content . keys ())) r_len_prop = max ( map ( len , self . _repr_content . values ())) params = self . _get_params () l_len_config = max ( map ( len , params . keys ())) r_len_config = max ( map ( len , map ( str , params . values ()))) out = ( \"Synthetic data generator \\n\\n \" + \" \\n \" . join ( k . rjust ( l_len_prop ) + \" \" + v . ljust ( r_len_prop ) for k , v in self . _repr_content . items ()) + \" \\n\\n Configuration \\n ------------- \\n \" + \" \\n \" . join ( k . rjust ( l_len_config ) + \" \" + str ( v ) . ljust ( r_len_config ) for k , v in params . items ()) ) return out","title":"Configuration"},{"location":"reference/spotPython/data/base/#spotPython.data.base.get_data_home","text":"Return the location where remote datasets are to be stored. By default the data directory is set to a folder named \u2018spotriver_data\u2019 in the user home folder. Alternatively, it can be set by the \u2018SPOTRIVER_DATA\u2019 environment variable or programmatically by giving an explicit folder path. The \u2018~\u2019 symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created. Parameters: Name Type Description Default data_home str or pathlib.Path The path to spotriver data directory. If None , the default path is ~/spotriver_data . None Returns: Name Type Description data_home pathlib . Path The path to the spotriver data directory. Examples: >>> from pathlib import Path >>> get_data_home () PosixPath('/home/user/spotriver_data') >>> get_data_home ( Path ( '/tmp/spotriver_data' )) PosixPath('/tmp/spotriver_data') Source code in spotPython/data/base.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def get_data_home ( data_home : Optional [ Union [ str , Path ]] = None ) -> Path : \"\"\"Return the location where remote datasets are to be stored. By default the data directory is set to a folder named 'spotriver_data' in the user home folder. Alternatively, it can be set by the 'SPOTRIVER_DATA' environment variable or programmatically by giving an explicit folder path. The '~' symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created. Args: data_home (str or pathlib.Path, optional): The path to spotriver data directory. If `None`, the default path is `~/spotriver_data`. Returns: data_home (pathlib.Path): The path to the spotriver data directory. Examples: >>> from pathlib import Path >>> get_data_home() PosixPath('/home/user/spotriver_data') >>> get_data_home(Path('/tmp/spotriver_data')) PosixPath('/tmp/spotriver_data') \"\"\" if data_home is None : data_home = environ . get ( \"SPOTRIVER_DATA\" , Path . home () / \"spotriver_data\" ) # Ensure data_home is a Path() object pointing to an absolute path data_home = Path ( data_home ) . absolute () # Create data directory if it does not exists. data_home . mkdir ( parents = True , exist_ok = True ) return data_home","title":"get_data_home()"},{"location":"reference/spotPython/data/light_hyper_dict/","text":"LightHyperDict \u00b6 Bases: base . FileConfig Lightning hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: Name Type Description filename str The name of the file where the hyperparameters are stored. Source code in spotPython/data/light_hyper_dict.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class LightHyperDict ( base . FileConfig ): \"\"\"Lightning hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: filename (str): The name of the file where the hyperparameters are stored. \"\"\" def __init__ ( self ): \"\"\"Initialize the LightHyperDict object. Examples: >>> lhd = LightHyperDict() \"\"\" super () . __init__ ( filename = \"light_hyper_dict.json\" , ) def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: dict: A dictionary containing the hyperparameters. Examples: >>> lhd = LightHyperDict() >>> hyperparams = lhd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d __init__ () \u00b6 Initialize the LightHyperDict object. Examples: >>> lhd = LightHyperDict () Source code in spotPython/data/light_hyper_dict.py 15 16 17 18 19 20 21 22 23 def __init__ ( self ): \"\"\"Initialize the LightHyperDict object. Examples: >>> lhd = LightHyperDict() \"\"\" super () . __init__ ( filename = \"light_hyper_dict.json\" , ) load () \u00b6 Load the hyperparameters from the file. Returns: Name Type Description dict dict A dictionary containing the hyperparameters. Examples: >>> lhd = LightHyperDict () >>> hyperparams = lhd . load () >>> print ( hyperparams ) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} Source code in spotPython/data/light_hyper_dict.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: dict: A dictionary containing the hyperparameters. Examples: >>> lhd = LightHyperDict() >>> hyperparams = lhd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d","title":"light_hyper_dict"},{"location":"reference/spotPython/data/light_hyper_dict/#spotPython.data.light_hyper_dict.LightHyperDict","text":"Bases: base . FileConfig Lightning hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: Name Type Description filename str The name of the file where the hyperparameters are stored. Source code in spotPython/data/light_hyper_dict.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class LightHyperDict ( base . FileConfig ): \"\"\"Lightning hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: filename (str): The name of the file where the hyperparameters are stored. \"\"\" def __init__ ( self ): \"\"\"Initialize the LightHyperDict object. Examples: >>> lhd = LightHyperDict() \"\"\" super () . __init__ ( filename = \"light_hyper_dict.json\" , ) def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: dict: A dictionary containing the hyperparameters. Examples: >>> lhd = LightHyperDict() >>> hyperparams = lhd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d","title":"LightHyperDict"},{"location":"reference/spotPython/data/light_hyper_dict/#spotPython.data.light_hyper_dict.LightHyperDict.__init__","text":"Initialize the LightHyperDict object. Examples: >>> lhd = LightHyperDict () Source code in spotPython/data/light_hyper_dict.py 15 16 17 18 19 20 21 22 23 def __init__ ( self ): \"\"\"Initialize the LightHyperDict object. Examples: >>> lhd = LightHyperDict() \"\"\" super () . __init__ ( filename = \"light_hyper_dict.json\" , )","title":"__init__()"},{"location":"reference/spotPython/data/light_hyper_dict/#spotPython.data.light_hyper_dict.LightHyperDict.load","text":"Load the hyperparameters from the file. Returns: Name Type Description dict dict A dictionary containing the hyperparameters. Examples: >>> lhd = LightHyperDict () >>> hyperparams = lhd . load () >>> print ( hyperparams ) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} Source code in spotPython/data/light_hyper_dict.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: dict: A dictionary containing the hyperparameters. Examples: >>> lhd = LightHyperDict() >>> hyperparams = lhd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d","title":"load()"},{"location":"reference/spotPython/data/sklearn_hyper_dict/","text":"SklearnHyperDict \u00b6 Bases: base . FileConfig Scikit-learn hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: Name Type Description filename str The name of the file where the hyperparameters are stored. Source code in spotPython/data/sklearn_hyper_dict.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class SklearnHyperDict ( base . FileConfig ): \"\"\"Scikit-learn hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: filename (str): The name of the file where the hyperparameters are stored. \"\"\" def __init__ ( self ): \"\"\"Initialize the SklearnHyperDict object. Examples: >>> shd = SklearnHyperDict() \"\"\" super () . __init__ ( filename = \"sklearn_hyper_dict.json\" , ) def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: (dict): A dictionary containing the hyperparameters. Examples: >>> shd = SklearnHyperDict() >>> hyperparams = shd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d __init__ () \u00b6 Initialize the SklearnHyperDict object. Examples: >>> shd = SklearnHyperDict () Source code in spotPython/data/sklearn_hyper_dict.py 14 15 16 17 18 19 20 21 22 def __init__ ( self ): \"\"\"Initialize the SklearnHyperDict object. Examples: >>> shd = SklearnHyperDict() \"\"\" super () . __init__ ( filename = \"sklearn_hyper_dict.json\" , ) load () \u00b6 Load the hyperparameters from the file. Returns: Type Description dict A dictionary containing the hyperparameters. Examples: >>> shd = SklearnHyperDict () >>> hyperparams = shd . load () >>> print ( hyperparams ) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} Source code in spotPython/data/sklearn_hyper_dict.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: (dict): A dictionary containing the hyperparameters. Examples: >>> shd = SklearnHyperDict() >>> hyperparams = shd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d","title":"sklearn_hyper_dict"},{"location":"reference/spotPython/data/sklearn_hyper_dict/#spotPython.data.sklearn_hyper_dict.SklearnHyperDict","text":"Bases: base . FileConfig Scikit-learn hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: Name Type Description filename str The name of the file where the hyperparameters are stored. Source code in spotPython/data/sklearn_hyper_dict.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class SklearnHyperDict ( base . FileConfig ): \"\"\"Scikit-learn hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: filename (str): The name of the file where the hyperparameters are stored. \"\"\" def __init__ ( self ): \"\"\"Initialize the SklearnHyperDict object. Examples: >>> shd = SklearnHyperDict() \"\"\" super () . __init__ ( filename = \"sklearn_hyper_dict.json\" , ) def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: (dict): A dictionary containing the hyperparameters. Examples: >>> shd = SklearnHyperDict() >>> hyperparams = shd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d","title":"SklearnHyperDict"},{"location":"reference/spotPython/data/sklearn_hyper_dict/#spotPython.data.sklearn_hyper_dict.SklearnHyperDict.__init__","text":"Initialize the SklearnHyperDict object. Examples: >>> shd = SklearnHyperDict () Source code in spotPython/data/sklearn_hyper_dict.py 14 15 16 17 18 19 20 21 22 def __init__ ( self ): \"\"\"Initialize the SklearnHyperDict object. Examples: >>> shd = SklearnHyperDict() \"\"\" super () . __init__ ( filename = \"sklearn_hyper_dict.json\" , )","title":"__init__()"},{"location":"reference/spotPython/data/sklearn_hyper_dict/#spotPython.data.sklearn_hyper_dict.SklearnHyperDict.load","text":"Load the hyperparameters from the file. Returns: Type Description dict A dictionary containing the hyperparameters. Examples: >>> shd = SklearnHyperDict () >>> hyperparams = shd . load () >>> print ( hyperparams ) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} Source code in spotPython/data/sklearn_hyper_dict.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: (dict): A dictionary containing the hyperparameters. Examples: >>> shd = SklearnHyperDict() >>> hyperparams = shd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d","title":"load()"},{"location":"reference/spotPython/data/torch_hyper_dict/","text":"TorchHyperDict \u00b6 Bases: base . FileConfig PyTorch hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: Name Type Description filename str The name of the file where the hyperparameters are stored. Source code in spotPython/data/torch_hyper_dict.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class TorchHyperDict ( base . FileConfig ): \"\"\"PyTorch hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: filename (str): The name of the file where the hyperparameters are stored. \"\"\" def __init__ ( self ): \"\"\"Initialize the TorchHyperDict object. Examples: >>> thd = TorchHyperDict() \"\"\" super () . __init__ ( filename = \"torch_hyper_dict.json\" , ) def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: (dict): A dictionary containing the hyperparameters. Examples: >>> thd = TorchHyperDict() >>> hyperparams = thd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d __init__ () \u00b6 Initialize the TorchHyperDict object. Examples: >>> thd = TorchHyperDict () Source code in spotPython/data/torch_hyper_dict.py 14 15 16 17 18 19 20 21 def __init__ ( self ): \"\"\"Initialize the TorchHyperDict object. Examples: >>> thd = TorchHyperDict() \"\"\" super () . __init__ ( filename = \"torch_hyper_dict.json\" , ) load () \u00b6 Load the hyperparameters from the file. Returns: Type Description dict A dictionary containing the hyperparameters. Examples: >>> thd = TorchHyperDict () >>> hyperparams = thd . load () >>> print ( hyperparams ) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} Source code in spotPython/data/torch_hyper_dict.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: (dict): A dictionary containing the hyperparameters. Examples: >>> thd = TorchHyperDict() >>> hyperparams = thd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d","title":"torch_hyper_dict"},{"location":"reference/spotPython/data/torch_hyper_dict/#spotPython.data.torch_hyper_dict.TorchHyperDict","text":"Bases: base . FileConfig PyTorch hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: Name Type Description filename str The name of the file where the hyperparameters are stored. Source code in spotPython/data/torch_hyper_dict.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class TorchHyperDict ( base . FileConfig ): \"\"\"PyTorch hyperparameter dictionary. This class extends the FileConfig class to provide a dictionary for storing hyperparameters. Attributes: filename (str): The name of the file where the hyperparameters are stored. \"\"\" def __init__ ( self ): \"\"\"Initialize the TorchHyperDict object. Examples: >>> thd = TorchHyperDict() \"\"\" super () . __init__ ( filename = \"torch_hyper_dict.json\" , ) def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: (dict): A dictionary containing the hyperparameters. Examples: >>> thd = TorchHyperDict() >>> hyperparams = thd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d","title":"TorchHyperDict"},{"location":"reference/spotPython/data/torch_hyper_dict/#spotPython.data.torch_hyper_dict.TorchHyperDict.__init__","text":"Initialize the TorchHyperDict object. Examples: >>> thd = TorchHyperDict () Source code in spotPython/data/torch_hyper_dict.py 14 15 16 17 18 19 20 21 def __init__ ( self ): \"\"\"Initialize the TorchHyperDict object. Examples: >>> thd = TorchHyperDict() \"\"\" super () . __init__ ( filename = \"torch_hyper_dict.json\" , )","title":"__init__()"},{"location":"reference/spotPython/data/torch_hyper_dict/#spotPython.data.torch_hyper_dict.TorchHyperDict.load","text":"Load the hyperparameters from the file. Returns: Type Description dict A dictionary containing the hyperparameters. Examples: >>> thd = TorchHyperDict () >>> hyperparams = thd . load () >>> print ( hyperparams ) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} Source code in spotPython/data/torch_hyper_dict.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def load ( self ) -> dict : \"\"\"Load the hyperparameters from the file. Returns: (dict): A dictionary containing the hyperparameters. Examples: >>> thd = TorchHyperDict() >>> hyperparams = thd.load() >>> print(hyperparams) {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10} \"\"\" with open ( self . path , \"r\" ) as f : d = json . load ( f ) return d","title":"load()"},{"location":"reference/spotPython/data/torchdata/","text":"load_data_cifar10 ( data_dir = './data' ) \u00b6 Load the CIFAR-10 dataset. This function loads the CIFAR-10 dataset using the torchvision library. The data is split into a training set and a test set. Parameters: Name Type Description Default data_dir str The directory where the data is stored. Defaults to \u201c./data\u201d. './data' Returns: Type Description Tuple [ datasets . CIFAR10 , datasets . CIFAR10 ] Tuple[datasets.CIFAR10, datasets.CIFAR10]: A tuple containing the training set and the test set. Examples: >>> trainset , testset = load_data_cifar10 () >>> print ( f \"Training set size: { len ( trainset ) } \" ) Training set size: 50000 >>> print ( f \"Test set size: { len ( testset ) } \" ) Test set size: 10000 Source code in spotPython/data/torchdata.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def load_data_cifar10 ( data_dir : str = \"./data\" ) -> Tuple [ datasets . CIFAR10 , datasets . CIFAR10 ]: \"\"\"Load the CIFAR-10 dataset. This function loads the CIFAR-10 dataset using the torchvision library. The data is split into a training set and a test set. Args: data_dir (str): The directory where the data is stored. Defaults to \"./data\". Returns: Tuple[datasets.CIFAR10, datasets.CIFAR10]: A tuple containing the training set and the test set. Examples: >>> trainset, testset = load_data_cifar10() >>> print(f\"Training set size: {len(trainset)}\") Training set size: 50000 >>> print(f\"Test set size: {len(testset)}\") Test set size: 10000 \"\"\" transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = datasets . CIFAR10 ( root = data_dir , train = True , download = True , transform = transform ) testset = datasets . CIFAR10 ( root = data_dir , train = False , download = True , transform = transform ) return trainset , testset","title":"torchdata"},{"location":"reference/spotPython/data/torchdata/#spotPython.data.torchdata.load_data_cifar10","text":"Load the CIFAR-10 dataset. This function loads the CIFAR-10 dataset using the torchvision library. The data is split into a training set and a test set. Parameters: Name Type Description Default data_dir str The directory where the data is stored. Defaults to \u201c./data\u201d. './data' Returns: Type Description Tuple [ datasets . CIFAR10 , datasets . CIFAR10 ] Tuple[datasets.CIFAR10, datasets.CIFAR10]: A tuple containing the training set and the test set. Examples: >>> trainset , testset = load_data_cifar10 () >>> print ( f \"Training set size: { len ( trainset ) } \" ) Training set size: 50000 >>> print ( f \"Test set size: { len ( testset ) } \" ) Test set size: 10000 Source code in spotPython/data/torchdata.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def load_data_cifar10 ( data_dir : str = \"./data\" ) -> Tuple [ datasets . CIFAR10 , datasets . CIFAR10 ]: \"\"\"Load the CIFAR-10 dataset. This function loads the CIFAR-10 dataset using the torchvision library. The data is split into a training set and a test set. Args: data_dir (str): The directory where the data is stored. Defaults to \"./data\". Returns: Tuple[datasets.CIFAR10, datasets.CIFAR10]: A tuple containing the training set and the test set. Examples: >>> trainset, testset = load_data_cifar10() >>> print(f\"Training set size: {len(trainset)}\") Training set size: 50000 >>> print(f\"Test set size: {len(testset)}\") Test set size: 10000 \"\"\" transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = datasets . CIFAR10 ( root = data_dir , train = True , download = True , transform = transform ) testset = datasets . CIFAR10 ( root = data_dir , train = False , download = True , transform = transform ) return trainset , testset","title":"load_data_cifar10()"},{"location":"reference/spotPython/data/vbdp/","text":"affinity_propagation_features ( X ) \u00b6 Clusters the features of a dataframe using Affinity Propagation. This function takes a dataframe with features and clusters them using the Affinity Propagation algorithm. The resulting dataframe contains the original features as well as a new feature representing the cluster labels. Parameters: Name Type Description Default X pd . DataFrame A dataframe with features. required Returns: Type Description pd . DataFrame A dataframe with the original features and a new cluster feature. Examples: >>> df = pd . DataFrame ({ \"a\" : [ True , False , True ], \"b\" : [ True , True , False ], \"c\" : [ False , False , True ]}) >>> df a b c 0 True True False 1 False True False 2 True False True >>> affinity_propagation_features ( df ) Estimated number of clusters: 3 a b c cluster 0 True True False 0 1 False True False 1 2 True False True 2 Source code in spotPython/data/vbdp.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def affinity_propagation_features ( X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Clusters the features of a dataframe using Affinity Propagation. This function takes a dataframe with features and clusters them using the Affinity Propagation algorithm. The resulting dataframe contains the original features as well as a new feature representing the cluster labels. Args: X (pd.DataFrame): A dataframe with features. Returns: (pd.DataFrame): A dataframe with the original features and a new cluster feature. Examples: >>> df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]}) >>> df a b c 0 True True False 1 False True False 2 True False True >>> affinity_propagation_features(df) Estimated number of clusters: 3 a b c cluster 0 True True False 0 1 False True False 1 2 True False True 2 \"\"\" D = manhattan_distances ( X ) af = AffinityPropagation ( random_state = 0 , affinity = \"precomputed\" ) . fit ( D ) cluster_centers_indices = af . cluster_centers_indices_ n_clusters_ = len ( cluster_centers_indices ) print ( \"Estimated number of clusters: %d \" % n_clusters_ ) X [ \"cluster\" ] = af . labels_ return X cluster_features ( X ) \u00b6 Clusters the features of a dataframe based on similarity. This function takes a dataframe with features and clusters them based on similarity. The resulting dataframe contains the original features as well as new features representing the clusters. Parameters: Name Type Description Default X pd . DataFrame A dataframe with features. required Returns: Type Description pd . DataFrame A dataframe with the original features and new cluster features. Examples: >>> df = pd . DataFrame ({ \"a\" : [ True , False , True ], \"b\" : [ True , True , False ], \"c\" : [ False , False , True ]}) >>> df a b c 0 True True False 1 False True False 2 True False True >>> cluster_features ( df ) a b c c_0 c_1 c_2 c_3 0 True True False 0 0 0 0 1 False True False 0 0 0 0 2 True False True 0 0 0 0 Source code in spotPython/data/vbdp.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def cluster_features ( X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Clusters the features of a dataframe based on similarity. This function takes a dataframe with features and clusters them based on similarity. The resulting dataframe contains the original features as well as new features representing the clusters. Args: X (pd.DataFrame): A dataframe with features. Returns: (pd.DataFrame): A dataframe with the original features and new cluster features. Examples: >>> df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]}) >>> df a b c 0 True True False 1 False True False 2 True False True >>> cluster_features(df) a b c c_0 c_1 c_2 c_3 0 True True False 0 0 0 0 1 False True False 0 0 0 0 2 True False True 0 0 0 0 \"\"\" c_0 = X . columns [ X . columns . str . contains ( \"pain\" )] c_1 = X . columns [ X . columns . str . contains ( \"inflammation\" )] c_2 = X . columns [ X . columns . str . contains ( \"bleed\" )] c_3 = X . columns [ X . columns . str . contains ( \"skin\" )] X [ \"c_0\" ] = X [ c_0 ] . sum ( axis = 1 ) X [ \"c_1\" ] = X [ c_1 ] . sum ( axis = 1 ) X [ \"c_2\" ] = X [ c_2 ] . sum ( axis = 1 ) X [ \"c_3\" ] = X [ c_3 ] . sum ( axis = 1 ) return X","title":"vbdp"},{"location":"reference/spotPython/data/vbdp/#spotPython.data.vbdp.affinity_propagation_features","text":"Clusters the features of a dataframe using Affinity Propagation. This function takes a dataframe with features and clusters them using the Affinity Propagation algorithm. The resulting dataframe contains the original features as well as a new feature representing the cluster labels. Parameters: Name Type Description Default X pd . DataFrame A dataframe with features. required Returns: Type Description pd . DataFrame A dataframe with the original features and a new cluster feature. Examples: >>> df = pd . DataFrame ({ \"a\" : [ True , False , True ], \"b\" : [ True , True , False ], \"c\" : [ False , False , True ]}) >>> df a b c 0 True True False 1 False True False 2 True False True >>> affinity_propagation_features ( df ) Estimated number of clusters: 3 a b c cluster 0 True True False 0 1 False True False 1 2 True False True 2 Source code in spotPython/data/vbdp.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def affinity_propagation_features ( X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Clusters the features of a dataframe using Affinity Propagation. This function takes a dataframe with features and clusters them using the Affinity Propagation algorithm. The resulting dataframe contains the original features as well as a new feature representing the cluster labels. Args: X (pd.DataFrame): A dataframe with features. Returns: (pd.DataFrame): A dataframe with the original features and a new cluster feature. Examples: >>> df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]}) >>> df a b c 0 True True False 1 False True False 2 True False True >>> affinity_propagation_features(df) Estimated number of clusters: 3 a b c cluster 0 True True False 0 1 False True False 1 2 True False True 2 \"\"\" D = manhattan_distances ( X ) af = AffinityPropagation ( random_state = 0 , affinity = \"precomputed\" ) . fit ( D ) cluster_centers_indices = af . cluster_centers_indices_ n_clusters_ = len ( cluster_centers_indices ) print ( \"Estimated number of clusters: %d \" % n_clusters_ ) X [ \"cluster\" ] = af . labels_ return X","title":"affinity_propagation_features()"},{"location":"reference/spotPython/data/vbdp/#spotPython.data.vbdp.cluster_features","text":"Clusters the features of a dataframe based on similarity. This function takes a dataframe with features and clusters them based on similarity. The resulting dataframe contains the original features as well as new features representing the clusters. Parameters: Name Type Description Default X pd . DataFrame A dataframe with features. required Returns: Type Description pd . DataFrame A dataframe with the original features and new cluster features. Examples: >>> df = pd . DataFrame ({ \"a\" : [ True , False , True ], \"b\" : [ True , True , False ], \"c\" : [ False , False , True ]}) >>> df a b c 0 True True False 1 False True False 2 True False True >>> cluster_features ( df ) a b c c_0 c_1 c_2 c_3 0 True True False 0 0 0 0 1 False True False 0 0 0 0 2 True False True 0 0 0 0 Source code in spotPython/data/vbdp.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def cluster_features ( X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Clusters the features of a dataframe based on similarity. This function takes a dataframe with features and clusters them based on similarity. The resulting dataframe contains the original features as well as new features representing the clusters. Args: X (pd.DataFrame): A dataframe with features. Returns: (pd.DataFrame): A dataframe with the original features and new cluster features. Examples: >>> df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]}) >>> df a b c 0 True True False 1 False True False 2 True False True >>> cluster_features(df) a b c c_0 c_1 c_2 c_3 0 True True False 0 0 0 0 1 False True False 0 0 0 0 2 True False True 0 0 0 0 \"\"\" c_0 = X . columns [ X . columns . str . contains ( \"pain\" )] c_1 = X . columns [ X . columns . str . contains ( \"inflammation\" )] c_2 = X . columns [ X . columns . str . contains ( \"bleed\" )] c_3 = X . columns [ X . columns . str . contains ( \"skin\" )] X [ \"c_0\" ] = X [ c_0 ] . sum ( axis = 1 ) X [ \"c_1\" ] = X [ c_1 ] . sum ( axis = 1 ) X [ \"c_2\" ] = X [ c_2 ] . sum ( axis = 1 ) X [ \"c_3\" ] = X [ c_3 ] . sum ( axis = 1 ) return X","title":"cluster_features()"},{"location":"reference/spotPython/design/designs/","text":"designs \u00b6 Super class for all design classes (factorial and spacefilling). Attributes: Name Type Description designs List A list of designs. k int The dimension of the design. seed int The seed for the random number generator. rng Generator A random number generator instance. Source code in spotPython/design/designs.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class designs : \"\"\" Super class for all design classes (factorial and spacefilling). Attributes: designs (List): A list of designs. k (int): The dimension of the design. seed (int): The seed for the random number generator. rng (Generator): A random number generator instance. \"\"\" def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Initializes a Designs object with the given dimension and seed. Args: k (int): The dimension of the design. Defaults to 2. seed (int): The seed for the random number generator. Defaults to 123. Examples: >>> designs = designs(k=2, seed=123) >>> designs.get_dim() 2 \"\"\" self . designs : List = [] self . k : int = k self . seed : int = seed self . rng = default_rng ( self . seed ) def get_dim ( self ) -> int : \"\"\" Returns the dimension of the design. Returns: int: The dimension of the design. \"\"\" return self . k __init__ ( k = 2 , seed = 123 ) \u00b6 Initializes a Designs object with the given dimension and seed. Parameters: Name Type Description Default k int The dimension of the design. Defaults to 2. 2 seed int The seed for the random number generator. Defaults to 123. 123 Examples: >>> designs = designs ( k = 2 , seed = 123 ) >>> designs . get_dim () 2 Source code in spotPython/design/designs.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Initializes a Designs object with the given dimension and seed. Args: k (int): The dimension of the design. Defaults to 2. seed (int): The seed for the random number generator. Defaults to 123. Examples: >>> designs = designs(k=2, seed=123) >>> designs.get_dim() 2 \"\"\" self . designs : List = [] self . k : int = k self . seed : int = seed self . rng = default_rng ( self . seed ) get_dim () \u00b6 Returns the dimension of the design. Returns: Name Type Description int int The dimension of the design. Source code in spotPython/design/designs.py 40 41 42 43 44 45 46 47 def get_dim ( self ) -> int : \"\"\" Returns the dimension of the design. Returns: int: The dimension of the design. \"\"\" return self . k","title":"designs"},{"location":"reference/spotPython/design/designs/#spotPython.design.designs.designs","text":"Super class for all design classes (factorial and spacefilling). Attributes: Name Type Description designs List A list of designs. k int The dimension of the design. seed int The seed for the random number generator. rng Generator A random number generator instance. Source code in spotPython/design/designs.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class designs : \"\"\" Super class for all design classes (factorial and spacefilling). Attributes: designs (List): A list of designs. k (int): The dimension of the design. seed (int): The seed for the random number generator. rng (Generator): A random number generator instance. \"\"\" def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Initializes a Designs object with the given dimension and seed. Args: k (int): The dimension of the design. Defaults to 2. seed (int): The seed for the random number generator. Defaults to 123. Examples: >>> designs = designs(k=2, seed=123) >>> designs.get_dim() 2 \"\"\" self . designs : List = [] self . k : int = k self . seed : int = seed self . rng = default_rng ( self . seed ) def get_dim ( self ) -> int : \"\"\" Returns the dimension of the design. Returns: int: The dimension of the design. \"\"\" return self . k","title":"designs"},{"location":"reference/spotPython/design/designs/#spotPython.design.designs.designs.__init__","text":"Initializes a Designs object with the given dimension and seed. Parameters: Name Type Description Default k int The dimension of the design. Defaults to 2. 2 seed int The seed for the random number generator. Defaults to 123. 123 Examples: >>> designs = designs ( k = 2 , seed = 123 ) >>> designs . get_dim () 2 Source code in spotPython/design/designs.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Initializes a Designs object with the given dimension and seed. Args: k (int): The dimension of the design. Defaults to 2. seed (int): The seed for the random number generator. Defaults to 123. Examples: >>> designs = designs(k=2, seed=123) >>> designs.get_dim() 2 \"\"\" self . designs : List = [] self . k : int = k self . seed : int = seed self . rng = default_rng ( self . seed )","title":"__init__()"},{"location":"reference/spotPython/design/designs/#spotPython.design.designs.designs.get_dim","text":"Returns the dimension of the design. Returns: Name Type Description int int The dimension of the design. Source code in spotPython/design/designs.py 40 41 42 43 44 45 46 47 def get_dim ( self ) -> int : \"\"\" Returns the dimension of the design. Returns: int: The dimension of the design. \"\"\" return self . k","title":"get_dim()"},{"location":"reference/spotPython/design/factorial/","text":"factorial \u00b6 Bases: designs Super class for factorial designs. Attributes: Name Type Description k int The number of factors. seed int The seed for the random number generator. Source code in spotPython/design/factorial.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class factorial ( designs ): \"\"\" Super class for factorial designs. Attributes: k (int): The number of factors. seed (int): The seed for the random number generator. \"\"\" def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Initializes a factorial design object. Args: k (int): The number of factors. Defaults to 2. seed (int): The seed for the random number generator. Defaults to 123. \"\"\" super () . __init__ ( k , seed ) def full_factorial ( self , p : int ) -> \"np.ndarray\" : \"\"\" Generates a full factorial design. Args: p (int): The number of levels for each factor. Returns: numpy.ndarray: A 2D array representing the full factorial design. Examples: >>> from spotPython.design.factorial import factorial factorial_design = factorial(k=2) factorial_design.full_factorial(p=2) array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]]) \"\"\" i = ( slice ( 0 , 1 , p * 1 j ),) * self . k return mgrid [ i ] . reshape ( self . k , p ** self . k ) . T __init__ ( k = 2 , seed = 123 ) \u00b6 Initializes a factorial design object. Parameters: Name Type Description Default k int The number of factors. Defaults to 2. 2 seed int The seed for the random number generator. Defaults to 123. 123 Source code in spotPython/design/factorial.py 15 16 17 18 19 20 21 22 23 def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Initializes a factorial design object. Args: k (int): The number of factors. Defaults to 2. seed (int): The seed for the random number generator. Defaults to 123. \"\"\" super () . __init__ ( k , seed ) full_factorial ( p ) \u00b6 Generates a full factorial design. Parameters: Name Type Description Default p int The number of levels for each factor. required Returns: Type Description ndarray numpy.ndarray: A 2D array representing the full factorial design. Examples: >>> from spotPython.design.factorial import factorial factorial_design = factorial(k=2) factorial_design.full_factorial(p=2) array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]]) Source code in spotPython/design/factorial.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def full_factorial ( self , p : int ) -> \"np.ndarray\" : \"\"\" Generates a full factorial design. Args: p (int): The number of levels for each factor. Returns: numpy.ndarray: A 2D array representing the full factorial design. Examples: >>> from spotPython.design.factorial import factorial factorial_design = factorial(k=2) factorial_design.full_factorial(p=2) array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]]) \"\"\" i = ( slice ( 0 , 1 , p * 1 j ),) * self . k return mgrid [ i ] . reshape ( self . k , p ** self . k ) . T","title":"factorial"},{"location":"reference/spotPython/design/factorial/#spotPython.design.factorial.factorial","text":"Bases: designs Super class for factorial designs. Attributes: Name Type Description k int The number of factors. seed int The seed for the random number generator. Source code in spotPython/design/factorial.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class factorial ( designs ): \"\"\" Super class for factorial designs. Attributes: k (int): The number of factors. seed (int): The seed for the random number generator. \"\"\" def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Initializes a factorial design object. Args: k (int): The number of factors. Defaults to 2. seed (int): The seed for the random number generator. Defaults to 123. \"\"\" super () . __init__ ( k , seed ) def full_factorial ( self , p : int ) -> \"np.ndarray\" : \"\"\" Generates a full factorial design. Args: p (int): The number of levels for each factor. Returns: numpy.ndarray: A 2D array representing the full factorial design. Examples: >>> from spotPython.design.factorial import factorial factorial_design = factorial(k=2) factorial_design.full_factorial(p=2) array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]]) \"\"\" i = ( slice ( 0 , 1 , p * 1 j ),) * self . k return mgrid [ i ] . reshape ( self . k , p ** self . k ) . T","title":"factorial"},{"location":"reference/spotPython/design/factorial/#spotPython.design.factorial.factorial.__init__","text":"Initializes a factorial design object. Parameters: Name Type Description Default k int The number of factors. Defaults to 2. 2 seed int The seed for the random number generator. Defaults to 123. 123 Source code in spotPython/design/factorial.py 15 16 17 18 19 20 21 22 23 def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Initializes a factorial design object. Args: k (int): The number of factors. Defaults to 2. seed (int): The seed for the random number generator. Defaults to 123. \"\"\" super () . __init__ ( k , seed )","title":"__init__()"},{"location":"reference/spotPython/design/factorial/#spotPython.design.factorial.factorial.full_factorial","text":"Generates a full factorial design. Parameters: Name Type Description Default p int The number of levels for each factor. required Returns: Type Description ndarray numpy.ndarray: A 2D array representing the full factorial design. Examples: >>> from spotPython.design.factorial import factorial factorial_design = factorial(k=2) factorial_design.full_factorial(p=2) array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]]) Source code in spotPython/design/factorial.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def full_factorial ( self , p : int ) -> \"np.ndarray\" : \"\"\" Generates a full factorial design. Args: p (int): The number of levels for each factor. Returns: numpy.ndarray: A 2D array representing the full factorial design. Examples: >>> from spotPython.design.factorial import factorial factorial_design = factorial(k=2) factorial_design.full_factorial(p=2) array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]]) \"\"\" i = ( slice ( 0 , 1 , p * 1 j ),) * self . k return mgrid [ i ] . reshape ( self . k , p ** self . k ) . T","title":"full_factorial()"},{"location":"reference/spotPython/design/spacefilling/","text":"spacefilling \u00b6 Bases: designs Source code in spotPython/design/spacefilling.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class spacefilling ( designs ): def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Spacefilling design class Args: k (int, optional): number of design variables (dimensions). Defaults to 2. seed (int, optional): random seed. Defaults to 123. \"\"\" self . k = k self . seed = seed super () . __init__ ( k , seed ) self . sampler = LatinHypercube ( d = self . k , seed = self . seed ) def scipy_lhd ( self , n : int , repeats : int = 1 , lower : Optional [ Union [ int , float ]] = None , upper : Optional [ Union [ int , float ]] = None , ) -> ndarray : \"\"\" Latin hypercube sampling based on scipy. Args: n (int): number of samples repeats (int): number of repeats (replicates) lower (int or float, optional): lower bound. Defaults to 0. upper (int or float, optional): upper bound. Defaults to 1. Returns: (ndarray): Latin hypercube design. Examples: >>> from spotPython.design.spacefilling import spacefilling import numpy as np lhd = spacefilling(k=2, seed=123) lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0,0]), upper=np.array([1,1])) array([[0.66352963, 0.5892358 ], [0.66352963, 0.5892358 ], [0.55592803, 0.96312564], [0.55592803, 0.96312564], [0.16481882, 0.0375811 ], [0.16481882, 0.0375811 ], [0.215331 , 0.34468512], [0.215331 , 0.34468512], [0.83604909, 0.62202146], [0.83604909, 0.62202146]]) \"\"\" if lower is None : lower = zeros ( self . k ) if upper is None : upper = ones ( self . k ) sample = self . sampler . random ( n = n ) des = scale ( sample , lower , upper ) return repeat ( des , repeats , axis = 0 ) __init__ ( k = 2 , seed = 123 ) \u00b6 Spacefilling design class Parameters: Name Type Description Default k int number of design variables (dimensions). Defaults to 2. 2 seed int random seed. Defaults to 123. 123 Source code in spotPython/design/spacefilling.py 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Spacefilling design class Args: k (int, optional): number of design variables (dimensions). Defaults to 2. seed (int, optional): random seed. Defaults to 123. \"\"\" self . k = k self . seed = seed super () . __init__ ( k , seed ) self . sampler = LatinHypercube ( d = self . k , seed = self . seed ) scipy_lhd ( n , repeats = 1 , lower = None , upper = None ) \u00b6 Latin hypercube sampling based on scipy. Parameters: Name Type Description Default n int number of samples required repeats int number of repeats (replicates) 1 lower int or float lower bound. Defaults to 0. None upper int or float upper bound. Defaults to 1. None Returns: Type Description ndarray Latin hypercube design. Examples: >>> from spotPython.design.spacefilling import spacefilling import numpy as np lhd = spacefilling(k=2, seed=123) lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0,0]), upper=np.array([1,1])) array([[0.66352963, 0.5892358 ], [0.66352963, 0.5892358 ], [0.55592803, 0.96312564], [0.55592803, 0.96312564], [0.16481882, 0.0375811 ], [0.16481882, 0.0375811 ], [0.215331 , 0.34468512], [0.215331 , 0.34468512], [0.83604909, 0.62202146], [0.83604909, 0.62202146]]) Source code in spotPython/design/spacefilling.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def scipy_lhd ( self , n : int , repeats : int = 1 , lower : Optional [ Union [ int , float ]] = None , upper : Optional [ Union [ int , float ]] = None , ) -> ndarray : \"\"\" Latin hypercube sampling based on scipy. Args: n (int): number of samples repeats (int): number of repeats (replicates) lower (int or float, optional): lower bound. Defaults to 0. upper (int or float, optional): upper bound. Defaults to 1. Returns: (ndarray): Latin hypercube design. Examples: >>> from spotPython.design.spacefilling import spacefilling import numpy as np lhd = spacefilling(k=2, seed=123) lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0,0]), upper=np.array([1,1])) array([[0.66352963, 0.5892358 ], [0.66352963, 0.5892358 ], [0.55592803, 0.96312564], [0.55592803, 0.96312564], [0.16481882, 0.0375811 ], [0.16481882, 0.0375811 ], [0.215331 , 0.34468512], [0.215331 , 0.34468512], [0.83604909, 0.62202146], [0.83604909, 0.62202146]]) \"\"\" if lower is None : lower = zeros ( self . k ) if upper is None : upper = ones ( self . k ) sample = self . sampler . random ( n = n ) des = scale ( sample , lower , upper ) return repeat ( des , repeats , axis = 0 )","title":"spacefilling"},{"location":"reference/spotPython/design/spacefilling/#spotPython.design.spacefilling.spacefilling","text":"Bases: designs Source code in spotPython/design/spacefilling.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class spacefilling ( designs ): def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Spacefilling design class Args: k (int, optional): number of design variables (dimensions). Defaults to 2. seed (int, optional): random seed. Defaults to 123. \"\"\" self . k = k self . seed = seed super () . __init__ ( k , seed ) self . sampler = LatinHypercube ( d = self . k , seed = self . seed ) def scipy_lhd ( self , n : int , repeats : int = 1 , lower : Optional [ Union [ int , float ]] = None , upper : Optional [ Union [ int , float ]] = None , ) -> ndarray : \"\"\" Latin hypercube sampling based on scipy. Args: n (int): number of samples repeats (int): number of repeats (replicates) lower (int or float, optional): lower bound. Defaults to 0. upper (int or float, optional): upper bound. Defaults to 1. Returns: (ndarray): Latin hypercube design. Examples: >>> from spotPython.design.spacefilling import spacefilling import numpy as np lhd = spacefilling(k=2, seed=123) lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0,0]), upper=np.array([1,1])) array([[0.66352963, 0.5892358 ], [0.66352963, 0.5892358 ], [0.55592803, 0.96312564], [0.55592803, 0.96312564], [0.16481882, 0.0375811 ], [0.16481882, 0.0375811 ], [0.215331 , 0.34468512], [0.215331 , 0.34468512], [0.83604909, 0.62202146], [0.83604909, 0.62202146]]) \"\"\" if lower is None : lower = zeros ( self . k ) if upper is None : upper = ones ( self . k ) sample = self . sampler . random ( n = n ) des = scale ( sample , lower , upper ) return repeat ( des , repeats , axis = 0 )","title":"spacefilling"},{"location":"reference/spotPython/design/spacefilling/#spotPython.design.spacefilling.spacefilling.__init__","text":"Spacefilling design class Parameters: Name Type Description Default k int number of design variables (dimensions). Defaults to 2. 2 seed int random seed. Defaults to 123. 123 Source code in spotPython/design/spacefilling.py 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , k : int = 2 , seed : int = 123 ) -> None : \"\"\" Spacefilling design class Args: k (int, optional): number of design variables (dimensions). Defaults to 2. seed (int, optional): random seed. Defaults to 123. \"\"\" self . k = k self . seed = seed super () . __init__ ( k , seed ) self . sampler = LatinHypercube ( d = self . k , seed = self . seed )","title":"__init__()"},{"location":"reference/spotPython/design/spacefilling/#spotPython.design.spacefilling.spacefilling.scipy_lhd","text":"Latin hypercube sampling based on scipy. Parameters: Name Type Description Default n int number of samples required repeats int number of repeats (replicates) 1 lower int or float lower bound. Defaults to 0. None upper int or float upper bound. Defaults to 1. None Returns: Type Description ndarray Latin hypercube design. Examples: >>> from spotPython.design.spacefilling import spacefilling import numpy as np lhd = spacefilling(k=2, seed=123) lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0,0]), upper=np.array([1,1])) array([[0.66352963, 0.5892358 ], [0.66352963, 0.5892358 ], [0.55592803, 0.96312564], [0.55592803, 0.96312564], [0.16481882, 0.0375811 ], [0.16481882, 0.0375811 ], [0.215331 , 0.34468512], [0.215331 , 0.34468512], [0.83604909, 0.62202146], [0.83604909, 0.62202146]]) Source code in spotPython/design/spacefilling.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def scipy_lhd ( self , n : int , repeats : int = 1 , lower : Optional [ Union [ int , float ]] = None , upper : Optional [ Union [ int , float ]] = None , ) -> ndarray : \"\"\" Latin hypercube sampling based on scipy. Args: n (int): number of samples repeats (int): number of repeats (replicates) lower (int or float, optional): lower bound. Defaults to 0. upper (int or float, optional): upper bound. Defaults to 1. Returns: (ndarray): Latin hypercube design. Examples: >>> from spotPython.design.spacefilling import spacefilling import numpy as np lhd = spacefilling(k=2, seed=123) lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0,0]), upper=np.array([1,1])) array([[0.66352963, 0.5892358 ], [0.66352963, 0.5892358 ], [0.55592803, 0.96312564], [0.55592803, 0.96312564], [0.16481882, 0.0375811 ], [0.16481882, 0.0375811 ], [0.215331 , 0.34468512], [0.215331 , 0.34468512], [0.83604909, 0.62202146], [0.83604909, 0.62202146]]) \"\"\" if lower is None : lower = zeros ( self . k ) if upper is None : upper = ones ( self . k ) sample = self . sampler . random ( n = n ) des = scale ( sample , lower , upper ) return repeat ( des , repeats , axis = 0 )","title":"scipy_lhd()"},{"location":"reference/spotPython/fun/hyperlight/","text":"HyperLight \u00b6 Hyperparameter Tuning for Lightning. Parameters: Name Type Description Default seed int seed for the random number generator. See Numpy Random Sampling. 126 log_level int log level for the logger. 50 Attributes: Name Type Description seed int seed for the random number generator. rng Generator random number generator. fun_control dict dictionary containing control parameters for the hyperparameter tuning. log_level int log level for the logger. Examples: >>> hyper_light = HyperLight ( seed = 126 , log_level = 50 ) >>> print ( hyper_light . seed ) 126 Source code in spotPython/fun/hyperlight.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class HyperLight : \"\"\" Hyperparameter Tuning for Lightning. Args: seed (int): seed for the random number generator. See Numpy Random Sampling. log_level (int): log level for the logger. Attributes: seed (int): seed for the random number generator. rng (Generator): random number generator. fun_control (dict): dictionary containing control parameters for the hyperparameter tuning. log_level (int): log level for the logger. Examples: >>> hyper_light = HyperLight(seed=126, log_level=50) >>> print(hyper_light.seed) 126 \"\"\" def __init__ ( self , seed : int = 126 , log_level : int = 50 ) -> None : self . seed = seed self . rng = default_rng ( seed = self . seed ) self . fun_control = { \"seed\" : None , \"data\" : None , \"step\" : 10_000 , \"horizon\" : None , \"grace_period\" : None , \"metric_river\" : None , \"metric_sklearn\" : None , \"weights\" : array ([ 1 , 0 , 0 ]), \"weight_coeff\" : 0.0 , \"log_level\" : log_level , \"var_name\" : [], \"var_type\" : [], } self . log_level = self . fun_control [ \"log_level\" ] logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) def check_X_shape ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Checks the shape of the input array X and raises an exception if it is not valid. Args: X (np.ndarray): input array. Returns: np.ndarray: input array with valid shape. Raises: Exception: if the shape of the input array is not valid. Examples: >>> hyper_light = HyperLight(seed=126, log_level=50) >>> X = np.array([[1, 2], [3, 4]]) >>> hyper_light.check_X_shape(X) array([[1, 2], [3, 4]]) \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception ( \"Invalid shape of input array X.\" ) return X def fun ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Evaluates the function for the given input array X and control parameters. Args: X (np.ndarray): input array. fun_control (dict): dictionary containing control parameters for the hyperparameter tuning. Returns: (np.ndarray): array containing the evaluation results. Examples: >>> hyper_light = HyperLight(seed=126, log_level=50) X = np.array([[1, 2], [3, 4]]) fun_control = {\"weights\": np.array([1, 0, 0])} hyper_light.fun(X, fun_control) array([nan, nan]) \"\"\" z_res = np . array ([], dtype = float ) if fun_control is not None : self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) # type information and transformations are considered in generate_one_config_from_var_dict: for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): logger . debug ( f \" \\n config: { config } \" ) # extract parameters like epochs, batch_size, lr, etc. from config # config_id = generate_config_id(config) try : print ( \"fun: Calling train_model\" ) df_eval = train_model ( config , self . fun_control ) print ( \"fun: train_model returned\" ) except Exception as err : logger . error ( f \"Error in fun(). Call to train_model failed. { err =} , { type ( err ) =} \" ) logger . error ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_val = self . fun_control [ \"weights\" ] * df_eval z_res = np . append ( z_res , z_val ) return z_res check_X_shape ( X ) \u00b6 Checks the shape of the input array X and raises an exception if it is not valid. Parameters: Name Type Description Default X np . ndarray input array. required Returns: Type Description np . ndarray np.ndarray: input array with valid shape. Examples: >>> hyper_light = HyperLight ( seed = 126 , log_level = 50 ) >>> X = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> hyper_light . check_X_shape ( X ) array([[1, 2], [3, 4]]) Source code in spotPython/fun/hyperlight.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def check_X_shape ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Checks the shape of the input array X and raises an exception if it is not valid. Args: X (np.ndarray): input array. Returns: np.ndarray: input array with valid shape. Raises: Exception: if the shape of the input array is not valid. Examples: >>> hyper_light = HyperLight(seed=126, log_level=50) >>> X = np.array([[1, 2], [3, 4]]) >>> hyper_light.check_X_shape(X) array([[1, 2], [3, 4]]) \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception ( \"Invalid shape of input array X.\" ) return X fun ( X , fun_control = None ) \u00b6 Evaluates the function for the given input array X and control parameters. Parameters: Name Type Description Default X np . ndarray input array. required fun_control dict dictionary containing control parameters for the hyperparameter tuning. None Returns: Type Description np . ndarray array containing the evaluation results. Examples: >>> hyper_light = HyperLight ( seed = 126 , log_level = 50 ) X = np.array([[1, 2], [3, 4]]) fun_control = {\"weights\": np.array([1, 0, 0])} hyper_light.fun(X, fun_control) array([nan, nan]) Source code in spotPython/fun/hyperlight.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def fun ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Evaluates the function for the given input array X and control parameters. Args: X (np.ndarray): input array. fun_control (dict): dictionary containing control parameters for the hyperparameter tuning. Returns: (np.ndarray): array containing the evaluation results. Examples: >>> hyper_light = HyperLight(seed=126, log_level=50) X = np.array([[1, 2], [3, 4]]) fun_control = {\"weights\": np.array([1, 0, 0])} hyper_light.fun(X, fun_control) array([nan, nan]) \"\"\" z_res = np . array ([], dtype = float ) if fun_control is not None : self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) # type information and transformations are considered in generate_one_config_from_var_dict: for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): logger . debug ( f \" \\n config: { config } \" ) # extract parameters like epochs, batch_size, lr, etc. from config # config_id = generate_config_id(config) try : print ( \"fun: Calling train_model\" ) df_eval = train_model ( config , self . fun_control ) print ( \"fun: train_model returned\" ) except Exception as err : logger . error ( f \"Error in fun(). Call to train_model failed. { err =} , { type ( err ) =} \" ) logger . error ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_val = self . fun_control [ \"weights\" ] * df_eval z_res = np . append ( z_res , z_val ) return z_res","title":"hyperlight"},{"location":"reference/spotPython/fun/hyperlight/#spotPython.fun.hyperlight.HyperLight","text":"Hyperparameter Tuning for Lightning. Parameters: Name Type Description Default seed int seed for the random number generator. See Numpy Random Sampling. 126 log_level int log level for the logger. 50 Attributes: Name Type Description seed int seed for the random number generator. rng Generator random number generator. fun_control dict dictionary containing control parameters for the hyperparameter tuning. log_level int log level for the logger. Examples: >>> hyper_light = HyperLight ( seed = 126 , log_level = 50 ) >>> print ( hyper_light . seed ) 126 Source code in spotPython/fun/hyperlight.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class HyperLight : \"\"\" Hyperparameter Tuning for Lightning. Args: seed (int): seed for the random number generator. See Numpy Random Sampling. log_level (int): log level for the logger. Attributes: seed (int): seed for the random number generator. rng (Generator): random number generator. fun_control (dict): dictionary containing control parameters for the hyperparameter tuning. log_level (int): log level for the logger. Examples: >>> hyper_light = HyperLight(seed=126, log_level=50) >>> print(hyper_light.seed) 126 \"\"\" def __init__ ( self , seed : int = 126 , log_level : int = 50 ) -> None : self . seed = seed self . rng = default_rng ( seed = self . seed ) self . fun_control = { \"seed\" : None , \"data\" : None , \"step\" : 10_000 , \"horizon\" : None , \"grace_period\" : None , \"metric_river\" : None , \"metric_sklearn\" : None , \"weights\" : array ([ 1 , 0 , 0 ]), \"weight_coeff\" : 0.0 , \"log_level\" : log_level , \"var_name\" : [], \"var_type\" : [], } self . log_level = self . fun_control [ \"log_level\" ] logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) def check_X_shape ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Checks the shape of the input array X and raises an exception if it is not valid. Args: X (np.ndarray): input array. Returns: np.ndarray: input array with valid shape. Raises: Exception: if the shape of the input array is not valid. Examples: >>> hyper_light = HyperLight(seed=126, log_level=50) >>> X = np.array([[1, 2], [3, 4]]) >>> hyper_light.check_X_shape(X) array([[1, 2], [3, 4]]) \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception ( \"Invalid shape of input array X.\" ) return X def fun ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Evaluates the function for the given input array X and control parameters. Args: X (np.ndarray): input array. fun_control (dict): dictionary containing control parameters for the hyperparameter tuning. Returns: (np.ndarray): array containing the evaluation results. Examples: >>> hyper_light = HyperLight(seed=126, log_level=50) X = np.array([[1, 2], [3, 4]]) fun_control = {\"weights\": np.array([1, 0, 0])} hyper_light.fun(X, fun_control) array([nan, nan]) \"\"\" z_res = np . array ([], dtype = float ) if fun_control is not None : self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) # type information and transformations are considered in generate_one_config_from_var_dict: for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): logger . debug ( f \" \\n config: { config } \" ) # extract parameters like epochs, batch_size, lr, etc. from config # config_id = generate_config_id(config) try : print ( \"fun: Calling train_model\" ) df_eval = train_model ( config , self . fun_control ) print ( \"fun: train_model returned\" ) except Exception as err : logger . error ( f \"Error in fun(). Call to train_model failed. { err =} , { type ( err ) =} \" ) logger . error ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_val = self . fun_control [ \"weights\" ] * df_eval z_res = np . append ( z_res , z_val ) return z_res","title":"HyperLight"},{"location":"reference/spotPython/fun/hyperlight/#spotPython.fun.hyperlight.HyperLight.check_X_shape","text":"Checks the shape of the input array X and raises an exception if it is not valid. Parameters: Name Type Description Default X np . ndarray input array. required Returns: Type Description np . ndarray np.ndarray: input array with valid shape. Examples: >>> hyper_light = HyperLight ( seed = 126 , log_level = 50 ) >>> X = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> hyper_light . check_X_shape ( X ) array([[1, 2], [3, 4]]) Source code in spotPython/fun/hyperlight.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def check_X_shape ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Checks the shape of the input array X and raises an exception if it is not valid. Args: X (np.ndarray): input array. Returns: np.ndarray: input array with valid shape. Raises: Exception: if the shape of the input array is not valid. Examples: >>> hyper_light = HyperLight(seed=126, log_level=50) >>> X = np.array([[1, 2], [3, 4]]) >>> hyper_light.check_X_shape(X) array([[1, 2], [3, 4]]) \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception ( \"Invalid shape of input array X.\" ) return X","title":"check_X_shape()"},{"location":"reference/spotPython/fun/hyperlight/#spotPython.fun.hyperlight.HyperLight.fun","text":"Evaluates the function for the given input array X and control parameters. Parameters: Name Type Description Default X np . ndarray input array. required fun_control dict dictionary containing control parameters for the hyperparameter tuning. None Returns: Type Description np . ndarray array containing the evaluation results. Examples: >>> hyper_light = HyperLight ( seed = 126 , log_level = 50 ) X = np.array([[1, 2], [3, 4]]) fun_control = {\"weights\": np.array([1, 0, 0])} hyper_light.fun(X, fun_control) array([nan, nan]) Source code in spotPython/fun/hyperlight.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def fun ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Evaluates the function for the given input array X and control parameters. Args: X (np.ndarray): input array. fun_control (dict): dictionary containing control parameters for the hyperparameter tuning. Returns: (np.ndarray): array containing the evaluation results. Examples: >>> hyper_light = HyperLight(seed=126, log_level=50) X = np.array([[1, 2], [3, 4]]) fun_control = {\"weights\": np.array([1, 0, 0])} hyper_light.fun(X, fun_control) array([nan, nan]) \"\"\" z_res = np . array ([], dtype = float ) if fun_control is not None : self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) # type information and transformations are considered in generate_one_config_from_var_dict: for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): logger . debug ( f \" \\n config: { config } \" ) # extract parameters like epochs, batch_size, lr, etc. from config # config_id = generate_config_id(config) try : print ( \"fun: Calling train_model\" ) df_eval = train_model ( config , self . fun_control ) print ( \"fun: train_model returned\" ) except Exception as err : logger . error ( f \"Error in fun(). Call to train_model failed. { err =} , { type ( err ) =} \" ) logger . error ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_val = self . fun_control [ \"weights\" ] * df_eval z_res = np . append ( z_res , z_val ) return z_res","title":"fun()"},{"location":"reference/spotPython/fun/hypersklearn/","text":"HyperSklearn \u00b6 Hyperparameter Tuning for Sklearn. Parameters: Name Type Description Default seed int seed. See Numpy Random Sampling 126 log_level int log level for logger. Default is 50. 50 Attributes: Name Type Description seed int seed for random number generator. rng Generator random number generator. fun_control dict dictionary containing control parameters for the function. log_level int log level for logger. Examples: >>> from spotPython.fun.hypersklearn import HyperSklearn >>> hyper_sklearn = HyperSklearn ( seed = 126 , log_level = 50 ) >>> print ( hyper_sklearn . seed ) 126 Source code in spotPython/fun/hypersklearn.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class HyperSklearn : \"\"\" Hyperparameter Tuning for Sklearn. Args: seed (int): seed. See Numpy Random Sampling log_level (int): log level for logger. Default is 50. Attributes: seed (int): seed for random number generator. rng (Generator): random number generator. fun_control (dict): dictionary containing control parameters for the function. log_level (int): log level for logger. Examples: >>> from spotPython.fun.hypersklearn import HyperSklearn >>> hyper_sklearn = HyperSklearn(seed=126, log_level=50) >>> print(hyper_sklearn.seed) 126 \"\"\" def __init__ ( self , seed : int = 126 , log_level : int = 50 ): self . seed = seed self . rng = default_rng ( seed = self . seed ) self . fun_control = { \"seed\" : None , \"data\" : None , \"step\" : 10_000 , \"horizon\" : None , \"grace_period\" : None , \"metric_river\" : None , \"metric_sklearn\" : mean_absolute_error , \"weights\" : array ([ 1 , 0 , 0 ]), \"weight_coeff\" : 0.0 , \"log_level\" : log_level , \"var_name\" : [], \"var_type\" : [], \"prep_model\" : None , \"predict_proba\" : False , } self . log_level = self . fun_control [ \"log_level\" ] logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) def check_X_shape ( self , X : np . ndarray ) -> None : \"\"\" Check the shape of the input array X. Args: X (np.ndarray): input array. Raises: Exception: if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypersklearn import HyperSklearn >>> hyper_sklearn = HyperSklearn(seed=126, log_level=50) >>> hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"] >>> hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]])) >>> hyper_sklearn.check_X_shape(X=np.array([[1, 2]])) Traceback (most recent call last): ... Exception \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception def get_sklearn_df_eval_preds ( self , model ) -> tuple : \"\"\" Get evaluation and prediction dataframes for a given model. Args: model (sklearn model): sklearn model. Returns: (tuple): tuple containing evaluation and prediction dataframes. Raises: Exception: if call to evaluate_model fails. \"\"\" try : df_eval , df_preds = self . evaluate_model ( model , self . fun_control ) except Exception as err : print ( f \"Error in get_sklearn_df_eval_preds(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval and df.preds to np.nan\" ) df_eval = np . nan df_preds = np . nan return df_eval , df_preds def fun_sklearn ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Evaluate a sklearn model using hyperparameters specified in X. Args: X (np.ndarray): input array containing hyperparameters. fun_control (dict): dictionary containing control parameters for the function. Default is None. Returns: (np.ndarray): array containing evaluation results. Raises: Exception: if call to evaluate_model fails. \"\"\" z_res = np . array ([], dtype = float ) self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): # config_id = generate_config_id(config) if self . fun_control [ \"prep_model\" ] is not None : model = make_pipeline ( self . fun_control [ \"prep_model\" ], self . fun_control [ \"core_model\" ]( ** config )) else : model = self . fun_control [ \"core_model\" ]( ** config ) try : eval_type = fun_control [ \"eval\" ] if eval_type == \"eval_oob_score\" : df_eval , _ = evaluate_model_oob ( model , self . fun_control ) elif eval_type == \"train_cv\" : df_eval , _ = evaluate_cv ( model , self . fun_control ) else : # eval_type == \"train_hold_out\": df_eval , _ = evaluate_hold_out ( model , self . fun_control ) except Exception as err : print ( f \"Error in fun_sklearn(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_res = np . append ( z_res , fun_control [ \"weights\" ] * df_eval ) return z_res check_X_shape ( X ) \u00b6 Check the shape of the input array X. Parameters: Name Type Description Default X np . ndarray input array. required Raises: Type Description Exception if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypersklearn import HyperSklearn >>> hyper_sklearn = HyperSklearn ( seed = 126 , log_level = 50 ) >>> hyper_sklearn . fun_control [ \"var_name\" ] = [ \"a\" , \"b\" , \"c\" ] >>> hyper_sklearn . check_X_shape ( X = np . array ([[ 1 , 2 , 3 ]])) >>> hyper_sklearn . check_X_shape ( X = np . array ([[ 1 , 2 ]])) Traceback (most recent call last): ... Exception Source code in spotPython/fun/hypersklearn.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def check_X_shape ( self , X : np . ndarray ) -> None : \"\"\" Check the shape of the input array X. Args: X (np.ndarray): input array. Raises: Exception: if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypersklearn import HyperSklearn >>> hyper_sklearn = HyperSklearn(seed=126, log_level=50) >>> hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"] >>> hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]])) >>> hyper_sklearn.check_X_shape(X=np.array([[1, 2]])) Traceback (most recent call last): ... Exception \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception fun_sklearn ( X , fun_control = None ) \u00b6 Evaluate a sklearn model using hyperparameters specified in X. Parameters: Name Type Description Default X np . ndarray input array containing hyperparameters. required fun_control dict dictionary containing control parameters for the function. Default is None. None Returns: Type Description np . ndarray array containing evaluation results. Raises: Type Description Exception if call to evaluate_model fails. Source code in spotPython/fun/hypersklearn.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def fun_sklearn ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Evaluate a sklearn model using hyperparameters specified in X. Args: X (np.ndarray): input array containing hyperparameters. fun_control (dict): dictionary containing control parameters for the function. Default is None. Returns: (np.ndarray): array containing evaluation results. Raises: Exception: if call to evaluate_model fails. \"\"\" z_res = np . array ([], dtype = float ) self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): # config_id = generate_config_id(config) if self . fun_control [ \"prep_model\" ] is not None : model = make_pipeline ( self . fun_control [ \"prep_model\" ], self . fun_control [ \"core_model\" ]( ** config )) else : model = self . fun_control [ \"core_model\" ]( ** config ) try : eval_type = fun_control [ \"eval\" ] if eval_type == \"eval_oob_score\" : df_eval , _ = evaluate_model_oob ( model , self . fun_control ) elif eval_type == \"train_cv\" : df_eval , _ = evaluate_cv ( model , self . fun_control ) else : # eval_type == \"train_hold_out\": df_eval , _ = evaluate_hold_out ( model , self . fun_control ) except Exception as err : print ( f \"Error in fun_sklearn(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_res = np . append ( z_res , fun_control [ \"weights\" ] * df_eval ) return z_res get_sklearn_df_eval_preds ( model ) \u00b6 Get evaluation and prediction dataframes for a given model. Parameters: Name Type Description Default model sklearn model sklearn model. required Returns: Type Description tuple tuple containing evaluation and prediction dataframes. Raises: Type Description Exception if call to evaluate_model fails. Source code in spotPython/fun/hypersklearn.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def get_sklearn_df_eval_preds ( self , model ) -> tuple : \"\"\" Get evaluation and prediction dataframes for a given model. Args: model (sklearn model): sklearn model. Returns: (tuple): tuple containing evaluation and prediction dataframes. Raises: Exception: if call to evaluate_model fails. \"\"\" try : df_eval , df_preds = self . evaluate_model ( model , self . fun_control ) except Exception as err : print ( f \"Error in get_sklearn_df_eval_preds(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval and df.preds to np.nan\" ) df_eval = np . nan df_preds = np . nan return df_eval , df_preds","title":"hypersklearn"},{"location":"reference/spotPython/fun/hypersklearn/#spotPython.fun.hypersklearn.HyperSklearn","text":"Hyperparameter Tuning for Sklearn. Parameters: Name Type Description Default seed int seed. See Numpy Random Sampling 126 log_level int log level for logger. Default is 50. 50 Attributes: Name Type Description seed int seed for random number generator. rng Generator random number generator. fun_control dict dictionary containing control parameters for the function. log_level int log level for logger. Examples: >>> from spotPython.fun.hypersklearn import HyperSklearn >>> hyper_sklearn = HyperSklearn ( seed = 126 , log_level = 50 ) >>> print ( hyper_sklearn . seed ) 126 Source code in spotPython/fun/hypersklearn.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class HyperSklearn : \"\"\" Hyperparameter Tuning for Sklearn. Args: seed (int): seed. See Numpy Random Sampling log_level (int): log level for logger. Default is 50. Attributes: seed (int): seed for random number generator. rng (Generator): random number generator. fun_control (dict): dictionary containing control parameters for the function. log_level (int): log level for logger. Examples: >>> from spotPython.fun.hypersklearn import HyperSklearn >>> hyper_sklearn = HyperSklearn(seed=126, log_level=50) >>> print(hyper_sklearn.seed) 126 \"\"\" def __init__ ( self , seed : int = 126 , log_level : int = 50 ): self . seed = seed self . rng = default_rng ( seed = self . seed ) self . fun_control = { \"seed\" : None , \"data\" : None , \"step\" : 10_000 , \"horizon\" : None , \"grace_period\" : None , \"metric_river\" : None , \"metric_sklearn\" : mean_absolute_error , \"weights\" : array ([ 1 , 0 , 0 ]), \"weight_coeff\" : 0.0 , \"log_level\" : log_level , \"var_name\" : [], \"var_type\" : [], \"prep_model\" : None , \"predict_proba\" : False , } self . log_level = self . fun_control [ \"log_level\" ] logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) def check_X_shape ( self , X : np . ndarray ) -> None : \"\"\" Check the shape of the input array X. Args: X (np.ndarray): input array. Raises: Exception: if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypersklearn import HyperSklearn >>> hyper_sklearn = HyperSklearn(seed=126, log_level=50) >>> hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"] >>> hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]])) >>> hyper_sklearn.check_X_shape(X=np.array([[1, 2]])) Traceback (most recent call last): ... Exception \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception def get_sklearn_df_eval_preds ( self , model ) -> tuple : \"\"\" Get evaluation and prediction dataframes for a given model. Args: model (sklearn model): sklearn model. Returns: (tuple): tuple containing evaluation and prediction dataframes. Raises: Exception: if call to evaluate_model fails. \"\"\" try : df_eval , df_preds = self . evaluate_model ( model , self . fun_control ) except Exception as err : print ( f \"Error in get_sklearn_df_eval_preds(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval and df.preds to np.nan\" ) df_eval = np . nan df_preds = np . nan return df_eval , df_preds def fun_sklearn ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Evaluate a sklearn model using hyperparameters specified in X. Args: X (np.ndarray): input array containing hyperparameters. fun_control (dict): dictionary containing control parameters for the function. Default is None. Returns: (np.ndarray): array containing evaluation results. Raises: Exception: if call to evaluate_model fails. \"\"\" z_res = np . array ([], dtype = float ) self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): # config_id = generate_config_id(config) if self . fun_control [ \"prep_model\" ] is not None : model = make_pipeline ( self . fun_control [ \"prep_model\" ], self . fun_control [ \"core_model\" ]( ** config )) else : model = self . fun_control [ \"core_model\" ]( ** config ) try : eval_type = fun_control [ \"eval\" ] if eval_type == \"eval_oob_score\" : df_eval , _ = evaluate_model_oob ( model , self . fun_control ) elif eval_type == \"train_cv\" : df_eval , _ = evaluate_cv ( model , self . fun_control ) else : # eval_type == \"train_hold_out\": df_eval , _ = evaluate_hold_out ( model , self . fun_control ) except Exception as err : print ( f \"Error in fun_sklearn(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_res = np . append ( z_res , fun_control [ \"weights\" ] * df_eval ) return z_res","title":"HyperSklearn"},{"location":"reference/spotPython/fun/hypersklearn/#spotPython.fun.hypersklearn.HyperSklearn.check_X_shape","text":"Check the shape of the input array X. Parameters: Name Type Description Default X np . ndarray input array. required Raises: Type Description Exception if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypersklearn import HyperSklearn >>> hyper_sklearn = HyperSklearn ( seed = 126 , log_level = 50 ) >>> hyper_sklearn . fun_control [ \"var_name\" ] = [ \"a\" , \"b\" , \"c\" ] >>> hyper_sklearn . check_X_shape ( X = np . array ([[ 1 , 2 , 3 ]])) >>> hyper_sklearn . check_X_shape ( X = np . array ([[ 1 , 2 ]])) Traceback (most recent call last): ... Exception Source code in spotPython/fun/hypersklearn.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def check_X_shape ( self , X : np . ndarray ) -> None : \"\"\" Check the shape of the input array X. Args: X (np.ndarray): input array. Raises: Exception: if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypersklearn import HyperSklearn >>> hyper_sklearn = HyperSklearn(seed=126, log_level=50) >>> hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"] >>> hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]])) >>> hyper_sklearn.check_X_shape(X=np.array([[1, 2]])) Traceback (most recent call last): ... Exception \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception","title":"check_X_shape()"},{"location":"reference/spotPython/fun/hypersklearn/#spotPython.fun.hypersklearn.HyperSklearn.fun_sklearn","text":"Evaluate a sklearn model using hyperparameters specified in X. Parameters: Name Type Description Default X np . ndarray input array containing hyperparameters. required fun_control dict dictionary containing control parameters for the function. Default is None. None Returns: Type Description np . ndarray array containing evaluation results. Raises: Type Description Exception if call to evaluate_model fails. Source code in spotPython/fun/hypersklearn.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def fun_sklearn ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Evaluate a sklearn model using hyperparameters specified in X. Args: X (np.ndarray): input array containing hyperparameters. fun_control (dict): dictionary containing control parameters for the function. Default is None. Returns: (np.ndarray): array containing evaluation results. Raises: Exception: if call to evaluate_model fails. \"\"\" z_res = np . array ([], dtype = float ) self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): # config_id = generate_config_id(config) if self . fun_control [ \"prep_model\" ] is not None : model = make_pipeline ( self . fun_control [ \"prep_model\" ], self . fun_control [ \"core_model\" ]( ** config )) else : model = self . fun_control [ \"core_model\" ]( ** config ) try : eval_type = fun_control [ \"eval\" ] if eval_type == \"eval_oob_score\" : df_eval , _ = evaluate_model_oob ( model , self . fun_control ) elif eval_type == \"train_cv\" : df_eval , _ = evaluate_cv ( model , self . fun_control ) else : # eval_type == \"train_hold_out\": df_eval , _ = evaluate_hold_out ( model , self . fun_control ) except Exception as err : print ( f \"Error in fun_sklearn(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_res = np . append ( z_res , fun_control [ \"weights\" ] * df_eval ) return z_res","title":"fun_sklearn()"},{"location":"reference/spotPython/fun/hypersklearn/#spotPython.fun.hypersklearn.HyperSklearn.get_sklearn_df_eval_preds","text":"Get evaluation and prediction dataframes for a given model. Parameters: Name Type Description Default model sklearn model sklearn model. required Returns: Type Description tuple tuple containing evaluation and prediction dataframes. Raises: Type Description Exception if call to evaluate_model fails. Source code in spotPython/fun/hypersklearn.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def get_sklearn_df_eval_preds ( self , model ) -> tuple : \"\"\" Get evaluation and prediction dataframes for a given model. Args: model (sklearn model): sklearn model. Returns: (tuple): tuple containing evaluation and prediction dataframes. Raises: Exception: if call to evaluate_model fails. \"\"\" try : df_eval , df_preds = self . evaluate_model ( model , self . fun_control ) except Exception as err : print ( f \"Error in get_sklearn_df_eval_preds(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval and df.preds to np.nan\" ) df_eval = np . nan df_preds = np . nan return df_eval , df_preds","title":"get_sklearn_df_eval_preds()"},{"location":"reference/spotPython/fun/hypertorch/","text":"HyperTorch \u00b6 Hyperparameter Tuning for Torch. Parameters: Name Type Description Default seed int seed for random number generator. See Numpy Random Sampling 126 log_level int log level for logger. Default is 50. 50 Attributes: Name Type Description seed int seed for random number generator. rng Generator random number generator. fun_control dict dictionary containing control parameters for the function. log_level int log level for logger. Source code in spotPython/fun/hypertorch.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 class HyperTorch : \"\"\" Hyperparameter Tuning for Torch. Args: seed (int): seed for random number generator. See Numpy Random Sampling log_level (int): log level for logger. Default is 50. Attributes: seed (int): seed for random number generator. rng (Generator): random number generator. fun_control (dict): dictionary containing control parameters for the function. log_level (int): log level for logger. \"\"\" def __init__ ( self , seed : int = 126 , log_level : int = 50 ): self . seed = seed self . rng = default_rng ( seed = self . seed ) self . fun_control = { \"seed\" : None , \"data\" : None , \"step\" : 10_000 , \"horizon\" : None , \"grace_period\" : None , \"metric_river\" : None , \"metric_sklearn\" : None , \"weights\" : array ([ 1 , 0 , 0 ]), \"weight_coeff\" : 0.0 , \"log_level\" : log_level , \"var_name\" : [], \"var_type\" : [], } self . log_level = self . fun_control [ \"log_level\" ] logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) def check_X_shape ( self , X : np . ndarray ) -> None : \"\"\" Check the shape of the input array X. Args: X (np.ndarray): input array. Raises: Exception: if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch(seed=126, log_level=50) >>> hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"] >>> hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]])) >>> hyper_torch.check_X_shape(np.array([1, 2])) Traceback (most recent call last): ... Exception \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception def fun_torch ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Function to be optimized. Args: X (np.ndarray): input array. fun_control (dict): dictionary containing control parameters for the function. Returns: np.ndarray: output array. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch(seed=126, log_level=50) >>> hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"] >>> hyper_torch.fun_torch(np.array([[1, 2], [3, 4]])) \"\"\" z_res = np . array ([], dtype = float ) self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): print ( f \" \\n config: { config } \" ) config_id = generate_config_id ( config ) if self . fun_control [ \"prep_model\" ] is not None : model = make_pipeline ( self . fun_control [ \"prep_model\" ], self . fun_control [ \"core_model\" ]( ** config )) else : model = self . fun_control [ \"core_model\" ]( ** config ) try : if self . fun_control [ \"eval\" ] == \"train_cv\" : df_eval , _ = evaluate_cv ( model , dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) elif self . fun_control [ \"eval\" ] == \"test_cv\" : df_eval , _ = evaluate_cv ( model , dataset = fun_control [ \"test\" ], shuffle = self . fun_control [ \"shuffle\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) elif self . fun_control [ \"eval\" ] == \"test_hold_out\" : df_eval , _ = evaluate_hold_out ( model , train_dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], loss_function = self . fun_control [ \"loss_function\" ], metric = self . fun_control [ \"metric_torch\" ], test_dataset = fun_control [ \"test\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], path = self . fun_control [ \"path\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) else : # eval == \"train_hold_out\" df_eval , _ = evaluate_hold_out ( model , train_dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], loss_function = self . fun_control [ \"loss_function\" ], metric = self . fun_control [ \"metric_torch\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], path = self . fun_control [ \"path\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) except Exception as err : print ( f \"Error in fun_torch(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_val = fun_control [ \"weights\" ] * df_eval if self . fun_control [ \"spot_writer\" ] is not None : writer = self . fun_control [ \"spot_writer\" ] writer . add_hparams ( config , { \"fun_torch: loss\" : z_val }) writer . flush () z_res = np . append ( z_res , z_val ) return z_res check_X_shape ( X ) \u00b6 Check the shape of the input array X. Parameters: Name Type Description Default X np . ndarray input array. required Raises: Type Description Exception if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch ( seed = 126 , log_level = 50 ) >>> hyper_torch . fun_control [ \"var_name\" ] = [ \"x1\" , \"x2\" ] >>> hyper_torch . check_X_shape ( np . array ([[ 1 , 2 ], [ 3 , 4 ]])) >>> hyper_torch . check_X_shape ( np . array ([ 1 , 2 ])) Traceback (most recent call last): ... Exception Source code in spotPython/fun/hypertorch.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def check_X_shape ( self , X : np . ndarray ) -> None : \"\"\" Check the shape of the input array X. Args: X (np.ndarray): input array. Raises: Exception: if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch(seed=126, log_level=50) >>> hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"] >>> hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]])) >>> hyper_torch.check_X_shape(np.array([1, 2])) Traceback (most recent call last): ... Exception \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception fun_torch ( X , fun_control = None ) \u00b6 Function to be optimized. Parameters: Name Type Description Default X np . ndarray input array. required fun_control dict dictionary containing control parameters for the function. None Returns: Type Description np . ndarray np.ndarray: output array. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch ( seed = 126 , log_level = 50 ) >>> hyper_torch . fun_control [ \"var_name\" ] = [ \"x1\" , \"x2\" ] >>> hyper_torch . fun_torch ( np . array ([[ 1 , 2 ], [ 3 , 4 ]])) Source code in spotPython/fun/hypertorch.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def fun_torch ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Function to be optimized. Args: X (np.ndarray): input array. fun_control (dict): dictionary containing control parameters for the function. Returns: np.ndarray: output array. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch(seed=126, log_level=50) >>> hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"] >>> hyper_torch.fun_torch(np.array([[1, 2], [3, 4]])) \"\"\" z_res = np . array ([], dtype = float ) self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): print ( f \" \\n config: { config } \" ) config_id = generate_config_id ( config ) if self . fun_control [ \"prep_model\" ] is not None : model = make_pipeline ( self . fun_control [ \"prep_model\" ], self . fun_control [ \"core_model\" ]( ** config )) else : model = self . fun_control [ \"core_model\" ]( ** config ) try : if self . fun_control [ \"eval\" ] == \"train_cv\" : df_eval , _ = evaluate_cv ( model , dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) elif self . fun_control [ \"eval\" ] == \"test_cv\" : df_eval , _ = evaluate_cv ( model , dataset = fun_control [ \"test\" ], shuffle = self . fun_control [ \"shuffle\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) elif self . fun_control [ \"eval\" ] == \"test_hold_out\" : df_eval , _ = evaluate_hold_out ( model , train_dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], loss_function = self . fun_control [ \"loss_function\" ], metric = self . fun_control [ \"metric_torch\" ], test_dataset = fun_control [ \"test\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], path = self . fun_control [ \"path\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) else : # eval == \"train_hold_out\" df_eval , _ = evaluate_hold_out ( model , train_dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], loss_function = self . fun_control [ \"loss_function\" ], metric = self . fun_control [ \"metric_torch\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], path = self . fun_control [ \"path\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) except Exception as err : print ( f \"Error in fun_torch(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_val = fun_control [ \"weights\" ] * df_eval if self . fun_control [ \"spot_writer\" ] is not None : writer = self . fun_control [ \"spot_writer\" ] writer . add_hparams ( config , { \"fun_torch: loss\" : z_val }) writer . flush () z_res = np . append ( z_res , z_val ) return z_res","title":"hypertorch"},{"location":"reference/spotPython/fun/hypertorch/#spotPython.fun.hypertorch.HyperTorch","text":"Hyperparameter Tuning for Torch. Parameters: Name Type Description Default seed int seed for random number generator. See Numpy Random Sampling 126 log_level int log level for logger. Default is 50. 50 Attributes: Name Type Description seed int seed for random number generator. rng Generator random number generator. fun_control dict dictionary containing control parameters for the function. log_level int log level for logger. Source code in spotPython/fun/hypertorch.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 class HyperTorch : \"\"\" Hyperparameter Tuning for Torch. Args: seed (int): seed for random number generator. See Numpy Random Sampling log_level (int): log level for logger. Default is 50. Attributes: seed (int): seed for random number generator. rng (Generator): random number generator. fun_control (dict): dictionary containing control parameters for the function. log_level (int): log level for logger. \"\"\" def __init__ ( self , seed : int = 126 , log_level : int = 50 ): self . seed = seed self . rng = default_rng ( seed = self . seed ) self . fun_control = { \"seed\" : None , \"data\" : None , \"step\" : 10_000 , \"horizon\" : None , \"grace_period\" : None , \"metric_river\" : None , \"metric_sklearn\" : None , \"weights\" : array ([ 1 , 0 , 0 ]), \"weight_coeff\" : 0.0 , \"log_level\" : log_level , \"var_name\" : [], \"var_type\" : [], } self . log_level = self . fun_control [ \"log_level\" ] logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) def check_X_shape ( self , X : np . ndarray ) -> None : \"\"\" Check the shape of the input array X. Args: X (np.ndarray): input array. Raises: Exception: if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch(seed=126, log_level=50) >>> hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"] >>> hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]])) >>> hyper_torch.check_X_shape(np.array([1, 2])) Traceback (most recent call last): ... Exception \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception def fun_torch ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Function to be optimized. Args: X (np.ndarray): input array. fun_control (dict): dictionary containing control parameters for the function. Returns: np.ndarray: output array. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch(seed=126, log_level=50) >>> hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"] >>> hyper_torch.fun_torch(np.array([[1, 2], [3, 4]])) \"\"\" z_res = np . array ([], dtype = float ) self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): print ( f \" \\n config: { config } \" ) config_id = generate_config_id ( config ) if self . fun_control [ \"prep_model\" ] is not None : model = make_pipeline ( self . fun_control [ \"prep_model\" ], self . fun_control [ \"core_model\" ]( ** config )) else : model = self . fun_control [ \"core_model\" ]( ** config ) try : if self . fun_control [ \"eval\" ] == \"train_cv\" : df_eval , _ = evaluate_cv ( model , dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) elif self . fun_control [ \"eval\" ] == \"test_cv\" : df_eval , _ = evaluate_cv ( model , dataset = fun_control [ \"test\" ], shuffle = self . fun_control [ \"shuffle\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) elif self . fun_control [ \"eval\" ] == \"test_hold_out\" : df_eval , _ = evaluate_hold_out ( model , train_dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], loss_function = self . fun_control [ \"loss_function\" ], metric = self . fun_control [ \"metric_torch\" ], test_dataset = fun_control [ \"test\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], path = self . fun_control [ \"path\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) else : # eval == \"train_hold_out\" df_eval , _ = evaluate_hold_out ( model , train_dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], loss_function = self . fun_control [ \"loss_function\" ], metric = self . fun_control [ \"metric_torch\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], path = self . fun_control [ \"path\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) except Exception as err : print ( f \"Error in fun_torch(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_val = fun_control [ \"weights\" ] * df_eval if self . fun_control [ \"spot_writer\" ] is not None : writer = self . fun_control [ \"spot_writer\" ] writer . add_hparams ( config , { \"fun_torch: loss\" : z_val }) writer . flush () z_res = np . append ( z_res , z_val ) return z_res","title":"HyperTorch"},{"location":"reference/spotPython/fun/hypertorch/#spotPython.fun.hypertorch.HyperTorch.check_X_shape","text":"Check the shape of the input array X. Parameters: Name Type Description Default X np . ndarray input array. required Raises: Type Description Exception if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch ( seed = 126 , log_level = 50 ) >>> hyper_torch . fun_control [ \"var_name\" ] = [ \"x1\" , \"x2\" ] >>> hyper_torch . check_X_shape ( np . array ([[ 1 , 2 ], [ 3 , 4 ]])) >>> hyper_torch . check_X_shape ( np . array ([ 1 , 2 ])) Traceback (most recent call last): ... Exception Source code in spotPython/fun/hypertorch.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def check_X_shape ( self , X : np . ndarray ) -> None : \"\"\" Check the shape of the input array X. Args: X (np.ndarray): input array. Raises: Exception: if the second dimension of X does not match the length of var_name in fun_control. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch(seed=126, log_level=50) >>> hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"] >>> hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]])) >>> hyper_torch.check_X_shape(np.array([1, 2])) Traceback (most recent call last): ... Exception \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != len ( self . fun_control [ \"var_name\" ]): raise Exception","title":"check_X_shape()"},{"location":"reference/spotPython/fun/hypertorch/#spotPython.fun.hypertorch.HyperTorch.fun_torch","text":"Function to be optimized. Parameters: Name Type Description Default X np . ndarray input array. required fun_control dict dictionary containing control parameters for the function. None Returns: Type Description np . ndarray np.ndarray: output array. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch ( seed = 126 , log_level = 50 ) >>> hyper_torch . fun_control [ \"var_name\" ] = [ \"x1\" , \"x2\" ] >>> hyper_torch . fun_torch ( np . array ([[ 1 , 2 ], [ 3 , 4 ]])) Source code in spotPython/fun/hypertorch.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def fun_torch ( self , X : np . ndarray , fun_control : dict = None ) -> np . ndarray : \"\"\" Function to be optimized. Args: X (np.ndarray): input array. fun_control (dict): dictionary containing control parameters for the function. Returns: np.ndarray: output array. Examples: >>> from spotPython.fun.hypertorch import HyperTorch >>> import numpy as np >>> hyper_torch = HyperTorch(seed=126, log_level=50) >>> hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"] >>> hyper_torch.fun_torch(np.array([[1, 2], [3, 4]])) \"\"\" z_res = np . array ([], dtype = float ) self . fun_control . update ( fun_control ) self . check_X_shape ( X ) var_dict = assign_values ( X , self . fun_control [ \"var_name\" ]) for config in generate_one_config_from_var_dict ( var_dict , self . fun_control ): print ( f \" \\n config: { config } \" ) config_id = generate_config_id ( config ) if self . fun_control [ \"prep_model\" ] is not None : model = make_pipeline ( self . fun_control [ \"prep_model\" ], self . fun_control [ \"core_model\" ]( ** config )) else : model = self . fun_control [ \"core_model\" ]( ** config ) try : if self . fun_control [ \"eval\" ] == \"train_cv\" : df_eval , _ = evaluate_cv ( model , dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) elif self . fun_control [ \"eval\" ] == \"test_cv\" : df_eval , _ = evaluate_cv ( model , dataset = fun_control [ \"test\" ], shuffle = self . fun_control [ \"shuffle\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) elif self . fun_control [ \"eval\" ] == \"test_hold_out\" : df_eval , _ = evaluate_hold_out ( model , train_dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], loss_function = self . fun_control [ \"loss_function\" ], metric = self . fun_control [ \"metric_torch\" ], test_dataset = fun_control [ \"test\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], path = self . fun_control [ \"path\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) else : # eval == \"train_hold_out\" df_eval , _ = evaluate_hold_out ( model , train_dataset = fun_control [ \"train\" ], shuffle = self . fun_control [ \"shuffle\" ], loss_function = self . fun_control [ \"loss_function\" ], metric = self . fun_control [ \"metric_torch\" ], device = self . fun_control [ \"device\" ], show_batch_interval = self . fun_control [ \"show_batch_interval\" ], path = self . fun_control [ \"path\" ], task = self . fun_control [ \"task\" ], writer = self . fun_control [ \"spot_writer\" ], writerId = config_id , ) except Exception as err : print ( f \"Error in fun_torch(). Call to evaluate_model failed. { err =} , { type ( err ) =} \" ) print ( \"Setting df_eval to np.nan\" ) df_eval = np . nan z_val = fun_control [ \"weights\" ] * df_eval if self . fun_control [ \"spot_writer\" ] is not None : writer = self . fun_control [ \"spot_writer\" ] writer . add_hparams ( config , { \"fun_torch: loss\" : z_val }) writer . flush () z_res = np . append ( z_res , z_val ) return z_res","title":"fun_torch()"},{"location":"reference/spotPython/fun/objectivefunctions/","text":"analytical \u00b6 Class for analytical test functions. Parameters: Name Type Description Default offset float Offset value. Defaults to 0.0. 0.0 hz float Horizontal value. Defaults to 0. 0 seed int Seed value for random number generation. Defaults to 126. 126 Note See Numpy Random Sampling Attributes: Name Type Description offset float Offset value. hz float Horizontal value. seed int Seed value for random number generation. rng Generator Numpy random number generator object. fun_control dict Dictionary containing control parameters for the function. Source code in spotPython/fun/objectivefunctions.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 class analytical : \"\"\" Class for analytical test functions. Args: offset (float): Offset value. Defaults to 0.0. hz (float): Horizontal value. Defaults to 0. seed (int): Seed value for random number generation. Defaults to 126. Note: See [Numpy Random Sampling](https://numpy.org/doc/stable/reference/random/index.html#random-quick-start) Attributes: offset (float): Offset value. hz (float): Horizontal value. seed (int): Seed value for random number generation. rng (Generator): Numpy random number generator object. fun_control (dict): Dictionary containing control parameters for the function. \"\"\" def __init__ ( self , offset : float = 0.0 , hz : float = 0 , seed : int = 126 ) -> None : self . offset = offset self . hz = hz self . seed = seed self . rng = default_rng ( seed = self . seed ) self . fun_control = { \"sigma\" : 0 , \"seed\" : None , \"sel_var\" : None } def __repr__ ( self ) -> str : return f \"analytical(offset= { self . offset } , hz= { self . hz } , seed= { self . seed } )\" def add_noise ( self , y : List [ float ]) -> np . ndarray : \"\"\" Adds noise to the input data. This method takes in a list of float values y as input and adds noise to the data using a random number generator. The method returns a numpy array containing the noisy data. Args: self (analytical): analytical class object. y (List[float]): Input data. Returns: np.ndarray: Noisy data. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> y = np.array([1, 2, 3, 4, 5]) >>> fun = analytical() >>> fun.add_noise(y) array([1. , 2. , 3. , 4. , 5. ]) \"\"\" # Use own rng: if self . fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = self . fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for y_i in y : noise_y = np . append ( noise_y , y_i + rng . normal ( loc = 0 , scale = self . fun_control [ \"sigma\" ], size = 1 ), ) return noise_y def fun_branin_factor ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\" Calculates the Branin function with an additional factor based on the value of x3. Args: X (np.ndarray): A 2D numpy array with shape (n, 3) where n is the number of samples. fun_control (Optional[Dict]): A dictionary containing control parameters for the function. If None, self.fun_control is used. Defaults to None. Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin_factor(X) \"\"\" if fun_control is None : fun_control = self . fun_control if len ( X . shape ) == 1 : X = np . array ([ X ]) if X . shape [ 1 ] != 3 : raise Exception ( \"X must have shape (n, 3)\" ) x1 = X [:, 0 ] x2 = X [:, 1 ] x3 = X [:, 2 ] a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) y = a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s for j in range ( X . shape [ 0 ]): if x3 [ j ] == 1 : y [ j ] = y [ j ] + 10 elif x3 [ j ] == 2 : y [ j ] = y [ j ] - 10 if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y def fun_linear ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Linear function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_linear(X) array([ 6., 15.]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError as err : print ( \"error message:\" , err ) X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum ( X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y def fun_sphere ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Sphere function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_sphere(X) array([14., 77.]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum (( X [ i ] - offset ) ** 2 )) # TODO: move to a separate function: if self . fun_control [ \"sigma\" ] > 0 : # Use own rng: if self . fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for y_i in y : noise_y = np . append ( noise_y , y_i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y def fun_cubed ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Cubed function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_cubed(X) array([ 0., 27.]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum (( X [ i ] - offset ) ** 3 )) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y def fun_forrester ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Forrester function. Function used by [Forr08a, p.83]. f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1]. Starts with three sample points at x=0, x=0.5, and x=1. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_forrester(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , ( 6.0 * X [ i ] - 2 ) ** 2 * np . sin ( 12 * X [ i ] - 4 )) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y def fun_branin ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Branin function. The 2-dim Branin function is defined as y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s, where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi). It has three global minima: f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475). Input domain: This function is usually evaluated on the square x1 in [-5, 10] x x2 in [0, 15]. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x1 = X [:, 0 ] x2 = X [:, 1 ] a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) y = a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y def fun_branin_modified ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Modified Branin function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin_modified(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x = X [:, 0 ] y = X [:, 1 ] X1 = 15 * x - 5 X2 = 15 * y a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi d = 6 e = 10 ff = 1 / ( 8 * np . pi ) y = ( a * ( X2 - b * X1 ** 2 + c * X1 - d ) ** 2 + e * ( 1 - ff ) * np . cos ( X1 ) + e ) + 5 * x # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y def branin_noise ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Branin function with noise. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.branin_noise(X) array([ 0. , 11.99999999]) \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x = X [:, 0 ] y = X [:, 1 ] X1 = 15 * x - 5 X2 = 15 * y a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi d = 6 e = 10 ff = 1 / ( 8 * np . pi ) noiseFree = ( a * ( X2 - b * X1 ** 2 + c * X1 - d ) ** 2 + e * ( 1 - ff ) * np . cos ( X1 ) + e ) + 5 * x noise_y = [] for i in noiseFree : noise_y . append ( i + np . random . standard_normal () * 15 ) return np . array ( noise_y ) def fun_sin_cos ( self , X , fun_control = None ): \"\"\"Sinusoidal function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_sin_cos(X) array([-1. , -0.41614684]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x0 = X [:, 0 ] x1 = X [:, 1 ] y = 2.0 * np . sin ( x0 + self . hz ) + 0.5 * np . cos ( x1 + self . hz ) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y # def fun_forrester_2(self, X): # \"\"\" # Function used by [Forr08a, p.83]. # f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1]. # Starts with three sample points at x=0, x=0.5, and x=1. # Args: # X (flooat): input values (1-dim) # Returns: # float: function value # \"\"\" # try: # X.shape[1] # except ValueError: # X = np.array(X) # if len(X.shape) < 2: # X = np.array([X]) # # y = X[:, 1] # y = (6.0 * X - 2) ** 2 * np.sin(12 * X - 4) # if self.sigma != 0: # noise_y = np.array([], dtype=float) # for i in y: # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) # ) # return noise_y # else: # return y def fun_runge ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k >= 1. Interval: -5 <= x <= 5 Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_runge(X) array([0.0625 , 0.015625 , 0.00390625]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , ( 1 / ( 1 + np . sum (( X [ i ] - offset ) ** 2 )))) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y def fun_wingwt ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Wing weight function. Example from Forrester et al. to understand the weight of an unpainted light aircraft wing as a function of nine design and operational parameters: W = 0.036 S_W**0.758 * Wfw**0.0035 ( A / (cos**2 Lambda))**0.6 * q**0.006 * lambda**0.04 * ( (100 Rtc)/(cos Lambda) ))**-0.3*(Nz Wdg)**0.49 | Symbol | Parameter | Baseline | Minimum | Maximum | |-----------|----------------------------------------|----------|---------|---------| | $S_W$ | Wing area ($ft^2$) | 174 | 150 | 200 | | $W_{fw}$ | Weight of fuel in wing (lb) | 252 | 220 | 300 | | $A$ | Aspect ratio | 7.52 | 6 | 10 | | $Lambda$ | Quarter-chord sweep (deg) | 0 | -10 | 10 | | $q$ | Dynamic pressure at cruise ($lb/ft^2$) | 34 | 16 | 45 | | $lambda$ | Taper ratio | 0.672 | 0.5 | 1 | | $R_{tc}$ | Aerofoil thickness to chord ratio | 0.12 | 0.08 | 0.18 | | $N_z$ | Ultimate load factor | 3.8 | 2.5 | 6 | | $W_{dg}$ | Flight design gross weight (lb) | 2000 | 1700 | 2500 | | $W_p$ | paint weight (lb/ft^2) | 0.064 | 0.025 | 0.08 | Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]]) >>> fun = analytical() >>> fun.fun_wingwt(X) array([0.0625 , 0.015625 , 0.00390625]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) # W_res = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): Sw = X [ i , 0 ] * ( 200 - 150 ) + 150 Wfw = X [ i , 1 ] * ( 300 - 220 ) + 220 A = X [ i , 2 ] * ( 10 - 6 ) + 6 L = ( X [ i , 3 ] * ( 10 - ( - 10 )) - 10 ) * np . pi / 180 q = X [ i , 4 ] * ( 45 - 16 ) + 16 la = X [ i , 5 ] * ( 1 - 0.5 ) + 0.5 Rtc = X [ i , 6 ] * ( 0.18 - 0.08 ) + 0.08 Nz = X [ i , 7 ] * ( 6 - 2.5 ) + 2.5 Wdg = X [ i , 8 ] * ( 2500 - 1700 ) + 1700 Wp = X [ i , 9 ] * ( 0.08 - 0.025 ) + 0.025 # calculation on natural scale W = 0.036 * Sw ** 0.758 * Wfw ** 0.0035 * ( A / np . cos ( L ) ** 2 ) ** 0.6 * q ** 0.006 W = W * la ** 0.04 * ( 100 * Rtc / np . cos ( L )) ** ( - 0.3 ) * ( Nz * Wdg ) ** ( 0.49 ) + Sw * Wp W_res = np . append ( W_res , W ) return W_res def fun_xsin ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Example function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]]) >>> fun = analytical() >>> fun.fun_xsin(X) array([0.84147098, 0.90929743, 0.14112001]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , X [ i ] * np . sin ( 1.0 / X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y def fun_rosen ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Rosenbrock function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2,], [4, 5 ]]) >>> fun = analytical() >>> fun.fun_rosen(X) array([24, 0]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x0 = X [:, 0 ] x1 = X [:, 1 ] b = 10 y = ( x0 - 1 ) ** 2 + b * ( x1 - x0 ** 2 ) ** 2 if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y def fun_random_error ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Return errors for testing spot stability. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2,], [4, 5 ]]) >>> fun = analytical() >>> fun.fun_random_error(X) array([24, 0]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError as err : print ( \"error message:\" , err ) X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): # provoke error: if random () < 0.1 : y = np . append ( y , np . nan ) else : y = np . append ( y , np . sum ( X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : print ( y ) return y add_noise ( y ) \u00b6 Adds noise to the input data. This method takes in a list of float values y as input and adds noise to the data using a random number generator. The method returns a numpy array containing the noisy data. Parameters: Name Type Description Default self analytical analytical class object. required y List [ float ] Input data. required Returns: Type Description np . ndarray np.ndarray: Noisy data. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> y = np . array ([ 1 , 2 , 3 , 4 , 5 ]) >>> fun = analytical () >>> fun . add_noise ( y ) array([1. , 2. , 3. , 4. , 5. ]) Source code in spotPython/fun/objectivefunctions.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def add_noise ( self , y : List [ float ]) -> np . ndarray : \"\"\" Adds noise to the input data. This method takes in a list of float values y as input and adds noise to the data using a random number generator. The method returns a numpy array containing the noisy data. Args: self (analytical): analytical class object. y (List[float]): Input data. Returns: np.ndarray: Noisy data. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> y = np.array([1, 2, 3, 4, 5]) >>> fun = analytical() >>> fun.add_noise(y) array([1. , 2. , 3. , 4. , 5. ]) \"\"\" # Use own rng: if self . fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = self . fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for y_i in y : noise_y = np . append ( noise_y , y_i + rng . normal ( loc = 0 , scale = self . fun_control [ \"sigma\" ], size = 1 ), ) return noise_y branin_noise ( X , fun_control = None ) \u00b6 Branin function with noise. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . branin_noise ( X ) array([ 0. , 11.99999999]) Source code in spotPython/fun/objectivefunctions.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 def branin_noise ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Branin function with noise. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.branin_noise(X) array([ 0. , 11.99999999]) \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x = X [:, 0 ] y = X [:, 1 ] X1 = 15 * x - 5 X2 = 15 * y a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi d = 6 e = 10 ff = 1 / ( 8 * np . pi ) noiseFree = ( a * ( X2 - b * X1 ** 2 + c * X1 - d ) ** 2 + e * ( 1 - ff ) * np . cos ( X1 ) + e ) + 5 * x noise_y = [] for i in noiseFree : noise_y . append ( i + np . random . standard_normal () * 15 ) return np . array ( noise_y ) fun_branin ( X , fun_control = None ) \u00b6 Branin function. The 2-dim Branin function is defined as y = a * (x2 - b * x1 2 + c * x1 - r) 2 + s * (1 - t) * np.cos(x1) + s, where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4 pi 2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8 pi). It has three global minima: f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475). Input domain: This function is usually evaluated on the square x1 in [-5, 10] x x2 in [0, 15]. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_branin ( X ) array([ 0. , 11.99999999]) Source code in spotPython/fun/objectivefunctions.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def fun_branin ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Branin function. The 2-dim Branin function is defined as y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s, where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi). It has three global minima: f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475). Input domain: This function is usually evaluated on the square x1 in [-5, 10] x x2 in [0, 15]. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x1 = X [:, 0 ] x2 = X [:, 1 ] a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) y = a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y fun_branin_factor ( X , fun_control = None ) \u00b6 Calculates the Branin function with an additional factor based on the value of x3. Parameters: Name Type Description Default X np . ndarray A 2D numpy array with shape (n, 3) where n is the number of samples. required fun_control Optional [ Dict ] A dictionary containing control parameters for the function. If None, self.fun_control is used. Defaults to None. None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_branin_factor ( X ) Source code in spotPython/fun/objectivefunctions.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def fun_branin_factor ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\" Calculates the Branin function with an additional factor based on the value of x3. Args: X (np.ndarray): A 2D numpy array with shape (n, 3) where n is the number of samples. fun_control (Optional[Dict]): A dictionary containing control parameters for the function. If None, self.fun_control is used. Defaults to None. Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin_factor(X) \"\"\" if fun_control is None : fun_control = self . fun_control if len ( X . shape ) == 1 : X = np . array ([ X ]) if X . shape [ 1 ] != 3 : raise Exception ( \"X must have shape (n, 3)\" ) x1 = X [:, 0 ] x2 = X [:, 1 ] x3 = X [:, 2 ] a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) y = a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s for j in range ( X . shape [ 0 ]): if x3 [ j ] == 1 : y [ j ] = y [ j ] + 10 elif x3 [ j ] == 2 : y [ j ] = y [ j ] - 10 if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y fun_branin_modified ( X , fun_control = None ) \u00b6 Modified Branin function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_branin_modified ( X ) array([ 0. , 11.99999999]) Source code in spotPython/fun/objectivefunctions.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 def fun_branin_modified ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Modified Branin function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin_modified(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x = X [:, 0 ] y = X [:, 1 ] X1 = 15 * x - 5 X2 = 15 * y a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi d = 6 e = 10 ff = 1 / ( 8 * np . pi ) y = ( a * ( X2 - b * X1 ** 2 + c * X1 - d ) ** 2 + e * ( 1 - ff ) * np . cos ( X1 ) + e ) + 5 * x # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y fun_cubed ( X , fun_control = None ) \u00b6 Cubed function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_cubed ( X ) array([ 0., 27.]) Source code in spotPython/fun/objectivefunctions.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def fun_cubed ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Cubed function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_cubed(X) array([ 0., 27.]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum (( X [ i ] - offset ) ** 3 )) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y fun_forrester ( X , fun_control = None ) \u00b6 Forrester function. Function used by [Forr08a, p.83]. f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1]. Starts with three sample points at x=0, x=0.5, and x=1. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_forrester ( X ) array([ 0. , 11.99999999]) Source code in spotPython/fun/objectivefunctions.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def fun_forrester ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Forrester function. Function used by [Forr08a, p.83]. f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1]. Starts with three sample points at x=0, x=0.5, and x=1. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_forrester(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , ( 6.0 * X [ i ] - 2 ) ** 2 * np . sin ( 12 * X [ i ] - 4 )) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y fun_linear ( X , fun_control = None ) \u00b6 Linear function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_linear ( X ) array([ 6., 15.]) Source code in spotPython/fun/objectivefunctions.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def fun_linear ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Linear function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_linear(X) array([ 6., 15.]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError as err : print ( \"error message:\" , err ) X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum ( X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y fun_random_error ( X , fun_control = None ) \u00b6 Return errors for testing spot stability. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 ,], [ 4 , 5 ]]) >>> fun = analytical () >>> fun . fun_random_error ( X ) array([24, 0]) Source code in spotPython/fun/objectivefunctions.py 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 def fun_random_error ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Return errors for testing spot stability. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2,], [4, 5 ]]) >>> fun = analytical() >>> fun.fun_random_error(X) array([24, 0]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError as err : print ( \"error message:\" , err ) X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): # provoke error: if random () < 0.1 : y = np . append ( y , np . nan ) else : y = np . append ( y , np . sum ( X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : print ( y ) return y fun_rosen ( X , fun_control = None ) \u00b6 Rosenbrock function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 ,], [ 4 , 5 ]]) >>> fun = analytical () >>> fun . fun_rosen ( X ) array([24, 0]) Source code in spotPython/fun/objectivefunctions.py 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 def fun_rosen ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Rosenbrock function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2,], [4, 5 ]]) >>> fun = analytical() >>> fun.fun_rosen(X) array([24, 0]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x0 = X [:, 0 ] x1 = X [:, 1 ] b = 10 y = ( x0 - 1 ) ** 2 + b * ( x1 - x0 ** 2 ) ** 2 if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y fun_runge ( X , fun_control = None ) \u00b6 Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k >= 1. Interval: -5 <= x <= 5 Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_runge ( X ) array([0.0625 , 0.015625 , 0.00390625]) Source code in spotPython/fun/objectivefunctions.py 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def fun_runge ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k >= 1. Interval: -5 <= x <= 5 Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_runge(X) array([0.0625 , 0.015625 , 0.00390625]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , ( 1 / ( 1 + np . sum (( X [ i ] - offset ) ** 2 )))) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y fun_sin_cos ( X , fun_control = None ) \u00b6 Sinusoidal function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_sin_cos ( X ) array([-1. , -0.41614684]) Source code in spotPython/fun/objectivefunctions.py 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 def fun_sin_cos ( self , X , fun_control = None ): \"\"\"Sinusoidal function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_sin_cos(X) array([-1. , -0.41614684]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x0 = X [:, 0 ] x1 = X [:, 1 ] y = 2.0 * np . sin ( x0 + self . hz ) + 0.5 * np . cos ( x1 + self . hz ) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y fun_sphere ( X , fun_control = None ) \u00b6 Sphere function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_sphere ( X ) array([14., 77.]) Source code in spotPython/fun/objectivefunctions.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def fun_sphere ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Sphere function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_sphere(X) array([14., 77.]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum (( X [ i ] - offset ) ** 2 )) # TODO: move to a separate function: if self . fun_control [ \"sigma\" ] > 0 : # Use own rng: if self . fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for y_i in y : noise_y = np . append ( noise_y , y_i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y fun_wingwt ( X , fun_control = None ) \u00b6 Wing weight function. Example from Forrester et al. to understand the weight of an unpainted light aircraft wing as a function of nine design and operational parameters: W = 0.036 S_W 0.758 * Wfw 0.0035 ( A / (cos 2 Lambda)) 0.6 * q 0.006 * lambda 0.04 * ( (100 Rtc)/(cos Lambda) )) -0.3*(Nz Wdg) 0.49 Symbol Parameter Baseline Minimum Maximum $S_W$ Wing area ($ft^2$) 174 150 200 $W_{fw}$ Weight of fuel in wing (lb) 252 220 300 $A$ Aspect ratio 7.52 6 10 $Lambda$ Quarter-chord sweep (deg) 0 -10 10 $q$ Dynamic pressure at cruise ($lb/ft^2$) 34 16 45 $lambda$ Taper ratio 0.672 0.5 1 $R_{tc}$ Aerofoil thickness to chord ratio 0.12 0.08 0.18 $N_z$ Ultimate load factor 3.8 2.5 6 $W_{dg}$ Flight design gross weight (lb) 2000 1700 2500 $W_p$ paint weight (lb/ft^2) 0.064 0.025 0.08 Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ], [ 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ]]) >>> fun = analytical () >>> fun . fun_wingwt ( X ) array([0.0625 , 0.015625 , 0.00390625]) Source code in spotPython/fun/objectivefunctions.py 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 def fun_wingwt ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Wing weight function. Example from Forrester et al. to understand the weight of an unpainted light aircraft wing as a function of nine design and operational parameters: W = 0.036 S_W**0.758 * Wfw**0.0035 ( A / (cos**2 Lambda))**0.6 * q**0.006 * lambda**0.04 * ( (100 Rtc)/(cos Lambda) ))**-0.3*(Nz Wdg)**0.49 | Symbol | Parameter | Baseline | Minimum | Maximum | |-----------|----------------------------------------|----------|---------|---------| | $S_W$ | Wing area ($ft^2$) | 174 | 150 | 200 | | $W_{fw}$ | Weight of fuel in wing (lb) | 252 | 220 | 300 | | $A$ | Aspect ratio | 7.52 | 6 | 10 | | $Lambda$ | Quarter-chord sweep (deg) | 0 | -10 | 10 | | $q$ | Dynamic pressure at cruise ($lb/ft^2$) | 34 | 16 | 45 | | $lambda$ | Taper ratio | 0.672 | 0.5 | 1 | | $R_{tc}$ | Aerofoil thickness to chord ratio | 0.12 | 0.08 | 0.18 | | $N_z$ | Ultimate load factor | 3.8 | 2.5 | 6 | | $W_{dg}$ | Flight design gross weight (lb) | 2000 | 1700 | 2500 | | $W_p$ | paint weight (lb/ft^2) | 0.064 | 0.025 | 0.08 | Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]]) >>> fun = analytical() >>> fun.fun_wingwt(X) array([0.0625 , 0.015625 , 0.00390625]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) # W_res = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): Sw = X [ i , 0 ] * ( 200 - 150 ) + 150 Wfw = X [ i , 1 ] * ( 300 - 220 ) + 220 A = X [ i , 2 ] * ( 10 - 6 ) + 6 L = ( X [ i , 3 ] * ( 10 - ( - 10 )) - 10 ) * np . pi / 180 q = X [ i , 4 ] * ( 45 - 16 ) + 16 la = X [ i , 5 ] * ( 1 - 0.5 ) + 0.5 Rtc = X [ i , 6 ] * ( 0.18 - 0.08 ) + 0.08 Nz = X [ i , 7 ] * ( 6 - 2.5 ) + 2.5 Wdg = X [ i , 8 ] * ( 2500 - 1700 ) + 1700 Wp = X [ i , 9 ] * ( 0.08 - 0.025 ) + 0.025 # calculation on natural scale W = 0.036 * Sw ** 0.758 * Wfw ** 0.0035 * ( A / np . cos ( L ) ** 2 ) ** 0.6 * q ** 0.006 W = W * la ** 0.04 * ( 100 * Rtc / np . cos ( L )) ** ( - 0.3 ) * ( Nz * Wdg ) ** ( 0.49 ) + Sw * Wp W_res = np . append ( W_res , W ) return W_res fun_xsin ( X , fun_control = None ) \u00b6 Example function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ], [ 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ]]) >>> fun = analytical () >>> fun . fun_xsin ( X ) array([0.84147098, 0.90929743, 0.14112001]) Source code in spotPython/fun/objectivefunctions.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 def fun_xsin ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Example function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]]) >>> fun = analytical() >>> fun.fun_xsin(X) array([0.84147098, 0.90929743, 0.14112001]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , X [ i ] * np . sin ( 1.0 / X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y","title":"objectivefunctions"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical","text":"Class for analytical test functions. Parameters: Name Type Description Default offset float Offset value. Defaults to 0.0. 0.0 hz float Horizontal value. Defaults to 0. 0 seed int Seed value for random number generation. Defaults to 126. 126 Note See Numpy Random Sampling Attributes: Name Type Description offset float Offset value. hz float Horizontal value. seed int Seed value for random number generation. rng Generator Numpy random number generator object. fun_control dict Dictionary containing control parameters for the function. Source code in spotPython/fun/objectivefunctions.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 class analytical : \"\"\" Class for analytical test functions. Args: offset (float): Offset value. Defaults to 0.0. hz (float): Horizontal value. Defaults to 0. seed (int): Seed value for random number generation. Defaults to 126. Note: See [Numpy Random Sampling](https://numpy.org/doc/stable/reference/random/index.html#random-quick-start) Attributes: offset (float): Offset value. hz (float): Horizontal value. seed (int): Seed value for random number generation. rng (Generator): Numpy random number generator object. fun_control (dict): Dictionary containing control parameters for the function. \"\"\" def __init__ ( self , offset : float = 0.0 , hz : float = 0 , seed : int = 126 ) -> None : self . offset = offset self . hz = hz self . seed = seed self . rng = default_rng ( seed = self . seed ) self . fun_control = { \"sigma\" : 0 , \"seed\" : None , \"sel_var\" : None } def __repr__ ( self ) -> str : return f \"analytical(offset= { self . offset } , hz= { self . hz } , seed= { self . seed } )\" def add_noise ( self , y : List [ float ]) -> np . ndarray : \"\"\" Adds noise to the input data. This method takes in a list of float values y as input and adds noise to the data using a random number generator. The method returns a numpy array containing the noisy data. Args: self (analytical): analytical class object. y (List[float]): Input data. Returns: np.ndarray: Noisy data. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> y = np.array([1, 2, 3, 4, 5]) >>> fun = analytical() >>> fun.add_noise(y) array([1. , 2. , 3. , 4. , 5. ]) \"\"\" # Use own rng: if self . fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = self . fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for y_i in y : noise_y = np . append ( noise_y , y_i + rng . normal ( loc = 0 , scale = self . fun_control [ \"sigma\" ], size = 1 ), ) return noise_y def fun_branin_factor ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\" Calculates the Branin function with an additional factor based on the value of x3. Args: X (np.ndarray): A 2D numpy array with shape (n, 3) where n is the number of samples. fun_control (Optional[Dict]): A dictionary containing control parameters for the function. If None, self.fun_control is used. Defaults to None. Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin_factor(X) \"\"\" if fun_control is None : fun_control = self . fun_control if len ( X . shape ) == 1 : X = np . array ([ X ]) if X . shape [ 1 ] != 3 : raise Exception ( \"X must have shape (n, 3)\" ) x1 = X [:, 0 ] x2 = X [:, 1 ] x3 = X [:, 2 ] a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) y = a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s for j in range ( X . shape [ 0 ]): if x3 [ j ] == 1 : y [ j ] = y [ j ] + 10 elif x3 [ j ] == 2 : y [ j ] = y [ j ] - 10 if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y def fun_linear ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Linear function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_linear(X) array([ 6., 15.]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError as err : print ( \"error message:\" , err ) X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum ( X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y def fun_sphere ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Sphere function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_sphere(X) array([14., 77.]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum (( X [ i ] - offset ) ** 2 )) # TODO: move to a separate function: if self . fun_control [ \"sigma\" ] > 0 : # Use own rng: if self . fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for y_i in y : noise_y = np . append ( noise_y , y_i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y def fun_cubed ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Cubed function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_cubed(X) array([ 0., 27.]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum (( X [ i ] - offset ) ** 3 )) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y def fun_forrester ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Forrester function. Function used by [Forr08a, p.83]. f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1]. Starts with three sample points at x=0, x=0.5, and x=1. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_forrester(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , ( 6.0 * X [ i ] - 2 ) ** 2 * np . sin ( 12 * X [ i ] - 4 )) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y def fun_branin ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Branin function. The 2-dim Branin function is defined as y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s, where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi). It has three global minima: f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475). Input domain: This function is usually evaluated on the square x1 in [-5, 10] x x2 in [0, 15]. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x1 = X [:, 0 ] x2 = X [:, 1 ] a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) y = a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y def fun_branin_modified ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Modified Branin function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin_modified(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x = X [:, 0 ] y = X [:, 1 ] X1 = 15 * x - 5 X2 = 15 * y a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi d = 6 e = 10 ff = 1 / ( 8 * np . pi ) y = ( a * ( X2 - b * X1 ** 2 + c * X1 - d ) ** 2 + e * ( 1 - ff ) * np . cos ( X1 ) + e ) + 5 * x # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y def branin_noise ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Branin function with noise. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.branin_noise(X) array([ 0. , 11.99999999]) \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x = X [:, 0 ] y = X [:, 1 ] X1 = 15 * x - 5 X2 = 15 * y a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi d = 6 e = 10 ff = 1 / ( 8 * np . pi ) noiseFree = ( a * ( X2 - b * X1 ** 2 + c * X1 - d ) ** 2 + e * ( 1 - ff ) * np . cos ( X1 ) + e ) + 5 * x noise_y = [] for i in noiseFree : noise_y . append ( i + np . random . standard_normal () * 15 ) return np . array ( noise_y ) def fun_sin_cos ( self , X , fun_control = None ): \"\"\"Sinusoidal function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_sin_cos(X) array([-1. , -0.41614684]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x0 = X [:, 0 ] x1 = X [:, 1 ] y = 2.0 * np . sin ( x0 + self . hz ) + 0.5 * np . cos ( x1 + self . hz ) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y # def fun_forrester_2(self, X): # \"\"\" # Function used by [Forr08a, p.83]. # f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1]. # Starts with three sample points at x=0, x=0.5, and x=1. # Args: # X (flooat): input values (1-dim) # Returns: # float: function value # \"\"\" # try: # X.shape[1] # except ValueError: # X = np.array(X) # if len(X.shape) < 2: # X = np.array([X]) # # y = X[:, 1] # y = (6.0 * X - 2) ** 2 * np.sin(12 * X - 4) # if self.sigma != 0: # noise_y = np.array([], dtype=float) # for i in y: # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) # ) # return noise_y # else: # return y def fun_runge ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k >= 1. Interval: -5 <= x <= 5 Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_runge(X) array([0.0625 , 0.015625 , 0.00390625]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , ( 1 / ( 1 + np . sum (( X [ i ] - offset ) ** 2 )))) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y def fun_wingwt ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Wing weight function. Example from Forrester et al. to understand the weight of an unpainted light aircraft wing as a function of nine design and operational parameters: W = 0.036 S_W**0.758 * Wfw**0.0035 ( A / (cos**2 Lambda))**0.6 * q**0.006 * lambda**0.04 * ( (100 Rtc)/(cos Lambda) ))**-0.3*(Nz Wdg)**0.49 | Symbol | Parameter | Baseline | Minimum | Maximum | |-----------|----------------------------------------|----------|---------|---------| | $S_W$ | Wing area ($ft^2$) | 174 | 150 | 200 | | $W_{fw}$ | Weight of fuel in wing (lb) | 252 | 220 | 300 | | $A$ | Aspect ratio | 7.52 | 6 | 10 | | $Lambda$ | Quarter-chord sweep (deg) | 0 | -10 | 10 | | $q$ | Dynamic pressure at cruise ($lb/ft^2$) | 34 | 16 | 45 | | $lambda$ | Taper ratio | 0.672 | 0.5 | 1 | | $R_{tc}$ | Aerofoil thickness to chord ratio | 0.12 | 0.08 | 0.18 | | $N_z$ | Ultimate load factor | 3.8 | 2.5 | 6 | | $W_{dg}$ | Flight design gross weight (lb) | 2000 | 1700 | 2500 | | $W_p$ | paint weight (lb/ft^2) | 0.064 | 0.025 | 0.08 | Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]]) >>> fun = analytical() >>> fun.fun_wingwt(X) array([0.0625 , 0.015625 , 0.00390625]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) # W_res = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): Sw = X [ i , 0 ] * ( 200 - 150 ) + 150 Wfw = X [ i , 1 ] * ( 300 - 220 ) + 220 A = X [ i , 2 ] * ( 10 - 6 ) + 6 L = ( X [ i , 3 ] * ( 10 - ( - 10 )) - 10 ) * np . pi / 180 q = X [ i , 4 ] * ( 45 - 16 ) + 16 la = X [ i , 5 ] * ( 1 - 0.5 ) + 0.5 Rtc = X [ i , 6 ] * ( 0.18 - 0.08 ) + 0.08 Nz = X [ i , 7 ] * ( 6 - 2.5 ) + 2.5 Wdg = X [ i , 8 ] * ( 2500 - 1700 ) + 1700 Wp = X [ i , 9 ] * ( 0.08 - 0.025 ) + 0.025 # calculation on natural scale W = 0.036 * Sw ** 0.758 * Wfw ** 0.0035 * ( A / np . cos ( L ) ** 2 ) ** 0.6 * q ** 0.006 W = W * la ** 0.04 * ( 100 * Rtc / np . cos ( L )) ** ( - 0.3 ) * ( Nz * Wdg ) ** ( 0.49 ) + Sw * Wp W_res = np . append ( W_res , W ) return W_res def fun_xsin ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Example function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]]) >>> fun = analytical() >>> fun.fun_xsin(X) array([0.84147098, 0.90929743, 0.14112001]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , X [ i ] * np . sin ( 1.0 / X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y def fun_rosen ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Rosenbrock function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2,], [4, 5 ]]) >>> fun = analytical() >>> fun.fun_rosen(X) array([24, 0]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x0 = X [:, 0 ] x1 = X [:, 1 ] b = 10 y = ( x0 - 1 ) ** 2 + b * ( x1 - x0 ** 2 ) ** 2 if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y def fun_random_error ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Return errors for testing spot stability. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2,], [4, 5 ]]) >>> fun = analytical() >>> fun.fun_random_error(X) array([24, 0]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError as err : print ( \"error message:\" , err ) X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): # provoke error: if random () < 0.1 : y = np . append ( y , np . nan ) else : y = np . append ( y , np . sum ( X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : print ( y ) return y","title":"analytical"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.add_noise","text":"Adds noise to the input data. This method takes in a list of float values y as input and adds noise to the data using a random number generator. The method returns a numpy array containing the noisy data. Parameters: Name Type Description Default self analytical analytical class object. required y List [ float ] Input data. required Returns: Type Description np . ndarray np.ndarray: Noisy data. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> y = np . array ([ 1 , 2 , 3 , 4 , 5 ]) >>> fun = analytical () >>> fun . add_noise ( y ) array([1. , 2. , 3. , 4. , 5. ]) Source code in spotPython/fun/objectivefunctions.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def add_noise ( self , y : List [ float ]) -> np . ndarray : \"\"\" Adds noise to the input data. This method takes in a list of float values y as input and adds noise to the data using a random number generator. The method returns a numpy array containing the noisy data. Args: self (analytical): analytical class object. y (List[float]): Input data. Returns: np.ndarray: Noisy data. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> y = np.array([1, 2, 3, 4, 5]) >>> fun = analytical() >>> fun.add_noise(y) array([1. , 2. , 3. , 4. , 5. ]) \"\"\" # Use own rng: if self . fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = self . fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for y_i in y : noise_y = np . append ( noise_y , y_i + rng . normal ( loc = 0 , scale = self . fun_control [ \"sigma\" ], size = 1 ), ) return noise_y","title":"add_noise()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.branin_noise","text":"Branin function with noise. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . branin_noise ( X ) array([ 0. , 11.99999999]) Source code in spotPython/fun/objectivefunctions.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 def branin_noise ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Branin function with noise. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.branin_noise(X) array([ 0. , 11.99999999]) \"\"\" try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x = X [:, 0 ] y = X [:, 1 ] X1 = 15 * x - 5 X2 = 15 * y a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi d = 6 e = 10 ff = 1 / ( 8 * np . pi ) noiseFree = ( a * ( X2 - b * X1 ** 2 + c * X1 - d ) ** 2 + e * ( 1 - ff ) * np . cos ( X1 ) + e ) + 5 * x noise_y = [] for i in noiseFree : noise_y . append ( i + np . random . standard_normal () * 15 ) return np . array ( noise_y )","title":"branin_noise()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_branin","text":"Branin function. The 2-dim Branin function is defined as y = a * (x2 - b * x1 2 + c * x1 - r) 2 + s * (1 - t) * np.cos(x1) + s, where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4 pi 2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8 pi). It has three global minima: f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475). Input domain: This function is usually evaluated on the square x1 in [-5, 10] x x2 in [0, 15]. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_branin ( X ) array([ 0. , 11.99999999]) Source code in spotPython/fun/objectivefunctions.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def fun_branin ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Branin function. The 2-dim Branin function is defined as y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s, where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2), c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi). It has three global minima: f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475). Input domain: This function is usually evaluated on the square x1 in [-5, 10] x x2 in [0, 15]. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x1 = X [:, 0 ] x2 = X [:, 1 ] a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) y = a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y","title":"fun_branin()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_branin_factor","text":"Calculates the Branin function with an additional factor based on the value of x3. Parameters: Name Type Description Default X np . ndarray A 2D numpy array with shape (n, 3) where n is the number of samples. required fun_control Optional [ Dict ] A dictionary containing control parameters for the function. If None, self.fun_control is used. Defaults to None. None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_branin_factor ( X ) Source code in spotPython/fun/objectivefunctions.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def fun_branin_factor ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\" Calculates the Branin function with an additional factor based on the value of x3. Args: X (np.ndarray): A 2D numpy array with shape (n, 3) where n is the number of samples. fun_control (Optional[Dict]): A dictionary containing control parameters for the function. If None, self.fun_control is used. Defaults to None. Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin_factor(X) \"\"\" if fun_control is None : fun_control = self . fun_control if len ( X . shape ) == 1 : X = np . array ([ X ]) if X . shape [ 1 ] != 3 : raise Exception ( \"X must have shape (n, 3)\" ) x1 = X [:, 0 ] x2 = X [:, 1 ] x3 = X [:, 2 ] a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi r = 6 s = 10 t = 1 / ( 8 * np . pi ) y = a * ( x2 - b * x1 ** 2 + c * x1 - r ) ** 2 + s * ( 1 - t ) * np . cos ( x1 ) + s for j in range ( X . shape [ 0 ]): if x3 [ j ] == 1 : y [ j ] = y [ j ] + 10 elif x3 [ j ] == 2 : y [ j ] = y [ j ] - 10 if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y","title":"fun_branin_factor()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_branin_modified","text":"Modified Branin function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_branin_modified ( X ) array([ 0. , 11.99999999]) Source code in spotPython/fun/objectivefunctions.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 def fun_branin_modified ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Modified Branin function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_branin_modified(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x = X [:, 0 ] y = X [:, 1 ] X1 = 15 * x - 5 X2 = 15 * y a = 1 b = 5.1 / ( 4 * np . pi ** 2 ) c = 5 / np . pi d = 6 e = 10 ff = 1 / ( 8 * np . pi ) y = ( a * ( X2 - b * X1 ** 2 + c * X1 - d ) ** 2 + e * ( 1 - ff ) * np . cos ( X1 ) + e ) + 5 * x # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y","title":"fun_branin_modified()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_cubed","text":"Cubed function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_cubed ( X ) array([ 0., 27.]) Source code in spotPython/fun/objectivefunctions.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def fun_cubed ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Cubed function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_cubed(X) array([ 0., 27.]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum (( X [ i ] - offset ) ** 3 )) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y","title":"fun_cubed()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_forrester","text":"Forrester function. Function used by [Forr08a, p.83]. f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1]. Starts with three sample points at x=0, x=0.5, and x=1. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_forrester ( X ) array([ 0. , 11.99999999]) Source code in spotPython/fun/objectivefunctions.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def fun_forrester ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Forrester function. Function used by [Forr08a, p.83]. f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1]. Starts with three sample points at x=0, x=0.5, and x=1. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_forrester(X) array([ 0. , 11.99999999]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , ( 6.0 * X [ i ] - 2 ) ** 2 * np . sin ( 12 * X [ i ] - 4 )) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : # noise_y = np.append( # noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1) noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y","title":"fun_forrester()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_linear","text":"Linear function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_linear ( X ) array([ 6., 15.]) Source code in spotPython/fun/objectivefunctions.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def fun_linear ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Linear function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_linear(X) array([ 6., 15.]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError as err : print ( \"error message:\" , err ) X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum ( X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y","title":"fun_linear()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_random_error","text":"Return errors for testing spot stability. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 ,], [ 4 , 5 ]]) >>> fun = analytical () >>> fun . fun_random_error ( X ) array([24, 0]) Source code in spotPython/fun/objectivefunctions.py 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 def fun_random_error ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Return errors for testing spot stability. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2,], [4, 5 ]]) >>> fun = analytical() >>> fun.fun_random_error(X) array([24, 0]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError as err : print ( \"error message:\" , err ) X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): # provoke error: if random () < 0.1 : y = np . append ( y , np . nan ) else : y = np . append ( y , np . sum ( X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : print ( y ) return y","title":"fun_random_error()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_rosen","text":"Rosenbrock function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 ,], [ 4 , 5 ]]) >>> fun = analytical () >>> fun . fun_rosen ( X ) array([24, 0]) Source code in spotPython/fun/objectivefunctions.py 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 def fun_rosen ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Rosenbrock function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2,], [4, 5 ]]) >>> fun = analytical() >>> fun.fun_rosen(X) array([24, 0]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x0 = X [:, 0 ] x1 = X [:, 1 ] b = 10 y = ( x0 - 1 ) ** 2 + b * ( x1 - x0 ** 2 ) ** 2 if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y","title":"fun_rosen()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_runge","text":"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k >= 1. Interval: -5 <= x <= 5 Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_runge ( X ) array([0.0625 , 0.015625 , 0.00390625]) Source code in spotPython/fun/objectivefunctions.py 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def fun_runge ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k >= 1. Interval: -5 <= x <= 5 Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_runge(X) array([0.0625 , 0.015625 , 0.00390625]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , ( 1 / ( 1 + np . sum (( X [ i ] - offset ) ** 2 )))) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y","title":"fun_runge()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_sin_cos","text":"Sinusoidal function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_sin_cos ( X ) array([-1. , -0.41614684]) Source code in spotPython/fun/objectivefunctions.py 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 def fun_sin_cos ( self , X , fun_control = None ): \"\"\"Sinusoidal function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_sin_cos(X) array([-1. , -0.41614684]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ([ X ]) if X . shape [ 1 ] != 2 : raise Exception x0 = X [:, 0 ] x1 = X [:, 1 ] y = 2.0 * np . sin ( x0 + self . hz ) + 0.5 * np . cos ( x1 + self . hz ) # TODO: move to a separate function: if fun_control [ \"sigma\" ] > 0 : # Use own rng: if fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for i in y : noise_y = np . append ( noise_y , i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y","title":"fun_sin_cos()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_sphere","text":"Sphere function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> fun = analytical () >>> fun . fun_sphere ( X ) array([14., 77.]) Source code in spotPython/fun/objectivefunctions.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def fun_sphere ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Sphere function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3], [4, 5, 6]]) >>> fun = analytical() >>> fun.fun_sphere(X) array([14., 77.]) \"\"\" if fun_control is not None : self . fun_control = fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) offset = np . ones ( X . shape [ 1 ]) * self . offset y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , np . sum (( X [ i ] - offset ) ** 2 )) # TODO: move to a separate function: if self . fun_control [ \"sigma\" ] > 0 : # Use own rng: if self . fun_control [ \"seed\" ] is not None : rng = default_rng ( seed = fun_control [ \"seed\" ]) # Use class rng: else : rng = self . rng noise_y = np . array ([], dtype = float ) for y_i in y : noise_y = np . append ( noise_y , y_i + rng . normal ( loc = 0 , scale = fun_control [ \"sigma\" ], size = 1 )) return noise_y else : return y","title":"fun_sphere()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_wingwt","text":"Wing weight function. Example from Forrester et al. to understand the weight of an unpainted light aircraft wing as a function of nine design and operational parameters: W = 0.036 S_W 0.758 * Wfw 0.0035 ( A / (cos 2 Lambda)) 0.6 * q 0.006 * lambda 0.04 * ( (100 Rtc)/(cos Lambda) )) -0.3*(Nz Wdg) 0.49 Symbol Parameter Baseline Minimum Maximum $S_W$ Wing area ($ft^2$) 174 150 200 $W_{fw}$ Weight of fuel in wing (lb) 252 220 300 $A$ Aspect ratio 7.52 6 10 $Lambda$ Quarter-chord sweep (deg) 0 -10 10 $q$ Dynamic pressure at cruise ($lb/ft^2$) 34 16 45 $lambda$ Taper ratio 0.672 0.5 1 $R_{tc}$ Aerofoil thickness to chord ratio 0.12 0.08 0.18 $N_z$ Ultimate load factor 3.8 2.5 6 $W_{dg}$ Flight design gross weight (lb) 2000 1700 2500 $W_p$ paint weight (lb/ft^2) 0.064 0.025 0.08 Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ], [ 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ]]) >>> fun = analytical () >>> fun . fun_wingwt ( X ) array([0.0625 , 0.015625 , 0.00390625]) Source code in spotPython/fun/objectivefunctions.py 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 def fun_wingwt ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Wing weight function. Example from Forrester et al. to understand the weight of an unpainted light aircraft wing as a function of nine design and operational parameters: W = 0.036 S_W**0.758 * Wfw**0.0035 ( A / (cos**2 Lambda))**0.6 * q**0.006 * lambda**0.04 * ( (100 Rtc)/(cos Lambda) ))**-0.3*(Nz Wdg)**0.49 | Symbol | Parameter | Baseline | Minimum | Maximum | |-----------|----------------------------------------|----------|---------|---------| | $S_W$ | Wing area ($ft^2$) | 174 | 150 | 200 | | $W_{fw}$ | Weight of fuel in wing (lb) | 252 | 220 | 300 | | $A$ | Aspect ratio | 7.52 | 6 | 10 | | $Lambda$ | Quarter-chord sweep (deg) | 0 | -10 | 10 | | $q$ | Dynamic pressure at cruise ($lb/ft^2$) | 34 | 16 | 45 | | $lambda$ | Taper ratio | 0.672 | 0.5 | 1 | | $R_{tc}$ | Aerofoil thickness to chord ratio | 0.12 | 0.08 | 0.18 | | $N_z$ | Ultimate load factor | 3.8 | 2.5 | 6 | | $W_{dg}$ | Flight design gross weight (lb) | 2000 | 1700 | 2500 | | $W_p$ | paint weight (lb/ft^2) | 0.064 | 0.025 | 0.08 | Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]]) >>> fun = analytical() >>> fun.fun_wingwt(X) array([0.0625 , 0.015625 , 0.00390625]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) # W_res = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): Sw = X [ i , 0 ] * ( 200 - 150 ) + 150 Wfw = X [ i , 1 ] * ( 300 - 220 ) + 220 A = X [ i , 2 ] * ( 10 - 6 ) + 6 L = ( X [ i , 3 ] * ( 10 - ( - 10 )) - 10 ) * np . pi / 180 q = X [ i , 4 ] * ( 45 - 16 ) + 16 la = X [ i , 5 ] * ( 1 - 0.5 ) + 0.5 Rtc = X [ i , 6 ] * ( 0.18 - 0.08 ) + 0.08 Nz = X [ i , 7 ] * ( 6 - 2.5 ) + 2.5 Wdg = X [ i , 8 ] * ( 2500 - 1700 ) + 1700 Wp = X [ i , 9 ] * ( 0.08 - 0.025 ) + 0.025 # calculation on natural scale W = 0.036 * Sw ** 0.758 * Wfw ** 0.0035 * ( A / np . cos ( L ) ** 2 ) ** 0.6 * q ** 0.006 W = W * la ** 0.04 * ( 100 * Rtc / np . cos ( L )) ** ( - 0.3 ) * ( Nz * Wdg ) ** ( 0.49 ) + Sw * Wp W_res = np . append ( W_res , W ) return W_res","title":"fun_wingwt()"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_xsin","text":"Example function. Parameters: Name Type Description Default X array input required fun_control dict dict with entries sigma (noise level) and seed (random seed). None Returns: Type Description np . ndarray np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ], [ 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ]]) >>> fun = analytical () >>> fun . fun_xsin ( X ) array([0.84147098, 0.90929743, 0.14112001]) Source code in spotPython/fun/objectivefunctions.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 def fun_xsin ( self , X : np . ndarray , fun_control : Optional [ Dict ] = None ) -> np . ndarray : \"\"\"Example function. Args: X (array): input fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed). Returns: np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples: >>> from spotPython.fun.objectivefunctions import analytical >>> import numpy as np >>> X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]]) >>> fun = analytical() >>> fun.fun_xsin(X) array([0.84147098, 0.90929743, 0.14112001]) \"\"\" if fun_control is None : fun_control = self . fun_control try : X . shape [ 1 ] except ValueError : X = np . array ( X ) if len ( X . shape ) < 2 : X = np . array ([ X ]) y = np . array ([], dtype = float ) for i in range ( X . shape [ 0 ]): y = np . append ( y , X [ i ] * np . sin ( 1.0 / X [ i ])) if self . fun_control [ \"sigma\" ] > 0 : return self . add_noise ( y ) else : return y","title":"fun_xsin()"},{"location":"reference/spotPython/hyperparameters/categorical/","text":"add_missing_elements ( a , b ) \u00b6 Add missing elements from list a to list b. Parameters: Name Type Description Default a list List of elements to check. required b list List of elements to add to. required Returns: Name Type Description list list List of elements with missing elements from list a added. Examples: >>> a = [ 1 , 4 ] b = [1, 2] add_missing_elements(a, b) [1, 2, 4] Source code in spotPython/hyperparameters/categorical.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def add_missing_elements ( a : list , b : list ) -> list : \"\"\"Add missing elements from list a to list b. Arguments: a (list): List of elements to check. b (list): List of elements to add to. Returns: list: List of elements with missing elements from list a added. Examples: >>> a = [1, 4] b = [1, 2] add_missing_elements(a, b) [1, 2, 4] \"\"\" for element in a : if element not in b : b . append ( element ) find_closest_key ( integer_value , encoding_dict ) \u00b6 Given an integer value and an encoding dictionary that maps keys to binary values, this function finds the key in the dictionary whose binary value is closest to the binary representation of the integer value. Parameters: Name Type Description Default integer_value int The integer value to find the closest key for. required encoding_dict dict The encoding dictionary that maps keys to binary values. required Returns: Name Type Description str str The key in the encoding dictionary whose binary value is str closest to the binary representation of the integer value. Examples: >>> encoding_dict = { 'A' : [ 1 , 0 , 0 ], 'B' : [ 0 , 1 , 0 ], 'C' : [ 0 , 0 , 1 ]} find_closest_key(6, encoding_dict) 'B' Source code in spotPython/hyperparameters/categorical.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def find_closest_key ( integer_value : int , encoding_dict : dict ) -> str : \"\"\" Given an integer value and an encoding dictionary that maps keys to binary values, this function finds the key in the dictionary whose binary value is closest to the binary representation of the integer value. Arguments: integer_value (int): The integer value to find the closest key for. encoding_dict (dict): The encoding dictionary that maps keys to binary values. Returns: str: The key in the encoding dictionary whose binary value is closest to the binary representation of the integer value. Examples: >>> encoding_dict = {'A': [1, 0, 0], 'B': [0, 1, 0], 'C': [0, 0, 1]} find_closest_key(6, encoding_dict) 'B' \"\"\" binary_value = [ int ( x ) for x in format ( integer_value , f \"0 { len ( list ( encoding_dict . values ())[ 0 ]) } b\" )] min_distance = float ( \"inf\" ) closest_key = None for key , encoded_value in encoding_dict . items (): distance = sum ([ x != y for x , y in zip ( binary_value , encoded_value )]) if distance < min_distance : min_distance = distance closest_key = key return closest_key get_one_hot ( alg , hyper_param , d = None , filename = 'data.json' ) \u00b6 Get one hot encoded values for a hyper parameter of an algorithm. Parameters: Name Type Description Default alg str Name of the algorithm. required hyper_param str Name of the hyper parameter. required d dict Dictionary of algorithms and their hyperparameters. None filename str Name of the file containing the dictionary. 'data.json' Returns: Name Type Description dict dict Dictionary of hyper parameter values and their one hot encoded values. Examples: >>> alg = \"HoeffdingAdaptiveTreeClassifier\" hyper_param = \"split_criterion\" d = { \"HoeffdingAdaptiveTreeClassifier\": { \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"], \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"], \"bootstrap_sampling\": [\"0\", \"1\"] }, \"HoeffdingTreeClassifier\": { \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"], \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"], \"binary_split\": [\"0\", \"1\"], \"stop_mem_management\": [\"0\", \"1\"] } } get_one_hot(alg, hyper_param, d) {'gini': [1, 0, 0], 'info_gain': [0, 1, 0], 'hellinger': [0, 0, 1]} Source code in spotPython/hyperparameters/categorical.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_one_hot ( alg : str , hyper_param : str , d : dict = None , filename : str = \"data.json\" ) -> dict : \"\"\"Get one hot encoded values for a hyper parameter of an algorithm. Arguments: alg (str): Name of the algorithm. hyper_param (str): Name of the hyper parameter. d (dict): Dictionary of algorithms and their hyperparameters. filename (str): Name of the file containing the dictionary. Returns: dict: Dictionary of hyper parameter values and their one hot encoded values. Examples: >>> alg = \"HoeffdingAdaptiveTreeClassifier\" hyper_param = \"split_criterion\" d = { \"HoeffdingAdaptiveTreeClassifier\": { \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"], \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"], \"bootstrap_sampling\": [\"0\", \"1\"] }, \"HoeffdingTreeClassifier\": { \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"], \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"], \"binary_split\": [\"0\", \"1\"], \"stop_mem_management\": [\"0\", \"1\"] } } get_one_hot(alg, hyper_param, d) {'gini': [1, 0, 0], 'info_gain': [0, 1, 0], 'hellinger': [0, 0, 1]} \"\"\" if d is None : with open ( filename , \"r\" ) as f : d = json . load ( f ) values = d [ alg ][ hyper_param ] one_hot_encoded_values = one_hot_encode ( values ) return one_hot_encoded_values one_hot_encode ( strings ) \u00b6 One hot encode a list of strings. Parameters: Name Type Description Default strings list List of strings to encode. required Returns: Name Type Description dict dict Dictionary of strings and their one hot encoded values. Examples: >>> one_hot_encode ([ 'a' , 'b' , 'c' ]) {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]} Source code in spotPython/hyperparameters/categorical.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def one_hot_encode ( strings ) -> dict : \"\"\"One hot encode a list of strings. Arguments: strings (list): List of strings to encode. Returns: dict: Dictionary of strings and their one hot encoded values. Examples: >>> one_hot_encode(['a', 'b', 'c']) {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]} \"\"\" n = len ( strings ) encoding_dict = {} for i , string in enumerate ( strings ): one_hot_encoded_value = [ 0 ] * n one_hot_encoded_value [ i ] = 1 encoding_dict [ string ] = one_hot_encoded_value return encoding_dict sum_encoded_values ( strings , encoding_dict ) \u00b6 Sum the encoded values of a list of strings. Parameters: Name Type Description Default strings list List of strings to encode. required encoding_dict dict Dictionary of strings and their one hot encoded values. required Returns: Name Type Description int int Decimal value of the sum of the encoded values. Examples: >>> encoding_dict = { 'a' : [ 1 , 0 , 0 ], 'b' : [ 0 , 1 , 0 ], 'c' : [ 0 , 0 , 1 ]} sum_encoded_values(['a', 'b', 'c'], encoding_dict) 7 sum_encoded_values(['a', 'c'], encoding_dict) 5 Source code in spotPython/hyperparameters/categorical.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def sum_encoded_values ( strings , encoding_dict ) -> int : \"\"\"Sum the encoded values of a list of strings. Args: strings (list): List of strings to encode. encoding_dict (dict): Dictionary of strings and their one hot encoded values. Returns: int: Decimal value of the sum of the encoded values. Examples: >>> encoding_dict = {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]} sum_encoded_values(['a', 'b', 'c'], encoding_dict) 7 sum_encoded_values(['a', 'c'], encoding_dict) 5 \"\"\" result = [ 0 ] * len ( list ( encoding_dict . values ())[ 0 ]) for string in strings : encoded_value = encoding_dict . get ( string ) if encoded_value : result = [ sum ( x ) for x in zip ( result , encoded_value )] decimal_result = 0 for i , value in enumerate ( result [:: - 1 ]): decimal_result += value * ( 2 ** i ) return decimal_result","title":"categorical"},{"location":"reference/spotPython/hyperparameters/categorical/#spotPython.hyperparameters.categorical.add_missing_elements","text":"Add missing elements from list a to list b. Parameters: Name Type Description Default a list List of elements to check. required b list List of elements to add to. required Returns: Name Type Description list list List of elements with missing elements from list a added. Examples: >>> a = [ 1 , 4 ] b = [1, 2] add_missing_elements(a, b) [1, 2, 4] Source code in spotPython/hyperparameters/categorical.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def add_missing_elements ( a : list , b : list ) -> list : \"\"\"Add missing elements from list a to list b. Arguments: a (list): List of elements to check. b (list): List of elements to add to. Returns: list: List of elements with missing elements from list a added. Examples: >>> a = [1, 4] b = [1, 2] add_missing_elements(a, b) [1, 2, 4] \"\"\" for element in a : if element not in b : b . append ( element )","title":"add_missing_elements()"},{"location":"reference/spotPython/hyperparameters/categorical/#spotPython.hyperparameters.categorical.find_closest_key","text":"Given an integer value and an encoding dictionary that maps keys to binary values, this function finds the key in the dictionary whose binary value is closest to the binary representation of the integer value. Parameters: Name Type Description Default integer_value int The integer value to find the closest key for. required encoding_dict dict The encoding dictionary that maps keys to binary values. required Returns: Name Type Description str str The key in the encoding dictionary whose binary value is str closest to the binary representation of the integer value. Examples: >>> encoding_dict = { 'A' : [ 1 , 0 , 0 ], 'B' : [ 0 , 1 , 0 ], 'C' : [ 0 , 0 , 1 ]} find_closest_key(6, encoding_dict) 'B' Source code in spotPython/hyperparameters/categorical.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def find_closest_key ( integer_value : int , encoding_dict : dict ) -> str : \"\"\" Given an integer value and an encoding dictionary that maps keys to binary values, this function finds the key in the dictionary whose binary value is closest to the binary representation of the integer value. Arguments: integer_value (int): The integer value to find the closest key for. encoding_dict (dict): The encoding dictionary that maps keys to binary values. Returns: str: The key in the encoding dictionary whose binary value is closest to the binary representation of the integer value. Examples: >>> encoding_dict = {'A': [1, 0, 0], 'B': [0, 1, 0], 'C': [0, 0, 1]} find_closest_key(6, encoding_dict) 'B' \"\"\" binary_value = [ int ( x ) for x in format ( integer_value , f \"0 { len ( list ( encoding_dict . values ())[ 0 ]) } b\" )] min_distance = float ( \"inf\" ) closest_key = None for key , encoded_value in encoding_dict . items (): distance = sum ([ x != y for x , y in zip ( binary_value , encoded_value )]) if distance < min_distance : min_distance = distance closest_key = key return closest_key","title":"find_closest_key()"},{"location":"reference/spotPython/hyperparameters/categorical/#spotPython.hyperparameters.categorical.get_one_hot","text":"Get one hot encoded values for a hyper parameter of an algorithm. Parameters: Name Type Description Default alg str Name of the algorithm. required hyper_param str Name of the hyper parameter. required d dict Dictionary of algorithms and their hyperparameters. None filename str Name of the file containing the dictionary. 'data.json' Returns: Name Type Description dict dict Dictionary of hyper parameter values and their one hot encoded values. Examples: >>> alg = \"HoeffdingAdaptiveTreeClassifier\" hyper_param = \"split_criterion\" d = { \"HoeffdingAdaptiveTreeClassifier\": { \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"], \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"], \"bootstrap_sampling\": [\"0\", \"1\"] }, \"HoeffdingTreeClassifier\": { \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"], \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"], \"binary_split\": [\"0\", \"1\"], \"stop_mem_management\": [\"0\", \"1\"] } } get_one_hot(alg, hyper_param, d) {'gini': [1, 0, 0], 'info_gain': [0, 1, 0], 'hellinger': [0, 0, 1]} Source code in spotPython/hyperparameters/categorical.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_one_hot ( alg : str , hyper_param : str , d : dict = None , filename : str = \"data.json\" ) -> dict : \"\"\"Get one hot encoded values for a hyper parameter of an algorithm. Arguments: alg (str): Name of the algorithm. hyper_param (str): Name of the hyper parameter. d (dict): Dictionary of algorithms and their hyperparameters. filename (str): Name of the file containing the dictionary. Returns: dict: Dictionary of hyper parameter values and their one hot encoded values. Examples: >>> alg = \"HoeffdingAdaptiveTreeClassifier\" hyper_param = \"split_criterion\" d = { \"HoeffdingAdaptiveTreeClassifier\": { \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"], \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"], \"bootstrap_sampling\": [\"0\", \"1\"] }, \"HoeffdingTreeClassifier\": { \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"], \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"], \"binary_split\": [\"0\", \"1\"], \"stop_mem_management\": [\"0\", \"1\"] } } get_one_hot(alg, hyper_param, d) {'gini': [1, 0, 0], 'info_gain': [0, 1, 0], 'hellinger': [0, 0, 1]} \"\"\" if d is None : with open ( filename , \"r\" ) as f : d = json . load ( f ) values = d [ alg ][ hyper_param ] one_hot_encoded_values = one_hot_encode ( values ) return one_hot_encoded_values","title":"get_one_hot()"},{"location":"reference/spotPython/hyperparameters/categorical/#spotPython.hyperparameters.categorical.one_hot_encode","text":"One hot encode a list of strings. Parameters: Name Type Description Default strings list List of strings to encode. required Returns: Name Type Description dict dict Dictionary of strings and their one hot encoded values. Examples: >>> one_hot_encode ([ 'a' , 'b' , 'c' ]) {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]} Source code in spotPython/hyperparameters/categorical.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def one_hot_encode ( strings ) -> dict : \"\"\"One hot encode a list of strings. Arguments: strings (list): List of strings to encode. Returns: dict: Dictionary of strings and their one hot encoded values. Examples: >>> one_hot_encode(['a', 'b', 'c']) {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]} \"\"\" n = len ( strings ) encoding_dict = {} for i , string in enumerate ( strings ): one_hot_encoded_value = [ 0 ] * n one_hot_encoded_value [ i ] = 1 encoding_dict [ string ] = one_hot_encoded_value return encoding_dict","title":"one_hot_encode()"},{"location":"reference/spotPython/hyperparameters/categorical/#spotPython.hyperparameters.categorical.sum_encoded_values","text":"Sum the encoded values of a list of strings. Parameters: Name Type Description Default strings list List of strings to encode. required encoding_dict dict Dictionary of strings and their one hot encoded values. required Returns: Name Type Description int int Decimal value of the sum of the encoded values. Examples: >>> encoding_dict = { 'a' : [ 1 , 0 , 0 ], 'b' : [ 0 , 1 , 0 ], 'c' : [ 0 , 0 , 1 ]} sum_encoded_values(['a', 'b', 'c'], encoding_dict) 7 sum_encoded_values(['a', 'c'], encoding_dict) 5 Source code in spotPython/hyperparameters/categorical.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def sum_encoded_values ( strings , encoding_dict ) -> int : \"\"\"Sum the encoded values of a list of strings. Args: strings (list): List of strings to encode. encoding_dict (dict): Dictionary of strings and their one hot encoded values. Returns: int: Decimal value of the sum of the encoded values. Examples: >>> encoding_dict = {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]} sum_encoded_values(['a', 'b', 'c'], encoding_dict) 7 sum_encoded_values(['a', 'c'], encoding_dict) 5 \"\"\" result = [ 0 ] * len ( list ( encoding_dict . values ())[ 0 ]) for string in strings : encoded_value = encoding_dict . get ( string ) if encoded_value : result = [ sum ( x ) for x in zip ( result , encoded_value )] decimal_result = 0 for i , value in enumerate ( result [:: - 1 ]): decimal_result += value * ( 2 ** i ) return decimal_result","title":"sum_encoded_values()"},{"location":"reference/spotPython/hyperparameters/optimizer/","text":"optimizer_handler ( optimizer_name , params , lr_mult = 1.0 , ** kwargs ) \u00b6 Returns an instance of the specified optimizer. Parameters: Name Type Description Default optimizer_name str The name of the optimizer to use. required params list or torch.Tensor The parameters to optimize. required lr_mult float A multiplier for the learning rate. Defaults to 1.0. 1.0 **kwargs Any Additional keyword arguments for the optimizer. {} Returns: Type Description torch . optim . Optimizer An instance of the specified optimizer. Examples: >>> model = torch . nn . Linear ( 10 , 1 ) >>> optimizer = optimizer_handler ( \"Adadelta\" , model . parameters (), lr_mult = 0.5 ) >>> print ( optimizer ) Adadelta ( Parameter Group 0 eps: 1e-06 lr: 0.5 rho: 0.9 weight_decay: 0 ) Source code in spotPython/hyperparameters/optimizer.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def optimizer_handler ( optimizer_name : str , params : Union [ list , torch . Tensor ], lr_mult : float = 1.0 , ** kwargs : Any ) -> torch . optim . Optimizer : \"\"\"Returns an instance of the specified optimizer. Args: optimizer_name (str): The name of the optimizer to use. params (list or torch.Tensor): The parameters to optimize. lr_mult (float, optional): A multiplier for the learning rate. Defaults to 1.0. **kwargs: Additional keyword arguments for the optimizer. Returns: (torch.optim.Optimizer): An instance of the specified optimizer. Examples: >>> model = torch.nn.Linear(10, 1) >>> optimizer = optimizer_handler(\"Adadelta\", model.parameters(), lr_mult=0.5) >>> print(optimizer) Adadelta ( Parameter Group 0 eps: 1e-06 lr: 0.5 rho: 0.9 weight_decay: 0 ) \"\"\" if optimizer_name == \"Adadelta\" : return torch . optim . Adadelta ( params , lr = lr_mult * 1.0 , rho = 0.9 , eps = 1e-06 , weight_decay = 0 , foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"Adagrad\" : return torch . optim . Adagrad ( params , lr = lr_mult * 0.01 , lr_decay = 0 , weight_decay = 0 , initial_accumulator_value = 0 , eps = 1e-10 , foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"Adam\" : return torch . optim . Adam ( params , lr = lr_mult * 0.001 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0 , amsgrad = False , foreach = None , maximize = False , capturable = False , # differentiable=False, fused = None , ) elif optimizer_name == \"AdamW\" : return torch . optim . AdamW ( params , lr = lr_mult * 0.001 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0.01 , amsgrad = False , foreach = None , maximize = False , capturable = False , # differentiable=False, # fused=None, ) elif optimizer_name == \"SparseAdam\" : return torch . optim . SparseAdam ( params , lr = 0.001 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , maximize = False ) elif optimizer_name == \"Adamax\" : return torch . optim . Adamax ( params , lr = lr_mult * 0.002 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0 , foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"ASGD\" : return torch . optim . ASGD ( params , lr = lr_mult * 0.01 , lambd = 0.0001 , alpha = 0.75 , t0 = 1000000.0 , weight_decay = 0 , foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"LBFGS\" : return torch . optim . LBFGS ( params , lr = lr_mult * 1 , max_iter = 20 , max_eval = None , tolerance_grad = 1e-07 , tolerance_change = 1e-09 , history_size = 100 , line_search_fn = None , ) elif optimizer_name == \"NAdam\" : return torch . optim . NAdam ( params , lr = lr_mult * 0.002 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0 , momentum_decay = 0.004 , foreach = None , # differentiable=False, ) elif optimizer_name == \"RAdam\" : return torch . optim . RAdam ( params , lr = 0.001 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0 , foreach = None , # differentiable=False ) elif optimizer_name == \"RMSprop\" : return torch . optim . RMSprop ( params , lr = lr_mult * 0.01 , alpha = 0.99 , eps = 1e-08 , weight_decay = 0 , momentum = 0 , centered = False , foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"Rprop\" : return torch . optim . Rprop ( params , lr = lr_mult * 0.01 , etas = ( 0.5 , 1.2 ), step_sizes = ( 1e-06 , 50 ), foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"SGD\" : return torch . optim . SGD ( params , lr = lr_mult * 1e-3 , momentum = 0 , dampening = 0 , weight_decay = 0 , nesterov = False , maximize = False , foreach = None , # differentiable=False, ) else : raise ValueError ( f \"Optimizer { optimizer_name } not supported\" )","title":"optimizer"},{"location":"reference/spotPython/hyperparameters/optimizer/#spotPython.hyperparameters.optimizer.optimizer_handler","text":"Returns an instance of the specified optimizer. Parameters: Name Type Description Default optimizer_name str The name of the optimizer to use. required params list or torch.Tensor The parameters to optimize. required lr_mult float A multiplier for the learning rate. Defaults to 1.0. 1.0 **kwargs Any Additional keyword arguments for the optimizer. {} Returns: Type Description torch . optim . Optimizer An instance of the specified optimizer. Examples: >>> model = torch . nn . Linear ( 10 , 1 ) >>> optimizer = optimizer_handler ( \"Adadelta\" , model . parameters (), lr_mult = 0.5 ) >>> print ( optimizer ) Adadelta ( Parameter Group 0 eps: 1e-06 lr: 0.5 rho: 0.9 weight_decay: 0 ) Source code in spotPython/hyperparameters/optimizer.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def optimizer_handler ( optimizer_name : str , params : Union [ list , torch . Tensor ], lr_mult : float = 1.0 , ** kwargs : Any ) -> torch . optim . Optimizer : \"\"\"Returns an instance of the specified optimizer. Args: optimizer_name (str): The name of the optimizer to use. params (list or torch.Tensor): The parameters to optimize. lr_mult (float, optional): A multiplier for the learning rate. Defaults to 1.0. **kwargs: Additional keyword arguments for the optimizer. Returns: (torch.optim.Optimizer): An instance of the specified optimizer. Examples: >>> model = torch.nn.Linear(10, 1) >>> optimizer = optimizer_handler(\"Adadelta\", model.parameters(), lr_mult=0.5) >>> print(optimizer) Adadelta ( Parameter Group 0 eps: 1e-06 lr: 0.5 rho: 0.9 weight_decay: 0 ) \"\"\" if optimizer_name == \"Adadelta\" : return torch . optim . Adadelta ( params , lr = lr_mult * 1.0 , rho = 0.9 , eps = 1e-06 , weight_decay = 0 , foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"Adagrad\" : return torch . optim . Adagrad ( params , lr = lr_mult * 0.01 , lr_decay = 0 , weight_decay = 0 , initial_accumulator_value = 0 , eps = 1e-10 , foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"Adam\" : return torch . optim . Adam ( params , lr = lr_mult * 0.001 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0 , amsgrad = False , foreach = None , maximize = False , capturable = False , # differentiable=False, fused = None , ) elif optimizer_name == \"AdamW\" : return torch . optim . AdamW ( params , lr = lr_mult * 0.001 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0.01 , amsgrad = False , foreach = None , maximize = False , capturable = False , # differentiable=False, # fused=None, ) elif optimizer_name == \"SparseAdam\" : return torch . optim . SparseAdam ( params , lr = 0.001 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , maximize = False ) elif optimizer_name == \"Adamax\" : return torch . optim . Adamax ( params , lr = lr_mult * 0.002 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0 , foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"ASGD\" : return torch . optim . ASGD ( params , lr = lr_mult * 0.01 , lambd = 0.0001 , alpha = 0.75 , t0 = 1000000.0 , weight_decay = 0 , foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"LBFGS\" : return torch . optim . LBFGS ( params , lr = lr_mult * 1 , max_iter = 20 , max_eval = None , tolerance_grad = 1e-07 , tolerance_change = 1e-09 , history_size = 100 , line_search_fn = None , ) elif optimizer_name == \"NAdam\" : return torch . optim . NAdam ( params , lr = lr_mult * 0.002 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0 , momentum_decay = 0.004 , foreach = None , # differentiable=False, ) elif optimizer_name == \"RAdam\" : return torch . optim . RAdam ( params , lr = 0.001 , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0 , foreach = None , # differentiable=False ) elif optimizer_name == \"RMSprop\" : return torch . optim . RMSprop ( params , lr = lr_mult * 0.01 , alpha = 0.99 , eps = 1e-08 , weight_decay = 0 , momentum = 0 , centered = False , foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"Rprop\" : return torch . optim . Rprop ( params , lr = lr_mult * 0.01 , etas = ( 0.5 , 1.2 ), step_sizes = ( 1e-06 , 50 ), foreach = None , maximize = False , # differentiable=False, ) elif optimizer_name == \"SGD\" : return torch . optim . SGD ( params , lr = lr_mult * 1e-3 , momentum = 0 , dampening = 0 , weight_decay = 0 , nesterov = False , maximize = False , foreach = None , # differentiable=False, ) else : raise ValueError ( f \"Optimizer { optimizer_name } not supported\" )","title":"optimizer_handler()"},{"location":"reference/spotPython/hyperparameters/values/","text":"add_core_model_to_fun_control ( core_model , fun_control , hyper_dict , filename = None ) \u00b6 Add the core model to the function control dictionary. Parameters: Name Type Description Default core_model class The core model. required fun_control dict The function control dictionary. required hyper_dict dict The hyper parameter dictionary. required filename str The name of the json file that contains the hyper parameter dictionary. Optional. Default is None. None Returns: Type Description dict The function control dictionary. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) Source code in spotPython/hyperparameters/values.py 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 def add_core_model_to_fun_control ( core_model , fun_control , hyper_dict , filename = None ) -> dict : \"\"\"Add the core model to the function control dictionary. Args: core_model (class): The core model. fun_control (dict): The function control dictionary. hyper_dict (dict): The hyper parameter dictionary. filename (str): The name of the json file that contains the hyper parameter dictionary. Optional. Default is None. Returns: (dict): The function control dictionary. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) \"\"\" fun_control . update ({ \"core_model\" : core_model }) if filename is None : new_hyper_dict = hyper_dict () . load () else : with open ( filename , \"r\" ) as f : new_hyper_dict = json . load ( f ) hyper_dict () . load () fun_control . update ({ \"core_model_hyper_dict\" : new_hyper_dict [ core_model . __name__ ]}) var_type = get_var_type ( fun_control ) var_name = get_var_name ( fun_control ) fun_control . update ({ \"var_type\" : var_type , \"var_name\" : var_name }) assign_values ( X , var_list ) \u00b6 This function takes an np.array X and a list of variable names as input arguments and returns a dictionary where the keys are the variable names and the values are assigned from X. Parameters: Name Type Description Default X np . array A 2D numpy array where each column represents a variable. required var_list list A list of strings representing variable names. required Returns: Name Type Description dict dict A dictionary where keys are variable names and values are assigned from X. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import assign_values >>> X = np . array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]]) >>> var_list = [ 'a' , 'b' ] >>> result = assign_values ( X , var_list ) >>> print ( result ) {'a': array([1, 3, 5]), 'b': array([2, 4, 6])} Source code in spotPython/hyperparameters/values.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def assign_values ( X : np . array , var_list : list ) -> dict : \"\"\" This function takes an np.array X and a list of variable names as input arguments and returns a dictionary where the keys are the variable names and the values are assigned from X. Args: X (np.array): A 2D numpy array where each column represents a variable. var_list (list): A list of strings representing variable names. Returns: dict: A dictionary where keys are variable names and values are assigned from X. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import assign_values >>> X = np.array([[1, 2], [3, 4], [5, 6]]) >>> var_list = ['a', 'b'] >>> result = assign_values(X, var_list) >>> print(result) {'a': array([1, 3, 5]), 'b': array([2, 4, 6])} \"\"\" result = {} for i , var_name in enumerate ( var_list ): result [ var_name ] = X [:, i ] return result convert_keys ( d , var_type ) \u00b6 Convert values in a dictionary to integers based on a list of variable types. This function takes a dictionary d and a list of variable types var_type as arguments. For each key in the dictionary, if the corresponding entry in var_type is not equal to \"num\" , the value associated with that key is converted to an integer. Parameters: Name Type Description Default d dict The input dictionary. required var_type list A list of variable types. If the entry is not \"num\" the corresponding value will be converted to the type \"int\" . required Returns: Name Type Description dict Dict [ str , Union [ int , float ]] The modified dictionary with values converted to integers based on var_type . Examples: >>> from spotPython.utils.prepare import convert_keys >>> d = { 'a' : '1.1' , 'b' : '2' , 'c' : '3.1' } >>> var_type = [ \"int\" , \"num\" , \"int\" ] >>> convert_keys ( d , var_type ) {'a': 1, 'b': '2', 'c': 3} Source code in spotPython/hyperparameters/values.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def convert_keys ( d : Dict [ str , Union [ int , float , str ]], var_type : List [ str ]) -> Dict [ str , Union [ int , float ]]: \"\"\"Convert values in a dictionary to integers based on a list of variable types. This function takes a dictionary `d` and a list of variable types `var_type` as arguments. For each key in the dictionary, if the corresponding entry in `var_type` is not equal to `\"num\"`, the value associated with that key is converted to an integer. Args: d (dict): The input dictionary. var_type (list): A list of variable types. If the entry is not `\"num\"` the corresponding value will be converted to the type `\"int\"`. Returns: dict: The modified dictionary with values converted to integers based on `var_type`. Examples: >>> from spotPython.utils.prepare import convert_keys >>> d = {'a': '1.1', 'b': '2', 'c': '3.1'} >>> var_type = [\"int\", \"num\", \"int\"] >>> convert_keys(d, var_type) {'a': 1, 'b': '2', 'c': 3} \"\"\" keys = list ( d . keys ()) for i in range ( len ( keys )): if var_type [ i ] not in [ \"num\" , \"float\" ]: d [ keys [ i ]] = int ( d [ keys [ i ]]) return d generate_one_config_from_var_dict ( var_dict , fun_control ) \u00b6 Generate one configuration from a dictionary of variables (as a generator). This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary. Parameters: Name Type Description Default var_dict dict A dictionary where keys are variable names and values are numpy arrays. required fun_control dict A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d. required Returns: Type Description Generator [ Dict [ str , Union [ int , float ]], None, None] Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import generate_one_config_from_var_dict >>> var_dict = { 'a' : np . array ([ 1 , 3 , 5 ]), 'b' : np . array ([ 2 , 4 , 6 ])} >>> fun_control = { \"var_type\" : [ \"int\" , \"num\" ]} >>> list ( generate_one_config_from_var_dict ( var_dict , fun_control )) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] Source code in spotPython/hyperparameters/values.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def generate_one_config_from_var_dict ( var_dict : Dict [ str , np . ndarray ], fun_control : Dict [ str , Union [ List [ str ], str ]] ) -> Generator [ Dict [ str , Union [ int , float ]], None , None ]: \"\"\"Generate one configuration from a dictionary of variables (as a generator). This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary. Args: var_dict (dict): A dictionary where keys are variable names and values are numpy arrays. fun_control (dict): A dictionary which (at least) has an entry with the following key: \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding value will be converted to the type \"int\". Returns: Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import generate_one_config_from_var_dict >>> var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])} >>> fun_control = {\"var_type\": [\"int\", \"num\"]} >>> list(generate_one_config_from_var_dict(var_dict, fun_control)) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] \"\"\" for values in iterate_dict_values ( var_dict ): values = convert_keys ( values , fun_control [ \"var_type\" ]) values = get_dict_with_levels_and_types ( fun_control = fun_control , v = values ) values = transform_hyper_parameter_values ( fun_control = fun_control , hyper_parameter_values = values ) yield values get_bound_values ( fun_control , bound , as_list = False ) \u00b6 Generate a list or array from a dictionary. This function takes the values from the keys \u201cbound\u201d in the fun_control[\u201ccore_model_hyper_dict\u201d] dictionary and returns a list or array of the values in the same order as the keys in the dictionary. Parameters: Name Type Description Default fun_control dict A dictionary containing a key \u201ccore_model_hyper_dict\u201d which is a dictionary with keys that have either an \u201cupper\u201d or \u201clower\u201d value. required bound str Either \u201cupper\u201d or \u201clower\u201d, indicating which value to extract from the inner dictionary. required as_list bool If True, return a list. If False, return a numpy array. Default is False. False Returns: Type Description Union [ List , np . ndarray ] list or np.ndarray: A list or array of the extracted values. Examples: >>> from spotPython.utils.prepare import get_bound_values >>> fun_control = { \"core_model_hyper_dict\" : { \"a\" : { \"upper\" : 1 }, \"b\" : { \"upper\" : 2 }}} >>> get_bound_values ( fun_control , \"upper\" , as_list = True ) [1, 2] Source code in spotPython/hyperparameters/values.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 def get_bound_values ( fun_control : dict , bound : str , as_list : bool = False ) -> Union [ List , np . ndarray ]: \"\"\"Generate a list or array from a dictionary. This function takes the values from the keys \"bound\" in the fun_control[\"core_model_hyper_dict\"] dictionary and returns a list or array of the values in the same order as the keys in the dictionary. Args: fun_control (dict): A dictionary containing a key \"core_model_hyper_dict\" which is a dictionary with keys that have either an \"upper\" or \"lower\" value. bound (str): Either \"upper\" or \"lower\", indicating which value to extract from the inner dictionary. as_list (bool): If True, return a list. If False, return a numpy array. Default is False. Returns: list or np.ndarray: A list or array of the extracted values. Raises: ValueError: If bound is not \"upper\" or \"lower\". Examples: >>> from spotPython.utils.prepare import get_bound_values >>> fun_control = {\"core_model_hyper_dict\": {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}}} >>> get_bound_values(fun_control, \"upper\", as_list=True) [1, 2] \"\"\" # Throw value error if bound is not upper or lower: if bound not in [ \"upper\" , \"lower\" ]: raise ValueError ( \"bound must be either 'upper' or 'lower'\" ) d = fun_control [ \"core_model_hyper_dict\" ] b = [] for key , value in d . items (): b . append ( value [ bound ]) if as_list : return b else : return np . array ( b ) get_default_hyperparameters_as_array ( fun_control ) \u00b6 Get the default hyper parameters as array. Parameters: Name Type Description Default fun_control dict The function control dictionary. required Returns: Type Description np . array The default hyper parameters as array. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) get_default_hyperparameters_as_array(fun_control) array([0, 0, 0, 0, 0]) Source code in spotPython/hyperparameters/values.py 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 def get_default_hyperparameters_as_array ( fun_control ) -> np . array : \"\"\"Get the default hyper parameters as array. Args: fun_control (dict): The function control dictionary. Returns: (np.array): The default hyper parameters as array. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) get_default_hyperparameters_as_array(fun_control) array([0, 0, 0, 0, 0]) \"\"\" X0 = get_default_values ( fun_control ) X0 = replace_levels_with_positions ( fun_control [ \"core_model_hyper_dict\" ], X0 ) X0 = get_values_from_dict ( X0 ) X0 = np . array ([ X0 ]) X0 . shape [ 1 ] return X0 get_default_hyperparameters_for_core_model ( fun_control ) \u00b6 Get the default hyper parameters for the core model. Parameters: Name Type Description Default fun_control dict The function control dictionary. required Returns: Type Description dict The default hyper parameters for the core model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) get_default_hyperparameters_for_core_model(fun_control) {'leaf_prediction': 'mean', 'leaf_model': 'NBAdaptive', 'splitter': 'HoeffdingAdaptiveTreeSplitter', 'binary_split': 'info_gain', 'stop_mem_management': False} Source code in spotPython/hyperparameters/values.py 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 def get_default_hyperparameters_for_core_model ( fun_control ) -> dict : \"\"\"Get the default hyper parameters for the core model. Args: fun_control (dict): The function control dictionary. Returns: (dict): The default hyper parameters for the core model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) get_default_hyperparameters_for_core_model(fun_control) {'leaf_prediction': 'mean', 'leaf_model': 'NBAdaptive', 'splitter': 'HoeffdingAdaptiveTreeSplitter', 'binary_split': 'info_gain', 'stop_mem_management': False} \"\"\" values = get_default_values ( fun_control ) values = get_dict_with_levels_and_types ( fun_control = fun_control , v = values ) values = convert_keys ( values , fun_control [ \"var_type\" ]) values = transform_hyper_parameter_values ( fun_control = fun_control , hyper_parameter_values = values ) return values get_default_values ( fun_control ) \u00b6 Get the values from the \u201cdefault\u201d keys from the dictionary fun_control as a dict. If the key of the value has as \u201ctype\u201d the value \u201cint\u201d or \u201cfloat\u201d, convert the value to the corresponding type. Parameters: Name Type Description Default fun_control dict dictionary with levels and types required Returns: Name Type Description new_dict dict dictionary with default values Examples: >>> from spotPython.utils.prepare import get_default_values d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_default_values_from_dict(d) {'leaf_prediction': 'mean', 'leaf_model': 'linear_model.LinearRegression', 'splitter': 'EBSTSplitter', 'binary_split': 0, 'stop_mem_management': 0} Source code in spotPython/hyperparameters/values.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def get_default_values ( fun_control ) -> dict : \"\"\"Get the values from the \"default\" keys from the dictionary fun_control as a dict. If the key of the value has as \"type\" the value \"int\" or \"float\", convert the value to the corresponding type. Args: fun_control (dict): dictionary with levels and types Returns: new_dict (dict): dictionary with default values Examples: >>> from spotPython.utils.prepare import get_default_values d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_default_values_from_dict(d) {'leaf_prediction': 'mean', 'leaf_model': 'linear_model.LinearRegression', 'splitter': 'EBSTSplitter', 'binary_split': 0, 'stop_mem_management': 0} \"\"\" d = fun_control [ \"core_model_hyper_dict\" ] new_dict = {} for key , value in d . items (): if value [ \"type\" ] == \"int\" : new_dict [ key ] = int ( value [ \"default\" ]) elif value [ \"type\" ] == \"float\" : new_dict [ key ] = float ( value [ \"default\" ]) else : new_dict [ key ] = value [ \"default\" ] return new_dict get_dict_with_levels_and_types ( fun_control , v ) \u00b6 Get dictionary with levels and types. The function maps the numerical output of the hyperparameter optimization to the corresponding levels of the hyperparameter needed by the core model, i.e., the tuned algorithm. The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. If the key value in the dictionary is 0, it takes the first value from the list, if it is 1, it takes the second and so on. If a key is not in fun_control, it takes the key from v. If the core_model_parameter_type value is instance, it returns the class of the value from the module via getattr(\u201cclass\u201d, value). Parameters: Name Type Description Default fun_control Dict [ str , Any ] A dictionary containing information about the core model hyperparameters. required v Dict [ str , Any ] A dictionary containing the numerical output of the hyperparameter optimization. required Returns: Type Description Dict [ str , Any ] Dict[str, Any]: A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. Examples: >>> fun_control = { ... \"core_model_hyper_dict\" : { ... \"leaf_prediction\" : { ... \"levels\" : [ \"mean\" , \"model\" , \"adaptive\" ], ... \"type\" : \"factor\" , ... \"default\" : \"mean\" , ... \"core_model_parameter_type\" : \"str\" ... }, ... \"leaf_model\" : { ... \"levels\" : [ ... \"linear_model.LinearRegression\" , ... \"linear_model.PARegressor\" , ... \"linear_model.Perceptron\" ... ], ... \"type\" : \"factor\" , ... \"default\" : \"LinearRegression\" , ... \"core_model_parameter_type\" : \"instance\" ... }, ... \"splitter\" : { ... \"levels\" : [ \"EBSTSplitter\" , \"TEBSTSplitter\" , \"QOSplitter\" ], ... \"type\" : \"factor\" , ... \"default\" : \"EBSTSplitter\" , ... \"core_model_parameter_type\" : \"instance()\" ... }, ... \"binary_split\" : { ... \"levels\" : [ 0 , 1 ], ... \"type\" : \"factor\" , ... \"default\" : 0 , ... \"core_model_parameter_type\" : \"bool\" ... }, ... \"stop_mem_management\" : { ... \"levels\" : [ 0 , 1 ], ... \"type\" : \"factor\" , ... \"default\" : 0 , ... \"core_model_parameter_type\" : \"bool\" ... } ... } ... } >>> v = { ... 'grace_period' : 200 , ... 'max_depth' : 10 , ... 'delta' : 1e-07 , ... 'tau' : 0.05 , ... 'leaf_prediction' : 0 , ... 'leaf_model' : 0 , ... 'model_selector_decay' : 0.95 , ... 'splitter' : 1 , ... 'min_samples_split' : 9 , ... 'binary_split' : 0 , ... 'max_size' : 500.0 ... } >>> get_dict_with_levels_and_types ( fun_control , v ) { 'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 'mean', 'leaf_model': linear_model.LinearRegression, 'model_selector_decay': 0.95, 'splitter': TEBSTSplitter, 'min_samples_split': 9, 'binary_split': False, 'max_size': 500.0 } Source code in spotPython/hyperparameters/values.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def get_dict_with_levels_and_types ( fun_control : Dict [ str , Any ], v : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Get dictionary with levels and types. The function maps the numerical output of the hyperparameter optimization to the corresponding levels of the hyperparameter needed by the core model, i.e., the tuned algorithm. The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. If the key value in the dictionary is 0, it takes the first value from the list, if it is 1, it takes the second and so on. If a key is not in fun_control, it takes the key from v. If the core_model_parameter_type value is instance, it returns the class of the value from the module via getattr(\"class\", value). Args: fun_control (Dict[str, Any]): A dictionary containing information about the core model hyperparameters. v (Dict[str, Any]): A dictionary containing the numerical output of the hyperparameter optimization. Returns: Dict[str, Any]: A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. Examples: >>> fun_control = { ... \"core_model_hyper_dict\": { ... \"leaf_prediction\": { ... \"levels\": [\"mean\", \"model\", \"adaptive\"], ... \"type\": \"factor\", ... \"default\": \"mean\", ... \"core_model_parameter_type\": \"str\" ... }, ... \"leaf_model\": { ... \"levels\": [ ... \"linear_model.LinearRegression\", ... \"linear_model.PARegressor\", ... \"linear_model.Perceptron\" ... ], ... \"type\": \"factor\", ... \"default\": \"LinearRegression\", ... \"core_model_parameter_type\": \"instance\" ... }, ... \"splitter\": { ... \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], ... \"type\": \"factor\", ... \"default\": \"EBSTSplitter\", ... \"core_model_parameter_type\": \"instance()\" ... }, ... \"binary_split\": { ... \"levels\": [0, 1], ... \"type\": \"factor\", ... \"default\": 0, ... \"core_model_parameter_type\": \"bool\" ... }, ... \"stop_mem_management\": { ... \"levels\": [0, 1], ... \"type\": \"factor\", ... \"default\": 0, ... \"core_model_parameter_type\": \"bool\" ... } ... } ... } >>> v = { ... 'grace_period': 200, ... 'max_depth': 10, ... 'delta': 1e-07, ... 'tau': 0.05, ... 'leaf_prediction': 0, ... 'leaf_model': 0, ... 'model_selector_decay': 0.95, ... 'splitter': 1, ... 'min_samples_split': 9, ... 'binary_split': 0, ... 'max_size': 500.0 ... } >>> get_dict_with_levels_and_types(fun_control, v) { 'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 'mean', 'leaf_model': linear_model.LinearRegression, 'model_selector_decay': 0.95, 'splitter': TEBSTSplitter, 'min_samples_split': 9, 'binary_split': False, 'max_size': 500.0 } \"\"\" d = fun_control [ \"core_model_hyper_dict\" ] new_dict = {} for key , value in v . items (): if key in d and d [ key ][ \"type\" ] == \"factor\" : if d [ key ][ \"core_model_parameter_type\" ] == \"instance\" : if \"class_name\" in d [ key ]: mdl = d [ key ][ \"class_name\" ] c = d [ key ][ \"levels\" ][ value ] new_dict [ key ] = class_for_name ( mdl , c ) elif d [ key ][ \"core_model_parameter_type\" ] == \"instance()\" : mdl = d [ key ][ \"class_name\" ] c = d [ key ][ \"levels\" ][ value ] k = class_for_name ( mdl , c ) new_dict [ key ] = k () else : new_dict [ key ] = d [ key ][ \"levels\" ][ value ] else : new_dict [ key ] = v [ key ] return new_dict get_one_config_from_X ( X , fun_control = None ) \u00b6 Get one config from X. Parameters: Name Type Description Default X np . array The array with the hyper parameter values. required fun_control dict The function control dictionary. None Returns: Type Description dict The config dictionary. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_config_from_X(X, fun_control) {'leaf_prediction': 'mean', 'leaf_model': 'NBAdaptive', 'splitter': 'HoeffdingAdaptiveTreeSplitter', 'binary_split': 'info_gain', 'stop_mem_management': False} Source code in spotPython/hyperparameters/values.py 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 def get_one_config_from_X ( X , fun_control = None ): \"\"\"Get one config from X. Args: X (np.array): The array with the hyper parameter values. fun_control (dict): The function control dictionary. Returns: (dict): The config dictionary. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_config_from_X(X, fun_control) {'leaf_prediction': 'mean', 'leaf_model': 'NBAdaptive', 'splitter': 'HoeffdingAdaptiveTreeSplitter', 'binary_split': 'info_gain', 'stop_mem_management': False} \"\"\" var_dict = assign_values ( X , fun_control [ \"var_name\" ]) config = return_conf_list_from_var_dict ( var_dict , fun_control )[ 0 ] return config get_one_core_model_from_X ( X , fun_control = None ) \u00b6 Get one core model from X. Parameters: Name Type Description Default X np . array The array with the hyper parameter values. required fun_control dict The function control dictionary. None Returns: Type Description class The core model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_core_model_from_X(X, fun_control) HoeffdingAdaptiveTreeRegressor() Source code in spotPython/hyperparameters/values.py 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 def get_one_core_model_from_X ( X , fun_control = None ): \"\"\"Get one core model from X. Args: X (np.array): The array with the hyper parameter values. fun_control (dict): The function control dictionary. Returns: (class): The core model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_core_model_from_X(X, fun_control) HoeffdingAdaptiveTreeRegressor() \"\"\" var_dict = assign_values ( X , fun_control [ \"var_name\" ]) config = return_conf_list_from_var_dict ( var_dict , fun_control )[ 0 ] core_model = fun_control [ \"core_model\" ]( ** config ) return core_model get_one_river_model_from_X ( X , fun_control = None ) \u00b6 Get one river model from X. Parameters: Name Type Description Default X np . array The array with the hyper parameter values. required fun_control dict The function control dictionary. None Returns: Type Description class The river model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_river_model_from_X(X, fun_control) HoeffdingAdaptiveTreeRegressor() Source code in spotPython/hyperparameters/values.py 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 def get_one_river_model_from_X ( X , fun_control = None ): \"\"\"Get one river model from X. Args: X (np.array): The array with the hyper parameter values. fun_control (dict): The function control dictionary. Returns: (class): The river model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_river_model_from_X(X, fun_control) HoeffdingAdaptiveTreeRegressor() \"\"\" core_model = get_one_core_model_from_X ( X = X , fun_control = fun_control ) if fun_control [ \"prep_model\" ] is not None : model = compose . Pipeline ( fun_control [ \"prep_model\" ], core_model ) else : model = core_model return model get_one_sklearn_model_from_X ( X , fun_control = None ) \u00b6 Get one sklearn model from X. Parameters: Name Type Description Default X np . array The array with the hyper parameter values. required fun_control dict The function control dictionary. None Returns: Type Description class The sklearn model. Examples: from sklearn.linear_model import LinearRegression from spotRiver.data.sklearn_hyper_dict import SklearnHyperDict fun_control = {} add_core_model_to_fun_control(core_model=LinearRegression, fun_control=func_control, hyper_dict=SklearnHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_sklearn_model_from_X(X, fun_control) LinearRegression() Source code in spotPython/hyperparameters/values.py 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 def get_one_sklearn_model_from_X ( X , fun_control = None ): \"\"\"Get one sklearn model from X. Args: X (np.array): The array with the hyper parameter values. fun_control (dict): The function control dictionary. Returns: (class): The sklearn model. Examples: >>> from sklearn.linear_model import LinearRegression from spotRiver.data.sklearn_hyper_dict import SklearnHyperDict fun_control = {} add_core_model_to_fun_control(core_model=LinearRegression, fun_control=func_control, hyper_dict=SklearnHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_sklearn_model_from_X(X, fun_control) LinearRegression() \"\"\" core_model = get_one_core_model_from_X ( X = X , fun_control = fun_control ) if fun_control [ \"prep_model\" ] is not None : model = make_pipeline ( fun_control [ \"prep_model\" ], core_model ) else : model = core_model return model get_transform ( fun_control ) \u00b6 Get the transformations of the values from the dictionary fun_control as a list. Parameters: Name Type Description Default fun_control dict dictionary with levels and types required Returns: Type Description list list with transformations Examples: >>> from spotPython.utils.prepare import get_transform d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"transform\": \"None\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"transform\": \"None\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"transform\": \"None\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"transform\": \"None\", \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"transform\": \"None\", \"core_model_parameter_type\": \"bool\"}}} get_transform(d) ['None', 'None', 'None', 'None', 'None'] Source code in spotPython/hyperparameters/values.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def get_transform ( fun_control ) -> list : \"\"\"Get the transformations of the values from the dictionary fun_control as a list. Args: fun_control (dict): dictionary with levels and types Returns: (list): list with transformations Examples: >>> from spotPython.utils.prepare import get_transform d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"transform\": \"None\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"transform\": \"None\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"transform\": \"None\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"transform\": \"None\", \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"transform\": \"None\", \"core_model_parameter_type\": \"bool\"}}} get_transform(d) ['None', 'None', 'None', 'None', 'None'] \"\"\" return list ( fun_control [ \"core_model_hyper_dict\" ][ key ][ \"transform\" ] for key in fun_control [ \"core_model_hyper_dict\" ] . keys () ) get_values_from_dict ( dictionary ) \u00b6 Get the values from a dictionary as an array. Generate an np.array that contains the values of the keys of a dictionary in the same order as the keys of the dictionary. Parameters: Name Type Description Default dictionary dict dictionary with values required Returns: Type Description np . array array with values Examples: >>> from spotPython.utils.prepare import get_values_from_dict >>> d = { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } >>> get_values_from_dict ( d ) array([1, 2, 3]) Source code in spotPython/hyperparameters/values.py 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 def get_values_from_dict ( dictionary ) -> np . array : \"\"\"Get the values from a dictionary as an array. Generate an np.array that contains the values of the keys of a dictionary in the same order as the keys of the dictionary. Args: dictionary (dict): dictionary with values Returns: (np.array): array with values Examples: >>> from spotPython.utils.prepare import get_values_from_dict >>> d = {\"a\": 1, \"b\": 2, \"c\": 3} >>> get_values_from_dict(d) array([1, 2, 3]) \"\"\" return np . array ( list ( dictionary . values ())) get_var_name ( fun_control ) \u00b6 Get the names of the values from the dictionary fun_control as a list. Parameters: Name Type Description Default fun_control dict dictionary with names required Returns: Type Description list ist with names Examples: >>> d = { \"core_model_hyper_dict\" :{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_var_name(d) [\u2018leaf_prediction\u2019, \u2018leaf_model\u2019, \u2018splitter\u2019, \u2018binary_split\u2019, \u2018stop_mem_management\u2019] Source code in spotPython/hyperparameters/values.py 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 def get_var_name ( fun_control ) -> list : \"\"\"Get the names of the values from the dictionary fun_control as a list. Args: fun_control (dict): dictionary with names Returns: (list): ist with names Examples: >>> d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_var_name(d) ['leaf_prediction', 'leaf_model', 'splitter', 'binary_split', 'stop_mem_management'] \"\"\" return list ( fun_control [ \"core_model_hyper_dict\" ] . keys ()) get_var_type ( fun_control ) \u00b6 Get the types of the values from the dictionary fun_control as a list. Parameters: Name Type Description Default fun_control dict dictionary with levels and types required Returns: Type Description list list with types Examples: >>> from spotPython.utils.prepare import get_var_type d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_var_type(d) ['factor', 'factor', 'factor', 'factor', 'factor'] Source code in spotPython/hyperparameters/values.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def get_var_type ( fun_control ) -> list : \"\"\"Get the types of the values from the dictionary fun_control as a list. Args: fun_control (dict): dictionary with levels and types Returns: (list): list with types Examples: >>> from spotPython.utils.prepare import get_var_type d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_var_type(d) ['factor', 'factor', 'factor', 'factor', 'factor'] \"\"\" return list ( fun_control [ \"core_model_hyper_dict\" ][ key ][ \"type\" ] for key in fun_control [ \"core_model_hyper_dict\" ] . keys () ) iterate_dict_values ( var_dict ) \u00b6 Iterate over the values of a dictionary of variables. This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary. Parameters: Name Type Description Default var_dict dict A dictionary where keys are variable names and values are numpy arrays. required Returns: Type Description Generator [ Dict [ str , Union [ int , float ]], None, None] Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import iterate_dict_values >>> var_dict = { 'a' : np . array ([ 1 , 3 , 5 ]), 'b' : np . array ([ 2 , 4 , 6 ])} >>> list ( iterate_dict_values ( var_dict )) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] Source code in spotPython/hyperparameters/values.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def iterate_dict_values ( var_dict : Dict [ str , np . ndarray ]) -> Generator [ Dict [ str , Union [ int , float ]], None , None ]: \"\"\"Iterate over the values of a dictionary of variables. This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary. Args: var_dict (dict): A dictionary where keys are variable names and values are numpy arrays. Returns: Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import iterate_dict_values >>> var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])} >>> list(iterate_dict_values(var_dict)) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] \"\"\" n = len ( next ( iter ( var_dict . values ()))) for i in range ( n ): yield { key : value [ i ] for key , value in var_dict . items ()} modify_hyper_parameter_bounds ( fun_control , hyperparameter , bounds ) \u00b6 Parameters: Name Type Description Default fun_control dict fun_control dictionary required hyperparameter str hyperparameter name required bounds list list of two bound values. The first value represents the lower bound and the second value represents the upper bound. required Returns: Name Type Description fun_control dict updated fun_control Examples: >>> from spotPython.utils.prepare import modify_hyper_parameter_levels fun_control = {} core_model = HoeffdingTreeRegressor fun_control.update({\"core_model\": core_model}) fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]}) bounds = [3, 11] fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds) Source code in spotPython/hyperparameters/values.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def modify_hyper_parameter_bounds ( fun_control , hyperparameter , bounds ) -> dict : \"\"\" Args: fun_control (dict): fun_control dictionary hyperparameter (str): hyperparameter name bounds (list): list of two bound values. The first value represents the lower bound and the second value represents the upper bound. Returns: fun_control (dict): updated fun_control Examples: >>> from spotPython.utils.prepare import modify_hyper_parameter_levels fun_control = {} core_model = HoeffdingTreeRegressor fun_control.update({\"core_model\": core_model}) fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]}) bounds = [3, 11] fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds) \"\"\" fun_control [ \"core_model_hyper_dict\" ][ hyperparameter ] . update ({ \"lower\" : bounds [ 0 ]}) fun_control [ \"core_model_hyper_dict\" ][ hyperparameter ] . update ({ \"upper\" : bounds [ 1 ]}) modify_hyper_parameter_levels ( fun_control , hyperparameter , levels ) \u00b6 This function modifies the levels of a hyperparameter in the fun_control dictionary. Parameters: Name Type Description Default fun_control dict fun_control dictionary required hyperparameter str hyperparameter name required levels list list of levels required Returns: Name Type Description fun_control dict updated fun_control Examples: >>> fun_control = {} from spotPython.utils.prepare import modify_hyper_parameter_levels core_model = HoeffdingTreeRegressor fun_control.update({\"core_model\": core_model}) fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]}) levels = [\"mean\", \"model\"] fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels) Source code in spotPython/hyperparameters/values.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def modify_hyper_parameter_levels ( fun_control , hyperparameter , levels ) -> dict : \"\"\" This function modifies the levels of a hyperparameter in the fun_control dictionary. Args: fun_control (dict): fun_control dictionary hyperparameter (str): hyperparameter name levels (list): list of levels Returns: fun_control (dict): updated fun_control Examples: >>> fun_control = {} from spotPython.utils.prepare import modify_hyper_parameter_levels core_model = HoeffdingTreeRegressor fun_control.update({\"core_model\": core_model}) fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]}) levels = [\"mean\", \"model\"] fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels) \"\"\" fun_control [ \"core_model_hyper_dict\" ][ hyperparameter ] . update ({ \"levels\" : levels }) fun_control [ \"core_model_hyper_dict\" ][ hyperparameter ] . update ({ \"lower\" : 0 }) fun_control [ \"core_model_hyper_dict\" ][ hyperparameter ] . update ({ \"upper\" : len ( levels ) - 1 }) replace_levels_with_positions ( hyper_dict , hyper_dict_values ) \u00b6 Replace the levels with the position in the levels list. The function that takes two dictionaries. The first contains as hyperparameters as keys. If the hyperparameter has the key \u201clevels\u201d, then the value of the corresponding hyperparameter in the second dictionary is replaced by the position of the value in the list of levels. The function returns a dictionary with the same keys as the second dictionary. For example, if the second dictionary is {\u201ca\u201d: 1, \u201cb\u201d: \u201cmodel1\u201d, \u201cc\u201d: 3} and the first dictionary is { \u201ca\u201d: {\u201ctype\u201d: \u201cint\u201d}, \u201cb\u201d: {\u201clevels\u201d: [\u201cmodel4\u201d, \u201cmodel5\u201d, \u201cmodel1\u201d]}, \u201cd\u201d: {\u201ctype\u201d: \u201cfloat\u201d}}, then the function should return {\u201ca\u201d: 1, \u201cb\u201d: 2, \u201cc\u201d: 3}. Parameters: Name Type Description Default hyper_dict dict dictionary with levels required hyper_dict_values dict dictionary with values required Returns: Type Description dict dictionary with values Examples: >>> from spotPython.utils.prepare import replace_levels_with_positions hyper_dict = {\"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}} hyper_dict_values = {\"leaf_prediction\": \"mean\", \"leaf_model\": \"linear_model.LinearRegression\", \"splitter\": \"EBSTSplitter\", \"binary_split\": 0, \"stop_mem_management\": 0} replace_levels_with_position(hyper_dict, hyper_dict_values) {'leaf_prediction': 0, 'leaf_model': 0, 'splitter': 0, 'binary_split': 0, 'stop_mem_management': 0} Source code in spotPython/hyperparameters/values.py 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 def replace_levels_with_positions ( hyper_dict , hyper_dict_values ) -> dict : \"\"\"Replace the levels with the position in the levels list. The function that takes two dictionaries. The first contains as hyperparameters as keys. If the hyperparameter has the key \"levels\", then the value of the corresponding hyperparameter in the second dictionary is replaced by the position of the value in the list of levels. The function returns a dictionary with the same keys as the second dictionary. For example, if the second dictionary is {\"a\": 1, \"b\": \"model1\", \"c\": 3} and the first dictionary is { \"a\": {\"type\": \"int\"}, \"b\": {\"levels\": [\"model4\", \"model5\", \"model1\"]}, \"d\": {\"type\": \"float\"}}, then the function should return {\"a\": 1, \"b\": 2, \"c\": 3}. Args: hyper_dict (dict): dictionary with levels hyper_dict_values (dict): dictionary with values Returns: (dict): dictionary with values Examples: >>> from spotPython.utils.prepare import replace_levels_with_positions hyper_dict = {\"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}} hyper_dict_values = {\"leaf_prediction\": \"mean\", \"leaf_model\": \"linear_model.LinearRegression\", \"splitter\": \"EBSTSplitter\", \"binary_split\": 0, \"stop_mem_management\": 0} replace_levels_with_position(hyper_dict, hyper_dict_values) {'leaf_prediction': 0, 'leaf_model': 0, 'splitter': 0, 'binary_split': 0, 'stop_mem_management': 0} \"\"\" hyper_dict_values_new = copy . deepcopy ( hyper_dict_values ) for key , value in hyper_dict_values . items (): if key in hyper_dict . keys (): if \"levels\" in hyper_dict [ key ] . keys (): hyper_dict_values_new [ key ] = hyper_dict [ key ][ \"levels\" ] . index ( value ) return hyper_dict_values_new return_conf_list_from_var_dict ( var_dict , fun_control ) \u00b6 Return a list of configurations from a dictionary of variables. This function takes a dictionary of variables and a dictionary of function control as input arguments. It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries of hyper parameter values. Parameters: Name Type Description Default var_dict dict A dictionary where keys are variable names and values are numpy arrays. required fun_control dict A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d. required Returns: Name Type Description list List [ Dict [ str , Union [ int , float ]]] A list of dictionaries of hyper parameter values. Transformations are applied to the values. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import return_conf_list_from_var_dict >>> var_dict = { 'a' : np . array ([ 1 , 3 , 5 ]), 'b' : np . array ([ 2 , 4 , 6 ])} >>> fun_control = { 'var_type' : [ 'int' , 'int' ]} >>> return_conf_list_from_var_dict ( var_dict , fun_control ) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] Source code in spotPython/hyperparameters/values.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def return_conf_list_from_var_dict ( var_dict : Dict [ str , np . ndarray ], fun_control : Dict [ str , Union [ List [ str ], str ]] ) -> List [ Dict [ str , Union [ int , float ]]]: \"\"\"Return a list of configurations from a dictionary of variables. This function takes a dictionary of variables and a dictionary of function control as input arguments. It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries of hyper parameter values. Args: var_dict (dict): A dictionary where keys are variable names and values are numpy arrays. fun_control (dict): A dictionary which (at least) has an entry with the following key: \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding value will be converted to the type \"int\". Returns: list: A list of dictionaries of hyper parameter values. Transformations are applied to the values. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import return_conf_list_from_var_dict >>> var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])} >>> fun_control = {'var_type': ['int', 'int']} >>> return_conf_list_from_var_dict(var_dict, fun_control) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] \"\"\" conf_list = [] for values in generate_one_config_from_var_dict ( var_dict , fun_control ): conf_list . append ( values ) return conf_list","title":"values"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.add_core_model_to_fun_control","text":"Add the core model to the function control dictionary. Parameters: Name Type Description Default core_model class The core model. required fun_control dict The function control dictionary. required hyper_dict dict The hyper parameter dictionary. required filename str The name of the json file that contains the hyper parameter dictionary. Optional. Default is None. None Returns: Type Description dict The function control dictionary. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) Source code in spotPython/hyperparameters/values.py 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 def add_core_model_to_fun_control ( core_model , fun_control , hyper_dict , filename = None ) -> dict : \"\"\"Add the core model to the function control dictionary. Args: core_model (class): The core model. fun_control (dict): The function control dictionary. hyper_dict (dict): The hyper parameter dictionary. filename (str): The name of the json file that contains the hyper parameter dictionary. Optional. Default is None. Returns: (dict): The function control dictionary. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) \"\"\" fun_control . update ({ \"core_model\" : core_model }) if filename is None : new_hyper_dict = hyper_dict () . load () else : with open ( filename , \"r\" ) as f : new_hyper_dict = json . load ( f ) hyper_dict () . load () fun_control . update ({ \"core_model_hyper_dict\" : new_hyper_dict [ core_model . __name__ ]}) var_type = get_var_type ( fun_control ) var_name = get_var_name ( fun_control ) fun_control . update ({ \"var_type\" : var_type , \"var_name\" : var_name })","title":"add_core_model_to_fun_control()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.assign_values","text":"This function takes an np.array X and a list of variable names as input arguments and returns a dictionary where the keys are the variable names and the values are assigned from X. Parameters: Name Type Description Default X np . array A 2D numpy array where each column represents a variable. required var_list list A list of strings representing variable names. required Returns: Name Type Description dict dict A dictionary where keys are variable names and values are assigned from X. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import assign_values >>> X = np . array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]]) >>> var_list = [ 'a' , 'b' ] >>> result = assign_values ( X , var_list ) >>> print ( result ) {'a': array([1, 3, 5]), 'b': array([2, 4, 6])} Source code in spotPython/hyperparameters/values.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def assign_values ( X : np . array , var_list : list ) -> dict : \"\"\" This function takes an np.array X and a list of variable names as input arguments and returns a dictionary where the keys are the variable names and the values are assigned from X. Args: X (np.array): A 2D numpy array where each column represents a variable. var_list (list): A list of strings representing variable names. Returns: dict: A dictionary where keys are variable names and values are assigned from X. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import assign_values >>> X = np.array([[1, 2], [3, 4], [5, 6]]) >>> var_list = ['a', 'b'] >>> result = assign_values(X, var_list) >>> print(result) {'a': array([1, 3, 5]), 'b': array([2, 4, 6])} \"\"\" result = {} for i , var_name in enumerate ( var_list ): result [ var_name ] = X [:, i ] return result","title":"assign_values()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.convert_keys","text":"Convert values in a dictionary to integers based on a list of variable types. This function takes a dictionary d and a list of variable types var_type as arguments. For each key in the dictionary, if the corresponding entry in var_type is not equal to \"num\" , the value associated with that key is converted to an integer. Parameters: Name Type Description Default d dict The input dictionary. required var_type list A list of variable types. If the entry is not \"num\" the corresponding value will be converted to the type \"int\" . required Returns: Name Type Description dict Dict [ str , Union [ int , float ]] The modified dictionary with values converted to integers based on var_type . Examples: >>> from spotPython.utils.prepare import convert_keys >>> d = { 'a' : '1.1' , 'b' : '2' , 'c' : '3.1' } >>> var_type = [ \"int\" , \"num\" , \"int\" ] >>> convert_keys ( d , var_type ) {'a': 1, 'b': '2', 'c': 3} Source code in spotPython/hyperparameters/values.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def convert_keys ( d : Dict [ str , Union [ int , float , str ]], var_type : List [ str ]) -> Dict [ str , Union [ int , float ]]: \"\"\"Convert values in a dictionary to integers based on a list of variable types. This function takes a dictionary `d` and a list of variable types `var_type` as arguments. For each key in the dictionary, if the corresponding entry in `var_type` is not equal to `\"num\"`, the value associated with that key is converted to an integer. Args: d (dict): The input dictionary. var_type (list): A list of variable types. If the entry is not `\"num\"` the corresponding value will be converted to the type `\"int\"`. Returns: dict: The modified dictionary with values converted to integers based on `var_type`. Examples: >>> from spotPython.utils.prepare import convert_keys >>> d = {'a': '1.1', 'b': '2', 'c': '3.1'} >>> var_type = [\"int\", \"num\", \"int\"] >>> convert_keys(d, var_type) {'a': 1, 'b': '2', 'c': 3} \"\"\" keys = list ( d . keys ()) for i in range ( len ( keys )): if var_type [ i ] not in [ \"num\" , \"float\" ]: d [ keys [ i ]] = int ( d [ keys [ i ]]) return d","title":"convert_keys()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.generate_one_config_from_var_dict","text":"Generate one configuration from a dictionary of variables (as a generator). This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary. Parameters: Name Type Description Default var_dict dict A dictionary where keys are variable names and values are numpy arrays. required fun_control dict A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d. required Returns: Type Description Generator [ Dict [ str , Union [ int , float ]], None, None] Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import generate_one_config_from_var_dict >>> var_dict = { 'a' : np . array ([ 1 , 3 , 5 ]), 'b' : np . array ([ 2 , 4 , 6 ])} >>> fun_control = { \"var_type\" : [ \"int\" , \"num\" ]} >>> list ( generate_one_config_from_var_dict ( var_dict , fun_control )) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] Source code in spotPython/hyperparameters/values.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def generate_one_config_from_var_dict ( var_dict : Dict [ str , np . ndarray ], fun_control : Dict [ str , Union [ List [ str ], str ]] ) -> Generator [ Dict [ str , Union [ int , float ]], None , None ]: \"\"\"Generate one configuration from a dictionary of variables (as a generator). This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary. Args: var_dict (dict): A dictionary where keys are variable names and values are numpy arrays. fun_control (dict): A dictionary which (at least) has an entry with the following key: \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding value will be converted to the type \"int\". Returns: Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import generate_one_config_from_var_dict >>> var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])} >>> fun_control = {\"var_type\": [\"int\", \"num\"]} >>> list(generate_one_config_from_var_dict(var_dict, fun_control)) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] \"\"\" for values in iterate_dict_values ( var_dict ): values = convert_keys ( values , fun_control [ \"var_type\" ]) values = get_dict_with_levels_and_types ( fun_control = fun_control , v = values ) values = transform_hyper_parameter_values ( fun_control = fun_control , hyper_parameter_values = values ) yield values","title":"generate_one_config_from_var_dict()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_bound_values","text":"Generate a list or array from a dictionary. This function takes the values from the keys \u201cbound\u201d in the fun_control[\u201ccore_model_hyper_dict\u201d] dictionary and returns a list or array of the values in the same order as the keys in the dictionary. Parameters: Name Type Description Default fun_control dict A dictionary containing a key \u201ccore_model_hyper_dict\u201d which is a dictionary with keys that have either an \u201cupper\u201d or \u201clower\u201d value. required bound str Either \u201cupper\u201d or \u201clower\u201d, indicating which value to extract from the inner dictionary. required as_list bool If True, return a list. If False, return a numpy array. Default is False. False Returns: Type Description Union [ List , np . ndarray ] list or np.ndarray: A list or array of the extracted values. Examples: >>> from spotPython.utils.prepare import get_bound_values >>> fun_control = { \"core_model_hyper_dict\" : { \"a\" : { \"upper\" : 1 }, \"b\" : { \"upper\" : 2 }}} >>> get_bound_values ( fun_control , \"upper\" , as_list = True ) [1, 2] Source code in spotPython/hyperparameters/values.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 def get_bound_values ( fun_control : dict , bound : str , as_list : bool = False ) -> Union [ List , np . ndarray ]: \"\"\"Generate a list or array from a dictionary. This function takes the values from the keys \"bound\" in the fun_control[\"core_model_hyper_dict\"] dictionary and returns a list or array of the values in the same order as the keys in the dictionary. Args: fun_control (dict): A dictionary containing a key \"core_model_hyper_dict\" which is a dictionary with keys that have either an \"upper\" or \"lower\" value. bound (str): Either \"upper\" or \"lower\", indicating which value to extract from the inner dictionary. as_list (bool): If True, return a list. If False, return a numpy array. Default is False. Returns: list or np.ndarray: A list or array of the extracted values. Raises: ValueError: If bound is not \"upper\" or \"lower\". Examples: >>> from spotPython.utils.prepare import get_bound_values >>> fun_control = {\"core_model_hyper_dict\": {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}}} >>> get_bound_values(fun_control, \"upper\", as_list=True) [1, 2] \"\"\" # Throw value error if bound is not upper or lower: if bound not in [ \"upper\" , \"lower\" ]: raise ValueError ( \"bound must be either 'upper' or 'lower'\" ) d = fun_control [ \"core_model_hyper_dict\" ] b = [] for key , value in d . items (): b . append ( value [ bound ]) if as_list : return b else : return np . array ( b )","title":"get_bound_values()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_default_hyperparameters_as_array","text":"Get the default hyper parameters as array. Parameters: Name Type Description Default fun_control dict The function control dictionary. required Returns: Type Description np . array The default hyper parameters as array. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) get_default_hyperparameters_as_array(fun_control) array([0, 0, 0, 0, 0]) Source code in spotPython/hyperparameters/values.py 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 def get_default_hyperparameters_as_array ( fun_control ) -> np . array : \"\"\"Get the default hyper parameters as array. Args: fun_control (dict): The function control dictionary. Returns: (np.array): The default hyper parameters as array. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) get_default_hyperparameters_as_array(fun_control) array([0, 0, 0, 0, 0]) \"\"\" X0 = get_default_values ( fun_control ) X0 = replace_levels_with_positions ( fun_control [ \"core_model_hyper_dict\" ], X0 ) X0 = get_values_from_dict ( X0 ) X0 = np . array ([ X0 ]) X0 . shape [ 1 ] return X0","title":"get_default_hyperparameters_as_array()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_default_hyperparameters_for_core_model","text":"Get the default hyper parameters for the core model. Parameters: Name Type Description Default fun_control dict The function control dictionary. required Returns: Type Description dict The default hyper parameters for the core model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) get_default_hyperparameters_for_core_model(fun_control) {'leaf_prediction': 'mean', 'leaf_model': 'NBAdaptive', 'splitter': 'HoeffdingAdaptiveTreeSplitter', 'binary_split': 'info_gain', 'stop_mem_management': False} Source code in spotPython/hyperparameters/values.py 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 def get_default_hyperparameters_for_core_model ( fun_control ) -> dict : \"\"\"Get the default hyper parameters for the core model. Args: fun_control (dict): The function control dictionary. Returns: (dict): The default hyper parameters for the core model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) get_default_hyperparameters_for_core_model(fun_control) {'leaf_prediction': 'mean', 'leaf_model': 'NBAdaptive', 'splitter': 'HoeffdingAdaptiveTreeSplitter', 'binary_split': 'info_gain', 'stop_mem_management': False} \"\"\" values = get_default_values ( fun_control ) values = get_dict_with_levels_and_types ( fun_control = fun_control , v = values ) values = convert_keys ( values , fun_control [ \"var_type\" ]) values = transform_hyper_parameter_values ( fun_control = fun_control , hyper_parameter_values = values ) return values","title":"get_default_hyperparameters_for_core_model()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_default_values","text":"Get the values from the \u201cdefault\u201d keys from the dictionary fun_control as a dict. If the key of the value has as \u201ctype\u201d the value \u201cint\u201d or \u201cfloat\u201d, convert the value to the corresponding type. Parameters: Name Type Description Default fun_control dict dictionary with levels and types required Returns: Name Type Description new_dict dict dictionary with default values Examples: >>> from spotPython.utils.prepare import get_default_values d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_default_values_from_dict(d) {'leaf_prediction': 'mean', 'leaf_model': 'linear_model.LinearRegression', 'splitter': 'EBSTSplitter', 'binary_split': 0, 'stop_mem_management': 0} Source code in spotPython/hyperparameters/values.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def get_default_values ( fun_control ) -> dict : \"\"\"Get the values from the \"default\" keys from the dictionary fun_control as a dict. If the key of the value has as \"type\" the value \"int\" or \"float\", convert the value to the corresponding type. Args: fun_control (dict): dictionary with levels and types Returns: new_dict (dict): dictionary with default values Examples: >>> from spotPython.utils.prepare import get_default_values d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_default_values_from_dict(d) {'leaf_prediction': 'mean', 'leaf_model': 'linear_model.LinearRegression', 'splitter': 'EBSTSplitter', 'binary_split': 0, 'stop_mem_management': 0} \"\"\" d = fun_control [ \"core_model_hyper_dict\" ] new_dict = {} for key , value in d . items (): if value [ \"type\" ] == \"int\" : new_dict [ key ] = int ( value [ \"default\" ]) elif value [ \"type\" ] == \"float\" : new_dict [ key ] = float ( value [ \"default\" ]) else : new_dict [ key ] = value [ \"default\" ] return new_dict","title":"get_default_values()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_dict_with_levels_and_types","text":"Get dictionary with levels and types. The function maps the numerical output of the hyperparameter optimization to the corresponding levels of the hyperparameter needed by the core model, i.e., the tuned algorithm. The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. If the key value in the dictionary is 0, it takes the first value from the list, if it is 1, it takes the second and so on. If a key is not in fun_control, it takes the key from v. If the core_model_parameter_type value is instance, it returns the class of the value from the module via getattr(\u201cclass\u201d, value). Parameters: Name Type Description Default fun_control Dict [ str , Any ] A dictionary containing information about the core model hyperparameters. required v Dict [ str , Any ] A dictionary containing the numerical output of the hyperparameter optimization. required Returns: Type Description Dict [ str , Any ] Dict[str, Any]: A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. Examples: >>> fun_control = { ... \"core_model_hyper_dict\" : { ... \"leaf_prediction\" : { ... \"levels\" : [ \"mean\" , \"model\" , \"adaptive\" ], ... \"type\" : \"factor\" , ... \"default\" : \"mean\" , ... \"core_model_parameter_type\" : \"str\" ... }, ... \"leaf_model\" : { ... \"levels\" : [ ... \"linear_model.LinearRegression\" , ... \"linear_model.PARegressor\" , ... \"linear_model.Perceptron\" ... ], ... \"type\" : \"factor\" , ... \"default\" : \"LinearRegression\" , ... \"core_model_parameter_type\" : \"instance\" ... }, ... \"splitter\" : { ... \"levels\" : [ \"EBSTSplitter\" , \"TEBSTSplitter\" , \"QOSplitter\" ], ... \"type\" : \"factor\" , ... \"default\" : \"EBSTSplitter\" , ... \"core_model_parameter_type\" : \"instance()\" ... }, ... \"binary_split\" : { ... \"levels\" : [ 0 , 1 ], ... \"type\" : \"factor\" , ... \"default\" : 0 , ... \"core_model_parameter_type\" : \"bool\" ... }, ... \"stop_mem_management\" : { ... \"levels\" : [ 0 , 1 ], ... \"type\" : \"factor\" , ... \"default\" : 0 , ... \"core_model_parameter_type\" : \"bool\" ... } ... } ... } >>> v = { ... 'grace_period' : 200 , ... 'max_depth' : 10 , ... 'delta' : 1e-07 , ... 'tau' : 0.05 , ... 'leaf_prediction' : 0 , ... 'leaf_model' : 0 , ... 'model_selector_decay' : 0.95 , ... 'splitter' : 1 , ... 'min_samples_split' : 9 , ... 'binary_split' : 0 , ... 'max_size' : 500.0 ... } >>> get_dict_with_levels_and_types ( fun_control , v ) { 'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 'mean', 'leaf_model': linear_model.LinearRegression, 'model_selector_decay': 0.95, 'splitter': TEBSTSplitter, 'min_samples_split': 9, 'binary_split': False, 'max_size': 500.0 } Source code in spotPython/hyperparameters/values.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def get_dict_with_levels_and_types ( fun_control : Dict [ str , Any ], v : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Get dictionary with levels and types. The function maps the numerical output of the hyperparameter optimization to the corresponding levels of the hyperparameter needed by the core model, i.e., the tuned algorithm. The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. If the key value in the dictionary is 0, it takes the first value from the list, if it is 1, it takes the second and so on. If a key is not in fun_control, it takes the key from v. If the core_model_parameter_type value is instance, it returns the class of the value from the module via getattr(\"class\", value). Args: fun_control (Dict[str, Any]): A dictionary containing information about the core model hyperparameters. v (Dict[str, Any]): A dictionary containing the numerical output of the hyperparameter optimization. Returns: Dict[str, Any]: A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. Examples: >>> fun_control = { ... \"core_model_hyper_dict\": { ... \"leaf_prediction\": { ... \"levels\": [\"mean\", \"model\", \"adaptive\"], ... \"type\": \"factor\", ... \"default\": \"mean\", ... \"core_model_parameter_type\": \"str\" ... }, ... \"leaf_model\": { ... \"levels\": [ ... \"linear_model.LinearRegression\", ... \"linear_model.PARegressor\", ... \"linear_model.Perceptron\" ... ], ... \"type\": \"factor\", ... \"default\": \"LinearRegression\", ... \"core_model_parameter_type\": \"instance\" ... }, ... \"splitter\": { ... \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], ... \"type\": \"factor\", ... \"default\": \"EBSTSplitter\", ... \"core_model_parameter_type\": \"instance()\" ... }, ... \"binary_split\": { ... \"levels\": [0, 1], ... \"type\": \"factor\", ... \"default\": 0, ... \"core_model_parameter_type\": \"bool\" ... }, ... \"stop_mem_management\": { ... \"levels\": [0, 1], ... \"type\": \"factor\", ... \"default\": 0, ... \"core_model_parameter_type\": \"bool\" ... } ... } ... } >>> v = { ... 'grace_period': 200, ... 'max_depth': 10, ... 'delta': 1e-07, ... 'tau': 0.05, ... 'leaf_prediction': 0, ... 'leaf_model': 0, ... 'model_selector_decay': 0.95, ... 'splitter': 1, ... 'min_samples_split': 9, ... 'binary_split': 0, ... 'max_size': 500.0 ... } >>> get_dict_with_levels_and_types(fun_control, v) { 'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 'mean', 'leaf_model': linear_model.LinearRegression, 'model_selector_decay': 0.95, 'splitter': TEBSTSplitter, 'min_samples_split': 9, 'binary_split': False, 'max_size': 500.0 } \"\"\" d = fun_control [ \"core_model_hyper_dict\" ] new_dict = {} for key , value in v . items (): if key in d and d [ key ][ \"type\" ] == \"factor\" : if d [ key ][ \"core_model_parameter_type\" ] == \"instance\" : if \"class_name\" in d [ key ]: mdl = d [ key ][ \"class_name\" ] c = d [ key ][ \"levels\" ][ value ] new_dict [ key ] = class_for_name ( mdl , c ) elif d [ key ][ \"core_model_parameter_type\" ] == \"instance()\" : mdl = d [ key ][ \"class_name\" ] c = d [ key ][ \"levels\" ][ value ] k = class_for_name ( mdl , c ) new_dict [ key ] = k () else : new_dict [ key ] = d [ key ][ \"levels\" ][ value ] else : new_dict [ key ] = v [ key ] return new_dict","title":"get_dict_with_levels_and_types()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_one_config_from_X","text":"Get one config from X. Parameters: Name Type Description Default X np . array The array with the hyper parameter values. required fun_control dict The function control dictionary. None Returns: Type Description dict The config dictionary. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_config_from_X(X, fun_control) {'leaf_prediction': 'mean', 'leaf_model': 'NBAdaptive', 'splitter': 'HoeffdingAdaptiveTreeSplitter', 'binary_split': 'info_gain', 'stop_mem_management': False} Source code in spotPython/hyperparameters/values.py 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 def get_one_config_from_X ( X , fun_control = None ): \"\"\"Get one config from X. Args: X (np.array): The array with the hyper parameter values. fun_control (dict): The function control dictionary. Returns: (dict): The config dictionary. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_config_from_X(X, fun_control) {'leaf_prediction': 'mean', 'leaf_model': 'NBAdaptive', 'splitter': 'HoeffdingAdaptiveTreeSplitter', 'binary_split': 'info_gain', 'stop_mem_management': False} \"\"\" var_dict = assign_values ( X , fun_control [ \"var_name\" ]) config = return_conf_list_from_var_dict ( var_dict , fun_control )[ 0 ] return config","title":"get_one_config_from_X()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_one_core_model_from_X","text":"Get one core model from X. Parameters: Name Type Description Default X np . array The array with the hyper parameter values. required fun_control dict The function control dictionary. None Returns: Type Description class The core model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_core_model_from_X(X, fun_control) HoeffdingAdaptiveTreeRegressor() Source code in spotPython/hyperparameters/values.py 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 def get_one_core_model_from_X ( X , fun_control = None ): \"\"\"Get one core model from X. Args: X (np.array): The array with the hyper parameter values. fun_control (dict): The function control dictionary. Returns: (class): The core model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_core_model_from_X(X, fun_control) HoeffdingAdaptiveTreeRegressor() \"\"\" var_dict = assign_values ( X , fun_control [ \"var_name\" ]) config = return_conf_list_from_var_dict ( var_dict , fun_control )[ 0 ] core_model = fun_control [ \"core_model\" ]( ** config ) return core_model","title":"get_one_core_model_from_X()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_one_river_model_from_X","text":"Get one river model from X. Parameters: Name Type Description Default X np . array The array with the hyper parameter values. required fun_control dict The function control dictionary. None Returns: Type Description class The river model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_river_model_from_X(X, fun_control) HoeffdingAdaptiveTreeRegressor() Source code in spotPython/hyperparameters/values.py 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 def get_one_river_model_from_X ( X , fun_control = None ): \"\"\"Get one river model from X. Args: X (np.array): The array with the hyper parameter values. fun_control (dict): The function control dictionary. Returns: (class): The river model. Examples: >>> from river.tree import HoeffdingAdaptiveTreeRegressor from spotRiver.data.river_hyper_dict import RiverHyperDict fun_control = {} add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor, fun_control=func_control, hyper_dict=RiverHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_river_model_from_X(X, fun_control) HoeffdingAdaptiveTreeRegressor() \"\"\" core_model = get_one_core_model_from_X ( X = X , fun_control = fun_control ) if fun_control [ \"prep_model\" ] is not None : model = compose . Pipeline ( fun_control [ \"prep_model\" ], core_model ) else : model = core_model return model","title":"get_one_river_model_from_X()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_one_sklearn_model_from_X","text":"Get one sklearn model from X. Parameters: Name Type Description Default X np . array The array with the hyper parameter values. required fun_control dict The function control dictionary. None Returns: Type Description class The sklearn model. Examples: from sklearn.linear_model import LinearRegression from spotRiver.data.sklearn_hyper_dict import SklearnHyperDict fun_control = {} add_core_model_to_fun_control(core_model=LinearRegression, fun_control=func_control, hyper_dict=SklearnHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_sklearn_model_from_X(X, fun_control) LinearRegression() Source code in spotPython/hyperparameters/values.py 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 def get_one_sklearn_model_from_X ( X , fun_control = None ): \"\"\"Get one sklearn model from X. Args: X (np.array): The array with the hyper parameter values. fun_control (dict): The function control dictionary. Returns: (class): The sklearn model. Examples: >>> from sklearn.linear_model import LinearRegression from spotRiver.data.sklearn_hyper_dict import SklearnHyperDict fun_control = {} add_core_model_to_fun_control(core_model=LinearRegression, fun_control=func_control, hyper_dict=SklearnHyperDict, filename=None) X = np.array([0, 0, 0, 0, 0]) get_one_sklearn_model_from_X(X, fun_control) LinearRegression() \"\"\" core_model = get_one_core_model_from_X ( X = X , fun_control = fun_control ) if fun_control [ \"prep_model\" ] is not None : model = make_pipeline ( fun_control [ \"prep_model\" ], core_model ) else : model = core_model return model","title":"get_one_sklearn_model_from_X()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_transform","text":"Get the transformations of the values from the dictionary fun_control as a list. Parameters: Name Type Description Default fun_control dict dictionary with levels and types required Returns: Type Description list list with transformations Examples: >>> from spotPython.utils.prepare import get_transform d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"transform\": \"None\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"transform\": \"None\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"transform\": \"None\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"transform\": \"None\", \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"transform\": \"None\", \"core_model_parameter_type\": \"bool\"}}} get_transform(d) ['None', 'None', 'None', 'None', 'None'] Source code in spotPython/hyperparameters/values.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def get_transform ( fun_control ) -> list : \"\"\"Get the transformations of the values from the dictionary fun_control as a list. Args: fun_control (dict): dictionary with levels and types Returns: (list): list with transformations Examples: >>> from spotPython.utils.prepare import get_transform d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"transform\": \"None\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"transform\": \"None\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"transform\": \"None\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"transform\": \"None\", \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"transform\": \"None\", \"core_model_parameter_type\": \"bool\"}}} get_transform(d) ['None', 'None', 'None', 'None', 'None'] \"\"\" return list ( fun_control [ \"core_model_hyper_dict\" ][ key ][ \"transform\" ] for key in fun_control [ \"core_model_hyper_dict\" ] . keys () )","title":"get_transform()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_values_from_dict","text":"Get the values from a dictionary as an array. Generate an np.array that contains the values of the keys of a dictionary in the same order as the keys of the dictionary. Parameters: Name Type Description Default dictionary dict dictionary with values required Returns: Type Description np . array array with values Examples: >>> from spotPython.utils.prepare import get_values_from_dict >>> d = { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } >>> get_values_from_dict ( d ) array([1, 2, 3]) Source code in spotPython/hyperparameters/values.py 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 def get_values_from_dict ( dictionary ) -> np . array : \"\"\"Get the values from a dictionary as an array. Generate an np.array that contains the values of the keys of a dictionary in the same order as the keys of the dictionary. Args: dictionary (dict): dictionary with values Returns: (np.array): array with values Examples: >>> from spotPython.utils.prepare import get_values_from_dict >>> d = {\"a\": 1, \"b\": 2, \"c\": 3} >>> get_values_from_dict(d) array([1, 2, 3]) \"\"\" return np . array ( list ( dictionary . values ()))","title":"get_values_from_dict()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_var_name","text":"Get the names of the values from the dictionary fun_control as a list. Parameters: Name Type Description Default fun_control dict dictionary with names required Returns: Type Description list ist with names Examples: >>> d = { \"core_model_hyper_dict\" :{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_var_name(d) [\u2018leaf_prediction\u2019, \u2018leaf_model\u2019, \u2018splitter\u2019, \u2018binary_split\u2019, \u2018stop_mem_management\u2019] Source code in spotPython/hyperparameters/values.py 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 def get_var_name ( fun_control ) -> list : \"\"\"Get the names of the values from the dictionary fun_control as a list. Args: fun_control (dict): dictionary with names Returns: (list): ist with names Examples: >>> d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_var_name(d) ['leaf_prediction', 'leaf_model', 'splitter', 'binary_split', 'stop_mem_management'] \"\"\" return list ( fun_control [ \"core_model_hyper_dict\" ] . keys ())","title":"get_var_name()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_var_type","text":"Get the types of the values from the dictionary fun_control as a list. Parameters: Name Type Description Default fun_control dict dictionary with levels and types required Returns: Type Description list list with types Examples: >>> from spotPython.utils.prepare import get_var_type d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_var_type(d) ['factor', 'factor', 'factor', 'factor', 'factor'] Source code in spotPython/hyperparameters/values.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def get_var_type ( fun_control ) -> list : \"\"\"Get the types of the values from the dictionary fun_control as a list. Args: fun_control (dict): dictionary with levels and types Returns: (list): list with types Examples: >>> from spotPython.utils.prepare import get_var_type d = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}}} get_var_type(d) ['factor', 'factor', 'factor', 'factor', 'factor'] \"\"\" return list ( fun_control [ \"core_model_hyper_dict\" ][ key ][ \"type\" ] for key in fun_control [ \"core_model_hyper_dict\" ] . keys () )","title":"get_var_type()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.iterate_dict_values","text":"Iterate over the values of a dictionary of variables. This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary. Parameters: Name Type Description Default var_dict dict A dictionary where keys are variable names and values are numpy arrays. required Returns: Type Description Generator [ Dict [ str , Union [ int , float ]], None, None] Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import iterate_dict_values >>> var_dict = { 'a' : np . array ([ 1 , 3 , 5 ]), 'b' : np . array ([ 2 , 4 , 6 ])} >>> list ( iterate_dict_values ( var_dict )) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] Source code in spotPython/hyperparameters/values.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def iterate_dict_values ( var_dict : Dict [ str , np . ndarray ]) -> Generator [ Dict [ str , Union [ int , float ]], None , None ]: \"\"\"Iterate over the values of a dictionary of variables. This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary. Args: var_dict (dict): A dictionary where keys are variable names and values are numpy arrays. Returns: Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import iterate_dict_values >>> var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])} >>> list(iterate_dict_values(var_dict)) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] \"\"\" n = len ( next ( iter ( var_dict . values ()))) for i in range ( n ): yield { key : value [ i ] for key , value in var_dict . items ()}","title":"iterate_dict_values()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.modify_hyper_parameter_bounds","text":"Parameters: Name Type Description Default fun_control dict fun_control dictionary required hyperparameter str hyperparameter name required bounds list list of two bound values. The first value represents the lower bound and the second value represents the upper bound. required Returns: Name Type Description fun_control dict updated fun_control Examples: >>> from spotPython.utils.prepare import modify_hyper_parameter_levels fun_control = {} core_model = HoeffdingTreeRegressor fun_control.update({\"core_model\": core_model}) fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]}) bounds = [3, 11] fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds) Source code in spotPython/hyperparameters/values.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def modify_hyper_parameter_bounds ( fun_control , hyperparameter , bounds ) -> dict : \"\"\" Args: fun_control (dict): fun_control dictionary hyperparameter (str): hyperparameter name bounds (list): list of two bound values. The first value represents the lower bound and the second value represents the upper bound. Returns: fun_control (dict): updated fun_control Examples: >>> from spotPython.utils.prepare import modify_hyper_parameter_levels fun_control = {} core_model = HoeffdingTreeRegressor fun_control.update({\"core_model\": core_model}) fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]}) bounds = [3, 11] fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds) \"\"\" fun_control [ \"core_model_hyper_dict\" ][ hyperparameter ] . update ({ \"lower\" : bounds [ 0 ]}) fun_control [ \"core_model_hyper_dict\" ][ hyperparameter ] . update ({ \"upper\" : bounds [ 1 ]})","title":"modify_hyper_parameter_bounds()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.modify_hyper_parameter_levels","text":"This function modifies the levels of a hyperparameter in the fun_control dictionary. Parameters: Name Type Description Default fun_control dict fun_control dictionary required hyperparameter str hyperparameter name required levels list list of levels required Returns: Name Type Description fun_control dict updated fun_control Examples: >>> fun_control = {} from spotPython.utils.prepare import modify_hyper_parameter_levels core_model = HoeffdingTreeRegressor fun_control.update({\"core_model\": core_model}) fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]}) levels = [\"mean\", \"model\"] fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels) Source code in spotPython/hyperparameters/values.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def modify_hyper_parameter_levels ( fun_control , hyperparameter , levels ) -> dict : \"\"\" This function modifies the levels of a hyperparameter in the fun_control dictionary. Args: fun_control (dict): fun_control dictionary hyperparameter (str): hyperparameter name levels (list): list of levels Returns: fun_control (dict): updated fun_control Examples: >>> fun_control = {} from spotPython.utils.prepare import modify_hyper_parameter_levels core_model = HoeffdingTreeRegressor fun_control.update({\"core_model\": core_model}) fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]}) levels = [\"mean\", \"model\"] fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels) \"\"\" fun_control [ \"core_model_hyper_dict\" ][ hyperparameter ] . update ({ \"levels\" : levels }) fun_control [ \"core_model_hyper_dict\" ][ hyperparameter ] . update ({ \"lower\" : 0 }) fun_control [ \"core_model_hyper_dict\" ][ hyperparameter ] . update ({ \"upper\" : len ( levels ) - 1 })","title":"modify_hyper_parameter_levels()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.replace_levels_with_positions","text":"Replace the levels with the position in the levels list. The function that takes two dictionaries. The first contains as hyperparameters as keys. If the hyperparameter has the key \u201clevels\u201d, then the value of the corresponding hyperparameter in the second dictionary is replaced by the position of the value in the list of levels. The function returns a dictionary with the same keys as the second dictionary. For example, if the second dictionary is {\u201ca\u201d: 1, \u201cb\u201d: \u201cmodel1\u201d, \u201cc\u201d: 3} and the first dictionary is { \u201ca\u201d: {\u201ctype\u201d: \u201cint\u201d}, \u201cb\u201d: {\u201clevels\u201d: [\u201cmodel4\u201d, \u201cmodel5\u201d, \u201cmodel1\u201d]}, \u201cd\u201d: {\u201ctype\u201d: \u201cfloat\u201d}}, then the function should return {\u201ca\u201d: 1, \u201cb\u201d: 2, \u201cc\u201d: 3}. Parameters: Name Type Description Default hyper_dict dict dictionary with levels required hyper_dict_values dict dictionary with values required Returns: Type Description dict dictionary with values Examples: >>> from spotPython.utils.prepare import replace_levels_with_positions hyper_dict = {\"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}} hyper_dict_values = {\"leaf_prediction\": \"mean\", \"leaf_model\": \"linear_model.LinearRegression\", \"splitter\": \"EBSTSplitter\", \"binary_split\": 0, \"stop_mem_management\": 0} replace_levels_with_position(hyper_dict, hyper_dict_values) {'leaf_prediction': 0, 'leaf_model': 0, 'splitter': 0, 'binary_split': 0, 'stop_mem_management': 0} Source code in spotPython/hyperparameters/values.py 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 def replace_levels_with_positions ( hyper_dict , hyper_dict_values ) -> dict : \"\"\"Replace the levels with the position in the levels list. The function that takes two dictionaries. The first contains as hyperparameters as keys. If the hyperparameter has the key \"levels\", then the value of the corresponding hyperparameter in the second dictionary is replaced by the position of the value in the list of levels. The function returns a dictionary with the same keys as the second dictionary. For example, if the second dictionary is {\"a\": 1, \"b\": \"model1\", \"c\": 3} and the first dictionary is { \"a\": {\"type\": \"int\"}, \"b\": {\"levels\": [\"model4\", \"model5\", \"model1\"]}, \"d\": {\"type\": \"float\"}}, then the function should return {\"a\": 1, \"b\": 2, \"c\": 3}. Args: hyper_dict (dict): dictionary with levels hyper_dict_values (dict): dictionary with values Returns: (dict): dictionary with values Examples: >>> from spotPython.utils.prepare import replace_levels_with_positions hyper_dict = {\"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"leaf_model\": { \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"], \"type\": \"factor\", \"default\": \"LinearRegression\", \"core_model_parameter_type\": \"instance\"}, \"splitter\": { \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"], \"type\": \"factor\", \"default\": \"EBSTSplitter\", \"core_model_parameter_type\": \"instance()\"}, \"binary_split\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}, \"stop_mem_management\": { \"levels\": [0, 1], \"type\": \"factor\", \"default\": 0, \"core_model_parameter_type\": \"bool\"}} hyper_dict_values = {\"leaf_prediction\": \"mean\", \"leaf_model\": \"linear_model.LinearRegression\", \"splitter\": \"EBSTSplitter\", \"binary_split\": 0, \"stop_mem_management\": 0} replace_levels_with_position(hyper_dict, hyper_dict_values) {'leaf_prediction': 0, 'leaf_model': 0, 'splitter': 0, 'binary_split': 0, 'stop_mem_management': 0} \"\"\" hyper_dict_values_new = copy . deepcopy ( hyper_dict_values ) for key , value in hyper_dict_values . items (): if key in hyper_dict . keys (): if \"levels\" in hyper_dict [ key ] . keys (): hyper_dict_values_new [ key ] = hyper_dict [ key ][ \"levels\" ] . index ( value ) return hyper_dict_values_new","title":"replace_levels_with_positions()"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.return_conf_list_from_var_dict","text":"Return a list of configurations from a dictionary of variables. This function takes a dictionary of variables and a dictionary of function control as input arguments. It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries of hyper parameter values. Parameters: Name Type Description Default var_dict dict A dictionary where keys are variable names and values are numpy arrays. required fun_control dict A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d. required Returns: Name Type Description list List [ Dict [ str , Union [ int , float ]]] A list of dictionaries of hyper parameter values. Transformations are applied to the values. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import return_conf_list_from_var_dict >>> var_dict = { 'a' : np . array ([ 1 , 3 , 5 ]), 'b' : np . array ([ 2 , 4 , 6 ])} >>> fun_control = { 'var_type' : [ 'int' , 'int' ]} >>> return_conf_list_from_var_dict ( var_dict , fun_control ) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] Source code in spotPython/hyperparameters/values.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def return_conf_list_from_var_dict ( var_dict : Dict [ str , np . ndarray ], fun_control : Dict [ str , Union [ List [ str ], str ]] ) -> List [ Dict [ str , Union [ int , float ]]]: \"\"\"Return a list of configurations from a dictionary of variables. This function takes a dictionary of variables and a dictionary of function control as input arguments. It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries of hyper parameter values. Args: var_dict (dict): A dictionary where keys are variable names and values are numpy arrays. fun_control (dict): A dictionary which (at least) has an entry with the following key: \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding value will be converted to the type \"int\". Returns: list: A list of dictionaries of hyper parameter values. Transformations are applied to the values. Examples: >>> import numpy as np >>> from spotPython.utils.prepare import return_conf_list_from_var_dict >>> var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])} >>> fun_control = {'var_type': ['int', 'int']} >>> return_conf_list_from_var_dict(var_dict, fun_control) [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}] \"\"\" conf_list = [] for values in generate_one_config_from_var_dict ( var_dict , fun_control ): conf_list . append ( values ) return conf_list","title":"return_conf_list_from_var_dict()"},{"location":"reference/spotPython/light/cifar10datamodule/","text":"CIFAR10DataModule \u00b6 Bases: pl . LightningDataModule A LightningDataModule for handling CIFAR10 data. Parameters: Name Type Description Default batch_size int The size of the batch. required data_dir str The directory where the data is stored. Defaults to \u201c./data\u201d. './data' num_workers int The number of workers for data loading. Defaults to 0. 0 Attributes: Name Type Description data_train Dataset The training dataset. data_val Dataset The validation dataset. data_test Dataset The test dataset. Source code in spotPython/light/cifar10datamodule.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 class CIFAR10DataModule ( pl . LightningDataModule ): \"\"\" A LightningDataModule for handling CIFAR10 data. Args: batch_size (int): The size of the batch. data_dir (str): The directory where the data is stored. Defaults to \"./data\". num_workers (int): The number of workers for data loading. Defaults to 0. Attributes: data_train (Dataset): The training dataset. data_val (Dataset): The validation dataset. data_test (Dataset): The test dataset. \"\"\" def __init__ ( self , batch_size : int , data_dir : str = \"./data\" , num_workers : int = 0 ): super () . __init__ () self . batch_size = batch_size self . data_dir = data_dir self . num_workers = num_workers def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download CIFAR10 ( root = self . data_dir , train = True , download = True ) CIFAR10 ( root = self . data_dir , train = False , download = True ) def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. \"\"\" # Assign train/val datasets for use in dataloaders if stage == \"fit\" or stage is None : transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) data_full = CIFAR10 ( root = self . data_dir , train = True , transform = transform ) # self.data_train, self.data_val = random_split(daata_full, [45000, 5000]) test_abs = int ( len ( data_full ) * 0.6 ) print ( \"dm.setup(): test_abs\" , test_abs ) self . data_train , self . data_val = random_split ( data_full , [ test_abs , len ( data_full ) - test_abs ]) # Assign test dataset for use in dataloader(s) if stage == \"test\" or stage is None : transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) self . data_test = CIFAR10 ( root = self . data_dir , train = False , transform = transform ) def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. \"\"\" print ( \"train_dataloader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_train , batch_size = self . batch_size , shuffle = True , num_workers = self . num_workers ) def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. \"\"\" print ( \"val_dataloader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_val , batch_size = self . batch_size , shuffle = False , num_workers = self . num_workers ) def test_dataloader ( self ) -> DataLoader : \"\"\" Returns the test dataloader. Returns: DataLoader: The test dataloader. \"\"\" print ( \"train_data_loader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_test , batch_size = self . batch_size , shuffle = False , num_workers = self . num_workers ) prepare_data () \u00b6 Prepares the data for use. Source code in spotPython/light/cifar10datamodule.py 29 30 31 32 33 def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download CIFAR10 ( root = self . data_dir , train = True , download = True ) CIFAR10 ( root = self . data_dir , train = False , download = True ) setup ( stage = None ) \u00b6 Sets up the data for use. Parameters: Name Type Description Default stage Optional [ str ] The current stage. Defaults to None. None Source code in spotPython/light/cifar10datamodule.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. \"\"\" # Assign train/val datasets for use in dataloaders if stage == \"fit\" or stage is None : transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) data_full = CIFAR10 ( root = self . data_dir , train = True , transform = transform ) # self.data_train, self.data_val = random_split(daata_full, [45000, 5000]) test_abs = int ( len ( data_full ) * 0.6 ) print ( \"dm.setup(): test_abs\" , test_abs ) self . data_train , self . data_val = random_split ( data_full , [ test_abs , len ( data_full ) - test_abs ]) # Assign test dataset for use in dataloader(s) if stage == \"test\" or stage is None : transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) self . data_test = CIFAR10 ( root = self . data_dir , train = False , transform = transform ) test_dataloader () \u00b6 Returns the test dataloader. Returns: Name Type Description DataLoader DataLoader The test dataloader. Source code in spotPython/light/cifar10datamodule.py 84 85 86 87 88 89 90 91 92 93 94 def test_dataloader ( self ) -> DataLoader : \"\"\" Returns the test dataloader. Returns: DataLoader: The test dataloader. \"\"\" print ( \"train_data_loader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_test , batch_size = self . batch_size , shuffle = False , num_workers = self . num_workers ) train_dataloader () \u00b6 Returns the training dataloader. Returns: Name Type Description DataLoader DataLoader The training dataloader. Source code in spotPython/light/cifar10datamodule.py 61 62 63 64 65 66 67 68 69 70 def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. \"\"\" print ( \"train_dataloader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_train , batch_size = self . batch_size , shuffle = True , num_workers = self . num_workers ) val_dataloader () \u00b6 Returns the validation dataloader. Returns: Name Type Description DataLoader DataLoader The validation dataloader. Source code in spotPython/light/cifar10datamodule.py 72 73 74 75 76 77 78 79 80 81 82 def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. \"\"\" print ( \"val_dataloader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_val , batch_size = self . batch_size , shuffle = False , num_workers = self . num_workers )","title":"cifar10datamodule"},{"location":"reference/spotPython/light/cifar10datamodule/#spotPython.light.cifar10datamodule.CIFAR10DataModule","text":"Bases: pl . LightningDataModule A LightningDataModule for handling CIFAR10 data. Parameters: Name Type Description Default batch_size int The size of the batch. required data_dir str The directory where the data is stored. Defaults to \u201c./data\u201d. './data' num_workers int The number of workers for data loading. Defaults to 0. 0 Attributes: Name Type Description data_train Dataset The training dataset. data_val Dataset The validation dataset. data_test Dataset The test dataset. Source code in spotPython/light/cifar10datamodule.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 class CIFAR10DataModule ( pl . LightningDataModule ): \"\"\" A LightningDataModule for handling CIFAR10 data. Args: batch_size (int): The size of the batch. data_dir (str): The directory where the data is stored. Defaults to \"./data\". num_workers (int): The number of workers for data loading. Defaults to 0. Attributes: data_train (Dataset): The training dataset. data_val (Dataset): The validation dataset. data_test (Dataset): The test dataset. \"\"\" def __init__ ( self , batch_size : int , data_dir : str = \"./data\" , num_workers : int = 0 ): super () . __init__ () self . batch_size = batch_size self . data_dir = data_dir self . num_workers = num_workers def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download CIFAR10 ( root = self . data_dir , train = True , download = True ) CIFAR10 ( root = self . data_dir , train = False , download = True ) def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. \"\"\" # Assign train/val datasets for use in dataloaders if stage == \"fit\" or stage is None : transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) data_full = CIFAR10 ( root = self . data_dir , train = True , transform = transform ) # self.data_train, self.data_val = random_split(daata_full, [45000, 5000]) test_abs = int ( len ( data_full ) * 0.6 ) print ( \"dm.setup(): test_abs\" , test_abs ) self . data_train , self . data_val = random_split ( data_full , [ test_abs , len ( data_full ) - test_abs ]) # Assign test dataset for use in dataloader(s) if stage == \"test\" or stage is None : transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) self . data_test = CIFAR10 ( root = self . data_dir , train = False , transform = transform ) def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. \"\"\" print ( \"train_dataloader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_train , batch_size = self . batch_size , shuffle = True , num_workers = self . num_workers ) def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. \"\"\" print ( \"val_dataloader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_val , batch_size = self . batch_size , shuffle = False , num_workers = self . num_workers ) def test_dataloader ( self ) -> DataLoader : \"\"\" Returns the test dataloader. Returns: DataLoader: The test dataloader. \"\"\" print ( \"train_data_loader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_test , batch_size = self . batch_size , shuffle = False , num_workers = self . num_workers )","title":"CIFAR10DataModule"},{"location":"reference/spotPython/light/cifar10datamodule/#spotPython.light.cifar10datamodule.CIFAR10DataModule.prepare_data","text":"Prepares the data for use. Source code in spotPython/light/cifar10datamodule.py 29 30 31 32 33 def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download CIFAR10 ( root = self . data_dir , train = True , download = True ) CIFAR10 ( root = self . data_dir , train = False , download = True )","title":"prepare_data()"},{"location":"reference/spotPython/light/cifar10datamodule/#spotPython.light.cifar10datamodule.CIFAR10DataModule.setup","text":"Sets up the data for use. Parameters: Name Type Description Default stage Optional [ str ] The current stage. Defaults to None. None Source code in spotPython/light/cifar10datamodule.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. \"\"\" # Assign train/val datasets for use in dataloaders if stage == \"fit\" or stage is None : transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) data_full = CIFAR10 ( root = self . data_dir , train = True , transform = transform ) # self.data_train, self.data_val = random_split(daata_full, [45000, 5000]) test_abs = int ( len ( data_full ) * 0.6 ) print ( \"dm.setup(): test_abs\" , test_abs ) self . data_train , self . data_val = random_split ( data_full , [ test_abs , len ( data_full ) - test_abs ]) # Assign test dataset for use in dataloader(s) if stage == \"test\" or stage is None : transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) self . data_test = CIFAR10 ( root = self . data_dir , train = False , transform = transform )","title":"setup()"},{"location":"reference/spotPython/light/cifar10datamodule/#spotPython.light.cifar10datamodule.CIFAR10DataModule.test_dataloader","text":"Returns the test dataloader. Returns: Name Type Description DataLoader DataLoader The test dataloader. Source code in spotPython/light/cifar10datamodule.py 84 85 86 87 88 89 90 91 92 93 94 def test_dataloader ( self ) -> DataLoader : \"\"\" Returns the test dataloader. Returns: DataLoader: The test dataloader. \"\"\" print ( \"train_data_loader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_test , batch_size = self . batch_size , shuffle = False , num_workers = self . num_workers )","title":"test_dataloader()"},{"location":"reference/spotPython/light/cifar10datamodule/#spotPython.light.cifar10datamodule.CIFAR10DataModule.train_dataloader","text":"Returns the training dataloader. Returns: Name Type Description DataLoader DataLoader The training dataloader. Source code in spotPython/light/cifar10datamodule.py 61 62 63 64 65 66 67 68 69 70 def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. \"\"\" print ( \"train_dataloader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_train , batch_size = self . batch_size , shuffle = True , num_workers = self . num_workers )","title":"train_dataloader()"},{"location":"reference/spotPython/light/cifar10datamodule/#spotPython.light.cifar10datamodule.CIFAR10DataModule.val_dataloader","text":"Returns the validation dataloader. Returns: Name Type Description DataLoader DataLoader The validation dataloader. Source code in spotPython/light/cifar10datamodule.py 72 73 74 75 76 77 78 79 80 81 82 def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. \"\"\" print ( \"val_dataloader: self.batch_size\" , self . batch_size ) return DataLoader ( self . data_val , batch_size = self . batch_size , shuffle = False , num_workers = self . num_workers )","title":"val_dataloader()"},{"location":"reference/spotPython/light/crossvalidationdatamodule/","text":"CrossValidationDataModule \u00b6 Bases: L . LightningDataModule A LightningDataModule for handling cross-validation data splits. Parameters: Name Type Description Default batch_size int The size of the batch. Defaults to 64. 64 k int The fold number. Defaults to 1. 1 split_seed int The random seed for splitting the data. Defaults to 42. 42 num_splits int The number of splits for cross-validation. Defaults to 10. 10 data_dir str The path to the dataset. Defaults to \u201c./data\u201d. './data' num_workers int The number of workers for data loading. Defaults to 0. 0 pin_memory bool Whether to pin memory for data loading. Defaults to False. False Attributes: Name Type Description data_train Optional [ Dataset ] The training dataset. data_val Optional [ Dataset ] The validation dataset. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule () >>> data_module . setup () >>> print ( f \"Training set size: { len ( data_module . data_train ) } \" ) Training set size: 45000 >>> print ( f \"Validation set size: { len ( data_module . data_val ) } \" ) Validation set size: 5000 >>> print ( f \"Test set size: { len ( data_module . data_test ) } \" ) Test set size: 10000 Source code in spotPython/light/crossvalidationdatamodule.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class CrossValidationDataModule ( L . LightningDataModule ): \"\"\" A LightningDataModule for handling cross-validation data splits. Args: batch_size (int): The size of the batch. Defaults to 64. k (int): The fold number. Defaults to 1. split_seed (int): The random seed for splitting the data. Defaults to 42. num_splits (int): The number of splits for cross-validation. Defaults to 10. data_dir (str): The path to the dataset. Defaults to \"./data\". num_workers (int): The number of workers for data loading. Defaults to 0. pin_memory (bool): Whether to pin memory for data loading. Defaults to False. Attributes: data_train (Optional[Dataset]): The training dataset. data_val (Optional[Dataset]): The validation dataset. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule() >>> data_module.setup() >>> print(f\"Training set size: {len(data_module.data_train)}\") Training set size: 45000 >>> print(f\"Validation set size: {len(data_module.data_val)}\") Validation set size: 5000 >>> print(f\"Test set size: {len(data_module.data_test)}\") Test set size: 10000 \"\"\" def __init__ ( self , batch_size = 64 , k : int = 1 , split_seed : int = 42 , num_splits : int = 10 , data_dir : str = \"./data\" , num_workers : int = 0 , pin_memory : bool = False , ): super () . __init__ () self . batch_size = batch_size self . data_dir = data_dir self . num_workers = num_workers self . k = k self . split_seed = split_seed self . num_splits = num_splits self . pin_memory = pin_memory self . save_hyperparameters ( logger = False ) assert 0 <= self . k < self . num_splits , \"incorrect fold number\" # no data transformations self . transforms = None self . data_train : Optional [ Dataset ] = None self . data_val : Optional [ Dataset ] = None def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download pass def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. \"\"\" if not self . data_train and not self . data_val : dataset_full = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) kf = KFold ( n_splits = self . hparams . num_splits , shuffle = True , random_state = self . hparams . split_seed ) all_splits = [ k for k in kf . split ( dataset_full )] train_indexes , val_indexes = all_splits [ self . hparams . k ] train_indexes , val_indexes = train_indexes . tolist (), val_indexes . tolist () self . data_train = Subset ( dataset_full , train_indexes ) print ( f \"Train Dataset Size: { len ( self . data_train ) } \" ) self . data_val = Subset ( dataset_full , val_indexes ) print ( f \"Val Dataset Size: { len ( self . data_val ) } \" ) def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule() >>> data_module.setup() >>> train_dataloader = data_module.train_dataloader() >>> print(f\"Training set size: {len(train_dataloader.dataset)}\") Training set size: 45000 \"\"\" return DataLoader ( dataset = self . data_train , batch_size = self . hparams . batch_size , num_workers = self . hparams . num_workers , pin_memory = self . hparams . pin_memory , shuffle = True , ) def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule() >>> data_module.setup() >>> val_dataloader = data_module.val_dataloader() >>> print(f\"Validation set size: {len(val_dataloader.dataset)}\") Validation set size: 5000 \"\"\" return DataLoader ( dataset = self . data_val , batch_size = self . hparams . batch_size , num_workers = self . hparams . num_workers , pin_memory = self . hparams . pin_memory , ) prepare_data () \u00b6 Prepares the data for use. Source code in spotPython/light/crossvalidationdatamodule.py 64 65 66 67 def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download pass setup ( stage = None ) \u00b6 Sets up the data for use. Parameters: Name Type Description Default stage Optional [ str ] The current stage. Defaults to None. None Source code in spotPython/light/crossvalidationdatamodule.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. \"\"\" if not self . data_train and not self . data_val : dataset_full = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) kf = KFold ( n_splits = self . hparams . num_splits , shuffle = True , random_state = self . hparams . split_seed ) all_splits = [ k for k in kf . split ( dataset_full )] train_indexes , val_indexes = all_splits [ self . hparams . k ] train_indexes , val_indexes = train_indexes . tolist (), val_indexes . tolist () self . data_train = Subset ( dataset_full , train_indexes ) print ( f \"Train Dataset Size: { len ( self . data_train ) } \" ) self . data_val = Subset ( dataset_full , val_indexes ) print ( f \"Val Dataset Size: { len ( self . data_val ) } \" ) train_dataloader () \u00b6 Returns the training dataloader. Returns: Name Type Description DataLoader DataLoader The training dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule () >>> data_module . setup () >>> train_dataloader = data_module . train_dataloader () >>> print ( f \"Training set size: { len ( train_dataloader . dataset ) } \" ) Training set size: 45000 Source code in spotPython/light/crossvalidationdatamodule.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule() >>> data_module.setup() >>> train_dataloader = data_module.train_dataloader() >>> print(f\"Training set size: {len(train_dataloader.dataset)}\") Training set size: 45000 \"\"\" return DataLoader ( dataset = self . data_train , batch_size = self . hparams . batch_size , num_workers = self . hparams . num_workers , pin_memory = self . hparams . pin_memory , shuffle = True , ) val_dataloader () \u00b6 Returns the validation dataloader. Returns: Name Type Description DataLoader DataLoader The validation dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule () >>> data_module . setup () >>> val_dataloader = data_module . val_dataloader () >>> print ( f \"Validation set size: { len ( val_dataloader . dataset ) } \" ) Validation set size: 5000 Source code in spotPython/light/crossvalidationdatamodule.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule() >>> data_module.setup() >>> val_dataloader = data_module.val_dataloader() >>> print(f\"Validation set size: {len(val_dataloader.dataset)}\") Validation set size: 5000 \"\"\" return DataLoader ( dataset = self . data_val , batch_size = self . hparams . batch_size , num_workers = self . hparams . num_workers , pin_memory = self . hparams . pin_memory , )","title":"crossvalidationdatamodule"},{"location":"reference/spotPython/light/crossvalidationdatamodule/#spotPython.light.crossvalidationdatamodule.CrossValidationDataModule","text":"Bases: L . LightningDataModule A LightningDataModule for handling cross-validation data splits. Parameters: Name Type Description Default batch_size int The size of the batch. Defaults to 64. 64 k int The fold number. Defaults to 1. 1 split_seed int The random seed for splitting the data. Defaults to 42. 42 num_splits int The number of splits for cross-validation. Defaults to 10. 10 data_dir str The path to the dataset. Defaults to \u201c./data\u201d. './data' num_workers int The number of workers for data loading. Defaults to 0. 0 pin_memory bool Whether to pin memory for data loading. Defaults to False. False Attributes: Name Type Description data_train Optional [ Dataset ] The training dataset. data_val Optional [ Dataset ] The validation dataset. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule () >>> data_module . setup () >>> print ( f \"Training set size: { len ( data_module . data_train ) } \" ) Training set size: 45000 >>> print ( f \"Validation set size: { len ( data_module . data_val ) } \" ) Validation set size: 5000 >>> print ( f \"Test set size: { len ( data_module . data_test ) } \" ) Test set size: 10000 Source code in spotPython/light/crossvalidationdatamodule.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class CrossValidationDataModule ( L . LightningDataModule ): \"\"\" A LightningDataModule for handling cross-validation data splits. Args: batch_size (int): The size of the batch. Defaults to 64. k (int): The fold number. Defaults to 1. split_seed (int): The random seed for splitting the data. Defaults to 42. num_splits (int): The number of splits for cross-validation. Defaults to 10. data_dir (str): The path to the dataset. Defaults to \"./data\". num_workers (int): The number of workers for data loading. Defaults to 0. pin_memory (bool): Whether to pin memory for data loading. Defaults to False. Attributes: data_train (Optional[Dataset]): The training dataset. data_val (Optional[Dataset]): The validation dataset. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule() >>> data_module.setup() >>> print(f\"Training set size: {len(data_module.data_train)}\") Training set size: 45000 >>> print(f\"Validation set size: {len(data_module.data_val)}\") Validation set size: 5000 >>> print(f\"Test set size: {len(data_module.data_test)}\") Test set size: 10000 \"\"\" def __init__ ( self , batch_size = 64 , k : int = 1 , split_seed : int = 42 , num_splits : int = 10 , data_dir : str = \"./data\" , num_workers : int = 0 , pin_memory : bool = False , ): super () . __init__ () self . batch_size = batch_size self . data_dir = data_dir self . num_workers = num_workers self . k = k self . split_seed = split_seed self . num_splits = num_splits self . pin_memory = pin_memory self . save_hyperparameters ( logger = False ) assert 0 <= self . k < self . num_splits , \"incorrect fold number\" # no data transformations self . transforms = None self . data_train : Optional [ Dataset ] = None self . data_val : Optional [ Dataset ] = None def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download pass def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. \"\"\" if not self . data_train and not self . data_val : dataset_full = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) kf = KFold ( n_splits = self . hparams . num_splits , shuffle = True , random_state = self . hparams . split_seed ) all_splits = [ k for k in kf . split ( dataset_full )] train_indexes , val_indexes = all_splits [ self . hparams . k ] train_indexes , val_indexes = train_indexes . tolist (), val_indexes . tolist () self . data_train = Subset ( dataset_full , train_indexes ) print ( f \"Train Dataset Size: { len ( self . data_train ) } \" ) self . data_val = Subset ( dataset_full , val_indexes ) print ( f \"Val Dataset Size: { len ( self . data_val ) } \" ) def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule() >>> data_module.setup() >>> train_dataloader = data_module.train_dataloader() >>> print(f\"Training set size: {len(train_dataloader.dataset)}\") Training set size: 45000 \"\"\" return DataLoader ( dataset = self . data_train , batch_size = self . hparams . batch_size , num_workers = self . hparams . num_workers , pin_memory = self . hparams . pin_memory , shuffle = True , ) def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule() >>> data_module.setup() >>> val_dataloader = data_module.val_dataloader() >>> print(f\"Validation set size: {len(val_dataloader.dataset)}\") Validation set size: 5000 \"\"\" return DataLoader ( dataset = self . data_val , batch_size = self . hparams . batch_size , num_workers = self . hparams . num_workers , pin_memory = self . hparams . pin_memory , )","title":"CrossValidationDataModule"},{"location":"reference/spotPython/light/crossvalidationdatamodule/#spotPython.light.crossvalidationdatamodule.CrossValidationDataModule.prepare_data","text":"Prepares the data for use. Source code in spotPython/light/crossvalidationdatamodule.py 64 65 66 67 def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download pass","title":"prepare_data()"},{"location":"reference/spotPython/light/crossvalidationdatamodule/#spotPython.light.crossvalidationdatamodule.CrossValidationDataModule.setup","text":"Sets up the data for use. Parameters: Name Type Description Default stage Optional [ str ] The current stage. Defaults to None. None Source code in spotPython/light/crossvalidationdatamodule.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. \"\"\" if not self . data_train and not self . data_val : dataset_full = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) kf = KFold ( n_splits = self . hparams . num_splits , shuffle = True , random_state = self . hparams . split_seed ) all_splits = [ k for k in kf . split ( dataset_full )] train_indexes , val_indexes = all_splits [ self . hparams . k ] train_indexes , val_indexes = train_indexes . tolist (), val_indexes . tolist () self . data_train = Subset ( dataset_full , train_indexes ) print ( f \"Train Dataset Size: { len ( self . data_train ) } \" ) self . data_val = Subset ( dataset_full , val_indexes ) print ( f \"Val Dataset Size: { len ( self . data_val ) } \" )","title":"setup()"},{"location":"reference/spotPython/light/crossvalidationdatamodule/#spotPython.light.crossvalidationdatamodule.CrossValidationDataModule.train_dataloader","text":"Returns the training dataloader. Returns: Name Type Description DataLoader DataLoader The training dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule () >>> data_module . setup () >>> train_dataloader = data_module . train_dataloader () >>> print ( f \"Training set size: { len ( train_dataloader . dataset ) } \" ) Training set size: 45000 Source code in spotPython/light/crossvalidationdatamodule.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule() >>> data_module.setup() >>> train_dataloader = data_module.train_dataloader() >>> print(f\"Training set size: {len(train_dataloader.dataset)}\") Training set size: 45000 \"\"\" return DataLoader ( dataset = self . data_train , batch_size = self . hparams . batch_size , num_workers = self . hparams . num_workers , pin_memory = self . hparams . pin_memory , shuffle = True , )","title":"train_dataloader()"},{"location":"reference/spotPython/light/crossvalidationdatamodule/#spotPython.light.crossvalidationdatamodule.CrossValidationDataModule.val_dataloader","text":"Returns the validation dataloader. Returns: Name Type Description DataLoader DataLoader The validation dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule () >>> data_module . setup () >>> val_dataloader = data_module . val_dataloader () >>> print ( f \"Validation set size: { len ( val_dataloader . dataset ) } \" ) Validation set size: 5000 Source code in spotPython/light/crossvalidationdatamodule.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. Examples: >>> from spotPython.light import CrossValidationDataModule >>> data_module = CrossValidationDataModule() >>> data_module.setup() >>> val_dataloader = data_module.val_dataloader() >>> print(f\"Validation set size: {len(val_dataloader.dataset)}\") Validation set size: 5000 \"\"\" return DataLoader ( dataset = self . data_val , batch_size = self . hparams . batch_size , num_workers = self . hparams . num_workers , pin_memory = self . hparams . pin_memory , )","title":"val_dataloader()"},{"location":"reference/spotPython/light/csvdatamodule/","text":"CSVDataModule \u00b6 Bases: L . LightningDataModule A LightningDataModule for handling CSV data. Parameters: Name Type Description Default batch_size int The size of the batch. required data_dir str The path to the dataset. Defaults to \u201c./data\u201d. './data' num_workers int The number of workers for data loading. Defaults to 0. 0 Attributes: Name Type Description data_train Dataset The training dataset. data_val Dataset The validation dataset. data_test Dataset The test dataset. Source code in spotPython/light/csvdatamodule.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class CSVDataModule ( L . LightningDataModule ): \"\"\" A LightningDataModule for handling CSV data. Args: batch_size (int): The size of the batch. data_dir (str): The path to the dataset. Defaults to \"./data\". num_workers (int): The number of workers for data loading. Defaults to 0. Attributes: data_train (Dataset): The training dataset. data_val (Dataset): The validation dataset. data_test (Dataset): The test dataset. \"\"\" def __init__ ( self , batch_size : int , data_dir : str = \"./data\" , num_workers : int = 0 ): super () . __init__ () self . batch_size = batch_size self . num_workers = num_workers def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download pass def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> print(f\"Training set size: {len(data_module.data_train)}\") Training set size: 45000 >>> print(f\"Validation set size: {len(data_module.data_val)}\") Validation set size: 5000 >>> print(f\"Test set size: {len(data_module.data_test)}\") Test set size: 10000 \"\"\" # Assign train/val datasets for use in dataloaders if stage == \"fit\" or stage is None : data_full = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) test_abs = int ( len ( data_full ) * 0.6 ) self . data_train , self . data_val = random_split ( data_full , [ test_abs , len ( data_full ) - test_abs ]) # Assign test dataset for use in dataloader(s) # TODO: Adapt this to the VBDP Situation if stage == \"test\" or stage is None : self . data_test = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> train_dataloader = data_module.train_dataloader() >>> print(f\"Training dataloader size: {len(train_dataloader)}\") Training dataloader size: 704 \"\"\" return DataLoader ( self . data_train , batch_size = self . batch_size , num_workers = self . num_workers ) def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> val_dataloader = data_module.val_dataloader() >>> print(f\"Validation dataloader size: {len(val_dataloader)}\") Validation dataloader size: 79 \"\"\" return DataLoader ( self . data_val , batch_size = self . batch_size , num_workers = self . num_workers ) def test_dataloader ( self ) -> DataLoader : \"\"\" Returns the test dataloader. Returns: DataLoader: The test dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> test_dataloader = data_module.test_dataloader() >>> print(f\"Test dataloader size: {len(test_dataloader)}\") Test dataloader size: 704 \"\"\" return DataLoader ( self . data_test , batch_size = self . batch_size , num_workers = self . num_workers ) prepare_data () \u00b6 Prepares the data for use. Source code in spotPython/light/csvdatamodule.py 27 28 29 30 def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download pass setup ( stage = None ) \u00b6 Sets up the data for use. Parameters: Name Type Description Default stage Optional [ str ] The current stage. Defaults to None. None Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule ( batch_size = 64 ) >>> data_module . setup () >>> print ( f \"Training set size: { len ( data_module . data_train ) } \" ) Training set size: 45000 >>> print ( f \"Validation set size: { len ( data_module . data_val ) } \" ) Validation set size: 5000 >>> print ( f \"Test set size: { len ( data_module . data_test ) } \" ) Test set size: 10000 Source code in spotPython/light/csvdatamodule.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> print(f\"Training set size: {len(data_module.data_train)}\") Training set size: 45000 >>> print(f\"Validation set size: {len(data_module.data_val)}\") Validation set size: 5000 >>> print(f\"Test set size: {len(data_module.data_test)}\") Test set size: 10000 \"\"\" # Assign train/val datasets for use in dataloaders if stage == \"fit\" or stage is None : data_full = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) test_abs = int ( len ( data_full ) * 0.6 ) self . data_train , self . data_val = random_split ( data_full , [ test_abs , len ( data_full ) - test_abs ]) # Assign test dataset for use in dataloader(s) # TODO: Adapt this to the VBDP Situation if stage == \"test\" or stage is None : self . data_test = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) test_dataloader () \u00b6 Returns the test dataloader. Returns: Name Type Description DataLoader DataLoader The test dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule ( batch_size = 64 ) >>> data_module . setup () >>> test_dataloader = data_module . test_dataloader () >>> print ( f \"Test dataloader size: { len ( test_dataloader ) } \" ) Test dataloader size: 704 Source code in spotPython/light/csvdatamodule.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def test_dataloader ( self ) -> DataLoader : \"\"\" Returns the test dataloader. Returns: DataLoader: The test dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> test_dataloader = data_module.test_dataloader() >>> print(f\"Test dataloader size: {len(test_dataloader)}\") Test dataloader size: 704 \"\"\" return DataLoader ( self . data_test , batch_size = self . batch_size , num_workers = self . num_workers ) train_dataloader () \u00b6 Returns the training dataloader. Returns: Name Type Description DataLoader DataLoader The training dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule ( batch_size = 64 ) >>> data_module . setup () >>> train_dataloader = data_module . train_dataloader () >>> print ( f \"Training dataloader size: { len ( train_dataloader ) } \" ) Training dataloader size: 704 Source code in spotPython/light/csvdatamodule.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> train_dataloader = data_module.train_dataloader() >>> print(f\"Training dataloader size: {len(train_dataloader)}\") Training dataloader size: 704 \"\"\" return DataLoader ( self . data_train , batch_size = self . batch_size , num_workers = self . num_workers ) val_dataloader () \u00b6 Returns the validation dataloader. Returns: Name Type Description DataLoader DataLoader The validation dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule ( batch_size = 64 ) >>> data_module . setup () >>> val_dataloader = data_module . val_dataloader () >>> print ( f \"Validation dataloader size: { len ( val_dataloader ) } \" ) Validation dataloader size: 79 Source code in spotPython/light/csvdatamodule.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> val_dataloader = data_module.val_dataloader() >>> print(f\"Validation dataloader size: {len(val_dataloader)}\") Validation dataloader size: 79 \"\"\" return DataLoader ( self . data_val , batch_size = self . batch_size , num_workers = self . num_workers )","title":"csvdatamodule"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule","text":"Bases: L . LightningDataModule A LightningDataModule for handling CSV data. Parameters: Name Type Description Default batch_size int The size of the batch. required data_dir str The path to the dataset. Defaults to \u201c./data\u201d. './data' num_workers int The number of workers for data loading. Defaults to 0. 0 Attributes: Name Type Description data_train Dataset The training dataset. data_val Dataset The validation dataset. data_test Dataset The test dataset. Source code in spotPython/light/csvdatamodule.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class CSVDataModule ( L . LightningDataModule ): \"\"\" A LightningDataModule for handling CSV data. Args: batch_size (int): The size of the batch. data_dir (str): The path to the dataset. Defaults to \"./data\". num_workers (int): The number of workers for data loading. Defaults to 0. Attributes: data_train (Dataset): The training dataset. data_val (Dataset): The validation dataset. data_test (Dataset): The test dataset. \"\"\" def __init__ ( self , batch_size : int , data_dir : str = \"./data\" , num_workers : int = 0 ): super () . __init__ () self . batch_size = batch_size self . num_workers = num_workers def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download pass def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> print(f\"Training set size: {len(data_module.data_train)}\") Training set size: 45000 >>> print(f\"Validation set size: {len(data_module.data_val)}\") Validation set size: 5000 >>> print(f\"Test set size: {len(data_module.data_test)}\") Test set size: 10000 \"\"\" # Assign train/val datasets for use in dataloaders if stage == \"fit\" or stage is None : data_full = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) test_abs = int ( len ( data_full ) * 0.6 ) self . data_train , self . data_val = random_split ( data_full , [ test_abs , len ( data_full ) - test_abs ]) # Assign test dataset for use in dataloader(s) # TODO: Adapt this to the VBDP Situation if stage == \"test\" or stage is None : self . data_test = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> train_dataloader = data_module.train_dataloader() >>> print(f\"Training dataloader size: {len(train_dataloader)}\") Training dataloader size: 704 \"\"\" return DataLoader ( self . data_train , batch_size = self . batch_size , num_workers = self . num_workers ) def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> val_dataloader = data_module.val_dataloader() >>> print(f\"Validation dataloader size: {len(val_dataloader)}\") Validation dataloader size: 79 \"\"\" return DataLoader ( self . data_val , batch_size = self . batch_size , num_workers = self . num_workers ) def test_dataloader ( self ) -> DataLoader : \"\"\" Returns the test dataloader. Returns: DataLoader: The test dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> test_dataloader = data_module.test_dataloader() >>> print(f\"Test dataloader size: {len(test_dataloader)}\") Test dataloader size: 704 \"\"\" return DataLoader ( self . data_test , batch_size = self . batch_size , num_workers = self . num_workers )","title":"CSVDataModule"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule.prepare_data","text":"Prepares the data for use. Source code in spotPython/light/csvdatamodule.py 27 28 29 30 def prepare_data ( self ) -> None : \"\"\"Prepares the data for use.\"\"\" # download pass","title":"prepare_data()"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule.setup","text":"Sets up the data for use. Parameters: Name Type Description Default stage Optional [ str ] The current stage. Defaults to None. None Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule ( batch_size = 64 ) >>> data_module . setup () >>> print ( f \"Training set size: { len ( data_module . data_train ) } \" ) Training set size: 45000 >>> print ( f \"Validation set size: { len ( data_module . data_val ) } \" ) Validation set size: 5000 >>> print ( f \"Test set size: { len ( data_module . data_test ) } \" ) Test set size: 10000 Source code in spotPython/light/csvdatamodule.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def setup ( self , stage : Optional [ str ] = None ) -> None : \"\"\" Sets up the data for use. Args: stage (Optional[str]): The current stage. Defaults to None. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> print(f\"Training set size: {len(data_module.data_train)}\") Training set size: 45000 >>> print(f\"Validation set size: {len(data_module.data_val)}\") Validation set size: 5000 >>> print(f\"Test set size: {len(data_module.data_test)}\") Test set size: 10000 \"\"\" # Assign train/val datasets for use in dataloaders if stage == \"fit\" or stage is None : data_full = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) test_abs = int ( len ( data_full ) * 0.6 ) self . data_train , self . data_val = random_split ( data_full , [ test_abs , len ( data_full ) - test_abs ]) # Assign test dataset for use in dataloader(s) # TODO: Adapt this to the VBDP Situation if stage == \"test\" or stage is None : self . data_test = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True )","title":"setup()"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule.test_dataloader","text":"Returns the test dataloader. Returns: Name Type Description DataLoader DataLoader The test dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule ( batch_size = 64 ) >>> data_module . setup () >>> test_dataloader = data_module . test_dataloader () >>> print ( f \"Test dataloader size: { len ( test_dataloader ) } \" ) Test dataloader size: 704 Source code in spotPython/light/csvdatamodule.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def test_dataloader ( self ) -> DataLoader : \"\"\" Returns the test dataloader. Returns: DataLoader: The test dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> test_dataloader = data_module.test_dataloader() >>> print(f\"Test dataloader size: {len(test_dataloader)}\") Test dataloader size: 704 \"\"\" return DataLoader ( self . data_test , batch_size = self . batch_size , num_workers = self . num_workers )","title":"test_dataloader()"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule.train_dataloader","text":"Returns the training dataloader. Returns: Name Type Description DataLoader DataLoader The training dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule ( batch_size = 64 ) >>> data_module . setup () >>> train_dataloader = data_module . train_dataloader () >>> print ( f \"Training dataloader size: { len ( train_dataloader ) } \" ) Training dataloader size: 704 Source code in spotPython/light/csvdatamodule.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training dataloader. Returns: DataLoader: The training dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> train_dataloader = data_module.train_dataloader() >>> print(f\"Training dataloader size: {len(train_dataloader)}\") Training dataloader size: 704 \"\"\" return DataLoader ( self . data_train , batch_size = self . batch_size , num_workers = self . num_workers )","title":"train_dataloader()"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule.val_dataloader","text":"Returns the validation dataloader. Returns: Name Type Description DataLoader DataLoader The validation dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule ( batch_size = 64 ) >>> data_module . setup () >>> val_dataloader = data_module . val_dataloader () >>> print ( f \"Validation dataloader size: { len ( val_dataloader ) } \" ) Validation dataloader size: 79 Source code in spotPython/light/csvdatamodule.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def val_dataloader ( self ) -> DataLoader : \"\"\" Returns the validation dataloader. Returns: DataLoader: The validation dataloader. Examples: >>> from spotPython.light import CSVDataModule >>> data_module = CSVDataModule(batch_size=64) >>> data_module.setup() >>> val_dataloader = data_module.val_dataloader() >>> print(f\"Validation dataloader size: {len(val_dataloader)}\") Validation dataloader size: 79 \"\"\" return DataLoader ( self . data_val , batch_size = self . batch_size , num_workers = self . num_workers )","title":"val_dataloader()"},{"location":"reference/spotPython/light/csvdataset/","text":"CSVDataset \u00b6 Bases: Dataset A PyTorch Dataset for handling CSV data. Parameters: Name Type Description Default csv_file str The path to the CSV file. Defaults to \u201c./data/VBDP/train.csv\u201d. './data/VBDP/train.csv' train bool Whether the dataset is for training or not. Defaults to True. True Attributes: Name Type Description data Tensor The data features. targets Tensor The data targets. Source code in spotPython/light/csvdataset.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 class CSVDataset ( Dataset ): \"\"\" A PyTorch Dataset for handling CSV data. Args: csv_file (str): The path to the CSV file. Defaults to \"./data/VBDP/train.csv\". train (bool): Whether the dataset is for training or not. Defaults to True. Attributes: data (Tensor): The data features. targets (Tensor): The data targets. \"\"\" def __init__ ( self , csv_file : str = \"./data/VBDP/train.csv\" , train : bool = True , ) -> None : super () . __init__ () self . csv_file = csv_file self . train = train self . data , self . targets = self . _load_data () def _load_data ( self ) -> tuple : \"\"\" Loads the data from the CSV file. Returns: tuple: A tuple containing the features and targets as tensors. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> print(dataset.data.shape) torch.Size([60000, 784]) >>> print(dataset.targets.shape) torch.Size([60000]) \"\"\" data_df = pd . read_csv ( self . csv_file ) # drop the id column data_df = data_df . drop ( columns = [ \"id\" ]) target_column = \"prognosis\" # Encode prognosis labels as integers label_encoder = LabelEncoder () targets = label_encoder . fit_transform ( data_df [ target_column ]) # Convert features to tensor features = data_df . drop ( columns = [ target_column ]) . values features_tensor = torch . tensor ( features , dtype = torch . float32 ) # Convert targets to tensor targets_tensor = torch . tensor ( targets , dtype = torch . long ) return features_tensor , targets_tensor def __getitem__ ( self , idx : int ) -> tuple : \"\"\" Returns the feature and target at the given index. Args: idx (int): The index. Returns: tuple: A tuple containing the feature and target at the given index. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> feature, target = dataset[0] >>> print(feature.shape) torch.Size([784]) >>> print(target) tensor(0) \"\"\" feature = self . data [ idx ] target = self . targets [ idx ] return feature , target def __len__ ( self ) -> int : \"\"\" Returns the length of the dataset. Returns: int: The length of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> print(len(dataset)) 60000 \"\"\" return len ( self . data ) def extra_repr ( self ) -> str : \"\"\" Returns a string representation of the dataset. Returns: str: A string representation of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> print(dataset) Split: Train \"\"\" split = \"Train\" if self . train else \"Test\" return f \"Split: { split } \" __getitem__ ( idx ) \u00b6 Returns the feature and target at the given index. Parameters: Name Type Description Default idx int The index. required Returns: Name Type Description tuple tuple A tuple containing the feature and target at the given index. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset () >>> feature , target = dataset [ 0 ] >>> print ( feature . shape ) torch.Size([784]) >>> print ( target ) tensor(0) Source code in spotPython/light/csvdataset.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def __getitem__ ( self , idx : int ) -> tuple : \"\"\" Returns the feature and target at the given index. Args: idx (int): The index. Returns: tuple: A tuple containing the feature and target at the given index. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> feature, target = dataset[0] >>> print(feature.shape) torch.Size([784]) >>> print(target) tensor(0) \"\"\" feature = self . data [ idx ] target = self . targets [ idx ] return feature , target __len__ () \u00b6 Returns the length of the dataset. Returns: Name Type Description int int The length of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset () >>> print ( len ( dataset )) 60000 Source code in spotPython/light/csvdataset.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __len__ ( self ) -> int : \"\"\" Returns the length of the dataset. Returns: int: The length of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> print(len(dataset)) 60000 \"\"\" return len ( self . data ) extra_repr () \u00b6 Returns a string representation of the dataset. Returns: Name Type Description str str A string representation of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset () >>> print ( dataset ) Split: Train Source code in spotPython/light/csvdataset.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def extra_repr ( self ) -> str : \"\"\" Returns a string representation of the dataset. Returns: str: A string representation of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> print(dataset) Split: Train \"\"\" split = \"Train\" if self . train else \"Test\" return f \"Split: { split } \"","title":"csvdataset"},{"location":"reference/spotPython/light/csvdataset/#spotPython.light.csvdataset.CSVDataset","text":"Bases: Dataset A PyTorch Dataset for handling CSV data. Parameters: Name Type Description Default csv_file str The path to the CSV file. Defaults to \u201c./data/VBDP/train.csv\u201d. './data/VBDP/train.csv' train bool Whether the dataset is for training or not. Defaults to True. True Attributes: Name Type Description data Tensor The data features. targets Tensor The data targets. Source code in spotPython/light/csvdataset.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 class CSVDataset ( Dataset ): \"\"\" A PyTorch Dataset for handling CSV data. Args: csv_file (str): The path to the CSV file. Defaults to \"./data/VBDP/train.csv\". train (bool): Whether the dataset is for training or not. Defaults to True. Attributes: data (Tensor): The data features. targets (Tensor): The data targets. \"\"\" def __init__ ( self , csv_file : str = \"./data/VBDP/train.csv\" , train : bool = True , ) -> None : super () . __init__ () self . csv_file = csv_file self . train = train self . data , self . targets = self . _load_data () def _load_data ( self ) -> tuple : \"\"\" Loads the data from the CSV file. Returns: tuple: A tuple containing the features and targets as tensors. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> print(dataset.data.shape) torch.Size([60000, 784]) >>> print(dataset.targets.shape) torch.Size([60000]) \"\"\" data_df = pd . read_csv ( self . csv_file ) # drop the id column data_df = data_df . drop ( columns = [ \"id\" ]) target_column = \"prognosis\" # Encode prognosis labels as integers label_encoder = LabelEncoder () targets = label_encoder . fit_transform ( data_df [ target_column ]) # Convert features to tensor features = data_df . drop ( columns = [ target_column ]) . values features_tensor = torch . tensor ( features , dtype = torch . float32 ) # Convert targets to tensor targets_tensor = torch . tensor ( targets , dtype = torch . long ) return features_tensor , targets_tensor def __getitem__ ( self , idx : int ) -> tuple : \"\"\" Returns the feature and target at the given index. Args: idx (int): The index. Returns: tuple: A tuple containing the feature and target at the given index. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> feature, target = dataset[0] >>> print(feature.shape) torch.Size([784]) >>> print(target) tensor(0) \"\"\" feature = self . data [ idx ] target = self . targets [ idx ] return feature , target def __len__ ( self ) -> int : \"\"\" Returns the length of the dataset. Returns: int: The length of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> print(len(dataset)) 60000 \"\"\" return len ( self . data ) def extra_repr ( self ) -> str : \"\"\" Returns a string representation of the dataset. Returns: str: A string representation of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> print(dataset) Split: Train \"\"\" split = \"Train\" if self . train else \"Test\" return f \"Split: { split } \"","title":"CSVDataset"},{"location":"reference/spotPython/light/csvdataset/#spotPython.light.csvdataset.CSVDataset.__getitem__","text":"Returns the feature and target at the given index. Parameters: Name Type Description Default idx int The index. required Returns: Name Type Description tuple tuple A tuple containing the feature and target at the given index. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset () >>> feature , target = dataset [ 0 ] >>> print ( feature . shape ) torch.Size([784]) >>> print ( target ) tensor(0) Source code in spotPython/light/csvdataset.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def __getitem__ ( self , idx : int ) -> tuple : \"\"\" Returns the feature and target at the given index. Args: idx (int): The index. Returns: tuple: A tuple containing the feature and target at the given index. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> feature, target = dataset[0] >>> print(feature.shape) torch.Size([784]) >>> print(target) tensor(0) \"\"\" feature = self . data [ idx ] target = self . targets [ idx ] return feature , target","title":"__getitem__()"},{"location":"reference/spotPython/light/csvdataset/#spotPython.light.csvdataset.CSVDataset.__len__","text":"Returns the length of the dataset. Returns: Name Type Description int int The length of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset () >>> print ( len ( dataset )) 60000 Source code in spotPython/light/csvdataset.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def __len__ ( self ) -> int : \"\"\" Returns the length of the dataset. Returns: int: The length of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> print(len(dataset)) 60000 \"\"\" return len ( self . data )","title":"__len__()"},{"location":"reference/spotPython/light/csvdataset/#spotPython.light.csvdataset.CSVDataset.extra_repr","text":"Returns a string representation of the dataset. Returns: Name Type Description str str A string representation of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset () >>> print ( dataset ) Split: Train Source code in spotPython/light/csvdataset.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def extra_repr ( self ) -> str : \"\"\" Returns a string representation of the dataset. Returns: str: A string representation of the dataset. Examples: >>> from spotPython.light import CSVDataset >>> dataset = CSVDataset() >>> print(dataset) Split: Train \"\"\" split = \"Train\" if self . train else \"Test\" return f \"Split: { split } \"","title":"extra_repr()"},{"location":"reference/spotPython/light/litmodel/","text":"LitModel \u00b6 Bases: L . LightningModule A LightningModule class for a simple neural network model. Attributes: Name Type Description l1 int The number of neurons in the first hidden layer. epochs int The number of epochs to train the model for. batch_size int The batch size to use during training. act_fn str The activation function to use in the hidden layers. optimizer str The optimizer to use during training. learning_rate float The learning rate for the optimizer. _L_in int The number of input features. _L_out int The number of output classes. model nn . Sequential The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> lit_model = LitModel ( l1 = 128 , epochs = 10 , batch_size = BATCH_SIZE , act_fn = 'relu' , optimizer = 'adam' ) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( lit_model , train_loader ) Source code in spotPython/light/litmodel.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class LitModel ( L . LightningModule ): \"\"\" A LightningModule class for a simple neural network model. Attributes: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. act_fn (str): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. learning_rate (float): The learning rate for the optimizer. _L_in (int): The number of input features. _L_out (int): The number of output classes. model (nn.Sequential): The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> lit_model = LitModel(l1=128, epochs=10, batch_size=BATCH_SIZE, act_fn='relu', optimizer='adam') >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(lit_model, train_loader) \"\"\" def __init__ ( self , l1 : int , epochs : int , batch_size : int , act_fn : str , optimizer : str , learning_rate : float = 2e-4 , _L_in : int = 28 * 28 , _L_out : int = 10 , ): \"\"\" Initializes the LitModel object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. act_fn (str): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. learning_rate (float, optional): The learning rate for the optimizer. Defaults to 2e-4. _L_in (int, optional): The number of input features. Defaults to 28 * 28. _L_out (int, optional): The number of output classes. Defaults to 10. Returns: (NoneType): None \"\"\" super () . __init__ () # We take in input dimensions as parameters and use those to dynamically build model. self . _L_out = _L_out self . l1 = l1 self . epochs = epochs self . batch_size = batch_size self . act_fn = act_fn self . optimizer = optimizer self . learning_rate = learning_rate self . model = nn . Sequential ( nn . Flatten (), nn . Linear ( _L_in , l1 ), nn . ReLU (), nn . Dropout ( 0.1 ), nn . Linear ( l1 , l1 ), nn . ReLU (), nn . Dropout ( 0.1 ), nn . Linear ( l1 , _L_out ), ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the log probabilities for each class. \"\"\" x = self . model ( x ) return F . log_softmax ( x , dim = 1 ) def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch: A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) return loss def validation_step ( self , batch : tuple , batch_idx : int ) -> None : \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. Returns: None \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = 10 ) self . log ( \"val_loss\" , loss , prog_bar = True ) self . log ( \"val_acc\" , acc , prog_bar = True ) def test_step ( self , batch : tuple , batch_idx : int ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = 10 ) self . log ( \"val_loss\" , loss , prog_bar = True ) self . log ( \"val_acc\" , acc , prog_bar = True ) return loss , acc def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer __init__ ( l1 , epochs , batch_size , act_fn , optimizer , learning_rate = 0.0002 , _L_in = 28 * 28 , _L_out = 10 ) \u00b6 Initializes the LitModel object. Parameters: Name Type Description Default l1 int The number of neurons in the first hidden layer. required epochs int The number of epochs to train the model for. required batch_size int The batch size to use during training. required act_fn str The activation function to use in the hidden layers. required optimizer str The optimizer to use during training. required learning_rate float The learning rate for the optimizer. Defaults to 2e-4. 0.0002 _L_in int The number of input features. Defaults to 28 * 28. 28 * 28 _L_out int The number of output classes. Defaults to 10. 10 Returns: Type Description NoneType None Source code in spotPython/light/litmodel.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def __init__ ( self , l1 : int , epochs : int , batch_size : int , act_fn : str , optimizer : str , learning_rate : float = 2e-4 , _L_in : int = 28 * 28 , _L_out : int = 10 , ): \"\"\" Initializes the LitModel object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. act_fn (str): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. learning_rate (float, optional): The learning rate for the optimizer. Defaults to 2e-4. _L_in (int, optional): The number of input features. Defaults to 28 * 28. _L_out (int, optional): The number of output classes. Defaults to 10. Returns: (NoneType): None \"\"\" super () . __init__ () # We take in input dimensions as parameters and use those to dynamically build model. self . _L_out = _L_out self . l1 = l1 self . epochs = epochs self . batch_size = batch_size self . act_fn = act_fn self . optimizer = optimizer self . learning_rate = learning_rate self . model = nn . Sequential ( nn . Flatten (), nn . Linear ( _L_in , l1 ), nn . ReLU (), nn . Dropout ( 0.1 ), nn . Linear ( l1 , l1 ), nn . ReLU (), nn . Dropout ( 0.1 ), nn . Linear ( l1 , _L_out ), ) configure_optimizers () \u00b6 Configures the optimizer for the model. Returns: Type Description torch . optim . Optimizer torch.optim.Optimizer: The optimizer to use during training. Source code in spotPython/light/litmodel.py 156 157 158 159 160 161 162 163 164 def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer forward ( x ) \u00b6 Performs a forward pass through the model. Parameters: Name Type Description Default x torch . Tensor A tensor containing a batch of input data. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the log probabilities for each class. Source code in spotPython/light/litmodel.py 89 90 91 92 93 94 95 96 97 98 99 100 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the log probabilities for each class. \"\"\" x = self . model ( x ) return F . log_softmax ( x , dim = 1 ) test_step ( batch , batch_idx ) \u00b6 Performs a single test step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required Returns: Name Type Description tuple tuple A tuple containing the loss and accuracy for this batch. Source code in spotPython/light/litmodel.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def test_step ( self , batch : tuple , batch_idx : int ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = 10 ) self . log ( \"val_loss\" , loss , prog_bar = True ) self . log ( \"val_acc\" , acc , prog_bar = True ) return loss , acc training_step ( batch ) \u00b6 Performs a single training step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the loss for this batch. Source code in spotPython/light/litmodel.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch: A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) return loss validation_step ( batch , batch_idx ) \u00b6 Performs a single validation step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required Returns: Type Description None None Source code in spotPython/light/litmodel.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def validation_step ( self , batch : tuple , batch_idx : int ) -> None : \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. Returns: None \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = 10 ) self . log ( \"val_loss\" , loss , prog_bar = True ) self . log ( \"val_acc\" , acc , prog_bar = True )","title":"litmodel"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel","text":"Bases: L . LightningModule A LightningModule class for a simple neural network model. Attributes: Name Type Description l1 int The number of neurons in the first hidden layer. epochs int The number of epochs to train the model for. batch_size int The batch size to use during training. act_fn str The activation function to use in the hidden layers. optimizer str The optimizer to use during training. learning_rate float The learning rate for the optimizer. _L_in int The number of input features. _L_out int The number of output classes. model nn . Sequential The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> lit_model = LitModel ( l1 = 128 , epochs = 10 , batch_size = BATCH_SIZE , act_fn = 'relu' , optimizer = 'adam' ) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( lit_model , train_loader ) Source code in spotPython/light/litmodel.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class LitModel ( L . LightningModule ): \"\"\" A LightningModule class for a simple neural network model. Attributes: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. act_fn (str): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. learning_rate (float): The learning rate for the optimizer. _L_in (int): The number of input features. _L_out (int): The number of output classes. model (nn.Sequential): The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> lit_model = LitModel(l1=128, epochs=10, batch_size=BATCH_SIZE, act_fn='relu', optimizer='adam') >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(lit_model, train_loader) \"\"\" def __init__ ( self , l1 : int , epochs : int , batch_size : int , act_fn : str , optimizer : str , learning_rate : float = 2e-4 , _L_in : int = 28 * 28 , _L_out : int = 10 , ): \"\"\" Initializes the LitModel object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. act_fn (str): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. learning_rate (float, optional): The learning rate for the optimizer. Defaults to 2e-4. _L_in (int, optional): The number of input features. Defaults to 28 * 28. _L_out (int, optional): The number of output classes. Defaults to 10. Returns: (NoneType): None \"\"\" super () . __init__ () # We take in input dimensions as parameters and use those to dynamically build model. self . _L_out = _L_out self . l1 = l1 self . epochs = epochs self . batch_size = batch_size self . act_fn = act_fn self . optimizer = optimizer self . learning_rate = learning_rate self . model = nn . Sequential ( nn . Flatten (), nn . Linear ( _L_in , l1 ), nn . ReLU (), nn . Dropout ( 0.1 ), nn . Linear ( l1 , l1 ), nn . ReLU (), nn . Dropout ( 0.1 ), nn . Linear ( l1 , _L_out ), ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the log probabilities for each class. \"\"\" x = self . model ( x ) return F . log_softmax ( x , dim = 1 ) def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch: A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) return loss def validation_step ( self , batch : tuple , batch_idx : int ) -> None : \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. Returns: None \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = 10 ) self . log ( \"val_loss\" , loss , prog_bar = True ) self . log ( \"val_acc\" , acc , prog_bar = True ) def test_step ( self , batch : tuple , batch_idx : int ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = 10 ) self . log ( \"val_loss\" , loss , prog_bar = True ) self . log ( \"val_acc\" , acc , prog_bar = True ) return loss , acc def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer","title":"LitModel"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.__init__","text":"Initializes the LitModel object. Parameters: Name Type Description Default l1 int The number of neurons in the first hidden layer. required epochs int The number of epochs to train the model for. required batch_size int The batch size to use during training. required act_fn str The activation function to use in the hidden layers. required optimizer str The optimizer to use during training. required learning_rate float The learning rate for the optimizer. Defaults to 2e-4. 0.0002 _L_in int The number of input features. Defaults to 28 * 28. 28 * 28 _L_out int The number of output classes. Defaults to 10. 10 Returns: Type Description NoneType None Source code in spotPython/light/litmodel.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def __init__ ( self , l1 : int , epochs : int , batch_size : int , act_fn : str , optimizer : str , learning_rate : float = 2e-4 , _L_in : int = 28 * 28 , _L_out : int = 10 , ): \"\"\" Initializes the LitModel object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. act_fn (str): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. learning_rate (float, optional): The learning rate for the optimizer. Defaults to 2e-4. _L_in (int, optional): The number of input features. Defaults to 28 * 28. _L_out (int, optional): The number of output classes. Defaults to 10. Returns: (NoneType): None \"\"\" super () . __init__ () # We take in input dimensions as parameters and use those to dynamically build model. self . _L_out = _L_out self . l1 = l1 self . epochs = epochs self . batch_size = batch_size self . act_fn = act_fn self . optimizer = optimizer self . learning_rate = learning_rate self . model = nn . Sequential ( nn . Flatten (), nn . Linear ( _L_in , l1 ), nn . ReLU (), nn . Dropout ( 0.1 ), nn . Linear ( l1 , l1 ), nn . ReLU (), nn . Dropout ( 0.1 ), nn . Linear ( l1 , _L_out ), )","title":"__init__()"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.configure_optimizers","text":"Configures the optimizer for the model. Returns: Type Description torch . optim . Optimizer torch.optim.Optimizer: The optimizer to use during training. Source code in spotPython/light/litmodel.py 156 157 158 159 160 161 162 163 164 def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer","title":"configure_optimizers()"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.forward","text":"Performs a forward pass through the model. Parameters: Name Type Description Default x torch . Tensor A tensor containing a batch of input data. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the log probabilities for each class. Source code in spotPython/light/litmodel.py 89 90 91 92 93 94 95 96 97 98 99 100 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the log probabilities for each class. \"\"\" x = self . model ( x ) return F . log_softmax ( x , dim = 1 )","title":"forward()"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.test_step","text":"Performs a single test step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required Returns: Name Type Description tuple tuple A tuple containing the loss and accuracy for this batch. Source code in spotPython/light/litmodel.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def test_step ( self , batch : tuple , batch_idx : int ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = 10 ) self . log ( \"val_loss\" , loss , prog_bar = True ) self . log ( \"val_acc\" , acc , prog_bar = True ) return loss , acc","title":"test_step()"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.training_step","text":"Performs a single training step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the loss for this batch. Source code in spotPython/light/litmodel.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch: A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) return loss","title":"training_step()"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.validation_step","text":"Performs a single validation step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required Returns: Type Description None None Source code in spotPython/light/litmodel.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def validation_step ( self , batch : tuple , batch_idx : int ) -> None : \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. Returns: None \"\"\" x , y = batch logits = self ( x ) loss = F . nll_loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = 10 ) self . log ( \"val_loss\" , loss , prog_bar = True ) self . log ( \"val_acc\" , acc , prog_bar = True )","title":"validation_step()"},{"location":"reference/spotPython/light/mnistdatamodule/","text":"","title":"mnistdatamodule"},{"location":"reference/spotPython/light/netlightbase/","text":"NetLightBase \u00b6 Bases: L . LightningModule A LightningModule class for a neural network model. Attributes: Name Type Description l1 int The number of neurons in the first hidden layer. epochs int The number of epochs to train the model for. batch_size int The batch size to use during training. initialization str The initialization method to use for the weights. act_fn nn . Module The activation function to use in the hidden layers. optimizer str The optimizer to use during training. dropout_prob float The probability of dropping out a neuron during training. lr_mult float The learning rate multiplier for the optimizer. patience int The number of epochs to wait before early stopping. _L_in int The number of input features. _L_out int The number of output classes. layers nn . Sequential The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader ( train_data , batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlightbase.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 class NetLightBase ( L . LightningModule ): \"\"\" A LightningModule class for a neural network model. Attributes: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. _L_out (int): The number of output classes. layers (nn.Sequential): The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" def __init__ ( self , l1 : int , epochs : int , batch_size : int , initialization : str , act_fn : nn . Module , optimizer : str , dropout_prob : float , lr_mult : float , patience : int , _L_in : int , _L_out : int , ): \"\"\" Initializes the NetLightBase object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network. _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network. Returns: (NoneType): None Raises: ValueError: If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" super () . __init__ () # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during # checkpointing. It is recommended to ignore them # using `self.save_hyperparameters(ignore=['act_fn'])` # self.save_hyperparameters(ignore=[\"act_fn\"]) # self . _L_in = _L_in self . _L_out = _L_out # _L_in and _L_out are not hyperparameters, but are needed to create the network self . save_hyperparameters ( ignore = [ \"_L_in\" , \"_L_out\" ]) if self . hparams . l1 < 4 : raise ValueError ( \"l1 must be at least 4\" ) hidden_sizes = [ self . hparams . l1 , self . hparams . l1 // 2 , self . hparams . l1 // 2 , self . hparams . l1 // 4 ] self . train_mapk = MAPK ( k = 3 ) self . valid_mapk = MAPK ( k = 3 ) self . test_mapk = MAPK ( k = 3 ) # Create the network based on the specified hidden sizes layers = [] layer_sizes = [ self . _L_in ] + hidden_sizes layer_size_last = layer_sizes [ 0 ] for layer_size in layer_sizes [ 1 :]: layers += [ nn . Linear ( layer_size_last , layer_size ), self . hparams . act_fn , nn . Dropout ( self . hparams . dropout_prob ), ] layer_size_last = layer_size layers += [ nn . Linear ( layer_sizes [ - 1 ], self . _L_out )] # nn.Sequential summarizes a list of modules into a single module, applying them in sequence self . layers = nn . Sequential ( * layers ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) \"\"\" x = self . layers ( x ) return F . softmax ( x , dim = 1 ) def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch (tuple): A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) # self.train_mapk(logits, y) # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False) return loss def validation_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ): \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: (NoneType): None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor()) >>> val_loader = DataLoader(val_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, val_loader) \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) # loss = F.nll_loss(logits, y) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . valid_mapk ( logits , y ) self . log ( \"valid_mapk\" , self . valid_mapk , on_step = False , on_epoch = True , prog_bar = prog_bar ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_acc\" , acc , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) def test_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . test_mapk ( logits , y ) self . log ( \"test_mapk\" , self . test_mapk , on_step = True , on_epoch = True , prog_bar = prog_bar ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_acc\" , acc , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) return loss , acc def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate) optimizer = optimizer_handler ( optimizer_name = self . hparams . optimizer , params = self . parameters (), lr_mult = self . hparams . lr_mult ) return optimizer __init__ ( l1 , epochs , batch_size , initialization , act_fn , optimizer , dropout_prob , lr_mult , patience , _L_in , _L_out ) \u00b6 Initializes the NetLightBase object. Parameters: Name Type Description Default l1 int The number of neurons in the first hidden layer. required epochs int The number of epochs to train the model for. required batch_size int The batch size to use during training. required initialization str The initialization method to use for the weights. required act_fn nn . Module The activation function to use in the hidden layers. required optimizer str The optimizer to use during training. required dropout_prob float The probability of dropping out a neuron during training. required lr_mult float The learning rate multiplier for the optimizer. required patience int The number of epochs to wait before early stopping. required _L_in int The number of input features. Not a hyperparameter, but needed to create the network. required _L_out int The number of output classes. Not a hyperparameter, but needed to create the network. required Returns: Type Description NoneType None Raises: Type Description ValueError If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs = 10 , batch_size = BATCH_SIZE , initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlightbase.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def __init__ ( self , l1 : int , epochs : int , batch_size : int , initialization : str , act_fn : nn . Module , optimizer : str , dropout_prob : float , lr_mult : float , patience : int , _L_in : int , _L_out : int , ): \"\"\" Initializes the NetLightBase object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network. _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network. Returns: (NoneType): None Raises: ValueError: If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" super () . __init__ () # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during # checkpointing. It is recommended to ignore them # using `self.save_hyperparameters(ignore=['act_fn'])` # self.save_hyperparameters(ignore=[\"act_fn\"]) # self . _L_in = _L_in self . _L_out = _L_out # _L_in and _L_out are not hyperparameters, but are needed to create the network self . save_hyperparameters ( ignore = [ \"_L_in\" , \"_L_out\" ]) if self . hparams . l1 < 4 : raise ValueError ( \"l1 must be at least 4\" ) hidden_sizes = [ self . hparams . l1 , self . hparams . l1 // 2 , self . hparams . l1 // 2 , self . hparams . l1 // 4 ] self . train_mapk = MAPK ( k = 3 ) self . valid_mapk = MAPK ( k = 3 ) self . test_mapk = MAPK ( k = 3 ) # Create the network based on the specified hidden sizes layers = [] layer_sizes = [ self . _L_in ] + hidden_sizes layer_size_last = layer_sizes [ 0 ] for layer_size in layer_sizes [ 1 :]: layers += [ nn . Linear ( layer_size_last , layer_size ), self . hparams . act_fn , nn . Dropout ( self . hparams . dropout_prob ), ] layer_size_last = layer_size layers += [ nn . Linear ( layer_sizes [ - 1 ], self . _L_out )] # nn.Sequential summarizes a list of modules into a single module, applying them in sequence self . layers = nn . Sequential ( * layers ) configure_optimizers () \u00b6 Configures the optimizer for the model. Returns: Type Description torch . optim . Optimizer torch.optim.Optimizer: The optimizer to use during training. Source code in spotPython/light/netlightbase.py 269 270 271 272 273 274 275 276 277 278 279 280 281 def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate) optimizer = optimizer_handler ( optimizer_name = self . hparams . optimizer , params = self . parameters (), lr_mult = self . hparams . lr_mult ) return optimizer forward ( x ) \u00b6 Performs a forward pass through the model. Parameters: Name Type Description Default x torch . Tensor A tensor containing a batch of input data. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) Source code in spotPython/light/netlightbase.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) \"\"\" x = self . layers ( x ) return F . softmax ( x , dim = 1 ) test_step ( batch , batch_idx , prog_bar = False ) \u00b6 Performs a single test step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required prog_bar bool Whether to display the progress bar. Defaults to False. False Returns: Name Type Description tuple tuple A tuple containing the loss and accuracy for this batch. Source code in spotPython/light/netlightbase.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def test_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . test_mapk ( logits , y ) self . log ( \"test_mapk\" , self . test_mapk , on_step = True , on_epoch = True , prog_bar = prog_bar ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_acc\" , acc , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) return loss , acc training_step ( batch ) \u00b6 Performs a single training step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlightbase.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch (tuple): A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) # self.train_mapk(logits, y) # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False) return loss validation_step ( batch , batch_idx , prog_bar = False ) \u00b6 Performs a single validation step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required prog_bar bool Whether to display the progress bar. Defaults to False. False Returns: Type Description NoneType None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST ( PATH_DATASETS , train = False , download = True , transform = ToTensor ()) >>> val_loader = DataLoader ( val_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , val_loader ) Source code in spotPython/light/netlightbase.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def validation_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ): \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: (NoneType): None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor()) >>> val_loader = DataLoader(val_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, val_loader) \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) # loss = F.nll_loss(logits, y) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . valid_mapk ( logits , y ) self . log ( \"valid_mapk\" , self . valid_mapk , on_step = False , on_epoch = True , prog_bar = prog_bar ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_acc\" , acc , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar )","title":"netlightbase"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase","text":"Bases: L . LightningModule A LightningModule class for a neural network model. Attributes: Name Type Description l1 int The number of neurons in the first hidden layer. epochs int The number of epochs to train the model for. batch_size int The batch size to use during training. initialization str The initialization method to use for the weights. act_fn nn . Module The activation function to use in the hidden layers. optimizer str The optimizer to use during training. dropout_prob float The probability of dropping out a neuron during training. lr_mult float The learning rate multiplier for the optimizer. patience int The number of epochs to wait before early stopping. _L_in int The number of input features. _L_out int The number of output classes. layers nn . Sequential The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader ( train_data , batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlightbase.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 class NetLightBase ( L . LightningModule ): \"\"\" A LightningModule class for a neural network model. Attributes: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. _L_out (int): The number of output classes. layers (nn.Sequential): The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" def __init__ ( self , l1 : int , epochs : int , batch_size : int , initialization : str , act_fn : nn . Module , optimizer : str , dropout_prob : float , lr_mult : float , patience : int , _L_in : int , _L_out : int , ): \"\"\" Initializes the NetLightBase object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network. _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network. Returns: (NoneType): None Raises: ValueError: If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" super () . __init__ () # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during # checkpointing. It is recommended to ignore them # using `self.save_hyperparameters(ignore=['act_fn'])` # self.save_hyperparameters(ignore=[\"act_fn\"]) # self . _L_in = _L_in self . _L_out = _L_out # _L_in and _L_out are not hyperparameters, but are needed to create the network self . save_hyperparameters ( ignore = [ \"_L_in\" , \"_L_out\" ]) if self . hparams . l1 < 4 : raise ValueError ( \"l1 must be at least 4\" ) hidden_sizes = [ self . hparams . l1 , self . hparams . l1 // 2 , self . hparams . l1 // 2 , self . hparams . l1 // 4 ] self . train_mapk = MAPK ( k = 3 ) self . valid_mapk = MAPK ( k = 3 ) self . test_mapk = MAPK ( k = 3 ) # Create the network based on the specified hidden sizes layers = [] layer_sizes = [ self . _L_in ] + hidden_sizes layer_size_last = layer_sizes [ 0 ] for layer_size in layer_sizes [ 1 :]: layers += [ nn . Linear ( layer_size_last , layer_size ), self . hparams . act_fn , nn . Dropout ( self . hparams . dropout_prob ), ] layer_size_last = layer_size layers += [ nn . Linear ( layer_sizes [ - 1 ], self . _L_out )] # nn.Sequential summarizes a list of modules into a single module, applying them in sequence self . layers = nn . Sequential ( * layers ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) \"\"\" x = self . layers ( x ) return F . softmax ( x , dim = 1 ) def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch (tuple): A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) # self.train_mapk(logits, y) # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False) return loss def validation_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ): \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: (NoneType): None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor()) >>> val_loader = DataLoader(val_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, val_loader) \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) # loss = F.nll_loss(logits, y) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . valid_mapk ( logits , y ) self . log ( \"valid_mapk\" , self . valid_mapk , on_step = False , on_epoch = True , prog_bar = prog_bar ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_acc\" , acc , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) def test_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . test_mapk ( logits , y ) self . log ( \"test_mapk\" , self . test_mapk , on_step = True , on_epoch = True , prog_bar = prog_bar ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_acc\" , acc , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) return loss , acc def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate) optimizer = optimizer_handler ( optimizer_name = self . hparams . optimizer , params = self . parameters (), lr_mult = self . hparams . lr_mult ) return optimizer","title":"NetLightBase"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.__init__","text":"Initializes the NetLightBase object. Parameters: Name Type Description Default l1 int The number of neurons in the first hidden layer. required epochs int The number of epochs to train the model for. required batch_size int The batch size to use during training. required initialization str The initialization method to use for the weights. required act_fn nn . Module The activation function to use in the hidden layers. required optimizer str The optimizer to use during training. required dropout_prob float The probability of dropping out a neuron during training. required lr_mult float The learning rate multiplier for the optimizer. required patience int The number of epochs to wait before early stopping. required _L_in int The number of input features. Not a hyperparameter, but needed to create the network. required _L_out int The number of output classes. Not a hyperparameter, but needed to create the network. required Returns: Type Description NoneType None Raises: Type Description ValueError If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs = 10 , batch_size = BATCH_SIZE , initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlightbase.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def __init__ ( self , l1 : int , epochs : int , batch_size : int , initialization : str , act_fn : nn . Module , optimizer : str , dropout_prob : float , lr_mult : float , patience : int , _L_in : int , _L_out : int , ): \"\"\" Initializes the NetLightBase object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network. _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network. Returns: (NoneType): None Raises: ValueError: If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" super () . __init__ () # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during # checkpointing. It is recommended to ignore them # using `self.save_hyperparameters(ignore=['act_fn'])` # self.save_hyperparameters(ignore=[\"act_fn\"]) # self . _L_in = _L_in self . _L_out = _L_out # _L_in and _L_out are not hyperparameters, but are needed to create the network self . save_hyperparameters ( ignore = [ \"_L_in\" , \"_L_out\" ]) if self . hparams . l1 < 4 : raise ValueError ( \"l1 must be at least 4\" ) hidden_sizes = [ self . hparams . l1 , self . hparams . l1 // 2 , self . hparams . l1 // 2 , self . hparams . l1 // 4 ] self . train_mapk = MAPK ( k = 3 ) self . valid_mapk = MAPK ( k = 3 ) self . test_mapk = MAPK ( k = 3 ) # Create the network based on the specified hidden sizes layers = [] layer_sizes = [ self . _L_in ] + hidden_sizes layer_size_last = layer_sizes [ 0 ] for layer_size in layer_sizes [ 1 :]: layers += [ nn . Linear ( layer_size_last , layer_size ), self . hparams . act_fn , nn . Dropout ( self . hparams . dropout_prob ), ] layer_size_last = layer_size layers += [ nn . Linear ( layer_sizes [ - 1 ], self . _L_out )] # nn.Sequential summarizes a list of modules into a single module, applying them in sequence self . layers = nn . Sequential ( * layers )","title":"__init__()"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.configure_optimizers","text":"Configures the optimizer for the model. Returns: Type Description torch . optim . Optimizer torch.optim.Optimizer: The optimizer to use during training. Source code in spotPython/light/netlightbase.py 269 270 271 272 273 274 275 276 277 278 279 280 281 def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate) optimizer = optimizer_handler ( optimizer_name = self . hparams . optimizer , params = self . parameters (), lr_mult = self . hparams . lr_mult ) return optimizer","title":"configure_optimizers()"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.forward","text":"Performs a forward pass through the model. Parameters: Name Type Description Default x torch . Tensor A tensor containing a batch of input data. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) Source code in spotPython/light/netlightbase.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) \"\"\" x = self . layers ( x ) return F . softmax ( x , dim = 1 )","title":"forward()"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.test_step","text":"Performs a single test step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required prog_bar bool Whether to display the progress bar. Defaults to False. False Returns: Name Type Description tuple tuple A tuple containing the loss and accuracy for this batch. Source code in spotPython/light/netlightbase.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def test_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . test_mapk ( logits , y ) self . log ( \"test_mapk\" , self . test_mapk , on_step = True , on_epoch = True , prog_bar = prog_bar ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_acc\" , acc , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) return loss , acc","title":"test_step()"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.training_step","text":"Performs a single training step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlightbase.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch (tuple): A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) # self.train_mapk(logits, y) # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False) return loss","title":"training_step()"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.validation_step","text":"Performs a single validation step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required prog_bar bool Whether to display the progress bar. Defaults to False. False Returns: Type Description NoneType None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST ( PATH_DATASETS , train = False , download = True , transform = ToTensor ()) >>> val_loader = DataLoader ( val_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , val_loader ) Source code in spotPython/light/netlightbase.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def validation_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ): \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: (NoneType): None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor()) >>> val_loader = DataLoader(val_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, val_loader) \"\"\" x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = F . cross_entropy ( logits , y ) # loss = F.nll_loss(logits, y) preds = torch . argmax ( logits , dim = 1 ) acc = accuracy ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . valid_mapk ( logits , y ) self . log ( \"valid_mapk\" , self . valid_mapk , on_step = False , on_epoch = True , prog_bar = prog_bar ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_acc\" , acc , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar )","title":"validation_step()"},{"location":"reference/spotPython/light/netlinearbase/","text":"NetLinearBase \u00b6 Bases: L . LightningModule A LightningModule class for a linear (dense) neural network model. Attributes: Name Type Description l1 int The number of neurons in the first hidden layer. epochs int The number of epochs to train the model for. batch_size int The batch size to use during training. initialization str The initialization method to use for the weights. act_fn nn . Module The activation function to use in the hidden layers. optimizer str The optimizer to use during training. dropout_prob float The probability of dropping out a neuron during training. lr_mult float The learning rate multiplier for the optimizer. patience int The number of epochs to wait before early stopping. _L_in int The number of input features. _L_out int The number of output classes. _metric torchmetrics . functional . accuracy The metric to use for evaluation. _loss torch . nn . functional . cross_entropy The loss function to use for training. layers nn . Sequential The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader ( train_data , batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlinearbase.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 class NetLinearBase ( L . LightningModule ): \"\"\" A LightningModule class for a linear (dense) neural network model. Attributes: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. _L_out (int): The number of output classes. _metric (torchmetrics.functional.accuracy): The metric to use for evaluation. _loss (torch.nn.functional.cross_entropy): The loss function to use for training. layers (nn.Sequential): The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" def __init__ ( self , l1 : int , epochs : int , batch_size : int , initialization : str , act_fn : nn . Module , optimizer : str , dropout_prob : float , lr_mult : float , patience : int , _L_in : int , _L_out : int , # _metric: torchmetrics.functional = accuracy, # _loss: torch.nn.functional = F.cross_entropy, ): \"\"\" Initializes the NetLightBase object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network. _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network. _metric (torchmetrics.functional.accuracy): The metric to use for evaluation. Not a hyperparameter, but needed to create the network. _loss (torch.nn.functional.cross_entropy): The loss function to use for training. Not a hyperparameter, but needed to create the network. Returns: (NoneType): None Raises: ValueError: If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" super () . __init__ () print ( \"NetLinearBase.__init__(): l1\" , l1 ) # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during # checkpointing. It is recommended to ignore them # using `self.save_hyperparameters(ignore=['act_fn'])` # self.save_hyperparameters(ignore=[\"act_fn\"]) # self . _L_in = _L_in self . _L_out = _L_out # _L_in and _L_out are not hyperparameters, but are needed to create the network self . _metric = accuracy self . _loss = F . cross_entropy self . save_hyperparameters ( ignore = [ \"_L_in\" , \"_L_out\" , \"_metric\" , \"_loss\" ]) if self . hparams . l1 < 4 : raise ValueError ( \"l1 must be at least 4\" ) hidden_sizes = [ self . hparams . l1 , self . hparams . l1 // 2 , self . hparams . l1 // 2 , self . hparams . l1 // 4 ] # Create the network based on the specified hidden sizes layers = [] layer_sizes = [ self . _L_in ] + hidden_sizes layer_size_last = layer_sizes [ 0 ] for layer_size in layer_sizes [ 1 :]: layers += [ nn . Linear ( layer_size_last , layer_size ), self . hparams . act_fn , nn . Dropout ( self . hparams . dropout_prob ), ] layer_size_last = layer_size layers += [ nn . Linear ( layer_sizes [ - 1 ], self . _L_out )] # nn.Sequential summarizes a list of modules into a single module, applying them in sequence self . layers = nn . Sequential ( * layers ) print ( \"Leaving NetLinearBase.__init__()\" ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) \"\"\" print ( \"Entering NetLinearBase.forward()\" ) x = self . layers ( x ) return F . softmax ( x , dim = 1 ) def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch (tuple): A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" print ( \"Entering NetLinearBase.training_step()\" ) # x, y = batch # print(\"NetLinearBase.training_step(): batch\") # logits = self(x) # print(\"NetLinearBase.training_step(): logits\") # # compute loss (default: cross entropy loss) from logits and y # loss = self._loss(logits, y) # # self.train_mapk(logits, y) # # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False) # return loss return 0.1234 def validation_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ): \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: (NoneType): None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor()) >>> val_loader = DataLoader(val_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, val_loader) \"\"\" print ( \"Entering NetLinearBase.validation_step()\" ) x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = self . _loss ( logits , y ) # loss = F.nll_loss(logits, y) preds = torch . argmax ( logits , dim = 1 ) metric = self . _metric ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_metric\" , metric , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) def test_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" print ( \"Entering NetLinearBase.test_step()\" ) x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = self . _loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) metric = self . _metric ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_metric\" , metric , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) return loss , metric def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate) print ( \"Entering NetLinearBase.configure_optimizers()\" ) optimizer = optimizer_handler ( optimizer_name = self . hparams . optimizer , params = self . parameters (), lr_mult = self . hparams . lr_mult ) print ( \"Leaving NetLinearBase.configure_optimizers()\" ) return optimizer __init__ ( l1 , epochs , batch_size , initialization , act_fn , optimizer , dropout_prob , lr_mult , patience , _L_in , _L_out ) \u00b6 Initializes the NetLightBase object. Parameters: Name Type Description Default l1 int The number of neurons in the first hidden layer. required epochs int The number of epochs to train the model for. required batch_size int The batch size to use during training. required initialization str The initialization method to use for the weights. required act_fn nn . Module The activation function to use in the hidden layers. required optimizer str The optimizer to use during training. required dropout_prob float The probability of dropping out a neuron during training. required lr_mult float The learning rate multiplier for the optimizer. required patience int The number of epochs to wait before early stopping. required _L_in int The number of input features. Not a hyperparameter, but needed to create the network. required _L_out int The number of output classes. Not a hyperparameter, but needed to create the network. required _metric torchmetrics . functional . accuracy The metric to use for evaluation. Not a hyperparameter, but needed to create the network. required _loss torch . nn . functional . cross_entropy The loss function to use for training. Not a hyperparameter, but needed to create the network. required Returns: Type Description NoneType None Raises: Type Description ValueError If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs = 10 , batch_size = BATCH_SIZE , initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlinearbase.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def __init__ ( self , l1 : int , epochs : int , batch_size : int , initialization : str , act_fn : nn . Module , optimizer : str , dropout_prob : float , lr_mult : float , patience : int , _L_in : int , _L_out : int , # _metric: torchmetrics.functional = accuracy, # _loss: torch.nn.functional = F.cross_entropy, ): \"\"\" Initializes the NetLightBase object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network. _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network. _metric (torchmetrics.functional.accuracy): The metric to use for evaluation. Not a hyperparameter, but needed to create the network. _loss (torch.nn.functional.cross_entropy): The loss function to use for training. Not a hyperparameter, but needed to create the network. Returns: (NoneType): None Raises: ValueError: If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" super () . __init__ () print ( \"NetLinearBase.__init__(): l1\" , l1 ) # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during # checkpointing. It is recommended to ignore them # using `self.save_hyperparameters(ignore=['act_fn'])` # self.save_hyperparameters(ignore=[\"act_fn\"]) # self . _L_in = _L_in self . _L_out = _L_out # _L_in and _L_out are not hyperparameters, but are needed to create the network self . _metric = accuracy self . _loss = F . cross_entropy self . save_hyperparameters ( ignore = [ \"_L_in\" , \"_L_out\" , \"_metric\" , \"_loss\" ]) if self . hparams . l1 < 4 : raise ValueError ( \"l1 must be at least 4\" ) hidden_sizes = [ self . hparams . l1 , self . hparams . l1 // 2 , self . hparams . l1 // 2 , self . hparams . l1 // 4 ] # Create the network based on the specified hidden sizes layers = [] layer_sizes = [ self . _L_in ] + hidden_sizes layer_size_last = layer_sizes [ 0 ] for layer_size in layer_sizes [ 1 :]: layers += [ nn . Linear ( layer_size_last , layer_size ), self . hparams . act_fn , nn . Dropout ( self . hparams . dropout_prob ), ] layer_size_last = layer_size layers += [ nn . Linear ( layer_sizes [ - 1 ], self . _L_out )] # nn.Sequential summarizes a list of modules into a single module, applying them in sequence self . layers = nn . Sequential ( * layers ) print ( \"Leaving NetLinearBase.__init__()\" ) configure_optimizers () \u00b6 Configures the optimizer for the model. Returns: Type Description torch . optim . Optimizer torch.optim.Optimizer: The optimizer to use during training. Source code in spotPython/light/netlinearbase.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate) print ( \"Entering NetLinearBase.configure_optimizers()\" ) optimizer = optimizer_handler ( optimizer_name = self . hparams . optimizer , params = self . parameters (), lr_mult = self . hparams . lr_mult ) print ( \"Leaving NetLinearBase.configure_optimizers()\" ) return optimizer forward ( x ) \u00b6 Performs a forward pass through the model. Parameters: Name Type Description Default x torch . Tensor A tensor containing a batch of input data. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) Source code in spotPython/light/netlinearbase.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) \"\"\" print ( \"Entering NetLinearBase.forward()\" ) x = self . layers ( x ) return F . softmax ( x , dim = 1 ) test_step ( batch , batch_idx , prog_bar = False ) \u00b6 Performs a single test step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required prog_bar bool Whether to display the progress bar. Defaults to False. False Returns: Name Type Description tuple tuple A tuple containing the loss and accuracy for this batch. Source code in spotPython/light/netlinearbase.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def test_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" print ( \"Entering NetLinearBase.test_step()\" ) x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = self . _loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) metric = self . _metric ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_metric\" , metric , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) return loss , metric training_step ( batch ) \u00b6 Performs a single training step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlinearbase.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch (tuple): A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" print ( \"Entering NetLinearBase.training_step()\" ) # x, y = batch # print(\"NetLinearBase.training_step(): batch\") # logits = self(x) # print(\"NetLinearBase.training_step(): logits\") # # compute loss (default: cross entropy loss) from logits and y # loss = self._loss(logits, y) # # self.train_mapk(logits, y) # # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False) # return loss return 0.1234 validation_step ( batch , batch_idx , prog_bar = False ) \u00b6 Performs a single validation step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required prog_bar bool Whether to display the progress bar. Defaults to False. False Returns: Type Description NoneType None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST ( PATH_DATASETS , train = False , download = True , transform = ToTensor ()) >>> val_loader = DataLoader ( val_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , val_loader ) Source code in spotPython/light/netlinearbase.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def validation_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ): \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: (NoneType): None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor()) >>> val_loader = DataLoader(val_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, val_loader) \"\"\" print ( \"Entering NetLinearBase.validation_step()\" ) x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = self . _loss ( logits , y ) # loss = F.nll_loss(logits, y) preds = torch . argmax ( logits , dim = 1 ) metric = self . _metric ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_metric\" , metric , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar )","title":"netlinearbase"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase","text":"Bases: L . LightningModule A LightningModule class for a linear (dense) neural network model. Attributes: Name Type Description l1 int The number of neurons in the first hidden layer. epochs int The number of epochs to train the model for. batch_size int The batch size to use during training. initialization str The initialization method to use for the weights. act_fn nn . Module The activation function to use in the hidden layers. optimizer str The optimizer to use during training. dropout_prob float The probability of dropping out a neuron during training. lr_mult float The learning rate multiplier for the optimizer. patience int The number of epochs to wait before early stopping. _L_in int The number of input features. _L_out int The number of output classes. _metric torchmetrics . functional . accuracy The metric to use for evaluation. _loss torch . nn . functional . cross_entropy The loss function to use for training. layers nn . Sequential The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader ( train_data , batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlinearbase.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 class NetLinearBase ( L . LightningModule ): \"\"\" A LightningModule class for a linear (dense) neural network model. Attributes: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. _L_out (int): The number of output classes. _metric (torchmetrics.functional.accuracy): The metric to use for evaluation. _loss (torch.nn.functional.cross_entropy): The loss function to use for training. layers (nn.Sequential): The neural network model. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" def __init__ ( self , l1 : int , epochs : int , batch_size : int , initialization : str , act_fn : nn . Module , optimizer : str , dropout_prob : float , lr_mult : float , patience : int , _L_in : int , _L_out : int , # _metric: torchmetrics.functional = accuracy, # _loss: torch.nn.functional = F.cross_entropy, ): \"\"\" Initializes the NetLightBase object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network. _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network. _metric (torchmetrics.functional.accuracy): The metric to use for evaluation. Not a hyperparameter, but needed to create the network. _loss (torch.nn.functional.cross_entropy): The loss function to use for training. Not a hyperparameter, but needed to create the network. Returns: (NoneType): None Raises: ValueError: If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" super () . __init__ () print ( \"NetLinearBase.__init__(): l1\" , l1 ) # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during # checkpointing. It is recommended to ignore them # using `self.save_hyperparameters(ignore=['act_fn'])` # self.save_hyperparameters(ignore=[\"act_fn\"]) # self . _L_in = _L_in self . _L_out = _L_out # _L_in and _L_out are not hyperparameters, but are needed to create the network self . _metric = accuracy self . _loss = F . cross_entropy self . save_hyperparameters ( ignore = [ \"_L_in\" , \"_L_out\" , \"_metric\" , \"_loss\" ]) if self . hparams . l1 < 4 : raise ValueError ( \"l1 must be at least 4\" ) hidden_sizes = [ self . hparams . l1 , self . hparams . l1 // 2 , self . hparams . l1 // 2 , self . hparams . l1 // 4 ] # Create the network based on the specified hidden sizes layers = [] layer_sizes = [ self . _L_in ] + hidden_sizes layer_size_last = layer_sizes [ 0 ] for layer_size in layer_sizes [ 1 :]: layers += [ nn . Linear ( layer_size_last , layer_size ), self . hparams . act_fn , nn . Dropout ( self . hparams . dropout_prob ), ] layer_size_last = layer_size layers += [ nn . Linear ( layer_sizes [ - 1 ], self . _L_out )] # nn.Sequential summarizes a list of modules into a single module, applying them in sequence self . layers = nn . Sequential ( * layers ) print ( \"Leaving NetLinearBase.__init__()\" ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) \"\"\" print ( \"Entering NetLinearBase.forward()\" ) x = self . layers ( x ) return F . softmax ( x , dim = 1 ) def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch (tuple): A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" print ( \"Entering NetLinearBase.training_step()\" ) # x, y = batch # print(\"NetLinearBase.training_step(): batch\") # logits = self(x) # print(\"NetLinearBase.training_step(): logits\") # # compute loss (default: cross entropy loss) from logits and y # loss = self._loss(logits, y) # # self.train_mapk(logits, y) # # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False) # return loss return 0.1234 def validation_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ): \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: (NoneType): None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor()) >>> val_loader = DataLoader(val_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, val_loader) \"\"\" print ( \"Entering NetLinearBase.validation_step()\" ) x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = self . _loss ( logits , y ) # loss = F.nll_loss(logits, y) preds = torch . argmax ( logits , dim = 1 ) metric = self . _metric ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_metric\" , metric , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) def test_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" print ( \"Entering NetLinearBase.test_step()\" ) x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = self . _loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) metric = self . _metric ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_metric\" , metric , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) return loss , metric def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate) print ( \"Entering NetLinearBase.configure_optimizers()\" ) optimizer = optimizer_handler ( optimizer_name = self . hparams . optimizer , params = self . parameters (), lr_mult = self . hparams . lr_mult ) print ( \"Leaving NetLinearBase.configure_optimizers()\" ) return optimizer","title":"NetLinearBase"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.__init__","text":"Initializes the NetLightBase object. Parameters: Name Type Description Default l1 int The number of neurons in the first hidden layer. required epochs int The number of epochs to train the model for. required batch_size int The batch size to use during training. required initialization str The initialization method to use for the weights. required act_fn nn . Module The activation function to use in the hidden layers. required optimizer str The optimizer to use during training. required dropout_prob float The probability of dropping out a neuron during training. required lr_mult float The learning rate multiplier for the optimizer. required patience int The number of epochs to wait before early stopping. required _L_in int The number of input features. Not a hyperparameter, but needed to create the network. required _L_out int The number of output classes. Not a hyperparameter, but needed to create the network. required _metric torchmetrics . functional . accuracy The metric to use for evaluation. Not a hyperparameter, but needed to create the network. required _loss torch . nn . functional . cross_entropy The loss function to use for training. Not a hyperparameter, but needed to create the network. required Returns: Type Description NoneType None Raises: Type Description ValueError If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs = 10 , batch_size = BATCH_SIZE , initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlinearbase.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def __init__ ( self , l1 : int , epochs : int , batch_size : int , initialization : str , act_fn : nn . Module , optimizer : str , dropout_prob : float , lr_mult : float , patience : int , _L_in : int , _L_out : int , # _metric: torchmetrics.functional = accuracy, # _loss: torch.nn.functional = F.cross_entropy, ): \"\"\" Initializes the NetLightBase object. Args: l1 (int): The number of neurons in the first hidden layer. epochs (int): The number of epochs to train the model for. batch_size (int): The batch size to use during training. initialization (str): The initialization method to use for the weights. act_fn (nn.Module): The activation function to use in the hidden layers. optimizer (str): The optimizer to use during training. dropout_prob (float): The probability of dropping out a neuron during training. lr_mult (float): The learning rate multiplier for the optimizer. patience (int): The number of epochs to wait before early stopping. _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network. _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network. _metric (torchmetrics.functional.accuracy): The metric to use for evaluation. Not a hyperparameter, but needed to create the network. _loss (torch.nn.functional.cross_entropy): The loss function to use for training. Not a hyperparameter, but needed to create the network. Returns: (NoneType): None Raises: ValueError: If l1 is less than 4. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" super () . __init__ () print ( \"NetLinearBase.__init__(): l1\" , l1 ) # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during # checkpointing. It is recommended to ignore them # using `self.save_hyperparameters(ignore=['act_fn'])` # self.save_hyperparameters(ignore=[\"act_fn\"]) # self . _L_in = _L_in self . _L_out = _L_out # _L_in and _L_out are not hyperparameters, but are needed to create the network self . _metric = accuracy self . _loss = F . cross_entropy self . save_hyperparameters ( ignore = [ \"_L_in\" , \"_L_out\" , \"_metric\" , \"_loss\" ]) if self . hparams . l1 < 4 : raise ValueError ( \"l1 must be at least 4\" ) hidden_sizes = [ self . hparams . l1 , self . hparams . l1 // 2 , self . hparams . l1 // 2 , self . hparams . l1 // 4 ] # Create the network based on the specified hidden sizes layers = [] layer_sizes = [ self . _L_in ] + hidden_sizes layer_size_last = layer_sizes [ 0 ] for layer_size in layer_sizes [ 1 :]: layers += [ nn . Linear ( layer_size_last , layer_size ), self . hparams . act_fn , nn . Dropout ( self . hparams . dropout_prob ), ] layer_size_last = layer_size layers += [ nn . Linear ( layer_sizes [ - 1 ], self . _L_out )] # nn.Sequential summarizes a list of modules into a single module, applying them in sequence self . layers = nn . Sequential ( * layers ) print ( \"Leaving NetLinearBase.__init__()\" )","title":"__init__()"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.configure_optimizers","text":"Configures the optimizer for the model. Returns: Type Description torch . optim . Optimizer torch.optim.Optimizer: The optimizer to use during training. Source code in spotPython/light/netlinearbase.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\" Configures the optimizer for the model. Returns: torch.optim.Optimizer: The optimizer to use during training. \"\"\" # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate) print ( \"Entering NetLinearBase.configure_optimizers()\" ) optimizer = optimizer_handler ( optimizer_name = self . hparams . optimizer , params = self . parameters (), lr_mult = self . hparams . lr_mult ) print ( \"Leaving NetLinearBase.configure_optimizers()\" ) return optimizer","title":"configure_optimizers()"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.forward","text":"Performs a forward pass through the model. Parameters: Name Type Description Default x torch . Tensor A tensor containing a batch of input data. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) Source code in spotPython/light/netlinearbase.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Performs a forward pass through the model. Args: x (torch.Tensor): A tensor containing a batch of input data. Returns: torch.Tensor: A tensor containing the probabilities for each class. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) \"\"\" print ( \"Entering NetLinearBase.forward()\" ) x = self . layers ( x ) return F . softmax ( x , dim = 1 )","title":"forward()"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.test_step","text":"Performs a single test step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required prog_bar bool Whether to display the progress bar. Defaults to False. False Returns: Name Type Description tuple tuple A tuple containing the loss and accuracy for this batch. Source code in spotPython/light/netlinearbase.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def test_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ) -> tuple : \"\"\" Performs a single test step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: tuple: A tuple containing the loss and accuracy for this batch. \"\"\" print ( \"Entering NetLinearBase.test_step()\" ) x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = self . _loss ( logits , y ) preds = torch . argmax ( logits , dim = 1 ) metric = self . _metric ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_metric\" , metric , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar ) return loss , metric","title":"test_step()"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.training_step","text":"Performs a single training step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required Returns: Type Description torch . Tensor torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST ( PATH_DATASETS , train = True , download = True , transform = ToTensor ()) >>> train_loader = DataLoader ( train_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , train_loader ) Source code in spotPython/light/netlinearbase.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def training_step ( self , batch : tuple ) -> torch . Tensor : \"\"\" Performs a single training step. Args: batch (tuple): A tuple containing a batch of input data and labels. Returns: torch.Tensor: A tensor containing the loss for this batch. Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor()) >>> train_loader = DataLoader(train_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, train_loader) \"\"\" print ( \"Entering NetLinearBase.training_step()\" ) # x, y = batch # print(\"NetLinearBase.training_step(): batch\") # logits = self(x) # print(\"NetLinearBase.training_step(): logits\") # # compute loss (default: cross entropy loss) from logits and y # loss = self._loss(logits, y) # # self.train_mapk(logits, y) # # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False) # return loss return 0.1234","title":"training_step()"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.validation_step","text":"Performs a single validation step. Parameters: Name Type Description Default batch tuple A tuple containing a batch of input data and labels. required batch_idx int The index of the current batch. required prog_bar bool Whether to display the progress bar. Defaults to False. False Returns: Type Description NoneType None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST ( PATH_DATASETS , train = False , download = True , transform = ToTensor ()) >>> val_loader = DataLoader ( val_data , batch_size = BATCH_SIZE ) >>> net_light_base = NetLightBase ( l1 = 128 , epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L . Trainer ( max_epochs = 10 ) >>> trainer . fit ( net_light_base , val_loader ) Source code in spotPython/light/netlinearbase.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def validation_step ( self , batch : tuple , batch_idx : int , prog_bar : bool = False ): \"\"\" Performs a single validation step. Args: batch (tuple): A tuple containing a batch of input data and labels. batch_idx (int): The index of the current batch. prog_bar (bool, optional): Whether to display the progress bar. Defaults to False. Returns: (NoneType): None Examples: >>> from torch.utils.data import DataLoader >>> from torchvision.datasets import MNIST >>> from torchvision.transforms import ToTensor >>> val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor()) >>> val_loader = DataLoader(val_data, batch_size=BATCH_SIZE) >>> net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE, initialization='xavier', act_fn=nn.ReLU(), optimizer='Adam', dropout_prob=0.1, lr_mult=0.1, patience=5) >>> trainer = L.Trainer(max_epochs=10) >>> trainer.fit(net_light_base, val_loader) \"\"\" print ( \"Entering NetLinearBase.validation_step()\" ) x , y = batch logits = self ( x ) # compute cross entropy loss from logits and y loss = self . _loss ( logits , y ) # loss = F.nll_loss(logits, y) preds = torch . argmax ( logits , dim = 1 ) metric = self . _metric ( preds , y , task = \"multiclass\" , num_classes = self . _L_out ) self . log ( \"val_loss\" , loss , prog_bar = prog_bar ) self . log ( \"val_metric\" , metric , prog_bar = prog_bar ) self . log ( \"hp_metric\" , loss , prog_bar = prog_bar )","title":"validation_step()"},{"location":"reference/spotPython/light/traintest/","text":"cv_model ( config , fun_control ) \u00b6 Performs k-fold cross-validation on a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description float The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... \"k_folds\" : 5 , ... } >>> mapk_score = cv_model ( config , fun_control ) Source code in spotPython/light/traintest.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def cv_model ( config : dict , fun_control : dict ) -> float : \"\"\" Performs k-fold cross-validation on a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: (float): The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... \"k_folds\": 5, ... } >>> mapk_score = cv_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"CV\" postfix to config_id config_id = generate_config_id ( config ) + \"_CV\" results = [] num_folds = fun_control [ \"k_folds\" ] split_seed = 12345 for k in range ( num_folds ): print ( \"k:\" , k ) model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") dm = CrossValidationDataModule ( k = k , num_splits = num_folds , split_seed = split_seed , batch_size = config [ \"batch_size\" ], data_dir = fun_control [ \"DATASET_PATH\" ], ) dm . prepare_data () dm . setup () # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") score = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) score = score [ 0 ] print ( f \"train_model result: { score } \" ) results . append ( score [ \"valid_mapk\" ]) mapk_score = sum ( results ) / num_folds # print(f\"cv_model mapk result: {mapk_score}\") return mapk_score load_light_from_checkpoint ( config , fun_control , postfix = '_TEST' ) \u00b6 Loads a model from a checkpoint using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required postfix str The postfix to append to the configuration ID when generating the checkpoint path. '_TEST' Returns: Name Type Description Any Any The loaded model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"core_model\" : MyModel , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> model = load_light_from_checkpoint ( config , fun_control ) Source code in spotPython/light/traintest.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def load_light_from_checkpoint ( config : dict , fun_control : dict , postfix : str = \"_TEST\" ) -> Any : \"\"\" Loads a model from a checkpoint using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. postfix (str): The postfix to append to the configuration ID when generating the checkpoint path. Returns: Any: The loaded model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"core_model\": MyModel, ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> model = load_light_from_checkpoint(config, fun_control) \"\"\" config_id = generate_config_id ( config ) + postfix default_root_dir = fun_control [ \"TENSORBOARD_PATH\" ] + \"lightning_logs/\" + config_id + \"/checkpoints/last.ckpt\" # default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id) print ( f \"Loading model from { default_root_dir } \" ) model = fun_control [ \"core_model\" ] . load_from_checkpoint ( default_root_dir , _L_in = fun_control [ \"_L_in\" ], _L_out = fun_control [ \"_L_out\" ] ) # disable randomness, dropout, etc... model . eval () return model test_model ( config , fun_control ) \u00b6 Tests a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description Tuple [ float , float ] Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss , val_acc = test_model ( config , fun_control ) Source code in spotPython/light/traintest.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def test_model ( config : dict , fun_control : dict ) -> Tuple [ float , float ]: \"\"\" Tests a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss, val_acc = test_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"TEST\" postfix to config_id config_id = generate_config_id ( config ) + \"_TEST\" # Init DataModule dm = CSVDataModule ( batch_size = config [ \"batch_size\" ], num_workers = fun_control [ \"num_workers\" ], data_dir = fun_control [ \"DATASET_PATH\" ], ) # Init model from datamodule's attributes model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ), ModelCheckpoint ( save_last = True ), # Save the last checkpoint ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) test_result = trainer . test ( datamodule = dm , ckpt_path = \"last\" ) test_result = test_result [ 0 ] # print(f\"test_model result: {test_result}\") return test_result [ \"val_loss\" ], test_result [ \"val_acc\" ] train_model ( config , fun_control ) \u00b6 Trains a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Name Type Description float float The validation loss of the trained model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss = train_model ( config , fun_control ) Source code in spotPython/light/traintest.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def train_model ( config : dict , fun_control : dict ) -> float : \"\"\" Trains a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: float: The validation loss of the trained model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss = train_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] # print(f\"_L_in: {_L_in}\") # print(f\"_L_out: {_L_out}\") if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] config_id = generate_config_id ( config ) model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init DataModule dm = CSVDataModule ( batch_size = config [ \"batch_size\" ], num_workers = fun_control [ \"num_workers\" ], data_dir = fun_control [ \"DATASET_PATH\" ], ) # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") result = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) result = result [ 0 ] # print(f\"train_model result: {result}\") return result [ \"val_loss\" ]","title":"traintest"},{"location":"reference/spotPython/light/traintest/#spotPython.light.traintest.cv_model","text":"Performs k-fold cross-validation on a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description float The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... \"k_folds\" : 5 , ... } >>> mapk_score = cv_model ( config , fun_control ) Source code in spotPython/light/traintest.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def cv_model ( config : dict , fun_control : dict ) -> float : \"\"\" Performs k-fold cross-validation on a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: (float): The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... \"k_folds\": 5, ... } >>> mapk_score = cv_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"CV\" postfix to config_id config_id = generate_config_id ( config ) + \"_CV\" results = [] num_folds = fun_control [ \"k_folds\" ] split_seed = 12345 for k in range ( num_folds ): print ( \"k:\" , k ) model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") dm = CrossValidationDataModule ( k = k , num_splits = num_folds , split_seed = split_seed , batch_size = config [ \"batch_size\" ], data_dir = fun_control [ \"DATASET_PATH\" ], ) dm . prepare_data () dm . setup () # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") score = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) score = score [ 0 ] print ( f \"train_model result: { score } \" ) results . append ( score [ \"valid_mapk\" ]) mapk_score = sum ( results ) / num_folds # print(f\"cv_model mapk result: {mapk_score}\") return mapk_score","title":"cv_model()"},{"location":"reference/spotPython/light/traintest/#spotPython.light.traintest.load_light_from_checkpoint","text":"Loads a model from a checkpoint using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required postfix str The postfix to append to the configuration ID when generating the checkpoint path. '_TEST' Returns: Name Type Description Any Any The loaded model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"core_model\" : MyModel , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> model = load_light_from_checkpoint ( config , fun_control ) Source code in spotPython/light/traintest.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def load_light_from_checkpoint ( config : dict , fun_control : dict , postfix : str = \"_TEST\" ) -> Any : \"\"\" Loads a model from a checkpoint using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. postfix (str): The postfix to append to the configuration ID when generating the checkpoint path. Returns: Any: The loaded model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"core_model\": MyModel, ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> model = load_light_from_checkpoint(config, fun_control) \"\"\" config_id = generate_config_id ( config ) + postfix default_root_dir = fun_control [ \"TENSORBOARD_PATH\" ] + \"lightning_logs/\" + config_id + \"/checkpoints/last.ckpt\" # default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id) print ( f \"Loading model from { default_root_dir } \" ) model = fun_control [ \"core_model\" ] . load_from_checkpoint ( default_root_dir , _L_in = fun_control [ \"_L_in\" ], _L_out = fun_control [ \"_L_out\" ] ) # disable randomness, dropout, etc... model . eval () return model","title":"load_light_from_checkpoint()"},{"location":"reference/spotPython/light/traintest/#spotPython.light.traintest.test_model","text":"Tests a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description Tuple [ float , float ] Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss , val_acc = test_model ( config , fun_control ) Source code in spotPython/light/traintest.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def test_model ( config : dict , fun_control : dict ) -> Tuple [ float , float ]: \"\"\" Tests a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss, val_acc = test_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"TEST\" postfix to config_id config_id = generate_config_id ( config ) + \"_TEST\" # Init DataModule dm = CSVDataModule ( batch_size = config [ \"batch_size\" ], num_workers = fun_control [ \"num_workers\" ], data_dir = fun_control [ \"DATASET_PATH\" ], ) # Init model from datamodule's attributes model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ), ModelCheckpoint ( save_last = True ), # Save the last checkpoint ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) test_result = trainer . test ( datamodule = dm , ckpt_path = \"last\" ) test_result = test_result [ 0 ] # print(f\"test_model result: {test_result}\") return test_result [ \"val_loss\" ], test_result [ \"val_acc\" ]","title":"test_model()"},{"location":"reference/spotPython/light/traintest/#spotPython.light.traintest.train_model","text":"Trains a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Name Type Description float float The validation loss of the trained model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss = train_model ( config , fun_control ) Source code in spotPython/light/traintest.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def train_model ( config : dict , fun_control : dict ) -> float : \"\"\" Trains a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: float: The validation loss of the trained model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss = train_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] # print(f\"_L_in: {_L_in}\") # print(f\"_L_out: {_L_out}\") if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] config_id = generate_config_id ( config ) model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init DataModule dm = CSVDataModule ( batch_size = config [ \"batch_size\" ], num_workers = fun_control [ \"num_workers\" ], data_dir = fun_control [ \"DATASET_PATH\" ], ) # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") result = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) result = result [ 0 ] # print(f\"train_model result: {result}\") return result [ \"val_loss\" ]","title":"train_model()"},{"location":"reference/spotPython/light/traintest_NEW/","text":"cv_model ( config , fun_control ) \u00b6 Performs k-fold cross-validation on a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description float The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... \"k_folds\" : 5 , ... } >>> mapk_score = cv_model ( config , fun_control ) Source code in spotPython/light/traintest_NEW.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def cv_model ( config : dict , fun_control : dict ) -> float : \"\"\" Performs k-fold cross-validation on a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: (float): The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... \"k_folds\": 5, ... } >>> mapk_score = cv_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"CV\" postfix to config_id config_id = generate_config_id ( config ) + \"_CV\" results = [] num_folds = fun_control [ \"k_folds\" ] split_seed = 12345 for k in range ( num_folds ): print ( \"k:\" , k ) model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") dm = CrossValidationDataModule ( k = k , num_splits = num_folds , split_seed = split_seed , batch_size = config [ \"batch_size\" ], data_dir = fun_control [ \"DATASET_PATH\" ], ) dm . prepare_data () dm . setup () # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") score = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) score = score [ 0 ] print ( f \"train_model result: { score } \" ) results . append ( score [ \"valid_mapk\" ]) mapk_score = sum ( results ) / num_folds # print(f\"cv_model mapk result: {mapk_score}\") return mapk_score load_light_from_checkpoint ( config , fun_control , postfix = '_TEST' ) \u00b6 Loads a model from a checkpoint using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required postfix str The postfix to append to the configuration ID when generating the checkpoint path. '_TEST' Returns: Name Type Description Any Any The loaded model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"core_model\" : MyModel , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> model = load_light_from_checkpoint ( config , fun_control ) Source code in spotPython/light/traintest_NEW.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def load_light_from_checkpoint ( config : dict , fun_control : dict , postfix : str = \"_TEST\" ) -> Any : \"\"\" Loads a model from a checkpoint using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. postfix (str): The postfix to append to the configuration ID when generating the checkpoint path. Returns: Any: The loaded model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"core_model\": MyModel, ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> model = load_light_from_checkpoint(config, fun_control) \"\"\" config_id = generate_config_id ( config ) + postfix default_root_dir = fun_control [ \"TENSORBOARD_PATH\" ] + \"lightning_logs/\" + config_id + \"/checkpoints/last.ckpt\" # default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id) print ( f \"Loading model from { default_root_dir } \" ) model = fun_control [ \"core_model\" ] . load_from_checkpoint ( default_root_dir , _L_in = fun_control [ \"_L_in\" ], _L_out = fun_control [ \"_L_out\" ] ) # disable randomness, dropout, etc... model . eval () return model test_model ( config , fun_control ) \u00b6 Tests a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description Tuple [ float , float ] Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss , val_acc = test_model ( config , fun_control ) Source code in spotPython/light/traintest_NEW.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def test_model ( config : dict , fun_control : dict ) -> Tuple [ float , float ]: \"\"\" Tests a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss, val_acc = test_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"TEST\" postfix to config_id config_id = generate_config_id ( config ) + \"_TEST\" # Init DataModule dm = CIFAR10DataModule ( batch_size = config [ \"batch_size\" ], data_dir = fun_control [ \"DATASET_PATH\" ], num_workers = fun_control [ \"num_workers\" ] ) # Init model from datamodule's attributes model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ), ModelCheckpoint ( save_last = True ), # Save the last checkpoint ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) test_result = trainer . test ( datamodule = dm , ckpt_path = \"last\" ) test_result = test_result [ 0 ] # print(f\"test_model result: {test_result}\") return test_result [ \"val_loss\" ], test_result [ \"val_acc\" ] train_model ( config , fun_control ) \u00b6 Trains a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Name Type Description float float The validation loss of the trained model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss = train_model ( config , fun_control ) Source code in spotPython/light/traintest_NEW.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def train_model ( config : dict , fun_control : dict ) -> float : \"\"\" Trains a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: float: The validation loss of the trained model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss = train_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] # print(f\"_L_in: {_L_in}\") # print(f\"_L_out: {_L_out}\") if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] config_id = generate_config_id ( config ) # print(f\"config_id: {config_id}\") # print(f\"config: {config}\") # print(f\"fun_control core model: {fun_control['core_model']}\") model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) # print(f\"model: {model}\") initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init DataModule dm = CIFAR10DataModule ( batch_size = config [ \"batch_size\" ], data_dir = fun_control [ \"DATASET_PATH\" ], num_workers = fun_control [ \"num_workers\" ] ) dm . prepare_data () dm . setup () print ( \"Leaving dm.setup()\" ) # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) print ( \"train.model: Entering trainer.fit()\" ) trainer . fit ( model = model , datamodule = dm ) print ( \"train.model: Leaving trainer.fit()\" ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") result = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) result = result [ 0 ] print ( f \"train_model result: { result } \" ) return result [ \"val_loss\" ]","title":"traintest_NEW"},{"location":"reference/spotPython/light/traintest_NEW/#spotPython.light.traintest_NEW.cv_model","text":"Performs k-fold cross-validation on a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description float The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... \"k_folds\" : 5 , ... } >>> mapk_score = cv_model ( config , fun_control ) Source code in spotPython/light/traintest_NEW.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def cv_model ( config : dict , fun_control : dict ) -> float : \"\"\" Performs k-fold cross-validation on a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: (float): The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... \"k_folds\": 5, ... } >>> mapk_score = cv_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"CV\" postfix to config_id config_id = generate_config_id ( config ) + \"_CV\" results = [] num_folds = fun_control [ \"k_folds\" ] split_seed = 12345 for k in range ( num_folds ): print ( \"k:\" , k ) model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") dm = CrossValidationDataModule ( k = k , num_splits = num_folds , split_seed = split_seed , batch_size = config [ \"batch_size\" ], data_dir = fun_control [ \"DATASET_PATH\" ], ) dm . prepare_data () dm . setup () # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") score = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) score = score [ 0 ] print ( f \"train_model result: { score } \" ) results . append ( score [ \"valid_mapk\" ]) mapk_score = sum ( results ) / num_folds # print(f\"cv_model mapk result: {mapk_score}\") return mapk_score","title":"cv_model()"},{"location":"reference/spotPython/light/traintest_NEW/#spotPython.light.traintest_NEW.load_light_from_checkpoint","text":"Loads a model from a checkpoint using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required postfix str The postfix to append to the configuration ID when generating the checkpoint path. '_TEST' Returns: Name Type Description Any Any The loaded model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"core_model\" : MyModel , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> model = load_light_from_checkpoint ( config , fun_control ) Source code in spotPython/light/traintest_NEW.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def load_light_from_checkpoint ( config : dict , fun_control : dict , postfix : str = \"_TEST\" ) -> Any : \"\"\" Loads a model from a checkpoint using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. postfix (str): The postfix to append to the configuration ID when generating the checkpoint path. Returns: Any: The loaded model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"core_model\": MyModel, ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> model = load_light_from_checkpoint(config, fun_control) \"\"\" config_id = generate_config_id ( config ) + postfix default_root_dir = fun_control [ \"TENSORBOARD_PATH\" ] + \"lightning_logs/\" + config_id + \"/checkpoints/last.ckpt\" # default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id) print ( f \"Loading model from { default_root_dir } \" ) model = fun_control [ \"core_model\" ] . load_from_checkpoint ( default_root_dir , _L_in = fun_control [ \"_L_in\" ], _L_out = fun_control [ \"_L_out\" ] ) # disable randomness, dropout, etc... model . eval () return model","title":"load_light_from_checkpoint()"},{"location":"reference/spotPython/light/traintest_NEW/#spotPython.light.traintest_NEW.test_model","text":"Tests a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description Tuple [ float , float ] Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss , val_acc = test_model ( config , fun_control ) Source code in spotPython/light/traintest_NEW.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def test_model ( config : dict , fun_control : dict ) -> Tuple [ float , float ]: \"\"\" Tests a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss, val_acc = test_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"TEST\" postfix to config_id config_id = generate_config_id ( config ) + \"_TEST\" # Init DataModule dm = CIFAR10DataModule ( batch_size = config [ \"batch_size\" ], data_dir = fun_control [ \"DATASET_PATH\" ], num_workers = fun_control [ \"num_workers\" ] ) # Init model from datamodule's attributes model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ), ModelCheckpoint ( save_last = True ), # Save the last checkpoint ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) test_result = trainer . test ( datamodule = dm , ckpt_path = \"last\" ) test_result = test_result [ 0 ] # print(f\"test_model result: {test_result}\") return test_result [ \"val_loss\" ], test_result [ \"val_acc\" ]","title":"test_model()"},{"location":"reference/spotPython/light/traintest_NEW/#spotPython.light.traintest_NEW.train_model","text":"Trains a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Name Type Description float float The validation loss of the trained model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss = train_model ( config , fun_control ) Source code in spotPython/light/traintest_NEW.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def train_model ( config : dict , fun_control : dict ) -> float : \"\"\" Trains a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: float: The validation loss of the trained model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss = train_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] # print(f\"_L_in: {_L_in}\") # print(f\"_L_out: {_L_out}\") if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] config_id = generate_config_id ( config ) # print(f\"config_id: {config_id}\") # print(f\"config: {config}\") # print(f\"fun_control core model: {fun_control['core_model']}\") model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) # print(f\"model: {model}\") initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init DataModule dm = CIFAR10DataModule ( batch_size = config [ \"batch_size\" ], data_dir = fun_control [ \"DATASET_PATH\" ], num_workers = fun_control [ \"num_workers\" ] ) dm . prepare_data () dm . setup () print ( \"Leaving dm.setup()\" ) # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) print ( \"train.model: Entering trainer.fit()\" ) trainer . fit ( model = model , datamodule = dm ) print ( \"train.model: Leaving trainer.fit()\" ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") result = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) result = result [ 0 ] print ( f \"train_model result: { result } \" ) return result [ \"val_loss\" ]","title":"train_model()"},{"location":"reference/spotPython/light/traintest_OLD/","text":"cv_model ( config , fun_control ) \u00b6 Performs k-fold cross-validation on a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description float The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... \"k_folds\" : 5 , ... } >>> mapk_score = cv_model ( config , fun_control ) Source code in spotPython/light/traintest_OLD.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def cv_model ( config : dict , fun_control : dict ) -> float : \"\"\" Performs k-fold cross-validation on a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: (float): The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... \"k_folds\": 5, ... } >>> mapk_score = cv_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"CV\" postfix to config_id config_id = generate_config_id ( config ) + \"_CV\" results = [] num_folds = fun_control [ \"k_folds\" ] split_seed = 12345 for k in range ( num_folds ): print ( \"k:\" , k ) model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") dm = CrossValidationDataModule ( k = k , num_splits = num_folds , split_seed = split_seed , batch_size = config [ \"batch_size\" ], DATASET_PATH = fun_control [ \"DATASET_PATH\" ], ) dm . prepare_data () dm . setup () # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") score = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) score = score [ 0 ] print ( f \"train_model result: { score } \" ) results . append ( score [ \"valid_mapk\" ]) mapk_score = sum ( results ) / num_folds # print(f\"cv_model mapk result: {mapk_score}\") return mapk_score load_light_from_checkpoint ( config , fun_control , postfix = '_TEST' ) \u00b6 Loads a model from a checkpoint using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required postfix str The postfix to append to the configuration ID when generating the checkpoint path. '_TEST' Returns: Name Type Description Any Any The loaded model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"core_model\" : MyModel , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> model = load_light_from_checkpoint ( config , fun_control ) Source code in spotPython/light/traintest_OLD.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def load_light_from_checkpoint ( config : dict , fun_control : dict , postfix : str = \"_TEST\" ) -> Any : \"\"\" Loads a model from a checkpoint using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. postfix (str): The postfix to append to the configuration ID when generating the checkpoint path. Returns: Any: The loaded model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"core_model\": MyModel, ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> model = load_light_from_checkpoint(config, fun_control) \"\"\" config_id = generate_config_id ( config ) + postfix default_root_dir = fun_control [ \"TENSORBOARD_PATH\" ] + \"lightning_logs/\" + config_id + \"/checkpoints/last.ckpt\" # default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id) print ( f \"Loading model from { default_root_dir } \" ) model = fun_control [ \"core_model\" ] . load_from_checkpoint ( default_root_dir , _L_in = fun_control [ \"_L_in\" ], _L_out = fun_control [ \"_L_out\" ] ) # disable randomness, dropout, etc... model . eval () return model test_model ( config , fun_control ) \u00b6 Tests a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description Tuple [ float , float ] Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss , val_acc = test_model ( config , fun_control ) Source code in spotPython/light/traintest_OLD.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def test_model ( config : dict , fun_control : dict ) -> Tuple [ float , float ]: \"\"\" Tests a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss, val_acc = test_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"TEST\" postfix to config_id config_id = generate_config_id ( config ) + \"_TEST\" # Init DataModule dm = CSVDataModule ( batch_size = config [ \"batch_size\" ], num_workers = fun_control [ \"num_workers\" ], DATASET_PATH = fun_control [ \"DATASET_PATH\" ], ) # Init model from datamodule's attributes model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ), ModelCheckpoint ( save_last = True ), # Save the last checkpoint ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) test_result = trainer . test ( datamodule = dm , ckpt_path = \"last\" ) test_result = test_result [ 0 ] # print(f\"test_model result: {test_result}\") return test_result [ \"val_loss\" ], test_result [ \"val_acc\" ] train_model ( config , fun_control ) \u00b6 Trains a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Name Type Description float float The validation loss of the trained model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss = train_model ( config , fun_control ) Source code in spotPython/light/traintest_OLD.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def train_model ( config : dict , fun_control : dict ) -> float : \"\"\" Trains a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: float: The validation loss of the trained model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss = train_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] # print(f\"_L_in: {_L_in}\") # print(f\"_L_out: {_L_out}\") if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] config_id = generate_config_id ( config ) model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init DataModule dm = CSVDataModule ( batch_size = config [ \"batch_size\" ], num_workers = fun_control [ \"num_workers\" ], DATASET_PATH = fun_control [ \"DATASET_PATH\" ], ) # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") result = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) result = result [ 0 ] # print(f\"train_model result: {result}\") return result [ \"val_loss\" ]","title":"traintest_OLD"},{"location":"reference/spotPython/light/traintest_OLD/#spotPython.light.traintest_OLD.cv_model","text":"Performs k-fold cross-validation on a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description float The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... \"k_folds\" : 5 , ... } >>> mapk_score = cv_model ( config , fun_control ) Source code in spotPython/light/traintest_OLD.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def cv_model ( config : dict , fun_control : dict ) -> float : \"\"\" Performs k-fold cross-validation on a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: (float): The mean average precision at k (MAP@k) score of the model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... \"k_folds\": 5, ... } >>> mapk_score = cv_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"CV\" postfix to config_id config_id = generate_config_id ( config ) + \"_CV\" results = [] num_folds = fun_control [ \"k_folds\" ] split_seed = 12345 for k in range ( num_folds ): print ( \"k:\" , k ) model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") dm = CrossValidationDataModule ( k = k , num_splits = num_folds , split_seed = split_seed , batch_size = config [ \"batch_size\" ], DATASET_PATH = fun_control [ \"DATASET_PATH\" ], ) dm . prepare_data () dm . setup () # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") score = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) score = score [ 0 ] print ( f \"train_model result: { score } \" ) results . append ( score [ \"valid_mapk\" ]) mapk_score = sum ( results ) / num_folds # print(f\"cv_model mapk result: {mapk_score}\") return mapk_score","title":"cv_model()"},{"location":"reference/spotPython/light/traintest_OLD/#spotPython.light.traintest_OLD.load_light_from_checkpoint","text":"Loads a model from a checkpoint using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required postfix str The postfix to append to the configuration ID when generating the checkpoint path. '_TEST' Returns: Name Type Description Any Any The loaded model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"core_model\" : MyModel , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> model = load_light_from_checkpoint ( config , fun_control ) Source code in spotPython/light/traintest_OLD.py 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def load_light_from_checkpoint ( config : dict , fun_control : dict , postfix : str = \"_TEST\" ) -> Any : \"\"\" Loads a model from a checkpoint using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. postfix (str): The postfix to append to the configuration ID when generating the checkpoint path. Returns: Any: The loaded model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"core_model\": MyModel, ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> model = load_light_from_checkpoint(config, fun_control) \"\"\" config_id = generate_config_id ( config ) + postfix default_root_dir = fun_control [ \"TENSORBOARD_PATH\" ] + \"lightning_logs/\" + config_id + \"/checkpoints/last.ckpt\" # default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id) print ( f \"Loading model from { default_root_dir } \" ) model = fun_control [ \"core_model\" ] . load_from_checkpoint ( default_root_dir , _L_in = fun_control [ \"_L_in\" ], _L_out = fun_control [ \"_L_out\" ] ) # disable randomness, dropout, etc... model . eval () return model","title":"load_light_from_checkpoint()"},{"location":"reference/spotPython/light/traintest_OLD/#spotPython.light.traintest_OLD.test_model","text":"Tests a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Type Description Tuple [ float , float ] Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss , val_acc = test_model ( config , fun_control ) Source code in spotPython/light/traintest_OLD.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def test_model ( config : dict , fun_control : dict ) -> Tuple [ float , float ]: \"\"\" Tests a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: Tuple[float, float]: The validation loss and accuracy of the tested model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss, val_acc = test_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] # Add \"TEST\" postfix to config_id config_id = generate_config_id ( config ) + \"_TEST\" # Init DataModule dm = CSVDataModule ( batch_size = config [ \"batch_size\" ], num_workers = fun_control [ \"num_workers\" ], DATASET_PATH = fun_control [ \"DATASET_PATH\" ], ) # Init model from datamodule's attributes model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ), ModelCheckpoint ( save_last = True ), # Save the last checkpoint ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) test_result = trainer . test ( datamodule = dm , ckpt_path = \"last\" ) test_result = test_result [ 0 ] # print(f\"test_model result: {test_result}\") return test_result [ \"val_loss\" ], test_result [ \"val_acc\" ]","title":"test_model()"},{"location":"reference/spotPython/light/traintest_OLD/#spotPython.light.traintest_OLD.train_model","text":"Trains a model using the given configuration and function control parameters. Parameters: Name Type Description Default config dict A dictionary containing the configuration parameters for the model. required fun_control dict A dictionary containing the function control parameters. required Returns: Name Type Description float float The validation loss of the trained model. Examples: >>> config = { ... \"initialization\" : \"Xavier\" , ... \"batch_size\" : 32 , ... \"patience\" : 10 , ... } >>> fun_control = { ... \"_L_in\" : 10 , ... \"_L_out\" : 1 , ... \"enable_progress_bar\" : True , ... \"core_model\" : MyModel , ... \"num_workers\" : 4 , ... \"DATASET_PATH\" : \"./data\" , ... \"CHECKPOINT_PATH\" : \"./checkpoints\" , ... \"TENSORBOARD_PATH\" : \"./tensorboard\" , ... } >>> val_loss = train_model ( config , fun_control ) Source code in spotPython/light/traintest_OLD.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def train_model ( config : dict , fun_control : dict ) -> float : \"\"\" Trains a model using the given configuration and function control parameters. Args: config (dict): A dictionary containing the configuration parameters for the model. fun_control (dict): A dictionary containing the function control parameters. Returns: float: The validation loss of the trained model. Examples: >>> config = { ... \"initialization\": \"Xavier\", ... \"batch_size\": 32, ... \"patience\": 10, ... } >>> fun_control = { ... \"_L_in\": 10, ... \"_L_out\": 1, ... \"enable_progress_bar\": True, ... \"core_model\": MyModel, ... \"num_workers\": 4, ... \"DATASET_PATH\": \"./data\", ... \"CHECKPOINT_PATH\": \"./checkpoints\", ... \"TENSORBOARD_PATH\": \"./tensorboard\", ... } >>> val_loss = train_model(config, fun_control) \"\"\" _L_in = fun_control [ \"_L_in\" ] _L_out = fun_control [ \"_L_out\" ] # print(f\"_L_in: {_L_in}\") # print(f\"_L_out: {_L_out}\") if fun_control [ \"enable_progress_bar\" ] is None : enable_progress_bar = False else : enable_progress_bar = fun_control [ \"enable_progress_bar\" ] config_id = generate_config_id ( config ) model = fun_control [ \"core_model\" ]( ** config , _L_in = _L_in , _L_out = _L_out ) initialization = config [ \"initialization\" ] if initialization == \"Xavier\" : xavier_init ( model ) elif initialization == \"Kaiming\" : kaiming_init ( model ) else : pass # print(f\"model: {model}\") # Init DataModule dm = CSVDataModule ( batch_size = config [ \"batch_size\" ], num_workers = fun_control [ \"num_workers\" ], DATASET_PATH = fun_control [ \"DATASET_PATH\" ], ) # Init trainer trainer = L . Trainer ( # Where to save models default_root_dir = os . path . join ( fun_control [ \"CHECKPOINT_PATH\" ], config_id ), max_epochs = model . hparams . epochs , accelerator = \"auto\" , devices = 1 , logger = TensorBoardLogger ( save_dir = fun_control [ \"TENSORBOARD_PATH\" ], version = config_id , default_hp_metric = True ), callbacks = [ EarlyStopping ( monitor = \"val_loss\" , patience = config [ \"patience\" ], mode = \"min\" , strict = False , verbose = False ) ], enable_progress_bar = enable_progress_bar , ) # Pass the datamodule as arg to trainer.fit to override model hooks :) trainer . fit ( model = model , datamodule = dm ) # Test best model on validation and test set # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\") result = trainer . validate ( model = model , datamodule = dm ) # unlist the result (from a list of one dict) result = result [ 0 ] # print(f\"train_model result: {result}\") return result [ \"val_loss\" ]","title":"train_model()"},{"location":"reference/spotPython/light/utils/","text":"","title":"utils"},{"location":"reference/spotPython/plot/contour/","text":"simple_contour ( fun , min_x =- 1 , max_x = 1 , min_y =- 1 , max_y = 1 , min_z = None , max_z = None , n_samples = 100 , n_levels = 30 ) \u00b6 Simple contour plot Parameters: Name Type Description Default fun _type_ description required min_x int description . Defaults to -1. -1 max_x int description . Defaults to 1. 1 min_y int description . Defaults to -1. -1 max_y int description . Defaults to 1. 1 min_z int description . Defaults to 0. None max_z int description . Defaults to 1. None n_samples int description . Defaults to 100. 100 n_levels int description . Defaults to 5. 30 Examples: >>> import matplotlib.pyplot as plt import numpy as np from spotPython.fun.objectivefunctions import analytical fun = analytical().fun_branin simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15) Source code in spotPython/plot/contour.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def simple_contour ( fun , min_x =- 1 , max_x = 1 , min_y =- 1 , max_y = 1 , min_z = None , max_z = None , n_samples = 100 , n_levels = 30 , ): \"\"\" Simple contour plot Args: fun (_type_): _description_ min_x (int, optional): _description_. Defaults to -1. max_x (int, optional): _description_. Defaults to 1. min_y (int, optional): _description_. Defaults to -1. max_y (int, optional): _description_. Defaults to 1. min_z (int, optional): _description_. Defaults to 0. max_z (int, optional): _description_. Defaults to 1. n_samples (int, optional): _description_. Defaults to 100. n_levels (int, optional): _description_. Defaults to 5. Examples: >>> import matplotlib.pyplot as plt import numpy as np from spotPython.fun.objectivefunctions import analytical fun = analytical().fun_branin simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15) \"\"\" XX , YY = np . meshgrid ( np . linspace ( min_x , max_x , n_samples ), np . linspace ( min_y , max_y , n_samples )) zz = np . array ([ fun ( np . array ([ xi , yi ]) . reshape ( - 1 , 2 )) for xi , yi in zip ( np . ravel ( XX ), np . ravel ( YY ))]) . reshape ( n_samples , n_samples ) fig , ax = plt . subplots ( figsize = ( 5 , 2.7 ), layout = \"constrained\" ) if min_z is None : min_z = np . min ( zz ) if max_z is None : max_z = np . max ( zz ) plt . contourf ( XX , YY , zz , levels = np . linspace ( min_z , max_z , n_levels ), zorder = 1 , cmap = \"jet\" , vmin = min_z , vmax = max_z , ) plt . colorbar ()","title":"contour"},{"location":"reference/spotPython/plot/contour/#spotPython.plot.contour.simple_contour","text":"Simple contour plot Parameters: Name Type Description Default fun _type_ description required min_x int description . Defaults to -1. -1 max_x int description . Defaults to 1. 1 min_y int description . Defaults to -1. -1 max_y int description . Defaults to 1. 1 min_z int description . Defaults to 0. None max_z int description . Defaults to 1. None n_samples int description . Defaults to 100. 100 n_levels int description . Defaults to 5. 30 Examples: >>> import matplotlib.pyplot as plt import numpy as np from spotPython.fun.objectivefunctions import analytical fun = analytical().fun_branin simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15) Source code in spotPython/plot/contour.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def simple_contour ( fun , min_x =- 1 , max_x = 1 , min_y =- 1 , max_y = 1 , min_z = None , max_z = None , n_samples = 100 , n_levels = 30 , ): \"\"\" Simple contour plot Args: fun (_type_): _description_ min_x (int, optional): _description_. Defaults to -1. max_x (int, optional): _description_. Defaults to 1. min_y (int, optional): _description_. Defaults to -1. max_y (int, optional): _description_. Defaults to 1. min_z (int, optional): _description_. Defaults to 0. max_z (int, optional): _description_. Defaults to 1. n_samples (int, optional): _description_. Defaults to 100. n_levels (int, optional): _description_. Defaults to 5. Examples: >>> import matplotlib.pyplot as plt import numpy as np from spotPython.fun.objectivefunctions import analytical fun = analytical().fun_branin simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15) \"\"\" XX , YY = np . meshgrid ( np . linspace ( min_x , max_x , n_samples ), np . linspace ( min_y , max_y , n_samples )) zz = np . array ([ fun ( np . array ([ xi , yi ]) . reshape ( - 1 , 2 )) for xi , yi in zip ( np . ravel ( XX ), np . ravel ( YY ))]) . reshape ( n_samples , n_samples ) fig , ax = plt . subplots ( figsize = ( 5 , 2.7 ), layout = \"constrained\" ) if min_z is None : min_z = np . min ( zz ) if max_z is None : max_z = np . max ( zz ) plt . contourf ( XX , YY , zz , levels = np . linspace ( min_z , max_z , n_levels ), zorder = 1 , cmap = \"jet\" , vmin = min_z , vmax = max_z , ) plt . colorbar ()","title":"simple_contour()"},{"location":"reference/spotPython/plot/validation/","text":"plot_confusion_matrix ( model , fun_control , target_names = None , title = None ) \u00b6 Plotting a confusion matrix Source code in spotPython/plot/validation.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def plot_confusion_matrix ( model , fun_control , target_names = None , title = None ): \"\"\" Plotting a confusion matrix \"\"\" X_train , y_train = get_Xy_from_df ( fun_control [ \"train\" ], fun_control [ \"target_column\" ]) X_test , y_test = get_Xy_from_df ( fun_control [ \"test\" ], fun_control [ \"target_column\" ]) model . fit ( X_train , y_train ) pred = model . predict ( X_test ) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ConfusionMatrixDisplay . from_predictions ( y_test , pred , ax = ax ) if target_names is not None : ax . xaxis . set_ticklabels ( target_names ) ax . yaxis . set_ticklabels ( target_names ) if title is not None : _ = ax . set_title ( title ) plot_cv_predictions ( model , fun_control ) \u00b6 Plots cross-validated predictions for regression. Uses sklearn.model_selection.cross_val_predict together with sklearn.metrics.PredictionErrorDisplay to visualize prediction errors. It is based on the example from the scikit-learn documentation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-download-auto-examples-model-selection-plot-cv-predict-py Parameters: Name Type Description Default model Any Sklearn model. The model to be used for cross-validation. required fun_control Dict Dictionary containing the data and the target column. required Returns: Type Description NoneType None Examples: >>> from sklearn.datasets import load_diabetes >>> from sklearn.linear_model import LinearRegression >>> X , y = load_diabetes ( return_X_y = True ) >>> lr = LinearRegression () >>> plot_cv_predictions ( lr , fun_control ) Source code in spotPython/plot/validation.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def plot_cv_predictions ( model : Any , fun_control : Dict ) -> None : \"\"\" Plots cross-validated predictions for regression. Uses `sklearn.model_selection.cross_val_predict` together with `sklearn.metrics.PredictionErrorDisplay` to visualize prediction errors. It is based on the example from the scikit-learn documentation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-download-auto-examples-model-selection-plot-cv-predict-py Args: model (Any): Sklearn model. The model to be used for cross-validation. fun_control (Dict): Dictionary containing the data and the target column. Returns: (NoneType): None Examples: >>> from sklearn.datasets import load_diabetes >>> from sklearn.linear_model import LinearRegression >>> X, y = load_diabetes(return_X_y=True) >>> lr = LinearRegression() >>> plot_cv_predictions(lr, fun_control) \"\"\" X_test , y_test = get_Xy_from_df ( fun_control [ \"test\" ], fun_control [ \"target_column\" ]) # cross_val_predict returns an array of the same size of y # where each entry is a prediction obtained by cross validation. y_pred = cross_val_predict ( model , X_test , y_test , cv = 10 ) fig , axs = plt . subplots ( ncols = 2 , figsize = ( 8 , 4 )) PredictionErrorDisplay . from_predictions ( y_test , y_pred = y_pred , kind = \"actual_vs_predicted\" , subsample = 100 , ax = axs [ 0 ], random_state = 0 , ) axs [ 0 ] . set_title ( \"Actual vs. Predicted values\" ) PredictionErrorDisplay . from_predictions ( y_test , y_pred = y_pred , kind = \"residual_vs_predicted\" , subsample = 100 , ax = axs [ 1 ], random_state = 0 , ) axs [ 1 ] . set_title ( \"Residuals vs. Predicted Values\" ) fig . suptitle ( \"Plotting cross-validated predictions\" ) plt . tight_layout () plt . show () plot_roc ( model_list , fun_control , alpha = 0.8 , model_names = None ) \u00b6 Plots ROC curves for a list of models using the Visualization API from scikit-learn. Parameters: Name Type Description Default model_list List [ BaseEstimator ] A list of scikit-learn models to plot ROC curves for. required fun_control Dict [ str , Union [ str , pd . DataFrame ]] A dictionary containing the train and test dataframes and the target column name. required alpha float The alpha value for the ROC curve. Defaults to 0.8. 0.8 model_names List [ str ] A list of names for the models. Defaults to None. None Returns: Type Description NoneType None Examples: >>> from sklearn.datasets import load_iris >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.tree import DecisionTreeClassifier >>> iris = load_iris () >>> X_train = iris . data [: 100 ] >>> y_train = iris . target [: 100 ] >>> X_test = iris . data [ 100 :] >>> y_test = iris . target [ 100 :] >>> train_df = pd . DataFrame ( X_train , columns = iris . feature_names ) >>> train_df [ 'target' ] = y_train >>> test_df = pd . DataFrame ( X_test , columns = iris . feature_names ) >>> test_df [ 'target' ] = y_test >>> fun_control = { \"train\" : train_df , \"test\" : test_df , \"target_column\" : \"target\" } >>> model_list = [ LogisticRegression (), DecisionTreeClassifier ()] >>> model_names = [ \"Logistic Regression\" , \"Decision Tree\" ] >>> plot_roc ( model_list , fun_control , model_names = model_names ) Source code in spotPython/plot/validation.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def plot_roc ( model_list : List [ BaseEstimator ], fun_control : Dict [ str , Union [ str , pd . DataFrame ]], alpha : float = 0.8 , model_names : List [ str ] = None , ) -> None : \"\"\" Plots ROC curves for a list of models using the Visualization API from scikit-learn. Args: model_list (List[BaseEstimator]): A list of scikit-learn models to plot ROC curves for. fun_control (Dict[str, Union[str, pd.DataFrame]]): A dictionary containing the train and test dataframes and the target column name. alpha (float, optional): The alpha value for the ROC curve. Defaults to 0.8. model_names (List[str], optional): A list of names for the models. Defaults to None. Returns: (NoneType): None Examples: >>> from sklearn.datasets import load_iris >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.tree import DecisionTreeClassifier >>> iris = load_iris() >>> X_train = iris.data[:100] >>> y_train = iris.target[:100] >>> X_test = iris.data[100:] >>> y_test = iris.target[100:] >>> train_df = pd.DataFrame(X_train, columns=iris.feature_names) >>> train_df['target'] = y_train >>> test_df = pd.DataFrame(X_test, columns=iris.feature_names) >>> test_df['target'] = y_test >>> fun_control = {\"train\": train_df, \"test\": test_df, \"target_column\": \"target\"} >>> model_list = [LogisticRegression(), DecisionTreeClassifier()] >>> model_names = [\"Logistic Regression\", \"Decision Tree\"] >>> plot_roc(model_list, fun_control, model_names=model_names) \"\"\" X_train , y_train = get_Xy_from_df ( fun_control [ \"train\" ], fun_control [ \"target_column\" ]) X_test , y_test = get_Xy_from_df ( fun_control [ \"test\" ], fun_control [ \"target_column\" ]) ax = plt . gca () for i , model in enumerate ( model_list ): model . fit ( X_train , y_train ) if model_names is not None : model_name = model_names [ i ] else : model_name = None y_pred = model . predict ( X_test ) RocCurveDisplay . from_predictions ( y_test , y_pred , ax = ax , alpha = alpha , name = model_name ) plt . show ()","title":"validation"},{"location":"reference/spotPython/plot/validation/#spotPython.plot.validation.plot_confusion_matrix","text":"Plotting a confusion matrix Source code in spotPython/plot/validation.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def plot_confusion_matrix ( model , fun_control , target_names = None , title = None ): \"\"\" Plotting a confusion matrix \"\"\" X_train , y_train = get_Xy_from_df ( fun_control [ \"train\" ], fun_control [ \"target_column\" ]) X_test , y_test = get_Xy_from_df ( fun_control [ \"test\" ], fun_control [ \"target_column\" ]) model . fit ( X_train , y_train ) pred = model . predict ( X_test ) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ConfusionMatrixDisplay . from_predictions ( y_test , pred , ax = ax ) if target_names is not None : ax . xaxis . set_ticklabels ( target_names ) ax . yaxis . set_ticklabels ( target_names ) if title is not None : _ = ax . set_title ( title )","title":"plot_confusion_matrix()"},{"location":"reference/spotPython/plot/validation/#spotPython.plot.validation.plot_cv_predictions","text":"Plots cross-validated predictions for regression. Uses sklearn.model_selection.cross_val_predict together with sklearn.metrics.PredictionErrorDisplay to visualize prediction errors. It is based on the example from the scikit-learn documentation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-download-auto-examples-model-selection-plot-cv-predict-py Parameters: Name Type Description Default model Any Sklearn model. The model to be used for cross-validation. required fun_control Dict Dictionary containing the data and the target column. required Returns: Type Description NoneType None Examples: >>> from sklearn.datasets import load_diabetes >>> from sklearn.linear_model import LinearRegression >>> X , y = load_diabetes ( return_X_y = True ) >>> lr = LinearRegression () >>> plot_cv_predictions ( lr , fun_control ) Source code in spotPython/plot/validation.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def plot_cv_predictions ( model : Any , fun_control : Dict ) -> None : \"\"\" Plots cross-validated predictions for regression. Uses `sklearn.model_selection.cross_val_predict` together with `sklearn.metrics.PredictionErrorDisplay` to visualize prediction errors. It is based on the example from the scikit-learn documentation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-download-auto-examples-model-selection-plot-cv-predict-py Args: model (Any): Sklearn model. The model to be used for cross-validation. fun_control (Dict): Dictionary containing the data and the target column. Returns: (NoneType): None Examples: >>> from sklearn.datasets import load_diabetes >>> from sklearn.linear_model import LinearRegression >>> X, y = load_diabetes(return_X_y=True) >>> lr = LinearRegression() >>> plot_cv_predictions(lr, fun_control) \"\"\" X_test , y_test = get_Xy_from_df ( fun_control [ \"test\" ], fun_control [ \"target_column\" ]) # cross_val_predict returns an array of the same size of y # where each entry is a prediction obtained by cross validation. y_pred = cross_val_predict ( model , X_test , y_test , cv = 10 ) fig , axs = plt . subplots ( ncols = 2 , figsize = ( 8 , 4 )) PredictionErrorDisplay . from_predictions ( y_test , y_pred = y_pred , kind = \"actual_vs_predicted\" , subsample = 100 , ax = axs [ 0 ], random_state = 0 , ) axs [ 0 ] . set_title ( \"Actual vs. Predicted values\" ) PredictionErrorDisplay . from_predictions ( y_test , y_pred = y_pred , kind = \"residual_vs_predicted\" , subsample = 100 , ax = axs [ 1 ], random_state = 0 , ) axs [ 1 ] . set_title ( \"Residuals vs. Predicted Values\" ) fig . suptitle ( \"Plotting cross-validated predictions\" ) plt . tight_layout () plt . show ()","title":"plot_cv_predictions()"},{"location":"reference/spotPython/plot/validation/#spotPython.plot.validation.plot_roc","text":"Plots ROC curves for a list of models using the Visualization API from scikit-learn. Parameters: Name Type Description Default model_list List [ BaseEstimator ] A list of scikit-learn models to plot ROC curves for. required fun_control Dict [ str , Union [ str , pd . DataFrame ]] A dictionary containing the train and test dataframes and the target column name. required alpha float The alpha value for the ROC curve. Defaults to 0.8. 0.8 model_names List [ str ] A list of names for the models. Defaults to None. None Returns: Type Description NoneType None Examples: >>> from sklearn.datasets import load_iris >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.tree import DecisionTreeClassifier >>> iris = load_iris () >>> X_train = iris . data [: 100 ] >>> y_train = iris . target [: 100 ] >>> X_test = iris . data [ 100 :] >>> y_test = iris . target [ 100 :] >>> train_df = pd . DataFrame ( X_train , columns = iris . feature_names ) >>> train_df [ 'target' ] = y_train >>> test_df = pd . DataFrame ( X_test , columns = iris . feature_names ) >>> test_df [ 'target' ] = y_test >>> fun_control = { \"train\" : train_df , \"test\" : test_df , \"target_column\" : \"target\" } >>> model_list = [ LogisticRegression (), DecisionTreeClassifier ()] >>> model_names = [ \"Logistic Regression\" , \"Decision Tree\" ] >>> plot_roc ( model_list , fun_control , model_names = model_names ) Source code in spotPython/plot/validation.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def plot_roc ( model_list : List [ BaseEstimator ], fun_control : Dict [ str , Union [ str , pd . DataFrame ]], alpha : float = 0.8 , model_names : List [ str ] = None , ) -> None : \"\"\" Plots ROC curves for a list of models using the Visualization API from scikit-learn. Args: model_list (List[BaseEstimator]): A list of scikit-learn models to plot ROC curves for. fun_control (Dict[str, Union[str, pd.DataFrame]]): A dictionary containing the train and test dataframes and the target column name. alpha (float, optional): The alpha value for the ROC curve. Defaults to 0.8. model_names (List[str], optional): A list of names for the models. Defaults to None. Returns: (NoneType): None Examples: >>> from sklearn.datasets import load_iris >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.tree import DecisionTreeClassifier >>> iris = load_iris() >>> X_train = iris.data[:100] >>> y_train = iris.target[:100] >>> X_test = iris.data[100:] >>> y_test = iris.target[100:] >>> train_df = pd.DataFrame(X_train, columns=iris.feature_names) >>> train_df['target'] = y_train >>> test_df = pd.DataFrame(X_test, columns=iris.feature_names) >>> test_df['target'] = y_test >>> fun_control = {\"train\": train_df, \"test\": test_df, \"target_column\": \"target\"} >>> model_list = [LogisticRegression(), DecisionTreeClassifier()] >>> model_names = [\"Logistic Regression\", \"Decision Tree\"] >>> plot_roc(model_list, fun_control, model_names=model_names) \"\"\" X_train , y_train = get_Xy_from_df ( fun_control [ \"train\" ], fun_control [ \"target_column\" ]) X_test , y_test = get_Xy_from_df ( fun_control [ \"test\" ], fun_control [ \"target_column\" ]) ax = plt . gca () for i , model in enumerate ( model_list ): model . fit ( X_train , y_train ) if model_names is not None : model_name = model_names [ i ] else : model_name = None y_pred = model . predict ( X_test ) RocCurveDisplay . from_predictions ( y_test , y_pred , ax = ax , alpha = alpha , name = model_name ) plt . show ()","title":"plot_roc()"},{"location":"reference/spotPython/sklearn/traintest/","text":"evaluate_model_oob ( model , fun_control ) \u00b6 Out-of-bag evaluation (Only for RandomForestClassifier). If fun_control[\u201ceval\u201d] == \u201ceval_oob_score\u201d. Source code in spotPython/sklearn/traintest.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def evaluate_model_oob ( model , fun_control ): \"\"\"Out-of-bag evaluation (Only for RandomForestClassifier). If fun_control[\"eval\"] == \"eval_oob_score\". \"\"\" try : X , y = get_Xy_from_df ( fun_control [ \"train\" ], fun_control [ \"target_column\" ]) model . fit ( X , y ) df_preds = model . oob_decision_function_ df_eval = fun_control [ \"metric_sklearn\" ]( y , df_preds , ** fun_control [ \"metric_params\" ]) except Exception as err : print ( f \"Error in fun_sklearn(). Call to evaluate_model_oob failed. { err =} , { type ( err ) =} \" ) df_eval = np . nan df_eval = np . nan return df_eval , df_preds","title":"traintest"},{"location":"reference/spotPython/sklearn/traintest/#spotPython.sklearn.traintest.evaluate_model_oob","text":"Out-of-bag evaluation (Only for RandomForestClassifier). If fun_control[\u201ceval\u201d] == \u201ceval_oob_score\u201d. Source code in spotPython/sklearn/traintest.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def evaluate_model_oob ( model , fun_control ): \"\"\"Out-of-bag evaluation (Only for RandomForestClassifier). If fun_control[\"eval\"] == \"eval_oob_score\". \"\"\" try : X , y = get_Xy_from_df ( fun_control [ \"train\" ], fun_control [ \"target_column\" ]) model . fit ( X , y ) df_preds = model . oob_decision_function_ df_eval = fun_control [ \"metric_sklearn\" ]( y , df_preds , ** fun_control [ \"metric_params\" ]) except Exception as err : print ( f \"Error in fun_sklearn(). Call to evaluate_model_oob failed. { err =} , { type ( err ) =} \" ) df_eval = np . nan df_eval = np . nan return df_eval , df_preds","title":"evaluate_model_oob()"},{"location":"reference/spotPython/spot/spot/","text":"Spot \u00b6 Spot base class to handle the following tasks in a uniform manner: Getting and setting parameters. This is done via the Spot initialization. Running surrogate based hyperparameter optimization. After the class is initialized, hyperparameter tuning runs can be performed via the run method. Displaying information. The plot method can be used for visualizing results. The print methods summarizes information about the tuning run. The Spot class is built in a modular manner. It combines the following three components: 1. Design 2. Surrogate 3. Optimizer For each of the three components different implementations can be selected and combined. Internal components are selected as default. These can be replaced by components from other packages, e.g., scikit-learn or scikit-optimize. Parameters: Name Type Description Default fun Callable objective function required lower np . array lower bound required upper np . array upper bound required fun_evals int number of function evaluations 15 fun_repeats int number of repeats (replicates). 1 max_time int maximum time (in minutes) inf noise bool deterministic or noisy objective function False tolerance_x float tolerance for new x solutions. Minimum distance of new solutions, generated by suggest_new_X , to already existing solutions. If zero (which is the default), every new solution is accepted. 0 ocba_delta int OCBA increment (only used if noise==True ) 0 var_type List [ str ] list of type information, can be either \u201cnum\u201d or \u201cfactor\u201d ['num'] var_name List [ str ] list of variable names, e.g., [\u201cx1\u201d, \u201cx2\u201d] None infill_criterion str Can be \"y\" , \"s\" , \"ei\" (negative expected improvement), or \"all\" . 'y' n_points int number of infill points 1 seed int initial seed 123 log_level int log level with the following settings: NOTSET ( 0 ), DEBUG ( 10 : Detailed information, typically of interest only when diagnosing problems.), INFO ( 20 : Confirmation that things are working as expected.), WARNING ( 30 : An indication that something unexpected happened, or indicative of some problem in the near future (e.g. \u2018disk space low\u2019). The software is still working as expected.), ERROR ( 40 : Due to a more serious problem, the software has not been able to perform some function.), and CRITICAL ( 50 : A serious error, indicating that the program itself may be unable to continue running.) 50 show_models bool Plot model. Currently only 1-dim functions are supported. False show_progress bool Show progress bar. True design object experimental design. None design_control Dict [ str , Union [ int , float ]] experimental design information stored as a dictionary with the following entries: \u201cinit_size\u201d: 10 , \u201crepeats\u201d: 1 . {} surrogate object surrogate model. If None , spotPython\u2019s kriging is used. None surrogate_control Dict [ str , Union [ int , float ]] surrogate model information stored as a dictionary with the following entries: \u201cmodel_optimizer\u201d: differential_evolution , \u201cmodel_fun_evals\u201d: None , \u201cmin_theta\u201d: -3. , \u201cmax_theta\u201d: 3. , \u201cn_theta\u201d: 1 , \u201cn_p\u201d: 1 , \u201coptim_p\u201d: False , \u201ccod_type\u201d: \"norm\" , \u201cvar_type\u201d: self.var_type , \u201cuse_cod_y\u201d: False . {} optimizer object optimizer. If None , scipy.optimize \u2018s differential_evolution is used. None optimizer_control Dict [ str , Union [ int , float ]] information about the optimizer stored as a dictionary with the following entries: \u201cmax_iter\u201d: 1000 . {} Returns: Type Description NoneType None Note Description in the source code refers to [bart21i]: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches. In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67\u2013114. Examples: >>> import numpy as np >>> from math import inf >>> from spotpy.spot_setup import Spot >>> def objective_function ( x ): >>> return x [ 0 ] ** 2 + x [ 1 ] ** 2 >>> lower = np . array ([ 0 , 0 ]) >>> upper = np . array ([ 10 , 10 ]) >>> spot = Spot ( fun = objective_function , >>> lower = lower , >>> upper = upper , >>> fun_evals = 100 , >>> fun_repeats = 1 , >>> max_time = inf , >>> noise = False , >>> tolerance_x = 0 , >>> ocba_delta = 0 , >>> var_type = [ \"num\" , \"num\" ], >>> var_name = [ \"x1\" , \"x2\" ], >>> infill_criterion = \"ei\" , >>> n_points = 10 , >>> seed = 123 , >>> log_level = 20 , >>> show_models = False , >>> show_progress = True , >>> design = None , >>> design_control = { \"init_size\" : 10 , \"repeats\" : 1 }, >>> surrogate = None , >>> surrogate_control = { \"model_optimizer\" : \"differential_evolution\" , >>> \"model_fun_evals\" : None , >>> \"min_theta\" : - 3. , >>> \"max_theta\" : 3. , >>> \"n_theta\" : 1 , >>> \"n_p\" : 1 , >>> \"optim_p\" : False , >>> \"cod_type\" : \"norm\" , >>> \"var_type\" : [ \"num\" , \"num\" ], >>> \"use_cod_y\" : False }, >>> optimizer_control = { \"max_iter\" : 1000 }) >>> spot . run () Source code in spotPython/spot/spot.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 class Spot : \"\"\" Spot base class to handle the following tasks in a uniform manner: * Getting and setting parameters. This is done via the `Spot` initialization. * Running surrogate based hyperparameter optimization. After the class is initialized, hyperparameter tuning runs can be performed via the `run` method. * Displaying information. The `plot` method can be used for visualizing results. The `print` methods summarizes information about the tuning run. The `Spot` class is built in a modular manner. It combines the following three components: 1. Design 2. Surrogate 3. Optimizer For each of the three components different implementations can be selected and combined. Internal components are selected as default. These can be replaced by components from other packages, e.g., scikit-learn or scikit-optimize. Args: fun (Callable): objective function lower (np.array): lower bound upper (np.array): upper bound fun_evals (int): number of function evaluations fun_repeats (int): number of repeats (replicates). max_time (int): maximum time (in minutes) noise (bool): deterministic or noisy objective function tolerance_x (float): tolerance for new x solutions. Minimum distance of new solutions, generated by `suggest_new_X`, to already existing solutions. If zero (which is the default), every new solution is accepted. ocba_delta (int): OCBA increment (only used if `noise==True`) var_type (List[str]): list of type information, can be either \"num\" or \"factor\" var_name (List[str]): list of variable names, e.g., [\"x1\", \"x2\"] infill_criterion (str): Can be `\"y\"`, `\"s\"`, `\"ei\"` (negative expected improvement), or `\"all\"`. n_points (int): number of infill points seed (int): initial seed log_level (int): log level with the following settings: `NOTSET` (`0`), `DEBUG` (`10`: Detailed information, typically of interest only when diagnosing problems.), `INFO` (`20`: Confirmation that things are working as expected.), `WARNING` (`30`: An indication that something unexpected happened, or indicative of some problem in the near future (e.g. \u2018disk space low\u2019). The software is still working as expected.), `ERROR` (`40`: Due to a more serious problem, the software has not been able to perform some function.), and `CRITICAL` (`50`: A serious error, indicating that the program itself may be unable to continue running.) show_models (bool): Plot model. Currently only 1-dim functions are supported. show_progress (bool): Show progress bar. design (object): experimental design. design_control (Dict[str, Union[int, float]]): experimental design information stored as a dictionary with the following entries: \"init_size\": `10`, \"repeats\": `1`. surrogate (object): surrogate model. If `None`, spotPython's `kriging` is used. surrogate_control (Dict[str, Union[int, float]]): surrogate model information stored as a dictionary with the following entries: \"model_optimizer\": `differential_evolution`, \"model_fun_evals\": `None`, \"min_theta\": `-3.`, \"max_theta\": `3.`, \"n_theta\": `1`, \"n_p\": `1`, \"optim_p\": `False`, \"cod_type\": `\"norm\"`, \"var_type\": `self.var_type`, \"use_cod_y\": `False`. optimizer (object): optimizer. If `None`, `scipy.optimize`'s `differential_evolution` is used. optimizer_control (Dict[str, Union[int, float]]): information about the optimizer stored as a dictionary with the following entries: \"max_iter\": `1000`. Returns: (NoneType): None Note: Description in the source code refers to [bart21i]: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches. In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67\u2013114. Examples: >>> import numpy as np >>> from math import inf >>> from spotpy.spot_setup import Spot >>> def objective_function(x): >>> return x[0]**2 + x[1]**2 >>> lower = np.array([0, 0]) >>> upper = np.array([10, 10]) >>> spot = Spot(fun=objective_function, >>> lower=lower, >>> upper=upper, >>> fun_evals=100, >>> fun_repeats=1, >>> max_time=inf, >>> noise=False, >>> tolerance_x=0, >>> ocba_delta=0, >>> var_type=[\"num\", \"num\"], >>> var_name=[\"x1\", \"x2\"], >>> infill_criterion=\"ei\", >>> n_points=10, >>> seed=123, >>> log_level=20, >>> show_models=False, >>> show_progress=True, >>> design=None, >>> design_control={\"init_size\": 10, \"repeats\": 1}, >>> surrogate=None, >>> surrogate_control={\"model_optimizer\": \"differential_evolution\", >>> \"model_fun_evals\": None, >>> \"min_theta\": -3., >>> \"max_theta\": 3., >>> \"n_theta\": 1, >>> \"n_p\": 1, >>> \"optim_p\": False, >>> \"cod_type\": \"norm\", >>> \"var_type\": [\"num\", \"num\"], >>> \"use_cod_y\": False}, >>> optimizer_control={\"max_iter\": 1000}) >>> spot.run() \"\"\" def __str__ ( self ): return self . __class__ . __name__ def __init__ ( self , fun : Callable , lower : np . array , upper : np . array , fun_evals : int = 15 , fun_repeats : int = 1 , fun_control : Dict [ str , Union [ int , float ]] = {}, max_time : int = inf , noise : bool = False , tolerance_x : float = 0 , var_type : List [ str ] = [ \"num\" ], var_name : List [ str ] = None , all_var_name : List [ str ] = None , infill_criterion : str = \"y\" , n_points : int = 1 , ocba_delta : int = 0 , seed : int = 123 , log_level : int = 50 , show_models : bool = False , show_progress : bool = True , design : object = None , design_control : Dict [ str , Union [ int , float ]] = {}, surrogate : object = None , surrogate_control : Dict [ str , Union [ int , float ]] = {}, optimizer : object = None , optimizer_control : Dict [ str , Union [ int , float ]] = {}, ): # use x0, x1, ... as default variable names: if var_name is None : var_name = [ \"x\" + str ( i ) for i in range ( len ( lower ))] # small value: self . eps = sqrt ( spacing ( 1 )) self . fun = fun self . lower = lower self . upper = upper self . var_type = var_type self . var_name = var_name self . all_var_name = all_var_name # Reduce dim based on lower == upper logic: # modifies lower, upper, and var_type self . to_red_dim () self . k = self . lower . size self . fun_evals = fun_evals self . fun_repeats = fun_repeats self . max_time = max_time self . noise = noise self . tolerance_x = tolerance_x self . ocba_delta = ocba_delta self . log_level = log_level self . show_models = show_models self . show_progress = show_progress # Random number generator: self . seed = seed self . rng = default_rng ( self . seed ) # Force numeric type as default in every dim: # assume all variable types are \"num\" if \"num\" is # specified once: if len ( self . var_type ) < self . k : self . var_type = self . var_type * self . k logger . warning ( \"Warning: All variable types forced to 'num'.\" ) self . infill_criterion = infill_criterion # Bounds de_bounds = [] for j in range ( self . k ): de_bounds . append ([ self . lower [ j ], self . upper [ j ]]) self . de_bounds = de_bounds # Infill points: self . n_points = n_points # Objective function related information: self . fun_control = { \"sigma\" : 0 , \"seed\" : None } self . fun_control . update ( fun_control ) # Design related information: self . design = design if design is None : self . design = spacefilling ( k = self . k , seed = self . seed ) self . design_control = { \"init_size\" : 10 , \"repeats\" : 1 } self . design_control . update ( design_control ) # Surrogate related information: self . surrogate = surrogate self . surrogate_control = { \"noise\" : self . noise , \"model_optimizer\" : differential_evolution , \"model_fun_evals\" : None , \"min_theta\" : - 3.0 , \"max_theta\" : 3.0 , \"n_theta\" : 1 , \"n_p\" : 1 , \"optim_p\" : False , \"cod_type\" : \"norm\" , \"var_type\" : self . var_type , \"seed\" : 124 , \"use_cod_y\" : False , } # Logging information: self . counter = 0 self . min_y = None self . min_X = None self . min_mean_X = None self . min_mean_y = None self . mean_X = None self . mean_y = None self . var_y = None logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) # if the key \"spot_writer\" is not in the dictionary fun_control, # set self.spot_writer to None else to the value of the key \"spot_writer\" self . spot_writer = fun_control . get ( \"spot_writer\" , None ) self . surrogate_control . update ( surrogate_control ) # If no surrogate model is specified, use the internal # spotPython kriging surrogate: if self . surrogate is None : # Call kriging with surrogate_control parameters: self . surrogate = Kriging ( name = \"kriging\" , noise = self . surrogate_control [ \"noise\" ], model_optimizer = self . surrogate_control [ \"model_optimizer\" ], model_fun_evals = self . surrogate_control [ \"model_fun_evals\" ], seed = self . surrogate_control [ \"seed\" ], log_level = self . log_level , min_theta = self . surrogate_control [ \"min_theta\" ], max_theta = self . surrogate_control [ \"max_theta\" ], n_theta = self . surrogate_control [ \"n_theta\" ], n_p = self . surrogate_control [ \"n_p\" ], optim_p = self . surrogate_control [ \"optim_p\" ], cod_type = self . surrogate_control [ \"cod_type\" ], var_type = self . surrogate_control [ \"var_type\" ], use_cod_y = self . surrogate_control [ \"use_cod_y\" ], spot_writer = self . spot_writer , counter = self . design_control [ \"init_size\" ] * self . design_control [ \"repeats\" ] - 1 , ) # Optimizer related information: self . optimizer = optimizer self . optimizer_control = { \"max_iter\" : 1000 , \"seed\" : 125 } self . optimizer_control . update ( optimizer_control ) if self . optimizer is None : self . optimizer = optimize . differential_evolution def to_red_dim ( self ): self . all_lower = self . lower self . all_upper = self . upper self . ident = ( self . upper - self . lower ) == 0 self . lower = self . lower [ ~ self . ident ] self . upper = self . upper [ ~ self . ident ] self . red_dim = self . ident . any () self . all_var_type = self . var_type self . var_type = [ x for x , y in zip ( self . all_var_type , self . ident ) if not y ] if self . var_name is not None : self . all_var_name = self . var_name self . var_name = [ x for x , y in zip ( self . all_var_name , self . ident ) if not y ] def to_all_dim ( self , X0 ): n = X0 . shape [ 0 ] k = len ( self . ident ) X = np . zeros (( n , k )) j = 0 for i in range ( k ): if self . ident [ i ]: X [:, i ] = self . all_lower [ i ] j += 1 else : X [:, i ] = X0 [:, i - j ] return X def to_all_dim_if_needed ( self , X ): if self . red_dim : return self . to_all_dim ( X ) else : return X def get_X_ocba ( self ): if self . noise and self . ocba_delta > 0 : return get_ocba_X ( self . mean_X , self . mean_y , self . var_y , self . ocba_delta ) else : return None def get_new_X0 ( self ): X0 = self . suggest_new_X () X0 = repair_non_numeric ( X0 , self . var_type ) # (S-16) Duplicate Handling: # Condition: select only X= that have min distance # to existing solutions X0 , X0_ind = selectNew ( A = X0 , X = self . X , tolerance = self . tolerance_x ) logger . debug ( \"XO values are new: %s %s \" , X0_ind , X0 ) # 1. There are X0 that fullfil the condition. # Note: The number of new X0 can be smaller than self.n_points! if X0 . shape [ 0 ] > 0 : return repeat ( X0 , self . fun_repeats , axis = 0 ) # 2. No X0 found. Then generate self.n_points new solutions: else : self . design = spacefilling ( k = self . k , seed = self . seed + self . counter ) X0 = self . generate_design ( size = self . n_points , repeats = self . design_control [ \"repeats\" ], lower = self . lower , upper = self . upper ) X0 = repair_non_numeric ( X0 , self . var_type ) logger . warning ( \"No new XO found on surrogate. Generate new solution %s \" , X0 ) return X0 def append_X_ocba ( self , X_ocba , X0 ): if self . noise and self . ocba_delta > 0 : return append ( X_ocba , X0 , axis = 0 ) else : return X0 def run ( self , X_start = None ): self . initialize_design ( X_start ) # New: self.update_stats() moved here: # self.update_stats() # (S-5) Calling the spotLoop Function # and # (S-9) Termination Criteria, Conditions: timeout_start = time . time () while self . should_continue ( timeout_start ): self . update_design () # (S-10): Subset Selection for the Surrogate: # Not implemented yet. # Update stats self . update_stats () # Update writer: self . update_writer () # (S-11) Surrogate Fit: self . fit_surrogate () # progress bar: self . show_progress_if_needed ( timeout_start ) if self . spot_writer is not None : writer = self . spot_writer writer . close () return self def initialize_design ( self , X_start = None ): # (S-2) Initial Design: X0 = self . generate_design ( size = self . design_control [ \"init_size\" ], repeats = self . design_control [ \"repeats\" ], lower = self . lower , upper = self . upper , ) if X_start is not None : try : X0 = append ( X_start , X0 , axis = 0 ) except ValueError : logger . warning ( \"X_start has wrong shape. Ignoring it.\" ) X0 = repair_non_numeric ( X0 , self . var_type ) self . X = X0 # (S-3): Eval initial design: X_all = self . to_all_dim_if_needed ( X0 ) self . y = self . fun ( X = X_all , fun_control = self . fun_control ) # TODO: Error if only nan values are returned logger . debug ( \"New y value: %s \" , self . y ) # self . counter = self . y . size if self . spot_writer is not None : writer = self . spot_writer # range goes to init_size -1 because the last value is added by update_stats(), # which always adds the last value. # Changed in 0.5.9: for j in range ( len ( self . y )): X_j = self . X [ j ] . copy () y_j = self . y [ j ] . copy () config = { self . var_name [ i ]: X_j [ i ] for i in range ( self . k )} writer . add_hparams ( config , { \"spot_y\" : y_j }) writer . flush () # self . X , self . y = remove_nan ( self . X , self . y ) # self.update_stats() moved to run()! # changed in 0.5.9: self . update_stats () # (S-4): Imputation: # Not implemented yet. # (S-11) Surrogate Fit: self . fit_surrogate () def should_continue ( self , timeout_start ): return ( self . counter < self . fun_evals ) and ( time . time () < timeout_start + self . max_time * 60 ) def update_design ( self ): # OCBA (only if noise) X_ocba = self . get_X_ocba () # (S-15) Compile Surrogate Results: X0 = self . get_new_X0 () # (S-18): Evaluating New Solutions: X0 = self . append_X_ocba ( X_ocba , X0 ) X_all = self . to_all_dim_if_needed ( X0 ) y0 = self . fun ( X = X_all , fun_control = self . fun_control ) X0 , y0 = remove_nan ( X0 , y0 ) # Append New Solutions: self . X = np . append ( self . X , X0 , axis = 0 ) self . y = np . append ( self . y , y0 ) def fit_surrogate ( self ): self . surrogate . fit ( self . X , self . y ) if self . show_models : self . plot_model () def show_progress_if_needed ( self , timeout_start ): if not self . show_progress : return if isfinite ( self . fun_evals ): progress_bar ( progress = self . counter / self . fun_evals , y = self . min_y ) else : progress_bar ( progress = ( time . time () - timeout_start ) / ( self . max_time * 60 ), y = self . min_y ) def generate_design ( self , size , repeats , lower , upper ): return self . design . scipy_lhd ( n = size , repeats = repeats , lower = lower , upper = upper ) def update_stats ( self ): \"\"\" Update the following stats: 1. `min_y` 2. `min_X` 3. `counter` If `noise` is `True`, additionally the following stats are computed: 1. `mean_X` 2. `mean_y` 3. `min_mean_y` 4. `min_mean_X`. \"\"\" self . min_y = min ( self . y ) self . min_X = self . X [ argmin ( self . y )] self . counter = self . y . size # Update aggregated x and y values (if noise): if self . noise : Z = aggregate_mean_var ( X = self . X , y = self . y ) self . mean_X = Z [ 0 ] self . mean_y = Z [ 1 ] self . var_y = Z [ 2 ] # X value of the best mean y value so far: self . min_mean_X = self . mean_X [ argmin ( self . mean_y )] # variance of the best mean y value so far: self . min_var_y = self . var_y [ argmin ( self . mean_y )] # best mean y value so far: self . min_mean_y = self . mean_y [ argmin ( self . mean_y )] def update_writer ( self ): if self . spot_writer is not None : writer = self . spot_writer # get the last y value: y_last = self . y [ - 1 ] . copy () if self . noise is False : y_min = self . min_y . copy () X_min = self . min_X . copy () # y_min: best y value so far # y_last: last y value, can be worse than y_min writer . add_scalars ( \"spot_y\" , { \"min\" : y_min , \"last\" : y_last }, self . counter ) # X_min: X value of the best y value so far writer . add_scalars ( \"spot_X\" , { f \"X_ { i } \" : X_min [ i ] for i in range ( self . k )}, self . counter ) else : # get the last n y values: y_last_n = self . y [ - self . fun_repeats :] . copy () # y_min_mean: best mean y value so far y_min_mean = self . min_mean_y . copy () # X_min_mean: X value of the best mean y value so far X_min_mean = self . min_mean_X . copy () # y_min_var: variance of the min y value so far y_min_var = self . min_var_y . copy () writer . add_scalar ( \"spot_y_min_var\" , y_min_var , self . counter ) # y_min_mean: best mean y value so far (see above) writer . add_scalar ( \"spot_y\" , y_min_mean , self . counter ) # last n y values (noisy): writer . add_scalars ( \"spot_y\" , { f \"y_last_n { i } \" : y_last_n [ i ] for i in range ( self . fun_repeats )}, self . counter ) # X_min_mean: X value of the best mean y value so far (see above) writer . add_scalars ( \"spot_X_noise\" , { f \"X_min_mean { i } \" : X_min_mean [ i ] for i in range ( self . k )}, self . counter ) # get last value of self.X and convert to dict. take the values from self.var_name as keys: X_last = self . X [ - 1 ] . copy () config = { self . var_name [ i ]: X_last [ i ] for i in range ( self . k )} # hyperparameters X and value y of the last configuration: writer . add_hparams ( config , { \"spot_y\" : y_last }) writer . flush () def suggest_new_X_old ( self ): \"\"\" Compute `n_points` new infill points in natural units. The optimizer searches in the ranges from `lower_j` to `upper_j`. The method `infill()` is used as the objective function. Returns: (numpy.ndarray): `n_points` infill points in natural units, each of dim k Note: This is step (S-14a) in [bart21i]. \"\"\" # (S-14a) Optimization on the surrogate: new_X = np . zeros ([ self . n_points , self . k ], dtype = float ) optimizer_name = self . optimizer . __name__ for i in range ( self . n_points ): if optimizer_name == \"dual_annealing\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) elif optimizer_name == \"differential_evolution\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds , maxiter = self . optimizer_control [ \"max_iter\" ], seed = self . optimizer_control [ \"seed\" ], # popsize=10, # updating=\"deferred\" ) elif optimizer_name == \"direct\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds , eps = 1e-2 ) elif optimizer_name == \"shgo\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) elif optimizer_name == \"basinhopping\" : result = self . optimizer ( func = self . infill , x0 = self . min_X ) else : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) new_X [ i ][:] = result . x return new_X def suggest_new_X ( self ): \"\"\" Compute `n_points` new infill points in natural units. The optimizer searches in the ranges from `lower_j` to `upper_j`. The method `infill()` is used as the objective function. Returns: (numpy.ndarray): `n_points` infill points in natural units, each of dim k Note: This is step (S-14a) in [bart21i]. \"\"\" # (S-14a) Optimization on the surrogate: new_X = np . zeros ([ self . n_points , self . k ], dtype = float ) optimizer_name = self . optimizer . __name__ optimizers = { \"dual_annealing\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), \"differential_evolution\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds , maxiter = self . optimizer_control [ \"max_iter\" ], seed = self . optimizer_control [ \"seed\" ], ), \"direct\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds , eps = 1e-2 ), \"shgo\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), \"basinhopping\" : lambda : self . optimizer ( func = self . infill , x0 = self . min_X ), \"default\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), } for i in range ( self . n_points ): result = optimizers . get ( optimizer_name , optimizers [ \"default\" ])() new_X [ i ][:] = result . x return new_X def infill ( self , x ): \"\"\" Infill (acquisition) function. Evaluates one point on the surrogate via `surrogate.predict(x.reshape(1,-1))`, if `sklearn` surrogates are used or `surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)` if the internal surrogate `kriging` is selected. This method is passed to the optimizer in `suggest_new_X`, i.e., the optimizer is called via `self.optimizer(func=self.infill)`. Args: x (array): point in natural units with shape `(1, dim)`. Returns: (numpy.ndarray): value based on infill criterion, e.g., `\"ei\"`. Shape `(1,)`. The objective function value `y` that is used as a base value for the infill criterion is calculated in natural units. Note: This is step (S-12) in [bart21i]. \"\"\" # Reshape x to have shape (1, -1) because the predict method expects a 2D array x_reshaped = x . reshape ( 1 , - 1 ) if isinstance ( self . surrogate , Kriging ): return self . surrogate . predict ( x_reshaped , return_val = self . infill_criterion ) else : return self . surrogate . predict ( x_reshaped ) def plot_progress ( self , show = True , log_x = False , log_y = False , filename = \"plot.png\" , style = [ \"ko\" , \"k\" , \"ro-\" ], dpi = 300 ) -> None : \"\"\"Plot the progress of the hyperparameter tuning (optimization). Args: show (bool): Show the plot. log_x (bool): Use logarithmic scale for x-axis. log_y (bool): Use logarithmic scale for y-axis. filename (str): Filename to save the plot. style (list): Style of the plot. Default: ['k', 'ro-'], i.e., the initial points are plotted as a black line and the subsequent points as red dots connected by a line. Returns: None \"\"\" fig = pylab . figure ( figsize = ( 9 , 6 )) s_y = pd . Series ( self . y ) s_c = s_y . cummin () n_init = self . design_control [ \"init_size\" ] * self . design_control [ \"repeats\" ] ax = fig . add_subplot ( 211 ) ax . plot ( range ( 1 , n_init + 1 ), s_y [: n_init ], style [ 0 ], range ( 1 , n_init + 2 ), [ s_c [: n_init ] . min ()] * ( n_init + 1 ), style [ 1 ], range ( n_init + 1 , len ( s_c ) + 1 ), s_c [ n_init :], style [ 2 ], ) if log_x : ax . set_xscale ( \"log\" ) if log_y : ax . set_yscale ( \"log\" ) if filename is not None : pylab . savefig ( filename , dpi = dpi , bbox_inches = \"tight\" ) if show : pylab . show () def plot_model ( self , y_min = None , y_max = None ): \"\"\" Plot the model fit for 1-dim objective functions. Args: y_min (float, optional): y range, lower bound. y_max (float, optional): y range, upper bound. \"\"\" if self . k == 1 : X_test = np . linspace ( self . lower [ 0 ], self . upper [ 0 ], 100 ) y_test = self . fun ( X = X_test . reshape ( - 1 , 1 ), fun_control = self . fun_control ) if isinstance ( self . surrogate , Kriging ): y_hat = self . surrogate . predict ( X_test [:, np . newaxis ], return_val = \"y\" ) else : y_hat = self . surrogate . predict ( X_test [:, np . newaxis ]) plt . plot ( X_test , y_hat , label = \"Model\" ) plt . plot ( X_test , y_test , label = \"True function\" ) plt . scatter ( self . X , self . y , edgecolor = \"b\" , s = 20 , label = \"Samples\" ) plt . scatter ( self . X [ - 1 ], self . y [ - 1 ], edgecolor = \"r\" , s = 30 , label = \"Last Sample\" ) if self . noise : plt . scatter ( self . min_mean_X , self . min_mean_y , edgecolor = \"g\" , s = 30 , label = \"Best Sample (mean)\" ) else : plt . scatter ( self . min_X , self . min_y , edgecolor = \"g\" , s = 30 , label = \"Best Sample\" ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) plt . xlim (( self . lower [ 0 ], self . upper [ 0 ])) if y_min is None : y_min = min ( min ( self . y ), min ( y_test )) if y_max is None : y_max = max ( max ( self . y ), max ( y_test )) plt . ylim (( y_min , y_max )) plt . legend ( loc = \"best\" ) # plt.title(self.surrogate.__class__.__name__ + \". \" + str(self.counter) + \": \" + str(self.min_y)) if self . noise : plt . title ( str ( self . counter ) + \". y (noise): \" + str ( np . round ( self . min_y , 6 )) + \" y mean: \" + str ( np . round ( self . min_mean_y , 6 )) ) else : plt . title ( str ( self . counter ) + \". y: \" + str ( np . round ( self . min_y , 6 ))) plt . show () def print_results ( self , print_screen = True ) -> list [ str ]: \"\"\"Print results from the run: 1. min y 2. min X If `noise == True`, additionally the following values are printed: 3. min mean y 4. min mean X Args: print_screen (bool, optional): print results to screen Returns: output (list): list of results \"\"\" output = [] if print_screen : print ( f \"min y: { self . min_y } \" ) res = self . to_all_dim ( self . min_X . reshape ( 1 , - 1 )) for i in range ( res . shape [ 1 ]): var_name = \"x\" + str ( i ) if self . all_var_name is None else self . all_var_name [ i ] if print_screen : print ( var_name + \":\" , res [ 0 ][ i ]) output . append ([ var_name , res [ 0 ][ i ]]) if self . noise : res = self . to_all_dim ( self . min_mean_X . reshape ( 1 , - 1 )) if print_screen : print ( f \"min mean y: { self . min_mean_y } \" ) for i in range ( res . shape [ 1 ]): var_name = \"x\" + str ( i ) if self . all_var_name is None else self . all_var_name [ i ] if print_screen : print ( var_name + \":\" , res [ 0 ][ i ]) output . append ([ var_name , res [ 0 ][ i ]]) return output def chg ( self , x , y , z0 , i , j ): \"\"\" Change the values of elements at indices `i` and `j` in the array `z0` to `x` and `y`, respectively. Args: x (int or float): The new value for the element at index `i`. y (int or float): The new value for the element at index `j`. z0 (list or numpy.ndarray): The array to be modified. i (int): The index of the element to be changed to `x`. j (int): The index of the element to be changed to `y`. Returns: (list) or (numpy.ndarray): The modified array. Examples: >>> z0 = [1, 2, 3] >>> chg(4, 5, z0, 0, 2) [4, 2, 5] \"\"\" z0 [ i ] = x z0 [ j ] = y return z0 def plot_contour ( self , i = 0 , j = 1 , min_z = None , max_z = None , show = True , filename = None , n_grid = 25 , contour_levels = 10 , dpi = 200 ) -> None : \"\"\"Plot the contour of any dimension. Args: i (int): the first dimension j (int): the second dimension min_z (float): the minimum value of z max_z (float): the maximum value of z show (bool): show the plot filename (str): save the plot to a file n_grid (int): number of grid points contour_levels (int): number of contour levels Returns: None \"\"\" fig = pylab . figure ( figsize = ( 9 , 6 )) # lower and upper x = np . linspace ( self . lower [ i ], self . upper [ i ], num = n_grid ) y = np . linspace ( self . lower [ j ], self . upper [ j ], num = n_grid ) X , Y = meshgrid ( x , y ) # Predict based on the optimized results z0 = np . mean ( np . array ([ self . lower , self . upper ]), axis = 0 ) zz = array ([ self . surrogate . predict ( array ([ self . chg ( x , y , z0 , i , j )])) for x , y in zip ( ravel ( X ), ravel ( Y ))]) zs = zz [:, 0 ] Z = zs . reshape ( X . shape ) if min_z is None : min_z = np . min ( Z ) if max_z is None : max_z = np . max ( Z ) ax = fig . add_subplot ( 221 ) # plot predicted values: plt . contourf ( X , Y , Z , contour_levels , zorder = 1 , cmap = \"jet\" , vmin = min_z , vmax = max_z ) if self . var_name is None : plt . xlabel ( \"x\" + str ( i )) plt . ylabel ( \"x\" + str ( j )) else : plt . xlabel ( \"x\" + str ( i ) + \": \" + self . var_name [ i ]) plt . ylabel ( \"x\" + str ( j ) + \": \" + self . var_name [ j ]) plt . title ( \"Surrogate\" ) pylab . colorbar () ax = fig . add_subplot ( 222 , projection = \"3d\" ) ax . plot_surface ( X , Y , Z , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" , vmin = min_z , vmax = max_z ) if self . var_name is None : plt . xlabel ( \"x\" + str ( i )) plt . ylabel ( \"x\" + str ( j )) else : plt . xlabel ( \"x\" + str ( i ) + \": \" + self . var_name [ i ]) plt . ylabel ( \"x\" + str ( j ) + \": \" + self . var_name [ j ]) if filename : pylab . savefig ( filename , bbox_inches = \"tight\" , dpi = dpi , pad_inches = 0 ), if show : pylab . show () def plot_important_hyperparameter_contour ( self , threshold = 0.025 , filename = None ): impo = self . print_importance ( threshold = threshold , print_screen = True ) var_plots = [ i for i , x in enumerate ( impo ) if x [ 1 ] > threshold ] min_z = min ( self . y ) max_z = max ( self . y ) for i in var_plots : for j in var_plots : if j > i : if filename is not None : filename_full = filename + \"_contour_\" + str ( i ) + \"_\" + str ( j ) + \".png\" else : filename_full = None self . plot_contour ( i = i , j = j , min_z = min_z , max_z = max_z , filename = filename_full ) def get_importance ( self ) -> list : \"\"\"Get importance of each variable and return the results as a list. Returns: output (list): list of results \"\"\" if self . surrogate . n_theta > 1 and self . var_name is not None : output = [ 0 ] * len ( self . all_var_name ) theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) ind = find_indices ( A = self . var_name , B = self . all_var_name ) j = 0 for i in ind : output [ i ] = imp [ j ] j = j + 1 return output else : print ( \"Importance requires more than one theta values (n_theta>1).\" ) def print_importance ( self , threshold = 0.1 , print_screen = True ) -> list : \"\"\"Print importance of each variable and return the results as a list. Args: threshold (float): threshold for printing print_screen (boolean): if `True`, values are also printed on the screen. Default is `True`. Returns: output (list): list of results \"\"\" output = [] if self . surrogate . n_theta > 1 : theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) # imp = imp[imp >= threshold] if self . var_name is None : for i in range ( len ( imp )): if imp [ i ] >= threshold : if print_screen : print ( \"x\" , i , \": \" , imp [ i ]) output . append ( \"x\" + str ( i ) + \": \" + str ( imp [ i ])) else : var_name = [ self . var_name [ i ] for i in range ( len ( imp ))] for i in range ( len ( imp )): if imp [ i ] >= threshold : if print_screen : print ( var_name [ i ] + \": \" , imp [ i ]) output . append ([ var_name [ i ], imp [ i ]]) else : print ( \"Importance requires more than one theta values (n_theta>1).\" ) return output def plot_importance ( self , threshold = 0.1 , filename = None , dpi = 300 ) -> None : \"\"\"Plot the importance of each variable. Args: threshold (float): The threshold of the importance. filename (str): The filename of the plot. Returns: None \"\"\" if self . surrogate . n_theta > 1 : theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) idx = np . where ( imp > threshold )[ 0 ] if self . var_name is None : plt . bar ( range ( len ( imp [ idx ])), imp [ idx ]) plt . xticks ( range ( len ( imp [ idx ])), [ \"x\" + str ( i ) for i in idx ]) else : var_name = [ self . var_name [ i ] for i in idx ] plt . bar ( range ( len ( imp [ idx ])), imp [ idx ]) plt . xticks ( range ( len ( imp [ idx ])), var_name ) if filename is not None : plt . savefig ( filename , bbox_inches = \"tight\" , dpi = dpi ) plt . show () def parallel_plot ( self ): X = self . X y = self . y df = pd . DataFrame ( np . concatenate (( X , y . reshape ( - 1 , 1 )), axis = 1 ), columns = self . var_name + [ \"y\" ]) fig = go . Figure ( data = go . Parcoords ( line = dict ( color = df [ \"y\" ], colorscale = \"Jet\" , showscale = True , cmin = min ( df [ \"y\" ]), cmax = max ( df [ \"y\" ])), dimensions = list ( [ dict ( range = [ min ( df . iloc [:, i ]), max ( df . iloc [:, i ])], label = df . columns [ i ], values = df . iloc [:, i ]) for i in range ( len ( df . columns ) - 1 ) ] ), ) ) fig . show () chg ( x , y , z0 , i , j ) \u00b6 Change the values of elements at indices i and j in the array z0 to x and y , respectively. Parameters: Name Type Description Default x int or float The new value for the element at index i . required y int or float The new value for the element at index j . required z0 list or numpy.ndarray The array to be modified. required i int The index of the element to be changed to x . required j int The index of the element to be changed to y . required Returns: Type Description list) or (numpy.ndarray The modified array. Examples: >>> z0 = [ 1 , 2 , 3 ] >>> chg ( 4 , 5 , z0 , 0 , 2 ) [4, 2, 5] Source code in spotPython/spot/spot.py 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 def chg ( self , x , y , z0 , i , j ): \"\"\" Change the values of elements at indices `i` and `j` in the array `z0` to `x` and `y`, respectively. Args: x (int or float): The new value for the element at index `i`. y (int or float): The new value for the element at index `j`. z0 (list or numpy.ndarray): The array to be modified. i (int): The index of the element to be changed to `x`. j (int): The index of the element to be changed to `y`. Returns: (list) or (numpy.ndarray): The modified array. Examples: >>> z0 = [1, 2, 3] >>> chg(4, 5, z0, 0, 2) [4, 2, 5] \"\"\" z0 [ i ] = x z0 [ j ] = y return z0 get_importance () \u00b6 Get importance of each variable and return the results as a list. Returns: Name Type Description output list list of results Source code in spotPython/spot/spot.py 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 def get_importance ( self ) -> list : \"\"\"Get importance of each variable and return the results as a list. Returns: output (list): list of results \"\"\" if self . surrogate . n_theta > 1 and self . var_name is not None : output = [ 0 ] * len ( self . all_var_name ) theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) ind = find_indices ( A = self . var_name , B = self . all_var_name ) j = 0 for i in ind : output [ i ] = imp [ j ] j = j + 1 return output else : print ( \"Importance requires more than one theta values (n_theta>1).\" ) infill ( x ) \u00b6 Infill (acquisition) function. Evaluates one point on the surrogate via surrogate.predict(x.reshape(1,-1)) , if sklearn surrogates are used or surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion) if the internal surrogate kriging is selected. This method is passed to the optimizer in suggest_new_X , i.e., the optimizer is called via self.optimizer(func=self.infill) . Parameters: Name Type Description Default x array point in natural units with shape (1, dim) . required Returns: Type Description numpy . ndarray value based on infill criterion, e.g., \"ei\" . Shape (1,) . The objective function value y that is used as a base value for the infill criterion is calculated in natural units. Note This is step (S-12) in [bart21i]. Source code in spotPython/spot/spot.py 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 def infill ( self , x ): \"\"\" Infill (acquisition) function. Evaluates one point on the surrogate via `surrogate.predict(x.reshape(1,-1))`, if `sklearn` surrogates are used or `surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)` if the internal surrogate `kriging` is selected. This method is passed to the optimizer in `suggest_new_X`, i.e., the optimizer is called via `self.optimizer(func=self.infill)`. Args: x (array): point in natural units with shape `(1, dim)`. Returns: (numpy.ndarray): value based on infill criterion, e.g., `\"ei\"`. Shape `(1,)`. The objective function value `y` that is used as a base value for the infill criterion is calculated in natural units. Note: This is step (S-12) in [bart21i]. \"\"\" # Reshape x to have shape (1, -1) because the predict method expects a 2D array x_reshaped = x . reshape ( 1 , - 1 ) if isinstance ( self . surrogate , Kriging ): return self . surrogate . predict ( x_reshaped , return_val = self . infill_criterion ) else : return self . surrogate . predict ( x_reshaped ) plot_contour ( i = 0 , j = 1 , min_z = None , max_z = None , show = True , filename = None , n_grid = 25 , contour_levels = 10 , dpi = 200 ) \u00b6 Plot the contour of any dimension. Parameters: Name Type Description Default i int the first dimension 0 j int the second dimension 1 min_z float the minimum value of z None max_z float the maximum value of z None show bool show the plot True filename str save the plot to a file None n_grid int number of grid points 25 contour_levels int number of contour levels 10 Returns: Type Description None None Source code in spotPython/spot/spot.py 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 def plot_contour ( self , i = 0 , j = 1 , min_z = None , max_z = None , show = True , filename = None , n_grid = 25 , contour_levels = 10 , dpi = 200 ) -> None : \"\"\"Plot the contour of any dimension. Args: i (int): the first dimension j (int): the second dimension min_z (float): the minimum value of z max_z (float): the maximum value of z show (bool): show the plot filename (str): save the plot to a file n_grid (int): number of grid points contour_levels (int): number of contour levels Returns: None \"\"\" fig = pylab . figure ( figsize = ( 9 , 6 )) # lower and upper x = np . linspace ( self . lower [ i ], self . upper [ i ], num = n_grid ) y = np . linspace ( self . lower [ j ], self . upper [ j ], num = n_grid ) X , Y = meshgrid ( x , y ) # Predict based on the optimized results z0 = np . mean ( np . array ([ self . lower , self . upper ]), axis = 0 ) zz = array ([ self . surrogate . predict ( array ([ self . chg ( x , y , z0 , i , j )])) for x , y in zip ( ravel ( X ), ravel ( Y ))]) zs = zz [:, 0 ] Z = zs . reshape ( X . shape ) if min_z is None : min_z = np . min ( Z ) if max_z is None : max_z = np . max ( Z ) ax = fig . add_subplot ( 221 ) # plot predicted values: plt . contourf ( X , Y , Z , contour_levels , zorder = 1 , cmap = \"jet\" , vmin = min_z , vmax = max_z ) if self . var_name is None : plt . xlabel ( \"x\" + str ( i )) plt . ylabel ( \"x\" + str ( j )) else : plt . xlabel ( \"x\" + str ( i ) + \": \" + self . var_name [ i ]) plt . ylabel ( \"x\" + str ( j ) + \": \" + self . var_name [ j ]) plt . title ( \"Surrogate\" ) pylab . colorbar () ax = fig . add_subplot ( 222 , projection = \"3d\" ) ax . plot_surface ( X , Y , Z , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" , vmin = min_z , vmax = max_z ) if self . var_name is None : plt . xlabel ( \"x\" + str ( i )) plt . ylabel ( \"x\" + str ( j )) else : plt . xlabel ( \"x\" + str ( i ) + \": \" + self . var_name [ i ]) plt . ylabel ( \"x\" + str ( j ) + \": \" + self . var_name [ j ]) if filename : pylab . savefig ( filename , bbox_inches = \"tight\" , dpi = dpi , pad_inches = 0 ), if show : pylab . show () plot_importance ( threshold = 0.1 , filename = None , dpi = 300 ) \u00b6 Plot the importance of each variable. Parameters: Name Type Description Default threshold float The threshold of the importance. 0.1 filename str The filename of the plot. None Returns: Type Description None None Source code in spotPython/spot/spot.py 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 def plot_importance ( self , threshold = 0.1 , filename = None , dpi = 300 ) -> None : \"\"\"Plot the importance of each variable. Args: threshold (float): The threshold of the importance. filename (str): The filename of the plot. Returns: None \"\"\" if self . surrogate . n_theta > 1 : theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) idx = np . where ( imp > threshold )[ 0 ] if self . var_name is None : plt . bar ( range ( len ( imp [ idx ])), imp [ idx ]) plt . xticks ( range ( len ( imp [ idx ])), [ \"x\" + str ( i ) for i in idx ]) else : var_name = [ self . var_name [ i ] for i in idx ] plt . bar ( range ( len ( imp [ idx ])), imp [ idx ]) plt . xticks ( range ( len ( imp [ idx ])), var_name ) if filename is not None : plt . savefig ( filename , bbox_inches = \"tight\" , dpi = dpi ) plt . show () plot_model ( y_min = None , y_max = None ) \u00b6 Plot the model fit for 1-dim objective functions. Parameters: Name Type Description Default y_min float y range, lower bound. None y_max float y range, upper bound. None Source code in spotPython/spot/spot.py 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 def plot_model ( self , y_min = None , y_max = None ): \"\"\" Plot the model fit for 1-dim objective functions. Args: y_min (float, optional): y range, lower bound. y_max (float, optional): y range, upper bound. \"\"\" if self . k == 1 : X_test = np . linspace ( self . lower [ 0 ], self . upper [ 0 ], 100 ) y_test = self . fun ( X = X_test . reshape ( - 1 , 1 ), fun_control = self . fun_control ) if isinstance ( self . surrogate , Kriging ): y_hat = self . surrogate . predict ( X_test [:, np . newaxis ], return_val = \"y\" ) else : y_hat = self . surrogate . predict ( X_test [:, np . newaxis ]) plt . plot ( X_test , y_hat , label = \"Model\" ) plt . plot ( X_test , y_test , label = \"True function\" ) plt . scatter ( self . X , self . y , edgecolor = \"b\" , s = 20 , label = \"Samples\" ) plt . scatter ( self . X [ - 1 ], self . y [ - 1 ], edgecolor = \"r\" , s = 30 , label = \"Last Sample\" ) if self . noise : plt . scatter ( self . min_mean_X , self . min_mean_y , edgecolor = \"g\" , s = 30 , label = \"Best Sample (mean)\" ) else : plt . scatter ( self . min_X , self . min_y , edgecolor = \"g\" , s = 30 , label = \"Best Sample\" ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) plt . xlim (( self . lower [ 0 ], self . upper [ 0 ])) if y_min is None : y_min = min ( min ( self . y ), min ( y_test )) if y_max is None : y_max = max ( max ( self . y ), max ( y_test )) plt . ylim (( y_min , y_max )) plt . legend ( loc = \"best\" ) # plt.title(self.surrogate.__class__.__name__ + \". \" + str(self.counter) + \": \" + str(self.min_y)) if self . noise : plt . title ( str ( self . counter ) + \". y (noise): \" + str ( np . round ( self . min_y , 6 )) + \" y mean: \" + str ( np . round ( self . min_mean_y , 6 )) ) else : plt . title ( str ( self . counter ) + \". y: \" + str ( np . round ( self . min_y , 6 ))) plt . show () plot_progress ( show = True , log_x = False , log_y = False , filename = 'plot.png' , style = [ 'ko' , 'k' , 'ro-' ], dpi = 300 ) \u00b6 Plot the progress of the hyperparameter tuning (optimization). Parameters: Name Type Description Default show bool Show the plot. True log_x bool Use logarithmic scale for x-axis. False log_y bool Use logarithmic scale for y-axis. False filename str Filename to save the plot. 'plot.png' style list Style of the plot. Default: [\u2018k\u2019, \u2018ro-\u2018], i.e., the initial points are plotted as a black line and the subsequent points as red dots connected by a line. ['ko', 'k', 'ro-'] Returns: Type Description None None Source code in spotPython/spot/spot.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 def plot_progress ( self , show = True , log_x = False , log_y = False , filename = \"plot.png\" , style = [ \"ko\" , \"k\" , \"ro-\" ], dpi = 300 ) -> None : \"\"\"Plot the progress of the hyperparameter tuning (optimization). Args: show (bool): Show the plot. log_x (bool): Use logarithmic scale for x-axis. log_y (bool): Use logarithmic scale for y-axis. filename (str): Filename to save the plot. style (list): Style of the plot. Default: ['k', 'ro-'], i.e., the initial points are plotted as a black line and the subsequent points as red dots connected by a line. Returns: None \"\"\" fig = pylab . figure ( figsize = ( 9 , 6 )) s_y = pd . Series ( self . y ) s_c = s_y . cummin () n_init = self . design_control [ \"init_size\" ] * self . design_control [ \"repeats\" ] ax = fig . add_subplot ( 211 ) ax . plot ( range ( 1 , n_init + 1 ), s_y [: n_init ], style [ 0 ], range ( 1 , n_init + 2 ), [ s_c [: n_init ] . min ()] * ( n_init + 1 ), style [ 1 ], range ( n_init + 1 , len ( s_c ) + 1 ), s_c [ n_init :], style [ 2 ], ) if log_x : ax . set_xscale ( \"log\" ) if log_y : ax . set_yscale ( \"log\" ) if filename is not None : pylab . savefig ( filename , dpi = dpi , bbox_inches = \"tight\" ) if show : pylab . show () print_importance ( threshold = 0.1 , print_screen = True ) \u00b6 Print importance of each variable and return the results as a list. Parameters: Name Type Description Default threshold float threshold for printing 0.1 print_screen boolean if True , values are also printed on the screen. Default is True . True Returns: Name Type Description output list list of results Source code in spotPython/spot/spot.py 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 def print_importance ( self , threshold = 0.1 , print_screen = True ) -> list : \"\"\"Print importance of each variable and return the results as a list. Args: threshold (float): threshold for printing print_screen (boolean): if `True`, values are also printed on the screen. Default is `True`. Returns: output (list): list of results \"\"\" output = [] if self . surrogate . n_theta > 1 : theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) # imp = imp[imp >= threshold] if self . var_name is None : for i in range ( len ( imp )): if imp [ i ] >= threshold : if print_screen : print ( \"x\" , i , \": \" , imp [ i ]) output . append ( \"x\" + str ( i ) + \": \" + str ( imp [ i ])) else : var_name = [ self . var_name [ i ] for i in range ( len ( imp ))] for i in range ( len ( imp )): if imp [ i ] >= threshold : if print_screen : print ( var_name [ i ] + \": \" , imp [ i ]) output . append ([ var_name [ i ], imp [ i ]]) else : print ( \"Importance requires more than one theta values (n_theta>1).\" ) return output print_results ( print_screen = True ) \u00b6 Print results from the run min y min X If noise == True , additionally the following values are printed: min mean y min mean X Parameters: Name Type Description Default print_screen bool print results to screen True Returns: Name Type Description output list list of results Source code in spotPython/spot/spot.py 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 def print_results ( self , print_screen = True ) -> list [ str ]: \"\"\"Print results from the run: 1. min y 2. min X If `noise == True`, additionally the following values are printed: 3. min mean y 4. min mean X Args: print_screen (bool, optional): print results to screen Returns: output (list): list of results \"\"\" output = [] if print_screen : print ( f \"min y: { self . min_y } \" ) res = self . to_all_dim ( self . min_X . reshape ( 1 , - 1 )) for i in range ( res . shape [ 1 ]): var_name = \"x\" + str ( i ) if self . all_var_name is None else self . all_var_name [ i ] if print_screen : print ( var_name + \":\" , res [ 0 ][ i ]) output . append ([ var_name , res [ 0 ][ i ]]) if self . noise : res = self . to_all_dim ( self . min_mean_X . reshape ( 1 , - 1 )) if print_screen : print ( f \"min mean y: { self . min_mean_y } \" ) for i in range ( res . shape [ 1 ]): var_name = \"x\" + str ( i ) if self . all_var_name is None else self . all_var_name [ i ] if print_screen : print ( var_name + \":\" , res [ 0 ][ i ]) output . append ([ var_name , res [ 0 ][ i ]]) return output suggest_new_X () \u00b6 Compute n_points new infill points in natural units. The optimizer searches in the ranges from lower_j to upper_j . The method infill() is used as the objective function. Returns: Type Description numpy . ndarray n_points infill points in natural units, each of dim k Note This is step (S-14a) in [bart21i]. Source code in spotPython/spot/spot.py 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 def suggest_new_X ( self ): \"\"\" Compute `n_points` new infill points in natural units. The optimizer searches in the ranges from `lower_j` to `upper_j`. The method `infill()` is used as the objective function. Returns: (numpy.ndarray): `n_points` infill points in natural units, each of dim k Note: This is step (S-14a) in [bart21i]. \"\"\" # (S-14a) Optimization on the surrogate: new_X = np . zeros ([ self . n_points , self . k ], dtype = float ) optimizer_name = self . optimizer . __name__ optimizers = { \"dual_annealing\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), \"differential_evolution\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds , maxiter = self . optimizer_control [ \"max_iter\" ], seed = self . optimizer_control [ \"seed\" ], ), \"direct\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds , eps = 1e-2 ), \"shgo\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), \"basinhopping\" : lambda : self . optimizer ( func = self . infill , x0 = self . min_X ), \"default\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), } for i in range ( self . n_points ): result = optimizers . get ( optimizer_name , optimizers [ \"default\" ])() new_X [ i ][:] = result . x return new_X suggest_new_X_old () \u00b6 Compute n_points new infill points in natural units. The optimizer searches in the ranges from lower_j to upper_j . The method infill() is used as the objective function. Returns: Type Description numpy . ndarray n_points infill points in natural units, each of dim k Note This is step (S-14a) in [bart21i]. Source code in spotPython/spot/spot.py 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 def suggest_new_X_old ( self ): \"\"\" Compute `n_points` new infill points in natural units. The optimizer searches in the ranges from `lower_j` to `upper_j`. The method `infill()` is used as the objective function. Returns: (numpy.ndarray): `n_points` infill points in natural units, each of dim k Note: This is step (S-14a) in [bart21i]. \"\"\" # (S-14a) Optimization on the surrogate: new_X = np . zeros ([ self . n_points , self . k ], dtype = float ) optimizer_name = self . optimizer . __name__ for i in range ( self . n_points ): if optimizer_name == \"dual_annealing\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) elif optimizer_name == \"differential_evolution\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds , maxiter = self . optimizer_control [ \"max_iter\" ], seed = self . optimizer_control [ \"seed\" ], # popsize=10, # updating=\"deferred\" ) elif optimizer_name == \"direct\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds , eps = 1e-2 ) elif optimizer_name == \"shgo\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) elif optimizer_name == \"basinhopping\" : result = self . optimizer ( func = self . infill , x0 = self . min_X ) else : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) new_X [ i ][:] = result . x return new_X update_stats () \u00b6 Update the following stats: 1. min_y 2. min_X 3. counter If noise is True , additionally the following stats are computed: 1. mean_X 2. mean_y 3. min_mean_y 4. min_mean_X . Source code in spotPython/spot/spot.py 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 def update_stats ( self ): \"\"\" Update the following stats: 1. `min_y` 2. `min_X` 3. `counter` If `noise` is `True`, additionally the following stats are computed: 1. `mean_X` 2. `mean_y` 3. `min_mean_y` 4. `min_mean_X`. \"\"\" self . min_y = min ( self . y ) self . min_X = self . X [ argmin ( self . y )] self . counter = self . y . size # Update aggregated x and y values (if noise): if self . noise : Z = aggregate_mean_var ( X = self . X , y = self . y ) self . mean_X = Z [ 0 ] self . mean_y = Z [ 1 ] self . var_y = Z [ 2 ] # X value of the best mean y value so far: self . min_mean_X = self . mean_X [ argmin ( self . mean_y )] # variance of the best mean y value so far: self . min_var_y = self . var_y [ argmin ( self . mean_y )] # best mean y value so far: self . min_mean_y = self . mean_y [ argmin ( self . mean_y )]","title":"spot"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot","text":"Spot base class to handle the following tasks in a uniform manner: Getting and setting parameters. This is done via the Spot initialization. Running surrogate based hyperparameter optimization. After the class is initialized, hyperparameter tuning runs can be performed via the run method. Displaying information. The plot method can be used for visualizing results. The print methods summarizes information about the tuning run. The Spot class is built in a modular manner. It combines the following three components: 1. Design 2. Surrogate 3. Optimizer For each of the three components different implementations can be selected and combined. Internal components are selected as default. These can be replaced by components from other packages, e.g., scikit-learn or scikit-optimize. Parameters: Name Type Description Default fun Callable objective function required lower np . array lower bound required upper np . array upper bound required fun_evals int number of function evaluations 15 fun_repeats int number of repeats (replicates). 1 max_time int maximum time (in minutes) inf noise bool deterministic or noisy objective function False tolerance_x float tolerance for new x solutions. Minimum distance of new solutions, generated by suggest_new_X , to already existing solutions. If zero (which is the default), every new solution is accepted. 0 ocba_delta int OCBA increment (only used if noise==True ) 0 var_type List [ str ] list of type information, can be either \u201cnum\u201d or \u201cfactor\u201d ['num'] var_name List [ str ] list of variable names, e.g., [\u201cx1\u201d, \u201cx2\u201d] None infill_criterion str Can be \"y\" , \"s\" , \"ei\" (negative expected improvement), or \"all\" . 'y' n_points int number of infill points 1 seed int initial seed 123 log_level int log level with the following settings: NOTSET ( 0 ), DEBUG ( 10 : Detailed information, typically of interest only when diagnosing problems.), INFO ( 20 : Confirmation that things are working as expected.), WARNING ( 30 : An indication that something unexpected happened, or indicative of some problem in the near future (e.g. \u2018disk space low\u2019). The software is still working as expected.), ERROR ( 40 : Due to a more serious problem, the software has not been able to perform some function.), and CRITICAL ( 50 : A serious error, indicating that the program itself may be unable to continue running.) 50 show_models bool Plot model. Currently only 1-dim functions are supported. False show_progress bool Show progress bar. True design object experimental design. None design_control Dict [ str , Union [ int , float ]] experimental design information stored as a dictionary with the following entries: \u201cinit_size\u201d: 10 , \u201crepeats\u201d: 1 . {} surrogate object surrogate model. If None , spotPython\u2019s kriging is used. None surrogate_control Dict [ str , Union [ int , float ]] surrogate model information stored as a dictionary with the following entries: \u201cmodel_optimizer\u201d: differential_evolution , \u201cmodel_fun_evals\u201d: None , \u201cmin_theta\u201d: -3. , \u201cmax_theta\u201d: 3. , \u201cn_theta\u201d: 1 , \u201cn_p\u201d: 1 , \u201coptim_p\u201d: False , \u201ccod_type\u201d: \"norm\" , \u201cvar_type\u201d: self.var_type , \u201cuse_cod_y\u201d: False . {} optimizer object optimizer. If None , scipy.optimize \u2018s differential_evolution is used. None optimizer_control Dict [ str , Union [ int , float ]] information about the optimizer stored as a dictionary with the following entries: \u201cmax_iter\u201d: 1000 . {} Returns: Type Description NoneType None Note Description in the source code refers to [bart21i]: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches. In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67\u2013114. Examples: >>> import numpy as np >>> from math import inf >>> from spotpy.spot_setup import Spot >>> def objective_function ( x ): >>> return x [ 0 ] ** 2 + x [ 1 ] ** 2 >>> lower = np . array ([ 0 , 0 ]) >>> upper = np . array ([ 10 , 10 ]) >>> spot = Spot ( fun = objective_function , >>> lower = lower , >>> upper = upper , >>> fun_evals = 100 , >>> fun_repeats = 1 , >>> max_time = inf , >>> noise = False , >>> tolerance_x = 0 , >>> ocba_delta = 0 , >>> var_type = [ \"num\" , \"num\" ], >>> var_name = [ \"x1\" , \"x2\" ], >>> infill_criterion = \"ei\" , >>> n_points = 10 , >>> seed = 123 , >>> log_level = 20 , >>> show_models = False , >>> show_progress = True , >>> design = None , >>> design_control = { \"init_size\" : 10 , \"repeats\" : 1 }, >>> surrogate = None , >>> surrogate_control = { \"model_optimizer\" : \"differential_evolution\" , >>> \"model_fun_evals\" : None , >>> \"min_theta\" : - 3. , >>> \"max_theta\" : 3. , >>> \"n_theta\" : 1 , >>> \"n_p\" : 1 , >>> \"optim_p\" : False , >>> \"cod_type\" : \"norm\" , >>> \"var_type\" : [ \"num\" , \"num\" ], >>> \"use_cod_y\" : False }, >>> optimizer_control = { \"max_iter\" : 1000 }) >>> spot . run () Source code in spotPython/spot/spot.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 class Spot : \"\"\" Spot base class to handle the following tasks in a uniform manner: * Getting and setting parameters. This is done via the `Spot` initialization. * Running surrogate based hyperparameter optimization. After the class is initialized, hyperparameter tuning runs can be performed via the `run` method. * Displaying information. The `plot` method can be used for visualizing results. The `print` methods summarizes information about the tuning run. The `Spot` class is built in a modular manner. It combines the following three components: 1. Design 2. Surrogate 3. Optimizer For each of the three components different implementations can be selected and combined. Internal components are selected as default. These can be replaced by components from other packages, e.g., scikit-learn or scikit-optimize. Args: fun (Callable): objective function lower (np.array): lower bound upper (np.array): upper bound fun_evals (int): number of function evaluations fun_repeats (int): number of repeats (replicates). max_time (int): maximum time (in minutes) noise (bool): deterministic or noisy objective function tolerance_x (float): tolerance for new x solutions. Minimum distance of new solutions, generated by `suggest_new_X`, to already existing solutions. If zero (which is the default), every new solution is accepted. ocba_delta (int): OCBA increment (only used if `noise==True`) var_type (List[str]): list of type information, can be either \"num\" or \"factor\" var_name (List[str]): list of variable names, e.g., [\"x1\", \"x2\"] infill_criterion (str): Can be `\"y\"`, `\"s\"`, `\"ei\"` (negative expected improvement), or `\"all\"`. n_points (int): number of infill points seed (int): initial seed log_level (int): log level with the following settings: `NOTSET` (`0`), `DEBUG` (`10`: Detailed information, typically of interest only when diagnosing problems.), `INFO` (`20`: Confirmation that things are working as expected.), `WARNING` (`30`: An indication that something unexpected happened, or indicative of some problem in the near future (e.g. \u2018disk space low\u2019). The software is still working as expected.), `ERROR` (`40`: Due to a more serious problem, the software has not been able to perform some function.), and `CRITICAL` (`50`: A serious error, indicating that the program itself may be unable to continue running.) show_models (bool): Plot model. Currently only 1-dim functions are supported. show_progress (bool): Show progress bar. design (object): experimental design. design_control (Dict[str, Union[int, float]]): experimental design information stored as a dictionary with the following entries: \"init_size\": `10`, \"repeats\": `1`. surrogate (object): surrogate model. If `None`, spotPython's `kriging` is used. surrogate_control (Dict[str, Union[int, float]]): surrogate model information stored as a dictionary with the following entries: \"model_optimizer\": `differential_evolution`, \"model_fun_evals\": `None`, \"min_theta\": `-3.`, \"max_theta\": `3.`, \"n_theta\": `1`, \"n_p\": `1`, \"optim_p\": `False`, \"cod_type\": `\"norm\"`, \"var_type\": `self.var_type`, \"use_cod_y\": `False`. optimizer (object): optimizer. If `None`, `scipy.optimize`'s `differential_evolution` is used. optimizer_control (Dict[str, Union[int, float]]): information about the optimizer stored as a dictionary with the following entries: \"max_iter\": `1000`. Returns: (NoneType): None Note: Description in the source code refers to [bart21i]: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches. In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67\u2013114. Examples: >>> import numpy as np >>> from math import inf >>> from spotpy.spot_setup import Spot >>> def objective_function(x): >>> return x[0]**2 + x[1]**2 >>> lower = np.array([0, 0]) >>> upper = np.array([10, 10]) >>> spot = Spot(fun=objective_function, >>> lower=lower, >>> upper=upper, >>> fun_evals=100, >>> fun_repeats=1, >>> max_time=inf, >>> noise=False, >>> tolerance_x=0, >>> ocba_delta=0, >>> var_type=[\"num\", \"num\"], >>> var_name=[\"x1\", \"x2\"], >>> infill_criterion=\"ei\", >>> n_points=10, >>> seed=123, >>> log_level=20, >>> show_models=False, >>> show_progress=True, >>> design=None, >>> design_control={\"init_size\": 10, \"repeats\": 1}, >>> surrogate=None, >>> surrogate_control={\"model_optimizer\": \"differential_evolution\", >>> \"model_fun_evals\": None, >>> \"min_theta\": -3., >>> \"max_theta\": 3., >>> \"n_theta\": 1, >>> \"n_p\": 1, >>> \"optim_p\": False, >>> \"cod_type\": \"norm\", >>> \"var_type\": [\"num\", \"num\"], >>> \"use_cod_y\": False}, >>> optimizer_control={\"max_iter\": 1000}) >>> spot.run() \"\"\" def __str__ ( self ): return self . __class__ . __name__ def __init__ ( self , fun : Callable , lower : np . array , upper : np . array , fun_evals : int = 15 , fun_repeats : int = 1 , fun_control : Dict [ str , Union [ int , float ]] = {}, max_time : int = inf , noise : bool = False , tolerance_x : float = 0 , var_type : List [ str ] = [ \"num\" ], var_name : List [ str ] = None , all_var_name : List [ str ] = None , infill_criterion : str = \"y\" , n_points : int = 1 , ocba_delta : int = 0 , seed : int = 123 , log_level : int = 50 , show_models : bool = False , show_progress : bool = True , design : object = None , design_control : Dict [ str , Union [ int , float ]] = {}, surrogate : object = None , surrogate_control : Dict [ str , Union [ int , float ]] = {}, optimizer : object = None , optimizer_control : Dict [ str , Union [ int , float ]] = {}, ): # use x0, x1, ... as default variable names: if var_name is None : var_name = [ \"x\" + str ( i ) for i in range ( len ( lower ))] # small value: self . eps = sqrt ( spacing ( 1 )) self . fun = fun self . lower = lower self . upper = upper self . var_type = var_type self . var_name = var_name self . all_var_name = all_var_name # Reduce dim based on lower == upper logic: # modifies lower, upper, and var_type self . to_red_dim () self . k = self . lower . size self . fun_evals = fun_evals self . fun_repeats = fun_repeats self . max_time = max_time self . noise = noise self . tolerance_x = tolerance_x self . ocba_delta = ocba_delta self . log_level = log_level self . show_models = show_models self . show_progress = show_progress # Random number generator: self . seed = seed self . rng = default_rng ( self . seed ) # Force numeric type as default in every dim: # assume all variable types are \"num\" if \"num\" is # specified once: if len ( self . var_type ) < self . k : self . var_type = self . var_type * self . k logger . warning ( \"Warning: All variable types forced to 'num'.\" ) self . infill_criterion = infill_criterion # Bounds de_bounds = [] for j in range ( self . k ): de_bounds . append ([ self . lower [ j ], self . upper [ j ]]) self . de_bounds = de_bounds # Infill points: self . n_points = n_points # Objective function related information: self . fun_control = { \"sigma\" : 0 , \"seed\" : None } self . fun_control . update ( fun_control ) # Design related information: self . design = design if design is None : self . design = spacefilling ( k = self . k , seed = self . seed ) self . design_control = { \"init_size\" : 10 , \"repeats\" : 1 } self . design_control . update ( design_control ) # Surrogate related information: self . surrogate = surrogate self . surrogate_control = { \"noise\" : self . noise , \"model_optimizer\" : differential_evolution , \"model_fun_evals\" : None , \"min_theta\" : - 3.0 , \"max_theta\" : 3.0 , \"n_theta\" : 1 , \"n_p\" : 1 , \"optim_p\" : False , \"cod_type\" : \"norm\" , \"var_type\" : self . var_type , \"seed\" : 124 , \"use_cod_y\" : False , } # Logging information: self . counter = 0 self . min_y = None self . min_X = None self . min_mean_X = None self . min_mean_y = None self . mean_X = None self . mean_y = None self . var_y = None logger . setLevel ( self . log_level ) logger . info ( f \"Starting the logger at level { self . log_level } for module { __name__ } :\" ) # if the key \"spot_writer\" is not in the dictionary fun_control, # set self.spot_writer to None else to the value of the key \"spot_writer\" self . spot_writer = fun_control . get ( \"spot_writer\" , None ) self . surrogate_control . update ( surrogate_control ) # If no surrogate model is specified, use the internal # spotPython kriging surrogate: if self . surrogate is None : # Call kriging with surrogate_control parameters: self . surrogate = Kriging ( name = \"kriging\" , noise = self . surrogate_control [ \"noise\" ], model_optimizer = self . surrogate_control [ \"model_optimizer\" ], model_fun_evals = self . surrogate_control [ \"model_fun_evals\" ], seed = self . surrogate_control [ \"seed\" ], log_level = self . log_level , min_theta = self . surrogate_control [ \"min_theta\" ], max_theta = self . surrogate_control [ \"max_theta\" ], n_theta = self . surrogate_control [ \"n_theta\" ], n_p = self . surrogate_control [ \"n_p\" ], optim_p = self . surrogate_control [ \"optim_p\" ], cod_type = self . surrogate_control [ \"cod_type\" ], var_type = self . surrogate_control [ \"var_type\" ], use_cod_y = self . surrogate_control [ \"use_cod_y\" ], spot_writer = self . spot_writer , counter = self . design_control [ \"init_size\" ] * self . design_control [ \"repeats\" ] - 1 , ) # Optimizer related information: self . optimizer = optimizer self . optimizer_control = { \"max_iter\" : 1000 , \"seed\" : 125 } self . optimizer_control . update ( optimizer_control ) if self . optimizer is None : self . optimizer = optimize . differential_evolution def to_red_dim ( self ): self . all_lower = self . lower self . all_upper = self . upper self . ident = ( self . upper - self . lower ) == 0 self . lower = self . lower [ ~ self . ident ] self . upper = self . upper [ ~ self . ident ] self . red_dim = self . ident . any () self . all_var_type = self . var_type self . var_type = [ x for x , y in zip ( self . all_var_type , self . ident ) if not y ] if self . var_name is not None : self . all_var_name = self . var_name self . var_name = [ x for x , y in zip ( self . all_var_name , self . ident ) if not y ] def to_all_dim ( self , X0 ): n = X0 . shape [ 0 ] k = len ( self . ident ) X = np . zeros (( n , k )) j = 0 for i in range ( k ): if self . ident [ i ]: X [:, i ] = self . all_lower [ i ] j += 1 else : X [:, i ] = X0 [:, i - j ] return X def to_all_dim_if_needed ( self , X ): if self . red_dim : return self . to_all_dim ( X ) else : return X def get_X_ocba ( self ): if self . noise and self . ocba_delta > 0 : return get_ocba_X ( self . mean_X , self . mean_y , self . var_y , self . ocba_delta ) else : return None def get_new_X0 ( self ): X0 = self . suggest_new_X () X0 = repair_non_numeric ( X0 , self . var_type ) # (S-16) Duplicate Handling: # Condition: select only X= that have min distance # to existing solutions X0 , X0_ind = selectNew ( A = X0 , X = self . X , tolerance = self . tolerance_x ) logger . debug ( \"XO values are new: %s %s \" , X0_ind , X0 ) # 1. There are X0 that fullfil the condition. # Note: The number of new X0 can be smaller than self.n_points! if X0 . shape [ 0 ] > 0 : return repeat ( X0 , self . fun_repeats , axis = 0 ) # 2. No X0 found. Then generate self.n_points new solutions: else : self . design = spacefilling ( k = self . k , seed = self . seed + self . counter ) X0 = self . generate_design ( size = self . n_points , repeats = self . design_control [ \"repeats\" ], lower = self . lower , upper = self . upper ) X0 = repair_non_numeric ( X0 , self . var_type ) logger . warning ( \"No new XO found on surrogate. Generate new solution %s \" , X0 ) return X0 def append_X_ocba ( self , X_ocba , X0 ): if self . noise and self . ocba_delta > 0 : return append ( X_ocba , X0 , axis = 0 ) else : return X0 def run ( self , X_start = None ): self . initialize_design ( X_start ) # New: self.update_stats() moved here: # self.update_stats() # (S-5) Calling the spotLoop Function # and # (S-9) Termination Criteria, Conditions: timeout_start = time . time () while self . should_continue ( timeout_start ): self . update_design () # (S-10): Subset Selection for the Surrogate: # Not implemented yet. # Update stats self . update_stats () # Update writer: self . update_writer () # (S-11) Surrogate Fit: self . fit_surrogate () # progress bar: self . show_progress_if_needed ( timeout_start ) if self . spot_writer is not None : writer = self . spot_writer writer . close () return self def initialize_design ( self , X_start = None ): # (S-2) Initial Design: X0 = self . generate_design ( size = self . design_control [ \"init_size\" ], repeats = self . design_control [ \"repeats\" ], lower = self . lower , upper = self . upper , ) if X_start is not None : try : X0 = append ( X_start , X0 , axis = 0 ) except ValueError : logger . warning ( \"X_start has wrong shape. Ignoring it.\" ) X0 = repair_non_numeric ( X0 , self . var_type ) self . X = X0 # (S-3): Eval initial design: X_all = self . to_all_dim_if_needed ( X0 ) self . y = self . fun ( X = X_all , fun_control = self . fun_control ) # TODO: Error if only nan values are returned logger . debug ( \"New y value: %s \" , self . y ) # self . counter = self . y . size if self . spot_writer is not None : writer = self . spot_writer # range goes to init_size -1 because the last value is added by update_stats(), # which always adds the last value. # Changed in 0.5.9: for j in range ( len ( self . y )): X_j = self . X [ j ] . copy () y_j = self . y [ j ] . copy () config = { self . var_name [ i ]: X_j [ i ] for i in range ( self . k )} writer . add_hparams ( config , { \"spot_y\" : y_j }) writer . flush () # self . X , self . y = remove_nan ( self . X , self . y ) # self.update_stats() moved to run()! # changed in 0.5.9: self . update_stats () # (S-4): Imputation: # Not implemented yet. # (S-11) Surrogate Fit: self . fit_surrogate () def should_continue ( self , timeout_start ): return ( self . counter < self . fun_evals ) and ( time . time () < timeout_start + self . max_time * 60 ) def update_design ( self ): # OCBA (only if noise) X_ocba = self . get_X_ocba () # (S-15) Compile Surrogate Results: X0 = self . get_new_X0 () # (S-18): Evaluating New Solutions: X0 = self . append_X_ocba ( X_ocba , X0 ) X_all = self . to_all_dim_if_needed ( X0 ) y0 = self . fun ( X = X_all , fun_control = self . fun_control ) X0 , y0 = remove_nan ( X0 , y0 ) # Append New Solutions: self . X = np . append ( self . X , X0 , axis = 0 ) self . y = np . append ( self . y , y0 ) def fit_surrogate ( self ): self . surrogate . fit ( self . X , self . y ) if self . show_models : self . plot_model () def show_progress_if_needed ( self , timeout_start ): if not self . show_progress : return if isfinite ( self . fun_evals ): progress_bar ( progress = self . counter / self . fun_evals , y = self . min_y ) else : progress_bar ( progress = ( time . time () - timeout_start ) / ( self . max_time * 60 ), y = self . min_y ) def generate_design ( self , size , repeats , lower , upper ): return self . design . scipy_lhd ( n = size , repeats = repeats , lower = lower , upper = upper ) def update_stats ( self ): \"\"\" Update the following stats: 1. `min_y` 2. `min_X` 3. `counter` If `noise` is `True`, additionally the following stats are computed: 1. `mean_X` 2. `mean_y` 3. `min_mean_y` 4. `min_mean_X`. \"\"\" self . min_y = min ( self . y ) self . min_X = self . X [ argmin ( self . y )] self . counter = self . y . size # Update aggregated x and y values (if noise): if self . noise : Z = aggregate_mean_var ( X = self . X , y = self . y ) self . mean_X = Z [ 0 ] self . mean_y = Z [ 1 ] self . var_y = Z [ 2 ] # X value of the best mean y value so far: self . min_mean_X = self . mean_X [ argmin ( self . mean_y )] # variance of the best mean y value so far: self . min_var_y = self . var_y [ argmin ( self . mean_y )] # best mean y value so far: self . min_mean_y = self . mean_y [ argmin ( self . mean_y )] def update_writer ( self ): if self . spot_writer is not None : writer = self . spot_writer # get the last y value: y_last = self . y [ - 1 ] . copy () if self . noise is False : y_min = self . min_y . copy () X_min = self . min_X . copy () # y_min: best y value so far # y_last: last y value, can be worse than y_min writer . add_scalars ( \"spot_y\" , { \"min\" : y_min , \"last\" : y_last }, self . counter ) # X_min: X value of the best y value so far writer . add_scalars ( \"spot_X\" , { f \"X_ { i } \" : X_min [ i ] for i in range ( self . k )}, self . counter ) else : # get the last n y values: y_last_n = self . y [ - self . fun_repeats :] . copy () # y_min_mean: best mean y value so far y_min_mean = self . min_mean_y . copy () # X_min_mean: X value of the best mean y value so far X_min_mean = self . min_mean_X . copy () # y_min_var: variance of the min y value so far y_min_var = self . min_var_y . copy () writer . add_scalar ( \"spot_y_min_var\" , y_min_var , self . counter ) # y_min_mean: best mean y value so far (see above) writer . add_scalar ( \"spot_y\" , y_min_mean , self . counter ) # last n y values (noisy): writer . add_scalars ( \"spot_y\" , { f \"y_last_n { i } \" : y_last_n [ i ] for i in range ( self . fun_repeats )}, self . counter ) # X_min_mean: X value of the best mean y value so far (see above) writer . add_scalars ( \"spot_X_noise\" , { f \"X_min_mean { i } \" : X_min_mean [ i ] for i in range ( self . k )}, self . counter ) # get last value of self.X and convert to dict. take the values from self.var_name as keys: X_last = self . X [ - 1 ] . copy () config = { self . var_name [ i ]: X_last [ i ] for i in range ( self . k )} # hyperparameters X and value y of the last configuration: writer . add_hparams ( config , { \"spot_y\" : y_last }) writer . flush () def suggest_new_X_old ( self ): \"\"\" Compute `n_points` new infill points in natural units. The optimizer searches in the ranges from `lower_j` to `upper_j`. The method `infill()` is used as the objective function. Returns: (numpy.ndarray): `n_points` infill points in natural units, each of dim k Note: This is step (S-14a) in [bart21i]. \"\"\" # (S-14a) Optimization on the surrogate: new_X = np . zeros ([ self . n_points , self . k ], dtype = float ) optimizer_name = self . optimizer . __name__ for i in range ( self . n_points ): if optimizer_name == \"dual_annealing\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) elif optimizer_name == \"differential_evolution\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds , maxiter = self . optimizer_control [ \"max_iter\" ], seed = self . optimizer_control [ \"seed\" ], # popsize=10, # updating=\"deferred\" ) elif optimizer_name == \"direct\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds , eps = 1e-2 ) elif optimizer_name == \"shgo\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) elif optimizer_name == \"basinhopping\" : result = self . optimizer ( func = self . infill , x0 = self . min_X ) else : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) new_X [ i ][:] = result . x return new_X def suggest_new_X ( self ): \"\"\" Compute `n_points` new infill points in natural units. The optimizer searches in the ranges from `lower_j` to `upper_j`. The method `infill()` is used as the objective function. Returns: (numpy.ndarray): `n_points` infill points in natural units, each of dim k Note: This is step (S-14a) in [bart21i]. \"\"\" # (S-14a) Optimization on the surrogate: new_X = np . zeros ([ self . n_points , self . k ], dtype = float ) optimizer_name = self . optimizer . __name__ optimizers = { \"dual_annealing\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), \"differential_evolution\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds , maxiter = self . optimizer_control [ \"max_iter\" ], seed = self . optimizer_control [ \"seed\" ], ), \"direct\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds , eps = 1e-2 ), \"shgo\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), \"basinhopping\" : lambda : self . optimizer ( func = self . infill , x0 = self . min_X ), \"default\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), } for i in range ( self . n_points ): result = optimizers . get ( optimizer_name , optimizers [ \"default\" ])() new_X [ i ][:] = result . x return new_X def infill ( self , x ): \"\"\" Infill (acquisition) function. Evaluates one point on the surrogate via `surrogate.predict(x.reshape(1,-1))`, if `sklearn` surrogates are used or `surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)` if the internal surrogate `kriging` is selected. This method is passed to the optimizer in `suggest_new_X`, i.e., the optimizer is called via `self.optimizer(func=self.infill)`. Args: x (array): point in natural units with shape `(1, dim)`. Returns: (numpy.ndarray): value based on infill criterion, e.g., `\"ei\"`. Shape `(1,)`. The objective function value `y` that is used as a base value for the infill criterion is calculated in natural units. Note: This is step (S-12) in [bart21i]. \"\"\" # Reshape x to have shape (1, -1) because the predict method expects a 2D array x_reshaped = x . reshape ( 1 , - 1 ) if isinstance ( self . surrogate , Kriging ): return self . surrogate . predict ( x_reshaped , return_val = self . infill_criterion ) else : return self . surrogate . predict ( x_reshaped ) def plot_progress ( self , show = True , log_x = False , log_y = False , filename = \"plot.png\" , style = [ \"ko\" , \"k\" , \"ro-\" ], dpi = 300 ) -> None : \"\"\"Plot the progress of the hyperparameter tuning (optimization). Args: show (bool): Show the plot. log_x (bool): Use logarithmic scale for x-axis. log_y (bool): Use logarithmic scale for y-axis. filename (str): Filename to save the plot. style (list): Style of the plot. Default: ['k', 'ro-'], i.e., the initial points are plotted as a black line and the subsequent points as red dots connected by a line. Returns: None \"\"\" fig = pylab . figure ( figsize = ( 9 , 6 )) s_y = pd . Series ( self . y ) s_c = s_y . cummin () n_init = self . design_control [ \"init_size\" ] * self . design_control [ \"repeats\" ] ax = fig . add_subplot ( 211 ) ax . plot ( range ( 1 , n_init + 1 ), s_y [: n_init ], style [ 0 ], range ( 1 , n_init + 2 ), [ s_c [: n_init ] . min ()] * ( n_init + 1 ), style [ 1 ], range ( n_init + 1 , len ( s_c ) + 1 ), s_c [ n_init :], style [ 2 ], ) if log_x : ax . set_xscale ( \"log\" ) if log_y : ax . set_yscale ( \"log\" ) if filename is not None : pylab . savefig ( filename , dpi = dpi , bbox_inches = \"tight\" ) if show : pylab . show () def plot_model ( self , y_min = None , y_max = None ): \"\"\" Plot the model fit for 1-dim objective functions. Args: y_min (float, optional): y range, lower bound. y_max (float, optional): y range, upper bound. \"\"\" if self . k == 1 : X_test = np . linspace ( self . lower [ 0 ], self . upper [ 0 ], 100 ) y_test = self . fun ( X = X_test . reshape ( - 1 , 1 ), fun_control = self . fun_control ) if isinstance ( self . surrogate , Kriging ): y_hat = self . surrogate . predict ( X_test [:, np . newaxis ], return_val = \"y\" ) else : y_hat = self . surrogate . predict ( X_test [:, np . newaxis ]) plt . plot ( X_test , y_hat , label = \"Model\" ) plt . plot ( X_test , y_test , label = \"True function\" ) plt . scatter ( self . X , self . y , edgecolor = \"b\" , s = 20 , label = \"Samples\" ) plt . scatter ( self . X [ - 1 ], self . y [ - 1 ], edgecolor = \"r\" , s = 30 , label = \"Last Sample\" ) if self . noise : plt . scatter ( self . min_mean_X , self . min_mean_y , edgecolor = \"g\" , s = 30 , label = \"Best Sample (mean)\" ) else : plt . scatter ( self . min_X , self . min_y , edgecolor = \"g\" , s = 30 , label = \"Best Sample\" ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) plt . xlim (( self . lower [ 0 ], self . upper [ 0 ])) if y_min is None : y_min = min ( min ( self . y ), min ( y_test )) if y_max is None : y_max = max ( max ( self . y ), max ( y_test )) plt . ylim (( y_min , y_max )) plt . legend ( loc = \"best\" ) # plt.title(self.surrogate.__class__.__name__ + \". \" + str(self.counter) + \": \" + str(self.min_y)) if self . noise : plt . title ( str ( self . counter ) + \". y (noise): \" + str ( np . round ( self . min_y , 6 )) + \" y mean: \" + str ( np . round ( self . min_mean_y , 6 )) ) else : plt . title ( str ( self . counter ) + \". y: \" + str ( np . round ( self . min_y , 6 ))) plt . show () def print_results ( self , print_screen = True ) -> list [ str ]: \"\"\"Print results from the run: 1. min y 2. min X If `noise == True`, additionally the following values are printed: 3. min mean y 4. min mean X Args: print_screen (bool, optional): print results to screen Returns: output (list): list of results \"\"\" output = [] if print_screen : print ( f \"min y: { self . min_y } \" ) res = self . to_all_dim ( self . min_X . reshape ( 1 , - 1 )) for i in range ( res . shape [ 1 ]): var_name = \"x\" + str ( i ) if self . all_var_name is None else self . all_var_name [ i ] if print_screen : print ( var_name + \":\" , res [ 0 ][ i ]) output . append ([ var_name , res [ 0 ][ i ]]) if self . noise : res = self . to_all_dim ( self . min_mean_X . reshape ( 1 , - 1 )) if print_screen : print ( f \"min mean y: { self . min_mean_y } \" ) for i in range ( res . shape [ 1 ]): var_name = \"x\" + str ( i ) if self . all_var_name is None else self . all_var_name [ i ] if print_screen : print ( var_name + \":\" , res [ 0 ][ i ]) output . append ([ var_name , res [ 0 ][ i ]]) return output def chg ( self , x , y , z0 , i , j ): \"\"\" Change the values of elements at indices `i` and `j` in the array `z0` to `x` and `y`, respectively. Args: x (int or float): The new value for the element at index `i`. y (int or float): The new value for the element at index `j`. z0 (list or numpy.ndarray): The array to be modified. i (int): The index of the element to be changed to `x`. j (int): The index of the element to be changed to `y`. Returns: (list) or (numpy.ndarray): The modified array. Examples: >>> z0 = [1, 2, 3] >>> chg(4, 5, z0, 0, 2) [4, 2, 5] \"\"\" z0 [ i ] = x z0 [ j ] = y return z0 def plot_contour ( self , i = 0 , j = 1 , min_z = None , max_z = None , show = True , filename = None , n_grid = 25 , contour_levels = 10 , dpi = 200 ) -> None : \"\"\"Plot the contour of any dimension. Args: i (int): the first dimension j (int): the second dimension min_z (float): the minimum value of z max_z (float): the maximum value of z show (bool): show the plot filename (str): save the plot to a file n_grid (int): number of grid points contour_levels (int): number of contour levels Returns: None \"\"\" fig = pylab . figure ( figsize = ( 9 , 6 )) # lower and upper x = np . linspace ( self . lower [ i ], self . upper [ i ], num = n_grid ) y = np . linspace ( self . lower [ j ], self . upper [ j ], num = n_grid ) X , Y = meshgrid ( x , y ) # Predict based on the optimized results z0 = np . mean ( np . array ([ self . lower , self . upper ]), axis = 0 ) zz = array ([ self . surrogate . predict ( array ([ self . chg ( x , y , z0 , i , j )])) for x , y in zip ( ravel ( X ), ravel ( Y ))]) zs = zz [:, 0 ] Z = zs . reshape ( X . shape ) if min_z is None : min_z = np . min ( Z ) if max_z is None : max_z = np . max ( Z ) ax = fig . add_subplot ( 221 ) # plot predicted values: plt . contourf ( X , Y , Z , contour_levels , zorder = 1 , cmap = \"jet\" , vmin = min_z , vmax = max_z ) if self . var_name is None : plt . xlabel ( \"x\" + str ( i )) plt . ylabel ( \"x\" + str ( j )) else : plt . xlabel ( \"x\" + str ( i ) + \": \" + self . var_name [ i ]) plt . ylabel ( \"x\" + str ( j ) + \": \" + self . var_name [ j ]) plt . title ( \"Surrogate\" ) pylab . colorbar () ax = fig . add_subplot ( 222 , projection = \"3d\" ) ax . plot_surface ( X , Y , Z , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" , vmin = min_z , vmax = max_z ) if self . var_name is None : plt . xlabel ( \"x\" + str ( i )) plt . ylabel ( \"x\" + str ( j )) else : plt . xlabel ( \"x\" + str ( i ) + \": \" + self . var_name [ i ]) plt . ylabel ( \"x\" + str ( j ) + \": \" + self . var_name [ j ]) if filename : pylab . savefig ( filename , bbox_inches = \"tight\" , dpi = dpi , pad_inches = 0 ), if show : pylab . show () def plot_important_hyperparameter_contour ( self , threshold = 0.025 , filename = None ): impo = self . print_importance ( threshold = threshold , print_screen = True ) var_plots = [ i for i , x in enumerate ( impo ) if x [ 1 ] > threshold ] min_z = min ( self . y ) max_z = max ( self . y ) for i in var_plots : for j in var_plots : if j > i : if filename is not None : filename_full = filename + \"_contour_\" + str ( i ) + \"_\" + str ( j ) + \".png\" else : filename_full = None self . plot_contour ( i = i , j = j , min_z = min_z , max_z = max_z , filename = filename_full ) def get_importance ( self ) -> list : \"\"\"Get importance of each variable and return the results as a list. Returns: output (list): list of results \"\"\" if self . surrogate . n_theta > 1 and self . var_name is not None : output = [ 0 ] * len ( self . all_var_name ) theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) ind = find_indices ( A = self . var_name , B = self . all_var_name ) j = 0 for i in ind : output [ i ] = imp [ j ] j = j + 1 return output else : print ( \"Importance requires more than one theta values (n_theta>1).\" ) def print_importance ( self , threshold = 0.1 , print_screen = True ) -> list : \"\"\"Print importance of each variable and return the results as a list. Args: threshold (float): threshold for printing print_screen (boolean): if `True`, values are also printed on the screen. Default is `True`. Returns: output (list): list of results \"\"\" output = [] if self . surrogate . n_theta > 1 : theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) # imp = imp[imp >= threshold] if self . var_name is None : for i in range ( len ( imp )): if imp [ i ] >= threshold : if print_screen : print ( \"x\" , i , \": \" , imp [ i ]) output . append ( \"x\" + str ( i ) + \": \" + str ( imp [ i ])) else : var_name = [ self . var_name [ i ] for i in range ( len ( imp ))] for i in range ( len ( imp )): if imp [ i ] >= threshold : if print_screen : print ( var_name [ i ] + \": \" , imp [ i ]) output . append ([ var_name [ i ], imp [ i ]]) else : print ( \"Importance requires more than one theta values (n_theta>1).\" ) return output def plot_importance ( self , threshold = 0.1 , filename = None , dpi = 300 ) -> None : \"\"\"Plot the importance of each variable. Args: threshold (float): The threshold of the importance. filename (str): The filename of the plot. Returns: None \"\"\" if self . surrogate . n_theta > 1 : theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) idx = np . where ( imp > threshold )[ 0 ] if self . var_name is None : plt . bar ( range ( len ( imp [ idx ])), imp [ idx ]) plt . xticks ( range ( len ( imp [ idx ])), [ \"x\" + str ( i ) for i in idx ]) else : var_name = [ self . var_name [ i ] for i in idx ] plt . bar ( range ( len ( imp [ idx ])), imp [ idx ]) plt . xticks ( range ( len ( imp [ idx ])), var_name ) if filename is not None : plt . savefig ( filename , bbox_inches = \"tight\" , dpi = dpi ) plt . show () def parallel_plot ( self ): X = self . X y = self . y df = pd . DataFrame ( np . concatenate (( X , y . reshape ( - 1 , 1 )), axis = 1 ), columns = self . var_name + [ \"y\" ]) fig = go . Figure ( data = go . Parcoords ( line = dict ( color = df [ \"y\" ], colorscale = \"Jet\" , showscale = True , cmin = min ( df [ \"y\" ]), cmax = max ( df [ \"y\" ])), dimensions = list ( [ dict ( range = [ min ( df . iloc [:, i ]), max ( df . iloc [:, i ])], label = df . columns [ i ], values = df . iloc [:, i ]) for i in range ( len ( df . columns ) - 1 ) ] ), ) ) fig . show ()","title":"Spot"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.chg","text":"Change the values of elements at indices i and j in the array z0 to x and y , respectively. Parameters: Name Type Description Default x int or float The new value for the element at index i . required y int or float The new value for the element at index j . required z0 list or numpy.ndarray The array to be modified. required i int The index of the element to be changed to x . required j int The index of the element to be changed to y . required Returns: Type Description list) or (numpy.ndarray The modified array. Examples: >>> z0 = [ 1 , 2 , 3 ] >>> chg ( 4 , 5 , z0 , 0 , 2 ) [4, 2, 5] Source code in spotPython/spot/spot.py 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 def chg ( self , x , y , z0 , i , j ): \"\"\" Change the values of elements at indices `i` and `j` in the array `z0` to `x` and `y`, respectively. Args: x (int or float): The new value for the element at index `i`. y (int or float): The new value for the element at index `j`. z0 (list or numpy.ndarray): The array to be modified. i (int): The index of the element to be changed to `x`. j (int): The index of the element to be changed to `y`. Returns: (list) or (numpy.ndarray): The modified array. Examples: >>> z0 = [1, 2, 3] >>> chg(4, 5, z0, 0, 2) [4, 2, 5] \"\"\" z0 [ i ] = x z0 [ j ] = y return z0","title":"chg()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.get_importance","text":"Get importance of each variable and return the results as a list. Returns: Name Type Description output list list of results Source code in spotPython/spot/spot.py 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 def get_importance ( self ) -> list : \"\"\"Get importance of each variable and return the results as a list. Returns: output (list): list of results \"\"\" if self . surrogate . n_theta > 1 and self . var_name is not None : output = [ 0 ] * len ( self . all_var_name ) theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) ind = find_indices ( A = self . var_name , B = self . all_var_name ) j = 0 for i in ind : output [ i ] = imp [ j ] j = j + 1 return output else : print ( \"Importance requires more than one theta values (n_theta>1).\" )","title":"get_importance()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.infill","text":"Infill (acquisition) function. Evaluates one point on the surrogate via surrogate.predict(x.reshape(1,-1)) , if sklearn surrogates are used or surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion) if the internal surrogate kriging is selected. This method is passed to the optimizer in suggest_new_X , i.e., the optimizer is called via self.optimizer(func=self.infill) . Parameters: Name Type Description Default x array point in natural units with shape (1, dim) . required Returns: Type Description numpy . ndarray value based on infill criterion, e.g., \"ei\" . Shape (1,) . The objective function value y that is used as a base value for the infill criterion is calculated in natural units. Note This is step (S-12) in [bart21i]. Source code in spotPython/spot/spot.py 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 def infill ( self , x ): \"\"\" Infill (acquisition) function. Evaluates one point on the surrogate via `surrogate.predict(x.reshape(1,-1))`, if `sklearn` surrogates are used or `surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)` if the internal surrogate `kriging` is selected. This method is passed to the optimizer in `suggest_new_X`, i.e., the optimizer is called via `self.optimizer(func=self.infill)`. Args: x (array): point in natural units with shape `(1, dim)`. Returns: (numpy.ndarray): value based on infill criterion, e.g., `\"ei\"`. Shape `(1,)`. The objective function value `y` that is used as a base value for the infill criterion is calculated in natural units. Note: This is step (S-12) in [bart21i]. \"\"\" # Reshape x to have shape (1, -1) because the predict method expects a 2D array x_reshaped = x . reshape ( 1 , - 1 ) if isinstance ( self . surrogate , Kriging ): return self . surrogate . predict ( x_reshaped , return_val = self . infill_criterion ) else : return self . surrogate . predict ( x_reshaped )","title":"infill()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.plot_contour","text":"Plot the contour of any dimension. Parameters: Name Type Description Default i int the first dimension 0 j int the second dimension 1 min_z float the minimum value of z None max_z float the maximum value of z None show bool show the plot True filename str save the plot to a file None n_grid int number of grid points 25 contour_levels int number of contour levels 10 Returns: Type Description None None Source code in spotPython/spot/spot.py 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 def plot_contour ( self , i = 0 , j = 1 , min_z = None , max_z = None , show = True , filename = None , n_grid = 25 , contour_levels = 10 , dpi = 200 ) -> None : \"\"\"Plot the contour of any dimension. Args: i (int): the first dimension j (int): the second dimension min_z (float): the minimum value of z max_z (float): the maximum value of z show (bool): show the plot filename (str): save the plot to a file n_grid (int): number of grid points contour_levels (int): number of contour levels Returns: None \"\"\" fig = pylab . figure ( figsize = ( 9 , 6 )) # lower and upper x = np . linspace ( self . lower [ i ], self . upper [ i ], num = n_grid ) y = np . linspace ( self . lower [ j ], self . upper [ j ], num = n_grid ) X , Y = meshgrid ( x , y ) # Predict based on the optimized results z0 = np . mean ( np . array ([ self . lower , self . upper ]), axis = 0 ) zz = array ([ self . surrogate . predict ( array ([ self . chg ( x , y , z0 , i , j )])) for x , y in zip ( ravel ( X ), ravel ( Y ))]) zs = zz [:, 0 ] Z = zs . reshape ( X . shape ) if min_z is None : min_z = np . min ( Z ) if max_z is None : max_z = np . max ( Z ) ax = fig . add_subplot ( 221 ) # plot predicted values: plt . contourf ( X , Y , Z , contour_levels , zorder = 1 , cmap = \"jet\" , vmin = min_z , vmax = max_z ) if self . var_name is None : plt . xlabel ( \"x\" + str ( i )) plt . ylabel ( \"x\" + str ( j )) else : plt . xlabel ( \"x\" + str ( i ) + \": \" + self . var_name [ i ]) plt . ylabel ( \"x\" + str ( j ) + \": \" + self . var_name [ j ]) plt . title ( \"Surrogate\" ) pylab . colorbar () ax = fig . add_subplot ( 222 , projection = \"3d\" ) ax . plot_surface ( X , Y , Z , rstride = 3 , cstride = 3 , alpha = 0.9 , cmap = \"jet\" , vmin = min_z , vmax = max_z ) if self . var_name is None : plt . xlabel ( \"x\" + str ( i )) plt . ylabel ( \"x\" + str ( j )) else : plt . xlabel ( \"x\" + str ( i ) + \": \" + self . var_name [ i ]) plt . ylabel ( \"x\" + str ( j ) + \": \" + self . var_name [ j ]) if filename : pylab . savefig ( filename , bbox_inches = \"tight\" , dpi = dpi , pad_inches = 0 ), if show : pylab . show ()","title":"plot_contour()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.plot_importance","text":"Plot the importance of each variable. Parameters: Name Type Description Default threshold float The threshold of the importance. 0.1 filename str The filename of the plot. None Returns: Type Description None None Source code in spotPython/spot/spot.py 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 def plot_importance ( self , threshold = 0.1 , filename = None , dpi = 300 ) -> None : \"\"\"Plot the importance of each variable. Args: threshold (float): The threshold of the importance. filename (str): The filename of the plot. Returns: None \"\"\" if self . surrogate . n_theta > 1 : theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) idx = np . where ( imp > threshold )[ 0 ] if self . var_name is None : plt . bar ( range ( len ( imp [ idx ])), imp [ idx ]) plt . xticks ( range ( len ( imp [ idx ])), [ \"x\" + str ( i ) for i in idx ]) else : var_name = [ self . var_name [ i ] for i in idx ] plt . bar ( range ( len ( imp [ idx ])), imp [ idx ]) plt . xticks ( range ( len ( imp [ idx ])), var_name ) if filename is not None : plt . savefig ( filename , bbox_inches = \"tight\" , dpi = dpi ) plt . show ()","title":"plot_importance()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.plot_model","text":"Plot the model fit for 1-dim objective functions. Parameters: Name Type Description Default y_min float y range, lower bound. None y_max float y range, upper bound. None Source code in spotPython/spot/spot.py 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 def plot_model ( self , y_min = None , y_max = None ): \"\"\" Plot the model fit for 1-dim objective functions. Args: y_min (float, optional): y range, lower bound. y_max (float, optional): y range, upper bound. \"\"\" if self . k == 1 : X_test = np . linspace ( self . lower [ 0 ], self . upper [ 0 ], 100 ) y_test = self . fun ( X = X_test . reshape ( - 1 , 1 ), fun_control = self . fun_control ) if isinstance ( self . surrogate , Kriging ): y_hat = self . surrogate . predict ( X_test [:, np . newaxis ], return_val = \"y\" ) else : y_hat = self . surrogate . predict ( X_test [:, np . newaxis ]) plt . plot ( X_test , y_hat , label = \"Model\" ) plt . plot ( X_test , y_test , label = \"True function\" ) plt . scatter ( self . X , self . y , edgecolor = \"b\" , s = 20 , label = \"Samples\" ) plt . scatter ( self . X [ - 1 ], self . y [ - 1 ], edgecolor = \"r\" , s = 30 , label = \"Last Sample\" ) if self . noise : plt . scatter ( self . min_mean_X , self . min_mean_y , edgecolor = \"g\" , s = 30 , label = \"Best Sample (mean)\" ) else : plt . scatter ( self . min_X , self . min_y , edgecolor = \"g\" , s = 30 , label = \"Best Sample\" ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) plt . xlim (( self . lower [ 0 ], self . upper [ 0 ])) if y_min is None : y_min = min ( min ( self . y ), min ( y_test )) if y_max is None : y_max = max ( max ( self . y ), max ( y_test )) plt . ylim (( y_min , y_max )) plt . legend ( loc = \"best\" ) # plt.title(self.surrogate.__class__.__name__ + \". \" + str(self.counter) + \": \" + str(self.min_y)) if self . noise : plt . title ( str ( self . counter ) + \". y (noise): \" + str ( np . round ( self . min_y , 6 )) + \" y mean: \" + str ( np . round ( self . min_mean_y , 6 )) ) else : plt . title ( str ( self . counter ) + \". y: \" + str ( np . round ( self . min_y , 6 ))) plt . show ()","title":"plot_model()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.plot_progress","text":"Plot the progress of the hyperparameter tuning (optimization). Parameters: Name Type Description Default show bool Show the plot. True log_x bool Use logarithmic scale for x-axis. False log_y bool Use logarithmic scale for y-axis. False filename str Filename to save the plot. 'plot.png' style list Style of the plot. Default: [\u2018k\u2019, \u2018ro-\u2018], i.e., the initial points are plotted as a black line and the subsequent points as red dots connected by a line. ['ko', 'k', 'ro-'] Returns: Type Description None None Source code in spotPython/spot/spot.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 def plot_progress ( self , show = True , log_x = False , log_y = False , filename = \"plot.png\" , style = [ \"ko\" , \"k\" , \"ro-\" ], dpi = 300 ) -> None : \"\"\"Plot the progress of the hyperparameter tuning (optimization). Args: show (bool): Show the plot. log_x (bool): Use logarithmic scale for x-axis. log_y (bool): Use logarithmic scale for y-axis. filename (str): Filename to save the plot. style (list): Style of the plot. Default: ['k', 'ro-'], i.e., the initial points are plotted as a black line and the subsequent points as red dots connected by a line. Returns: None \"\"\" fig = pylab . figure ( figsize = ( 9 , 6 )) s_y = pd . Series ( self . y ) s_c = s_y . cummin () n_init = self . design_control [ \"init_size\" ] * self . design_control [ \"repeats\" ] ax = fig . add_subplot ( 211 ) ax . plot ( range ( 1 , n_init + 1 ), s_y [: n_init ], style [ 0 ], range ( 1 , n_init + 2 ), [ s_c [: n_init ] . min ()] * ( n_init + 1 ), style [ 1 ], range ( n_init + 1 , len ( s_c ) + 1 ), s_c [ n_init :], style [ 2 ], ) if log_x : ax . set_xscale ( \"log\" ) if log_y : ax . set_yscale ( \"log\" ) if filename is not None : pylab . savefig ( filename , dpi = dpi , bbox_inches = \"tight\" ) if show : pylab . show ()","title":"plot_progress()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.print_importance","text":"Print importance of each variable and return the results as a list. Parameters: Name Type Description Default threshold float threshold for printing 0.1 print_screen boolean if True , values are also printed on the screen. Default is True . True Returns: Name Type Description output list list of results Source code in spotPython/spot/spot.py 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 def print_importance ( self , threshold = 0.1 , print_screen = True ) -> list : \"\"\"Print importance of each variable and return the results as a list. Args: threshold (float): threshold for printing print_screen (boolean): if `True`, values are also printed on the screen. Default is `True`. Returns: output (list): list of results \"\"\" output = [] if self . surrogate . n_theta > 1 : theta = np . power ( 10 , self . surrogate . theta ) imp = 100 * theta / np . max ( theta ) # imp = imp[imp >= threshold] if self . var_name is None : for i in range ( len ( imp )): if imp [ i ] >= threshold : if print_screen : print ( \"x\" , i , \": \" , imp [ i ]) output . append ( \"x\" + str ( i ) + \": \" + str ( imp [ i ])) else : var_name = [ self . var_name [ i ] for i in range ( len ( imp ))] for i in range ( len ( imp )): if imp [ i ] >= threshold : if print_screen : print ( var_name [ i ] + \": \" , imp [ i ]) output . append ([ var_name [ i ], imp [ i ]]) else : print ( \"Importance requires more than one theta values (n_theta>1).\" ) return output","title":"print_importance()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.print_results","text":"Print results from the run min y min X If noise == True , additionally the following values are printed: min mean y min mean X Parameters: Name Type Description Default print_screen bool print results to screen True Returns: Name Type Description output list list of results Source code in spotPython/spot/spot.py 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 def print_results ( self , print_screen = True ) -> list [ str ]: \"\"\"Print results from the run: 1. min y 2. min X If `noise == True`, additionally the following values are printed: 3. min mean y 4. min mean X Args: print_screen (bool, optional): print results to screen Returns: output (list): list of results \"\"\" output = [] if print_screen : print ( f \"min y: { self . min_y } \" ) res = self . to_all_dim ( self . min_X . reshape ( 1 , - 1 )) for i in range ( res . shape [ 1 ]): var_name = \"x\" + str ( i ) if self . all_var_name is None else self . all_var_name [ i ] if print_screen : print ( var_name + \":\" , res [ 0 ][ i ]) output . append ([ var_name , res [ 0 ][ i ]]) if self . noise : res = self . to_all_dim ( self . min_mean_X . reshape ( 1 , - 1 )) if print_screen : print ( f \"min mean y: { self . min_mean_y } \" ) for i in range ( res . shape [ 1 ]): var_name = \"x\" + str ( i ) if self . all_var_name is None else self . all_var_name [ i ] if print_screen : print ( var_name + \":\" , res [ 0 ][ i ]) output . append ([ var_name , res [ 0 ][ i ]]) return output","title":"print_results()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.suggest_new_X","text":"Compute n_points new infill points in natural units. The optimizer searches in the ranges from lower_j to upper_j . The method infill() is used as the objective function. Returns: Type Description numpy . ndarray n_points infill points in natural units, each of dim k Note This is step (S-14a) in [bart21i]. Source code in spotPython/spot/spot.py 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 def suggest_new_X ( self ): \"\"\" Compute `n_points` new infill points in natural units. The optimizer searches in the ranges from `lower_j` to `upper_j`. The method `infill()` is used as the objective function. Returns: (numpy.ndarray): `n_points` infill points in natural units, each of dim k Note: This is step (S-14a) in [bart21i]. \"\"\" # (S-14a) Optimization on the surrogate: new_X = np . zeros ([ self . n_points , self . k ], dtype = float ) optimizer_name = self . optimizer . __name__ optimizers = { \"dual_annealing\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), \"differential_evolution\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds , maxiter = self . optimizer_control [ \"max_iter\" ], seed = self . optimizer_control [ \"seed\" ], ), \"direct\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds , eps = 1e-2 ), \"shgo\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), \"basinhopping\" : lambda : self . optimizer ( func = self . infill , x0 = self . min_X ), \"default\" : lambda : self . optimizer ( func = self . infill , bounds = self . de_bounds ), } for i in range ( self . n_points ): result = optimizers . get ( optimizer_name , optimizers [ \"default\" ])() new_X [ i ][:] = result . x return new_X","title":"suggest_new_X()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.suggest_new_X_old","text":"Compute n_points new infill points in natural units. The optimizer searches in the ranges from lower_j to upper_j . The method infill() is used as the objective function. Returns: Type Description numpy . ndarray n_points infill points in natural units, each of dim k Note This is step (S-14a) in [bart21i]. Source code in spotPython/spot/spot.py 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 def suggest_new_X_old ( self ): \"\"\" Compute `n_points` new infill points in natural units. The optimizer searches in the ranges from `lower_j` to `upper_j`. The method `infill()` is used as the objective function. Returns: (numpy.ndarray): `n_points` infill points in natural units, each of dim k Note: This is step (S-14a) in [bart21i]. \"\"\" # (S-14a) Optimization on the surrogate: new_X = np . zeros ([ self . n_points , self . k ], dtype = float ) optimizer_name = self . optimizer . __name__ for i in range ( self . n_points ): if optimizer_name == \"dual_annealing\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) elif optimizer_name == \"differential_evolution\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds , maxiter = self . optimizer_control [ \"max_iter\" ], seed = self . optimizer_control [ \"seed\" ], # popsize=10, # updating=\"deferred\" ) elif optimizer_name == \"direct\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds , eps = 1e-2 ) elif optimizer_name == \"shgo\" : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) elif optimizer_name == \"basinhopping\" : result = self . optimizer ( func = self . infill , x0 = self . min_X ) else : result = self . optimizer ( func = self . infill , bounds = self . de_bounds ) new_X [ i ][:] = result . x return new_X","title":"suggest_new_X_old()"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.update_stats","text":"Update the following stats: 1. min_y 2. min_X 3. counter If noise is True , additionally the following stats are computed: 1. mean_X 2. mean_y 3. min_mean_y 4. min_mean_X . Source code in spotPython/spot/spot.py 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 def update_stats ( self ): \"\"\" Update the following stats: 1. `min_y` 2. `min_X` 3. `counter` If `noise` is `True`, additionally the following stats are computed: 1. `mean_X` 2. `mean_y` 3. `min_mean_y` 4. `min_mean_X`. \"\"\" self . min_y = min ( self . y ) self . min_X = self . X [ argmin ( self . y )] self . counter = self . y . size # Update aggregated x and y values (if noise): if self . noise : Z = aggregate_mean_var ( X = self . X , y = self . y ) self . mean_X = Z [ 0 ] self . mean_y = Z [ 1 ] self . var_y = Z [ 2 ] # X value of the best mean y value so far: self . min_mean_X = self . mean_X [ argmin ( self . mean_y )] # variance of the best mean y value so far: self . min_var_y = self . var_y [ argmin ( self . mean_y )] # best mean y value so far: self . min_mean_y = self . mean_y [ argmin ( self . mean_y )]","title":"update_stats()"},{"location":"reference/spotPython/torch/activation/","text":"","title":"activation"},{"location":"reference/spotPython/torch/dataframedataset/","text":"","title":"dataframedataset"},{"location":"reference/spotPython/torch/initialization/","text":"","title":"initialization"},{"location":"reference/spotPython/torch/mapk/","text":"MAPK \u00b6 Bases: torchmetrics . Metric Mean Average Precision at K (MAPK) metric. This class inherits from the Metric class of the torchmetrics library. Parameters: Name Type Description Default k int The number of top predictions to consider when calculating the metric. 10 dist_sync_on_step bool Whether to synchronize the metric states across processes during the forward pass. False Attributes: Name Type Description total torch . Tensor The cumulative sum of the metric scores across all batches. count torch . Tensor The number of batches processed. Examples: >>> from spotPython.torch.mapk import MAPK import torch mapk = MAPK(k=2) target = torch.tensor([0, 1, 2, 2]) preds = torch.tensor( [ [0.5, 0.2, 0.2], # 0 is in top 2 [0.3, 0.4, 0.2], # 1 is in top 2 [0.2, 0.4, 0.3], # 2 is in top 2 [0.7, 0.2, 0.1], # 2 isn't in top 2 ] ) mapk.update(preds, target) print(mapk.compute()) # tensor(0.6250) Source code in spotPython/torch/mapk.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class MAPK ( torchmetrics . Metric ): \"\"\" Mean Average Precision at K (MAPK) metric. This class inherits from the `Metric` class of the `torchmetrics` library. Args: k (int): The number of top predictions to consider when calculating the metric. dist_sync_on_step (bool): Whether to synchronize the metric states across processes during the forward pass. Attributes: total (torch.Tensor): The cumulative sum of the metric scores across all batches. count (torch.Tensor): The number of batches processed. Examples: >>> from spotPython.torch.mapk import MAPK import torch mapk = MAPK(k=2) target = torch.tensor([0, 1, 2, 2]) preds = torch.tensor( [ [0.5, 0.2, 0.2], # 0 is in top 2 [0.3, 0.4, 0.2], # 1 is in top 2 [0.2, 0.4, 0.3], # 2 is in top 2 [0.7, 0.2, 0.1], # 2 isn't in top 2 ] ) mapk.update(preds, target) print(mapk.compute()) # tensor(0.6250) \"\"\" def __init__ ( self , k = 10 , dist_sync_on_step = False ): super () . __init__ ( dist_sync_on_step = dist_sync_on_step ) self . k = k self . add_state ( \"total\" , default = torch . tensor ( 0.0 ), dist_reduce_fx = \"sum\" ) self . add_state ( \"count\" , default = torch . tensor ( 0 ), dist_reduce_fx = \"sum\" ) def update ( self , predicted : torch . Tensor , actual : torch . Tensor ): \"\"\" Update the state variables with a new batch of data. Args: predicted (torch.Tensor): A 2D tensor containing the predicted scores for each class. actual (torch.Tensor): A 1D tensor containing the ground truth labels. Returns: (NoneType): None Examples: >>> from spotPython.torch.mapk import MAPK >>> import torch >>> mapk = MAPK(k=2) >>> target = torch.tensor([0, 1, 2, 2]) >>> preds = torch.tensor( ... [ ... [0.5, 0.2, 0.2], # 0 is in top 2 ... [0.3, 0.4, 0.2], # 1 is in top 2 ... [0.2, 0.4, 0.3], # 2 is in top 2 ... [0.7, 0.2, 0.1], # 2 isn't in top 2 ... ] ... ) >>> mapk.update(preds, target) >>> print(mapk.compute()) # tensor(0.6250) Raises: AssertionError: If the actual tensor is not 1D or the predicted tensor is not 2D. AssertionError: If the number of elements in the actual and predicted tensors are not equal. \"\"\" assert len ( actual . shape ) == 1 , \"actual must be a 1D tensor\" assert len ( predicted . shape ) == 2 , \"predicted must be a 2D tensor\" assert actual . shape [ 0 ] == predicted . shape [ 0 ], \"actual and predicted must have the same number of elements\" # Convert actual to list of lists actual = actual . tolist () actual = [[ a ] for a in actual ] # Convert predicted to list of lists of indices sorted by confidence score _ , predicted = predicted . topk ( k = self . k , dim = 1 ) predicted = predicted . tolist () # Code modified according to: \"Inplace update to inference tensor outside InferenceMode # is not allowed. You can make a clone to get a normal tensor before doing inplace update.\" score = np . mean ([ self . apk ( p , a , self . k ) for p , a in zip ( predicted , actual )]) self . total = self . total + score self . count = self . count + 1 def compute ( self ) -> float : \"\"\" Compute the mean average precision at k. Args: self (MAPK): The current instance of the class. Returns: (float): The mean average precision at k. Examples: >>> evaluator = Evaluator() >>> evaluator.total = 3.0 >>> evaluator.count = 2 >>> evaluator.compute() 1.5 \"\"\" return self . total / self . count @staticmethod def apk ( predicted : List [ int ], actual : List [ int ], k : int = 10 ) -> float : \"\"\" Calculate the average precision at k for a single pair of actual and predicted labels. Args: predicted (list): A list of predicted labels. actual (list): A list of ground truth labels. k (int): The number of top predictions to consider. Returns: float: The average precision at k. Examples: >>> Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3) 0.8888888888888888 \"\"\" if not actual : return 0.0 if len ( predicted ) > k : predicted = predicted [: k ] score = 0.0 num_hits = 0.0 for i , p in enumerate ( predicted ): if p in actual and p not in predicted [: i ]: num_hits += 1.0 score += num_hits / ( i + 1.0 ) return score / min ( len ( actual ), k ) apk ( predicted , actual , k = 10 ) staticmethod \u00b6 Calculate the average precision at k for a single pair of actual and predicted labels. Parameters: Name Type Description Default predicted list A list of predicted labels. required actual list A list of ground truth labels. required k int The number of top predictions to consider. 10 Returns: Name Type Description float float The average precision at k. Examples: >>> Evaluator . apk ([ 1 , 3 , 2 , 4 ], [ 1 , 2 , 3 ], 3 ) 0.8888888888888888 Source code in spotPython/torch/mapk.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 @staticmethod def apk ( predicted : List [ int ], actual : List [ int ], k : int = 10 ) -> float : \"\"\" Calculate the average precision at k for a single pair of actual and predicted labels. Args: predicted (list): A list of predicted labels. actual (list): A list of ground truth labels. k (int): The number of top predictions to consider. Returns: float: The average precision at k. Examples: >>> Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3) 0.8888888888888888 \"\"\" if not actual : return 0.0 if len ( predicted ) > k : predicted = predicted [: k ] score = 0.0 num_hits = 0.0 for i , p in enumerate ( predicted ): if p in actual and p not in predicted [: i ]: num_hits += 1.0 score += num_hits / ( i + 1.0 ) return score / min ( len ( actual ), k ) compute () \u00b6 Compute the mean average precision at k. Parameters: Name Type Description Default self MAPK The current instance of the class. required Returns: Type Description float The mean average precision at k. Examples: >>> evaluator = Evaluator () >>> evaluator . total = 3.0 >>> evaluator . count = 2 >>> evaluator . compute () 1.5 Source code in spotPython/torch/mapk.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def compute ( self ) -> float : \"\"\" Compute the mean average precision at k. Args: self (MAPK): The current instance of the class. Returns: (float): The mean average precision at k. Examples: >>> evaluator = Evaluator() >>> evaluator.total = 3.0 >>> evaluator.count = 2 >>> evaluator.compute() 1.5 \"\"\" return self . total / self . count update ( predicted , actual ) \u00b6 Update the state variables with a new batch of data. Parameters: Name Type Description Default predicted torch . Tensor A 2D tensor containing the predicted scores for each class. required actual torch . Tensor A 1D tensor containing the ground truth labels. required Returns: Type Description NoneType None Examples: >>> from spotPython.torch.mapk import MAPK >>> import torch >>> mapk = MAPK ( k = 2 ) >>> target = torch . tensor ([ 0 , 1 , 2 , 2 ]) >>> preds = torch . tensor ( ... [ ... [ 0.5 , 0.2 , 0.2 ], # 0 is in top 2 ... [ 0.3 , 0.4 , 0.2 ], # 1 is in top 2 ... [ 0.2 , 0.4 , 0.3 ], # 2 is in top 2 ... [ 0.7 , 0.2 , 0.1 ], # 2 isn't in top 2 ... ] ... ) >>> mapk . update ( preds , target ) >>> print ( mapk . compute ()) # tensor(0.6250) Raises: Type Description AssertionError If the actual tensor is not 1D or the predicted tensor is not 2D. AssertionError If the number of elements in the actual and predicted tensors are not equal. Source code in spotPython/torch/mapk.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def update ( self , predicted : torch . Tensor , actual : torch . Tensor ): \"\"\" Update the state variables with a new batch of data. Args: predicted (torch.Tensor): A 2D tensor containing the predicted scores for each class. actual (torch.Tensor): A 1D tensor containing the ground truth labels. Returns: (NoneType): None Examples: >>> from spotPython.torch.mapk import MAPK >>> import torch >>> mapk = MAPK(k=2) >>> target = torch.tensor([0, 1, 2, 2]) >>> preds = torch.tensor( ... [ ... [0.5, 0.2, 0.2], # 0 is in top 2 ... [0.3, 0.4, 0.2], # 1 is in top 2 ... [0.2, 0.4, 0.3], # 2 is in top 2 ... [0.7, 0.2, 0.1], # 2 isn't in top 2 ... ] ... ) >>> mapk.update(preds, target) >>> print(mapk.compute()) # tensor(0.6250) Raises: AssertionError: If the actual tensor is not 1D or the predicted tensor is not 2D. AssertionError: If the number of elements in the actual and predicted tensors are not equal. \"\"\" assert len ( actual . shape ) == 1 , \"actual must be a 1D tensor\" assert len ( predicted . shape ) == 2 , \"predicted must be a 2D tensor\" assert actual . shape [ 0 ] == predicted . shape [ 0 ], \"actual and predicted must have the same number of elements\" # Convert actual to list of lists actual = actual . tolist () actual = [[ a ] for a in actual ] # Convert predicted to list of lists of indices sorted by confidence score _ , predicted = predicted . topk ( k = self . k , dim = 1 ) predicted = predicted . tolist () # Code modified according to: \"Inplace update to inference tensor outside InferenceMode # is not allowed. You can make a clone to get a normal tensor before doing inplace update.\" score = np . mean ([ self . apk ( p , a , self . k ) for p , a in zip ( predicted , actual )]) self . total = self . total + score self . count = self . count + 1","title":"mapk"},{"location":"reference/spotPython/torch/mapk/#spotPython.torch.mapk.MAPK","text":"Bases: torchmetrics . Metric Mean Average Precision at K (MAPK) metric. This class inherits from the Metric class of the torchmetrics library. Parameters: Name Type Description Default k int The number of top predictions to consider when calculating the metric. 10 dist_sync_on_step bool Whether to synchronize the metric states across processes during the forward pass. False Attributes: Name Type Description total torch . Tensor The cumulative sum of the metric scores across all batches. count torch . Tensor The number of batches processed. Examples: >>> from spotPython.torch.mapk import MAPK import torch mapk = MAPK(k=2) target = torch.tensor([0, 1, 2, 2]) preds = torch.tensor( [ [0.5, 0.2, 0.2], # 0 is in top 2 [0.3, 0.4, 0.2], # 1 is in top 2 [0.2, 0.4, 0.3], # 2 is in top 2 [0.7, 0.2, 0.1], # 2 isn't in top 2 ] ) mapk.update(preds, target) print(mapk.compute()) # tensor(0.6250) Source code in spotPython/torch/mapk.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class MAPK ( torchmetrics . Metric ): \"\"\" Mean Average Precision at K (MAPK) metric. This class inherits from the `Metric` class of the `torchmetrics` library. Args: k (int): The number of top predictions to consider when calculating the metric. dist_sync_on_step (bool): Whether to synchronize the metric states across processes during the forward pass. Attributes: total (torch.Tensor): The cumulative sum of the metric scores across all batches. count (torch.Tensor): The number of batches processed. Examples: >>> from spotPython.torch.mapk import MAPK import torch mapk = MAPK(k=2) target = torch.tensor([0, 1, 2, 2]) preds = torch.tensor( [ [0.5, 0.2, 0.2], # 0 is in top 2 [0.3, 0.4, 0.2], # 1 is in top 2 [0.2, 0.4, 0.3], # 2 is in top 2 [0.7, 0.2, 0.1], # 2 isn't in top 2 ] ) mapk.update(preds, target) print(mapk.compute()) # tensor(0.6250) \"\"\" def __init__ ( self , k = 10 , dist_sync_on_step = False ): super () . __init__ ( dist_sync_on_step = dist_sync_on_step ) self . k = k self . add_state ( \"total\" , default = torch . tensor ( 0.0 ), dist_reduce_fx = \"sum\" ) self . add_state ( \"count\" , default = torch . tensor ( 0 ), dist_reduce_fx = \"sum\" ) def update ( self , predicted : torch . Tensor , actual : torch . Tensor ): \"\"\" Update the state variables with a new batch of data. Args: predicted (torch.Tensor): A 2D tensor containing the predicted scores for each class. actual (torch.Tensor): A 1D tensor containing the ground truth labels. Returns: (NoneType): None Examples: >>> from spotPython.torch.mapk import MAPK >>> import torch >>> mapk = MAPK(k=2) >>> target = torch.tensor([0, 1, 2, 2]) >>> preds = torch.tensor( ... [ ... [0.5, 0.2, 0.2], # 0 is in top 2 ... [0.3, 0.4, 0.2], # 1 is in top 2 ... [0.2, 0.4, 0.3], # 2 is in top 2 ... [0.7, 0.2, 0.1], # 2 isn't in top 2 ... ] ... ) >>> mapk.update(preds, target) >>> print(mapk.compute()) # tensor(0.6250) Raises: AssertionError: If the actual tensor is not 1D or the predicted tensor is not 2D. AssertionError: If the number of elements in the actual and predicted tensors are not equal. \"\"\" assert len ( actual . shape ) == 1 , \"actual must be a 1D tensor\" assert len ( predicted . shape ) == 2 , \"predicted must be a 2D tensor\" assert actual . shape [ 0 ] == predicted . shape [ 0 ], \"actual and predicted must have the same number of elements\" # Convert actual to list of lists actual = actual . tolist () actual = [[ a ] for a in actual ] # Convert predicted to list of lists of indices sorted by confidence score _ , predicted = predicted . topk ( k = self . k , dim = 1 ) predicted = predicted . tolist () # Code modified according to: \"Inplace update to inference tensor outside InferenceMode # is not allowed. You can make a clone to get a normal tensor before doing inplace update.\" score = np . mean ([ self . apk ( p , a , self . k ) for p , a in zip ( predicted , actual )]) self . total = self . total + score self . count = self . count + 1 def compute ( self ) -> float : \"\"\" Compute the mean average precision at k. Args: self (MAPK): The current instance of the class. Returns: (float): The mean average precision at k. Examples: >>> evaluator = Evaluator() >>> evaluator.total = 3.0 >>> evaluator.count = 2 >>> evaluator.compute() 1.5 \"\"\" return self . total / self . count @staticmethod def apk ( predicted : List [ int ], actual : List [ int ], k : int = 10 ) -> float : \"\"\" Calculate the average precision at k for a single pair of actual and predicted labels. Args: predicted (list): A list of predicted labels. actual (list): A list of ground truth labels. k (int): The number of top predictions to consider. Returns: float: The average precision at k. Examples: >>> Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3) 0.8888888888888888 \"\"\" if not actual : return 0.0 if len ( predicted ) > k : predicted = predicted [: k ] score = 0.0 num_hits = 0.0 for i , p in enumerate ( predicted ): if p in actual and p not in predicted [: i ]: num_hits += 1.0 score += num_hits / ( i + 1.0 ) return score / min ( len ( actual ), k )","title":"MAPK"},{"location":"reference/spotPython/torch/mapk/#spotPython.torch.mapk.MAPK.apk","text":"Calculate the average precision at k for a single pair of actual and predicted labels. Parameters: Name Type Description Default predicted list A list of predicted labels. required actual list A list of ground truth labels. required k int The number of top predictions to consider. 10 Returns: Name Type Description float float The average precision at k. Examples: >>> Evaluator . apk ([ 1 , 3 , 2 , 4 ], [ 1 , 2 , 3 ], 3 ) 0.8888888888888888 Source code in spotPython/torch/mapk.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 @staticmethod def apk ( predicted : List [ int ], actual : List [ int ], k : int = 10 ) -> float : \"\"\" Calculate the average precision at k for a single pair of actual and predicted labels. Args: predicted (list): A list of predicted labels. actual (list): A list of ground truth labels. k (int): The number of top predictions to consider. Returns: float: The average precision at k. Examples: >>> Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3) 0.8888888888888888 \"\"\" if not actual : return 0.0 if len ( predicted ) > k : predicted = predicted [: k ] score = 0.0 num_hits = 0.0 for i , p in enumerate ( predicted ): if p in actual and p not in predicted [: i ]: num_hits += 1.0 score += num_hits / ( i + 1.0 ) return score / min ( len ( actual ), k )","title":"apk()"},{"location":"reference/spotPython/torch/mapk/#spotPython.torch.mapk.MAPK.compute","text":"Compute the mean average precision at k. Parameters: Name Type Description Default self MAPK The current instance of the class. required Returns: Type Description float The mean average precision at k. Examples: >>> evaluator = Evaluator () >>> evaluator . total = 3.0 >>> evaluator . count = 2 >>> evaluator . compute () 1.5 Source code in spotPython/torch/mapk.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def compute ( self ) -> float : \"\"\" Compute the mean average precision at k. Args: self (MAPK): The current instance of the class. Returns: (float): The mean average precision at k. Examples: >>> evaluator = Evaluator() >>> evaluator.total = 3.0 >>> evaluator.count = 2 >>> evaluator.compute() 1.5 \"\"\" return self . total / self . count","title":"compute()"},{"location":"reference/spotPython/torch/mapk/#spotPython.torch.mapk.MAPK.update","text":"Update the state variables with a new batch of data. Parameters: Name Type Description Default predicted torch . Tensor A 2D tensor containing the predicted scores for each class. required actual torch . Tensor A 1D tensor containing the ground truth labels. required Returns: Type Description NoneType None Examples: >>> from spotPython.torch.mapk import MAPK >>> import torch >>> mapk = MAPK ( k = 2 ) >>> target = torch . tensor ([ 0 , 1 , 2 , 2 ]) >>> preds = torch . tensor ( ... [ ... [ 0.5 , 0.2 , 0.2 ], # 0 is in top 2 ... [ 0.3 , 0.4 , 0.2 ], # 1 is in top 2 ... [ 0.2 , 0.4 , 0.3 ], # 2 is in top 2 ... [ 0.7 , 0.2 , 0.1 ], # 2 isn't in top 2 ... ] ... ) >>> mapk . update ( preds , target ) >>> print ( mapk . compute ()) # tensor(0.6250) Raises: Type Description AssertionError If the actual tensor is not 1D or the predicted tensor is not 2D. AssertionError If the number of elements in the actual and predicted tensors are not equal. Source code in spotPython/torch/mapk.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def update ( self , predicted : torch . Tensor , actual : torch . Tensor ): \"\"\" Update the state variables with a new batch of data. Args: predicted (torch.Tensor): A 2D tensor containing the predicted scores for each class. actual (torch.Tensor): A 1D tensor containing the ground truth labels. Returns: (NoneType): None Examples: >>> from spotPython.torch.mapk import MAPK >>> import torch >>> mapk = MAPK(k=2) >>> target = torch.tensor([0, 1, 2, 2]) >>> preds = torch.tensor( ... [ ... [0.5, 0.2, 0.2], # 0 is in top 2 ... [0.3, 0.4, 0.2], # 1 is in top 2 ... [0.2, 0.4, 0.3], # 2 is in top 2 ... [0.7, 0.2, 0.1], # 2 isn't in top 2 ... ] ... ) >>> mapk.update(preds, target) >>> print(mapk.compute()) # tensor(0.6250) Raises: AssertionError: If the actual tensor is not 1D or the predicted tensor is not 2D. AssertionError: If the number of elements in the actual and predicted tensors are not equal. \"\"\" assert len ( actual . shape ) == 1 , \"actual must be a 1D tensor\" assert len ( predicted . shape ) == 2 , \"predicted must be a 2D tensor\" assert actual . shape [ 0 ] == predicted . shape [ 0 ], \"actual and predicted must have the same number of elements\" # Convert actual to list of lists actual = actual . tolist () actual = [[ a ] for a in actual ] # Convert predicted to list of lists of indices sorted by confidence score _ , predicted = predicted . topk ( k = self . k , dim = 1 ) predicted = predicted . tolist () # Code modified according to: \"Inplace update to inference tensor outside InferenceMode # is not allowed. You can make a clone to get a normal tensor before doing inplace update.\" score = np . mean ([ self . apk ( p , a , self . k ) for p , a in zip ( predicted , actual )]) self . total = self . total + score self . count = self . count + 1","title":"update()"},{"location":"reference/spotPython/torch/netcifar10/","text":"","title":"netcifar10"},{"location":"reference/spotPython/torch/netcore/","text":"","title":"netcore"},{"location":"reference/spotPython/torch/netfashionMNIST/","text":"","title":"netfashionMNIST"},{"location":"reference/spotPython/torch/netregression/","text":"","title":"netregression"},{"location":"reference/spotPython/torch/netvbdp/","text":"","title":"netvbdp"},{"location":"reference/spotPython/torch/traintest/","text":"","title":"traintest"},{"location":"reference/spotPython/utils/aggregate/","text":"aggregate_mean_var ( X , y , sort = False ) \u00b6 Aggregate array to mean. Parameters: Name Type Description Default X numpy . ndarray X array, shape (n, k) . required y numpy . ndarray values, shape (n,) . required sort bool Whether to sort the resulting DataFrame by the group keys. False Returns: Type Description numpy . ndarray aggregated X values, shape (n-m, k) , if m duplicates in X . numpy . ndarray aggregated (mean per group) y values, shape (1,) , if m duplicates in X . numpy . ndarray aggregated (variance per group) y values, shape (1,) , if m duplicates in X . Examples: >>> X = np . array ([[ 1 , 2 ], [ 3 , 4 ], [ 1 , 2 ]]) y = np.array([1, 2, 3]) X_agg, y_mean, y_var = aggregate_mean_var(X, y) print(X_agg) [[1. 2.] [3. 4.]] print(y_mean) [2. 2.] print(y_var) [1. 0.] Source code in spotPython/utils/aggregate.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def aggregate_mean_var ( X , y , sort = False ): \"\"\" Aggregate array to mean. Args: X (numpy.ndarray): X array, shape `(n, k)`. y (numpy.ndarray): values, shape `(n,)`. sort (bool): Whether to sort the resulting DataFrame by the group keys. Returns: (numpy.ndarray): aggregated `X` values, shape `(n-m, k)`, if `m` duplicates in `X`. (numpy.ndarray): aggregated (mean per group) `y` values, shape `(1,)`, if `m` duplicates in `X`. (numpy.ndarray): aggregated (variance per group) `y` values, shape `(1,)`, if `m` duplicates in `X`. Examples: >>> X = np.array([[1, 2], [3, 4], [1, 2]]) y = np.array([1, 2, 3]) X_agg, y_mean, y_var = aggregate_mean_var(X, y) print(X_agg) [[1. 2.] [3. 4.]] print(y_mean) [2. 2.] print(y_var) [1. 0.] \"\"\" # Create a DataFrame from X and y df = pd . DataFrame ( X ) df [ \"y\" ] = y # Group by all columns except 'y' and calculate the mean and variance of 'y' for each group grouped = df . groupby ( list ( df . columns . difference ([ \"y\" ])), as_index = False , sort = sort ) df_mean = grouped . mean () df_var = grouped . var () # Convert the resulting DataFrames to numpy arrays mean_array = df_mean . to_numpy () var_array = df_var . to_numpy () # Split the resulting arrays into separate arrays for X and y X_agg = np . delete ( mean_array , - 1 , 1 ) y_mean = mean_array [:, - 1 ] y_var = var_array [:, - 1 ] return X_agg , y_mean , y_var get_ranks ( x ) \u00b6 Returns a numpy array containing ranks of numbers within an input numpy array x: Examples: get_ranks([2, 1]) [1, 0] get_ranks([20, 10, 100]) [1, 0, 2] Parameters: Name Type Description Default x numpy . ndarray numpy array required Returns: Type Description numpy . ndarray ranks Source code in spotPython/utils/aggregate.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_ranks ( x ): \"\"\" Returns a numpy array containing ranks of numbers within an input numpy array x: Examples: get_ranks([2, 1]) [1, 0] get_ranks([20, 10, 100]) [1, 0, 2] Args: x (numpy.ndarray): numpy array Returns: (numpy.ndarray): ranks \"\"\" ts = x . argsort () ranks = np . empty_like ( ts ) ranks [ ts ] = np . arange ( len ( x )) return ranks","title":"aggregate"},{"location":"reference/spotPython/utils/aggregate/#spotPython.utils.aggregate.aggregate_mean_var","text":"Aggregate array to mean. Parameters: Name Type Description Default X numpy . ndarray X array, shape (n, k) . required y numpy . ndarray values, shape (n,) . required sort bool Whether to sort the resulting DataFrame by the group keys. False Returns: Type Description numpy . ndarray aggregated X values, shape (n-m, k) , if m duplicates in X . numpy . ndarray aggregated (mean per group) y values, shape (1,) , if m duplicates in X . numpy . ndarray aggregated (variance per group) y values, shape (1,) , if m duplicates in X . Examples: >>> X = np . array ([[ 1 , 2 ], [ 3 , 4 ], [ 1 , 2 ]]) y = np.array([1, 2, 3]) X_agg, y_mean, y_var = aggregate_mean_var(X, y) print(X_agg) [[1. 2.] [3. 4.]] print(y_mean) [2. 2.] print(y_var) [1. 0.] Source code in spotPython/utils/aggregate.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def aggregate_mean_var ( X , y , sort = False ): \"\"\" Aggregate array to mean. Args: X (numpy.ndarray): X array, shape `(n, k)`. y (numpy.ndarray): values, shape `(n,)`. sort (bool): Whether to sort the resulting DataFrame by the group keys. Returns: (numpy.ndarray): aggregated `X` values, shape `(n-m, k)`, if `m` duplicates in `X`. (numpy.ndarray): aggregated (mean per group) `y` values, shape `(1,)`, if `m` duplicates in `X`. (numpy.ndarray): aggregated (variance per group) `y` values, shape `(1,)`, if `m` duplicates in `X`. Examples: >>> X = np.array([[1, 2], [3, 4], [1, 2]]) y = np.array([1, 2, 3]) X_agg, y_mean, y_var = aggregate_mean_var(X, y) print(X_agg) [[1. 2.] [3. 4.]] print(y_mean) [2. 2.] print(y_var) [1. 0.] \"\"\" # Create a DataFrame from X and y df = pd . DataFrame ( X ) df [ \"y\" ] = y # Group by all columns except 'y' and calculate the mean and variance of 'y' for each group grouped = df . groupby ( list ( df . columns . difference ([ \"y\" ])), as_index = False , sort = sort ) df_mean = grouped . mean () df_var = grouped . var () # Convert the resulting DataFrames to numpy arrays mean_array = df_mean . to_numpy () var_array = df_var . to_numpy () # Split the resulting arrays into separate arrays for X and y X_agg = np . delete ( mean_array , - 1 , 1 ) y_mean = mean_array [:, - 1 ] y_var = var_array [:, - 1 ] return X_agg , y_mean , y_var","title":"aggregate_mean_var()"},{"location":"reference/spotPython/utils/aggregate/#spotPython.utils.aggregate.get_ranks","text":"Returns a numpy array containing ranks of numbers within an input numpy array x: Examples: get_ranks([2, 1]) [1, 0] get_ranks([20, 10, 100]) [1, 0, 2] Parameters: Name Type Description Default x numpy . ndarray numpy array required Returns: Type Description numpy . ndarray ranks Source code in spotPython/utils/aggregate.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_ranks ( x ): \"\"\" Returns a numpy array containing ranks of numbers within an input numpy array x: Examples: get_ranks([2, 1]) [1, 0] get_ranks([20, 10, 100]) [1, 0, 2] Args: x (numpy.ndarray): numpy array Returns: (numpy.ndarray): ranks \"\"\" ts = x . argsort () ranks = np . empty_like ( ts ) ranks [ ts ] = np . arange ( len ( x )) return ranks","title":"get_ranks()"},{"location":"reference/spotPython/utils/classes/","text":"","title":"classes"},{"location":"reference/spotPython/utils/compare/","text":"find_equal_in_lists ( a , b ) \u00b6 Find equal values in two lists. Parameters: Name Type Description Default a list list with a values required b list list with b values required Returns: Name Type Description list List [ int ] list with 1 if equal, otherwise 0 Examples: >>> from spotPython.utils.compare import find_equal_in_lists a = [1, 2, 3, 4, 5] b = [1, 2, 3, 4, 5] find_equal_in_lists(a, b) [1, 1, 1, 1, 1] Source code in spotPython/utils/compare.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def find_equal_in_lists ( a : List [ int ], b : List [ int ]) -> List [ int ]: \"\"\"Find equal values in two lists. Args: a (list): list with a values b (list): list with b values Returns: list: list with 1 if equal, otherwise 0 Examples: >>> from spotPython.utils.compare import find_equal_in_lists a = [1, 2, 3, 4, 5] b = [1, 2, 3, 4, 5] find_equal_in_lists(a, b) [1, 1, 1, 1, 1] \"\"\" equal = [ 1 if a [ i ] == b [ i ] else 0 for i in range ( len ( a ))] return equal selectNew ( A , X , tolerance = 0 ) \u00b6 Select rows from A that are not in X. Parameters: Name Type Description Default A numpy . ndarray A array with new values required X numpy . ndarray X array with known values required tolerance float tolerance value for comparison 0 Returns: Type Description numpy . ndarray array with unknown (new) values numpy . ndarray array with True if value is new, otherwise False . Examples: >>> from spotPython.utils.compare import selectNew A = np.array([[1,2,3],[4,5,6]]) X = np.array([[1,2,3],[4,5,6]]) selectNew(A, X) (array([], shape=(0, 3), dtype=int64), array([], dtype=bool)) Source code in spotPython/utils/compare.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def selectNew ( A : np . ndarray , X : np . ndarray , tolerance : float = 0 ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Select rows from A that are not in X. Args: A (numpy.ndarray): A array with new values X (numpy.ndarray): X array with known values tolerance (float): tolerance value for comparison Returns: (numpy.ndarray): array with unknown (new) values (numpy.ndarray): array with `True` if value is new, otherwise `False`. Examples: >>> from spotPython.utils.compare import selectNew A = np.array([[1,2,3],[4,5,6]]) X = np.array([[1,2,3],[4,5,6]]) selectNew(A, X) (array([], shape=(0, 3), dtype=int64), array([], dtype=bool)) \"\"\" B = np . abs ( A [:, None ] - X ) ind = np . any ( np . all ( B <= tolerance , axis = 2 ), axis = 1 ) return A [ ~ ind ], ~ ind","title":"compare"},{"location":"reference/spotPython/utils/compare/#spotPython.utils.compare.find_equal_in_lists","text":"Find equal values in two lists. Parameters: Name Type Description Default a list list with a values required b list list with b values required Returns: Name Type Description list List [ int ] list with 1 if equal, otherwise 0 Examples: >>> from spotPython.utils.compare import find_equal_in_lists a = [1, 2, 3, 4, 5] b = [1, 2, 3, 4, 5] find_equal_in_lists(a, b) [1, 1, 1, 1, 1] Source code in spotPython/utils/compare.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def find_equal_in_lists ( a : List [ int ], b : List [ int ]) -> List [ int ]: \"\"\"Find equal values in two lists. Args: a (list): list with a values b (list): list with b values Returns: list: list with 1 if equal, otherwise 0 Examples: >>> from spotPython.utils.compare import find_equal_in_lists a = [1, 2, 3, 4, 5] b = [1, 2, 3, 4, 5] find_equal_in_lists(a, b) [1, 1, 1, 1, 1] \"\"\" equal = [ 1 if a [ i ] == b [ i ] else 0 for i in range ( len ( a ))] return equal","title":"find_equal_in_lists()"},{"location":"reference/spotPython/utils/compare/#spotPython.utils.compare.selectNew","text":"Select rows from A that are not in X. Parameters: Name Type Description Default A numpy . ndarray A array with new values required X numpy . ndarray X array with known values required tolerance float tolerance value for comparison 0 Returns: Type Description numpy . ndarray array with unknown (new) values numpy . ndarray array with True if value is new, otherwise False . Examples: >>> from spotPython.utils.compare import selectNew A = np.array([[1,2,3],[4,5,6]]) X = np.array([[1,2,3],[4,5,6]]) selectNew(A, X) (array([], shape=(0, 3), dtype=int64), array([], dtype=bool)) Source code in spotPython/utils/compare.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def selectNew ( A : np . ndarray , X : np . ndarray , tolerance : float = 0 ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Select rows from A that are not in X. Args: A (numpy.ndarray): A array with new values X (numpy.ndarray): X array with known values tolerance (float): tolerance value for comparison Returns: (numpy.ndarray): array with unknown (new) values (numpy.ndarray): array with `True` if value is new, otherwise `False`. Examples: >>> from spotPython.utils.compare import selectNew A = np.array([[1,2,3],[4,5,6]]) X = np.array([[1,2,3],[4,5,6]]) selectNew(A, X) (array([], shape=(0, 3), dtype=int64), array([], dtype=bool)) \"\"\" B = np . abs ( A [:, None ] - X ) ind = np . any ( np . all ( B <= tolerance , axis = 2 ), axis = 1 ) return A [ ~ ind ], ~ ind","title":"selectNew()"},{"location":"reference/spotPython/utils/convert/","text":"add_logical_columns ( X , arity = 2 , operations = [ 'and' , 'or' , 'xor' ]) \u00b6 Combines all features in a dataframe with each other using bitwise operations Parameters: Name Type Description Default X pd . DataFrame dataframe with features required arity int the number of columns to combine at once 2 operations list of str the operations to apply. Possible values are \u2018and\u2019, \u2018or\u2019 and \u2018xor\u2019 ['and', 'or', 'xor'] Returns: Name Type Description X pd . DataFrame dataframe with new features Examples: >>> X = pd . DataFrame ({ \"a\" : [ True , False , True ], \"b\" : [ True , True , False ], \"c\" : [ False , False , True ]}) >>> add_logical_columns ( X ) a b c a_and_b a_and_c b_and_c a_or_b a_or_c b_or_c a_xor_b a_xor_c b_xor_c 0 True True False True False False True True True False True True 1 False True False False False False True False True True True False 2 True False True False True False True True True True False True Source code in spotPython/utils/convert.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def add_logical_columns ( X , arity = 2 , operations = [ \"and\" , \"or\" , \"xor\" ]): \"\"\"Combines all features in a dataframe with each other using bitwise operations Args: X (pd.DataFrame): dataframe with features arity (int): the number of columns to combine at once operations (list of str): the operations to apply. Possible values are 'and', 'or' and 'xor' Returns: X (pd.DataFrame): dataframe with new features Examples: >>> X = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]}) >>> add_logical_columns(X) a b c a_and_b a_and_c b_and_c a_or_b a_or_c b_or_c a_xor_b a_xor_c b_xor_c 0 True True False True False False True True True False True True 1 False True False False False False True False True True True False 2 True False True False True False True True True True False True \"\"\" new_cols = [] # Iterate over all combinations of columns of the given arity for cols in combinations ( X . columns , arity ): # Create new columns for the specified operations if \"and\" in operations : and_col = X [ list ( cols )] . apply ( lambda x : x . all (), axis = 1 ) new_cols . append ( and_col ) if \"or\" in operations : or_col = X [ list ( cols )] . apply ( lambda x : x . any (), axis = 1 ) new_cols . append ( or_col ) if \"xor\" in operations : xor_col = X [ list ( cols )] . apply ( lambda x : x . sum () % 2 == 1 , axis = 1 ) new_cols . append ( xor_col ) # Join all the new columns at once X = pd . concat ([ X ] + new_cols , axis = 1 ) return X class_for_name ( module_name , class_name ) \u00b6 Returns a class for a given module and class name. Parameters: Name Type Description Default module_name str The name of the module. required class_name str The name of the class. required Returns: Name Type Description object object The class. Examples: >>> from spotPython.utils.convert import class_for_name from scipy.optimize import rosen bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)] shgo_class = class_for_name(\"scipy.optimize\", \"shgo\") result = shgo_class(rosen, bounds) Source code in spotPython/utils/convert.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def class_for_name ( module_name , class_name ) -> object : \"\"\"Returns a class for a given module and class name. Parameters: module_name (str): The name of the module. class_name (str): The name of the class. Returns: object: The class. Examples: >>> from spotPython.utils.convert import class_for_name from scipy.optimize import rosen bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)] shgo_class = class_for_name(\"scipy.optimize\", \"shgo\") result = shgo_class(rosen, bounds) \"\"\" m = importlib . import_module ( module_name ) c = getattr ( m , class_name ) return c get_Xy_from_df ( df , target_column ) \u00b6 Get X and y from a dataframe. Parameters: Name Type Description Default df pandas . DataFrame The input dataframe. required target_column str The name of the target column. required Returns: Name Type Description tuple tuple The tuple (X, y). Examples: >>> from spotPython.utils.convert import get_Xy_from_df >>> import pandas as pd >>> df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ], \"c\" : [ 7 , 8 , 9 ]}) >>> X , y = get_Xy_from_df ( df , \"c\" ) Source code in spotPython/utils/convert.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def get_Xy_from_df ( df , target_column ) -> tuple : \"\"\"Get X and y from a dataframe. Parameters: df (pandas.DataFrame): The input dataframe. target_column (str): The name of the target column. Returns: tuple: The tuple (X, y). Examples: >>> from spotPython.utils.convert import get_Xy_from_df >>> import pandas as pd >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]}) >>> X, y = get_Xy_from_df(df, \"c\") \"\"\" X = df . drop ( columns = [ target_column ]) y = df [ target_column ] return X , y series_to_array ( series ) \u00b6 Converts a pandas series to a numpy array. Parameters: Name Type Description Default series pandas . Series The input series. required Returns: Type Description numpy . ndarray The output array. Examples: >>> from spotPython.utils.convert import series_to_array >>> import pandas as pd >>> series = pd . Series ([ 1 , 2 , 3 ]) >>> series_to_array ( series ) array([1, 2, 3]) Source code in spotPython/utils/convert.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def series_to_array ( series ): \"\"\"Converts a pandas series to a numpy array. Args: series (pandas.Series): The input series. Returns: (numpy.ndarray): The output array. Examples: >>> from spotPython.utils.convert import series_to_array >>> import pandas as pd >>> series = pd.Series([1, 2, 3]) >>> series_to_array(series) array([1, 2, 3]) \"\"\" if isinstance ( series , np . ndarray ): return series else : return series . to_numpy ()","title":"convert"},{"location":"reference/spotPython/utils/convert/#spotPython.utils.convert.add_logical_columns","text":"Combines all features in a dataframe with each other using bitwise operations Parameters: Name Type Description Default X pd . DataFrame dataframe with features required arity int the number of columns to combine at once 2 operations list of str the operations to apply. Possible values are \u2018and\u2019, \u2018or\u2019 and \u2018xor\u2019 ['and', 'or', 'xor'] Returns: Name Type Description X pd . DataFrame dataframe with new features Examples: >>> X = pd . DataFrame ({ \"a\" : [ True , False , True ], \"b\" : [ True , True , False ], \"c\" : [ False , False , True ]}) >>> add_logical_columns ( X ) a b c a_and_b a_and_c b_and_c a_or_b a_or_c b_or_c a_xor_b a_xor_c b_xor_c 0 True True False True False False True True True False True True 1 False True False False False False True False True True True False 2 True False True False True False True True True True False True Source code in spotPython/utils/convert.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def add_logical_columns ( X , arity = 2 , operations = [ \"and\" , \"or\" , \"xor\" ]): \"\"\"Combines all features in a dataframe with each other using bitwise operations Args: X (pd.DataFrame): dataframe with features arity (int): the number of columns to combine at once operations (list of str): the operations to apply. Possible values are 'and', 'or' and 'xor' Returns: X (pd.DataFrame): dataframe with new features Examples: >>> X = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]}) >>> add_logical_columns(X) a b c a_and_b a_and_c b_and_c a_or_b a_or_c b_or_c a_xor_b a_xor_c b_xor_c 0 True True False True False False True True True False True True 1 False True False False False False True False True True True False 2 True False True False True False True True True True False True \"\"\" new_cols = [] # Iterate over all combinations of columns of the given arity for cols in combinations ( X . columns , arity ): # Create new columns for the specified operations if \"and\" in operations : and_col = X [ list ( cols )] . apply ( lambda x : x . all (), axis = 1 ) new_cols . append ( and_col ) if \"or\" in operations : or_col = X [ list ( cols )] . apply ( lambda x : x . any (), axis = 1 ) new_cols . append ( or_col ) if \"xor\" in operations : xor_col = X [ list ( cols )] . apply ( lambda x : x . sum () % 2 == 1 , axis = 1 ) new_cols . append ( xor_col ) # Join all the new columns at once X = pd . concat ([ X ] + new_cols , axis = 1 ) return X","title":"add_logical_columns()"},{"location":"reference/spotPython/utils/convert/#spotPython.utils.convert.class_for_name","text":"Returns a class for a given module and class name. Parameters: Name Type Description Default module_name str The name of the module. required class_name str The name of the class. required Returns: Name Type Description object object The class. Examples: >>> from spotPython.utils.convert import class_for_name from scipy.optimize import rosen bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)] shgo_class = class_for_name(\"scipy.optimize\", \"shgo\") result = shgo_class(rosen, bounds) Source code in spotPython/utils/convert.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def class_for_name ( module_name , class_name ) -> object : \"\"\"Returns a class for a given module and class name. Parameters: module_name (str): The name of the module. class_name (str): The name of the class. Returns: object: The class. Examples: >>> from spotPython.utils.convert import class_for_name from scipy.optimize import rosen bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)] shgo_class = class_for_name(\"scipy.optimize\", \"shgo\") result = shgo_class(rosen, bounds) \"\"\" m = importlib . import_module ( module_name ) c = getattr ( m , class_name ) return c","title":"class_for_name()"},{"location":"reference/spotPython/utils/convert/#spotPython.utils.convert.get_Xy_from_df","text":"Get X and y from a dataframe. Parameters: Name Type Description Default df pandas . DataFrame The input dataframe. required target_column str The name of the target column. required Returns: Name Type Description tuple tuple The tuple (X, y). Examples: >>> from spotPython.utils.convert import get_Xy_from_df >>> import pandas as pd >>> df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ], \"c\" : [ 7 , 8 , 9 ]}) >>> X , y = get_Xy_from_df ( df , \"c\" ) Source code in spotPython/utils/convert.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def get_Xy_from_df ( df , target_column ) -> tuple : \"\"\"Get X and y from a dataframe. Parameters: df (pandas.DataFrame): The input dataframe. target_column (str): The name of the target column. Returns: tuple: The tuple (X, y). Examples: >>> from spotPython.utils.convert import get_Xy_from_df >>> import pandas as pd >>> df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]}) >>> X, y = get_Xy_from_df(df, \"c\") \"\"\" X = df . drop ( columns = [ target_column ]) y = df [ target_column ] return X , y","title":"get_Xy_from_df()"},{"location":"reference/spotPython/utils/convert/#spotPython.utils.convert.series_to_array","text":"Converts a pandas series to a numpy array. Parameters: Name Type Description Default series pandas . Series The input series. required Returns: Type Description numpy . ndarray The output array. Examples: >>> from spotPython.utils.convert import series_to_array >>> import pandas as pd >>> series = pd . Series ([ 1 , 2 , 3 ]) >>> series_to_array ( series ) array([1, 2, 3]) Source code in spotPython/utils/convert.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def series_to_array ( series ): \"\"\"Converts a pandas series to a numpy array. Args: series (pandas.Series): The input series. Returns: (numpy.ndarray): The output array. Examples: >>> from spotPython.utils.convert import series_to_array >>> import pandas as pd >>> series = pd.Series([1, 2, 3]) >>> series_to_array(series) array([1, 2, 3]) \"\"\" if isinstance ( series , np . ndarray ): return series else : return series . to_numpy ()","title":"series_to_array()"},{"location":"reference/spotPython/utils/device/","text":"getDevice ( device = None ) \u00b6 Get cpu, gpu or mps device for training. Parameters: Name Type Description Default device str Device for training. If None or \u201cauto\u201d the device is selected automatically. None Returns: Name Type Description device str Device for training. Examples: >>> from spotPython.utils.device import getDevice >>> getDevice () 'cuda:0' Source code in spotPython/utils/device.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def getDevice ( device = None ): \"\"\"Get cpu, gpu or mps device for training. Args: device (str): Device for training. If None or \"auto\" the device is selected automatically. Returns: device (str): Device for training. Examples: >>> from spotPython.utils.device import getDevice >>> getDevice() 'cuda:0' \"\"\" if device is None or device == \"auto\" : device = \"cpu\" if torch . cuda . is_available (): device = \"cuda:0\" elif torch . backends . mps . is_available (): device = \"mps\" return device","title":"device"},{"location":"reference/spotPython/utils/device/#spotPython.utils.device.getDevice","text":"Get cpu, gpu or mps device for training. Parameters: Name Type Description Default device str Device for training. If None or \u201cauto\u201d the device is selected automatically. None Returns: Name Type Description device str Device for training. Examples: >>> from spotPython.utils.device import getDevice >>> getDevice () 'cuda:0' Source code in spotPython/utils/device.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def getDevice ( device = None ): \"\"\"Get cpu, gpu or mps device for training. Args: device (str): Device for training. If None or \"auto\" the device is selected automatically. Returns: device (str): Device for training. Examples: >>> from spotPython.utils.device import getDevice >>> getDevice() 'cuda:0' \"\"\" if device is None or device == \"auto\" : device = \"cpu\" if torch . cuda . is_available (): device = \"cuda:0\" elif torch . backends . mps . is_available (): device = \"mps\" return device","title":"getDevice()"},{"location":"reference/spotPython/utils/eda/","text":"compare_two_tree_models ( model1 , model2 , headers = [ 'Parameter' , 'Default' , 'Spot' ]) \u00b6 Compares two tree models. Parameters: Name Type Description Default model1 object A tree model. required model2 object A tree model. required headers list A list with the headers of the table. ['Parameter', 'Default', 'Spot'] Returns: Type Description str A table with the comparison of the two models. Examples: >>> from spotPython.utils.eda import compare_two_tree_models >>> from spotPython.hyperparameters.values import get_default_values >>> fun_control = { ... \"x1\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x2\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x3\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x4\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x5\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x6\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x7\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x8\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x9\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x10\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... } >>> default_values = get_default_values ( fun_control ) >>> model1 = spot_tuner . get_model ( \"rf\" , default_values ) >>> model2 = spot_tuner . get_model ( \"rf\" , default_values ) >>> compare_two_tree_models ( model1 , model2 ) Source code in spotPython/utils/eda.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def compare_two_tree_models ( model1 , model2 , headers = [ \"Parameter\" , \"Default\" , \"Spot\" ]): \"\"\"Compares two tree models. Args: model1 (object): A tree model. model2 (object): A tree model. headers (list): A list with the headers of the table. Returns: (str): A table with the comparison of the two models. Examples: >>> from spotPython.utils.eda import compare_two_tree_models >>> from spotPython.hyperparameters.values import get_default_values >>> fun_control = { ... \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... } >>> default_values = get_default_values(fun_control) >>> model1 = spot_tuner.get_model(\"rf\", default_values) >>> model2 = spot_tuner.get_model(\"rf\", default_values) >>> compare_two_tree_models(model1, model2) \"\"\" keys = model1 . summary . keys () values1 = model1 . summary . values () values2 = model2 . summary . values () tbl = [] for key , value1 , value2 in zip ( keys , values1 , values2 ): tbl . append ([ key , value1 , value2 ]) return tabulate ( tbl , headers = headers , numalign = \"right\" , tablefmt = \"github\" ) gen_design_table ( fun_control , spot = None , tablefmt = 'github' ) \u00b6 Generates a table with the design variables and their bounds. Parameters: Name Type Description Default fun_control dict A dictionary with function design variables. required spot object A spot object. Defaults to None. None Returns: Type Description str a table with the design variables, their default values, and their bounds. If a spot object is provided, the table will also include the value and the importance of each hyperparameter. Use the print function to display the table. Examples: >>> from spotPython.utils.eda import gen_design_table >>> from spotPython.hyperparameters.values import get_default_values >>> fun_control = { ... \"x1\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x2\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x3\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x4\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x5\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x6\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x7\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x8\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x9\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x10\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... } Source code in spotPython/utils/eda.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def gen_design_table ( fun_control : dict , spot : object = None , tablefmt = \"github\" ) -> str : \"\"\"Generates a table with the design variables and their bounds. Args: fun_control (dict): A dictionary with function design variables. spot (object): A spot object. Defaults to None. Returns: (str): a table with the design variables, their default values, and their bounds. If a spot object is provided, the table will also include the value and the importance of each hyperparameter. Use the `print` function to display the table. Examples: >>> from spotPython.utils.eda import gen_design_table >>> from spotPython.hyperparameters.values import get_default_values >>> fun_control = { ... \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... } \"\"\" default_values = get_default_values ( fun_control ) defaults = list ( default_values . values ()) if spot is None : tab = tabulate ( { \"name\" : get_var_name ( fun_control ), \"type\" : get_var_type ( fun_control ), \"default\" : defaults , \"lower\" : get_bound_values ( fun_control , \"lower\" , as_list = True ), \"upper\" : get_bound_values ( fun_control , \"upper\" , as_list = True ), \"transform\" : get_transform ( fun_control ), }, headers = \"keys\" , tablefmt = tablefmt , ) else : res = spot . print_results ( print_screen = False ) tuned = [ item [ 1 ] for item in res ] # imp = spot.print_importance(threshold=0.0, print_screen=False) # importance = [item[1] for item in imp] importance = spot . get_importance () stars = get_stars ( importance ) tab = tabulate ( { \"name\" : get_var_name ( fun_control ), \"type\" : get_var_type ( fun_control ), \"default\" : defaults , \"lower\" : get_bound_values ( fun_control , \"lower\" , as_list = True ), \"upper\" : get_bound_values ( fun_control , \"upper\" , as_list = True ), \"tuned\" : tuned , \"transform\" : get_transform ( fun_control ), \"importance\" : importance , \"stars\" : stars , }, headers = \"keys\" , numalign = \"right\" , floatfmt = ( \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \".2f\" ), tablefmt = tablefmt , ) return tab generate_config_id ( config ) \u00b6 Generates a unique id for a configuration. Parameters: Name Type Description Default config dict A dictionary with the configuration. required Returns: Type Description str A unique id for the configuration. Examples: >>> from spotPython.hyperparameters.values import get_one_config_from_X >>> X = spot_tuner . to_all_dim ( spot_tuner . min_X . reshape ( 1 , - 1 )) >>> config = get_one_config_from_X ( X , fun_control ) >>> generate_config_id ( config ) Source code in spotPython/utils/eda.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def generate_config_id ( config ): \"\"\"Generates a unique id for a configuration. Args: config (dict): A dictionary with the configuration. Returns: (str): A unique id for the configuration. Examples: >>> from spotPython.hyperparameters.values import get_one_config_from_X >>> X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1)) >>> config = get_one_config_from_X(X, fun_control) >>> generate_config_id(config) \"\"\" config_id = \"\" for key in config : config_id += str ( config [ key ]) + \"_\" return config_id [: - 1 ] get_stars ( input_list ) \u00b6 Converts a list of values to a list of stars, which can be used to visualize the importance of a variable. Parameters: Name Type Description Default input_list list A list of values. required Returns: Type Description list A list of strings. Examples: >>> from spotPython.utils.eda import convert_list >>> get_stars ([ 100 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 ]) [***, '', '', '', '', '', '', '', ''] Source code in spotPython/utils/eda.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def get_stars ( input_list ) -> list : \"\"\"Converts a list of values to a list of stars, which can be used to visualize the importance of a variable. Args: input_list (list): A list of values. Returns: (list): A list of strings. Examples: >>> from spotPython.utils.eda import convert_list >>> get_stars([100, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]) [***, '', '', '', '', '', '', '', ''] \"\"\" output_list = [] for value in input_list : if value > 95 : output_list . append ( \"***\" ) elif value > 50 : output_list . append ( \"**\" ) elif value > 1 : output_list . append ( \"*\" ) elif value > 0.1 : output_list . append ( \".\" ) else : output_list . append ( \"\" ) return output_list visualize_activations ( net , device = 'cpu' , color = 'C0' ) \u00b6 Visualizes the activations of a neural network. PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS, Author: Phillip Lippe, License: CC BY-SA. Parameters: Name Type Description Default net object A neural network. required device str The device to use. Defaults to \u201ccpu\u201d. 'cpu' color str The color to use. Defaults to \u201cC0\u201d. 'C0' Examples: >>> from spotPython.hyperparameters.values import get_one_config_from_X >>> X = spot_tuner . to_all_dim ( spot_tuner . min_X . reshape ( 1 , - 1 )) >>> config = get_one_config_from_X ( X , fun_control ) >>> model = fun_control [ \"core_model\" ]( ** config , _L_in = 64 , _L_out = 11 ) >>> visualize_activations ( model , device = \"cpu\" , color = f \"C { 0 } \" ) Source code in spotPython/utils/eda.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def visualize_activations ( net , device = \"cpu\" , color = \"C0\" ): \"\"\"Visualizes the activations of a neural network. Code is based on: PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS, Author: Phillip Lippe, License: CC BY-SA. Args: net (object): A neural network. device (str, optional): The device to use. Defaults to \"cpu\". color (str, optional): The color to use. Defaults to \"C0\". Examples: >>> from spotPython.hyperparameters.values import get_one_config_from_X >>> X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1)) >>> config = get_one_config_from_X(X, fun_control) >>> model = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11) >>> visualize_activations(model, device=\"cpu\", color=f\"C{0}\") \"\"\" activations = {} net . eval () # Create an instance of CSVDataset dataset = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) # Set batch size for DataLoader batch_size = 128 # Create DataLoader dataloader = DataLoader ( dataset , batch_size = batch_size , shuffle = True ) # for batch in dataloader: # inputs, targets = batch # small_loader = data.DataLoader(train_set, batch_size=1024) inputs , _ = next ( iter ( dataloader )) with torch . no_grad (): layer_index = 0 inputs = inputs . to ( device ) inputs = inputs . view ( inputs . size ( 0 ), - 1 ) # We need to manually loop through the layers to save all activations for layer_index , layer in enumerate ( net . layers [: - 1 ]): inputs = layer ( inputs ) activations [ layer_index ] = inputs . view ( - 1 ) . cpu () . numpy () # Plotting columns = 4 rows = math . ceil ( len ( activations ) / columns ) fig , ax = plt . subplots ( rows , columns , figsize = ( columns * 2.7 , rows * 2.5 )) fig_index = 0 for key in activations : key_ax = ax [ fig_index // columns ][ fig_index % columns ] sns . histplot ( data = activations [ key ], bins = 50 , ax = key_ax , color = color , kde = True , stat = \"density\" ) key_ax . set_title ( f \"Layer { key } - { net . layers [ key ] . __class__ . __name__ } \" ) fig_index += 1 fig . suptitle ( f \"Activation distribution for activation function { net . hparams . act_fn } \" , fontsize = 14 ) fig . subplots_adjust ( hspace = 0.4 , wspace = 0.4 ) plt . show () plt . close ()","title":"eda"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.compare_two_tree_models","text":"Compares two tree models. Parameters: Name Type Description Default model1 object A tree model. required model2 object A tree model. required headers list A list with the headers of the table. ['Parameter', 'Default', 'Spot'] Returns: Type Description str A table with the comparison of the two models. Examples: >>> from spotPython.utils.eda import compare_two_tree_models >>> from spotPython.hyperparameters.values import get_default_values >>> fun_control = { ... \"x1\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x2\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x3\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x4\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x5\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x6\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x7\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x8\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x9\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x10\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... } >>> default_values = get_default_values ( fun_control ) >>> model1 = spot_tuner . get_model ( \"rf\" , default_values ) >>> model2 = spot_tuner . get_model ( \"rf\" , default_values ) >>> compare_two_tree_models ( model1 , model2 ) Source code in spotPython/utils/eda.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def compare_two_tree_models ( model1 , model2 , headers = [ \"Parameter\" , \"Default\" , \"Spot\" ]): \"\"\"Compares two tree models. Args: model1 (object): A tree model. model2 (object): A tree model. headers (list): A list with the headers of the table. Returns: (str): A table with the comparison of the two models. Examples: >>> from spotPython.utils.eda import compare_two_tree_models >>> from spotPython.hyperparameters.values import get_default_values >>> fun_control = { ... \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... } >>> default_values = get_default_values(fun_control) >>> model1 = spot_tuner.get_model(\"rf\", default_values) >>> model2 = spot_tuner.get_model(\"rf\", default_values) >>> compare_two_tree_models(model1, model2) \"\"\" keys = model1 . summary . keys () values1 = model1 . summary . values () values2 = model2 . summary . values () tbl = [] for key , value1 , value2 in zip ( keys , values1 , values2 ): tbl . append ([ key , value1 , value2 ]) return tabulate ( tbl , headers = headers , numalign = \"right\" , tablefmt = \"github\" )","title":"compare_two_tree_models()"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.gen_design_table","text":"Generates a table with the design variables and their bounds. Parameters: Name Type Description Default fun_control dict A dictionary with function design variables. required spot object A spot object. Defaults to None. None Returns: Type Description str a table with the design variables, their default values, and their bounds. If a spot object is provided, the table will also include the value and the importance of each hyperparameter. Use the print function to display the table. Examples: >>> from spotPython.utils.eda import gen_design_table >>> from spotPython.hyperparameters.values import get_default_values >>> fun_control = { ... \"x1\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x2\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x3\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x4\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x5\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x6\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x7\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x8\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x9\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... \"x10\" : { \"type\" : \"int\" , \"default\" : 1 , \"lower\" : 1 , \"upper\" : 10 }, ... } Source code in spotPython/utils/eda.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def gen_design_table ( fun_control : dict , spot : object = None , tablefmt = \"github\" ) -> str : \"\"\"Generates a table with the design variables and their bounds. Args: fun_control (dict): A dictionary with function design variables. spot (object): A spot object. Defaults to None. Returns: (str): a table with the design variables, their default values, and their bounds. If a spot object is provided, the table will also include the value and the importance of each hyperparameter. Use the `print` function to display the table. Examples: >>> from spotPython.utils.eda import gen_design_table >>> from spotPython.hyperparameters.values import get_default_values >>> fun_control = { ... \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10}, ... } \"\"\" default_values = get_default_values ( fun_control ) defaults = list ( default_values . values ()) if spot is None : tab = tabulate ( { \"name\" : get_var_name ( fun_control ), \"type\" : get_var_type ( fun_control ), \"default\" : defaults , \"lower\" : get_bound_values ( fun_control , \"lower\" , as_list = True ), \"upper\" : get_bound_values ( fun_control , \"upper\" , as_list = True ), \"transform\" : get_transform ( fun_control ), }, headers = \"keys\" , tablefmt = tablefmt , ) else : res = spot . print_results ( print_screen = False ) tuned = [ item [ 1 ] for item in res ] # imp = spot.print_importance(threshold=0.0, print_screen=False) # importance = [item[1] for item in imp] importance = spot . get_importance () stars = get_stars ( importance ) tab = tabulate ( { \"name\" : get_var_name ( fun_control ), \"type\" : get_var_type ( fun_control ), \"default\" : defaults , \"lower\" : get_bound_values ( fun_control , \"lower\" , as_list = True ), \"upper\" : get_bound_values ( fun_control , \"upper\" , as_list = True ), \"tuned\" : tuned , \"transform\" : get_transform ( fun_control ), \"importance\" : importance , \"stars\" : stars , }, headers = \"keys\" , numalign = \"right\" , floatfmt = ( \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \".2f\" ), tablefmt = tablefmt , ) return tab","title":"gen_design_table()"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.generate_config_id","text":"Generates a unique id for a configuration. Parameters: Name Type Description Default config dict A dictionary with the configuration. required Returns: Type Description str A unique id for the configuration. Examples: >>> from spotPython.hyperparameters.values import get_one_config_from_X >>> X = spot_tuner . to_all_dim ( spot_tuner . min_X . reshape ( 1 , - 1 )) >>> config = get_one_config_from_X ( X , fun_control ) >>> generate_config_id ( config ) Source code in spotPython/utils/eda.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def generate_config_id ( config ): \"\"\"Generates a unique id for a configuration. Args: config (dict): A dictionary with the configuration. Returns: (str): A unique id for the configuration. Examples: >>> from spotPython.hyperparameters.values import get_one_config_from_X >>> X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1)) >>> config = get_one_config_from_X(X, fun_control) >>> generate_config_id(config) \"\"\" config_id = \"\" for key in config : config_id += str ( config [ key ]) + \"_\" return config_id [: - 1 ]","title":"generate_config_id()"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.get_stars","text":"Converts a list of values to a list of stars, which can be used to visualize the importance of a variable. Parameters: Name Type Description Default input_list list A list of values. required Returns: Type Description list A list of strings. Examples: >>> from spotPython.utils.eda import convert_list >>> get_stars ([ 100 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 ]) [***, '', '', '', '', '', '', '', ''] Source code in spotPython/utils/eda.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def get_stars ( input_list ) -> list : \"\"\"Converts a list of values to a list of stars, which can be used to visualize the importance of a variable. Args: input_list (list): A list of values. Returns: (list): A list of strings. Examples: >>> from spotPython.utils.eda import convert_list >>> get_stars([100, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]) [***, '', '', '', '', '', '', '', ''] \"\"\" output_list = [] for value in input_list : if value > 95 : output_list . append ( \"***\" ) elif value > 50 : output_list . append ( \"**\" ) elif value > 1 : output_list . append ( \"*\" ) elif value > 0.1 : output_list . append ( \".\" ) else : output_list . append ( \"\" ) return output_list","title":"get_stars()"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.visualize_activations","text":"Visualizes the activations of a neural network. PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS, Author: Phillip Lippe, License: CC BY-SA. Parameters: Name Type Description Default net object A neural network. required device str The device to use. Defaults to \u201ccpu\u201d. 'cpu' color str The color to use. Defaults to \u201cC0\u201d. 'C0' Examples: >>> from spotPython.hyperparameters.values import get_one_config_from_X >>> X = spot_tuner . to_all_dim ( spot_tuner . min_X . reshape ( 1 , - 1 )) >>> config = get_one_config_from_X ( X , fun_control ) >>> model = fun_control [ \"core_model\" ]( ** config , _L_in = 64 , _L_out = 11 ) >>> visualize_activations ( model , device = \"cpu\" , color = f \"C { 0 } \" ) Source code in spotPython/utils/eda.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def visualize_activations ( net , device = \"cpu\" , color = \"C0\" ): \"\"\"Visualizes the activations of a neural network. Code is based on: PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS, Author: Phillip Lippe, License: CC BY-SA. Args: net (object): A neural network. device (str, optional): The device to use. Defaults to \"cpu\". color (str, optional): The color to use. Defaults to \"C0\". Examples: >>> from spotPython.hyperparameters.values import get_one_config_from_X >>> X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1)) >>> config = get_one_config_from_X(X, fun_control) >>> model = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11) >>> visualize_activations(model, device=\"cpu\", color=f\"C{0}\") \"\"\" activations = {} net . eval () # Create an instance of CSVDataset dataset = CSVDataset ( csv_file = \"./data/VBDP/train.csv\" , train = True ) # Set batch size for DataLoader batch_size = 128 # Create DataLoader dataloader = DataLoader ( dataset , batch_size = batch_size , shuffle = True ) # for batch in dataloader: # inputs, targets = batch # small_loader = data.DataLoader(train_set, batch_size=1024) inputs , _ = next ( iter ( dataloader )) with torch . no_grad (): layer_index = 0 inputs = inputs . to ( device ) inputs = inputs . view ( inputs . size ( 0 ), - 1 ) # We need to manually loop through the layers to save all activations for layer_index , layer in enumerate ( net . layers [: - 1 ]): inputs = layer ( inputs ) activations [ layer_index ] = inputs . view ( - 1 ) . cpu () . numpy () # Plotting columns = 4 rows = math . ceil ( len ( activations ) / columns ) fig , ax = plt . subplots ( rows , columns , figsize = ( columns * 2.7 , rows * 2.5 )) fig_index = 0 for key in activations : key_ax = ax [ fig_index // columns ][ fig_index % columns ] sns . histplot ( data = activations [ key ], bins = 50 , ax = key_ax , color = color , kde = True , stat = \"density\" ) key_ax . set_title ( f \"Layer { key } - { net . layers [ key ] . __class__ . __name__ } \" ) fig_index += 1 fig . suptitle ( f \"Activation distribution for activation function { net . hparams . act_fn } \" , fontsize = 14 ) fig . subplots_adjust ( hspace = 0.4 , wspace = 0.4 ) plt . show () plt . close ()","title":"visualize_activations()"},{"location":"reference/spotPython/utils/file/","text":"get_experiment_name ( prefix = '00' ) \u00b6 Returns a unique experiment name with a given prefix. Parameters: Name Type Description Default prefix str Prefix for the experiment name. Defaults to \u201c00\u201d. '00' Returns: Name Type Description str str Unique experiment name. Examples: >>> from spotPython.utils.file import get_experiment_name >>> get_experiment_name ( prefix = \"00\" ) 00_ubuntu_2021-08-31_14-30-00 Source code in spotPython/utils/file.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def get_experiment_name ( prefix : str = \"00\" ) -> str : \"\"\"Returns a unique experiment name with a given prefix. Args: prefix (str, optional): Prefix for the experiment name. Defaults to \"00\". Returns: str: Unique experiment name. Examples: >>> from spotPython.utils.file import get_experiment_name >>> get_experiment_name(prefix=\"00\") 00_ubuntu_2021-08-31_14-30-00 \"\"\" start_time = datetime . now ( tzlocal ()) HOSTNAME = socket . gethostname () . split ( \".\" )[ 0 ] experiment_name = prefix + \"_\" + HOSTNAME + \"_\" + str ( start_time ) . split ( \".\" , 1 )[ 0 ] . replace ( \" \" , \"_\" ) experiment_name = experiment_name . replace ( \":\" , \"-\" ) return experiment_name get_spot_tensorboard_path ( experiment_name ) \u00b6 Get the path to the spot tensorboard files. Parameters: Name Type Description Default experiment_name str The name of the experiment. required Returns: Name Type Description spot_tensorboard_path str The path to the folder where the spot tensorboard files are saved. Source code in spotPython/utils/file.py 80 81 82 83 84 85 86 87 88 89 def get_spot_tensorboard_path ( experiment_name ): \"\"\"Get the path to the spot tensorboard files. Args: experiment_name (str): The name of the experiment. Returns: spot_tensorboard_path (str): The path to the folder where the spot tensorboard files are saved. \"\"\" spot_tensorboard_path = os . environ . get ( \"PATH_TENSORBOARD\" , \"runs/spot_logs/\" ) spot_tensorboard_path = os . path . join ( spot_tensorboard_path , experiment_name ) return spot_tensorboard_path get_tensorboard_path ( fun_control ) \u00b6 Get the path to the tensorboard files. Parameters: Name Type Description Default fun_control dict The function control dictionary. required Returns: Name Type Description tensorboard_path str The path to the folder where the tensorboard files are saved. Source code in spotPython/utils/file.py 92 93 94 95 96 97 98 99 def get_tensorboard_path ( fun_control ): \"\"\"Get the path to the tensorboard files. Args: fun_control (dict): The function control dictionary. Returns: tensorboard_path (str): The path to the folder where the tensorboard files are saved. \"\"\" return fun_control [ \"TENSORBOARD_PATH\" ] load_data ( data_dir = './data' ) \u00b6 Loads the CIFAR10 dataset. Parameters: Name Type Description Default data_dir str Directory to save the data. Defaults to \u201c./data\u201d. './data' Returns: Name Type Description trainset torchvision . datasets . CIFAR10 Training dataset. Examples: >>> from spotPython.utils.file import load_data >>> trainset = load_data ( data_dir = \"./data\" ) Source code in spotPython/utils/file.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def load_data ( data_dir = \"./data\" ): \"\"\"Loads the CIFAR10 dataset. Args: data_dir (str, optional): Directory to save the data. Defaults to \"./data\". Returns: trainset (torchvision.datasets.CIFAR10): Training dataset. Examples: >>> from spotPython.utils.file import load_data >>> trainset = load_data(data_dir=\"./data\") \"\"\" transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = data_dir , train = True , download = True , transform = transform ) testset = torchvision . datasets . CIFAR10 ( root = data_dir , train = False , download = True , transform = transform ) return trainset , testset load_pickle ( filename ) \u00b6 Loads a pickle file. Add .pkl to the filename. Parameters: Name Type Description Default filename str Name of the pickle file. required Returns: Type Description object Loaded object. Examples: >>> from spotPython.utils.file import load_pickle >>> obj = load_pickle ( filename = \"obj.pkl\" ) Source code in spotPython/utils/file.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def load_pickle ( filename : str ): \"\"\"Loads a pickle file. Add .pkl to the filename. Args: filename (str): Name of the pickle file. Returns: (object): Loaded object. Examples: >>> from spotPython.utils.file import load_pickle >>> obj = load_pickle(filename=\"obj.pkl\") \"\"\" filename = filename + \".pkl\" with open ( filename , \"rb\" ) as f : obj = pickle . load ( f ) return obj save_pickle ( obj , filename ) \u00b6 Saves an object as a pickle file. Add .pkl to the filename. Parameters: Name Type Description Default obj object Object to be saved. required filename str Name of the pickle file. required Examples: >>> from spotPython.utils.file import save_pickle >>> save_pickle ( obj , filename = \"obj.pkl\" ) Source code in spotPython/utils/file.py 48 49 50 51 52 53 54 55 56 57 58 59 60 def save_pickle ( obj , filename : str ): \"\"\"Saves an object as a pickle file. Add .pkl to the filename. Args: obj (object): Object to be saved. filename (str): Name of the pickle file. Examples: >>> from spotPython.utils.file import save_pickle >>> save_pickle(obj, filename=\"obj.pkl\") \"\"\" filename = filename + \".pkl\" with open ( filename , \"wb\" ) as f : pickle . dump ( obj , f )","title":"file"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.get_experiment_name","text":"Returns a unique experiment name with a given prefix. Parameters: Name Type Description Default prefix str Prefix for the experiment name. Defaults to \u201c00\u201d. '00' Returns: Name Type Description str str Unique experiment name. Examples: >>> from spotPython.utils.file import get_experiment_name >>> get_experiment_name ( prefix = \"00\" ) 00_ubuntu_2021-08-31_14-30-00 Source code in spotPython/utils/file.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def get_experiment_name ( prefix : str = \"00\" ) -> str : \"\"\"Returns a unique experiment name with a given prefix. Args: prefix (str, optional): Prefix for the experiment name. Defaults to \"00\". Returns: str: Unique experiment name. Examples: >>> from spotPython.utils.file import get_experiment_name >>> get_experiment_name(prefix=\"00\") 00_ubuntu_2021-08-31_14-30-00 \"\"\" start_time = datetime . now ( tzlocal ()) HOSTNAME = socket . gethostname () . split ( \".\" )[ 0 ] experiment_name = prefix + \"_\" + HOSTNAME + \"_\" + str ( start_time ) . split ( \".\" , 1 )[ 0 ] . replace ( \" \" , \"_\" ) experiment_name = experiment_name . replace ( \":\" , \"-\" ) return experiment_name","title":"get_experiment_name()"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.get_spot_tensorboard_path","text":"Get the path to the spot tensorboard files. Parameters: Name Type Description Default experiment_name str The name of the experiment. required Returns: Name Type Description spot_tensorboard_path str The path to the folder where the spot tensorboard files are saved. Source code in spotPython/utils/file.py 80 81 82 83 84 85 86 87 88 89 def get_spot_tensorboard_path ( experiment_name ): \"\"\"Get the path to the spot tensorboard files. Args: experiment_name (str): The name of the experiment. Returns: spot_tensorboard_path (str): The path to the folder where the spot tensorboard files are saved. \"\"\" spot_tensorboard_path = os . environ . get ( \"PATH_TENSORBOARD\" , \"runs/spot_logs/\" ) spot_tensorboard_path = os . path . join ( spot_tensorboard_path , experiment_name ) return spot_tensorboard_path","title":"get_spot_tensorboard_path()"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.get_tensorboard_path","text":"Get the path to the tensorboard files. Parameters: Name Type Description Default fun_control dict The function control dictionary. required Returns: Name Type Description tensorboard_path str The path to the folder where the tensorboard files are saved. Source code in spotPython/utils/file.py 92 93 94 95 96 97 98 99 def get_tensorboard_path ( fun_control ): \"\"\"Get the path to the tensorboard files. Args: fun_control (dict): The function control dictionary. Returns: tensorboard_path (str): The path to the folder where the tensorboard files are saved. \"\"\" return fun_control [ \"TENSORBOARD_PATH\" ]","title":"get_tensorboard_path()"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.load_data","text":"Loads the CIFAR10 dataset. Parameters: Name Type Description Default data_dir str Directory to save the data. Defaults to \u201c./data\u201d. './data' Returns: Name Type Description trainset torchvision . datasets . CIFAR10 Training dataset. Examples: >>> from spotPython.utils.file import load_data >>> trainset = load_data ( data_dir = \"./data\" ) Source code in spotPython/utils/file.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def load_data ( data_dir = \"./data\" ): \"\"\"Loads the CIFAR10 dataset. Args: data_dir (str, optional): Directory to save the data. Defaults to \"./data\". Returns: trainset (torchvision.datasets.CIFAR10): Training dataset. Examples: >>> from spotPython.utils.file import load_data >>> trainset = load_data(data_dir=\"./data\") \"\"\" transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = data_dir , train = True , download = True , transform = transform ) testset = torchvision . datasets . CIFAR10 ( root = data_dir , train = False , download = True , transform = transform ) return trainset , testset","title":"load_data()"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.load_pickle","text":"Loads a pickle file. Add .pkl to the filename. Parameters: Name Type Description Default filename str Name of the pickle file. required Returns: Type Description object Loaded object. Examples: >>> from spotPython.utils.file import load_pickle >>> obj = load_pickle ( filename = \"obj.pkl\" ) Source code in spotPython/utils/file.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def load_pickle ( filename : str ): \"\"\"Loads a pickle file. Add .pkl to the filename. Args: filename (str): Name of the pickle file. Returns: (object): Loaded object. Examples: >>> from spotPython.utils.file import load_pickle >>> obj = load_pickle(filename=\"obj.pkl\") \"\"\" filename = filename + \".pkl\" with open ( filename , \"rb\" ) as f : obj = pickle . load ( f ) return obj","title":"load_pickle()"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.save_pickle","text":"Saves an object as a pickle file. Add .pkl to the filename. Parameters: Name Type Description Default obj object Object to be saved. required filename str Name of the pickle file. required Examples: >>> from spotPython.utils.file import save_pickle >>> save_pickle ( obj , filename = \"obj.pkl\" ) Source code in spotPython/utils/file.py 48 49 50 51 52 53 54 55 56 57 58 59 60 def save_pickle ( obj , filename : str ): \"\"\"Saves an object as a pickle file. Add .pkl to the filename. Args: obj (object): Object to be saved. filename (str): Name of the pickle file. Examples: >>> from spotPython.utils.file import save_pickle >>> save_pickle(obj, filename=\"obj.pkl\") \"\"\" filename = filename + \".pkl\" with open ( filename , \"wb\" ) as f : pickle . dump ( obj , f )","title":"save_pickle()"},{"location":"reference/spotPython/utils/init/","text":"check_and_create_dir ( path ) \u00b6 Check if the path exists and create it if it does not. Parameters: Name Type Description Default path str Path to the directory. required Returns: Type Description noneType None Examples: >>> fromspotPy . utils . init import check_and_create_dir >>> check_and_create_dir ( \"data/\" ) Source code in spotPython/utils/init.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def check_and_create_dir ( path ): \"\"\"Check if the path exists and create it if it does not. Args: path (str): Path to the directory. Returns: (noneType): None Examples: >>> fromspotPy.utils.init import check_and_create_dir >>> check_and_create_dir(\"data/\") \"\"\" if not isinstance ( path , str ): raise Exception ( \"path must be a string\" ) if not os . path . exists ( path ): os . makedirs ( path ) fun_control_init ( task = 'classification' , _L_in = None , _L_out = None , enable_progress_bar = False , spot_tensorboard_path = None , TENSORBOARD_CLEAN = False , num_workers = 0 , device = None , seed = 1234 , sigma = 0.0 ) \u00b6 Initialize fun_control dictionary. Parameters: Name Type Description Default task str The task to perform. It can be either \u201cclassification\u201d or \u201cregression\u201d. 'classification' _L_in int The number of input features. None _L_out int The number of output features. None enable_progress_bar bool Whether to enable the progress bar or not. False spot_tensorboard_path str The path to the folder where the spot tensorboard files are saved. If None, no spot tensorboard files are saved. None num_workers int The number of workers to use for the data loading. 0 device str The device to use for the training. It can be either \u201ccpu\u201d, \u201cmps\u201d, or \u201ccuda\u201d. None Returns: Name Type Description fun_control dict A dictionary containing the information about the core model, loss function, metrics, and the hyperparameters. Examples: >>> fun_control = fun_control_init ( _L_in = 64 , _L_out = 11 , num_workers = 0 , device = None ) >>> fun_control >>> { 'CHECKPOINT_PATH' : 'saved_models/' , 'DATASET_PATH': 'data/', 'RESULTS_PATH': 'results/', 'TENSORBOARD_PATH': 'runs/', '_L_in': 64, '_L_out': 11, 'data': None, 'data_dir': './data', 'device': None, 'enable_progress_bar': False, 'eval': None, 'k_folds': None, 'loss_function': None, 'metric_river': None, 'metric_sklearn': None, 'metric_torch': None, 'metric_params': {}, 'model_dict': {}, 'n_samples': None, 'num_workers': 0, 'optimizer': None, 'path': None, 'prep_model': None, 'save_model': False, 'seed': 1234, 'show_batch_interval': 1000000, 'shuffle': None, 'sigma': 0.0, 'target_column': None, 'train': None, 'test': None, 'task': 'classification', 'tensorboard_path': None, 'weights': 1.0, 'writer': None} Source code in spotPython/utils/init.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def fun_control_init ( task = \"classification\" , _L_in = None , _L_out = None , enable_progress_bar = False , spot_tensorboard_path = None , TENSORBOARD_CLEAN = False , num_workers = 0 , device = None , seed = 1234 , sigma = 0.0 , ): \"\"\"Initialize fun_control dictionary. Args: task (str): The task to perform. It can be either \"classification\" or \"regression\". _L_in (int): The number of input features. _L_out (int): The number of output features. enable_progress_bar (bool): Whether to enable the progress bar or not. spot_tensorboard_path (str): The path to the folder where the spot tensorboard files are saved. If None, no spot tensorboard files are saved. num_workers (int): The number of workers to use for the data loading. device (str): The device to use for the training. It can be either \"cpu\", \"mps\", or \"cuda\". Returns: fun_control (dict): A dictionary containing the information about the core model, loss function, metrics, and the hyperparameters. Examples: >>> fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None) >>> fun_control >>> {'CHECKPOINT_PATH': 'saved_models/', 'DATASET_PATH': 'data/', 'RESULTS_PATH': 'results/', 'TENSORBOARD_PATH': 'runs/', '_L_in': 64, '_L_out': 11, 'data': None, 'data_dir': './data', 'device': None, 'enable_progress_bar': False, 'eval': None, 'k_folds': None, 'loss_function': None, 'metric_river': None, 'metric_sklearn': None, 'metric_torch': None, 'metric_params': {}, 'model_dict': {}, 'n_samples': None, 'num_workers': 0, 'optimizer': None, 'path': None, 'prep_model': None, 'save_model': False, 'seed': 1234, 'show_batch_interval': 1000000, 'shuffle': None, 'sigma': 0.0, 'target_column': None, 'train': None, 'test': None, 'task': 'classification', 'tensorboard_path': None, 'weights': 1.0, 'writer': None} \"\"\" # Setting the seed L . seed_everything ( 42 ) # Path to the folder where the pretrained models are saved CHECKPOINT_PATH = os . environ . get ( \"PATH_CHECKPOINT\" , \"saved_models/\" ) os . makedirs ( CHECKPOINT_PATH , exist_ok = True ) # Path to the folder where the datasets are/should be downloaded (e.g. MNIST) DATASET_PATH = os . environ . get ( \"PATH_DATASETS\" , \"data/\" ) os . makedirs ( DATASET_PATH , exist_ok = True ) # Path to the folder where the results (plots, csv, etc.) are saved RESULTS_PATH = os . environ . get ( \"PATH_RESULTS\" , \"results/\" ) os . makedirs ( RESULTS_PATH , exist_ok = True ) # Path to the folder where the tensorboard files are saved TENSORBOARD_PATH = os . environ . get ( \"PATH_TENSORBOARD\" , \"runs/\" ) if TENSORBOARD_CLEAN : # if the folder \"runs\" exists, move it to \"runs_Y_M_D_H_M_S\" to avoid overwriting old tensorboard files if os . path . exists ( TENSORBOARD_PATH ): now = datetime . datetime . now () os . makedirs ( \"runs_OLD\" , exist_ok = True ) # use [:-1] to remove \"/\" from the end of the path TENSORBOARD_PATH_OLD = \"runs_OLD/\" + TENSORBOARD_PATH [: - 1 ] + \"_\" + now . strftime ( \"%Y_%m_ %d _%H_%M_%S\" ) os . rename ( TENSORBOARD_PATH [: - 1 ], TENSORBOARD_PATH_OLD ) os . makedirs ( TENSORBOARD_PATH , exist_ok = True ) if spot_tensorboard_path is not None : os . makedirs ( spot_tensorboard_path , exist_ok = True ) spot_writer = SummaryWriter ( spot_tensorboard_path ) else : spot_writer = None if not os . path . exists ( \"./figures\" ): os . makedirs ( \"./figures\" ) fun_control = { \"CHECKPOINT_PATH\" : CHECKPOINT_PATH , \"DATASET_PATH\" : DATASET_PATH , \"RESULTS_PATH\" : RESULTS_PATH , \"TENSORBOARD_PATH\" : TENSORBOARD_PATH , \"_L_in\" : _L_in , \"_L_out\" : _L_out , \"data\" : None , \"data_dir\" : \"./data\" , \"device\" : device , \"enable_progress_bar\" : enable_progress_bar , \"eval\" : None , \"k_folds\" : 3 , \"loss_function\" : None , \"metric_river\" : None , \"metric_sklearn\" : None , \"metric_torch\" : None , \"metric_params\" : {}, \"model_dict\" : {}, \"n_samples\" : None , \"num_workers\" : num_workers , \"optimizer\" : None , \"path\" : None , \"prep_model\" : None , \"save_model\" : False , \"seed\" : seed , \"show_batch_interval\" : 1_000_000 , \"shuffle\" : None , \"sigma\" : sigma , \"target_column\" : None , \"train\" : None , \"test\" : None , \"task\" : task , \"spot_tensorboard_path\" : spot_tensorboard_path , \"weights\" : 1.0 , \"spot_writer\" : spot_writer , } return fun_control","title":"init"},{"location":"reference/spotPython/utils/init/#spotPython.utils.init.check_and_create_dir","text":"Check if the path exists and create it if it does not. Parameters: Name Type Description Default path str Path to the directory. required Returns: Type Description noneType None Examples: >>> fromspotPy . utils . init import check_and_create_dir >>> check_and_create_dir ( \"data/\" ) Source code in spotPython/utils/init.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def check_and_create_dir ( path ): \"\"\"Check if the path exists and create it if it does not. Args: path (str): Path to the directory. Returns: (noneType): None Examples: >>> fromspotPy.utils.init import check_and_create_dir >>> check_and_create_dir(\"data/\") \"\"\" if not isinstance ( path , str ): raise Exception ( \"path must be a string\" ) if not os . path . exists ( path ): os . makedirs ( path )","title":"check_and_create_dir()"},{"location":"reference/spotPython/utils/init/#spotPython.utils.init.fun_control_init","text":"Initialize fun_control dictionary. Parameters: Name Type Description Default task str The task to perform. It can be either \u201cclassification\u201d or \u201cregression\u201d. 'classification' _L_in int The number of input features. None _L_out int The number of output features. None enable_progress_bar bool Whether to enable the progress bar or not. False spot_tensorboard_path str The path to the folder where the spot tensorboard files are saved. If None, no spot tensorboard files are saved. None num_workers int The number of workers to use for the data loading. 0 device str The device to use for the training. It can be either \u201ccpu\u201d, \u201cmps\u201d, or \u201ccuda\u201d. None Returns: Name Type Description fun_control dict A dictionary containing the information about the core model, loss function, metrics, and the hyperparameters. Examples: >>> fun_control = fun_control_init ( _L_in = 64 , _L_out = 11 , num_workers = 0 , device = None ) >>> fun_control >>> { 'CHECKPOINT_PATH' : 'saved_models/' , 'DATASET_PATH': 'data/', 'RESULTS_PATH': 'results/', 'TENSORBOARD_PATH': 'runs/', '_L_in': 64, '_L_out': 11, 'data': None, 'data_dir': './data', 'device': None, 'enable_progress_bar': False, 'eval': None, 'k_folds': None, 'loss_function': None, 'metric_river': None, 'metric_sklearn': None, 'metric_torch': None, 'metric_params': {}, 'model_dict': {}, 'n_samples': None, 'num_workers': 0, 'optimizer': None, 'path': None, 'prep_model': None, 'save_model': False, 'seed': 1234, 'show_batch_interval': 1000000, 'shuffle': None, 'sigma': 0.0, 'target_column': None, 'train': None, 'test': None, 'task': 'classification', 'tensorboard_path': None, 'weights': 1.0, 'writer': None} Source code in spotPython/utils/init.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def fun_control_init ( task = \"classification\" , _L_in = None , _L_out = None , enable_progress_bar = False , spot_tensorboard_path = None , TENSORBOARD_CLEAN = False , num_workers = 0 , device = None , seed = 1234 , sigma = 0.0 , ): \"\"\"Initialize fun_control dictionary. Args: task (str): The task to perform. It can be either \"classification\" or \"regression\". _L_in (int): The number of input features. _L_out (int): The number of output features. enable_progress_bar (bool): Whether to enable the progress bar or not. spot_tensorboard_path (str): The path to the folder where the spot tensorboard files are saved. If None, no spot tensorboard files are saved. num_workers (int): The number of workers to use for the data loading. device (str): The device to use for the training. It can be either \"cpu\", \"mps\", or \"cuda\". Returns: fun_control (dict): A dictionary containing the information about the core model, loss function, metrics, and the hyperparameters. Examples: >>> fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None) >>> fun_control >>> {'CHECKPOINT_PATH': 'saved_models/', 'DATASET_PATH': 'data/', 'RESULTS_PATH': 'results/', 'TENSORBOARD_PATH': 'runs/', '_L_in': 64, '_L_out': 11, 'data': None, 'data_dir': './data', 'device': None, 'enable_progress_bar': False, 'eval': None, 'k_folds': None, 'loss_function': None, 'metric_river': None, 'metric_sklearn': None, 'metric_torch': None, 'metric_params': {}, 'model_dict': {}, 'n_samples': None, 'num_workers': 0, 'optimizer': None, 'path': None, 'prep_model': None, 'save_model': False, 'seed': 1234, 'show_batch_interval': 1000000, 'shuffle': None, 'sigma': 0.0, 'target_column': None, 'train': None, 'test': None, 'task': 'classification', 'tensorboard_path': None, 'weights': 1.0, 'writer': None} \"\"\" # Setting the seed L . seed_everything ( 42 ) # Path to the folder where the pretrained models are saved CHECKPOINT_PATH = os . environ . get ( \"PATH_CHECKPOINT\" , \"saved_models/\" ) os . makedirs ( CHECKPOINT_PATH , exist_ok = True ) # Path to the folder where the datasets are/should be downloaded (e.g. MNIST) DATASET_PATH = os . environ . get ( \"PATH_DATASETS\" , \"data/\" ) os . makedirs ( DATASET_PATH , exist_ok = True ) # Path to the folder where the results (plots, csv, etc.) are saved RESULTS_PATH = os . environ . get ( \"PATH_RESULTS\" , \"results/\" ) os . makedirs ( RESULTS_PATH , exist_ok = True ) # Path to the folder where the tensorboard files are saved TENSORBOARD_PATH = os . environ . get ( \"PATH_TENSORBOARD\" , \"runs/\" ) if TENSORBOARD_CLEAN : # if the folder \"runs\" exists, move it to \"runs_Y_M_D_H_M_S\" to avoid overwriting old tensorboard files if os . path . exists ( TENSORBOARD_PATH ): now = datetime . datetime . now () os . makedirs ( \"runs_OLD\" , exist_ok = True ) # use [:-1] to remove \"/\" from the end of the path TENSORBOARD_PATH_OLD = \"runs_OLD/\" + TENSORBOARD_PATH [: - 1 ] + \"_\" + now . strftime ( \"%Y_%m_ %d _%H_%M_%S\" ) os . rename ( TENSORBOARD_PATH [: - 1 ], TENSORBOARD_PATH_OLD ) os . makedirs ( TENSORBOARD_PATH , exist_ok = True ) if spot_tensorboard_path is not None : os . makedirs ( spot_tensorboard_path , exist_ok = True ) spot_writer = SummaryWriter ( spot_tensorboard_path ) else : spot_writer = None if not os . path . exists ( \"./figures\" ): os . makedirs ( \"./figures\" ) fun_control = { \"CHECKPOINT_PATH\" : CHECKPOINT_PATH , \"DATASET_PATH\" : DATASET_PATH , \"RESULTS_PATH\" : RESULTS_PATH , \"TENSORBOARD_PATH\" : TENSORBOARD_PATH , \"_L_in\" : _L_in , \"_L_out\" : _L_out , \"data\" : None , \"data_dir\" : \"./data\" , \"device\" : device , \"enable_progress_bar\" : enable_progress_bar , \"eval\" : None , \"k_folds\" : 3 , \"loss_function\" : None , \"metric_river\" : None , \"metric_sklearn\" : None , \"metric_torch\" : None , \"metric_params\" : {}, \"model_dict\" : {}, \"n_samples\" : None , \"num_workers\" : num_workers , \"optimizer\" : None , \"path\" : None , \"prep_model\" : None , \"save_model\" : False , \"seed\" : seed , \"show_batch_interval\" : 1_000_000 , \"shuffle\" : None , \"sigma\" : sigma , \"target_column\" : None , \"train\" : None , \"test\" : None , \"task\" : task , \"spot_tensorboard_path\" : spot_tensorboard_path , \"weights\" : 1.0 , \"spot_writer\" : spot_writer , } return fun_control","title":"fun_control_init()"},{"location":"reference/spotPython/utils/metrics/","text":"apk ( actual , predicted , k = 10 ) \u00b6 Computes the average precision at k. This function computes the average precision at k between two lists of items. Parameters list A list of elements that are to be predicted (order doesn\u2019t matter) list A list of predicted elements (order does matter) int, optional The maximum number of predicted elements Returns \u00b6 double The average precision at k over the input lists Source code in spotPython/utils/metrics.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def apk ( actual , predicted , k = 10 ): \"\"\" Computes the average precision at k. This function computes the average precision at k between two lists of items. Parameters ---------- actual : list A list of elements that are to be predicted (order doesn't matter) predicted : list A list of predicted elements (order does matter) k : int, optional The maximum number of predicted elements Returns ------- score : double The average precision at k over the input lists \"\"\" if len ( predicted ) > k : predicted = predicted [: k ] score = 0.0 num_hits = 0.0 for i , p in enumerate ( predicted ): if p in actual and p not in predicted [: i ]: num_hits += 1.0 score += num_hits / ( i + 1.0 ) if not actual : return 0.0 return score / min ( len ( actual ), k ) mapk ( actual , predicted , k = 10 ) \u00b6 Computes the mean average precision at k. This function computes the mean average precision at k between two lists of lists of items. Parameters list A list of lists of elements that are to be predicted (order doesn\u2019t matter in the lists) list A list of lists of predicted elements (order matters in the lists) int, optional The maximum number of predicted elements Returns \u00b6 double The mean average precision at k over the input lists Source code in spotPython/utils/metrics.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def mapk ( actual , predicted , k = 10 ): \"\"\" Computes the mean average precision at k. This function computes the mean average precision at k between two lists of lists of items. Parameters ---------- actual : list A list of lists of elements that are to be predicted (order doesn't matter in the lists) predicted : list A list of lists of predicted elements (order matters in the lists) k : int, optional The maximum number of predicted elements Returns ------- score : double The mean average precision at k over the input lists \"\"\" return np . mean ([ apk ( a , p , k ) for a , p in zip ( actual , predicted )]) mapk_score ( y_true , y_pred , k = 3 ) \u00b6 Wrapper for mapk func using numpy arrays Args: y_true (np.array): array of true values y_pred (np.array): array of predicted values k (int): number of predictions Returns: Name Type Description score float mean average precision at k Examples: >>> y_true = np . array ([ 0 , 1 , 2 , 2 ]) >>> y_pred = np . array ([[ 0.5 , 0.2 , 0.2 ], # 0 is in top 2 [0.3, 0.4, 0.2], # 1 is in top 2 [0.2, 0.4, 0.3], # 2 is in top 2 [0.7, 0.2, 0.1]]) # 2 isn't in top 2 >>> mapk_score ( y_true , y_pred , k = 1 ) 0.25 >>> mapk_score ( y_true , y_pred , k = 2 ) 0.375 >>> mapk_score ( y_true , y_pred , k = 3 ) 0.4583333333333333 >>> mapk_score ( y_true , y_pred , k = 4 ) 0.4583333333333333 Source code in spotPython/utils/metrics.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def mapk_score ( y_true , y_pred , k = 3 ): \"\"\"Wrapper for mapk func using numpy arrays Args: y_true (np.array): array of true values y_pred (np.array): array of predicted values k (int): number of predictions Returns: score (float): mean average precision at k Examples: >>> y_true = np.array([0, 1, 2, 2]) >>> y_pred = np.array([[0.5, 0.2, 0.2], # 0 is in top 2 [0.3, 0.4, 0.2], # 1 is in top 2 [0.2, 0.4, 0.3], # 2 is in top 2 [0.7, 0.2, 0.1]]) # 2 isn't in top 2 >>> mapk_score(y_true, y_pred, k=1) 0.25 >>> mapk_score(y_true, y_pred, k=2) 0.375 >>> mapk_score(y_true, y_pred, k=3) 0.4583333333333333 >>> mapk_score(y_true, y_pred, k=4) 0.4583333333333333 \"\"\" y_true = series_to_array ( y_true ) sorted_prediction_ids = np . argsort ( - y_pred , axis = 1 ) top_k_prediction_ids = sorted_prediction_ids [:, : k ] score = mapk ( y_true . reshape ( - 1 , 1 ), top_k_prediction_ids , k = k ) return score mapk_scorer ( estimator , X , y ) \u00b6 Scorer for mean average precision at k. This function computes the mean average precision at k between two lists of lists of items. Parameters sklearn estimator The estimator to be used for prediction. array-like of shape (n_samples, n_features) The input samples. array-like of shape (n_samples,) The target values. Returns \u00b6 double The mean average precision at k over the input lists Source code in spotPython/utils/metrics.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def mapk_scorer ( estimator , X , y ): \"\"\" Scorer for mean average precision at k. This function computes the mean average precision at k between two lists of lists of items. Parameters ---------- estimator : sklearn estimator The estimator to be used for prediction. X : array-like of shape (n_samples, n_features) The input samples. y : array-like of shape (n_samples,) The target values. Returns ------- score : double The mean average precision at k over the input lists \"\"\" y_pred = estimator . predict_proba ( X ) score = mapk_score ( y , y_pred , k = 3 ) return score","title":"metrics"},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.apk","text":"Computes the average precision at k. This function computes the average precision at k between two lists of items. Parameters list A list of elements that are to be predicted (order doesn\u2019t matter) list A list of predicted elements (order does matter) int, optional The maximum number of predicted elements","title":"apk()"},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.apk--returns","text":"double The average precision at k over the input lists Source code in spotPython/utils/metrics.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def apk ( actual , predicted , k = 10 ): \"\"\" Computes the average precision at k. This function computes the average precision at k between two lists of items. Parameters ---------- actual : list A list of elements that are to be predicted (order doesn't matter) predicted : list A list of predicted elements (order does matter) k : int, optional The maximum number of predicted elements Returns ------- score : double The average precision at k over the input lists \"\"\" if len ( predicted ) > k : predicted = predicted [: k ] score = 0.0 num_hits = 0.0 for i , p in enumerate ( predicted ): if p in actual and p not in predicted [: i ]: num_hits += 1.0 score += num_hits / ( i + 1.0 ) if not actual : return 0.0 return score / min ( len ( actual ), k )","title":"Returns"},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.mapk","text":"Computes the mean average precision at k. This function computes the mean average precision at k between two lists of lists of items. Parameters list A list of lists of elements that are to be predicted (order doesn\u2019t matter in the lists) list A list of lists of predicted elements (order matters in the lists) int, optional The maximum number of predicted elements","title":"mapk()"},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.mapk--returns","text":"double The mean average precision at k over the input lists Source code in spotPython/utils/metrics.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def mapk ( actual , predicted , k = 10 ): \"\"\" Computes the mean average precision at k. This function computes the mean average precision at k between two lists of lists of items. Parameters ---------- actual : list A list of lists of elements that are to be predicted (order doesn't matter in the lists) predicted : list A list of lists of predicted elements (order matters in the lists) k : int, optional The maximum number of predicted elements Returns ------- score : double The mean average precision at k over the input lists \"\"\" return np . mean ([ apk ( a , p , k ) for a , p in zip ( actual , predicted )])","title":"Returns"},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.mapk_score","text":"Wrapper for mapk func using numpy arrays Args: y_true (np.array): array of true values y_pred (np.array): array of predicted values k (int): number of predictions Returns: Name Type Description score float mean average precision at k Examples: >>> y_true = np . array ([ 0 , 1 , 2 , 2 ]) >>> y_pred = np . array ([[ 0.5 , 0.2 , 0.2 ], # 0 is in top 2 [0.3, 0.4, 0.2], # 1 is in top 2 [0.2, 0.4, 0.3], # 2 is in top 2 [0.7, 0.2, 0.1]]) # 2 isn't in top 2 >>> mapk_score ( y_true , y_pred , k = 1 ) 0.25 >>> mapk_score ( y_true , y_pred , k = 2 ) 0.375 >>> mapk_score ( y_true , y_pred , k = 3 ) 0.4583333333333333 >>> mapk_score ( y_true , y_pred , k = 4 ) 0.4583333333333333 Source code in spotPython/utils/metrics.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def mapk_score ( y_true , y_pred , k = 3 ): \"\"\"Wrapper for mapk func using numpy arrays Args: y_true (np.array): array of true values y_pred (np.array): array of predicted values k (int): number of predictions Returns: score (float): mean average precision at k Examples: >>> y_true = np.array([0, 1, 2, 2]) >>> y_pred = np.array([[0.5, 0.2, 0.2], # 0 is in top 2 [0.3, 0.4, 0.2], # 1 is in top 2 [0.2, 0.4, 0.3], # 2 is in top 2 [0.7, 0.2, 0.1]]) # 2 isn't in top 2 >>> mapk_score(y_true, y_pred, k=1) 0.25 >>> mapk_score(y_true, y_pred, k=2) 0.375 >>> mapk_score(y_true, y_pred, k=3) 0.4583333333333333 >>> mapk_score(y_true, y_pred, k=4) 0.4583333333333333 \"\"\" y_true = series_to_array ( y_true ) sorted_prediction_ids = np . argsort ( - y_pred , axis = 1 ) top_k_prediction_ids = sorted_prediction_ids [:, : k ] score = mapk ( y_true . reshape ( - 1 , 1 ), top_k_prediction_ids , k = k ) return score","title":"mapk_score()"},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.mapk_scorer","text":"Scorer for mean average precision at k. This function computes the mean average precision at k between two lists of lists of items. Parameters sklearn estimator The estimator to be used for prediction. array-like of shape (n_samples, n_features) The input samples. array-like of shape (n_samples,) The target values.","title":"mapk_scorer()"},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.mapk_scorer--returns","text":"double The mean average precision at k over the input lists Source code in spotPython/utils/metrics.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def mapk_scorer ( estimator , X , y ): \"\"\" Scorer for mean average precision at k. This function computes the mean average precision at k between two lists of lists of items. Parameters ---------- estimator : sklearn estimator The estimator to be used for prediction. X : array-like of shape (n_samples, n_features) The input samples. y : array-like of shape (n_samples,) The target values. Returns ------- score : double The mean average precision at k over the input lists \"\"\" y_pred = estimator . predict_proba ( X ) score = mapk_score ( y , y_pred , k = 3 ) return score","title":"Returns"},{"location":"reference/spotPython/utils/progress/","text":"progress_bar ( progress , bar_length = 10 , message = 'spotPython tuning:' , y = None ) \u00b6 Displays or updates a console progress bar. Parameters: Name Type Description Default progress float a float between 0 and 1. Any int will be converted to a float. A value under 0 represents a halt. A value at 1 or bigger represents 100%. required bar_length int length of the progress bar 10 message str message text to display 'spotPython tuning:' Source code in spotPython/utils/progress.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def progress_bar ( progress : float , bar_length : int = 10 , message : str = \"spotPython tuning:\" , y = None ) -> None : \"\"\" Displays or updates a console progress bar. Args: progress (float): a float between 0 and 1. Any int will be converted to a float. A value under 0 represents a halt. A value at 1 or bigger represents 100%. bar_length (int): length of the progress bar message (str): message text to display \"\"\" status = \"\" if y is not None : message = f \" { message } { y } \" if progress < 0 : progress = 0 status = \"Halt... \\r\\n \" elif progress >= 1 : progress = 1 status = \"Done... \\r\\n \" block = int ( round ( bar_length * progress )) text = f \" { message } [ { '#' * block + '-' * ( bar_length - block ) } ] { progress * 100 : .2f } % { status } \\r\\n \" stdout . write ( text ) stdout . flush ()","title":"progress"},{"location":"reference/spotPython/utils/progress/#spotPython.utils.progress.progress_bar","text":"Displays or updates a console progress bar. Parameters: Name Type Description Default progress float a float between 0 and 1. Any int will be converted to a float. A value under 0 represents a halt. A value at 1 or bigger represents 100%. required bar_length int length of the progress bar 10 message str message text to display 'spotPython tuning:' Source code in spotPython/utils/progress.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def progress_bar ( progress : float , bar_length : int = 10 , message : str = \"spotPython tuning:\" , y = None ) -> None : \"\"\" Displays or updates a console progress bar. Args: progress (float): a float between 0 and 1. Any int will be converted to a float. A value under 0 represents a halt. A value at 1 or bigger represents 100%. bar_length (int): length of the progress bar message (str): message text to display \"\"\" status = \"\" if y is not None : message = f \" { message } { y } \" if progress < 0 : progress = 0 status = \"Halt... \\r\\n \" elif progress >= 1 : progress = 1 status = \"Done... \\r\\n \" block = int ( round ( bar_length * progress )) text = f \" { message } [ { '#' * block + '-' * ( bar_length - block ) } ] { progress * 100 : .2f } % { status } \\r\\n \" stdout . write ( text ) stdout . flush ()","title":"progress_bar()"},{"location":"reference/spotPython/utils/repair/","text":"remove_nan ( X , y ) \u00b6 Remove rows from X and y where y contains NaN values. Parameters: Name Type Description Default X numpy . ndarray X array required y numpy . ndarray y array required Returns: Type Description Tuple [ np . ndarray , np . ndarray ] Tuple[numpy.ndarray, numpy.ndarray]: X and y arrays with rows containing NaN values in y removed Examples: >>> X = np . array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]]) >>> y = np . array ([ 1 , np . nan , 2 ]) >>> remove_nan ( X , y ) (array([[1, 2], [5, 6]]), array([1., 2.])) Source code in spotPython/utils/repair.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def remove_nan ( X : np . ndarray , y : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Remove rows from X and y where y contains NaN values. Args: X (numpy.ndarray): X array y (numpy.ndarray): y array Returns: Tuple[numpy.ndarray, numpy.ndarray]: X and y arrays with rows containing NaN values in y removed Examples: >>> X = np.array([[1, 2], [3, 4], [5, 6]]) >>> y = np.array([1, np.nan, 2]) >>> remove_nan(X, y) (array([[1, 2], [5, 6]]), array([1., 2.])) \"\"\" ind = np . isfinite ( y ) y = y [ ind ] X = X [ ind , :] return X , y repair_non_numeric ( X , var_type ) \u00b6 Round non-numeric values to integers. This applies to all variables except for \u201cnum\u201d and \u201cfloat\u201d. Parameters: Name Type Description Default X numpy . ndarray X array required var_type list list with type information required Returns: Type Description np . ndarray numpy.ndarray: X array with non-numeric values rounded to integers Examples: >>> X = np . array ([[ 1.2 , 2.3 ], [ 3.4 , 4.5 ]]) >>> var_type = [ \"num\" , \"factor\" ] >>> repair_non_numeric ( X , var_type ) array([[1., 2.], [3., 4.]]) Source code in spotPython/utils/repair.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def repair_non_numeric ( X : np . ndarray , var_type : List [ str ]) -> np . ndarray : \"\"\" Round non-numeric values to integers. This applies to all variables except for \"num\" and \"float\". Args: X (numpy.ndarray): X array var_type (list): list with type information Returns: numpy.ndarray: X array with non-numeric values rounded to integers Examples: >>> X = np.array([[1.2, 2.3], [3.4, 4.5]]) >>> var_type = [\"num\", \"factor\"] >>> repair_non_numeric(X, var_type) array([[1., 2.], [3., 4.]]) \"\"\" mask = np . isin ( var_type , [ \"num\" , \"float\" ], invert = True ) X [:, mask ] = np . around ( X [:, mask ]) return X","title":"repair"},{"location":"reference/spotPython/utils/repair/#spotPython.utils.repair.remove_nan","text":"Remove rows from X and y where y contains NaN values. Parameters: Name Type Description Default X numpy . ndarray X array required y numpy . ndarray y array required Returns: Type Description Tuple [ np . ndarray , np . ndarray ] Tuple[numpy.ndarray, numpy.ndarray]: X and y arrays with rows containing NaN values in y removed Examples: >>> X = np . array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]]) >>> y = np . array ([ 1 , np . nan , 2 ]) >>> remove_nan ( X , y ) (array([[1, 2], [5, 6]]), array([1., 2.])) Source code in spotPython/utils/repair.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def remove_nan ( X : np . ndarray , y : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Remove rows from X and y where y contains NaN values. Args: X (numpy.ndarray): X array y (numpy.ndarray): y array Returns: Tuple[numpy.ndarray, numpy.ndarray]: X and y arrays with rows containing NaN values in y removed Examples: >>> X = np.array([[1, 2], [3, 4], [5, 6]]) >>> y = np.array([1, np.nan, 2]) >>> remove_nan(X, y) (array([[1, 2], [5, 6]]), array([1., 2.])) \"\"\" ind = np . isfinite ( y ) y = y [ ind ] X = X [ ind , :] return X , y","title":"remove_nan()"},{"location":"reference/spotPython/utils/repair/#spotPython.utils.repair.repair_non_numeric","text":"Round non-numeric values to integers. This applies to all variables except for \u201cnum\u201d and \u201cfloat\u201d. Parameters: Name Type Description Default X numpy . ndarray X array required var_type list list with type information required Returns: Type Description np . ndarray numpy.ndarray: X array with non-numeric values rounded to integers Examples: >>> X = np . array ([[ 1.2 , 2.3 ], [ 3.4 , 4.5 ]]) >>> var_type = [ \"num\" , \"factor\" ] >>> repair_non_numeric ( X , var_type ) array([[1., 2.], [3., 4.]]) Source code in spotPython/utils/repair.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def repair_non_numeric ( X : np . ndarray , var_type : List [ str ]) -> np . ndarray : \"\"\" Round non-numeric values to integers. This applies to all variables except for \"num\" and \"float\". Args: X (numpy.ndarray): X array var_type (list): list with type information Returns: numpy.ndarray: X array with non-numeric values rounded to integers Examples: >>> X = np.array([[1.2, 2.3], [3.4, 4.5]]) >>> var_type = [\"num\", \"factor\"] >>> repair_non_numeric(X, var_type) array([[1., 2.], [3., 4.]]) \"\"\" mask = np . isin ( var_type , [ \"num\" , \"float\" ], invert = True ) X [:, mask ] = np . around ( X [:, mask ]) return X","title":"repair_non_numeric()"},{"location":"reference/spotPython/utils/transform/","text":"scale ( X , lower , upper ) \u00b6 Sample scaling from unit hypercube to different bounds. Converts a sample from [0, 1) to [a, b) . The following transformation is used: (b - a) * X + a Note equal lower and upper bounds are feasible. Parameters: Name Type Description Default X array Sample to scale. required lower array lower bound of transformed data. required upper array upper bounds of transformed data. required Returns: Type Description array Scaled sample. Examples: Transform three samples in the unit hypercube to (lower, upper) bounds: >>> import numpy as np >>> from scipy.stats import qmc >>> from spotPython.utils.transform import scale >>> lower = np . array ([ 6 , 0 ]) >>> upper = np . array ([ 6 , 5 ]) >>> sample = np . array ([[ 0.5 , 0.75 ], >>> [ 0.5 , 0.5 ], >>> [ 0.75 , 0.25 ]]) >>> scale ( sample , lower , upper ) Source code in spotPython/utils/transform.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def scale ( X : np . ndarray , lower : np . ndarray , upper : np . ndarray ) -> np . ndarray : \"\"\" Sample scaling from unit hypercube to different bounds. Converts a sample from `[0, 1)` to `[a, b)`. The following transformation is used: `(b - a) * X + a` Note: equal lower and upper bounds are feasible. Args: X (array): Sample to scale. lower (array): lower bound of transformed data. upper (array): upper bounds of transformed data. Returns: (array): Scaled sample. Examples: Transform three samples in the unit hypercube to (lower, upper) bounds: >>> import numpy as np >>> from scipy.stats import qmc >>> from spotPython.utils.transform import scale >>> lower = np.array([6, 0]) >>> upper = np.array([6, 5]) >>> sample = np.array([[0.5 , 0.75], >>> [0.5 , 0.5], >>> [0.75, 0.25]]) >>> scale(sample, lower, upper) \"\"\" # Checking that X is within (0,1) interval if ( X . max () > 1.0 ) or ( X . min () < 0.0 ): raise ValueError ( \"Sample is not in unit hypercube\" ) # Vectorized scaling operation X = ( upper - lower ) * X + lower # Handling case where lower == upper X [:, lower == upper ] = lower [ lower == upper ] return X transform_hyper_parameter_values ( fun_control , hyper_parameter_values ) \u00b6 Transform the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \u201cint\u201d, or \u201cfloat\u201d or \u201cnum\u201d. Let fun_control = {\u201ccore_model_hyper_dict\u201d:{ \u201cleaf_prediction\u201d: { \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d], \u201ctype\u201d: \u201cfactor\u201d, \u201cdefault\u201d: \u201cmean\u201d, \u201ccore_model_parameter_type\u201d: \u201cstr\u201d}, \u201cmax_depth\u201d: { \u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 20, \u201ctransform\u201d: \u201ctransform_power_2\u201d, \u201clower\u201d: 2, \u201cupper\u201d: 20}}} and v = {\u2018max_depth\u2019: 20,\u2019leaf_prediction\u2019: \u2018mean\u2019} and def transform_power_2(x): return 2**x. The function takes fun_control and v as input and returns a dictionary with the same structure as v. The function transforms the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \u201cint\u201d, or \u201cfloat\u201d or \u201cnum\u201d. For example, transform_hyper_parameter_values(fun_control, v) returns {\u2018max_depth\u2019: 1048576, \u2018leaf_prediction\u2019: \u2018mean\u2019}. Parameters: Name Type Description Default fun_control dict A dictionary containing the information about the core model and the hyperparameters. required hyper_parameter_values dict A dictionary containing the values of the hyperparameters. required Returns: Type Description dict A dictionary containing the values of the hyperparameters. Examples: >>> import copy from spotPython.utils.prepare import transform_hyper_parameter_values fun_control = { \"core_model_hyper_dict\": { \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"max_depth\": {\"type\": \"int\", \"default\": 20 \"transform\": \"transform_power_2\", \"lower\": 2, \"upper\": 20}}} hyper_parameter_values = {'max_depth': 20, 'leaf_prediction': 'mean'} transform_hyper_parameter_values(fun_control, hyper_parameter_values) {'max_depth': 1048576, 'leaf_prediction': 'mean'} Source code in spotPython/utils/transform.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def transform_hyper_parameter_values ( fun_control , hyper_parameter_values ): \"\"\" Transform the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \"int\", or \"float\" or \"num\". Let fun_control = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"max_depth\": { \"type\": \"int\", \"default\": 20, \"transform\": \"transform_power_2\", \"lower\": 2, \"upper\": 20}}} and v = {'max_depth': 20,'leaf_prediction': 'mean'} and def transform_power_2(x): return 2**x. The function takes fun_control and v as input and returns a dictionary with the same structure as v. The function transforms the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \"int\", or \"float\" or \"num\". For example, transform_hyper_parameter_values(fun_control, v) returns {'max_depth': 1048576, 'leaf_prediction': 'mean'}. Args: fun_control (dict): A dictionary containing the information about the core model and the hyperparameters. hyper_parameter_values (dict): A dictionary containing the values of the hyperparameters. Returns: (dict): A dictionary containing the values of the hyperparameters. Examples: >>> import copy from spotPython.utils.prepare import transform_hyper_parameter_values fun_control = { \"core_model_hyper_dict\": { \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"max_depth\": {\"type\": \"int\", \"default\": 20 \"transform\": \"transform_power_2\", \"lower\": 2, \"upper\": 20}}} hyper_parameter_values = {'max_depth': 20, 'leaf_prediction': 'mean'} transform_hyper_parameter_values(fun_control, hyper_parameter_values) {'max_depth': 1048576, 'leaf_prediction': 'mean'} \"\"\" hyper_parameter_values = copy . deepcopy ( hyper_parameter_values ) for key , value in hyper_parameter_values . items (): if ( fun_control [ \"core_model_hyper_dict\" ][ key ][ \"type\" ] in [ \"int\" , \"float\" , \"num\" , \"factor\" ] and fun_control [ \"core_model_hyper_dict\" ][ key ][ \"transform\" ] != \"None\" ): hyper_parameter_values [ key ] = eval ( fun_control [ \"core_model_hyper_dict\" ][ key ][ \"transform\" ])( value ) return hyper_parameter_values transform_none_to_None ( x ) \u00b6 Transformations for hyperparameters of type None. Parameters: Name Type Description Default x str The string to transform. required Returns: Type Description str The transformed string. Examples: >>> from spotPython.utils.transform import transform_none_to_None >>> transform_none_to_None ( \"none\" ) None Note Needed for sklearn.linear_model.LogisticRegression Source code in spotPython/utils/transform.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def transform_none_to_None ( x ): \"\"\" Transformations for hyperparameters of type None. Args: x (str): The string to transform. Returns: (str): The transformed string. Examples: >>> from spotPython.utils.transform import transform_none_to_None >>> transform_none_to_None(\"none\") None Note: Needed for sklearn.linear_model.LogisticRegression \"\"\" if x == \"none\" : return None else : return x transform_power ( base , x , as_int = False ) \u00b6 Raises a given base to the power of x. Parameters: Name Type Description Default base int The base to raise to the power of x. required x int The exponent. required as_int bool If True, returns the result as an integer. False Returns: Type Description float The result of raising the base to the power of x. Examples: >>> from spotPython.utils.transform import transform_power >>> transform_power ( 2 , 3 ) 8 Source code in spotPython/utils/transform.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def transform_power ( base : int , x : int , as_int : bool = False ) -> float : \"\"\" Raises a given base to the power of x. Args: base (int): The base to raise to the power of x. x (int): The exponent. as_int (bool): If True, returns the result as an integer. Returns: (float): The result of raising the base to the power of x. Examples: >>> from spotPython.utils.transform import transform_power >>> transform_power(2, 3) 8 \"\"\" result = base ** x if as_int : result = int ( result ) return result transform_power_10 ( x ) \u00b6 Transformations for hyperparameters of type float. Parameters: Name Type Description Default x float The exponent. required Returns: Type Description float The result of raising 10 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_10 >>> transform_power_10 ( 3 ) 1000 Source code in spotPython/utils/transform.py 91 92 93 94 95 96 97 98 99 100 101 102 def transform_power_10 ( x ): \"\"\"Transformations for hyperparameters of type float. Args: x (float): The exponent. Returns: (float): The result of raising 10 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_10 >>> transform_power_10(3) 1000 \"\"\" return 10 ** x transform_power_10_int ( x ) \u00b6 Transformations for hyperparameters of type int. Parameters: Name Type Description Default x int The exponent. required Returns: Type Description int The result of raising 10 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_10_int >>> transform_power_10_int ( 3 ) 1000 Source code in spotPython/utils/transform.py 63 64 65 66 67 68 69 70 71 72 73 74 def transform_power_10_int ( x : int ) -> int : \"\"\"Transformations for hyperparameters of type int. Args: x (int): The exponent. Returns: (int): The result of raising 10 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_10_int >>> transform_power_10_int(3) 1000 \"\"\" return int ( 10 ** x ) transform_power_2 ( x ) \u00b6 Transformations for hyperparameters of type float. Parameters: Name Type Description Default x float The exponent. required Returns: Type Description float The result of raising 2 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_2 >>> transform_power_2 ( 3 ) 8 Source code in spotPython/utils/transform.py 77 78 79 80 81 82 83 84 85 86 87 88 def transform_power_2 ( x ): \"\"\"Transformations for hyperparameters of type float. Args: x (float): The exponent. Returns: (float): The result of raising 2 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_2 >>> transform_power_2(3) 8 \"\"\" return 2 ** x transform_power_2_int ( x ) \u00b6 Transformations for hyperparameters of type int. Parameters: Name Type Description Default x int The exponent. required Returns: Type Description int The result of raising 2 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_2_int >>> transform_power_2_int ( 3 ) 8 Source code in spotPython/utils/transform.py 49 50 51 52 53 54 55 56 57 58 59 60 def transform_power_2_int ( x : int ) -> int : \"\"\"Transformations for hyperparameters of type int. Args: x (int): The exponent. Returns: (int): The result of raising 2 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_2_int >>> transform_power_2_int(3) 8 \"\"\" return int ( 2 ** x )","title":"transform"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.scale","text":"Sample scaling from unit hypercube to different bounds. Converts a sample from [0, 1) to [a, b) . The following transformation is used: (b - a) * X + a Note equal lower and upper bounds are feasible. Parameters: Name Type Description Default X array Sample to scale. required lower array lower bound of transformed data. required upper array upper bounds of transformed data. required Returns: Type Description array Scaled sample. Examples: Transform three samples in the unit hypercube to (lower, upper) bounds: >>> import numpy as np >>> from scipy.stats import qmc >>> from spotPython.utils.transform import scale >>> lower = np . array ([ 6 , 0 ]) >>> upper = np . array ([ 6 , 5 ]) >>> sample = np . array ([[ 0.5 , 0.75 ], >>> [ 0.5 , 0.5 ], >>> [ 0.75 , 0.25 ]]) >>> scale ( sample , lower , upper ) Source code in spotPython/utils/transform.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def scale ( X : np . ndarray , lower : np . ndarray , upper : np . ndarray ) -> np . ndarray : \"\"\" Sample scaling from unit hypercube to different bounds. Converts a sample from `[0, 1)` to `[a, b)`. The following transformation is used: `(b - a) * X + a` Note: equal lower and upper bounds are feasible. Args: X (array): Sample to scale. lower (array): lower bound of transformed data. upper (array): upper bounds of transformed data. Returns: (array): Scaled sample. Examples: Transform three samples in the unit hypercube to (lower, upper) bounds: >>> import numpy as np >>> from scipy.stats import qmc >>> from spotPython.utils.transform import scale >>> lower = np.array([6, 0]) >>> upper = np.array([6, 5]) >>> sample = np.array([[0.5 , 0.75], >>> [0.5 , 0.5], >>> [0.75, 0.25]]) >>> scale(sample, lower, upper) \"\"\" # Checking that X is within (0,1) interval if ( X . max () > 1.0 ) or ( X . min () < 0.0 ): raise ValueError ( \"Sample is not in unit hypercube\" ) # Vectorized scaling operation X = ( upper - lower ) * X + lower # Handling case where lower == upper X [:, lower == upper ] = lower [ lower == upper ] return X","title":"scale()"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_hyper_parameter_values","text":"Transform the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \u201cint\u201d, or \u201cfloat\u201d or \u201cnum\u201d. Let fun_control = {\u201ccore_model_hyper_dict\u201d:{ \u201cleaf_prediction\u201d: { \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d], \u201ctype\u201d: \u201cfactor\u201d, \u201cdefault\u201d: \u201cmean\u201d, \u201ccore_model_parameter_type\u201d: \u201cstr\u201d}, \u201cmax_depth\u201d: { \u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 20, \u201ctransform\u201d: \u201ctransform_power_2\u201d, \u201clower\u201d: 2, \u201cupper\u201d: 20}}} and v = {\u2018max_depth\u2019: 20,\u2019leaf_prediction\u2019: \u2018mean\u2019} and def transform_power_2(x): return 2**x. The function takes fun_control and v as input and returns a dictionary with the same structure as v. The function transforms the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \u201cint\u201d, or \u201cfloat\u201d or \u201cnum\u201d. For example, transform_hyper_parameter_values(fun_control, v) returns {\u2018max_depth\u2019: 1048576, \u2018leaf_prediction\u2019: \u2018mean\u2019}. Parameters: Name Type Description Default fun_control dict A dictionary containing the information about the core model and the hyperparameters. required hyper_parameter_values dict A dictionary containing the values of the hyperparameters. required Returns: Type Description dict A dictionary containing the values of the hyperparameters. Examples: >>> import copy from spotPython.utils.prepare import transform_hyper_parameter_values fun_control = { \"core_model_hyper_dict\": { \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"max_depth\": {\"type\": \"int\", \"default\": 20 \"transform\": \"transform_power_2\", \"lower\": 2, \"upper\": 20}}} hyper_parameter_values = {'max_depth': 20, 'leaf_prediction': 'mean'} transform_hyper_parameter_values(fun_control, hyper_parameter_values) {'max_depth': 1048576, 'leaf_prediction': 'mean'} Source code in spotPython/utils/transform.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def transform_hyper_parameter_values ( fun_control , hyper_parameter_values ): \"\"\" Transform the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \"int\", or \"float\" or \"num\". Let fun_control = {\"core_model_hyper_dict\":{ \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"max_depth\": { \"type\": \"int\", \"default\": 20, \"transform\": \"transform_power_2\", \"lower\": 2, \"upper\": 20}}} and v = {'max_depth': 20,'leaf_prediction': 'mean'} and def transform_power_2(x): return 2**x. The function takes fun_control and v as input and returns a dictionary with the same structure as v. The function transforms the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \"int\", or \"float\" or \"num\". For example, transform_hyper_parameter_values(fun_control, v) returns {'max_depth': 1048576, 'leaf_prediction': 'mean'}. Args: fun_control (dict): A dictionary containing the information about the core model and the hyperparameters. hyper_parameter_values (dict): A dictionary containing the values of the hyperparameters. Returns: (dict): A dictionary containing the values of the hyperparameters. Examples: >>> import copy from spotPython.utils.prepare import transform_hyper_parameter_values fun_control = { \"core_model_hyper_dict\": { \"leaf_prediction\": { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"}, \"max_depth\": {\"type\": \"int\", \"default\": 20 \"transform\": \"transform_power_2\", \"lower\": 2, \"upper\": 20}}} hyper_parameter_values = {'max_depth': 20, 'leaf_prediction': 'mean'} transform_hyper_parameter_values(fun_control, hyper_parameter_values) {'max_depth': 1048576, 'leaf_prediction': 'mean'} \"\"\" hyper_parameter_values = copy . deepcopy ( hyper_parameter_values ) for key , value in hyper_parameter_values . items (): if ( fun_control [ \"core_model_hyper_dict\" ][ key ][ \"type\" ] in [ \"int\" , \"float\" , \"num\" , \"factor\" ] and fun_control [ \"core_model_hyper_dict\" ][ key ][ \"transform\" ] != \"None\" ): hyper_parameter_values [ key ] = eval ( fun_control [ \"core_model_hyper_dict\" ][ key ][ \"transform\" ])( value ) return hyper_parameter_values","title":"transform_hyper_parameter_values()"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_none_to_None","text":"Transformations for hyperparameters of type None. Parameters: Name Type Description Default x str The string to transform. required Returns: Type Description str The transformed string. Examples: >>> from spotPython.utils.transform import transform_none_to_None >>> transform_none_to_None ( \"none\" ) None Note Needed for sklearn.linear_model.LogisticRegression Source code in spotPython/utils/transform.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def transform_none_to_None ( x ): \"\"\" Transformations for hyperparameters of type None. Args: x (str): The string to transform. Returns: (str): The transformed string. Examples: >>> from spotPython.utils.transform import transform_none_to_None >>> transform_none_to_None(\"none\") None Note: Needed for sklearn.linear_model.LogisticRegression \"\"\" if x == \"none\" : return None else : return x","title":"transform_none_to_None()"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_power","text":"Raises a given base to the power of x. Parameters: Name Type Description Default base int The base to raise to the power of x. required x int The exponent. required as_int bool If True, returns the result as an integer. False Returns: Type Description float The result of raising the base to the power of x. Examples: >>> from spotPython.utils.transform import transform_power >>> transform_power ( 2 , 3 ) 8 Source code in spotPython/utils/transform.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def transform_power ( base : int , x : int , as_int : bool = False ) -> float : \"\"\" Raises a given base to the power of x. Args: base (int): The base to raise to the power of x. x (int): The exponent. as_int (bool): If True, returns the result as an integer. Returns: (float): The result of raising the base to the power of x. Examples: >>> from spotPython.utils.transform import transform_power >>> transform_power(2, 3) 8 \"\"\" result = base ** x if as_int : result = int ( result ) return result","title":"transform_power()"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_power_10","text":"Transformations for hyperparameters of type float. Parameters: Name Type Description Default x float The exponent. required Returns: Type Description float The result of raising 10 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_10 >>> transform_power_10 ( 3 ) 1000 Source code in spotPython/utils/transform.py 91 92 93 94 95 96 97 98 99 100 101 102 def transform_power_10 ( x ): \"\"\"Transformations for hyperparameters of type float. Args: x (float): The exponent. Returns: (float): The result of raising 10 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_10 >>> transform_power_10(3) 1000 \"\"\" return 10 ** x","title":"transform_power_10()"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_power_10_int","text":"Transformations for hyperparameters of type int. Parameters: Name Type Description Default x int The exponent. required Returns: Type Description int The result of raising 10 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_10_int >>> transform_power_10_int ( 3 ) 1000 Source code in spotPython/utils/transform.py 63 64 65 66 67 68 69 70 71 72 73 74 def transform_power_10_int ( x : int ) -> int : \"\"\"Transformations for hyperparameters of type int. Args: x (int): The exponent. Returns: (int): The result of raising 10 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_10_int >>> transform_power_10_int(3) 1000 \"\"\" return int ( 10 ** x )","title":"transform_power_10_int()"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_power_2","text":"Transformations for hyperparameters of type float. Parameters: Name Type Description Default x float The exponent. required Returns: Type Description float The result of raising 2 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_2 >>> transform_power_2 ( 3 ) 8 Source code in spotPython/utils/transform.py 77 78 79 80 81 82 83 84 85 86 87 88 def transform_power_2 ( x ): \"\"\"Transformations for hyperparameters of type float. Args: x (float): The exponent. Returns: (float): The result of raising 2 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_2 >>> transform_power_2(3) 8 \"\"\" return 2 ** x","title":"transform_power_2()"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_power_2_int","text":"Transformations for hyperparameters of type int. Parameters: Name Type Description Default x int The exponent. required Returns: Type Description int The result of raising 2 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_2_int >>> transform_power_2_int ( 3 ) 8 Source code in spotPython/utils/transform.py 49 50 51 52 53 54 55 56 57 58 59 60 def transform_power_2_int ( x : int ) -> int : \"\"\"Transformations for hyperparameters of type int. Args: x (int): The exponent. Returns: (int): The result of raising 2 to the power of x. Examples: >>> from spotPython.utils.transform import transform_power_2_int >>> transform_power_2_int(3) 8 \"\"\" return int ( 2 ** x )","title":"transform_power_2_int()"}]}