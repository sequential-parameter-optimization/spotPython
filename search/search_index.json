{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"spotPython","text":""},{"location":"#surrogate-model-based-optimization-and-hyperparameter-tuning-in-python","title":"Surrogate Model Based Optimization and Hyperparameter Tuning in Python","text":"<ul> <li>Documentation for spotPython see Hyperparameter Tuning Cookbook, a guide for scikit-learn, PyTorch, river, and spotPython.</li> <li>News and updates related to spotPython see SPOTSeven</li> </ul>"},{"location":"about/","title":"Contact/Privacy Policy","text":""},{"location":"about/#address","title":"Address","text":"<p>Prof. Dr. Thomas Bartz-Beielstein TH K\u00f6ln Raum 1.519 Steinm\u00fcllerallee 6 51643 Gummersbach +49 (0)2261 8196 6391 thomas.bartz-beielstein [at] th-koeln.de www.spotseven.de</p>"},{"location":"about/#privacy-policy","title":"Privacy Policy","text":"<p>We are very delighted that you have shown interest in our enterprise. Data protection is of a particularly high priority for the management of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. The use of the Internet pages of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is possible without any indication of personal data; however, if a data subject wants to use special enterprise services via our website, processing of personal data could become necessary. If the processing of personal data is necessary and there is no statutory basis for such processing, we generally obtain consent from the data subject.</p> <p>The processing of personal data, such as the name, address, e-mail address, or telephone number of a data subject shall always be in line with the General Data Protection Regulation (GDPR), and in accordance with the country-specific data protection regulations applicable to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. By means of this data protection declaration, our enterprise would like to inform the general public of the nature, scope, and purpose of the personal data we collect, use and process. Furthermore, data subjects are informed, by means of this data protection declaration, of the rights to which they are entitled.</p> <p>As the controller, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab has implemented numerous technical and organizational measures to ensure the most complete protection of personal data processed through this website. However, Internet-based data transmissions may in principle have security gaps, so absolute protection may not be guaranteed. For this reason, every data subject is free to transfer personal data to us via alternative means, e.g. by telephone.</p> <ol> <li>Definitions</li> </ol> <p>The data protection declaration of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is based on the terms used by the European legislator for the adoption of the General Data Protection Regulation (GDPR). Our data protection declaration should be legible and understandable for the general public, as well as our customers and business partners. To ensure this, we would like to first explain the terminology used.</p> <p>In this data protection declaration, we use, inter alia, the following terms:</p> <p>a)    Personal data</p> <p>Personal data means any information relating to an identified or identifiable natural person (\u201cdata subject\u201d). An identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.</p> <p>b) Data subject</p> <p>Data subject is any identified or identifiable natural person, whose personal data is processed by the controller responsible for the processing.</p> <p>c)    Processing</p> <p>Processing is any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.</p> <p>d)    Restriction of processing</p> <p>Restriction of processing is the marking of stored personal data with the aim of limiting their processing in the future.</p> <p>e)    Profiling</p> <p>Profiling means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person\u2019s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements.</p> <p>f)     Pseudonymisation</p> <p>Pseudonymisation is the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person.</p> <p>g)    Controller or controller responsible for the processing</p> <p>Controller or controller responsible for the processing is the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law.</p> <p>h)    Processor</p> <p>Processor is a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller.</p> <p>i)      Recipient</p> <p>Recipient is a natural or legal person, public authority, agency or another body, to which the personal data are disclosed, whether a third party or not. However, public authorities which may receive personal data in the framework of a particular inquiry in accordance with Union or Member State law shall not be regarded as recipients; the processing of those data by those public authorities shall be in compliance with the applicable data protection rules according to the purposes of the processing.</p> <p>j)      Third party</p> <p>Third party is a natural or legal person, public authority, agency or body other than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data.</p> <p>k)    Consent</p> <p>Consent of the data subject is any freely given, specific, informed and unambiguous indication of the data subject\u2019s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.</p> <ol> <li>Name and Address of the controller</li> </ol> <p>Controller for the purposes of the General Data Protection Regulation (GDPR), other data protection laws applicable in Member states of the European Union and other provisions related to data protection is:</p> <p>TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab</p> <p>Steinm\u00fcllerallee 1</p> <p>51643 Gummersbach</p> <p>Deutschland</p> <p>Phone: +49 2261 81966391</p> <p>Email: thomas.bartz-beielstein@th-koeln.de</p> <p>Website: www.spotseven.de</p> <ol> <li>Collection of general data and information</li> </ol> <p>The website of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab collects a series of general data and information when a data subject or automated system calls up the website. This general data and information are stored in the server log files. Collected may be (1) the browser types and versions used, (2) the operating system used by the accessing system, (3) the website from which an accessing system reaches our website (so-called referrers), (4) the sub-websites, (5) the date and time of access to the Internet site, (6) an Internet protocol address (IP address), (7) the Internet service provider of the accessing system, and (8) any other similar data and information that may be used in the event of attacks on our information technology systems.</p> <p>When using these general data and information, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab does not draw any conclusions about the data subject. Rather, this information is needed to (1) deliver the content of our website correctly, (2) optimize the content of our website as well as its advertisement, (3) ensure the long-term viability of our information technology systems and website technology, and (4) provide law enforcement authorities with the information necessary for criminal prosecution in case of a cyber-attack. Therefore, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab analyzes anonymously collected data and information statistically, with the aim of increasing the data protection and data security of our enterprise, and to ensure an optimal level of protection for the personal data we process. The anonymous data of the server log files are stored separately from all personal data provided by a data subject.</p> <ol> <li>Comments function in the blog on the website</li> </ol> <p>The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab offers users the possibility to leave individual comments on individual blog contributions on a blog, which is on the website of the controller. A blog is a web-based, publicly-accessible portal, through which one or more people called bloggers or web-bloggers may post articles or write down thoughts in so-called blogposts. Blogposts may usually be commented by third parties.</p> <p>If a data subject leaves a comment on the blog published on this website, the comments made by the data subject are also stored and published, as well as information on the date of the commentary and on the user\u2019s (pseudonym) chosen by the data subject. In addition, the IP address assigned by the Internet service provider (ISP) to the data subject is also logged. This storage of the IP address takes place for security reasons, and in case the data subject violates the rights of third parties, or posts illegal content through a given comment. The storage of these personal data is, therefore, in the own interest of the data controller, so that he can exculpate in the event of an infringement. This collected personal data will not be passed to third parties, unless such a transfer is required by law or serves the aim of the defense of the data controller.</p> <ol> <li>Routine erasure and blocking of personal data</li> </ol> <p>The data controller shall process and store the personal data of the data subject only for the period necessary to achieve the purpose of storage, or as far as this is granted by the European legislator or other legislators in laws or regulations to which the controller is subject to.</p> <p>If the storage purpose is not applicable, or if a storage period prescribed by the European legislator or another competent legislator expires, the personal data are routinely blocked or erased in accordance with legal requirements.</p> <ol> <li>Rights of the data subject</li> </ol> <p>a) Right of confirmation</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller the confirmation as to whether or not personal data concerning him or her are being processed. If a data subject wishes to avail himself of this right of confirmation, he or she may, at any time, contact our Data Protection Officer or another employee of the controller.</p> <p>b) Right of access</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller free information about his or her personal data stored at any time and a copy of this information. Furthermore, the European directives and regulations grant the data subject access to the following information:</p> <p>the purposes of the processing; the categories of personal data concerned; the recipients or categories of recipients to whom the personal data have been or will be disclosed, in particular recipients in third countries or international organisations; where possible, the envisaged period for which the personal data will be stored, or, if not possible, the criteria used to determine that period; the existence of the right to request from the controller rectification or erasure of personal data, or restriction of processing of personal data concerning the data subject, or to object to such processing; the existence of the right to lodge a complaint with a supervisory authority; where the personal data are not collected from the data subject, any available information as to their source; the existence of automated decision-making, including profiling, referred to in Article 22(1) and (4) of the GDPR and, at least in those cases, meaningful information about the logic involved, as well as the significance and envisaged consequences of such processing for the data subject. Furthermore, the data subject shall have a right to obtain information as to whether personal data are transferred to a third country or to an international organisation. Where this is the case, the data subject shall have the right to be informed of the appropriate safeguards relating to the transfer.</p> <p>If a data subject wishes to avail himself of this right of access, he or she may at any time contact our Data Protection Officer or another employee of the controller.</p> <p>c) Right to rectification</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller without undue delay the rectification of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement.</p> <p>If a data subject wishes to exercise this right to rectification, he or she may, at any time, contact our Data Protection Officer or another employee of the controller.</p> <p>d) Right to erasure (Right to be forgotten)</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies, as long as the processing is not necessary:</p> <p>The personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed. The data subject withdraws consent to which the processing is based according to point (a) of Article 6(1) of the GDPR, or point (a) of Article 9(2) of the GDPR, and where there is no other legal ground for the processing. The data subject objects to the processing pursuant to Article 21(1) of the GDPR and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2) of the GDPR. The personal data have been unlawfully processed. The personal data must be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject. The personal data have been collected in relation to the offer of information society services referred to in Article 8(1) of the GDPR. If one of the aforementioned reasons applies, and a data subject wishes to request the erasure of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee shall promptly ensure that the erasure request is complied with immediately.</p> <p>Where the controller has made personal data public and is obliged pursuant to Article 17(1) to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform other controllers processing the personal data that the data subject has requested erasure by such controllers of any links to, or copy or replication of, those personal data, as far as processing is not required. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the necessary measures in individual cases.</p> <p>e) Right of restriction of processing</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller restriction of processing where one of the following applies:</p> <p>The accuracy of the personal data is contested by the data subject, for a period enabling the controller to verify the accuracy of the personal data. The processing is unlawful and the data subject opposes the erasure of the personal data and requests instead the restriction of their use instead. The controller no longer needs the personal data for the purposes of the processing, but they are required by the data subject for the establishment, exercise or defence of legal claims. The data subject has objected to processing pursuant to Article 21(1) of the GDPR pending the verification whether the legitimate grounds of the controller override those of the data subject. If one of the aforementioned conditions is met, and a data subject wishes to request the restriction of the processing of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the restriction of the processing.</p> <p>f) Right to data portability</p> <p>Each data subject shall have the right granted by the European legislator, to receive the personal data concerning him or her, which was provided to a controller, in a structured, commonly used and machine-readable format. He or she shall have the right to transmit those data to another controller without hindrance from the controller to which the personal data have been provided, as long as the processing is based on consent pursuant to point (a) of Article 6(1) of the GDPR or point (a) of Article 9(2) of the GDPR, or on a contract pursuant to point (b) of Article 6(1) of the GDPR, and the processing is carried out by automated means, as long as the processing is not necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller.</p> <p>Furthermore, in exercising his or her right to data portability pursuant to Article 20(1) of the GDPR, the data subject shall have the right to have personal data transmitted directly from one controller to another, where technically feasible and when doing so does not adversely affect the rights and freedoms of others.</p> <p>In order to assert the right to data portability, the data subject may at any time contact the Data Protection Officer designated by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee.</p> <p>g) Right to object</p> <p>Each data subject shall have the right granted by the European legislator to object, on grounds relating to his or her particular situation, at any time, to processing of personal data concerning him or her, which is based on point (e) or (f) of Article 6(1) of the GDPR. This also applies to profiling based on these provisions.</p> <p>The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall no longer process the personal data in the event of the objection, unless we can demonstrate compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject, or for the establishment, exercise or defence of legal claims.</p> <p>If the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab processes personal data for direct marketing purposes, the data subject shall have the right to object at any time to processing of personal data concerning him or her for such marketing. This applies to profiling to the extent that it is related to such direct marketing. If the data subject objects to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab to the processing for direct marketing purposes, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab will no longer process the personal data for these purposes.</p> <p>In addition, the data subject has the right, on grounds relating to his or her particular situation, to object to processing of personal data concerning him or her by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab for scientific or historical research purposes, or for statistical purposes pursuant to Article 89(1) of the GDPR, unless the processing is necessary for the performance of a task carried out for reasons of public interest.</p> <p>In order to exercise the right to object, the data subject may directly contact the Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee. In addition, the data subject is free in the context of the use of information society services, and notwithstanding Directive 2002/58/EC, to use his or her right to object by automated means using technical specifications.</p> <p>h) Automated individual decision-making, including profiling</p> <p>Each data subject shall have the right granted by the European legislator not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her, or similarly significantly affects him or her, as long as the decision (1) is not is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) is not authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, or (3) is not based on the data subject\u2019s explicit consent.</p> <p>If the decision (1) is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) it is based on the data subject\u2019s explicit consent, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall implement suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and contest the decision.</p> <p>If the data subject wishes to exercise the rights concerning automated individual decision-making, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller.</p> <p>i) Right to withdraw data protection consent</p> <p>Each data subject shall have the right granted by the European legislator to withdraw his or her consent to processing of his or her personal data at any time.</p> <p>f the data subject wishes to exercise the right to withdraw the consent, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller.</p> <ol> <li>Data protection provisions about the application and use of Facebook</li> </ol> <p>On this website, the controller has integrated components of the enterprise Facebook. Facebook is a social network.</p> <p>A social network is a place for social meetings on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Facebook allows social network users to include the creation of private profiles, upload photos, and network through friend requests.</p> <p>The operating company of Facebook is Facebook, Inc., 1 Hacker Way, Menlo Park, CA 94025, United States. If a person lives outside of the United States or Canada, the controller is the Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Ireland.</p> <p>With each call-up to one of the individual pages of this Internet website, which is operated by the controller and into which a Facebook component (Facebook plug-ins) was integrated, the web browser on the information technology system of the data subject is automatically prompted to download display of the corresponding Facebook component from Facebook through the Facebook component. An overview of all the Facebook Plug-ins may be accessed under https://developers.facebook.com/docs/plugins/. During the course of this technical procedure, Facebook is made aware of what specific sub-site of our website was visited by the data subject.</p> <p>If the data subject is logged in at the same time on Facebook, Facebook detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-site of our Internet page was visited by the data subject. This information is collected through the Facebook component and associated with the respective Facebook account of the data subject. If the data subject clicks on one of the Facebook buttons integrated into our website, e.g. the \u201cLike\u201d button, or if the data subject submits a comment, then Facebook matches this information with the personal Facebook user account of the data subject and stores the personal data.</p> <p>Facebook always receives, through the Facebook component, information about a visit to our website by the data subject, whenever the data subject is logged in at the same time on Facebook during the time of the call-up to our website. This occurs regardless of whether the data subject clicks on the Facebook component or not. If such a transmission of information to Facebook is not desirable for the data subject, then he or she may prevent this by logging off from their Facebook account before a call-up to our website is made.</p> <p>The data protection guideline published by Facebook, which is available at https://facebook.com/about/privacy/, provides information about the collection, processing and use of personal data by Facebook. In addition, it is explained there what setting options Facebook offers to protect the privacy of the data subject. In addition, different configuration options are made available to allow the elimination of data transmission to Facebook, e.g. the Facebook blocker of the provider Webgraph, which may be obtained under http://webgraph.com/resources/facebookblocker/. These applications may be used by the data subject to eliminate a data transmission to Facebook.</p> <ol> <li>Data protection provisions about the application and use of Google+</li> </ol> <p>On this website, the controller has integrated the Google+ button as a component. Google+ is a so-called social network. A social network is a social meeting place on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Google+ allows users of the social network to include the creation of private profiles, upload photos and network through friend requests.</p> <p>The operating company of Google+ is Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this website, which is operated by the controller and on which a Google+ button has been integrated, the Internet browser on the information technology system of the data subject automatically downloads a display of the corresponding Google+ button of Google through the respective Google+ button component. During the course of this technical procedure, Google is made aware of what specific sub-page of our website was visited by the data subject. More detailed information about Google+ is available under https://developers.google.com/+/.</p> <p>If the data subject is logged in at the same time to Google+, Google recognizes with each call-up to our website by the data subject and for the entire duration of his or her stay on our Internet site, which specific sub-pages of our Internet page were visited by the data subject. This information is collected through the Google+ button and Google matches this with the respective Google+ account associated with the data subject.</p> <p>If the data subject clicks on the Google+ button integrated on our website and thus gives a Google+ 1 recommendation, then Google assigns this information to the personal Google+ user account of the data subject and stores the personal data. Google stores the Google+ 1 recommendation of the data subject, making it publicly available in accordance with the terms and conditions accepted by the data subject in this regard. Subsequently, a Google+ 1 recommendation given by the data subject on this website together with other personal data, such as the Google+ account name used by the data subject and the stored photo, is stored and processed on other Google services, such as search-engine results of the Google search engine, the Google account of the data subject or in other places, e.g. on Internet pages, or in relation to advertisements. Google is also able to link the visit to this website with other personal data stored on Google. Google further records this personal information with the purpose of improving or optimizing the various Google services.</p> <p>Through the Google+ button, Google receives information that the data subject visited our website, if the data subject at the time of the call-up to our website is logged in to Google+. This occurs regardless of whether the data subject clicks or doesn\u2019t click on the Google+ button.</p> <p>If the data subject does not wish to transmit personal data to Google, he or she may prevent such transmission by logging out of his Google+ account before calling up our website.</p> <p>Further information and the data protection provisions of Google may be retrieved under https://www.google.com/intl/en/policies/privacy/. More references from Google about the Google+ 1 button may be obtained under https://developers.google.com/+/web/buttons-policy.</p> <ol> <li>Data protection provisions about the application and use of Jetpack for WordPress</li> </ol> <p>On this website, the controller has integrated Jetpack. Jetpack is a WordPress plug-in, which provides additional features to the operator of a website based on WordPress. Jetpack allows the Internet site operator, inter alia, an overview of the visitors of the site. By displaying related posts and publications, or the ability to share content on the page, it is also possible to increase visitor numbers. In addition, security features are integrated into Jetpack, so a Jetpack-using site is better protected against brute-force attacks. Jetpack also optimizes and accelerates the loading of images on the website.</p> <p>The operating company of Jetpack Plug-Ins for WordPress is the Automattic Inc., 132 Hawthorne Street, San Francisco, CA 94107, UNITED STATES. The operating enterprise uses the tracking technology created by Quantcast Inc., 201 Third Street, San Francisco, CA 94103, UNITED STATES.</p> <p>Jetpack sets a cookie on the information technology system used by the data subject. The definition of cookies is explained above. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Jetpack component was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to submit data through the Jetpack component for analysis purposes to Automattic. During the course of this technical procedure Automattic receives data that is used to create an overview of website visits. The data obtained in this way serves the analysis of the behaviour of the data subject, which has access to the Internet page of the controller and is analyzed with the aim to optimize the website. The data collected through the Jetpack component is not used to identify the data subject without a prior obtaining of a separate express consent of the data subject. The data comes also to the notice of Quantcast. Quantcast uses the data for the same purposes as Automattic.</p> <p>The data subject can, as stated above, prevent the setting of cookies through our website at any time by means of a corresponding adjustment of the web browser used and thus permanently deny the setting of cookies. Such an adjustment to the Internet browser used would also prevent Automattic/Quantcast from setting a cookie on the information technology system of the data subject. In addition, cookies already in use by Automattic/Quantcast may be deleted at any time via a web browser or other software programs.</p> <p>In addition, the data subject has the possibility of objecting to a collection of data relating to a use of this Internet site that are generated by the Jetpack cookie as well as the processing of these data by Automattic/Quantcast and the chance to preclude any such. For this purpose, the data subject must press the \u2018opt-out\u2019 button under the link https://www.quantcast.com/opt-out/ which sets an opt-out cookie. The opt-out cookie set with this purpose is placed on the information technology system used by the data subject. If the cookies are deleted on the system of the data subject, then the data subject must call up the link again and set a new opt-out cookie.</p> <p>With the setting of the opt-out cookie, however, the possibility exists that the websites of the controller are not fully usable anymore by the data subject.</p> <p>The applicable data protection provisions of Automattic may be accessed under https://automattic.com/privacy/. The applicable data protection provisions of Quantcast can be accessed under https://www.quantcast.com/privacy/.</p> <ol> <li>Data protection provisions about the application and use of LinkedIn</li> </ol> <p>The controller has integrated components of the LinkedIn Corporation on this website. LinkedIn is a web-based social network that enables users with existing business contacts to connect and to make new business contacts. Over 400 million registered people in more than 200 countries use LinkedIn. Thus, LinkedIn is currently the largest platform for business contacts and one of the most visited websites in the world.</p> <p>The operating company of LinkedIn is LinkedIn Corporation, 2029 Stierlin Court Mountain View, CA 94043, UNITED STATES. For privacy matters outside of the UNITED STATES LinkedIn Ireland, Privacy Policy Issues, Wilton Plaza, Wilton Place, Dublin 2, Ireland, is responsible.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a LinkedIn component (LinkedIn plug-in) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to the download of a display of the corresponding LinkedIn component of LinkedIn. Further information about the LinkedIn plug-in may be accessed under https://developer.linkedin.com/plugins. During the course of this technical procedure, LinkedIn gains knowledge of what specific sub-page of our website was visited by the data subject.</p> <p>If the data subject is logged in at the same time on LinkedIn, LinkedIn detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-page of our Internet page was visited by the data subject. This information is collected through the LinkedIn component and associated with the respective LinkedIn account of the data subject. If the data subject clicks on one of the LinkedIn buttons integrated on our website, then LinkedIn assigns this information to the personal LinkedIn user account of the data subject and stores the personal data.</p> <p>LinkedIn receives information via the LinkedIn component that the data subject has visited our website, provided that the data subject is logged in at LinkedIn at the time of the call-up to our website. This occurs regardless of whether the person clicks on the LinkedIn button or not. If such a transmission of information to LinkedIn is not desirable for the data subject, then he or she may prevent this by logging off from their LinkedIn account before a call-up to our website is made.</p> <p>LinkedIn provides under https://www.linkedin.com/psettings/guest-controls the possibility to unsubscribe from e-mail messages, SMS messages and targeted ads, as well as the ability to manage ad settings. LinkedIn also uses affiliates such as Eire, Google Analytics, BlueKai, DoubleClick, Nielsen, Comscore, Eloqua, and Lotame. The setting of such cookies may be denied under https://www.linkedin.com/legal/cookie-policy. The applicable privacy policy for LinkedIn is available under https://www.linkedin.com/legal/privacy-policy. The LinkedIn Cookie Policy is available under https://www.linkedin.com/legal/cookie-policy.</p> <ol> <li>Data protection provisions about the application and use of Twitter</li> </ol> <p>On this website, the controller has integrated components of Twitter. Twitter is a multilingual, publicly-accessible microblogging service on which users may publish and spread so-called \u2018tweets,\u2019 e.g. short messages, which are limited to 140 characters. These short messages are available for everyone, including those who are not logged on to Twitter. The tweets are also displayed to so-called followers of the respective user. Followers are other Twitter users who follow a user\u2019s tweets. Furthermore, Twitter allows you to address a wide audience via hashtags, links or retweets.</p> <p>The operating company of Twitter is Twitter, Inc., 1355 Market Street, Suite 900, San Francisco, CA 94103, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Twitter component (Twitter button) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding Twitter component of Twitter. Further information about the Twitter buttons is available under https://about.twitter.com/de/resources/buttons. During the course of this technical procedure, Twitter gains knowledge of what specific sub-page of our website was visited by the data subject. The purpose of the integration of the Twitter component is a retransmission of the contents of this website to allow our users to introduce this web page to the digital world and increase our visitor numbers.</p> <p>If the data subject is logged in at the same time on Twitter, Twitter detects with every call-up to our website by the data subject and for the entire duration of their stay on our Internet site which specific sub-page of our Internet page was visited by the data subject. This information is collected through the Twitter component and associated with the respective Twitter account of the data subject. If the data subject clicks on one of the Twitter buttons integrated on our website, then Twitter assigns this information to the personal Twitter user account of the data subject and stores the personal data.</p> <p>Twitter receives information via the Twitter component that the data subject has visited our website, provided that the data subject is logged in on Twitter at the time of the call-up to our website. This occurs regardless of whether the person clicks on the Twitter component or not. If such a transmission of information to Twitter is not desirable for the data subject, then he or she may prevent this by logging off from their Twitter account before a call-up to our website is made.</p> <p>The applicable data protection provisions of Twitter may be accessed under https://twitter.com/privacy?lang=en.</p> <ol> <li>Data protection provisions about the application and use of YouTube</li> </ol> <p>On this website, the controller has integrated components of YouTube. YouTube is an Internet video portal that enables video publishers to set video clips and other users free of charge, which also provides free viewing, review and commenting on them. YouTube allows you to publish all kinds of videos, so you can access both full movies and TV broadcasts, as well as music videos, trailers, and videos made by users via the Internet portal.</p> <p>The operating company of YouTube is YouTube, LLC, 901 Cherry Ave., San Bruno, CA 94066, UNITED STATES. The YouTube, LLC is a subsidiary of Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a YouTube component (YouTube video) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding YouTube component. Further information about YouTube may be obtained under https://www.youtube.com/yt/about/en/. During the course of this technical procedure, YouTube and Google gain knowledge of what specific sub-page of our website was visited by the data subject.</p> <p>If the data subject is logged in on YouTube, YouTube recognizes with each call-up to a sub-page that contains a YouTube video, which specific sub-page of our Internet site was visited by the data subject. This information is collected by YouTube and Google and assigned to the respective YouTube account of the data subject.</p> <p>YouTube and Google will receive information through the YouTube component that the data subject has visited our website, if the data subject at the time of the call to our website is logged in on YouTube; this occurs regardless of whether the person clicks on a YouTube video or not. If such a transmission of this information to YouTube and Google is not desirable for the data subject, the delivery may be prevented if the data subject logs off from their own YouTube account before a call-up to our website is made.</p> <p>YouTube\u2019s data protection provisions, available at https://www.google.com/intl/en/policies/privacy/, provide information about the collection, processing and use of personal data by YouTube and Google.</p> <ol> <li>Legal basis for the processing</li> </ol> <p>Art. 6(1) lit. a GDPR serves as the legal basis for processing operations for which we obtain consent for a specific processing purpose. If the processing of personal data is necessary for the performance of a contract to which the data subject is party, as is the case, for example, when processing operations are necessary for the supply of goods or to provide any other service, the processing is based on Article 6(1) lit. b GDPR. The same applies to such processing operations which are necessary for carrying out pre-contractual measures, for example in the case of inquiries concerning our products or services. Is our company subject to a legal obligation by which processing of personal data is required, such as for the fulfillment of tax obligations, the processing is based on Art. 6(1) lit. c GDPR. In rare cases, the processing of personal data may be necessary to protect the vital interests of the data subject or of another natural person. This would be the case, for example, if a visitor were injured in our company and his name, age, health insurance data or other vital information would have to be passed on to a doctor, hospital or other third party. Then the processing would be based on Art. 6(1) lit. d GDPR. Finally, processing operations could be based on Article 6(1) lit. f GDPR. This legal basis is used for processing operations which are not covered by any of the abovementioned legal grounds, if processing is necessary for the purposes of the legitimate interests pursued by our company or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data. Such processing operations are particularly permissible because they have been specifically mentioned by the European legislator. He considered that a legitimate interest could be assumed if the data subject is a client of the controller (Recital 47 Sentence 2 GDPR).</p> <ol> <li>The legitimate interests pursued by the controller or by a third party</li> </ol> <p>Where the processing of personal data is based on Article 6(1) lit. f GDPR our legitimate interest is to carry out our business in favor of the well-being of all our employees and the shareholders.</p> <ol> <li>Period for which the personal data will be stored</li> </ol> <p>The criteria used to determine the period of storage of personal data is the respective statutory retention period. After expiration of that period, the corresponding data is routinely deleted, as long as it is no longer necessary for the fulfillment of the contract or the initiation of a contract.</p> <ol> <li>Provision of personal data as statutory or contractual requirement; Requirement necessary to enter into a contract; Obligation of the data subject to provide the personal data; possible consequences of failure to provide such data</li> </ol> <p>We clarify that the provision of personal data is partly required by law (e.g. tax regulations) or can also result from contractual provisions (e.g. information on the contractual partner). Sometimes it may be necessary to conclude a contract that the data subject provides us with personal data, which must subsequently be processed by us. The data subject is, for example, obliged to provide us with personal data when our company signs a contract with him or her. The non-provision of the personal data would have the consequence that the contract with the data subject could not be concluded. Before personal data is provided by the data subject, the data subject must contact our Data Protection Officer. Our Data Protection Officer clarifies to the data subject whether the provision of the personal data is required by law or contract or is necessary for the conclusion of the contract, whether there is an obligation to provide the personal data and the consequences of non-provision of the personal data.</p> <ol> <li>Existence of automated decision-making</li> </ol> <p>As a responsible company, we do not use automatic decision-making or profiling.</p> <p>This Privacy Policy has been generated by the Privacy Policy Generator of the External Data Protection Officers that was developed in cooperation with RC GmbH, which sells used notebooks and the Media Law Lawyers from WBS-LAW.</p>"},{"location":"download/","title":"Install spotPython","text":"<pre><code>pip install spotPython\n</code></pre>"},{"location":"examples/","title":"SPOT Examples","text":""},{"location":"examples/#simple-spotpython-run","title":"Simple spotPython run","text":"<pre><code>import numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\n# number of initial points:\nni = 7\n# number of points\nn = 10\n\nfun = analytical().fun_sphere\nlower = np.array([-1])\nupper = np.array([1])\ndesign_control={\"init_size\": ni}\n\nspot_1 = spot.Spot(fun=fun,\n            lower = lower,\n            upper= upper,\n            fun_evals = n,\n            show_progress=True,\n            design_control=design_control,)\nspot_1.run()\n</code></pre>"},{"location":"examples/#further-examples","title":"Further Examples","text":"<p>Examples can be found in the Hyperparameter Tuning Cookbook, e.g., Documentation of the Sequential Parameter Optimization.</p>"},{"location":"hyperparameter-tuning-cookbook/","title":"Hyperparameter Tuning Cookbook","text":"<p>The following is a cookbook of hyperparameter tuning recipes. It is not meant to be exhaustive, but instead act as a place to capture a number of the common patterns used in hyperparameter tuning.</p> <p>Hyperparameter Tuning Cookbook</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>spotPython<ul> <li>budget<ul> <li>ocba</li> </ul> </li> <li>build<ul> <li>kriging</li> <li>surrogates</li> </ul> </li> <li>data<ul> <li>base</li> <li>light_hyper_dict</li> <li>lightning_hyper_dict</li> <li>sklearn_hyper_dict</li> <li>torch_hyper_dict</li> <li>torchdata</li> <li>vbdp</li> </ul> </li> <li>design<ul> <li>designs</li> <li>factorial</li> <li>spacefilling</li> </ul> </li> <li>fun<ul> <li>hyperlight</li> <li>hyperlightning</li> <li>hypersklearn</li> <li>hypertorch</li> <li>objectivefunctions</li> </ul> </li> <li>hyperparameters<ul> <li>categorical</li> <li>optimizer</li> <li>values</li> </ul> </li> <li>light<ul> <li>cifar10<ul> <li>cifar10datamodule</li> </ul> </li> <li>cnn<ul> <li>googlenet</li> <li>inceptionblock</li> <li>netcnnbase</li> </ul> </li> <li>crossvalidationdatamodule</li> <li>csvdatamodule</li> <li>csvdataset</li> <li>litmodel</li> <li>mnistdatamodule</li> <li>netlightbase</li> <li>netlinearbase</li> <li>trainmodel</li> <li>traintest</li> <li>traintest_NEW</li> <li>traintest_OLD</li> <li>utils</li> </ul> </li> <li>plot<ul> <li>contour</li> <li>validation</li> </ul> </li> <li>sklearn<ul> <li>traintest</li> </ul> </li> <li>spot<ul> <li>spot</li> </ul> </li> <li>torch<ul> <li>activation</li> <li>dataframedataset</li> <li>initialization</li> <li>mapk</li> <li>netcifar10</li> <li>netcore</li> <li>netfashionMNIST</li> <li>netregression</li> <li>netvbdp</li> <li>traintest</li> </ul> </li> <li>utils<ul> <li>aggregate</li> <li>classes</li> <li>compare</li> <li>convert</li> <li>device</li> <li>eda</li> <li>file</li> <li>init</li> <li>metrics</li> <li>progress</li> <li>repair</li> <li>tensorboard</li> <li>transform</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/spotPython/budget/ocba/","title":"ocba","text":"<p>OCBA: Optimal Computing Budget Allocation</p>"},{"location":"reference/spotPython/budget/ocba/#spotPython.budget.ocba.get_ocba","title":"<code>get_ocba(means, vars, delta)</code>","text":"<p>Optimal Computer Budget Allocation (OCBA)</p> <p>This function calculates the budget recommendations for a given set of means, variances, and incremental budget using the OCBA algorithm.</p> References <p>[1]: Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization: An Optimal Computer Budget Allocation, pp. 49 and pp. 215 [2]: C.S.M Currie and T. Monks: How to choose the best setup for a system. A tutorial for the Simulation Workshop 2021, see: https://colab.research.google.com/github/TomMonks/sim-tools/blob/master/examples/sw21_tutorial.ipynb and https://github.com/TomMonks/sim-tools</p> <p>Parameters:</p> Name Type Description Default <code>means</code> <code>array</code> <p>An array of means.</p> required <code>vars</code> <code>array</code> <p>An array of variances.</p> required <code>delta</code> <code>int</code> <p>The incremental budget.</p> required <p>Returns:</p> Type Description <code>array</code> <p>An array of budget recommendations.</p> Note <p>The implementation is based on the pseudo-code in the Chen et al. (p. 49), see [1].</p> <p>Examples:</p> <p>From the Chen et al. book (p. 49):</p> <pre><code>&gt;&gt;&gt; mean_y = np.array([1,2,3,4,5])\n    var_y = np.array([1,1,9,9,4])\n    get_ocba(mean_y, var_y, 50)\n    [11  9 19  9  2]\n</code></pre> Source code in <code>spotPython/budget/ocba.py</code> <pre><code>def get_ocba(means, vars, delta) -&gt; int32:\n    \"\"\"\n    Optimal Computer Budget Allocation (OCBA)\n\n    This function calculates the budget recommendations for a given set of means,\n    variances, and incremental budget using the OCBA algorithm.\n\n    References:\n        [1]: Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization: An Optimal Computer Budget Allocation,\n        pp. 49 and pp. 215\n        [2]: C.S.M Currie and T. Monks: How to choose the best setup for a system.\n        A tutorial for the Simulation Workshop 2021, see:\n        https://colab.research.google.com/github/TomMonks/sim-tools/blob/master/examples/sw21_tutorial.ipynb\n        and\n        https://github.com/TomMonks/sim-tools\n\n    Args:\n        means (numpy.array): An array of means.\n        vars (numpy.array): An array of variances.\n        delta (int): The incremental budget.\n\n    Returns:\n        (numpy.array): An array of budget recommendations.\n\n    Note:\n        The implementation is based on the pseudo-code in the Chen et al. (p. 49), see [1].\n\n    Examples:\n        From the Chen et al. book (p. 49):\n        &gt;&gt;&gt; mean_y = np.array([1,2,3,4,5])\n            var_y = np.array([1,1,9,9,4])\n            get_ocba(mean_y, var_y, 50)\n            [11  9 19  9  2]\n    \"\"\"\n    n_designs = means.shape[0]\n    allocations = zeros(n_designs, int32)\n    ratios = zeros(n_designs, float64)\n    budget = delta\n    ranks = get_ranks(means)\n    best, second_best = argpartition(ranks, 2)[:2]\n    ratios[second_best] = 1.0\n    select = [i for i in range(n_designs) if i not in [best, second_best]]\n    temp = (means[best] - means[second_best]) / (means[best] - means[select])\n    ratios[select] = square(temp) * (vars[select] / vars[second_best])\n    select = [i for i in range(n_designs) if i not in [best]]\n    temp = (square(ratios[select]) / vars[select]).sum()\n    ratios[best] = sqrt(vars[best] * temp)\n    more_runs = full(n_designs, True, dtype=bool)\n    add_budget = zeros(n_designs, dtype=float)\n    more_alloc = True\n    while more_alloc:\n        more_alloc = False\n        ratio_s = (more_runs * ratios).sum()\n        add_budget[more_runs] = (budget / ratio_s) * ratios[more_runs]\n        add_budget = around(add_budget).astype(int)\n        mask = add_budget &lt; allocations\n        add_budget[mask] = allocations[mask]\n        more_runs[mask] = 0\n        if mask.sum() &gt; 0:\n            more_alloc = True\n        if more_alloc:\n            budget = allocations.sum() + delta\n            budget -= (add_budget * ~more_runs).sum()\n    t_budget = add_budget.sum()\n    add_budget[best] += allocations.sum() + delta - t_budget\n    return add_budget - allocations\n</code></pre>"},{"location":"reference/spotPython/budget/ocba/#spotPython.budget.ocba.get_ocba_X","title":"<code>get_ocba_X(X, means, vars, delta)</code>","text":"<p>This function calculates the OCBA allocation and repeats the input array X along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input array to be repeated.</p> required <code>means</code> <code>list</code> <p>List of means for each alternative.</p> required <code>vars</code> <code>list</code> <p>List of variances for each alternative.</p> required <code>delta</code> <code>float</code> <p>Indifference zone parameter.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Repeated array of X along the specified axis based on the OCBA allocation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.array([[1,2,3],[4,5,6]])\n    means = [1,2,3]\n    vars = [1,1,1]\n    delta = 50\n    get_ocba_X(X, means, vars, delta)\n    array([[1, 2, 3],\n             [1, 2, 3],\n                [1, 2, 3],\n                [4, 5, 6],\n                [4, 5, 6],\n                [4, 5, 6]])\n</code></pre> Source code in <code>spotPython/budget/ocba.py</code> <pre><code>def get_ocba_X(X, means, vars, delta) -&gt; float64:\n    \"\"\"\n    This function calculates the OCBA allocation and repeats the input array X along the specified axis.\n\n    Args:\n        X (numpy.ndarray): Input array to be repeated.\n        means (list): List of means for each alternative.\n        vars (list): List of variances for each alternative.\n        delta (float): Indifference zone parameter.\n\n    Returns:\n        (numpy.ndarray): Repeated array of X along the specified axis based on the OCBA allocation.\n\n    Examples:\n        &gt;&gt;&gt; X = np.array([[1,2,3],[4,5,6]])\n            means = [1,2,3]\n            vars = [1,1,1]\n            delta = 50\n            get_ocba_X(X, means, vars, delta)\n            array([[1, 2, 3],\n                     [1, 2, 3],\n                        [1, 2, 3],\n                        [4, 5, 6],\n                        [4, 5, 6],\n                        [4, 5, 6]])\n\n    \"\"\"\n    o = get_ocba(means=means, vars=vars, delta=delta)\n    return repeat(X, o, axis=0)\n</code></pre>"},{"location":"reference/spotPython/build/kriging/","title":"kriging","text":""},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging","title":"<code>Kriging</code>","text":"<p>             Bases: <code>surrogates</code></p> <p>Kriging surrogate.</p> <p>Attributes:</p> Name Type Description <code>nat_range_X</code> <code>list</code> <p>List of X natural ranges.</p> <code>nat_range_y</code> <code>list</code> <p>List of y nat ranges.</p> <code>noise</code> <code>bool</code> <p>noisy objective function. Default: False. If <code>True</code>, regression kriging will be used.</p> <code>var_type</code> <code>str</code> <p>variable type. Can be either <code>\"num</code>\u201d (numerical) of <code>\"factor\"</code> (factor).</p> <code>num_mask</code> <code>array</code> <p>array of bool variables. <code>True</code> represent numerical (float) variables.</p> <code>factor_mask</code> <code>array</code> <p>array of factor variables. <code>True</code> represents factor (unordered) variables.</p> <code>int_mask</code> <code>array</code> <p>array of integer variables. <code>True</code> represents integers (ordered) variables.</p> <code>ordered_mask</code> <code>array</code> <p>array of ordered variables. <code>True</code> represents integers or float (ordered) variables. Set of veriables which an order relation, i.e., they are either num (float) or int.</p> <code>name</code> <code>str</code> <p>Surrogate name</p> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>use_cod_y</code> <code>bool</code> <p>Use coded y values.</p> <code>sigma</code> <code>float</code> <p>Kriging sigma.</p> <code>gen</code> <code>method</code> <p>Design generator, e.g., spotPython.design.spacefilling.spacefilling.</p> <code>min_theta</code> <code>float</code> <p>min log10 theta value. Defaults: -6.</p> <code>max_theta</code> <code>float</code> <p>max log10 theta value. Defaults: 3.</p> <code>min_p</code> <code>float</code> <p>min p value. Default: 1.</p> <code>max_p</code> <code>float</code> <p>max p value. Default: 2.</p> Source code in <code>spotPython/build/kriging.py</code> <pre><code>class Kriging(surrogates):\n    \"\"\"Kriging surrogate.\n\n    Attributes:\n        nat_range_X (list):\n            List of X natural ranges.\n        nat_range_y (list):\n            List of y nat ranges.\n        noise (bool):\n            noisy objective function. Default: False. If `True`, regression kriging will be used.\n        var_type (str):\n            variable type. Can be either `\"num`\" (numerical) of `\"factor\"` (factor).\n        num_mask (array):\n            array of bool variables. `True` represent numerical (float) variables.\n        factor_mask (array):\n            array of factor variables. `True` represents factor (unordered) variables.\n        int_mask (array):\n            array of integer variables. `True` represents integers (ordered) variables.\n        ordered_mask (array):\n            array of ordered variables. `True` represents integers or float (ordered) variables.\n            Set of veriables which an order relation, i.e., they are either num (float) or int.\n        name (str):\n            Surrogate name\n        seed (int):\n            Random seed.\n        use_cod_y (bool):\n            Use coded y values.\n        sigma (float):\n            Kriging sigma.\n        gen (method):\n            Design generator, e.g., spotPython.design.spacefilling.spacefilling.\n        min_theta (float):\n            min log10 theta value. Defaults: -6.\n        max_theta (float):\n            max log10 theta value. Defaults: 3.\n        min_p (float):\n            min p value. Default: 1.\n        max_p (float):\n            max p value. Default: 2.\n    \"\"\"\n    def __init__(\n            self: object,\n            noise: bool = False,\n            cod_type: Optional[str] = \"norm\",\n            var_type: List[str] = [\"num\"],\n            use_cod_y: bool = False,\n            name: str = \"kriging\",\n            seed: int = 124,\n            model_optimizer=None,\n            model_fun_evals: Optional[int] = None,\n            min_theta: float = -3,\n            max_theta: float = 2,\n            n_theta: int = 1,\n            n_p: int = 1,\n            optim_p: bool = False,\n            log_level: int = 50,\n            spot_writer=None,\n            counter=None,\n            **kwargs\n    ):\n        \"\"\"\n        Initialize the Kriging surrogate.\n\n        Args:\n            noise (bool): Use regression instead of interpolation kriging. Defaults to False.\n            cod_type (Optional[str]):\n                Normalize or standardize X and values.\n                Can be None, \"norm\", or \"std\". Defaults to \"norm\".\n            var_type (List[str]):\n                Variable type. Can be either \"num\" (numerical) or \"factor\" (factor).\n                Defaults to [\"num\"].\n            use_cod_y (bool): Use coded y values (instead of natural one). Defaults to False.\n            name (str): Surrogate name. Defaults to \"kriging\".\n            seed (int): Random seed. Defaults to 124.\n            model_optimizer : Optimizer on the surrogate. If None, differential_evolution is selected.\n            model_fun_evals (Optional[int]): Number of iterations used by the optimizer on the surrogate.\n            min_theta (float): Min log10 theta value. Defaults to -3.\n            max_theta (float): Max log10 theta value. Defaults to 2.\n            n_theta (int): Number of theta values. Defaults to 1.\n            n_p (int): Number of p values. Defaults to 1.\n            optim_p (bool): Determines whether p should be optimized.\n            log_level (int): Logging level, e.g., 20 is \"INFO\". Defaults to 50 (\"CRITICAL\").\n            spot_writer : Spot writer.\n            counter : Counter.\n\n        Examples:\n            Surrogate of the x*sin(x) function, see [1].\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n                import numpy as np\n                import matplotlib.pyplot as plt\n                rng = np.random.RandomState(1)\n                X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n                y = np.squeeze(X * np.sin(X))\n                training_indices = rng.choice(arange(y.size), size=6, replace=False)\n                X_train, y_train = X[training_indices], y[training_indices]\n                S = Kriging(name='kriging', seed=124)\n                S.fit(X_train, y_train)\n                mean_prediction, std_prediction = S.predict(X)\n                plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n                plt.scatter(X_train, y_train, label=\"Observations\")\n                plt.plot(X, mean_prediction, label=\"Mean prediction\")\n                plt.fill_between(\n                    X.ravel(),\n                    mean_prediction - 1.96 * std_prediction,\n                    mean_prediction + 1.96 * std_prediction,\n                    alpha=0.5,\n                    label=r\"95% confidence interval\",\n                    )\n                plt.legend()\n                plt.xlabel(\"$x$\")\n                plt.ylabel(\"$f(x)$\")\n                _ = plt.title(\"Gaussian process regression on noise-free dataset\")\n                plt.show()\n\n        References:\n\n            [[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)]\n            scikit-learn: Gaussian Processes regression: basic introductory example\n\n        \"\"\"\n        super().__init__(name, seed, log_level)\n\n        self.noise = noise\n        self.var_type = var_type\n        self.cod_type = cod_type\n        self.use_cod_y = use_cod_y\n        self.name = name\n        self.seed = seed\n        self.log_level = log_level\n        self.spot_writer = spot_writer\n        self.counter = counter\n\n        self.sigma = 0\n        self.eps = sqrt(spacing(1))\n        self.min_theta = min_theta\n        self.max_theta = max_theta\n        self.min_p = 1\n        self.max_p = 2\n        self.min_Lambda = 1e-9\n        self.max_Lambda = 1.\n        self.n_theta = n_theta\n        self.n_p = n_p\n        self.optim_p = optim_p\n        # Psi matrix condition:\n        self.cnd_Psi = 0\n        self.inf_Psi = False\n\n        self.model_optimizer = model_optimizer\n        if self.model_optimizer is None:\n            self.model_optimizer = differential_evolution\n        self.model_fun_evals = model_fun_evals\n        # differential evaluation uses maxiter = 1000\n        # and sets the number of function evaluations to\n        # (maxiter + 1) * popsize * N, which results in\n        # 1000 * 15 * k, because the default popsize is 15 and\n        # N is the number of parameters. This seems to be quite large:\n        # for k=2 these are 30 000 iterations. Therefore we set this value to\n        # 100\n        if self.model_fun_evals is None:\n            self.model_fun_evals = 100\n\n        # Logging information\n        self.log[\"negLnLike\"] = []\n        self.log[\"theta\"] = []\n        self.log[\"p\"] = []\n        self.log[\"Lambda\"] = []\n        # Logger\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def exp_imp(self, y0: float, s0: float) -&gt; float:\n        \"\"\"\n        Calculates the expected improvement for a given function value and error in coded units.\n\n        Args:\n            self (object): The Kriging object.\n            y0 (float): The function value in coded units.\n            s0 (float): The error value.\n\n        Returns:\n            float: The expected improvement value.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; S = Kriging(name='kriging', seed=124)\n            &gt;&gt;&gt; S.cod_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n            &gt;&gt;&gt; S.mean_cod_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n            &gt;&gt;&gt; S.exp_imp(1.0, 2.0)\n            0.0\n\n        \"\"\"\n        # y_min = min(self.cod_y)\n        y_min = min(self.mean_cod_y)\n        if s0 &lt;= 0.0:\n            EI = 0.0\n        elif s0 &gt; 0.0:\n            EI_one = (y_min - y0) * (\n                    0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * ((y_min - y0) / s0))\n            )\n            EI_two = (s0 * (1.0 / sqrt(2.0 * pi))) * (\n                exp(-(1.0 / 2.0) * ((y_min - y0) ** 2.0 / s0 ** 2.0))\n            )\n            EI = EI_one + EI_two\n        return EI\n\n    def set_de_bounds(self) -&gt; None:\n        \"\"\"\n        Determine search bounds for model_optimizer, e.g., differential evolution.\n\n        This method sets the attribute `de_bounds` of the object to a list of lists,\n        where each inner list represents the lower and upper bounds for a parameter\n        being optimized. The number of inner lists is determined by the number of\n        parameters being optimized (`n_theta` and `n_p`), as well as whether noise is\n        being considered (`noise`).\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n            &gt;&gt;&gt; obj = MyClass()\n            &gt;&gt;&gt; obj.set_de_bounds()\n            &gt;&gt;&gt; print(obj.de_bounds)\n            [[min_theta, max_theta], [min_theta, max_theta], ..., [min_p, max_p], [min_Lambda, max_Lambda]]\n\n        Returns:\n            None\n        \"\"\"\n        de_bounds = [[self.min_theta, self.max_theta] for _ in range(self.n_theta)]\n        if self.optim_p:\n            de_bounds += [[self.min_p, self.max_p] for _ in range(self.n_p)]\n            if self.noise:\n                de_bounds.append([self.min_Lambda, self.max_Lambda])\n        else:\n            if self.noise:\n                de_bounds.append([self.min_Lambda, self.max_Lambda])\n        self.de_bounds = de_bounds\n\n    def extract_from_bounds(self, new_theta_p_Lambda: np.ndarray) -&gt; None:\n        \"\"\"\n        Extract `theta`, `p`, and `Lambda` from bounds. The kriging object stores\n        `theta` as an array,  `p` as an array, and `Lambda` as a float.\n\n        Args:\n            self (object): The Kriging object.\n            new_theta_p_Lambda (np.ndarray):\n                1d-array with theta, p, and Lambda values. Order is important.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n            &gt;&gt;&gt; obj = MyClass()\n            &gt;&gt;&gt; obj.extract_from_bounds(np.array([1, 2, 3]))\n            &gt;&gt;&gt; print(obj.theta)\n            [1]\n            &gt;&gt;&gt; print(obj.p)\n            [2]\n            &gt;&gt;&gt; print(obj.Lambda)\n            3\n\n        Returns:\n            None\n        \"\"\"\n        self.theta = new_theta_p_Lambda[:self.n_theta]\n        if self.optim_p:\n            self.p = new_theta_p_Lambda[self.n_theta:self.n_theta + self.n_p]\n            if self.noise:\n                self.Lambda = new_theta_p_Lambda[self.n_theta + self.n_p]\n        else:\n            if self.noise:\n                self.Lambda = new_theta_p_Lambda[self.n_theta]\n\n    def optimize_model(self) -&gt; Union[List[float], Tuple[float]]:\n        \"\"\"\n        Optimize the model using the specified model_optimizer.\n\n        This method uses the specified model_optimizer to optimize the\n        likelihood function (`fun_likelihood`) with respect to the model parameters.\n        The optimization is performed within the bounds specified by the attribute\n        `de_bounds`.\n        The result of the optimization is returned as a list or tuple of optimized parameter values.\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n            &gt;&gt;&gt; obj = MyClass()\n            &gt;&gt;&gt; result = obj.optimize_model()\n            &gt;&gt;&gt; print(result)\n            [optimized_theta, optimized_p, optimized_Lambda]\n\n        Returns:\n            result[\"x\"] (Union[List[float], Tuple[float]]):\n                A list or tuple of optimized parameter values.\n        \"\"\"\n        if self.model_optimizer.__name__ == 'dual_annealing':\n            result = self.model_optimizer(func=self.fun_likelihood,\n                                          bounds=self.de_bounds)\n        elif self.model_optimizer.__name__ == 'differential_evolution':\n            result = self.model_optimizer(func=self.fun_likelihood,\n                                          bounds=self.de_bounds,\n                                          maxiter=self.model_fun_evals,\n                                          seed=self.seed)\n        elif self.model_optimizer.__name__ == 'direct':\n            result = self.model_optimizer(func=self.fun_likelihood,\n                                          bounds=self.de_bounds,\n                                          # maxfun=self.model_fun_evals,\n                                          eps=1e-2)\n        elif self.model_optimizer.__name__ == 'shgo':\n            result = self.model_optimizer(func=self.fun_likelihood,\n                                          bounds=self.de_bounds)\n        elif self.model_optimizer.__name__ == 'basinhopping':\n            result = self.model_optimizer(func=self.fun_likelihood,\n                                          x0=mean(self.de_bounds, axis=1))\n        else:\n            result = self.model_optimizer(func=self.fun_likelihood, bounds=self.de_bounds)\n        return result[\"x\"]\n\n    def update_log(self) -&gt; None:\n        \"\"\"\n        Update the log with the current values of negLnLike, theta, p, and Lambda.\n\n        This method appends the current values of negLnLike, theta, p (if optim_p is True),\n        and Lambda (if noise is True)\n        to their respective lists in the log dictionary.\n        It also updates the log_length attribute with the current length\n        of the negLnLike list in the log.\n\n        If spot_writer is not None, this method also writes the current values of\n        negLnLike, theta, p (if optim_p is True),\n        and Lambda (if noise is True) to the spot_writer object.\n\n        Args:\n            self (object): The Kriging object.\n\n        Returns:\n            None\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n            &gt;&gt;&gt; obj = MyClass()\n            &gt;&gt;&gt; obj.update_log()\n            &gt;&gt;&gt; print(obj.log)\n            {'negLnLike': [0.5], 'theta': [0.1], 'p': [0.2], 'Lambda': [0.3]}\n        \"\"\"\n        self.log[\"negLnLike\"] = append(self.log[\"negLnLike\"], self.negLnLike)\n        self.log[\"theta\"] = append(self.log[\"theta\"], self.theta)\n        if self.optim_p:\n            self.log[\"p\"] = append(self.log[\"p\"], self.p)\n        if self.noise:\n            self.log[\"Lambda\"] = append(self.log[\"Lambda\"], self.Lambda)\n        # get the length of the log\n        self.log_length = len(self.log[\"negLnLike\"])\n        if self.spot_writer is not None:\n            writer = self.spot_writer\n            negLnLike = self.negLnLike.copy()\n            writer.add_scalar(\"spot_negLnLike\", negLnLike, self.counter+self.log_length)\n            # add the self.n_theta theta values to the writer with one key \"theta\",\n            # i.e, the same key for all theta values\n            theta = self.theta.copy()\n            writer.add_scalars(\"spot_theta\", {f\"theta_{i}\": theta[i] for i in range(self.n_theta)},\n                               self.counter+self.log_length)\n            if self.noise:\n                Lambda = self.Lambda.copy()\n                writer.add_scalar(\"spot_Lambda\", Lambda, self.counter+self.log_length)\n            if self.optim_p:\n                p = self.p.copy()\n                writer.add_scalars(\"spot_p\", {f\"p_{i}\": p[i] for i in range(self.n_p)}, self.counter+self.log_length)\n            writer.flush()\n\n    def fit(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; object:\n        \"\"\"\n        Fits the hyperparameters (`theta`, `p`, `Lambda`) of the Kriging model.\n\n        The function computes the following internal values:\n        1. `theta`, `p`, and `Lambda` values via optimization of the function `fun_likelihood()`.\n        2. Correlation matrix `Psi` via `rebuildPsi()`.\n\n        Args:\n            self (object): The Kriging object.\n            nat_X (np.ndarray): Sample points.\n            nat_y (np.ndarray): Function values.\n\n        Returns:\n            object: Fitted estimator.\n\n        Attributes:\n            theta (np.ndarray): Kriging theta values. Shape (k,).\n            p (np.ndarray): Kriging p values. Shape (k,).\n            LnDetPsi (np.float64): Determinant Psi matrix.\n            Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n            psi (np.ndarray): psi vector. Shape (n,).\n            one (np.ndarray): vector of ones. Shape (n,).\n            mu (np.float64): Kriging expected mean value mu.\n            U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n            SigmaSqr (np.float64): Sigma squared value.\n            Lambda (float): lambda noise value.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; nat_X = np.array([[1, 2], [3, 4]])\n            &gt;&gt;&gt; nat_y = np.array([1, 2])\n            &gt;&gt;&gt; surrogate = Kriging()\n            &gt;&gt;&gt; surrogate.fit(nat_X, nat_y)\n        \"\"\"\n        self.initialize_variables(nat_X, nat_y)\n        self.set_variable_types()\n        self.nat_to_cod_init()\n        self.set_theta_values()\n        self.initialize_matrices()\n        # build_Psi() and build_U() are called in fun_likelihood\n        self.set_de_bounds()\n        # Finally, set new theta and p values and update the surrogate again\n        # for new_theta_p_Lambda in de_results[\"x\"]:\n        new_theta_p_Lambda = self.optimize_model()\n        self.extract_from_bounds(new_theta_p_Lambda)\n        self.build_Psi()\n        self.build_U()\n        # TODO: check if the following line is necessary!\n        self.likelihood()\n        self.update_log()\n\n    def initialize_variables(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; None:\n        \"\"\"\n        Initialize variables for the class instance.\n\n        This method takes in the independent and dependent variable data as input\n        and initializes the class instance variables.\n        It creates deep copies of the input data and stores them in the\n        instance variables `nat_X` and `nat_y`.\n        It also calculates the number of observations `n` and\n        the number of independent variables `k` from the shape of `nat_X`.\n        Finally, it creates empty arrays with the same shape as `nat_X`\n        and `nat_y` and stores them in the instance variables `cod_X` and `cod_y`.\n\n        Args:\n            self (object): The Kriging object.\n            nat_X (np.ndarray): The independent variable data.\n            nat_y (np.ndarray): The dependent variable data.\n\n        Returns:\n            None\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; surrogate = Kriging()\n            &gt;&gt;&gt; nat_X = np.array([[1, 2], [3, 4]])\n            &gt;&gt;&gt; nat_y = np.array([1, 2])\n            &gt;&gt;&gt; surrogate.initialize_variables(nat_X, nat_y)\n            &gt;&gt;&gt; surrogate.nat_X\n            array([[1, 2],\n                     [3, 4]])\n            &gt;&gt;&gt; surrogate.nat_y\n            array([1, 2])\n\n        \"\"\"\n        self.nat_X = copy.deepcopy(nat_X)\n        self.nat_y = copy.deepcopy(nat_y)\n        self.n = self.nat_X.shape[0]\n        self.k = self.nat_X.shape[1]\n        self.cod_X = np.empty_like(self.nat_X)\n        self.cod_y = np.empty_like(self.nat_y)\n\n    def set_variable_types(self) -&gt; None:\n        \"\"\"\n        Set the variable types for the class instance.\n\n        This method sets the variable types for the class instance based\n        on the `var_type` attribute. If the length of `var_type` is less\n        than `k`, all variable types are forced to 'num' and a warning is logged.\n        The method then creates masks for each variable\n        type ('num', 'factor', 'int', 'float') using numpy arrays.\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; class MyClass(Kriging):\n            &gt;&gt;&gt;     def __init__(self):\n            &gt;&gt;&gt;         super().__init__()\n            &gt;&gt;&gt;         self.var_type = [\"num\", \"factor\"]\n            &gt;&gt;&gt; instance = MyClass()\n            &gt;&gt;&gt; instance.set_variable_types()\n            &gt;&gt;&gt; instance.num_mask\n            array([ True, False])\n\n        Returns:\n            None\n        \"\"\"\n        # assume all variable types are \"num\" if \"num\" is\n        # specified once:\n        if len(self.var_type) &lt; self.k:\n            self.var_type = self.var_type * self.k\n            logger.warning(\"Warning: All variable types forced to 'num'.\")\n        self.num_mask = np.array(list(map(lambda x: x == \"num\", self.var_type)))\n        self.factor_mask = np.array(list(map(lambda x: x == \"factor\", self.var_type)))\n        self.int_mask = np.array(list(map(lambda x: x == \"int\", self.var_type)))\n        self.ordered_mask = np.array(list(map(lambda x: x == \"int\" or x == \"num\" or x == \"float\", self.var_type)))\n\n    def set_theta_values(self) -&gt; None:\n        \"\"\"\n        Set the theta values for the class instance.\n\n        This method sets the theta values for the class instance based\n        on the `n_theta` and `k` attributes. If `n_theta` is greater than\n        `k`, `n_theta` is set to `k` and a warning is logged.\n        The method then initializes the `theta` attribute as a list\n        of zeros with length `n_theta`.\n        The `x0_theta` attribute is also initialized as a list of ones\n        with length `n_theta`, multiplied by `n / (100 * k)`.\n\n        Args:\n            self (object): The Kriging object.\n        Returns:\n            None\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; class MyClass(Kriging):\n            &gt;&gt;&gt;     def __init__(self):\n            &gt;&gt;&gt;         super().__init__()\n            &gt;&gt;&gt;         self.n_theta = 3\n            &gt;&gt;&gt;         self.k = 2\n            &gt;&gt;&gt; instance = MyClass()\n            &gt;&gt;&gt; instance.set_theta_values()\n            &gt;&gt;&gt; instance.theta\n            array([0., 0., 0.])\n        \"\"\"\n        if self.n_theta &gt; self.k:\n            self.n_theta = self.k\n            logger.warning(\"More theta values than dimensions. `n_theta` set to `k`.\")\n        self.theta: List[float] = zeros(self.n_theta)\n        # TODO: Currently not used:\n        self.x0_theta: List[float] = ones((self.n_theta,)) * self.n / (100 * self.k)\n\n    def initialize_matrices(self) -&gt; None:\n        \"\"\"\n        Initialize the matrices for the class instance.\n\n        This method initializes several matrices and attributes for the class instance.\n        The `p` attribute is initialized as a list of ones with length `n_p`, multiplied by 2.0.\n        The `pen_val` attribute is initialized as the natural logarithm of the\n        variance of `nat_y`, multiplied by `n`, plus 1e4.\n        The `negLnLike`, `LnDetPsi`, `mu`, `U`, `SigmaSqr`, and `Lambda` attributes are all set to None.\n        The `gen` attribute is initialized using the `spacefilling` function with arguments `k` and `seed`.\n        The `Psi` attribute is initialized as a zero matrix with shape `(n, n)` and dtype `float64`.\n        The `psi` attribute is initialized as a zero matrix with shape `(n, 1)`.\n        The `one` attribute is initialized as a list of ones with length `n`.\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; class MyClass(Kriging):\n            &gt;&gt;&gt;     def __init__(self):\n            &gt;&gt;&gt;         super().__init__()\n            &gt;&gt;&gt;         self.n_p = 2\n            &gt;&gt;&gt;         self.n = 3\n            &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n            &gt;&gt;&gt;         self.k = 2\n            &gt;&gt;&gt;         self.seed = 1\n            &gt;&gt;&gt; instance = MyClass()\n            &gt;&gt;&gt; instance.initialize_matrices()\n\n        Returns:\n            None\n        \"\"\"\n        self.p = ones(self.n_p) * 2.0\n        self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n        self.negLnLike = None\n        self.gen = spacefilling(k=self.k, seed=self.seed)\n        self.LnDetPsi = None\n        self.Psi = zeros((self.n, self.n), dtype=float64)\n        self.psi = zeros((self.n, 1))\n        self.one = ones(self.n)\n        self.mu = None\n        self.U = None\n        self.SigmaSqr = None\n        self.Lambda = None\n\n    def fun_likelihood(self, new_theta_p_Lambda: np.ndarray) -&gt; float:\n        \"\"\"\n        Compute log likelihood for a set of hyperparameters (theta, p, Lambda).\n\n        This method computes the log likelihood for a set of hyperparameters\n        (theta, p, Lambda) by performing the following steps:\n        1. Extracts the hyperparameters from the input array using `extract_from_bounds()`.\n        2. Checks if any element in `10^theta` is equal to 0. If so, logs a warning and\n        returns the penalty value (`pen_val`).\n        3. Builds the `Psi` matrix using `build_Psi()`.\n        4. Checks if `Psi` is ill-conditioned or infinite. If so, logs a warning and returns\n        the penalty value (`pen_val`).\n        5. Builds the `U` matrix using `build_U()`. If an exception occurs, logs an error and\n        returns the penalty value (`pen_val`).\n        6. Computes the negative log likelihood using `likelihood()`.\n        7. Returns the computed negative log likelihood (`negLnLike`).\n\n        Args:\n            self (object): The Kriging object.\n            new_theta_p_Lambda (np.ndarray):\n                An array containing the `theta`, `p`, and `Lambda` values.\n\n        Returns:\n            float:\n                The negative log likelihood of the surface at the specified hyperparameters.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; class MyClass(Kriging):\n            &gt;&gt;&gt;     def __init__(self):\n            &gt;&gt;&gt;         super().__init__()\n            &gt;&gt;&gt;         self.n_p = 2\n            &gt;&gt;&gt;         self.n = 3\n            &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n            &gt;&gt;&gt;         self.k = 2\n            &gt;&gt;&gt;         self.seed = 1\n            &gt;&gt;&gt; instance = MyClass()\n            &gt;&gt;&gt; negLnLike = instance.fun_likelihood(new_theta_p_Lambda)\n            &gt;&gt;&gt; print(negLnLike)\n\n        \"\"\"\n        self.extract_from_bounds(new_theta_p_Lambda)\n        if self.__is_any__(power(10.0, self.theta), 0):\n            logger.warning(\"Failure in fun_likelihood: 10^theta == 0. Setting negLnLike to %s\", self.pen_val)\n            return self.pen_val\n        self.build_Psi()\n        if (self.inf_Psi or self.cnd_Psi &gt; 1e9):\n            logger.warning(\"Failure in fun_likelihood: Psi is ill conditioned: %s\", self.cnd_Psi)\n            logger.warning(\"Setting negLnLike to: %s\", self.pen_val)\n            return self.pen_val\n\n        try:\n            self.build_U()\n        except Exception as error:\n            penalty_value = self.pen_val\n            print(\"Error in fun_likelihood(). Call to build_U() failed.\")\n            print(\"error=%s, type(error)=%s\" % (error, type(error)))\n            print(\"Setting negLnLike to %.2f.\" % self.pen_val)\n            return penalty_value\n        self.likelihood()\n        return self.negLnLike\n\n    def __is_any__(self, x: Union[np.ndarray, Any], v: Any) -&gt; bool:\n        \"\"\"\n        Check if any element in `x` is equal to `v`.\n\n        This method checks if any element in the input array `x` is equal to the value `v`.\n        If `x` is not an instance of `ndarray`, it is first converted to a numpy array using\n        the `array()` function.\n\n        Args:\n            self (object): The Kriging object.\n            x (np.ndarray or array-like):\n                The input array to check for the presence of value `v`.\n            v (scalar):\n                The value to check for in the input array `x`.\n\n        Returns:\n            bool:\n                True if any element in `x` is equal to `v`, False otherwise.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; class MyClass(Kriging):\n            &gt;&gt;&gt;     def __init__(self):\n            &gt;&gt;&gt;         super().__init__()\n            &gt;&gt;&gt;         self.n_p = 2\n            &gt;&gt;&gt;         self.n = 3\n            &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n            &gt;&gt;&gt;         self.k = 2\n            &gt;&gt;&gt;         self.seed = 1\n\n            &gt;&gt;&gt; instance = MyClass()\n            &gt;&gt;&gt; result = instance.__is_any__(x, v)\n            &gt;&gt;&gt; print(result)\n\n        \"\"\"\n        if not isinstance(x, ndarray):\n            x = array([x])\n        return any(x == v)\n\n    def build_Psi(self) -&gt; None:\n        \"\"\"\n        Constructs a new (n x n) correlation matrix Psi to reflect new data\n        or a change in hyperparameters.\n\n        This method uses `theta`, `p`, and coded `X` values to construct the\n        correlation matrix as described in [Forr08a, p.57].\n\n        Args:\n            self (object): The Kriging object.\n\n        Returns:\n            None\n\n        Raises:\n            LinAlgError: If building Psi fails.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; class MyClass(Kriging):\n            &gt;&gt;&gt;     def __init__(self):\n            &gt;&gt;&gt;         super().__init__()\n            &gt;&gt;&gt;         self.n_p = 2\n            &gt;&gt;&gt;         self.n = 3\n            &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n            &gt;&gt;&gt;         self.k = 2\n            &gt;&gt;&gt;         self.seed = 1\n\n            &gt;&gt;&gt; obj = MyClass()\n            &gt;&gt;&gt; obj.build_Psi()\n\n        \"\"\"\n        self.Psi = zeros((self.n, self.n), dtype=float64)\n        theta = power(10.0, self.theta)\n        if self.n_theta == 1:\n            theta = theta * ones(self.k)\n        try:\n            D = zeros((self.n, self.n))\n            if self.ordered_mask.any():\n                X_ordered = self.cod_X[:, self.ordered_mask]\n                D = squareform(\n                    pdist(\n                        X_ordered, metric='sqeuclidean', out=None, w=theta[self.ordered_mask]))\n            if self.factor_mask.any():\n                X_factor = self.cod_X[:, self.factor_mask]\n                D = (D + squareform(\n                    pdist(X_factor,\n                          metric='hamming',\n                          out=None,\n                          w=theta[self.factor_mask])))\n            self.Psi = exp(-D)\n        except LinAlgError as err:\n            print(f\"Building Psi failed:\\n {self.Psi}. {err=}, {type(err)=}\")\n        if self.noise:\n            self.Psi[diag_indices_from(self.Psi)] += self.Lambda\n        else:\n            self.Psi[diag_indices_from(self.Psi)] += self.eps\n        if (isinf(self.Psi)).any():\n            self.inf_Psi = True\n        self.cnd_Psi = cond(self.Psi)\n\n    def build_U(self, scipy: bool = True) -&gt; None:\n        \"\"\"\n        Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57].\n\n        This method uses either `scipy_cholesky` or numpy's `cholesky` to perform the Cholesky factorization of Psi.\n\n        Args:\n            self (object):\n                The Kriging object.\n            scipy (bool):\n                If True, use `scipy_cholesky`.\n                If False, use numpy's `cholesky`.\n                Defaults to True.\n\n        Returns:\n            None\n\n        Raises:\n            LinAlgError:\n                If Cholesky factorization fails for Psi.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; class MyClass(Kriging):\n            &gt;&gt;&gt;     def __init__(self):\n            &gt;&gt;&gt;         super().__init__()\n            &gt;&gt;&gt;         self.n_p = 2\n            &gt;&gt;&gt;         self.n = 3\n            &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n            &gt;&gt;&gt;         self.k = 2\n            &gt;&gt;&gt;         self.seed = 1\n\n            &gt;&gt;&gt; obj = MyClass()\n            &gt;&gt;&gt; obj.build_U()\n        \"\"\"\n        try:\n            self.U = scipy_cholesky(self.Psi, lower=True) if scipy else cholesky(self.Psi)\n            self.U = self.U.T\n        except LinAlgError as err:\n            print(f\"build_U() Cholesky failed for Psi:\\n {self.Psi}. {err=}, {type(err)=}\")\n\n    def likelihood(self) -&gt; None:\n        \"\"\"\n        Calculates the negative of the concentrated log-likelihood.\n\n        This method implements equation (2.32) in [Forr08a] to calculate\n        the negative of the concentrated log-likelihood. It also modifies `mu`,\n        `SigmaSqr`, `LnDetPsi`, and `negLnLike`.\n\n        Note:\n            `build_Psi` and `build_U` should be called first.\n\n        Args:\n            self (object):\n                The Kriging object.\n\n        Returns:\n            None\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; class MyClass(Kriging):\n            &gt;&gt;&gt;     def __init__(self):\n            &gt;&gt;&gt;         super().__init__()\n            &gt;&gt;&gt;         self.n_p = 2\n            &gt;&gt;&gt;         self.n = 3\n            &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n            &gt;&gt;&gt;         self.k = 2\n            &gt;&gt;&gt;         self.seed = 1\n\n            &gt;&gt;&gt; obj = MyClass()\n            &gt;&gt;&gt; obj.build_Psi()\n            &gt;&gt;&gt; obj.build_U()\n            &gt;&gt;&gt; obj.likelihood()\n        \"\"\"\n        # (2.20) in [Forr08a]:\n        U_T_inv_one = solve(self.U.T, self.one)\n        U_T_inv_cod_y = solve(self.U.T, self.cod_y)\n        mu = self.one.T.dot(solve(self.U, U_T_inv_cod_y)) / self.one.T.dot(solve(self.U, U_T_inv_one))\n        self.mu = mu\n        # (2.31) in [Forr08a]\n        cod_y_minus_mu = self.cod_y - self.one.dot(self.mu)\n        self.SigmaSqr = cod_y_minus_mu.T.dot(solve(self.U, solve(self.U.T, cod_y_minus_mu))) / self.n\n        # (2.32) in [Forr08a]\n        self.LnDetPsi = 2.0 * sum(log(abs(diag(self.U))))\n        self.negLnLike = -1.0 * (-(self.n / 2.0) * log(self.SigmaSqr) - 0.5 * self.LnDetPsi)\n\n    def plot(self, show: Optional[bool] = True) -&gt; None:\n        \"\"\"\n        This function plots 1D and 2D surrogates.\n\n        Args:\n            self (object):\n                The Kriging object.\n            show (bool):\n                If `True`, the plots are displayed.\n                If `False`, `plt.show()` should be called outside this function.\n\n        Returns:\n            None\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; class MyClass(Kriging):\n            &gt;&gt;&gt;     def __init__(self):\n            &gt;&gt;&gt;         super().__init__()\n            &gt;&gt;&gt;         self.n_p = 2\n            &gt;&gt;&gt;         self.n = 3\n            &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n            &gt;&gt;&gt;         self.k = 2\n            &gt;&gt;&gt;         self.seed = 1\n\n            &gt;&gt;&gt; plot(show=True)\n        \"\"\"\n        if self.k == 1:\n            # TODO: Improve plot (add conf. interval etc.)\n            fig = pylab.figure(figsize=(9, 6))\n            # t1 = array(arange(0.0, 1.0, 0.01))\n            # y1 = array([self.predict(array([x]), return_val=\"y\") for x in t1])\n            # plt.figure()\n            # plt.plot(t1, y1, \"k\")\n            # if show:\n            #     plt.show()\n            #\n            n_grid = 100\n            x = linspace(\n                self.nat_range_X[0][0], self.nat_range_X[0][1], num=n_grid\n            )\n            y = self.predict(x)\n            plt.figure()\n            plt.plot(x, y, \"k\")\n            if show:\n                plt.show()\n\n        if self.k == 2:\n            fig = pylab.figure(figsize=(9, 6))\n            n_grid = 100\n            x = linspace(\n                self.nat_range_X[0][0], self.nat_range_X[0][1], num=n_grid\n            )\n            y = linspace(\n                self.nat_range_X[1][0], self.nat_range_X[1][1], num=n_grid\n            )\n            X, Y = meshgrid(x, y)\n            # Predict based on the optimized results\n            zz = array(\n                [self.predict(array([x, y]), return_val=\"all\") for x, y in zip(ravel(X), ravel(Y))]\n            )\n            zs = zz[:, 0, :]\n            zse = zz[:, 1, :]\n            Z = zs.reshape(X.shape)\n            Ze = zse.reshape(X.shape)\n\n            if self.cod_type == \"norm\":\n                nat_point_X = (\n                                      self.cod_X[:, 0] * (self.nat_range_X[0][1] - self.nat_range_X[0][0])\n                              ) + self.nat_range_X[0][0]\n                nat_point_Y = (\n                                      self.cod_X[:, 1] * (self.nat_range_X[1][1] - self.nat_range_X[1][0])\n                              ) + self.nat_range_X[1][0]\n            elif self.cod_type == \"std\":\n                nat_point_X = self.cod_X[:, 0] * self.nat_std_X[0] + self.nat_mean_X[0]\n                nat_point_Y = self.cod_X[:, 1] * self.nat_std_X[1] + self.nat_mean_X[1]\n            else:\n                nat_point_X = self.cod_X[:, 0]\n                nat_point_Y = self.cod_X[:, 1]\n            contour_levels = 30\n            ax = fig.add_subplot(224)\n            # plot predicted values:\n            pylab.contourf(X, Y, Ze, contour_levels, cmap=\"jet\")\n            pylab.title(\"Error\")\n            pylab.colorbar()\n            # plot observed points:\n            pylab.plot(nat_point_X, nat_point_Y, \"ow\")\n            #\n            ax = fig.add_subplot(223)\n            # plot predicted values:\n            plt.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\")\n            plt.title(\"Surrogate\")\n            # plot observed points:\n            pylab.plot(nat_point_X, nat_point_Y, \"ow\", zorder=3)\n            pylab.colorbar()\n            #\n            ax = fig.add_subplot(221, projection=\"3d\")\n            ax.plot_surface(X, Y, Z, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n            #\n            ax = fig.add_subplot(222, projection=\"3d\")\n            ax.plot_surface(X, Y, Ze, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n            #\n            pylab.show()\n\n    def predict(self, nat_X: ndarray, nat: bool = True, return_val: str = \"y\") -&gt; Union[float,\n                                                                                        Tuple[float,\n                                                                                              float,\n                                                                                              float]]:\n        \"\"\"\n        This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X.\n\n        Args:\n            self (object):\n                The Kriging object.\n            nat_X (ndarray):\n                Design variable to evaluate in natural units.\n            nat (bool):\n                argument `nat_X` is in natural range. Default: `True`.\n                If set to `False`, `nat_X` will not be normalized (which might be useful\n                if already normalized y values are used).\n            return_val (str):\n                whether `y`, `s`, neg. `ei` (negative expected improvement),\n                or all three values are returned.\n                Default is (for compatibility with sklearn) \"y\". To return `s`, select \"s\",\n                to return neg. `ei`, select \"ei\".\n                To return the tuple `(y, s, ei)`, select \"all\".\n\n        Returns:\n            float:\n                The predicted value in natural units if return_val is \"y\".\n            float:\n                predicted error if return_val is \"s\".\n            float:\n                expected improvement if return_val is \"ei\".\n            Tuple[float, float, float]:\n                The predicted value in natural units, predicted error\n                and expected improvement if return_val is \"all\".\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; from numpy import array\n            &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n            &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n            &gt;&gt;&gt; k = Kriging(X, y)\n            &gt;&gt;&gt; k.predict(array([[0.3, 0.3]]))\n            array([0.09])\n\n        \"\"\"\n        # Check for the shape and the type of the Input\n        if isinstance(nat_X, ndarray):\n            try:\n                X = nat_X.reshape(-1, self.nat_X.shape[1])\n                X = repair_non_numeric(X, self.var_type)\n            except Exception:\n                raise TypeError(\"13.1: Input to predict was not convertible to the size of X\")\n        else:\n            raise TypeError(f\"type of the given input is an {type(nat_X)} instead of an ndarray\")\n        n = X.shape[0]\n        y = empty(n, dtype=float)\n        s = empty(n, dtype=float)\n        ei = empty(n, dtype=float)\n        for i in range(n):\n            if nat:\n                x = self.nat_to_cod_x(X[i, :])\n            else:\n                x = X[i, :]\n            y[i], s[i], ei[i] = self.predict_coded(x)\n        if return_val == \"y\":\n            return y\n        elif return_val == \"s\":\n            return s\n        elif return_val == \"ei\":\n            return -1.0 * ei\n        else:\n            return y, s, -1.0 * ei\n\n    def build_psi_vec(self, cod_x: ndarray) -&gt; None:\n        \"\"\"\n        Build the psi vector. Needed by `predict_cod`, `predict_err_coded`,\n        `regression_predict_coded`. Modifies `self.psi`.\n\n        Args:\n            self (object):\n                The Kriging object.\n            cod_x (ndarray):\n                point to calculate psi\n\n        Returns:\n            None\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; from numpy import array\n            &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n            &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n            &gt;&gt;&gt; k = Kriging(X, y)\n            &gt;&gt;&gt; cod_x = array([0.3, 0.3])\n            &gt;&gt;&gt; build_psi_vec(cod_x)\n\n        \"\"\"\n        self.psi = zeros((self.n))\n        # theta = self.theta  # TODO:\n        theta = power(10.0, self.theta)\n        if self.n_theta == 1:\n            theta = theta * ones(self.k)\n        try:\n            D = zeros((self.n))\n            if self.ordered_mask.any():\n                X_ordered = self.cod_X[:, self.ordered_mask]\n                x_ordered = cod_x[self.ordered_mask]\n                D = cdist(x_ordered.reshape(-1, sum(self.ordered_mask)),\n                          X_ordered.reshape(-1, sum(self.ordered_mask)),\n                          metric='sqeuclidean',\n                          out=None,\n                          w=theta[self.ordered_mask])\n            if self.factor_mask.any():\n                X_factor = self.cod_X[:, self.factor_mask]\n                x_factor = cod_x[self.factor_mask]\n                D = (D + cdist(x_factor.reshape(-1, sum(self.factor_mask)),\n                               X_factor.reshape(-1, sum(self.factor_mask)),\n                               metric='hamming',\n                               out=None,\n                               w=theta[self.factor_mask]))\n            self.psi = exp(-D).T\n        except LinAlgError as err:\n            print(f\"Building psi failed:\\n {self.psi}. {err=}, {type(err)=}\")\n\n    def predict_coded(self, cod_x: np.ndarray) -&gt; Tuple[float, float, float]:\n        \"\"\"\n        Kriging prediction of one point in the coded units as described in (2.20) in [Forr08a].\n        The error is returned as well.\n\n        Args:\n            self (object):\n                The Kriging object.\n            cod_x (np.ndarray):\n                Point in coded units to make prediction at.\n\n        Returns:\n            f (float): Predicted value in coded units.\n            SSqr (float): Predicted error.\n            EI (float): Expected improvement.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; from numpy import array\n            &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n            &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n            &gt;&gt;&gt; k = Kriging(X, y)\n            &gt;&gt;&gt; cod_x = array([0.3, 0.3])\n            &gt;&gt;&gt; k.predict_coded(cod_x)\n            (0.09, 0.0, 0.0)\n\n        Note:\n            `self.mu` and `self.SigmaSqr` are computed in `likelihood`, not here.\n            See also [Forr08a, p.60].\n        \"\"\"\n        self.build_psi_vec(cod_x)\n        U_T_inv = solve(self.U.T, self.cod_y - self.one.dot(self.mu))\n        f = self.mu + self.psi.T.dot(solve(self.U, U_T_inv))\n        if self.noise:\n            Lambda = self.Lambda\n        else:\n            Lambda = 0.0\n        # Error in [Forr08a, p.87]:\n        SSqr = self.SigmaSqr * (1 + Lambda - self.psi.T.dot(solve(self.U, solve(self.U.T, self.psi))))\n        SSqr = power(abs(SSqr[0]), 0.5)[0]\n        EI = self.exp_imp(y0=f[0], s0=SSqr)\n        return f[0], SSqr, EI\n\n    def weighted_exp_imp(self, cod_x: np.ndarray, w: float) -&gt; float:\n        \"\"\"\n        Weighted expected improvement.\n\n        Args:\n            self (object): The Kriging object.\n            cod_x (np.ndarray): A coded design vector.\n            w (float): Weight.\n\n        Returns:\n            EI (float): Weighted expected improvement.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; from numpy import array\n            &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n            &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n            &gt;&gt;&gt; k = Kriging(X, y)\n            &gt;&gt;&gt; cod_x = array([0.3, 0.3])\n            &gt;&gt;&gt; w = 0.5\n            &gt;&gt;&gt; k.weighted_exp_imp(cod_x, w)\n            0.0\n\n        References:\n\n            [Sobester et al. 2005].\n        \"\"\"\n        y0, s0 = self.predict_coded(cod_x)\n        y_min = min(self.cod_y)\n        if s0 &lt;= 0.0:\n            EI = 0.0\n        else:\n            y_min_y0 = y_min - y0\n            EI_one = w * (\n                    y_min_y0\n                    * (0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * (y_min_y0 / s0)))\n            )\n            EI_two = (\n                    (1.0 - w)\n                    * (s0 * (1.0 / sqrt(2.0 * pi)))\n                    * (exp(-(1.0 / 2.0) * ((y_min_y0) ** 2.0 / s0 ** 2.0)))\n            )\n            EI = EI_one + EI_two\n        return EI\n\n    def calculate_mean_MSE(self, n_samples: int = 200, points: Optional[np.ndarray] = None) -&gt; Tuple[float, float]:\n        \"\"\"\n        Calculates the mean MSE metric of the model by evaluating MSE at a number of points.\n\n        Args:\n            self (object):\n                The Kriging object.\n            n_samples (int):\n                Number of points to sample the mean squared error at.\n                Ignored if the points argument is specified.\n            points (np.ndarray):\n                An array of points to sample the model at.\n\n        Returns:\n            mean_MSE (float): The mean value of MSE.\n            std_MSE (float): The standard deviation of the MSE points.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; from numpy import array\n            &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n            &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n            &gt;&gt;&gt; k = Kriging(X, y)\n            &gt;&gt;&gt; n_samples = 200\n            &gt;&gt;&gt; mean_MSE, std_MSE = k.calculate_mean_MSE(n_samples)\n            &gt;&gt;&gt; print(f\"Mean MSE: {mean_MSE}, Standard deviation of MSE: {std_MSE}\")\n\n        \"\"\"\n        if points is None:\n            points = self.gen.lhd(n_samples)\n        values = [self.predict(cod_X=point, nat=True, return_val=\"s\") for point in points]\n        return mean(values), std(values)\n\n    def cod_to_nat_x(self, cod_X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Converts an array representing one point in normalized (coded) units to natural (physical or real world) units.\n\n        Args:\n            self (object): The Kriging object.\n            cod_X (np.ndarray):\n                An array representing one point (self.k long) in normalized (coded) units.\n\n        Returns:\n            X (np.ndarray): An array of natural (physical or real world) units.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; from numpy import array\n            &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n            &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n            &gt;&gt;&gt; k = Kriging(X, y)\n            &gt;&gt;&gt; cod_X = array([0.3, 0.3])\n            &gt;&gt;&gt; nat_X = k.cod_to_nat_x(cod_X)\n            &gt;&gt;&gt; print(f\"Natural units: {nat_X}\")\n\n        \"\"\"\n        X = copy.deepcopy(cod_X)\n        if self.cod_type == \"norm\":\n            for i in range(self.k):\n                X[i] = (\n                    X[i] * float(self.nat_range_X[i][1] - self.nat_range_X[i][0])\n                ) + self.nat_range_X[i][0]\n            return X\n        elif self.cod_type == \"std\":\n            for i in range(self.k):\n                X[i] = X[i] * self.nat_std_X[i] + self.nat_mean_X[i]\n            return X\n        else:\n            return cod_X\n\n    def cod_to_nat_y(self, cod_y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Converts a normalized array of coded (model) units in the range of [0,1]\n        to an array of observed values in real-world units.\n\n        Args:\n            self (object): The Kriging object.\n            cod_y (np.ndarray):\n                A normalized array of coded (model) units in the range of [0,1].\n\n        Returns:\n            y (np.ndarray): An array of observed values in real-world units.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; from numpy import array\n            &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n            &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n            &gt;&gt;&gt; k = Kriging(X, y)\n            &gt;&gt;&gt; cod_y = array([0.5, 0.5])\n            &gt;&gt;&gt; nat_y = k.cod_to_nat_y(cod_y)\n            &gt;&gt;&gt; print(f\"Real-world units: {nat_y}\")\n\n        \"\"\"\n        return (\n            cod_y * (self.nat_range_y[1] - self.nat_range_y[0]) + self.nat_range_y[0]\n            if self.cod_type == \"norm\"\n            else cod_y * self.nat_std_y + self.nat_mean_y\n            if self.cod_type == \"std\"\n            else cod_y\n        )\n\n    def nat_to_cod_x(self, nat_X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Normalizes one point (row) of nat_X array to [0,1]. The internal nat_range_X values are not updated.\n\n        Args:\n            self (object): The Kriging object.\n            nat_X (np.ndarray):\n                An array representing one point (self.k long) in natural (physical or real world) units.\n\n        Returns:\n            X (np.ndarray): An array of coded values in the range of [0,1] for each dimension.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; from numpy import array\n            &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n            &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n            &gt;&gt;&gt; k = Kriging(X, y)\n            &gt;&gt;&gt; nat_X = array([5.0, 5.0])\n            &gt;&gt;&gt; cod_X = k.nat_to_cod_x(nat_X)\n            &gt;&gt;&gt; print(f\"Coded values: {cod_X}\")\n\n        \"\"\"\n        X = copy.deepcopy(nat_X)\n        if self.cod_type == \"norm\":\n            for i in range(self.k):\n                # TODO: Check Implementation of range correction if range == 0:\n                # rangex &lt;- xmax - xmin\n                # rangey &lt;- ymax - ymin\n                # xmin[rangex == 0] &lt;- xmin[rangex == 0] - 0.5\n                # xmax[rangex == 0] &lt;- xmax[rangex == 0] + 0.5\n                # rangex[rangex == 0] &lt;- 1\n                # logger.debug(f\"self.nat_range_X[{i}]:\\n {self.nat_range_X[i]}\")\n                # logger.debug(f\"X[{i}]:\\n {X[i]}\")\n                rangex = float(self.nat_range_X[i][1] - self.nat_range_X[i][0])\n                if rangex == 0:\n                    self.nat_range_X[i][0] = self.nat_range_X[i][0] - 0.5\n                    self.nat_range_X[i][1] = self.nat_range_X[i][1] + 0.5\n                X[i] = (X[i] - self.nat_range_X[i][0]) / float(\n                    self.nat_range_X[i][1] - self.nat_range_X[i][0]\n                )\n            return X\n        elif self.cod_type == \"std\":\n            for i in range(self.k):\n                X[i] = (X[i] - self.nat_mean_X[i]) / self.nat_std_X[i]\n            return X\n        else:\n            return nat_X\n\n    def nat_to_cod_y(self, nat_y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Normalizes natural y values to [0,1].\n\n        Args:\n            self (object): The Kriging object.\n            nat_y (np.ndarray):\n                An array of observed values in natural (real-world) units.\n\n        Returns:\n            y (np.ndarray):\n                A normalized array of coded (model) units in the range of [0,1].\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; kriging = Kriging()\n            &gt;&gt;&gt; nat_y = np.array([5.0, 5.0])\n            &gt;&gt;&gt; cod_y = kriging.nat_to_cod_y(nat_y)\n            &gt;&gt;&gt; print(f\"Coded values: {cod_y}\")\n        \"\"\"\n        return (\n            (nat_y - self.nat_range_y[0]) / (self.nat_range_y[1] - self.nat_range_y[0])\n            if self.use_cod_y and self.cod_type == \"norm\"\n            else (nat_y - self.nat_mean_y) / self.nat_std_y\n            if self.use_cod_y and self.cod_type == \"std\"\n            else nat_y\n        )\n\n    def nat_to_cod_init(self) -&gt; None:\n        \"\"\"\n        Determines max and min of each dimension and normalizes that axis to a range of [0,1].\n        Called when 1) surrogate is initialized and 2) new points arrive, i.e.,\n        suggested by the surrogate as infill points.\n        This method calls `nat_to_cod_x` and `nat_to_cod_y` and updates the ranges `nat_range_X` and `nat_range_y`.\n\n        Args:\n            self (object): The Kriging object.\n\n        Examples:\n\n            &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            &gt;&gt;&gt; kriging = Kriging()\n            &gt;&gt;&gt; kriging.nat_to_cod_init()\n        \"\"\"\n        self.nat_range_X = []\n        self.nat_range_y = []\n        for i in range(self.k):\n            self.nat_range_X.append([min(self.nat_X[:, i]), max(self.nat_X[:, i])])\n        self.nat_range_y.append(min(self.nat_y))\n        self.nat_range_y.append(max(self.nat_y))\n        self.nat_mean_X = mean(self.nat_X, axis=0)\n        self.nat_std_X = std(self.nat_X, axis=0)\n        self.nat_mean_y = mean(self.nat_y)\n        self.nat_std_y = std(self.nat_y)\n        Z = aggregate_mean_var(X=self.nat_X, y=self.nat_y)\n        mu = Z[1]\n        self.mean_cod_y = empty_like(mu)\n\n        for i in range(self.n):\n            self.cod_X[i] = self.nat_to_cod_x(self.nat_X[i])\n        for i in range(self.n):\n            self.cod_y[i] = self.nat_to_cod_y(self.nat_y[i])\n        for i in range(mu.shape[0]):\n            self.mean_cod_y[i] = self.nat_to_cod_y(mu[i])\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.__init__","title":"<code>__init__(noise=False, cod_type='norm', var_type=['num'], use_cod_y=False, name='kriging', seed=124, model_optimizer=None, model_fun_evals=None, min_theta=-3, max_theta=2, n_theta=1, n_p=1, optim_p=False, log_level=50, spot_writer=None, counter=None, **kwargs)</code>","text":"<p>Initialize the Kriging surrogate.</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>bool</code> <p>Use regression instead of interpolation kriging. Defaults to False.</p> <code>False</code> <code>cod_type</code> <code>Optional[str]</code> <p>Normalize or standardize X and values. Can be None, \u201cnorm\u201d, or \u201cstd\u201d. Defaults to \u201cnorm\u201d.</p> <code>'norm'</code> <code>var_type</code> <code>List[str]</code> <p>Variable type. Can be either \u201cnum\u201d (numerical) or \u201cfactor\u201d (factor). Defaults to [\u201cnum\u201d].</p> <code>['num']</code> <code>use_cod_y</code> <code>bool</code> <p>Use coded y values (instead of natural one). Defaults to False.</p> <code>False</code> <code>name</code> <code>str</code> <p>Surrogate name. Defaults to \u201ckriging\u201d.</p> <code>'kriging'</code> <code>seed</code> <code>int</code> <p>Random seed. Defaults to 124.</p> <code>124</code> <code>model_optimizer</code> <p>Optimizer on the surrogate. If None, differential_evolution is selected.</p> <code>None</code> <code>model_fun_evals</code> <code>Optional[int]</code> <p>Number of iterations used by the optimizer on the surrogate.</p> <code>None</code> <code>min_theta</code> <code>float</code> <p>Min log10 theta value. Defaults to -3.</p> <code>-3</code> <code>max_theta</code> <code>float</code> <p>Max log10 theta value. Defaults to 2.</p> <code>2</code> <code>n_theta</code> <code>int</code> <p>Number of theta values. Defaults to 1.</p> <code>1</code> <code>n_p</code> <code>int</code> <p>Number of p values. Defaults to 1.</p> <code>1</code> <code>optim_p</code> <code>bool</code> <p>Determines whether p should be optimized.</p> <code>False</code> <code>log_level</code> <code>int</code> <p>Logging level, e.g., 20 is \u201cINFO\u201d. Defaults to 50 (\u201cCRITICAL\u201d).</p> <code>50</code> <code>spot_writer</code> <p>Spot writer.</p> <code>None</code> <code>counter</code> <p>Counter.</p> <code>None</code> <p>Examples:</p> <p>Surrogate of the x*sin(x) function, see [1].</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n    import numpy as np\n    import matplotlib.pyplot as plt\n    rng = np.random.RandomState(1)\n    X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n    y = np.squeeze(X * np.sin(X))\n    training_indices = rng.choice(arange(y.size), size=6, replace=False)\n    X_train, y_train = X[training_indices], y[training_indices]\n    S = Kriging(name='kriging', seed=124)\n    S.fit(X_train, y_train)\n    mean_prediction, std_prediction = S.predict(X)\n    plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n    plt.scatter(X_train, y_train, label=\"Observations\")\n    plt.plot(X, mean_prediction, label=\"Mean prediction\")\n    plt.fill_between(\n        X.ravel(),\n        mean_prediction - 1.96 * std_prediction,\n        mean_prediction + 1.96 * std_prediction,\n        alpha=0.5,\n        label=r\"95% confidence interval\",\n        )\n    plt.legend()\n    plt.xlabel(\"$x$\")\n    plt.ylabel(\"$f(x)$\")\n    _ = plt.title(\"Gaussian process regression on noise-free dataset\")\n    plt.show()\n</code></pre> <p>References:</p> <pre><code>[[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)]\nscikit-learn: Gaussian Processes regression: basic introductory example\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def __init__(\n        self: object,\n        noise: bool = False,\n        cod_type: Optional[str] = \"norm\",\n        var_type: List[str] = [\"num\"],\n        use_cod_y: bool = False,\n        name: str = \"kriging\",\n        seed: int = 124,\n        model_optimizer=None,\n        model_fun_evals: Optional[int] = None,\n        min_theta: float = -3,\n        max_theta: float = 2,\n        n_theta: int = 1,\n        n_p: int = 1,\n        optim_p: bool = False,\n        log_level: int = 50,\n        spot_writer=None,\n        counter=None,\n        **kwargs\n):\n    \"\"\"\n    Initialize the Kriging surrogate.\n\n    Args:\n        noise (bool): Use regression instead of interpolation kriging. Defaults to False.\n        cod_type (Optional[str]):\n            Normalize or standardize X and values.\n            Can be None, \"norm\", or \"std\". Defaults to \"norm\".\n        var_type (List[str]):\n            Variable type. Can be either \"num\" (numerical) or \"factor\" (factor).\n            Defaults to [\"num\"].\n        use_cod_y (bool): Use coded y values (instead of natural one). Defaults to False.\n        name (str): Surrogate name. Defaults to \"kriging\".\n        seed (int): Random seed. Defaults to 124.\n        model_optimizer : Optimizer on the surrogate. If None, differential_evolution is selected.\n        model_fun_evals (Optional[int]): Number of iterations used by the optimizer on the surrogate.\n        min_theta (float): Min log10 theta value. Defaults to -3.\n        max_theta (float): Max log10 theta value. Defaults to 2.\n        n_theta (int): Number of theta values. Defaults to 1.\n        n_p (int): Number of p values. Defaults to 1.\n        optim_p (bool): Determines whether p should be optimized.\n        log_level (int): Logging level, e.g., 20 is \"INFO\". Defaults to 50 (\"CRITICAL\").\n        spot_writer : Spot writer.\n        counter : Counter.\n\n    Examples:\n        Surrogate of the x*sin(x) function, see [1].\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n            import numpy as np\n            import matplotlib.pyplot as plt\n            rng = np.random.RandomState(1)\n            X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n            y = np.squeeze(X * np.sin(X))\n            training_indices = rng.choice(arange(y.size), size=6, replace=False)\n            X_train, y_train = X[training_indices], y[training_indices]\n            S = Kriging(name='kriging', seed=124)\n            S.fit(X_train, y_train)\n            mean_prediction, std_prediction = S.predict(X)\n            plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n            plt.scatter(X_train, y_train, label=\"Observations\")\n            plt.plot(X, mean_prediction, label=\"Mean prediction\")\n            plt.fill_between(\n                X.ravel(),\n                mean_prediction - 1.96 * std_prediction,\n                mean_prediction + 1.96 * std_prediction,\n                alpha=0.5,\n                label=r\"95% confidence interval\",\n                )\n            plt.legend()\n            plt.xlabel(\"$x$\")\n            plt.ylabel(\"$f(x)$\")\n            _ = plt.title(\"Gaussian process regression on noise-free dataset\")\n            plt.show()\n\n    References:\n\n        [[1](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)]\n        scikit-learn: Gaussian Processes regression: basic introductory example\n\n    \"\"\"\n    super().__init__(name, seed, log_level)\n\n    self.noise = noise\n    self.var_type = var_type\n    self.cod_type = cod_type\n    self.use_cod_y = use_cod_y\n    self.name = name\n    self.seed = seed\n    self.log_level = log_level\n    self.spot_writer = spot_writer\n    self.counter = counter\n\n    self.sigma = 0\n    self.eps = sqrt(spacing(1))\n    self.min_theta = min_theta\n    self.max_theta = max_theta\n    self.min_p = 1\n    self.max_p = 2\n    self.min_Lambda = 1e-9\n    self.max_Lambda = 1.\n    self.n_theta = n_theta\n    self.n_p = n_p\n    self.optim_p = optim_p\n    # Psi matrix condition:\n    self.cnd_Psi = 0\n    self.inf_Psi = False\n\n    self.model_optimizer = model_optimizer\n    if self.model_optimizer is None:\n        self.model_optimizer = differential_evolution\n    self.model_fun_evals = model_fun_evals\n    # differential evaluation uses maxiter = 1000\n    # and sets the number of function evaluations to\n    # (maxiter + 1) * popsize * N, which results in\n    # 1000 * 15 * k, because the default popsize is 15 and\n    # N is the number of parameters. This seems to be quite large:\n    # for k=2 these are 30 000 iterations. Therefore we set this value to\n    # 100\n    if self.model_fun_evals is None:\n        self.model_fun_evals = 100\n\n    # Logging information\n    self.log[\"negLnLike\"] = []\n    self.log[\"theta\"] = []\n    self.log[\"p\"] = []\n    self.log[\"Lambda\"] = []\n    # Logger\n    logger.setLevel(self.log_level)\n    logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.__is_any__","title":"<code>__is_any__(x, v)</code>","text":"<p>Check if any element in <code>x</code> is equal to <code>v</code>.</p> <p>This method checks if any element in the input array <code>x</code> is equal to the value <code>v</code>. If <code>x</code> is not an instance of <code>ndarray</code>, it is first converted to a numpy array using the <code>array()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>x</code> <code>ndarray or array - like</code> <p>The input array to check for the presence of value <code>v</code>.</p> required <code>v</code> <code>scalar</code> <p>The value to check for in the input array <code>x</code>.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if any element in <code>x</code> is equal to <code>v</code>, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; class MyClass(Kriging):\n&gt;&gt;&gt;     def __init__(self):\n&gt;&gt;&gt;         super().__init__()\n&gt;&gt;&gt;         self.n_p = 2\n&gt;&gt;&gt;         self.n = 3\n&gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n&gt;&gt;&gt;         self.k = 2\n&gt;&gt;&gt;         self.seed = 1\n\n&gt;&gt;&gt; instance = MyClass()\n&gt;&gt;&gt; result = instance.__is_any__(x, v)\n&gt;&gt;&gt; print(result)\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def __is_any__(self, x: Union[np.ndarray, Any], v: Any) -&gt; bool:\n    \"\"\"\n    Check if any element in `x` is equal to `v`.\n\n    This method checks if any element in the input array `x` is equal to the value `v`.\n    If `x` is not an instance of `ndarray`, it is first converted to a numpy array using\n    the `array()` function.\n\n    Args:\n        self (object): The Kriging object.\n        x (np.ndarray or array-like):\n            The input array to check for the presence of value `v`.\n        v (scalar):\n            The value to check for in the input array `x`.\n\n    Returns:\n        bool:\n            True if any element in `x` is equal to `v`, False otherwise.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; class MyClass(Kriging):\n        &gt;&gt;&gt;     def __init__(self):\n        &gt;&gt;&gt;         super().__init__()\n        &gt;&gt;&gt;         self.n_p = 2\n        &gt;&gt;&gt;         self.n = 3\n        &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n        &gt;&gt;&gt;         self.k = 2\n        &gt;&gt;&gt;         self.seed = 1\n\n        &gt;&gt;&gt; instance = MyClass()\n        &gt;&gt;&gt; result = instance.__is_any__(x, v)\n        &gt;&gt;&gt; print(result)\n\n    \"\"\"\n    if not isinstance(x, ndarray):\n        x = array([x])\n    return any(x == v)\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.build_Psi","title":"<code>build_Psi()</code>","text":"<p>Constructs a new (n x n) correlation matrix Psi to reflect new data or a change in hyperparameters.</p> <p>This method uses <code>theta</code>, <code>p</code>, and coded <code>X</code> values to construct the correlation matrix as described in [Forr08a, p.57].</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>LinAlgError</code> <p>If building Psi fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; class MyClass(Kriging):\n&gt;&gt;&gt;     def __init__(self):\n&gt;&gt;&gt;         super().__init__()\n&gt;&gt;&gt;         self.n_p = 2\n&gt;&gt;&gt;         self.n = 3\n&gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n&gt;&gt;&gt;         self.k = 2\n&gt;&gt;&gt;         self.seed = 1\n\n&gt;&gt;&gt; obj = MyClass()\n&gt;&gt;&gt; obj.build_Psi()\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def build_Psi(self) -&gt; None:\n    \"\"\"\n    Constructs a new (n x n) correlation matrix Psi to reflect new data\n    or a change in hyperparameters.\n\n    This method uses `theta`, `p`, and coded `X` values to construct the\n    correlation matrix as described in [Forr08a, p.57].\n\n    Args:\n        self (object): The Kriging object.\n\n    Returns:\n        None\n\n    Raises:\n        LinAlgError: If building Psi fails.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; class MyClass(Kriging):\n        &gt;&gt;&gt;     def __init__(self):\n        &gt;&gt;&gt;         super().__init__()\n        &gt;&gt;&gt;         self.n_p = 2\n        &gt;&gt;&gt;         self.n = 3\n        &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n        &gt;&gt;&gt;         self.k = 2\n        &gt;&gt;&gt;         self.seed = 1\n\n        &gt;&gt;&gt; obj = MyClass()\n        &gt;&gt;&gt; obj.build_Psi()\n\n    \"\"\"\n    self.Psi = zeros((self.n, self.n), dtype=float64)\n    theta = power(10.0, self.theta)\n    if self.n_theta == 1:\n        theta = theta * ones(self.k)\n    try:\n        D = zeros((self.n, self.n))\n        if self.ordered_mask.any():\n            X_ordered = self.cod_X[:, self.ordered_mask]\n            D = squareform(\n                pdist(\n                    X_ordered, metric='sqeuclidean', out=None, w=theta[self.ordered_mask]))\n        if self.factor_mask.any():\n            X_factor = self.cod_X[:, self.factor_mask]\n            D = (D + squareform(\n                pdist(X_factor,\n                      metric='hamming',\n                      out=None,\n                      w=theta[self.factor_mask])))\n        self.Psi = exp(-D)\n    except LinAlgError as err:\n        print(f\"Building Psi failed:\\n {self.Psi}. {err=}, {type(err)=}\")\n    if self.noise:\n        self.Psi[diag_indices_from(self.Psi)] += self.Lambda\n    else:\n        self.Psi[diag_indices_from(self.Psi)] += self.eps\n    if (isinf(self.Psi)).any():\n        self.inf_Psi = True\n    self.cnd_Psi = cond(self.Psi)\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.build_U","title":"<code>build_U(scipy=True)</code>","text":"<p>Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57].</p> <p>This method uses either <code>scipy_cholesky</code> or numpy\u2019s <code>cholesky</code> to perform the Cholesky factorization of Psi.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>scipy</code> <code>bool</code> <p>If True, use <code>scipy_cholesky</code>. If False, use numpy\u2019s <code>cholesky</code>. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>LinAlgError</code> <p>If Cholesky factorization fails for Psi.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; class MyClass(Kriging):\n&gt;&gt;&gt;     def __init__(self):\n&gt;&gt;&gt;         super().__init__()\n&gt;&gt;&gt;         self.n_p = 2\n&gt;&gt;&gt;         self.n = 3\n&gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n&gt;&gt;&gt;         self.k = 2\n&gt;&gt;&gt;         self.seed = 1\n\n&gt;&gt;&gt; obj = MyClass()\n&gt;&gt;&gt; obj.build_U()\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def build_U(self, scipy: bool = True) -&gt; None:\n    \"\"\"\n    Performs Cholesky factorization of Psi as U as described in [Forr08a, p.57].\n\n    This method uses either `scipy_cholesky` or numpy's `cholesky` to perform the Cholesky factorization of Psi.\n\n    Args:\n        self (object):\n            The Kriging object.\n        scipy (bool):\n            If True, use `scipy_cholesky`.\n            If False, use numpy's `cholesky`.\n            Defaults to True.\n\n    Returns:\n        None\n\n    Raises:\n        LinAlgError:\n            If Cholesky factorization fails for Psi.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; class MyClass(Kriging):\n        &gt;&gt;&gt;     def __init__(self):\n        &gt;&gt;&gt;         super().__init__()\n        &gt;&gt;&gt;         self.n_p = 2\n        &gt;&gt;&gt;         self.n = 3\n        &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n        &gt;&gt;&gt;         self.k = 2\n        &gt;&gt;&gt;         self.seed = 1\n\n        &gt;&gt;&gt; obj = MyClass()\n        &gt;&gt;&gt; obj.build_U()\n    \"\"\"\n    try:\n        self.U = scipy_cholesky(self.Psi, lower=True) if scipy else cholesky(self.Psi)\n        self.U = self.U.T\n    except LinAlgError as err:\n        print(f\"build_U() Cholesky failed for Psi:\\n {self.Psi}. {err=}, {type(err)=}\")\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.build_psi_vec","title":"<code>build_psi_vec(cod_x)</code>","text":"<p>Build the psi vector. Needed by <code>predict_cod</code>, <code>predict_err_coded</code>, <code>regression_predict_coded</code>. Modifies <code>self.psi</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>cod_x</code> <code>ndarray</code> <p>point to calculate psi</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; from numpy import array\n&gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n&gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n&gt;&gt;&gt; k = Kriging(X, y)\n&gt;&gt;&gt; cod_x = array([0.3, 0.3])\n&gt;&gt;&gt; build_psi_vec(cod_x)\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def build_psi_vec(self, cod_x: ndarray) -&gt; None:\n    \"\"\"\n    Build the psi vector. Needed by `predict_cod`, `predict_err_coded`,\n    `regression_predict_coded`. Modifies `self.psi`.\n\n    Args:\n        self (object):\n            The Kriging object.\n        cod_x (ndarray):\n            point to calculate psi\n\n    Returns:\n        None\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; from numpy import array\n        &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n        &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n        &gt;&gt;&gt; k = Kriging(X, y)\n        &gt;&gt;&gt; cod_x = array([0.3, 0.3])\n        &gt;&gt;&gt; build_psi_vec(cod_x)\n\n    \"\"\"\n    self.psi = zeros((self.n))\n    # theta = self.theta  # TODO:\n    theta = power(10.0, self.theta)\n    if self.n_theta == 1:\n        theta = theta * ones(self.k)\n    try:\n        D = zeros((self.n))\n        if self.ordered_mask.any():\n            X_ordered = self.cod_X[:, self.ordered_mask]\n            x_ordered = cod_x[self.ordered_mask]\n            D = cdist(x_ordered.reshape(-1, sum(self.ordered_mask)),\n                      X_ordered.reshape(-1, sum(self.ordered_mask)),\n                      metric='sqeuclidean',\n                      out=None,\n                      w=theta[self.ordered_mask])\n        if self.factor_mask.any():\n            X_factor = self.cod_X[:, self.factor_mask]\n            x_factor = cod_x[self.factor_mask]\n            D = (D + cdist(x_factor.reshape(-1, sum(self.factor_mask)),\n                           X_factor.reshape(-1, sum(self.factor_mask)),\n                           metric='hamming',\n                           out=None,\n                           w=theta[self.factor_mask]))\n        self.psi = exp(-D).T\n    except LinAlgError as err:\n        print(f\"Building psi failed:\\n {self.psi}. {err=}, {type(err)=}\")\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.calculate_mean_MSE","title":"<code>calculate_mean_MSE(n_samples=200, points=None)</code>","text":"<p>Calculates the mean MSE metric of the model by evaluating MSE at a number of points.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>n_samples</code> <code>int</code> <p>Number of points to sample the mean squared error at. Ignored if the points argument is specified.</p> <code>200</code> <code>points</code> <code>ndarray</code> <p>An array of points to sample the model at.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mean_MSE</code> <code>float</code> <p>The mean value of MSE.</p> <code>std_MSE</code> <code>float</code> <p>The standard deviation of the MSE points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; from numpy import array\n&gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n&gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n&gt;&gt;&gt; k = Kriging(X, y)\n&gt;&gt;&gt; n_samples = 200\n&gt;&gt;&gt; mean_MSE, std_MSE = k.calculate_mean_MSE(n_samples)\n&gt;&gt;&gt; print(f\"Mean MSE: {mean_MSE}, Standard deviation of MSE: {std_MSE}\")\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def calculate_mean_MSE(self, n_samples: int = 200, points: Optional[np.ndarray] = None) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculates the mean MSE metric of the model by evaluating MSE at a number of points.\n\n    Args:\n        self (object):\n            The Kriging object.\n        n_samples (int):\n            Number of points to sample the mean squared error at.\n            Ignored if the points argument is specified.\n        points (np.ndarray):\n            An array of points to sample the model at.\n\n    Returns:\n        mean_MSE (float): The mean value of MSE.\n        std_MSE (float): The standard deviation of the MSE points.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; from numpy import array\n        &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n        &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n        &gt;&gt;&gt; k = Kriging(X, y)\n        &gt;&gt;&gt; n_samples = 200\n        &gt;&gt;&gt; mean_MSE, std_MSE = k.calculate_mean_MSE(n_samples)\n        &gt;&gt;&gt; print(f\"Mean MSE: {mean_MSE}, Standard deviation of MSE: {std_MSE}\")\n\n    \"\"\"\n    if points is None:\n        points = self.gen.lhd(n_samples)\n    values = [self.predict(cod_X=point, nat=True, return_val=\"s\") for point in points]\n    return mean(values), std(values)\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.cod_to_nat_x","title":"<code>cod_to_nat_x(cod_X)</code>","text":"<p>Converts an array representing one point in normalized (coded) units to natural (physical or real world) units.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>cod_X</code> <code>ndarray</code> <p>An array representing one point (self.k long) in normalized (coded) units.</p> required <p>Returns:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>An array of natural (physical or real world) units.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; from numpy import array\n&gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n&gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n&gt;&gt;&gt; k = Kriging(X, y)\n&gt;&gt;&gt; cod_X = array([0.3, 0.3])\n&gt;&gt;&gt; nat_X = k.cod_to_nat_x(cod_X)\n&gt;&gt;&gt; print(f\"Natural units: {nat_X}\")\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def cod_to_nat_x(self, cod_X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Converts an array representing one point in normalized (coded) units to natural (physical or real world) units.\n\n    Args:\n        self (object): The Kriging object.\n        cod_X (np.ndarray):\n            An array representing one point (self.k long) in normalized (coded) units.\n\n    Returns:\n        X (np.ndarray): An array of natural (physical or real world) units.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; from numpy import array\n        &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n        &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n        &gt;&gt;&gt; k = Kriging(X, y)\n        &gt;&gt;&gt; cod_X = array([0.3, 0.3])\n        &gt;&gt;&gt; nat_X = k.cod_to_nat_x(cod_X)\n        &gt;&gt;&gt; print(f\"Natural units: {nat_X}\")\n\n    \"\"\"\n    X = copy.deepcopy(cod_X)\n    if self.cod_type == \"norm\":\n        for i in range(self.k):\n            X[i] = (\n                X[i] * float(self.nat_range_X[i][1] - self.nat_range_X[i][0])\n            ) + self.nat_range_X[i][0]\n        return X\n    elif self.cod_type == \"std\":\n        for i in range(self.k):\n            X[i] = X[i] * self.nat_std_X[i] + self.nat_mean_X[i]\n        return X\n    else:\n        return cod_X\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.cod_to_nat_y","title":"<code>cod_to_nat_y(cod_y)</code>","text":"<p>Converts a normalized array of coded (model) units in the range of [0,1] to an array of observed values in real-world units.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>cod_y</code> <code>ndarray</code> <p>A normalized array of coded (model) units in the range of [0,1].</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>An array of observed values in real-world units.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; from numpy import array\n&gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n&gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n&gt;&gt;&gt; k = Kriging(X, y)\n&gt;&gt;&gt; cod_y = array([0.5, 0.5])\n&gt;&gt;&gt; nat_y = k.cod_to_nat_y(cod_y)\n&gt;&gt;&gt; print(f\"Real-world units: {nat_y}\")\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def cod_to_nat_y(self, cod_y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Converts a normalized array of coded (model) units in the range of [0,1]\n    to an array of observed values in real-world units.\n\n    Args:\n        self (object): The Kriging object.\n        cod_y (np.ndarray):\n            A normalized array of coded (model) units in the range of [0,1].\n\n    Returns:\n        y (np.ndarray): An array of observed values in real-world units.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; from numpy import array\n        &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n        &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n        &gt;&gt;&gt; k = Kriging(X, y)\n        &gt;&gt;&gt; cod_y = array([0.5, 0.5])\n        &gt;&gt;&gt; nat_y = k.cod_to_nat_y(cod_y)\n        &gt;&gt;&gt; print(f\"Real-world units: {nat_y}\")\n\n    \"\"\"\n    return (\n        cod_y * (self.nat_range_y[1] - self.nat_range_y[0]) + self.nat_range_y[0]\n        if self.cod_type == \"norm\"\n        else cod_y * self.nat_std_y + self.nat_mean_y\n        if self.cod_type == \"std\"\n        else cod_y\n    )\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.exp_imp","title":"<code>exp_imp(y0, s0)</code>","text":"<p>Calculates the expected improvement for a given function value and error in coded units.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>y0</code> <code>float</code> <p>The function value in coded units.</p> required <code>s0</code> <code>float</code> <p>The error value.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The expected improvement value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; S = Kriging(name='kriging', seed=124)\n&gt;&gt;&gt; S.cod_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n&gt;&gt;&gt; S.mean_cod_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n&gt;&gt;&gt; S.exp_imp(1.0, 2.0)\n0.0\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def exp_imp(self, y0: float, s0: float) -&gt; float:\n    \"\"\"\n    Calculates the expected improvement for a given function value and error in coded units.\n\n    Args:\n        self (object): The Kriging object.\n        y0 (float): The function value in coded units.\n        s0 (float): The error value.\n\n    Returns:\n        float: The expected improvement value.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; S = Kriging(name='kriging', seed=124)\n        &gt;&gt;&gt; S.cod_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n        &gt;&gt;&gt; S.mean_cod_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n        &gt;&gt;&gt; S.exp_imp(1.0, 2.0)\n        0.0\n\n    \"\"\"\n    # y_min = min(self.cod_y)\n    y_min = min(self.mean_cod_y)\n    if s0 &lt;= 0.0:\n        EI = 0.0\n    elif s0 &gt; 0.0:\n        EI_one = (y_min - y0) * (\n                0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * ((y_min - y0) / s0))\n        )\n        EI_two = (s0 * (1.0 / sqrt(2.0 * pi))) * (\n            exp(-(1.0 / 2.0) * ((y_min - y0) ** 2.0 / s0 ** 2.0))\n        )\n        EI = EI_one + EI_two\n    return EI\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.extract_from_bounds","title":"<code>extract_from_bounds(new_theta_p_Lambda)</code>","text":"<p>Extract <code>theta</code>, <code>p</code>, and <code>Lambda</code> from bounds. The kriging object stores <code>theta</code> as an array,  <code>p</code> as an array, and <code>Lambda</code> as a float.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>new_theta_p_Lambda</code> <code>ndarray</code> <p>1d-array with theta, p, and Lambda values. Order is important.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n&gt;&gt;&gt; obj = MyClass()\n&gt;&gt;&gt; obj.extract_from_bounds(np.array([1, 2, 3]))\n&gt;&gt;&gt; print(obj.theta)\n[1]\n&gt;&gt;&gt; print(obj.p)\n[2]\n&gt;&gt;&gt; print(obj.Lambda)\n3\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def extract_from_bounds(self, new_theta_p_Lambda: np.ndarray) -&gt; None:\n    \"\"\"\n    Extract `theta`, `p`, and `Lambda` from bounds. The kriging object stores\n    `theta` as an array,  `p` as an array, and `Lambda` as a float.\n\n    Args:\n        self (object): The Kriging object.\n        new_theta_p_Lambda (np.ndarray):\n            1d-array with theta, p, and Lambda values. Order is important.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n        &gt;&gt;&gt; obj = MyClass()\n        &gt;&gt;&gt; obj.extract_from_bounds(np.array([1, 2, 3]))\n        &gt;&gt;&gt; print(obj.theta)\n        [1]\n        &gt;&gt;&gt; print(obj.p)\n        [2]\n        &gt;&gt;&gt; print(obj.Lambda)\n        3\n\n    Returns:\n        None\n    \"\"\"\n    self.theta = new_theta_p_Lambda[:self.n_theta]\n    if self.optim_p:\n        self.p = new_theta_p_Lambda[self.n_theta:self.n_theta + self.n_p]\n        if self.noise:\n            self.Lambda = new_theta_p_Lambda[self.n_theta + self.n_p]\n    else:\n        if self.noise:\n            self.Lambda = new_theta_p_Lambda[self.n_theta]\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.fit","title":"<code>fit(nat_X, nat_y)</code>","text":"<p>Fits the hyperparameters (<code>theta</code>, <code>p</code>, <code>Lambda</code>) of the Kriging model.</p> <p>The function computes the following internal values: 1. <code>theta</code>, <code>p</code>, and <code>Lambda</code> values via optimization of the function <code>fun_likelihood()</code>. 2. Correlation matrix <code>Psi</code> via <code>rebuildPsi()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>nat_X</code> <code>ndarray</code> <p>Sample points.</p> required <code>nat_y</code> <code>ndarray</code> <p>Function values.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>Fitted estimator.</p> <p>Attributes:</p> Name Type Description <code>theta</code> <code>ndarray</code> <p>Kriging theta values. Shape (k,).</p> <code>p</code> <code>ndarray</code> <p>Kriging p values. Shape (k,).</p> <code>LnDetPsi</code> <code>float64</code> <p>Determinant Psi matrix.</p> <code>Psi</code> <code>matrix</code> <p>Correlation matrix Psi. Shape (n,n).</p> <code>psi</code> <code>ndarray</code> <p>psi vector. Shape (n,).</p> <code>one</code> <code>ndarray</code> <p>vector of ones. Shape (n,).</p> <code>mu</code> <code>float64</code> <p>Kriging expected mean value mu.</p> <code>U</code> <code>matrix</code> <p>Kriging U matrix, Cholesky decomposition. Shape (n,n).</p> <code>SigmaSqr</code> <code>float64</code> <p>Sigma squared value.</p> <code>Lambda</code> <code>float</code> <p>lambda noise value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; nat_X = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; nat_y = np.array([1, 2])\n&gt;&gt;&gt; surrogate = Kriging()\n&gt;&gt;&gt; surrogate.fit(nat_X, nat_y)\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def fit(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; object:\n    \"\"\"\n    Fits the hyperparameters (`theta`, `p`, `Lambda`) of the Kriging model.\n\n    The function computes the following internal values:\n    1. `theta`, `p`, and `Lambda` values via optimization of the function `fun_likelihood()`.\n    2. Correlation matrix `Psi` via `rebuildPsi()`.\n\n    Args:\n        self (object): The Kriging object.\n        nat_X (np.ndarray): Sample points.\n        nat_y (np.ndarray): Function values.\n\n    Returns:\n        object: Fitted estimator.\n\n    Attributes:\n        theta (np.ndarray): Kriging theta values. Shape (k,).\n        p (np.ndarray): Kriging p values. Shape (k,).\n        LnDetPsi (np.float64): Determinant Psi matrix.\n        Psi (np.matrix): Correlation matrix Psi. Shape (n,n).\n        psi (np.ndarray): psi vector. Shape (n,).\n        one (np.ndarray): vector of ones. Shape (n,).\n        mu (np.float64): Kriging expected mean value mu.\n        U (np.matrix): Kriging U matrix, Cholesky decomposition. Shape (n,n).\n        SigmaSqr (np.float64): Sigma squared value.\n        Lambda (float): lambda noise value.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; nat_X = np.array([[1, 2], [3, 4]])\n        &gt;&gt;&gt; nat_y = np.array([1, 2])\n        &gt;&gt;&gt; surrogate = Kriging()\n        &gt;&gt;&gt; surrogate.fit(nat_X, nat_y)\n    \"\"\"\n    self.initialize_variables(nat_X, nat_y)\n    self.set_variable_types()\n    self.nat_to_cod_init()\n    self.set_theta_values()\n    self.initialize_matrices()\n    # build_Psi() and build_U() are called in fun_likelihood\n    self.set_de_bounds()\n    # Finally, set new theta and p values and update the surrogate again\n    # for new_theta_p_Lambda in de_results[\"x\"]:\n    new_theta_p_Lambda = self.optimize_model()\n    self.extract_from_bounds(new_theta_p_Lambda)\n    self.build_Psi()\n    self.build_U()\n    # TODO: check if the following line is necessary!\n    self.likelihood()\n    self.update_log()\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.fun_likelihood","title":"<code>fun_likelihood(new_theta_p_Lambda)</code>","text":"<p>Compute log likelihood for a set of hyperparameters (theta, p, Lambda).</p> <p>This method computes the log likelihood for a set of hyperparameters (theta, p, Lambda) by performing the following steps: 1. Extracts the hyperparameters from the input array using <code>extract_from_bounds()</code>. 2. Checks if any element in <code>10^theta</code> is equal to 0. If so, logs a warning and returns the penalty value (<code>pen_val</code>). 3. Builds the <code>Psi</code> matrix using <code>build_Psi()</code>. 4. Checks if <code>Psi</code> is ill-conditioned or infinite. If so, logs a warning and returns the penalty value (<code>pen_val</code>). 5. Builds the <code>U</code> matrix using <code>build_U()</code>. If an exception occurs, logs an error and returns the penalty value (<code>pen_val</code>). 6. Computes the negative log likelihood using <code>likelihood()</code>. 7. Returns the computed negative log likelihood (<code>negLnLike</code>).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>new_theta_p_Lambda</code> <code>ndarray</code> <p>An array containing the <code>theta</code>, <code>p</code>, and <code>Lambda</code> values.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The negative log likelihood of the surface at the specified hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; class MyClass(Kriging):\n&gt;&gt;&gt;     def __init__(self):\n&gt;&gt;&gt;         super().__init__()\n&gt;&gt;&gt;         self.n_p = 2\n&gt;&gt;&gt;         self.n = 3\n&gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n&gt;&gt;&gt;         self.k = 2\n&gt;&gt;&gt;         self.seed = 1\n&gt;&gt;&gt; instance = MyClass()\n&gt;&gt;&gt; negLnLike = instance.fun_likelihood(new_theta_p_Lambda)\n&gt;&gt;&gt; print(negLnLike)\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def fun_likelihood(self, new_theta_p_Lambda: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute log likelihood for a set of hyperparameters (theta, p, Lambda).\n\n    This method computes the log likelihood for a set of hyperparameters\n    (theta, p, Lambda) by performing the following steps:\n    1. Extracts the hyperparameters from the input array using `extract_from_bounds()`.\n    2. Checks if any element in `10^theta` is equal to 0. If so, logs a warning and\n    returns the penalty value (`pen_val`).\n    3. Builds the `Psi` matrix using `build_Psi()`.\n    4. Checks if `Psi` is ill-conditioned or infinite. If so, logs a warning and returns\n    the penalty value (`pen_val`).\n    5. Builds the `U` matrix using `build_U()`. If an exception occurs, logs an error and\n    returns the penalty value (`pen_val`).\n    6. Computes the negative log likelihood using `likelihood()`.\n    7. Returns the computed negative log likelihood (`negLnLike`).\n\n    Args:\n        self (object): The Kriging object.\n        new_theta_p_Lambda (np.ndarray):\n            An array containing the `theta`, `p`, and `Lambda` values.\n\n    Returns:\n        float:\n            The negative log likelihood of the surface at the specified hyperparameters.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; class MyClass(Kriging):\n        &gt;&gt;&gt;     def __init__(self):\n        &gt;&gt;&gt;         super().__init__()\n        &gt;&gt;&gt;         self.n_p = 2\n        &gt;&gt;&gt;         self.n = 3\n        &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n        &gt;&gt;&gt;         self.k = 2\n        &gt;&gt;&gt;         self.seed = 1\n        &gt;&gt;&gt; instance = MyClass()\n        &gt;&gt;&gt; negLnLike = instance.fun_likelihood(new_theta_p_Lambda)\n        &gt;&gt;&gt; print(negLnLike)\n\n    \"\"\"\n    self.extract_from_bounds(new_theta_p_Lambda)\n    if self.__is_any__(power(10.0, self.theta), 0):\n        logger.warning(\"Failure in fun_likelihood: 10^theta == 0. Setting negLnLike to %s\", self.pen_val)\n        return self.pen_val\n    self.build_Psi()\n    if (self.inf_Psi or self.cnd_Psi &gt; 1e9):\n        logger.warning(\"Failure in fun_likelihood: Psi is ill conditioned: %s\", self.cnd_Psi)\n        logger.warning(\"Setting negLnLike to: %s\", self.pen_val)\n        return self.pen_val\n\n    try:\n        self.build_U()\n    except Exception as error:\n        penalty_value = self.pen_val\n        print(\"Error in fun_likelihood(). Call to build_U() failed.\")\n        print(\"error=%s, type(error)=%s\" % (error, type(error)))\n        print(\"Setting negLnLike to %.2f.\" % self.pen_val)\n        return penalty_value\n    self.likelihood()\n    return self.negLnLike\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.initialize_matrices","title":"<code>initialize_matrices()</code>","text":"<p>Initialize the matrices for the class instance.</p> <p>This method initializes several matrices and attributes for the class instance. The <code>p</code> attribute is initialized as a list of ones with length <code>n_p</code>, multiplied by 2.0. The <code>pen_val</code> attribute is initialized as the natural logarithm of the variance of <code>nat_y</code>, multiplied by <code>n</code>, plus 1e4. The <code>negLnLike</code>, <code>LnDetPsi</code>, <code>mu</code>, <code>U</code>, <code>SigmaSqr</code>, and <code>Lambda</code> attributes are all set to None. The <code>gen</code> attribute is initialized using the <code>spacefilling</code> function with arguments <code>k</code> and <code>seed</code>. The <code>Psi</code> attribute is initialized as a zero matrix with shape <code>(n, n)</code> and dtype <code>float64</code>. The <code>psi</code> attribute is initialized as a zero matrix with shape <code>(n, 1)</code>. The <code>one</code> attribute is initialized as a list of ones with length <code>n</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; class MyClass(Kriging):\n&gt;&gt;&gt;     def __init__(self):\n&gt;&gt;&gt;         super().__init__()\n&gt;&gt;&gt;         self.n_p = 2\n&gt;&gt;&gt;         self.n = 3\n&gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n&gt;&gt;&gt;         self.k = 2\n&gt;&gt;&gt;         self.seed = 1\n&gt;&gt;&gt; instance = MyClass()\n&gt;&gt;&gt; instance.initialize_matrices()\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def initialize_matrices(self) -&gt; None:\n    \"\"\"\n    Initialize the matrices for the class instance.\n\n    This method initializes several matrices and attributes for the class instance.\n    The `p` attribute is initialized as a list of ones with length `n_p`, multiplied by 2.0.\n    The `pen_val` attribute is initialized as the natural logarithm of the\n    variance of `nat_y`, multiplied by `n`, plus 1e4.\n    The `negLnLike`, `LnDetPsi`, `mu`, `U`, `SigmaSqr`, and `Lambda` attributes are all set to None.\n    The `gen` attribute is initialized using the `spacefilling` function with arguments `k` and `seed`.\n    The `Psi` attribute is initialized as a zero matrix with shape `(n, n)` and dtype `float64`.\n    The `psi` attribute is initialized as a zero matrix with shape `(n, 1)`.\n    The `one` attribute is initialized as a list of ones with length `n`.\n\n    Args:\n        self (object): The Kriging object.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; class MyClass(Kriging):\n        &gt;&gt;&gt;     def __init__(self):\n        &gt;&gt;&gt;         super().__init__()\n        &gt;&gt;&gt;         self.n_p = 2\n        &gt;&gt;&gt;         self.n = 3\n        &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n        &gt;&gt;&gt;         self.k = 2\n        &gt;&gt;&gt;         self.seed = 1\n        &gt;&gt;&gt; instance = MyClass()\n        &gt;&gt;&gt; instance.initialize_matrices()\n\n    Returns:\n        None\n    \"\"\"\n    self.p = ones(self.n_p) * 2.0\n    self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n    self.negLnLike = None\n    self.gen = spacefilling(k=self.k, seed=self.seed)\n    self.LnDetPsi = None\n    self.Psi = zeros((self.n, self.n), dtype=float64)\n    self.psi = zeros((self.n, 1))\n    self.one = ones(self.n)\n    self.mu = None\n    self.U = None\n    self.SigmaSqr = None\n    self.Lambda = None\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.initialize_variables","title":"<code>initialize_variables(nat_X, nat_y)</code>","text":"<p>Initialize variables for the class instance.</p> <p>This method takes in the independent and dependent variable data as input and initializes the class instance variables. It creates deep copies of the input data and stores them in the instance variables <code>nat_X</code> and <code>nat_y</code>. It also calculates the number of observations <code>n</code> and the number of independent variables <code>k</code> from the shape of <code>nat_X</code>. Finally, it creates empty arrays with the same shape as <code>nat_X</code> and <code>nat_y</code> and stores them in the instance variables <code>cod_X</code> and <code>cod_y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>nat_X</code> <code>ndarray</code> <p>The independent variable data.</p> required <code>nat_y</code> <code>ndarray</code> <p>The dependent variable data.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; surrogate = Kriging()\n&gt;&gt;&gt; nat_X = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; nat_y = np.array([1, 2])\n&gt;&gt;&gt; surrogate.initialize_variables(nat_X, nat_y)\n&gt;&gt;&gt; surrogate.nat_X\narray([[1, 2],\n         [3, 4]])\n&gt;&gt;&gt; surrogate.nat_y\narray([1, 2])\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def initialize_variables(self, nat_X: np.ndarray, nat_y: np.ndarray) -&gt; None:\n    \"\"\"\n    Initialize variables for the class instance.\n\n    This method takes in the independent and dependent variable data as input\n    and initializes the class instance variables.\n    It creates deep copies of the input data and stores them in the\n    instance variables `nat_X` and `nat_y`.\n    It also calculates the number of observations `n` and\n    the number of independent variables `k` from the shape of `nat_X`.\n    Finally, it creates empty arrays with the same shape as `nat_X`\n    and `nat_y` and stores them in the instance variables `cod_X` and `cod_y`.\n\n    Args:\n        self (object): The Kriging object.\n        nat_X (np.ndarray): The independent variable data.\n        nat_y (np.ndarray): The dependent variable data.\n\n    Returns:\n        None\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; surrogate = Kriging()\n        &gt;&gt;&gt; nat_X = np.array([[1, 2], [3, 4]])\n        &gt;&gt;&gt; nat_y = np.array([1, 2])\n        &gt;&gt;&gt; surrogate.initialize_variables(nat_X, nat_y)\n        &gt;&gt;&gt; surrogate.nat_X\n        array([[1, 2],\n                 [3, 4]])\n        &gt;&gt;&gt; surrogate.nat_y\n        array([1, 2])\n\n    \"\"\"\n    self.nat_X = copy.deepcopy(nat_X)\n    self.nat_y = copy.deepcopy(nat_y)\n    self.n = self.nat_X.shape[0]\n    self.k = self.nat_X.shape[1]\n    self.cod_X = np.empty_like(self.nat_X)\n    self.cod_y = np.empty_like(self.nat_y)\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.likelihood","title":"<code>likelihood()</code>","text":"<p>Calculates the negative of the concentrated log-likelihood.</p> <p>This method implements equation (2.32) in [Forr08a] to calculate the negative of the concentrated log-likelihood. It also modifies <code>mu</code>, <code>SigmaSqr</code>, <code>LnDetPsi</code>, and <code>negLnLike</code>.</p> Note <p><code>build_Psi</code> and <code>build_U</code> should be called first.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; class MyClass(Kriging):\n&gt;&gt;&gt;     def __init__(self):\n&gt;&gt;&gt;         super().__init__()\n&gt;&gt;&gt;         self.n_p = 2\n&gt;&gt;&gt;         self.n = 3\n&gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n&gt;&gt;&gt;         self.k = 2\n&gt;&gt;&gt;         self.seed = 1\n\n&gt;&gt;&gt; obj = MyClass()\n&gt;&gt;&gt; obj.build_Psi()\n&gt;&gt;&gt; obj.build_U()\n&gt;&gt;&gt; obj.likelihood()\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def likelihood(self) -&gt; None:\n    \"\"\"\n    Calculates the negative of the concentrated log-likelihood.\n\n    This method implements equation (2.32) in [Forr08a] to calculate\n    the negative of the concentrated log-likelihood. It also modifies `mu`,\n    `SigmaSqr`, `LnDetPsi`, and `negLnLike`.\n\n    Note:\n        `build_Psi` and `build_U` should be called first.\n\n    Args:\n        self (object):\n            The Kriging object.\n\n    Returns:\n        None\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; class MyClass(Kriging):\n        &gt;&gt;&gt;     def __init__(self):\n        &gt;&gt;&gt;         super().__init__()\n        &gt;&gt;&gt;         self.n_p = 2\n        &gt;&gt;&gt;         self.n = 3\n        &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n        &gt;&gt;&gt;         self.k = 2\n        &gt;&gt;&gt;         self.seed = 1\n\n        &gt;&gt;&gt; obj = MyClass()\n        &gt;&gt;&gt; obj.build_Psi()\n        &gt;&gt;&gt; obj.build_U()\n        &gt;&gt;&gt; obj.likelihood()\n    \"\"\"\n    # (2.20) in [Forr08a]:\n    U_T_inv_one = solve(self.U.T, self.one)\n    U_T_inv_cod_y = solve(self.U.T, self.cod_y)\n    mu = self.one.T.dot(solve(self.U, U_T_inv_cod_y)) / self.one.T.dot(solve(self.U, U_T_inv_one))\n    self.mu = mu\n    # (2.31) in [Forr08a]\n    cod_y_minus_mu = self.cod_y - self.one.dot(self.mu)\n    self.SigmaSqr = cod_y_minus_mu.T.dot(solve(self.U, solve(self.U.T, cod_y_minus_mu))) / self.n\n    # (2.32) in [Forr08a]\n    self.LnDetPsi = 2.0 * sum(log(abs(diag(self.U))))\n    self.negLnLike = -1.0 * (-(self.n / 2.0) * log(self.SigmaSqr) - 0.5 * self.LnDetPsi)\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.nat_to_cod_init","title":"<code>nat_to_cod_init()</code>","text":"<p>Determines max and min of each dimension and normalizes that axis to a range of [0,1]. Called when 1) surrogate is initialized and 2) new points arrive, i.e., suggested by the surrogate as infill points. This method calls <code>nat_to_cod_x</code> and <code>nat_to_cod_y</code> and updates the ranges <code>nat_range_X</code> and <code>nat_range_y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; kriging = Kriging()\n&gt;&gt;&gt; kriging.nat_to_cod_init()\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def nat_to_cod_init(self) -&gt; None:\n    \"\"\"\n    Determines max and min of each dimension and normalizes that axis to a range of [0,1].\n    Called when 1) surrogate is initialized and 2) new points arrive, i.e.,\n    suggested by the surrogate as infill points.\n    This method calls `nat_to_cod_x` and `nat_to_cod_y` and updates the ranges `nat_range_X` and `nat_range_y`.\n\n    Args:\n        self (object): The Kriging object.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; kriging = Kriging()\n        &gt;&gt;&gt; kriging.nat_to_cod_init()\n    \"\"\"\n    self.nat_range_X = []\n    self.nat_range_y = []\n    for i in range(self.k):\n        self.nat_range_X.append([min(self.nat_X[:, i]), max(self.nat_X[:, i])])\n    self.nat_range_y.append(min(self.nat_y))\n    self.nat_range_y.append(max(self.nat_y))\n    self.nat_mean_X = mean(self.nat_X, axis=0)\n    self.nat_std_X = std(self.nat_X, axis=0)\n    self.nat_mean_y = mean(self.nat_y)\n    self.nat_std_y = std(self.nat_y)\n    Z = aggregate_mean_var(X=self.nat_X, y=self.nat_y)\n    mu = Z[1]\n    self.mean_cod_y = empty_like(mu)\n\n    for i in range(self.n):\n        self.cod_X[i] = self.nat_to_cod_x(self.nat_X[i])\n    for i in range(self.n):\n        self.cod_y[i] = self.nat_to_cod_y(self.nat_y[i])\n    for i in range(mu.shape[0]):\n        self.mean_cod_y[i] = self.nat_to_cod_y(mu[i])\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.nat_to_cod_x","title":"<code>nat_to_cod_x(nat_X)</code>","text":"<p>Normalizes one point (row) of nat_X array to [0,1]. The internal nat_range_X values are not updated.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>nat_X</code> <code>ndarray</code> <p>An array representing one point (self.k long) in natural (physical or real world) units.</p> required <p>Returns:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>An array of coded values in the range of [0,1] for each dimension.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; from numpy import array\n&gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n&gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n&gt;&gt;&gt; k = Kriging(X, y)\n&gt;&gt;&gt; nat_X = array([5.0, 5.0])\n&gt;&gt;&gt; cod_X = k.nat_to_cod_x(nat_X)\n&gt;&gt;&gt; print(f\"Coded values: {cod_X}\")\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def nat_to_cod_x(self, nat_X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Normalizes one point (row) of nat_X array to [0,1]. The internal nat_range_X values are not updated.\n\n    Args:\n        self (object): The Kriging object.\n        nat_X (np.ndarray):\n            An array representing one point (self.k long) in natural (physical or real world) units.\n\n    Returns:\n        X (np.ndarray): An array of coded values in the range of [0,1] for each dimension.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; from numpy import array\n        &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n        &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n        &gt;&gt;&gt; k = Kriging(X, y)\n        &gt;&gt;&gt; nat_X = array([5.0, 5.0])\n        &gt;&gt;&gt; cod_X = k.nat_to_cod_x(nat_X)\n        &gt;&gt;&gt; print(f\"Coded values: {cod_X}\")\n\n    \"\"\"\n    X = copy.deepcopy(nat_X)\n    if self.cod_type == \"norm\":\n        for i in range(self.k):\n            # TODO: Check Implementation of range correction if range == 0:\n            # rangex &lt;- xmax - xmin\n            # rangey &lt;- ymax - ymin\n            # xmin[rangex == 0] &lt;- xmin[rangex == 0] - 0.5\n            # xmax[rangex == 0] &lt;- xmax[rangex == 0] + 0.5\n            # rangex[rangex == 0] &lt;- 1\n            # logger.debug(f\"self.nat_range_X[{i}]:\\n {self.nat_range_X[i]}\")\n            # logger.debug(f\"X[{i}]:\\n {X[i]}\")\n            rangex = float(self.nat_range_X[i][1] - self.nat_range_X[i][0])\n            if rangex == 0:\n                self.nat_range_X[i][0] = self.nat_range_X[i][0] - 0.5\n                self.nat_range_X[i][1] = self.nat_range_X[i][1] + 0.5\n            X[i] = (X[i] - self.nat_range_X[i][0]) / float(\n                self.nat_range_X[i][1] - self.nat_range_X[i][0]\n            )\n        return X\n    elif self.cod_type == \"std\":\n        for i in range(self.k):\n            X[i] = (X[i] - self.nat_mean_X[i]) / self.nat_std_X[i]\n        return X\n    else:\n        return nat_X\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.nat_to_cod_y","title":"<code>nat_to_cod_y(nat_y)</code>","text":"<p>Normalizes natural y values to [0,1].</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>nat_y</code> <code>ndarray</code> <p>An array of observed values in natural (real-world) units.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>A normalized array of coded (model) units in the range of [0,1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; kriging = Kriging()\n&gt;&gt;&gt; nat_y = np.array([5.0, 5.0])\n&gt;&gt;&gt; cod_y = kriging.nat_to_cod_y(nat_y)\n&gt;&gt;&gt; print(f\"Coded values: {cod_y}\")\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def nat_to_cod_y(self, nat_y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Normalizes natural y values to [0,1].\n\n    Args:\n        self (object): The Kriging object.\n        nat_y (np.ndarray):\n            An array of observed values in natural (real-world) units.\n\n    Returns:\n        y (np.ndarray):\n            A normalized array of coded (model) units in the range of [0,1].\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; kriging = Kriging()\n        &gt;&gt;&gt; nat_y = np.array([5.0, 5.0])\n        &gt;&gt;&gt; cod_y = kriging.nat_to_cod_y(nat_y)\n        &gt;&gt;&gt; print(f\"Coded values: {cod_y}\")\n    \"\"\"\n    return (\n        (nat_y - self.nat_range_y[0]) / (self.nat_range_y[1] - self.nat_range_y[0])\n        if self.use_cod_y and self.cod_type == \"norm\"\n        else (nat_y - self.nat_mean_y) / self.nat_std_y\n        if self.use_cod_y and self.cod_type == \"std\"\n        else nat_y\n    )\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.optimize_model","title":"<code>optimize_model()</code>","text":"<p>Optimize the model using the specified model_optimizer.</p> <p>This method uses the specified model_optimizer to optimize the likelihood function (<code>fun_likelihood</code>) with respect to the model parameters. The optimization is performed within the bounds specified by the attribute <code>de_bounds</code>. The result of the optimization is returned as a list or tuple of optimized parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n&gt;&gt;&gt; obj = MyClass()\n&gt;&gt;&gt; result = obj.optimize_model()\n&gt;&gt;&gt; print(result)\n[optimized_theta, optimized_p, optimized_Lambda]\n</code></pre> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[float]]</code> <p>result[\u201cx\u201d] (Union[List[float], Tuple[float]]): A list or tuple of optimized parameter values.</p> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def optimize_model(self) -&gt; Union[List[float], Tuple[float]]:\n    \"\"\"\n    Optimize the model using the specified model_optimizer.\n\n    This method uses the specified model_optimizer to optimize the\n    likelihood function (`fun_likelihood`) with respect to the model parameters.\n    The optimization is performed within the bounds specified by the attribute\n    `de_bounds`.\n    The result of the optimization is returned as a list or tuple of optimized parameter values.\n\n    Args:\n        self (object): The Kriging object.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n        &gt;&gt;&gt; obj = MyClass()\n        &gt;&gt;&gt; result = obj.optimize_model()\n        &gt;&gt;&gt; print(result)\n        [optimized_theta, optimized_p, optimized_Lambda]\n\n    Returns:\n        result[\"x\"] (Union[List[float], Tuple[float]]):\n            A list or tuple of optimized parameter values.\n    \"\"\"\n    if self.model_optimizer.__name__ == 'dual_annealing':\n        result = self.model_optimizer(func=self.fun_likelihood,\n                                      bounds=self.de_bounds)\n    elif self.model_optimizer.__name__ == 'differential_evolution':\n        result = self.model_optimizer(func=self.fun_likelihood,\n                                      bounds=self.de_bounds,\n                                      maxiter=self.model_fun_evals,\n                                      seed=self.seed)\n    elif self.model_optimizer.__name__ == 'direct':\n        result = self.model_optimizer(func=self.fun_likelihood,\n                                      bounds=self.de_bounds,\n                                      # maxfun=self.model_fun_evals,\n                                      eps=1e-2)\n    elif self.model_optimizer.__name__ == 'shgo':\n        result = self.model_optimizer(func=self.fun_likelihood,\n                                      bounds=self.de_bounds)\n    elif self.model_optimizer.__name__ == 'basinhopping':\n        result = self.model_optimizer(func=self.fun_likelihood,\n                                      x0=mean(self.de_bounds, axis=1))\n    else:\n        result = self.model_optimizer(func=self.fun_likelihood, bounds=self.de_bounds)\n    return result[\"x\"]\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.plot","title":"<code>plot(show=True)</code>","text":"<p>This function plots 1D and 2D surrogates.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>show</code> <code>bool</code> <p>If <code>True</code>, the plots are displayed. If <code>False</code>, <code>plt.show()</code> should be called outside this function.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; class MyClass(Kriging):\n&gt;&gt;&gt;     def __init__(self):\n&gt;&gt;&gt;         super().__init__()\n&gt;&gt;&gt;         self.n_p = 2\n&gt;&gt;&gt;         self.n = 3\n&gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n&gt;&gt;&gt;         self.k = 2\n&gt;&gt;&gt;         self.seed = 1\n\n&gt;&gt;&gt; plot(show=True)\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def plot(self, show: Optional[bool] = True) -&gt; None:\n    \"\"\"\n    This function plots 1D and 2D surrogates.\n\n    Args:\n        self (object):\n            The Kriging object.\n        show (bool):\n            If `True`, the plots are displayed.\n            If `False`, `plt.show()` should be called outside this function.\n\n    Returns:\n        None\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; class MyClass(Kriging):\n        &gt;&gt;&gt;     def __init__(self):\n        &gt;&gt;&gt;         super().__init__()\n        &gt;&gt;&gt;         self.n_p = 2\n        &gt;&gt;&gt;         self.n = 3\n        &gt;&gt;&gt;         self.nat_y = np.array([1, 2, 3])\n        &gt;&gt;&gt;         self.k = 2\n        &gt;&gt;&gt;         self.seed = 1\n\n        &gt;&gt;&gt; plot(show=True)\n    \"\"\"\n    if self.k == 1:\n        # TODO: Improve plot (add conf. interval etc.)\n        fig = pylab.figure(figsize=(9, 6))\n        # t1 = array(arange(0.0, 1.0, 0.01))\n        # y1 = array([self.predict(array([x]), return_val=\"y\") for x in t1])\n        # plt.figure()\n        # plt.plot(t1, y1, \"k\")\n        # if show:\n        #     plt.show()\n        #\n        n_grid = 100\n        x = linspace(\n            self.nat_range_X[0][0], self.nat_range_X[0][1], num=n_grid\n        )\n        y = self.predict(x)\n        plt.figure()\n        plt.plot(x, y, \"k\")\n        if show:\n            plt.show()\n\n    if self.k == 2:\n        fig = pylab.figure(figsize=(9, 6))\n        n_grid = 100\n        x = linspace(\n            self.nat_range_X[0][0], self.nat_range_X[0][1], num=n_grid\n        )\n        y = linspace(\n            self.nat_range_X[1][0], self.nat_range_X[1][1], num=n_grid\n        )\n        X, Y = meshgrid(x, y)\n        # Predict based on the optimized results\n        zz = array(\n            [self.predict(array([x, y]), return_val=\"all\") for x, y in zip(ravel(X), ravel(Y))]\n        )\n        zs = zz[:, 0, :]\n        zse = zz[:, 1, :]\n        Z = zs.reshape(X.shape)\n        Ze = zse.reshape(X.shape)\n\n        if self.cod_type == \"norm\":\n            nat_point_X = (\n                                  self.cod_X[:, 0] * (self.nat_range_X[0][1] - self.nat_range_X[0][0])\n                          ) + self.nat_range_X[0][0]\n            nat_point_Y = (\n                                  self.cod_X[:, 1] * (self.nat_range_X[1][1] - self.nat_range_X[1][0])\n                          ) + self.nat_range_X[1][0]\n        elif self.cod_type == \"std\":\n            nat_point_X = self.cod_X[:, 0] * self.nat_std_X[0] + self.nat_mean_X[0]\n            nat_point_Y = self.cod_X[:, 1] * self.nat_std_X[1] + self.nat_mean_X[1]\n        else:\n            nat_point_X = self.cod_X[:, 0]\n            nat_point_Y = self.cod_X[:, 1]\n        contour_levels = 30\n        ax = fig.add_subplot(224)\n        # plot predicted values:\n        pylab.contourf(X, Y, Ze, contour_levels, cmap=\"jet\")\n        pylab.title(\"Error\")\n        pylab.colorbar()\n        # plot observed points:\n        pylab.plot(nat_point_X, nat_point_Y, \"ow\")\n        #\n        ax = fig.add_subplot(223)\n        # plot predicted values:\n        plt.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\")\n        plt.title(\"Surrogate\")\n        # plot observed points:\n        pylab.plot(nat_point_X, nat_point_Y, \"ow\", zorder=3)\n        pylab.colorbar()\n        #\n        ax = fig.add_subplot(221, projection=\"3d\")\n        ax.plot_surface(X, Y, Z, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n        #\n        ax = fig.add_subplot(222, projection=\"3d\")\n        ax.plot_surface(X, Y, Ze, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\")\n        #\n        pylab.show()\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.predict","title":"<code>predict(nat_X, nat=True, return_val='y')</code>","text":"<p>This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>nat_X</code> <code>ndarray</code> <p>Design variable to evaluate in natural units.</p> required <code>nat</code> <code>bool</code> <p>argument <code>nat_X</code> is in natural range. Default: <code>True</code>. If set to <code>False</code>, <code>nat_X</code> will not be normalized (which might be useful if already normalized y values are used).</p> <code>True</code> <code>return_val</code> <code>str</code> <p>whether <code>y</code>, <code>s</code>, neg. <code>ei</code> (negative expected improvement), or all three values are returned. Default is (for compatibility with sklearn) \u201cy\u201d. To return <code>s</code>, select \u201cs\u201d, to return neg. <code>ei</code>, select \u201cei\u201d. To return the tuple <code>(y, s, ei)</code>, select \u201call\u201d.</p> <code>'y'</code> <p>Returns:</p> Name Type Description <code>float</code> <code>Union[float, Tuple[float, float, float]]</code> <p>The predicted value in natural units if return_val is \u201cy\u201d.</p> <code>float</code> <code>Union[float, Tuple[float, float, float]]</code> <p>predicted error if return_val is \u201cs\u201d.</p> <code>float</code> <code>Union[float, Tuple[float, float, float]]</code> <p>expected improvement if return_val is \u201cei\u201d.</p> <code>Union[float, Tuple[float, float, float]]</code> <p>Tuple[float, float, float]: The predicted value in natural units, predicted error and expected improvement if return_val is \u201call\u201d.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; from numpy import array\n&gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n&gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n&gt;&gt;&gt; k = Kriging(X, y)\n&gt;&gt;&gt; k.predict(array([[0.3, 0.3]]))\narray([0.09])\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def predict(self, nat_X: ndarray, nat: bool = True, return_val: str = \"y\") -&gt; Union[float,\n                                                                                    Tuple[float,\n                                                                                          float,\n                                                                                          float]]:\n    \"\"\"\n    This function returns the prediction (in natural units) of the surrogate at the natural coordinates of X.\n\n    Args:\n        self (object):\n            The Kriging object.\n        nat_X (ndarray):\n            Design variable to evaluate in natural units.\n        nat (bool):\n            argument `nat_X` is in natural range. Default: `True`.\n            If set to `False`, `nat_X` will not be normalized (which might be useful\n            if already normalized y values are used).\n        return_val (str):\n            whether `y`, `s`, neg. `ei` (negative expected improvement),\n            or all three values are returned.\n            Default is (for compatibility with sklearn) \"y\". To return `s`, select \"s\",\n            to return neg. `ei`, select \"ei\".\n            To return the tuple `(y, s, ei)`, select \"all\".\n\n    Returns:\n        float:\n            The predicted value in natural units if return_val is \"y\".\n        float:\n            predicted error if return_val is \"s\".\n        float:\n            expected improvement if return_val is \"ei\".\n        Tuple[float, float, float]:\n            The predicted value in natural units, predicted error\n            and expected improvement if return_val is \"all\".\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; from numpy import array\n        &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n        &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n        &gt;&gt;&gt; k = Kriging(X, y)\n        &gt;&gt;&gt; k.predict(array([[0.3, 0.3]]))\n        array([0.09])\n\n    \"\"\"\n    # Check for the shape and the type of the Input\n    if isinstance(nat_X, ndarray):\n        try:\n            X = nat_X.reshape(-1, self.nat_X.shape[1])\n            X = repair_non_numeric(X, self.var_type)\n        except Exception:\n            raise TypeError(\"13.1: Input to predict was not convertible to the size of X\")\n    else:\n        raise TypeError(f\"type of the given input is an {type(nat_X)} instead of an ndarray\")\n    n = X.shape[0]\n    y = empty(n, dtype=float)\n    s = empty(n, dtype=float)\n    ei = empty(n, dtype=float)\n    for i in range(n):\n        if nat:\n            x = self.nat_to_cod_x(X[i, :])\n        else:\n            x = X[i, :]\n        y[i], s[i], ei[i] = self.predict_coded(x)\n    if return_val == \"y\":\n        return y\n    elif return_val == \"s\":\n        return s\n    elif return_val == \"ei\":\n        return -1.0 * ei\n    else:\n        return y, s, -1.0 * ei\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.predict_coded","title":"<code>predict_coded(cod_x)</code>","text":"<p>Kriging prediction of one point in the coded units as described in (2.20) in [Forr08a]. The error is returned as well.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>cod_x</code> <code>ndarray</code> <p>Point in coded units to make prediction at.</p> required <p>Returns:</p> Name Type Description <code>f</code> <code>float</code> <p>Predicted value in coded units.</p> <code>SSqr</code> <code>float</code> <p>Predicted error.</p> <code>EI</code> <code>float</code> <p>Expected improvement.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; from numpy import array\n&gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n&gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n&gt;&gt;&gt; k = Kriging(X, y)\n&gt;&gt;&gt; cod_x = array([0.3, 0.3])\n&gt;&gt;&gt; k.predict_coded(cod_x)\n(0.09, 0.0, 0.0)\n</code></pre> Note <p><code>self.mu</code> and <code>self.SigmaSqr</code> are computed in <code>likelihood</code>, not here. See also [Forr08a, p.60].</p> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def predict_coded(self, cod_x: np.ndarray) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    Kriging prediction of one point in the coded units as described in (2.20) in [Forr08a].\n    The error is returned as well.\n\n    Args:\n        self (object):\n            The Kriging object.\n        cod_x (np.ndarray):\n            Point in coded units to make prediction at.\n\n    Returns:\n        f (float): Predicted value in coded units.\n        SSqr (float): Predicted error.\n        EI (float): Expected improvement.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; from numpy import array\n        &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n        &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n        &gt;&gt;&gt; k = Kriging(X, y)\n        &gt;&gt;&gt; cod_x = array([0.3, 0.3])\n        &gt;&gt;&gt; k.predict_coded(cod_x)\n        (0.09, 0.0, 0.0)\n\n    Note:\n        `self.mu` and `self.SigmaSqr` are computed in `likelihood`, not here.\n        See also [Forr08a, p.60].\n    \"\"\"\n    self.build_psi_vec(cod_x)\n    U_T_inv = solve(self.U.T, self.cod_y - self.one.dot(self.mu))\n    f = self.mu + self.psi.T.dot(solve(self.U, U_T_inv))\n    if self.noise:\n        Lambda = self.Lambda\n    else:\n        Lambda = 0.0\n    # Error in [Forr08a, p.87]:\n    SSqr = self.SigmaSqr * (1 + Lambda - self.psi.T.dot(solve(self.U, solve(self.U.T, self.psi))))\n    SSqr = power(abs(SSqr[0]), 0.5)[0]\n    EI = self.exp_imp(y0=f[0], s0=SSqr)\n    return f[0], SSqr, EI\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.set_de_bounds","title":"<code>set_de_bounds()</code>","text":"<p>Determine search bounds for model_optimizer, e.g., differential evolution.</p> <p>This method sets the attribute <code>de_bounds</code> of the object to a list of lists, where each inner list represents the lower and upper bounds for a parameter being optimized. The number of inner lists is determined by the number of parameters being optimized (<code>n_theta</code> and <code>n_p</code>), as well as whether noise is being considered (<code>noise</code>).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n&gt;&gt;&gt; obj = MyClass()\n&gt;&gt;&gt; obj.set_de_bounds()\n&gt;&gt;&gt; print(obj.de_bounds)\n[[min_theta, max_theta], [min_theta, max_theta], ..., [min_p, max_p], [min_Lambda, max_Lambda]]\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def set_de_bounds(self) -&gt; None:\n    \"\"\"\n    Determine search bounds for model_optimizer, e.g., differential evolution.\n\n    This method sets the attribute `de_bounds` of the object to a list of lists,\n    where each inner list represents the lower and upper bounds for a parameter\n    being optimized. The number of inner lists is determined by the number of\n    parameters being optimized (`n_theta` and `n_p`), as well as whether noise is\n    being considered (`noise`).\n\n    Args:\n        self (object): The Kriging object.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n        &gt;&gt;&gt; obj = MyClass()\n        &gt;&gt;&gt; obj.set_de_bounds()\n        &gt;&gt;&gt; print(obj.de_bounds)\n        [[min_theta, max_theta], [min_theta, max_theta], ..., [min_p, max_p], [min_Lambda, max_Lambda]]\n\n    Returns:\n        None\n    \"\"\"\n    de_bounds = [[self.min_theta, self.max_theta] for _ in range(self.n_theta)]\n    if self.optim_p:\n        de_bounds += [[self.min_p, self.max_p] for _ in range(self.n_p)]\n        if self.noise:\n            de_bounds.append([self.min_Lambda, self.max_Lambda])\n    else:\n        if self.noise:\n            de_bounds.append([self.min_Lambda, self.max_Lambda])\n    self.de_bounds = de_bounds\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.set_theta_values","title":"<code>set_theta_values()</code>","text":"<p>Set the theta values for the class instance.</p> <p>This method sets the theta values for the class instance based on the <code>n_theta</code> and <code>k</code> attributes. If <code>n_theta</code> is greater than <code>k</code>, <code>n_theta</code> is set to <code>k</code> and a warning is logged. The method then initializes the <code>theta</code> attribute as a list of zeros with length <code>n_theta</code>. The <code>x0_theta</code> attribute is also initialized as a list of ones with length <code>n_theta</code>, multiplied by <code>n / (100 * k)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Returns:     None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; class MyClass(Kriging):\n&gt;&gt;&gt;     def __init__(self):\n&gt;&gt;&gt;         super().__init__()\n&gt;&gt;&gt;         self.n_theta = 3\n&gt;&gt;&gt;         self.k = 2\n&gt;&gt;&gt; instance = MyClass()\n&gt;&gt;&gt; instance.set_theta_values()\n&gt;&gt;&gt; instance.theta\narray([0., 0., 0.])\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def set_theta_values(self) -&gt; None:\n    \"\"\"\n    Set the theta values for the class instance.\n\n    This method sets the theta values for the class instance based\n    on the `n_theta` and `k` attributes. If `n_theta` is greater than\n    `k`, `n_theta` is set to `k` and a warning is logged.\n    The method then initializes the `theta` attribute as a list\n    of zeros with length `n_theta`.\n    The `x0_theta` attribute is also initialized as a list of ones\n    with length `n_theta`, multiplied by `n / (100 * k)`.\n\n    Args:\n        self (object): The Kriging object.\n    Returns:\n        None\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; class MyClass(Kriging):\n        &gt;&gt;&gt;     def __init__(self):\n        &gt;&gt;&gt;         super().__init__()\n        &gt;&gt;&gt;         self.n_theta = 3\n        &gt;&gt;&gt;         self.k = 2\n        &gt;&gt;&gt; instance = MyClass()\n        &gt;&gt;&gt; instance.set_theta_values()\n        &gt;&gt;&gt; instance.theta\n        array([0., 0., 0.])\n    \"\"\"\n    if self.n_theta &gt; self.k:\n        self.n_theta = self.k\n        logger.warning(\"More theta values than dimensions. `n_theta` set to `k`.\")\n    self.theta: List[float] = zeros(self.n_theta)\n    # TODO: Currently not used:\n    self.x0_theta: List[float] = ones((self.n_theta,)) * self.n / (100 * self.k)\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.set_variable_types","title":"<code>set_variable_types()</code>","text":"<p>Set the variable types for the class instance.</p> <p>This method sets the variable types for the class instance based on the <code>var_type</code> attribute. If the length of <code>var_type</code> is less than <code>k</code>, all variable types are forced to \u2018num\u2019 and a warning is logged. The method then creates masks for each variable type (\u2018num\u2019, \u2018factor\u2019, \u2018int\u2019, \u2018float\u2019) using numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; class MyClass(Kriging):\n&gt;&gt;&gt;     def __init__(self):\n&gt;&gt;&gt;         super().__init__()\n&gt;&gt;&gt;         self.var_type = [\"num\", \"factor\"]\n&gt;&gt;&gt; instance = MyClass()\n&gt;&gt;&gt; instance.set_variable_types()\n&gt;&gt;&gt; instance.num_mask\narray([ True, False])\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def set_variable_types(self) -&gt; None:\n    \"\"\"\n    Set the variable types for the class instance.\n\n    This method sets the variable types for the class instance based\n    on the `var_type` attribute. If the length of `var_type` is less\n    than `k`, all variable types are forced to 'num' and a warning is logged.\n    The method then creates masks for each variable\n    type ('num', 'factor', 'int', 'float') using numpy arrays.\n\n    Args:\n        self (object): The Kriging object.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; class MyClass(Kriging):\n        &gt;&gt;&gt;     def __init__(self):\n        &gt;&gt;&gt;         super().__init__()\n        &gt;&gt;&gt;         self.var_type = [\"num\", \"factor\"]\n        &gt;&gt;&gt; instance = MyClass()\n        &gt;&gt;&gt; instance.set_variable_types()\n        &gt;&gt;&gt; instance.num_mask\n        array([ True, False])\n\n    Returns:\n        None\n    \"\"\"\n    # assume all variable types are \"num\" if \"num\" is\n    # specified once:\n    if len(self.var_type) &lt; self.k:\n        self.var_type = self.var_type * self.k\n        logger.warning(\"Warning: All variable types forced to 'num'.\")\n    self.num_mask = np.array(list(map(lambda x: x == \"num\", self.var_type)))\n    self.factor_mask = np.array(list(map(lambda x: x == \"factor\", self.var_type)))\n    self.int_mask = np.array(list(map(lambda x: x == \"int\", self.var_type)))\n    self.ordered_mask = np.array(list(map(lambda x: x == \"int\" or x == \"num\" or x == \"float\", self.var_type)))\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.update_log","title":"<code>update_log()</code>","text":"<p>Update the log with the current values of negLnLike, theta, p, and Lambda.</p> <p>This method appends the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to their respective lists in the log dictionary. It also updates the log_length attribute with the current length of the negLnLike list in the log.</p> <p>If spot_writer is not None, this method also writes the current values of negLnLike, theta, p (if optim_p is True), and Lambda (if noise is True) to the spot_writer object.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n&gt;&gt;&gt; obj = MyClass()\n&gt;&gt;&gt; obj.update_log()\n&gt;&gt;&gt; print(obj.log)\n{'negLnLike': [0.5], 'theta': [0.1], 'p': [0.2], 'Lambda': [0.3]}\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def update_log(self) -&gt; None:\n    \"\"\"\n    Update the log with the current values of negLnLike, theta, p, and Lambda.\n\n    This method appends the current values of negLnLike, theta, p (if optim_p is True),\n    and Lambda (if noise is True)\n    to their respective lists in the log dictionary.\n    It also updates the log_length attribute with the current length\n    of the negLnLike list in the log.\n\n    If spot_writer is not None, this method also writes the current values of\n    negLnLike, theta, p (if optim_p is True),\n    and Lambda (if noise is True) to the spot_writer object.\n\n    Args:\n        self (object): The Kriging object.\n\n    Returns:\n        None\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; MyClass = Kriging(name='kriging', seed=124)\n        &gt;&gt;&gt; obj = MyClass()\n        &gt;&gt;&gt; obj.update_log()\n        &gt;&gt;&gt; print(obj.log)\n        {'negLnLike': [0.5], 'theta': [0.1], 'p': [0.2], 'Lambda': [0.3]}\n    \"\"\"\n    self.log[\"negLnLike\"] = append(self.log[\"negLnLike\"], self.negLnLike)\n    self.log[\"theta\"] = append(self.log[\"theta\"], self.theta)\n    if self.optim_p:\n        self.log[\"p\"] = append(self.log[\"p\"], self.p)\n    if self.noise:\n        self.log[\"Lambda\"] = append(self.log[\"Lambda\"], self.Lambda)\n    # get the length of the log\n    self.log_length = len(self.log[\"negLnLike\"])\n    if self.spot_writer is not None:\n        writer = self.spot_writer\n        negLnLike = self.negLnLike.copy()\n        writer.add_scalar(\"spot_negLnLike\", negLnLike, self.counter+self.log_length)\n        # add the self.n_theta theta values to the writer with one key \"theta\",\n        # i.e, the same key for all theta values\n        theta = self.theta.copy()\n        writer.add_scalars(\"spot_theta\", {f\"theta_{i}\": theta[i] for i in range(self.n_theta)},\n                           self.counter+self.log_length)\n        if self.noise:\n            Lambda = self.Lambda.copy()\n            writer.add_scalar(\"spot_Lambda\", Lambda, self.counter+self.log_length)\n        if self.optim_p:\n            p = self.p.copy()\n            writer.add_scalars(\"spot_p\", {f\"p_{i}\": p[i] for i in range(self.n_p)}, self.counter+self.log_length)\n        writer.flush()\n</code></pre>"},{"location":"reference/spotPython/build/kriging/#spotPython.build.kriging.Kriging.weighted_exp_imp","title":"<code>weighted_exp_imp(cod_x, w)</code>","text":"<p>Weighted expected improvement.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The Kriging object.</p> required <code>cod_x</code> <code>ndarray</code> <p>A coded design vector.</p> required <code>w</code> <code>float</code> <p>Weight.</p> required <p>Returns:</p> Name Type Description <code>EI</code> <code>float</code> <p>Weighted expected improvement.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.build.kriging import Kriging\n&gt;&gt;&gt; from numpy import array\n&gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n&gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n&gt;&gt;&gt; k = Kriging(X, y)\n&gt;&gt;&gt; cod_x = array([0.3, 0.3])\n&gt;&gt;&gt; w = 0.5\n&gt;&gt;&gt; k.weighted_exp_imp(cod_x, w)\n0.0\n</code></pre> <p>References:</p> <pre><code>[Sobester et al. 2005].\n</code></pre> Source code in <code>spotPython/build/kriging.py</code> <pre><code>def weighted_exp_imp(self, cod_x: np.ndarray, w: float) -&gt; float:\n    \"\"\"\n    Weighted expected improvement.\n\n    Args:\n        self (object): The Kriging object.\n        cod_x (np.ndarray): A coded design vector.\n        w (float): Weight.\n\n    Returns:\n        EI (float): Weighted expected improvement.\n\n    Examples:\n\n        &gt;&gt;&gt; from spotPython.build.kriging import Kriging\n        &gt;&gt;&gt; from numpy import array\n        &gt;&gt;&gt; X = array([[0.0, 0.0], [0.1, 0.1], [0.2, 0.2]])\n        &gt;&gt;&gt; y = array([0.0, 0.01, 0.04])\n        &gt;&gt;&gt; k = Kriging(X, y)\n        &gt;&gt;&gt; cod_x = array([0.3, 0.3])\n        &gt;&gt;&gt; w = 0.5\n        &gt;&gt;&gt; k.weighted_exp_imp(cod_x, w)\n        0.0\n\n    References:\n\n        [Sobester et al. 2005].\n    \"\"\"\n    y0, s0 = self.predict_coded(cod_x)\n    y_min = min(self.cod_y)\n    if s0 &lt;= 0.0:\n        EI = 0.0\n    else:\n        y_min_y0 = y_min - y0\n        EI_one = w * (\n                y_min_y0\n                * (0.5 + 0.5 * erf((1.0 / sqrt(2.0)) * (y_min_y0 / s0)))\n        )\n        EI_two = (\n                (1.0 - w)\n                * (s0 * (1.0 / sqrt(2.0 * pi)))\n                * (exp(-(1.0 / 2.0) * ((y_min_y0) ** 2.0 / s0 ** 2.0)))\n        )\n        EI = EI_one + EI_two\n    return EI\n</code></pre>"},{"location":"reference/spotPython/build/surrogates/","title":"surrogates","text":""},{"location":"reference/spotPython/build/surrogates/#spotPython.build.surrogates.surrogates","title":"<code>surrogates</code>","text":"<p>Super class for all surrogate model classes (e.g., Kriging)</p> Source code in <code>spotPython/build/surrogates.py</code> <pre><code>class surrogates:\n    \"\"\"\n    Super class for all surrogate model classes (e.g., Kriging)\n    \"\"\"\n    def __init__(self, name=\"\", seed=123, verbosity=0):\n        self.name = name\n        self.seed = seed\n        self.rng = default_rng(self.seed)\n        self.log = {}\n        self.verbosity = verbosity\n</code></pre>"},{"location":"reference/spotPython/data/","title":"data","text":"<p>Datasets.</p> <p>This module contains a collection of datasets for multiple tasks: classification, regression, etc. The data corresponds to popular datasets and are conveniently wrapped to easily iterate over the data in a stream fashion. All datasets have fixed size.</p>"},{"location":"reference/spotPython/data/base/","title":"base","text":""},{"location":"reference/spotPython/data/base/#spotPython.data.base.Config","title":"<code>Config</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for all configurations.</p> <p>All configurations inherit from this class, be they stored in a file or generated on the fly.</p> <p>Attributes:</p> Name Type Description <code>desc</code> <code>str</code> <p>The description from the docstring.</p> <code>_repr_content</code> <code>dict</code> <p>The items that are displayed in the repr method.</p> Source code in <code>spotPython/data/base.py</code> <pre><code>class Config(abc.ABC):\n    \"\"\"Base class for all configurations.\n\n    All configurations inherit from this class, be they stored in a file or generated on the fly.\n\n    Attributes:\n        desc (str): The description from the docstring.\n        _repr_content (dict): The items that are displayed in the __repr__ method.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize a Config object.\"\"\"\n        pass\n\n    @property\n    def desc(self) -&gt; str:\n        \"\"\"Return the description from the docstring.\n\n        Returns:\n            str: The description from the docstring.\n\n        Examples:\n            &gt;&gt;&gt; class MyConfig(Config):\n            ...     '''My configuration class.'''\n            ...     pass\n            &gt;&gt;&gt; MyConfig().desc\n            'My configuration class.'\n        \"\"\"\n        desc = re.split(pattern=r\"\\w+\\n\\s{4}\\-{3,}\", string=self.__doc__, maxsplit=0)[0]\n        return inspect.cleandoc(desc)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The items that are displayed in the __repr__ method.\n\n        This property can be overridden in order to modify the output of the __repr__ method.\n\n        Returns:\n            dict: A dictionary containing the items to be displayed in the __repr__ method.\n\n        Examples:\n            &gt;&gt;&gt; class MyConfig(Config):\n            ...     '''My configuration class.'''\n            ...     pass\n            &gt;&gt;&gt; MyConfig()._repr_content\n            {'Name': 'MyConfig'}\n        \"\"\"\n        content = {}\n        content[\"Name\"] = self.__class__.__name__\n        return content\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Config.desc","title":"<code>desc: str</code>  <code>property</code>","text":"<p>Return the description from the docstring.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The description from the docstring.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyConfig(Config):\n...     '''My configuration class.'''\n...     pass\n&gt;&gt;&gt; MyConfig().desc\n'My configuration class.'\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Config.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a Config object.</p> Source code in <code>spotPython/data/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize a Config object.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for all datasets.</p> <p>All datasets inherit from this class, be they stored in a file or generated on the fly.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d</p> required <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> <code>None</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset, only applies to classification datasets.</p> <code>None</code> <code>n_outputs</code> <code>int</code> <p>Number of outputs the target is made of, only applies to multi-output datasets.</p> <code>None</code> <code>sparse</code> <code>bool</code> <p>Whether the dataset is sparse or not.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>desc</code> <code>str</code> <p>The description from the docstring.</p> <code>_repr_content</code> <code>dict</code> <p>The items that are displayed in the repr method.</p> Source code in <code>spotPython/data/base.py</code> <pre><code>class Dataset(abc.ABC):\n    \"\"\"Base class for all datasets.\n\n    All datasets inherit from this class, be they stored in a file or generated on the fly.\n\n    Args:\n        task (str): Type of task the dataset is meant for. Should be one of:\n            - \"Regression\"\n            - \"Binary classification\"\n            - \"Multi-class classification\"\n            - \"Multi-output binary classification\"\n            - \"Multi-output regression\"\n        n_features (int): Number of features in the dataset.\n        n_samples (int, optional): Number of samples in the dataset.\n        n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets.\n        n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets.\n        sparse (bool, optional): Whether the dataset is sparse or not.\n\n    Attributes:\n        desc (str): The description from the docstring.\n        _repr_content (dict): The items that are displayed in the __repr__ method.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        n_features: int,\n        n_samples: Optional[int] = None,\n        n_classes: Optional[int] = None,\n        n_outputs: Optional[int] = None,\n        sparse: bool = False,\n    ):\n        \"\"\"Initialize a Dataset object.\n\n        Args:\n            task (str): Type of task the dataset is meant for. Should be one of:\n                - \"Regression\"\n                - \"Binary classification\"\n                - \"Multi-class classification\"\n                - \"Multi-output binary classification\"\n                - \"Multi-output regression\"\n            n_features (int): Number of features in the dataset.\n            n_samples (int, optional): Number of samples in the dataset. Defaults to None.\n            n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets.\n                Defaults to None.\n            n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets.\n                Defaults to None.\n            sparse (bool, optional): Whether the dataset is sparse or not. Defaults to False.\n        \"\"\"\n        self.task = task\n        self.n_features = n_features\n        self.n_samples = n_samples\n        self.n_outputs = n_outputs\n        self.n_classes = n_classes\n        self.sparse = sparse\n\n    @abc.abstractmethod\n    def __iter__(self):\n        \"\"\"Abstract method for iterating over samples in the dataset.\"\"\"\n        raise NotImplementedError\n\n    def take(self, k: int) -&gt; itertools.islice:\n        \"\"\"Iterate over the k samples.\n\n        Args:\n            k (int): The number of samples to iterate over.\n\n        Returns:\n            itertools.islice: An iterator over the first k samples in the dataset.\n\n        Examples:\n            &gt;&gt;&gt; class MyDataset(Dataset):\n            ...     def __init__(self):\n            ...         super().__init__('Regression', 10)\n            ...     def __iter__(self):\n            ...         yield from range(10)\n            &gt;&gt;&gt; list(MyDataset().take(5))\n            [0, 1, 2, 3, 4]\n        \"\"\"\n        return itertools.islice(self, k)\n\n    @property\n    def desc(self) -&gt; str:\n        \"\"\"Return the description from the docstring.\n\n        Returns:\n            str: The description from the docstring.\n\n        Examples:\n            &gt;&gt;&gt; class MyDataset(Dataset):\n            ...     '''My dataset class.'''\n            ...     def __init__(self):\n            ...         super().__init__('Regression', 10)\n            ...     def __iter__(self):\n            ...         yield from range(10)\n            &gt;&gt;&gt; MyDataset().desc\n            'My dataset class.'\n        \"\"\"\n        desc = re.split(pattern=r\"\\w+\\n\\s{4}\\-{3,}\", string=self.__doc__, maxsplit=0)[0]\n        return inspect.cleandoc(desc)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The items that are displayed in the __repr__ method.\n\n        This property can be overridden in order to modify the output of the __repr__ method.\n\n        Returns:\n            dict: A dictionary containing the items to be displayed in the __repr__ method.\n        \"\"\"\n\n        content = {}\n        content[\"Name\"] = self.__class__.__name__\n        content[\"Task\"] = self.task\n        if isinstance(self, SyntheticDataset) and self.n_samples is None:\n            content[\"Samples\"] = \"\u221e\"\n        elif self.n_samples:\n            content[\"Samples\"] = f\"{self.n_samples:,}\"\n        if self.n_features:\n            content[\"Features\"] = f\"{self.n_features:,}\"\n        if self.n_outputs:\n            content[\"Outputs\"] = f\"{self.n_outputs:,}\"\n        if self.n_classes:\n            content[\"Classes\"] = f\"{self.n_classes:,}\"\n        content[\"Sparse\"] = str(self.sparse)\n\n        return content\n\n    def __repr__(self):\n        l_len = max(map(len, self._repr_content.keys()))\n        r_len = max(map(len, self._repr_content.values()))\n\n        out = f\"{self.desc}\\n\\n\" + \"\\n\".join(\n            k.rjust(l_len) + \"  \" + v.ljust(r_len) for k, v in self._repr_content.items()\n        )\n\n        if \"Parameters\\n    ----------\" in self.__doc__:\n            params = re.split(\n                r\"\\w+\\n\\s{4}\\-{3,}\",\n                re.split(\"Parameters\\n    ----------\", self.__doc__)[1],\n            )[0].rstrip()\n            out += f\"\\n\\nParameters\\n----------{params}\"\n\n        return out\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Dataset.desc","title":"<code>desc: str</code>  <code>property</code>","text":"<p>Return the description from the docstring.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The description from the docstring.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyDataset(Dataset):\n...     '''My dataset class.'''\n...     def __init__(self):\n...         super().__init__('Regression', 10)\n...     def __iter__(self):\n...         yield from range(10)\n&gt;&gt;&gt; MyDataset().desc\n'My dataset class.'\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Dataset.__init__","title":"<code>__init__(task, n_features, n_samples=None, n_classes=None, n_outputs=None, sparse=False)</code>","text":"<p>Initialize a Dataset object.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d</p> required <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset. Defaults to None.</p> <code>None</code> <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset, only applies to classification datasets. Defaults to None.</p> <code>None</code> <code>n_outputs</code> <code>int</code> <p>Number of outputs the target is made of, only applies to multi-output datasets. Defaults to None.</p> <code>None</code> <code>sparse</code> <code>bool</code> <p>Whether the dataset is sparse or not. Defaults to False.</p> <code>False</code> Source code in <code>spotPython/data/base.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    n_features: int,\n    n_samples: Optional[int] = None,\n    n_classes: Optional[int] = None,\n    n_outputs: Optional[int] = None,\n    sparse: bool = False,\n):\n    \"\"\"Initialize a Dataset object.\n\n    Args:\n        task (str): Type of task the dataset is meant for. Should be one of:\n            - \"Regression\"\n            - \"Binary classification\"\n            - \"Multi-class classification\"\n            - \"Multi-output binary classification\"\n            - \"Multi-output regression\"\n        n_features (int): Number of features in the dataset.\n        n_samples (int, optional): Number of samples in the dataset. Defaults to None.\n        n_classes (int, optional): Number of classes in the dataset, only applies to classification datasets.\n            Defaults to None.\n        n_outputs (int, optional): Number of outputs the target is made of, only applies to multi-output datasets.\n            Defaults to None.\n        sparse (bool, optional): Whether the dataset is sparse or not. Defaults to False.\n    \"\"\"\n    self.task = task\n    self.n_features = n_features\n    self.n_samples = n_samples\n    self.n_outputs = n_outputs\n    self.n_classes = n_classes\n    self.sparse = sparse\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Dataset.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Abstract method for iterating over samples in the dataset.</p> Source code in <code>spotPython/data/base.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self):\n    \"\"\"Abstract method for iterating over samples in the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.Dataset.take","title":"<code>take(k)</code>","text":"<p>Iterate over the k samples.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of samples to iterate over.</p> required <p>Returns:</p> Type Description <code>islice</code> <p>itertools.islice: An iterator over the first k samples in the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyDataset(Dataset):\n...     def __init__(self):\n...         super().__init__('Regression', 10)\n...     def __iter__(self):\n...         yield from range(10)\n&gt;&gt;&gt; list(MyDataset().take(5))\n[0, 1, 2, 3, 4]\n</code></pre> Source code in <code>spotPython/data/base.py</code> <pre><code>def take(self, k: int) -&gt; itertools.islice:\n    \"\"\"Iterate over the k samples.\n\n    Args:\n        k (int): The number of samples to iterate over.\n\n    Returns:\n        itertools.islice: An iterator over the first k samples in the dataset.\n\n    Examples:\n        &gt;&gt;&gt; class MyDataset(Dataset):\n        ...     def __init__(self):\n        ...         super().__init__('Regression', 10)\n        ...     def __iter__(self):\n        ...         yield from range(10)\n        &gt;&gt;&gt; list(MyDataset().take(5))\n        [0, 1, 2, 3, 4]\n    \"\"\"\n    return itertools.islice(self, k)\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.FileConfig","title":"<code>FileConfig</code>","text":"<p>             Bases: <code>Config</code></p> <p>Base class for configurations that are stored in a local file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The file\u2019s name.</p> required <code>directory</code> <code>Optional[str]</code> <p>The directory where the file is contained. Defaults to the location of the <code>datasets</code> module.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra config parameters to pass as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FileConfig</code> <p>A FileConfig object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n</code></pre> Source code in <code>spotPython/data/base.py</code> <pre><code>class FileConfig(Config):\n    \"\"\"Base class for configurations that are stored in a local file.\n\n    Args:\n        filename (str): The file's name.\n        directory (Optional[str]):\n            The directory where the file is contained.\n            Defaults to the location of the `datasets` module.\n        desc (dict): Extra config parameters to pass as keyword arguments.\n\n    Returns:\n        (FileConfig): A FileConfig object.\n\n    Examples:\n        &gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n    \"\"\"\n\n    def __init__(self, filename: str, directory: Optional[str] = None, **desc):\n        super().__init__(**desc)\n        self.filename = filename\n        self.directory = directory\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"The path to the configuration file.\n\n        Returns:\n            pathlib.Path: The path to the configuration file.\n\n        Examples:\n            &gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; config.path\n            PosixPath('/path/to/directory/config.json')\n        \"\"\"\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The content of the string representation of the FileConfig object.\n\n        Returns:\n            dict: A dictionary containing the content of the string representation of the FileConfig object.\n\n        Examples:\n            &gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; config._repr_content\n            {'Path': '/path/to/directory/config.json'}\n        \"\"\"\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.FileConfig.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>The path to the configuration file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the configuration file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = FileConfig(filename=\"config.json\", directory=\"/path/to/directory\")\n&gt;&gt;&gt; config.path\nPosixPath('/path/to/directory/config.json')\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.FileDataset","title":"<code>FileDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Base class for datasets that are stored in a local file.</p> <p>Small datasets that are part of the spotRiver package inherit from this class.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The file\u2019s name.</p> required <code>directory</code> <code>Optional[str]</code> <p>The directory where the file is contained. Defaults to the location of the <code>datasets</code> module.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra dataset parameters to pass as keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FileDataset</code> <p>A FileDataset object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n</code></pre> Source code in <code>spotPython/data/base.py</code> <pre><code>class FileDataset(Dataset):\n    \"\"\"Base class for datasets that are stored in a local file.\n\n    Small datasets that are part of the spotRiver package inherit from this class.\n\n    Args:\n        filename (str): The file's name.\n        directory (Optional[str]):\n            The directory where the file is contained.\n            Defaults to the location of the `datasets` module.\n        desc (dict): Extra dataset parameters to pass as keyword arguments.\n\n    Returns:\n        (FileDataset): A FileDataset object.\n\n    Examples:\n        &gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n    \"\"\"\n\n    def __init__(self, filename: str, directory: Optional[str] = None, **desc):\n        super().__init__(**desc)\n        self.filename = filename\n        self.directory = directory\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"The path to the dataset file.\n\n        Returns:\n            pathlib.Path: The path to the dataset file.\n\n        Examples:\n            &gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; dataset.path\n            PosixPath('/path/to/directory/dataset.csv')\n        \"\"\"\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The content of the string representation of the FileDataset object.\n\n        Returns:\n            dict: A dictionary containing the content of the string representation of the FileDataset object.\n\n        Examples:\n            &gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n            &gt;&gt;&gt; dataset._repr_content\n            {'Path': '/path/to/directory/dataset.csv'}\n        \"\"\"\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.FileDataset.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>The path to the dataset file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the dataset file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = FileDataset(filename=\"dataset.csv\", directory=\"/path/to/directory\")\n&gt;&gt;&gt; dataset.path\nPosixPath('/path/to/directory/dataset.csv')\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.GenericFileDataset","title":"<code>GenericFileDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Base class for datasets that are stored in a local file.</p> <p>Small datasets that are part of the spotRiver package inherit from this class.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The file\u2019s name.</p> required <code>target</code> <code>str</code> <p>The name of the target variable.</p> required <code>converters</code> <code>dict</code> <p>A dictionary specifying how to convert the columns of the dataset. Defaults to None.</p> <code>None</code> <code>parse_dates</code> <code>list</code> <p>A list of columns to parse as dates. Defaults to None.</p> <code>None</code> <code>directory</code> <code>str</code> <p>The directory where the file is contained. Defaults to the location of the <code>datasets</code> module.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra dataset parameters to pass as keyword arguments.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.datasets import Iris\n&gt;&gt;&gt; dataset = Iris()\n&gt;&gt;&gt; for x, y in dataset:\n...     print(x, y)\n...     break\n({'sepal_length': 5.1,\n  'sepal_width': 3.5,\n  'petal_length': 1.4,\n  'petal_width': 0.2},\n  'setosa')\n</code></pre> Source code in <code>spotPython/data/base.py</code> <pre><code>class GenericFileDataset(Dataset):\n    \"\"\"Base class for datasets that are stored in a local file.\n\n    Small datasets that are part of the spotRiver package inherit from this class.\n\n    Args:\n        filename (str): The file's name.\n        target (str): The name of the target variable.\n        converters (dict):\n            A dictionary specifying how to convert the columns of the dataset. Defaults to None.\n        parse_dates (list): A list of columns to parse as dates. Defaults to None.\n        directory (str):\n            The directory where the file is contained. Defaults to the location of the `datasets` module.\n        desc (dict): Extra dataset parameters to pass as keyword arguments.\n\n\n    Examples:\n\n        &gt;&gt;&gt; from river.datasets import Iris\n        &gt;&gt;&gt; dataset = Iris()\n        &gt;&gt;&gt; for x, y in dataset:\n        ...     print(x, y)\n        ...     break\n        ({'sepal_length': 5.1,\n          'sepal_width': 3.5,\n          'petal_length': 1.4,\n          'petal_width': 0.2},\n          'setosa')\n\n    \"\"\"\n\n    def __init__(\n        self,\n        filename: str,\n        target: str,\n        converters: dict = None,\n        parse_dates: list = None,\n        directory: str = None,\n        **desc: dict,\n    ):\n        super().__init__(**desc)\n        self.filename = filename\n        self.directory = directory\n        self.target = target\n        self.converters = converters\n        self.parse_dates = parse_dates\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"Returns the path where the dataset is stored.\"\"\"\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self):\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.GenericFileDataset.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>Returns the path where the dataset is stored.</p>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.RemoteDataset","title":"<code>RemoteDataset</code>","text":"<p>             Bases: <code>FileDataset</code></p> <p>Base class for datasets that are stored in a remote file.</p> <p>Medium and large datasets that are not part of the river package inherit from this class.</p> <p>The filename doesn\u2019t have to be provided if unpack is False. Indeed in the latter case the filename will be inferred from the URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL the dataset is located at.</p> required <code>size</code> <code>int</code> <p>The expected download size.</p> required <code>unpack</code> <code>bool</code> <p>Whether to unpack the download or not. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>An optional name to given to the file if the file is unpacked. Defaults to None.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra dataset parameters to pass as keyword arguments.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.datasets import AirlinePassengers\n&gt;&gt;&gt; dataset = AirlinePassengers()\n&gt;&gt;&gt; for x, y in dataset:\n...     print(x, y)\n...     break\n({'month': datetime.datetime(1949, 1, 1, 0, 0)}, 112)\n</code></pre> Source code in <code>spotPython/data/base.py</code> <pre><code>class RemoteDataset(FileDataset):\n    \"\"\"Base class for datasets that are stored in a remote file.\n\n    Medium and large datasets that are not part of the river package inherit from this class.\n\n    The filename doesn't have to be provided if unpack is False. Indeed in the latter case the\n    filename will be inferred from the URL.\n\n    Args:\n        url (str): The URL the dataset is located at.\n        size (int): The expected download size.\n        unpack (bool): Whether to unpack the download or not. Defaults to True.\n        filename (str):\n            An optional name to given to the file if the file is unpacked. Defaults to None.\n        desc (dict): Extra dataset parameters to pass as keyword arguments.\n\n    Examples:\n\n        &gt;&gt;&gt; from river.datasets import AirlinePassengers\n        &gt;&gt;&gt; dataset = AirlinePassengers()\n        &gt;&gt;&gt; for x, y in dataset:\n        ...     print(x, y)\n        ...     break\n        ({'month': datetime.datetime(1949, 1, 1, 0, 0)}, 112)\n\n    \"\"\"\n\n    def __init__(self, url: str, size: int, unpack: bool = True, filename: str = None, **desc: dict):\n        if filename is None:\n            filename = path.basename(url)\n\n        super().__init__(filename=filename, **desc)\n        self.url = url\n        self.size = size\n        self.unpack = unpack\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"Returns the path where the dataset is stored.\"\"\"\n        return pathlib.Path(get_data_home(), self.__class__.__name__, self.filename)\n\n    def download(self, force: bool = False, verbose: bool = True) -&gt; None:\n        \"\"\"Downloads the dataset.\n\n        Args:\n            force (bool):\n                Whether to force the download even if the data is already downloaded.\n                Defaults to False.\n            verbose (bool):\n                Whether to display information about the download. Defaults to True.\n\n        \"\"\"\n        if not force and self.is_downloaded:\n            return\n\n        # Determine where to download the archive\n        directory = self.path.parent\n        directory.mkdir(parents=True, exist_ok=True)\n        archive_path = directory.joinpath(path.basename(self.url))\n\n        with request.urlopen(self.url) as r:\n            # Notify the user\n            if verbose:\n                meta = r.info()\n                try:\n                    n_bytes = int(meta[\"Content-Length\"])\n                    msg = f\"Downloading {self.url} ({n_bytes})\"\n                except KeyError:\n                    msg = f\"Downloading {self.url}\"\n                print(msg)\n\n            # Now dump the contents of the requests\n            with open(archive_path, \"wb\") as f:\n                shutil.copyfileobj(r, f)\n\n        if not self.unpack:\n            return\n\n        if verbose:\n            print(f\"Uncompressing into {directory}\")\n\n        if archive_path.suffix.endswith(\"zip\"):\n            with zipfile.ZipFile(archive_path, \"r\") as zf:\n                zf.extractall(directory)\n\n        elif archive_path.suffix.endswith((\"gz\", \"tar\")):\n            mode = \"r:\" if archive_path.suffix.endswith(\"tar\") else \"r:gz\"\n            tar = tarfile.open(archive_path, mode)\n            tar.extractall(directory)\n            tar.close()\n\n        else:\n            raise RuntimeError(f\"Unhandled extension type: {archive_path.suffix}\")\n\n        # Delete the archive file now that it has been uncompressed\n        archive_path.unlink()\n\n    @abc.abstractmethod\n    def _iter(self):\n        pass\n\n    @property\n    def is_downloaded(self) -&gt; bool:\n        \"\"\"Indicate whether or not the data has been correctly downloaded.\"\"\"\n        if self.path.exists():\n            if self.path.is_file():\n                return self.path.stat().st_size == self.size\n            return sum(f.stat().st_size for f in self.path.glob(\"**/*\") if f.is_file())\n\n        return False\n\n    def __iter__(self):\n        \"\"\"Iterates over the samples of a dataset.\"\"\"\n        if not self.is_downloaded:\n            self.download(verbose=True)\n        if not self.is_downloaded:\n            raise RuntimeError(\"Something went wrong during the download\")\n        yield from self._iter()\n\n    @property\n    def _repr_content(self):\n        content = super()._repr_content\n        content[\"URL\"] = self.url\n        content[\"Size\"] = self.size\n        content[\"Downloaded\"] = str(self.is_downloaded)\n        return content\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.RemoteDataset.is_downloaded","title":"<code>is_downloaded: bool</code>  <code>property</code>","text":"<p>Indicate whether or not the data has been correctly downloaded.</p>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.RemoteDataset.path","title":"<code>path: pathlib.Path</code>  <code>property</code>","text":"<p>Returns the path where the dataset is stored.</p>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.RemoteDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over the samples of a dataset.</p> Source code in <code>spotPython/data/base.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterates over the samples of a dataset.\"\"\"\n    if not self.is_downloaded:\n        self.download(verbose=True)\n    if not self.is_downloaded:\n        raise RuntimeError(\"Something went wrong during the download\")\n    yield from self._iter()\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.RemoteDataset.download","title":"<code>download(force=False, verbose=True)</code>","text":"<p>Downloads the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>Whether to force the download even if the data is already downloaded. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to display information about the download. Defaults to True.</p> <code>True</code> Source code in <code>spotPython/data/base.py</code> <pre><code>def download(self, force: bool = False, verbose: bool = True) -&gt; None:\n    \"\"\"Downloads the dataset.\n\n    Args:\n        force (bool):\n            Whether to force the download even if the data is already downloaded.\n            Defaults to False.\n        verbose (bool):\n            Whether to display information about the download. Defaults to True.\n\n    \"\"\"\n    if not force and self.is_downloaded:\n        return\n\n    # Determine where to download the archive\n    directory = self.path.parent\n    directory.mkdir(parents=True, exist_ok=True)\n    archive_path = directory.joinpath(path.basename(self.url))\n\n    with request.urlopen(self.url) as r:\n        # Notify the user\n        if verbose:\n            meta = r.info()\n            try:\n                n_bytes = int(meta[\"Content-Length\"])\n                msg = f\"Downloading {self.url} ({n_bytes})\"\n            except KeyError:\n                msg = f\"Downloading {self.url}\"\n            print(msg)\n\n        # Now dump the contents of the requests\n        with open(archive_path, \"wb\") as f:\n            shutil.copyfileobj(r, f)\n\n    if not self.unpack:\n        return\n\n    if verbose:\n        print(f\"Uncompressing into {directory}\")\n\n    if archive_path.suffix.endswith(\"zip\"):\n        with zipfile.ZipFile(archive_path, \"r\") as zf:\n            zf.extractall(directory)\n\n    elif archive_path.suffix.endswith((\"gz\", \"tar\")):\n        mode = \"r:\" if archive_path.suffix.endswith(\"tar\") else \"r:gz\"\n        tar = tarfile.open(archive_path, mode)\n        tar.extractall(directory)\n        tar.close()\n\n    else:\n        raise RuntimeError(f\"Unhandled extension type: {archive_path.suffix}\")\n\n    # Delete the archive file now that it has been uncompressed\n    archive_path.unlink()\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.SyntheticDataset","title":"<code>SyntheticDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>A synthetic dataset.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Type of task the dataset is meant for. Should be one of: - \u201cRegression\u201d - \u201cBinary classification\u201d - \u201cMulti-class classification\u201d - \u201cMulti-output binary classification\u201d - \u201cMulti-output regression\u201d</p> required <code>n_features</code> <code>int</code> <p>Number of features in the dataset.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes in the dataset, only applies to classification datasets.</p> <code>None</code> <code>n_outputs</code> <code>int</code> <p>Number of outputs the target is made of, only applies to multi-output datasets.</p> <code>None</code> <code>sparse</code> <code>bool</code> <p>Whether the dataset is sparse or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>SyntheticDataset</code> <p>A synthetic dataset object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n&gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                    n_features=4,\n                                    n_samples=100,\n                                    n_classes=2,\n                                    n_outputs=1,\n                                    sparse=False)\n</code></pre> Source code in <code>spotPython/data/base.py</code> <pre><code>class SyntheticDataset(Dataset):\n    \"\"\"A synthetic dataset.\n\n    Args:\n        task (str): Type of task the dataset is meant for. Should be one of:\n            - \"Regression\"\n            - \"Binary classification\"\n            - \"Multi-class classification\"\n            - \"Multi-output binary classification\"\n            - \"Multi-output regression\"\n        n_features (int): Number of features in the dataset.\n        n_samples (int): Number of samples in the dataset.\n        n_classes (int): Number of classes in the dataset, only applies to classification datasets.\n        n_outputs (int): Number of outputs the target is made of, only applies to multi-output datasets.\n        sparse (bool): Whether the dataset is sparse or not.\n\n    Returns:\n        (SyntheticDataset): A synthetic dataset object.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import make_classification\n        &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n        &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                            n_features=4,\n                                            n_samples=100,\n                                            n_classes=2,\n                                            n_outputs=1,\n                                            sparse=False)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        n_features: int,\n        n_samples: int,\n        n_classes: Union[int, None] = None,\n        n_outputs: Union[int, None] = None,\n        sparse: bool = False,\n    ):\n        pass\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the SyntheticDataset object.\n\n        Returns:\n            str: A string representation of the SyntheticDataset object.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.datasets import make_classification\n            &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n            &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                                n_features=4,\n                                                n_samples=100,\n                                                n_classes=2,\n                                                n_outputs=1,\n                                                sparse=False)\n            &gt;&gt;&gt; print(dataset)\n            Synthetic data generator\n\n            Configuration\n            -------------\n                task  Binary classification\n          n_features  4\n           n_samples  100\n           n_classes  2\n           n_outputs  1\n              sparse  False\n        \"\"\"\n        l_len_prop = max(map(len, self._repr_content.keys()))\n        r_len_prop = max(map(len, self._repr_content.values()))\n        params = self._get_params()\n        l_len_config = max(map(len, params.keys()))\n        r_len_config = max(map(len, map(str, params.values())))\n\n        out = (\n            \"Synthetic data generator\\n\\n\"\n            + \"\\n\".join(k.rjust(l_len_prop) + \"  \" + v.ljust(r_len_prop) for k, v in self._repr_content.items())\n            + \"\\n\\nConfiguration\\n-------------\\n\"\n            + \"\\n\".join(k.rjust(l_len_config) + \"  \" + str(v).ljust(r_len_config) for k, v in params.items())\n        )\n\n        return out\n\n    def _get_params(self) -&gt; typing.Dict[str, typing.Any]:\n        \"\"\"Return the parameters that were used during initialization.\n\n        Returns:\n            dict: A dictionary containing the parameters that were used during initialization.\n\n        Examples:\n            &gt;&gt;&gt; from sklearn.datasets import make_classification\n            &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n            &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                            n_features=4,\n                                            n_samples=100,\n                                            n_classes=2,\n                                            n_outputs=1,\n                                            sparse=False)\n            &gt;&gt;&gt; dataset._get_params()\n            {'task': 'Binary classification',\n             'n_features': 4,\n             'n_samples': 100,\n             'n_classes': 2,\n             'n_outputs': 1,\n             'sparse': False}\n        \"\"\"\n        return {\n            name: getattr(self, name)\n            for name, param in inspect.signature(self.__init__).parameters.items()  # type: ignore\n            if param.kind != param.VAR_KEYWORD\n        }\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.SyntheticDataset.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the SyntheticDataset object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the SyntheticDataset object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n&gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                    n_features=4,\n                                    n_samples=100,\n                                    n_classes=2,\n                                    n_outputs=1,\n                                    sparse=False)\n&gt;&gt;&gt; print(dataset)\nSynthetic data generator\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.SyntheticDataset.__repr__--configuration","title":"Configuration","text":"<pre><code>task  Binary classification\n</code></pre> <p>n_features  4    n_samples  100    n_classes  2    n_outputs  1       sparse  False</p> Source code in <code>spotPython/data/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the SyntheticDataset object.\n\n    Returns:\n        str: A string representation of the SyntheticDataset object.\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import make_classification\n        &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)\n        &gt;&gt;&gt; dataset = SyntheticDataset(task=\"Binary classification\",\n                                            n_features=4,\n                                            n_samples=100,\n                                            n_classes=2,\n                                            n_outputs=1,\n                                            sparse=False)\n        &gt;&gt;&gt; print(dataset)\n        Synthetic data generator\n\n        Configuration\n        -------------\n            task  Binary classification\n      n_features  4\n       n_samples  100\n       n_classes  2\n       n_outputs  1\n          sparse  False\n    \"\"\"\n    l_len_prop = max(map(len, self._repr_content.keys()))\n    r_len_prop = max(map(len, self._repr_content.values()))\n    params = self._get_params()\n    l_len_config = max(map(len, params.keys()))\n    r_len_config = max(map(len, map(str, params.values())))\n\n    out = (\n        \"Synthetic data generator\\n\\n\"\n        + \"\\n\".join(k.rjust(l_len_prop) + \"  \" + v.ljust(r_len_prop) for k, v in self._repr_content.items())\n        + \"\\n\\nConfiguration\\n-------------\\n\"\n        + \"\\n\".join(k.rjust(l_len_config) + \"  \" + str(v).ljust(r_len_config) for k, v in params.items())\n    )\n\n    return out\n</code></pre>"},{"location":"reference/spotPython/data/base/#spotPython.data.base.get_data_home","title":"<code>get_data_home(data_home=None)</code>","text":"<p>Return the location where remote datasets are to be stored.</p> <p>By default the data directory is set to a folder named \u2018spotriver_data\u2019 in the user home folder. Alternatively, it can be set by the \u2018SPOTRIVER_DATA\u2019 environment variable or programmatically by giving an explicit folder path. The \u2018~\u2019 symbol is expanded to the user home folder. If the folder does not already exist, it is automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>data_home</code> <code>str or Path</code> <p>The path to spotriver data directory. If <code>None</code>, the default path is <code>~/spotriver_data</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data_home</code> <code>Path</code> <p>The path to the spotriver data directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; get_data_home()\nPosixPath('/home/user/spotriver_data')\n&gt;&gt;&gt; get_data_home(Path('/tmp/spotriver_data'))\nPosixPath('/tmp/spotriver_data')\n</code></pre> Source code in <code>spotPython/data/base.py</code> <pre><code>def get_data_home(data_home: Optional[Union[str, Path]] = None) -&gt; Path:\n    \"\"\"Return the location where remote datasets are to be stored.\n\n    By default the data directory is set to a folder named 'spotriver_data' in the\n    user home folder. Alternatively, it can be set by the 'SPOTRIVER_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n\n    Args:\n        data_home (str or pathlib.Path, optional):\n            The path to spotriver data directory. If `None`, the default path\n            is `~/spotriver_data`.\n\n    Returns:\n        data_home (pathlib.Path):\n            The path to the spotriver data directory.\n\n    Examples:\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; get_data_home()\n        PosixPath('/home/user/spotriver_data')\n        &gt;&gt;&gt; get_data_home(Path('/tmp/spotriver_data'))\n        PosixPath('/tmp/spotriver_data')\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get(\"SPOTRIVER_DATA\", Path.home() / \"spotriver_data\")\n    # Ensure data_home is a Path() object pointing to an absolute path\n    data_home = Path(data_home).absolute()\n    # Create data directory if it does not exists.\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n</code></pre>"},{"location":"reference/spotPython/data/light_hyper_dict/","title":"light_hyper_dict","text":""},{"location":"reference/spotPython/data/light_hyper_dict/#spotPython.data.light_hyper_dict.LightHyperDict","title":"<code>LightHyperDict</code>","text":"<p>             Bases: <code>FileConfig</code></p> <p>Lightning hyperparameter dictionary.</p> <p>This class extends the FileConfig class to provide a dictionary for storing hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The name of the file where the hyperparameters are stored.</p> Source code in <code>spotPython/data/light_hyper_dict.py</code> <pre><code>class LightHyperDict(base.FileConfig):\n    \"\"\"Lightning hyperparameter dictionary.\n\n    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n\n    Attributes:\n        filename (str):\n            The name of the file where the hyperparameters are stored.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the LightHyperDict object.\n\n        Examples:\n            &gt;&gt;&gt; lhd = LightHyperDict()\n        \"\"\"\n        super().__init__(\n            filename=\"light_hyper_dict.json\",\n        )\n\n    def load(self) -&gt; dict:\n        \"\"\"Load the hyperparameters from the file.\n\n        Returns:\n            dict: A dictionary containing the hyperparameters.\n\n        Examples:\n            &gt;&gt;&gt; lhd = LightHyperDict()\n            &gt;&gt;&gt; hyperparams = lhd.load()\n            &gt;&gt;&gt; print(hyperparams)\n            {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n        \"\"\"\n        with open(self.path, \"r\") as f:\n            d = json.load(f)\n        return d\n</code></pre>"},{"location":"reference/spotPython/data/light_hyper_dict/#spotPython.data.light_hyper_dict.LightHyperDict.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the LightHyperDict object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lhd = LightHyperDict()\n</code></pre> Source code in <code>spotPython/data/light_hyper_dict.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the LightHyperDict object.\n\n    Examples:\n        &gt;&gt;&gt; lhd = LightHyperDict()\n    \"\"\"\n    super().__init__(\n        filename=\"light_hyper_dict.json\",\n    )\n</code></pre>"},{"location":"reference/spotPython/data/light_hyper_dict/#spotPython.data.light_hyper_dict.LightHyperDict.load","title":"<code>load()</code>","text":"<p>Load the hyperparameters from the file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lhd = LightHyperDict()\n&gt;&gt;&gt; hyperparams = lhd.load()\n&gt;&gt;&gt; print(hyperparams)\n{'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n</code></pre> Source code in <code>spotPython/data/light_hyper_dict.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"Load the hyperparameters from the file.\n\n    Returns:\n        dict: A dictionary containing the hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; lhd = LightHyperDict()\n        &gt;&gt;&gt; hyperparams = lhd.load()\n        &gt;&gt;&gt; print(hyperparams)\n        {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n    \"\"\"\n    with open(self.path, \"r\") as f:\n        d = json.load(f)\n    return d\n</code></pre>"},{"location":"reference/spotPython/data/lightning_hyper_dict/","title":"lightning_hyper_dict","text":""},{"location":"reference/spotPython/data/lightning_hyper_dict/#spotPython.data.lightning_hyper_dict.LightningHyperDict","title":"<code>LightningHyperDict</code>","text":"<p>             Bases: <code>FileConfig</code></p> <p>Lightning hyperparameter dictionary.</p> <p>This class extends the FileConfig class to provide a dictionary for storing hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The name of the file where the hyperparameters are stored.</p> Source code in <code>spotPython/data/lightning_hyper_dict.py</code> <pre><code>class LightningHyperDict(base.FileConfig):\n    \"\"\"Lightning hyperparameter dictionary.\n\n    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n\n    Attributes:\n        filename (str):\n            The name of the file where the hyperparameters are stored.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the LightHyperDict object.\n\n        Examples:\n            &gt;&gt;&gt; lhd = LightHyperDict()\n        \"\"\"\n        super().__init__(\n            filename=\"lightning_hyper_dict.json\",\n        )\n\n    def load(self) -&gt; dict:\n        \"\"\"Load the hyperparameters from the file.\n\n        Returns:\n            dict: A dictionary containing the hyperparameters.\n\n        Examples:\n            &gt;&gt;&gt; lhd = LightHyperDict()\n            &gt;&gt;&gt; hyperparams = lhd.load()\n            &gt;&gt;&gt; print(hyperparams)\n            {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n        \"\"\"\n        with open(self.path, \"r\") as f:\n            d = json.load(f)\n        return d\n</code></pre>"},{"location":"reference/spotPython/data/lightning_hyper_dict/#spotPython.data.lightning_hyper_dict.LightningHyperDict.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the LightHyperDict object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lhd = LightHyperDict()\n</code></pre> Source code in <code>spotPython/data/lightning_hyper_dict.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the LightHyperDict object.\n\n    Examples:\n        &gt;&gt;&gt; lhd = LightHyperDict()\n    \"\"\"\n    super().__init__(\n        filename=\"lightning_hyper_dict.json\",\n    )\n</code></pre>"},{"location":"reference/spotPython/data/lightning_hyper_dict/#spotPython.data.lightning_hyper_dict.LightningHyperDict.load","title":"<code>load()</code>","text":"<p>Load the hyperparameters from the file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lhd = LightHyperDict()\n&gt;&gt;&gt; hyperparams = lhd.load()\n&gt;&gt;&gt; print(hyperparams)\n{'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n</code></pre> Source code in <code>spotPython/data/lightning_hyper_dict.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"Load the hyperparameters from the file.\n\n    Returns:\n        dict: A dictionary containing the hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; lhd = LightHyperDict()\n        &gt;&gt;&gt; hyperparams = lhd.load()\n        &gt;&gt;&gt; print(hyperparams)\n        {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n    \"\"\"\n    with open(self.path, \"r\") as f:\n        d = json.load(f)\n    return d\n</code></pre>"},{"location":"reference/spotPython/data/sklearn_hyper_dict/","title":"sklearn_hyper_dict","text":""},{"location":"reference/spotPython/data/sklearn_hyper_dict/#spotPython.data.sklearn_hyper_dict.SklearnHyperDict","title":"<code>SklearnHyperDict</code>","text":"<p>             Bases: <code>FileConfig</code></p> <p>Scikit-learn hyperparameter dictionary.</p> <p>This class extends the FileConfig class to provide a dictionary for storing hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The name of the file where the hyperparameters are stored.</p> Source code in <code>spotPython/data/sklearn_hyper_dict.py</code> <pre><code>class SklearnHyperDict(base.FileConfig):\n    \"\"\"Scikit-learn hyperparameter dictionary.\n\n    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n\n    Attributes:\n        filename (str): The name of the file where the hyperparameters are stored.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the SklearnHyperDict object.\n\n        Examples:\n            &gt;&gt;&gt; shd = SklearnHyperDict()\n        \"\"\"\n        super().__init__(\n            filename=\"sklearn_hyper_dict.json\",\n        )\n\n    def load(self) -&gt; dict:\n        \"\"\"Load the hyperparameters from the file.\n\n        Returns:\n            (dict): A dictionary containing the hyperparameters.\n        Examples:\n            &gt;&gt;&gt; shd = SklearnHyperDict()\n            &gt;&gt;&gt; hyperparams = shd.load()\n            &gt;&gt;&gt; print(hyperparams)\n            {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n        \"\"\"\n        with open(self.path, \"r\") as f:\n            d = json.load(f)\n        return d\n</code></pre>"},{"location":"reference/spotPython/data/sklearn_hyper_dict/#spotPython.data.sklearn_hyper_dict.SklearnHyperDict.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the SklearnHyperDict object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; shd = SklearnHyperDict()\n</code></pre> Source code in <code>spotPython/data/sklearn_hyper_dict.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the SklearnHyperDict object.\n\n    Examples:\n        &gt;&gt;&gt; shd = SklearnHyperDict()\n    \"\"\"\n    super().__init__(\n        filename=\"sklearn_hyper_dict.json\",\n    )\n</code></pre>"},{"location":"reference/spotPython/data/sklearn_hyper_dict/#spotPython.data.sklearn_hyper_dict.SklearnHyperDict.load","title":"<code>load()</code>","text":"<p>Load the hyperparameters from the file.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the hyperparameters.</p> <p>Examples:     &gt;&gt;&gt; shd = SklearnHyperDict()     &gt;&gt;&gt; hyperparams = shd.load()     &gt;&gt;&gt; print(hyperparams)     {\u2018learning_rate\u2019: 0.001, \u2018batch_size\u2019: 32, \u2018epochs\u2019: 10}</p> Source code in <code>spotPython/data/sklearn_hyper_dict.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"Load the hyperparameters from the file.\n\n    Returns:\n        (dict): A dictionary containing the hyperparameters.\n    Examples:\n        &gt;&gt;&gt; shd = SklearnHyperDict()\n        &gt;&gt;&gt; hyperparams = shd.load()\n        &gt;&gt;&gt; print(hyperparams)\n        {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n    \"\"\"\n    with open(self.path, \"r\") as f:\n        d = json.load(f)\n    return d\n</code></pre>"},{"location":"reference/spotPython/data/torch_hyper_dict/","title":"torch_hyper_dict","text":""},{"location":"reference/spotPython/data/torch_hyper_dict/#spotPython.data.torch_hyper_dict.TorchHyperDict","title":"<code>TorchHyperDict</code>","text":"<p>             Bases: <code>FileConfig</code></p> <p>PyTorch hyperparameter dictionary.</p> <p>This class extends the FileConfig class to provide a dictionary for storing hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str</code> <p>The name of the file where the hyperparameters are stored.</p> Source code in <code>spotPython/data/torch_hyper_dict.py</code> <pre><code>class TorchHyperDict(base.FileConfig):\n    \"\"\"PyTorch hyperparameter dictionary.\n\n    This class extends the FileConfig class to provide a dictionary for storing hyperparameters.\n\n    Attributes:\n        filename (str): The name of the file where the hyperparameters are stored.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the TorchHyperDict object.\n        Examples:\n            &gt;&gt;&gt; thd = TorchHyperDict()\n        \"\"\"\n        super().__init__(\n            filename=\"torch_hyper_dict.json\",\n        )\n\n    def load(self) -&gt; dict:\n        \"\"\"Load the hyperparameters from the file.\n\n        Returns:\n            (dict): A dictionary containing the hyperparameters.\n        Examples:\n            &gt;&gt;&gt; thd = TorchHyperDict()\n            &gt;&gt;&gt; hyperparams = thd.load()\n            &gt;&gt;&gt; print(hyperparams)\n            {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n        \"\"\"\n        with open(self.path, \"r\") as f:\n            d = json.load(f)\n        return d\n</code></pre>"},{"location":"reference/spotPython/data/torch_hyper_dict/#spotPython.data.torch_hyper_dict.TorchHyperDict.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the TorchHyperDict object. Examples:     &gt;&gt;&gt; thd = TorchHyperDict()</p> Source code in <code>spotPython/data/torch_hyper_dict.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the TorchHyperDict object.\n    Examples:\n        &gt;&gt;&gt; thd = TorchHyperDict()\n    \"\"\"\n    super().__init__(\n        filename=\"torch_hyper_dict.json\",\n    )\n</code></pre>"},{"location":"reference/spotPython/data/torch_hyper_dict/#spotPython.data.torch_hyper_dict.TorchHyperDict.load","title":"<code>load()</code>","text":"<p>Load the hyperparameters from the file.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the hyperparameters.</p> <p>Examples:     &gt;&gt;&gt; thd = TorchHyperDict()     &gt;&gt;&gt; hyperparams = thd.load()     &gt;&gt;&gt; print(hyperparams)     {\u2018learning_rate\u2019: 0.001, \u2018batch_size\u2019: 32, \u2018epochs\u2019: 10}</p> Source code in <code>spotPython/data/torch_hyper_dict.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"Load the hyperparameters from the file.\n\n    Returns:\n        (dict): A dictionary containing the hyperparameters.\n    Examples:\n        &gt;&gt;&gt; thd = TorchHyperDict()\n        &gt;&gt;&gt; hyperparams = thd.load()\n        &gt;&gt;&gt; print(hyperparams)\n        {'learning_rate': 0.001, 'batch_size': 32, 'epochs': 10}\n    \"\"\"\n    with open(self.path, \"r\") as f:\n        d = json.load(f)\n    return d\n</code></pre>"},{"location":"reference/spotPython/data/torchdata/","title":"torchdata","text":""},{"location":"reference/spotPython/data/torchdata/#spotPython.data.torchdata.load_data_cifar10","title":"<code>load_data_cifar10(data_dir='./data')</code>","text":"<p>Load the CIFAR-10 dataset.     This function loads the CIFAR-10 dataset using the torchvision library.     The data is split into a training set and a test set.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>The directory where the data is stored. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <p>Returns:</p> Type Description <code>Tuple[CIFAR10, CIFAR10]</code> <p>Tuple[datasets.CIFAR10, datasets.CIFAR10]: A tuple containing the training set and the test set.</p> <p>Examples:     &gt;&gt;&gt; trainset, testset = load_data_cifar10()     &gt;&gt;&gt; print(f\u201dTraining set size: {len(trainset)}\u201d)     Training set size: 50000     &gt;&gt;&gt; print(f\u201dTest set size: {len(testset)}\u201d)     Test set size: 10000</p> Source code in <code>spotPython/data/torchdata.py</code> <pre><code>def load_data_cifar10(data_dir: str = \"./data\") -&gt; Tuple[datasets.CIFAR10, datasets.CIFAR10]:\n    \"\"\"Load the CIFAR-10 dataset.\n        This function loads the CIFAR-10 dataset using the torchvision library.\n        The data is split into a training set and a test set.\n\n    Args:\n        data_dir (str):\n            The directory where the data is stored. Defaults to \"./data\".\n\n    Returns:\n        Tuple[datasets.CIFAR10, datasets.CIFAR10]:\n            A tuple containing the training set and the test set.\n    Examples:\n        &gt;&gt;&gt; trainset, testset = load_data_cifar10()\n        &gt;&gt;&gt; print(f\"Training set size: {len(trainset)}\")\n        Training set size: 50000\n        &gt;&gt;&gt; print(f\"Test set size: {len(testset)}\")\n        Test set size: 10000\n\n    \"\"\"\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n    trainset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n\n    testset = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n\n    return trainset, testset\n</code></pre>"},{"location":"reference/spotPython/data/vbdp/","title":"vbdp","text":""},{"location":"reference/spotPython/data/vbdp/#spotPython.data.vbdp.affinity_propagation_features","title":"<code>affinity_propagation_features(X)</code>","text":"<p>Clusters the features of a dataframe using Affinity Propagation.</p> <p>This function takes a dataframe with features and clusters them using the Affinity Propagation algorithm. The resulting dataframe contains the original features as well as a new feature representing the cluster labels.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>A dataframe with features.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the original features and a new cluster feature.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n&gt;&gt;&gt; df\n    a      b      c\n0  True   True   False\n1  False  True   False\n2  True   False  True\n&gt;&gt;&gt; affinity_propagation_features(df)\nEstimated number of clusters: 3\n    a      b      c  cluster\n0  True   True   False       0\n1  False  True   False       1\n2  True   False  True        2\n</code></pre> Source code in <code>spotPython/data/vbdp.py</code> <pre><code>def affinity_propagation_features(X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Clusters the features of a dataframe using Affinity Propagation.\n\n    This function takes a dataframe with features and clusters them using the\n    Affinity Propagation algorithm. The resulting dataframe contains the original\n    features as well as a new feature representing the cluster labels.\n\n    Args:\n        X (pd.DataFrame):\n            A dataframe with features.\n\n    Returns:\n        (pd.DataFrame):\n            A dataframe with the original features and a new cluster feature.\n\n    Examples:\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n        &gt;&gt;&gt; df\n            a      b      c\n        0  True   True   False\n        1  False  True   False\n        2  True   False  True\n        &gt;&gt;&gt; affinity_propagation_features(df)\n        Estimated number of clusters: 3\n            a      b      c  cluster\n        0  True   True   False       0\n        1  False  True   False       1\n        2  True   False  True        2\n    \"\"\"\n    D = manhattan_distances(X)\n    af = AffinityPropagation(random_state=0, affinity=\"precomputed\").fit(D)\n    cluster_centers_indices = af.cluster_centers_indices_\n    n_clusters_ = len(cluster_centers_indices)\n    print(\"Estimated number of clusters: %d\" % n_clusters_)\n    X[\"cluster\"] = af.labels_\n    return X\n</code></pre>"},{"location":"reference/spotPython/data/vbdp/#spotPython.data.vbdp.cluster_features","title":"<code>cluster_features(X)</code>","text":"<p>Clusters the features of a dataframe based on similarity.</p> <p>This function takes a dataframe with features and clusters them based on similarity. The resulting dataframe contains the original features as well as new features representing the clusters.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>A dataframe with features.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the original features and new cluster features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n&gt;&gt;&gt; df\n    a      b      c\n0  True   True  False\n1 False   True  False\n2  True  False   True\n&gt;&gt;&gt; cluster_features(df)\n    a      b      c  c_0  c_1  c_2  c_3\n0  True   True  False    0    0    0    0\n1 False   True  False    0    0    0    0\n2  True  False   True    0    0    0    0\n</code></pre> Source code in <code>spotPython/data/vbdp.py</code> <pre><code>def cluster_features(X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Clusters the features of a dataframe based on similarity.\n\n    This function takes a dataframe with features and clusters them based on similarity.\n    The resulting dataframe contains the original features as well as new features representing the clusters.\n\n    Args:\n        X (pd.DataFrame): A dataframe with features.\n\n    Returns:\n        (pd.DataFrame): A dataframe with the original features and new cluster features.\n\n    Examples:\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n        &gt;&gt;&gt; df\n            a      b      c\n        0  True   True  False\n        1 False   True  False\n        2  True  False   True\n        &gt;&gt;&gt; cluster_features(df)\n            a      b      c  c_0  c_1  c_2  c_3\n        0  True   True  False    0    0    0    0\n        1 False   True  False    0    0    0    0\n        2  True  False   True    0    0    0    0\n    \"\"\"\n    c_0 = X.columns[X.columns.str.contains(\"pain\")]\n    c_1 = X.columns[X.columns.str.contains(\"inflammation\")]\n    c_2 = X.columns[X.columns.str.contains(\"bleed\")]\n    c_3 = X.columns[X.columns.str.contains(\"skin\")]\n    X[\"c_0\"] = X[c_0].sum(axis=1)\n    X[\"c_1\"] = X[c_1].sum(axis=1)\n    X[\"c_2\"] = X[c_2].sum(axis=1)\n    X[\"c_3\"] = X[c_3].sum(axis=1)\n    return X\n</code></pre>"},{"location":"reference/spotPython/design/designs/","title":"designs","text":""},{"location":"reference/spotPython/design/designs/#spotPython.design.designs.designs","title":"<code>designs</code>","text":"<p>Super class for all design classes (factorial and spacefilling).</p> <p>Attributes:</p> Name Type Description <code>designs</code> <code>List</code> <p>A list of designs.</p> <code>k</code> <code>int</code> <p>The dimension of the design.</p> <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> <code>rng</code> <code>Generator</code> <p>A random number generator instance.</p> Source code in <code>spotPython/design/designs.py</code> <pre><code>class designs:\n    \"\"\"\n    Super class for all design classes (factorial and spacefilling).\n\n    Attributes:\n        designs (List):\n            A list of designs.\n        k (int):\n            The dimension of the design.\n        seed (int):\n            The seed for the random number generator.\n        rng (Generator):\n            A random number generator instance.\n    \"\"\"\n\n    def __init__(self, k: int = 2, seed: int = 123) -&gt; None:\n        \"\"\"\n        Initializes a Designs object with the given dimension and seed.\n\n        Args:\n            k (int):\n                The dimension of the design. Defaults to 2.\n            seed (int):\n                The seed for the random number generator. Defaults to 123.\n        Examples:\n            &gt;&gt;&gt; designs = designs(k=2, seed=123)\n            &gt;&gt;&gt; designs.get_dim()\n            2\n\n        \"\"\"\n        self.designs: List = []\n        self.k: int = k\n        self.seed: int = seed\n        self.rng = default_rng(self.seed)\n\n    def get_dim(self) -&gt; int:\n        \"\"\"\n        Returns the dimension of the design.\n\n        Returns:\n            int: The dimension of the design.\n        \"\"\"\n        return self.k\n</code></pre>"},{"location":"reference/spotPython/design/designs/#spotPython.design.designs.designs.__init__","title":"<code>__init__(k=2, seed=123)</code>","text":"<p>Initializes a Designs object with the given dimension and seed.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The dimension of the design. Defaults to 2.</p> <code>2</code> <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 123.</p> <code>123</code> <p>Examples:     &gt;&gt;&gt; designs = designs(k=2, seed=123)     &gt;&gt;&gt; designs.get_dim()     2</p> Source code in <code>spotPython/design/designs.py</code> <pre><code>def __init__(self, k: int = 2, seed: int = 123) -&gt; None:\n    \"\"\"\n    Initializes a Designs object with the given dimension and seed.\n\n    Args:\n        k (int):\n            The dimension of the design. Defaults to 2.\n        seed (int):\n            The seed for the random number generator. Defaults to 123.\n    Examples:\n        &gt;&gt;&gt; designs = designs(k=2, seed=123)\n        &gt;&gt;&gt; designs.get_dim()\n        2\n\n    \"\"\"\n    self.designs: List = []\n    self.k: int = k\n    self.seed: int = seed\n    self.rng = default_rng(self.seed)\n</code></pre>"},{"location":"reference/spotPython/design/designs/#spotPython.design.designs.designs.get_dim","title":"<code>get_dim()</code>","text":"<p>Returns the dimension of the design.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The dimension of the design.</p> Source code in <code>spotPython/design/designs.py</code> <pre><code>def get_dim(self) -&gt; int:\n    \"\"\"\n    Returns the dimension of the design.\n\n    Returns:\n        int: The dimension of the design.\n    \"\"\"\n    return self.k\n</code></pre>"},{"location":"reference/spotPython/design/factorial/","title":"factorial","text":""},{"location":"reference/spotPython/design/factorial/#spotPython.design.factorial.factorial","title":"<code>factorial</code>","text":"<p>             Bases: <code>designs</code></p> <p>Super class for factorial designs.</p> <p>Attributes:</p> Name Type Description <code>k</code> <code>int</code> <p>The number of factors.</p> <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> Source code in <code>spotPython/design/factorial.py</code> <pre><code>class factorial(designs):\n    \"\"\"\n    Super class for factorial designs.\n\n    Attributes:\n        k (int): The number of factors.\n        seed (int): The seed for the random number generator.\n    \"\"\"\n\n    def __init__(self, k: int = 2, seed: int = 123) -&gt; None:\n        \"\"\"\n        Initializes a factorial design object.\n\n        Args:\n            k (int): The number of factors. Defaults to 2.\n            seed (int): The seed for the random number generator. Defaults to 123.\n        \"\"\"\n        super().__init__(k, seed)\n\n    def full_factorial(self, p: int) -&gt; \"np.ndarray\":\n        \"\"\"\n        Generates a full factorial design.\n\n        Args:\n            p (int): The number of levels for each factor.\n\n        Returns:\n            numpy.ndarray: A 2D array representing the full factorial design.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.design.factorial import factorial\n                factorial_design = factorial(k=2)\n                factorial_design.full_factorial(p=2)\n                array([[0., 0.],\n                    [0., 1.],\n                    [1., 0.],\n                    [1., 1.]])\n        \"\"\"\n        i = (slice(0, 1, p * 1j),) * self.k\n        return mgrid[i].reshape(self.k, p**self.k).T\n</code></pre>"},{"location":"reference/spotPython/design/factorial/#spotPython.design.factorial.factorial.__init__","title":"<code>__init__(k=2, seed=123)</code>","text":"<p>Initializes a factorial design object.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of factors. Defaults to 2.</p> <code>2</code> <code>seed</code> <code>int</code> <p>The seed for the random number generator. Defaults to 123.</p> <code>123</code> Source code in <code>spotPython/design/factorial.py</code> <pre><code>def __init__(self, k: int = 2, seed: int = 123) -&gt; None:\n    \"\"\"\n    Initializes a factorial design object.\n\n    Args:\n        k (int): The number of factors. Defaults to 2.\n        seed (int): The seed for the random number generator. Defaults to 123.\n    \"\"\"\n    super().__init__(k, seed)\n</code></pre>"},{"location":"reference/spotPython/design/factorial/#spotPython.design.factorial.factorial.full_factorial","title":"<code>full_factorial(p)</code>","text":"<p>Generates a full factorial design.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>int</code> <p>The number of levels for each factor.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: A 2D array representing the full factorial design.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.design.factorial import factorial\n    factorial_design = factorial(k=2)\n    factorial_design.full_factorial(p=2)\n    array([[0., 0.],\n        [0., 1.],\n        [1., 0.],\n        [1., 1.]])\n</code></pre> Source code in <code>spotPython/design/factorial.py</code> <pre><code>def full_factorial(self, p: int) -&gt; \"np.ndarray\":\n    \"\"\"\n    Generates a full factorial design.\n\n    Args:\n        p (int): The number of levels for each factor.\n\n    Returns:\n        numpy.ndarray: A 2D array representing the full factorial design.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.design.factorial import factorial\n            factorial_design = factorial(k=2)\n            factorial_design.full_factorial(p=2)\n            array([[0., 0.],\n                [0., 1.],\n                [1., 0.],\n                [1., 1.]])\n    \"\"\"\n    i = (slice(0, 1, p * 1j),) * self.k\n    return mgrid[i].reshape(self.k, p**self.k).T\n</code></pre>"},{"location":"reference/spotPython/design/spacefilling/","title":"spacefilling","text":""},{"location":"reference/spotPython/design/spacefilling/#spotPython.design.spacefilling.spacefilling","title":"<code>spacefilling</code>","text":"<p>             Bases: <code>designs</code></p> Source code in <code>spotPython/design/spacefilling.py</code> <pre><code>class spacefilling(designs):\n    def __init__(self, k: int = 2, seed: int = 123) -&gt; None:\n        \"\"\"\n        Spacefilling design class\n\n        Args:\n            k (int, optional): number of design variables (dimensions). Defaults to 2.\n            seed (int, optional): random seed. Defaults to 123.\n        \"\"\"\n        self.k = k\n        self.seed = seed\n        super().__init__(k, seed)\n        self.sampler = LatinHypercube(d=self.k, seed=self.seed)\n\n    def scipy_lhd(\n        self,\n        n: int,\n        repeats: int = 1,\n        lower: Optional[Union[int, float]] = None,\n        upper: Optional[Union[int, float]] = None,\n    ) -&gt; ndarray:\n        \"\"\"\n        Latin hypercube sampling based on scipy.\n\n        Args:\n            n (int): number of samples\n            repeats (int): number of repeats (replicates)\n            lower (int or float, optional): lower bound. Defaults to 0.\n            upper (int or float, optional): upper bound. Defaults to 1.\n\n        Returns:\n            (ndarray): Latin hypercube design.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.design.spacefilling import spacefilling\n                import numpy as np\n                lhd = spacefilling(k=2, seed=123)\n                lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0,0]), upper=np.array([1,1]))\n                array([[0.66352963, 0.5892358 ],\n                    [0.66352963, 0.5892358 ],\n                    [0.55592803, 0.96312564],\n                    [0.55592803, 0.96312564],\n                    [0.16481882, 0.0375811 ],\n                    [0.16481882, 0.0375811 ],\n                    [0.215331  , 0.34468512],\n                    [0.215331  , 0.34468512],\n                    [0.83604909, 0.62202146],\n                    [0.83604909, 0.62202146]])\n        \"\"\"\n        if lower is None:\n            lower = zeros(self.k)\n        if upper is None:\n            upper = ones(self.k)\n        sample = self.sampler.random(n=n)\n        des = scale(sample, lower, upper)\n        return repeat(des, repeats, axis=0)\n</code></pre>"},{"location":"reference/spotPython/design/spacefilling/#spotPython.design.spacefilling.spacefilling.__init__","title":"<code>__init__(k=2, seed=123)</code>","text":"<p>Spacefilling design class</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>number of design variables (dimensions). Defaults to 2.</p> <code>2</code> <code>seed</code> <code>int</code> <p>random seed. Defaults to 123.</p> <code>123</code> Source code in <code>spotPython/design/spacefilling.py</code> <pre><code>def __init__(self, k: int = 2, seed: int = 123) -&gt; None:\n    \"\"\"\n    Spacefilling design class\n\n    Args:\n        k (int, optional): number of design variables (dimensions). Defaults to 2.\n        seed (int, optional): random seed. Defaults to 123.\n    \"\"\"\n    self.k = k\n    self.seed = seed\n    super().__init__(k, seed)\n    self.sampler = LatinHypercube(d=self.k, seed=self.seed)\n</code></pre>"},{"location":"reference/spotPython/design/spacefilling/#spotPython.design.spacefilling.spacefilling.scipy_lhd","title":"<code>scipy_lhd(n, repeats=1, lower=None, upper=None)</code>","text":"<p>Latin hypercube sampling based on scipy.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>number of samples</p> required <code>repeats</code> <code>int</code> <p>number of repeats (replicates)</p> <code>1</code> <code>lower</code> <code>int or float</code> <p>lower bound. Defaults to 0.</p> <code>None</code> <code>upper</code> <code>int or float</code> <p>upper bound. Defaults to 1.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Latin hypercube design.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.design.spacefilling import spacefilling\n    import numpy as np\n    lhd = spacefilling(k=2, seed=123)\n    lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0,0]), upper=np.array([1,1]))\n    array([[0.66352963, 0.5892358 ],\n        [0.66352963, 0.5892358 ],\n        [0.55592803, 0.96312564],\n        [0.55592803, 0.96312564],\n        [0.16481882, 0.0375811 ],\n        [0.16481882, 0.0375811 ],\n        [0.215331  , 0.34468512],\n        [0.215331  , 0.34468512],\n        [0.83604909, 0.62202146],\n        [0.83604909, 0.62202146]])\n</code></pre> Source code in <code>spotPython/design/spacefilling.py</code> <pre><code>def scipy_lhd(\n    self,\n    n: int,\n    repeats: int = 1,\n    lower: Optional[Union[int, float]] = None,\n    upper: Optional[Union[int, float]] = None,\n) -&gt; ndarray:\n    \"\"\"\n    Latin hypercube sampling based on scipy.\n\n    Args:\n        n (int): number of samples\n        repeats (int): number of repeats (replicates)\n        lower (int or float, optional): lower bound. Defaults to 0.\n        upper (int or float, optional): upper bound. Defaults to 1.\n\n    Returns:\n        (ndarray): Latin hypercube design.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.design.spacefilling import spacefilling\n            import numpy as np\n            lhd = spacefilling(k=2, seed=123)\n            lhd.scipy_lhd(n=5, repeats=2, lower=np.array([0,0]), upper=np.array([1,1]))\n            array([[0.66352963, 0.5892358 ],\n                [0.66352963, 0.5892358 ],\n                [0.55592803, 0.96312564],\n                [0.55592803, 0.96312564],\n                [0.16481882, 0.0375811 ],\n                [0.16481882, 0.0375811 ],\n                [0.215331  , 0.34468512],\n                [0.215331  , 0.34468512],\n                [0.83604909, 0.62202146],\n                [0.83604909, 0.62202146]])\n    \"\"\"\n    if lower is None:\n        lower = zeros(self.k)\n    if upper is None:\n        upper = ones(self.k)\n    sample = self.sampler.random(n=n)\n    des = scale(sample, lower, upper)\n    return repeat(des, repeats, axis=0)\n</code></pre>"},{"location":"reference/spotPython/fun/hyperlight/","title":"hyperlight","text":""},{"location":"reference/spotPython/fun/hyperlight/#spotPython.fun.hyperlight.HyperLight","title":"<code>HyperLight</code>","text":"<p>Hyperparameter Tuning for Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for the random number generator. See Numpy Random Sampling.</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for the random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n&gt;&gt;&gt; print(hyper_light.seed)\n126\n</code></pre> Source code in <code>spotPython/fun/hyperlight.py</code> <pre><code>class HyperLight:\n    \"\"\"\n    Hyperparameter Tuning for Lightning.\n\n    Args:\n        seed (int): seed for the random number generator. See Numpy Random Sampling.\n        log_level (int): log level for the logger.\n\n    Attributes:\n        seed (int): seed for the random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the hyperparameter tuning.\n        log_level (int): log level for the logger.\n\n    Examples:\n        &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n        &gt;&gt;&gt; print(hyper_light.seed)\n        126\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50) -&gt; None:\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\n            \"seed\": None,\n            \"data\": None,\n            \"step\": 10_000,\n            \"horizon\": None,\n            \"grace_period\": None,\n            \"metric_river\": None,\n            \"metric_sklearn\": None,\n            \"weights\": array([1, 0, 0]),\n            \"weight_coeff\": 0.0,\n            \"log_level\": log_level,\n            \"var_name\": [],\n            \"var_type\": [],\n        }\n        self.log_level = self.fun_control[\"log_level\"]\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Checks the shape of the input array X and raises an exception if it is not valid.\n\n        Args:\n            X (np.ndarray):\n                input array.\n\n        Returns:\n            np.ndarray:\n                input array with valid shape.\n\n        Raises:\n            Exception:\n                if the shape of the input array is not valid.\n\n        Examples:\n            &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n            &gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n            &gt;&gt;&gt; hyper_light.check_X_shape(X)\n            array([[1, 2],\n                   [3, 4]])\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(self.fun_control[\"var_name\"]):\n            raise Exception(\"Invalid shape of input array X.\")\n        return X\n\n    def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluates the function for the given input array X and control parameters.\n\n        Args:\n            X (np.ndarray):\n                input array.\n            fun_control (dict):\n                dictionary containing control parameters for the hyperparameter tuning.\n\n        Returns:\n            (np.ndarray):\n                array containing the evaluation results.\n\n        Examples:\n            &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n                X = np.array([[1, 2], [3, 4]])\n                fun_control = {\"weights\": np.array([1, 0, 0])}\n                hyper_light.fun(X, fun_control)\n                array([nan, nan])\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        if fun_control is not None:\n            self.fun_control.update(fun_control)\n        self.check_X_shape(X)\n        var_dict = assign_values(X, self.fun_control[\"var_name\"])\n        # type information and transformations are considered in generate_one_config_from_var_dict:\n        for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n            logger.debug(f\"\\nconfig: {config}\")\n            # extract parameters like epochs, batch_size, lr, etc. from config\n            # config_id = generate_config_id(config)\n            try:\n                print(\"fun: Calling train_model\")\n                df_eval = train_model(config, self.fun_control)\n                print(\"fun: train_model returned\")\n            except Exception as err:\n                logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                logger.error(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            z_val = self.fun_control[\"weights\"] * df_eval\n            z_res = np.append(z_res, z_val)\n        return z_res\n</code></pre>"},{"location":"reference/spotPython/fun/hyperlight/#spotPython.fun.hyperlight.HyperLight.check_X_shape","title":"<code>check_X_shape(X)</code>","text":"<p>Checks the shape of the input array X and raises an exception if it is not valid.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: input array with valid shape.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if the shape of the input array is not valid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; hyper_light.check_X_shape(X)\narray([[1, 2],\n       [3, 4]])\n</code></pre> Source code in <code>spotPython/fun/hyperlight.py</code> <pre><code>def check_X_shape(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Checks the shape of the input array X and raises an exception if it is not valid.\n\n    Args:\n        X (np.ndarray):\n            input array.\n\n    Returns:\n        np.ndarray:\n            input array with valid shape.\n\n    Raises:\n        Exception:\n            if the shape of the input array is not valid.\n\n    Examples:\n        &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n        &gt;&gt;&gt; hyper_light.check_X_shape(X)\n        array([[1, 2],\n               [3, 4]])\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(self.fun_control[\"var_name\"]):\n        raise Exception(\"Invalid shape of input array X.\")\n    return X\n</code></pre>"},{"location":"reference/spotPython/fun/hyperlight/#spotPython.fun.hyperlight.HyperLight.fun","title":"<code>fun(X, fun_control=None)</code>","text":"<p>Evaluates the function for the given input array X and control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing the evaluation results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n    X = np.array([[1, 2], [3, 4]])\n    fun_control = {\"weights\": np.array([1, 0, 0])}\n    hyper_light.fun(X, fun_control)\n    array([nan, nan])\n</code></pre> Source code in <code>spotPython/fun/hyperlight.py</code> <pre><code>def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates the function for the given input array X and control parameters.\n\n    Args:\n        X (np.ndarray):\n            input array.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        (np.ndarray):\n            array containing the evaluation results.\n\n    Examples:\n        &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n            X = np.array([[1, 2], [3, 4]])\n            fun_control = {\"weights\": np.array([1, 0, 0])}\n            hyper_light.fun(X, fun_control)\n            array([nan, nan])\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    if fun_control is not None:\n        self.fun_control.update(fun_control)\n    self.check_X_shape(X)\n    var_dict = assign_values(X, self.fun_control[\"var_name\"])\n    # type information and transformations are considered in generate_one_config_from_var_dict:\n    for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n        logger.debug(f\"\\nconfig: {config}\")\n        # extract parameters like epochs, batch_size, lr, etc. from config\n        # config_id = generate_config_id(config)\n        try:\n            print(\"fun: Calling train_model\")\n            df_eval = train_model(config, self.fun_control)\n            print(\"fun: train_model returned\")\n        except Exception as err:\n            logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n            logger.error(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        z_val = self.fun_control[\"weights\"] * df_eval\n        z_res = np.append(z_res, z_val)\n    return z_res\n</code></pre>"},{"location":"reference/spotPython/fun/hyperlightning/","title":"hyperlightning","text":""},{"location":"reference/spotPython/fun/hyperlightning/#spotPython.fun.hyperlightning.HyperLightning","title":"<code>HyperLightning</code>","text":"<p>Hyperparameter Tuning for Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for the random number generator. See Numpy Random Sampling.</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for the random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>log_level</code> <code>int</code> <p>log level for the logger.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n&gt;&gt;&gt; print(hyper_light.seed)\n126\n</code></pre> Source code in <code>spotPython/fun/hyperlightning.py</code> <pre><code>class HyperLightning:\n    \"\"\"\n    Hyperparameter Tuning for Lightning.\n\n    Args:\n        seed (int): seed for the random number generator. See Numpy Random Sampling.\n        log_level (int): log level for the logger.\n\n    Attributes:\n        seed (int): seed for the random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the hyperparameter tuning.\n        log_level (int): log level for the logger.\n\n    Examples:\n        &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n        &gt;&gt;&gt; print(hyper_light.seed)\n        126\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50) -&gt; None:\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\n            \"seed\": None,\n            \"data\": None,\n            \"step\": 10_000,\n            \"horizon\": None,\n            \"grace_period\": None,\n            \"metric_river\": None,\n            \"metric_sklearn\": None,\n            \"weights\": array([1, 0, 0]),\n            \"weight_coeff\": 0.0,\n            \"log_level\": log_level,\n            \"var_name\": [],\n            \"var_type\": [],\n        }\n        self.log_level = self.fun_control[\"log_level\"]\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Checks the shape of the input array X and raises an exception if it is not valid.\n\n        Args:\n            X (np.ndarray):\n                input array.\n\n        Returns:\n            np.ndarray:\n                input array with valid shape.\n\n        Raises:\n            Exception:\n                if the shape of the input array is not valid.\n\n        Examples:\n            &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n            &gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n            &gt;&gt;&gt; hyper_light.check_X_shape(X)\n            array([[1, 2],\n                   [3, 4]])\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(self.fun_control[\"var_name\"]):\n            raise Exception(\"Invalid shape of input array X.\")\n        return X\n\n    def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluates the function for the given input array X and control parameters.\n\n        Args:\n            X (np.ndarray):\n                input array.\n            fun_control (dict):\n                dictionary containing control parameters for the hyperparameter tuning.\n\n        Returns:\n            (np.ndarray):\n                array containing the evaluation results.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.utils.init import fun_control_init\n                from spotPython.utils.file import get_experiment_name, get_spot_tensorboard_path\n                from spotPython.utils.device import getDevice\n                from spotPython.light.cnn.googlenet import GoogleNet\n                from spotPython.data.lightning_hyper_dict import LightningHyperDict\n                from spotPython.hyperparameters.values import add_core_model_to_fun_control\n                from spotPython.fun.hyperlightning import HyperLightning\n                from spotPython.hyperparameters.values import get_default_hyperparameters_as_array\n                MAX_TIME = 1\n                INIT_SIZE = 3\n                WORKERS = 8\n                PREFIX=\"TEST\"\n                experiment_name = get_experiment_name(prefix=PREFIX)\n                fun_control = fun_control_init(\n                    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n                    num_workers=WORKERS,\n                    device=getDevice(),\n                    _L_in=3,\n                    _L_out=10,\n                    TENSORBOARD_CLEAN=True)\n                add_core_model_to_fun_control(core_model=GoogleNet,\n                                            fun_control=fun_control,\n                                            hyper_dict= LightningHyperDict)\n                X_start = get_default_hyperparameters_as_array(fun_control)\n                hyper_lightning = HyperLightning(seed=126, log_level=50)\n                hyper_lightning.fun(X=X_start, fun_control=fun_control)\n\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        if fun_control is not None:\n            self.fun_control.update(fun_control)\n        self.check_X_shape(X)\n        var_dict = assign_values(X, self.fun_control[\"var_name\"])\n        # type information and transformations are considered in generate_one_config_from_var_dict:\n        for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n            logger.debug(f\"\\nconfig: {config}\")\n            print(f\"\\ncore_model: {fun_control['core_model']}\")\n            print(f\"config: {config}\")\n            # extract parameters like epochs, batch_size, lr, etc. from config\n            # config_id = generate_config_id(config)\n            try:\n                print(\"fun: Calling train_model\")\n                df_eval = train_model(config, self.fun_control)\n                print(\"fun: train_model returned\")\n            except Exception as err:\n                logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n                logger.error(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            z_val = self.fun_control[\"weights\"] * df_eval\n            z_res = np.append(z_res, z_val)\n        return z_res\n</code></pre>"},{"location":"reference/spotPython/fun/hyperlightning/#spotPython.fun.hyperlightning.HyperLightning.check_X_shape","title":"<code>check_X_shape(X)</code>","text":"<p>Checks the shape of the input array X and raises an exception if it is not valid.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: input array with valid shape.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if the shape of the input array is not valid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; hyper_light.check_X_shape(X)\narray([[1, 2],\n       [3, 4]])\n</code></pre> Source code in <code>spotPython/fun/hyperlightning.py</code> <pre><code>def check_X_shape(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Checks the shape of the input array X and raises an exception if it is not valid.\n\n    Args:\n        X (np.ndarray):\n            input array.\n\n    Returns:\n        np.ndarray:\n            input array with valid shape.\n\n    Raises:\n        Exception:\n            if the shape of the input array is not valid.\n\n    Examples:\n        &gt;&gt;&gt; hyper_light = HyperLight(seed=126, log_level=50)\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n        &gt;&gt;&gt; hyper_light.check_X_shape(X)\n        array([[1, 2],\n               [3, 4]])\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(self.fun_control[\"var_name\"]):\n        raise Exception(\"Invalid shape of input array X.\")\n    return X\n</code></pre>"},{"location":"reference/spotPython/fun/hyperlightning/#spotPython.fun.hyperlightning.HyperLightning.fun","title":"<code>fun(X, fun_control=None)</code>","text":"<p>Evaluates the function for the given input array X and control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing the evaluation results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.utils.init import fun_control_init\n    from spotPython.utils.file import get_experiment_name, get_spot_tensorboard_path\n    from spotPython.utils.device import getDevice\n    from spotPython.light.cnn.googlenet import GoogleNet\n    from spotPython.data.lightning_hyper_dict import LightningHyperDict\n    from spotPython.hyperparameters.values import add_core_model_to_fun_control\n    from spotPython.fun.hyperlightning import HyperLightning\n    from spotPython.hyperparameters.values import get_default_hyperparameters_as_array\n    MAX_TIME = 1\n    INIT_SIZE = 3\n    WORKERS = 8\n    PREFIX=\"TEST\"\n    experiment_name = get_experiment_name(prefix=PREFIX)\n    fun_control = fun_control_init(\n        spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n        num_workers=WORKERS,\n        device=getDevice(),\n        _L_in=3,\n        _L_out=10,\n        TENSORBOARD_CLEAN=True)\n    add_core_model_to_fun_control(core_model=GoogleNet,\n                                fun_control=fun_control,\n                                hyper_dict= LightningHyperDict)\n    X_start = get_default_hyperparameters_as_array(fun_control)\n    hyper_lightning = HyperLightning(seed=126, log_level=50)\n    hyper_lightning.fun(X=X_start, fun_control=fun_control)\n</code></pre> Source code in <code>spotPython/fun/hyperlightning.py</code> <pre><code>def fun(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates the function for the given input array X and control parameters.\n\n    Args:\n        X (np.ndarray):\n            input array.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        (np.ndarray):\n            array containing the evaluation results.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.init import fun_control_init\n            from spotPython.utils.file import get_experiment_name, get_spot_tensorboard_path\n            from spotPython.utils.device import getDevice\n            from spotPython.light.cnn.googlenet import GoogleNet\n            from spotPython.data.lightning_hyper_dict import LightningHyperDict\n            from spotPython.hyperparameters.values import add_core_model_to_fun_control\n            from spotPython.fun.hyperlightning import HyperLightning\n            from spotPython.hyperparameters.values import get_default_hyperparameters_as_array\n            MAX_TIME = 1\n            INIT_SIZE = 3\n            WORKERS = 8\n            PREFIX=\"TEST\"\n            experiment_name = get_experiment_name(prefix=PREFIX)\n            fun_control = fun_control_init(\n                spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n                num_workers=WORKERS,\n                device=getDevice(),\n                _L_in=3,\n                _L_out=10,\n                TENSORBOARD_CLEAN=True)\n            add_core_model_to_fun_control(core_model=GoogleNet,\n                                        fun_control=fun_control,\n                                        hyper_dict= LightningHyperDict)\n            X_start = get_default_hyperparameters_as_array(fun_control)\n            hyper_lightning = HyperLightning(seed=126, log_level=50)\n            hyper_lightning.fun(X=X_start, fun_control=fun_control)\n\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    if fun_control is not None:\n        self.fun_control.update(fun_control)\n    self.check_X_shape(X)\n    var_dict = assign_values(X, self.fun_control[\"var_name\"])\n    # type information and transformations are considered in generate_one_config_from_var_dict:\n    for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n        logger.debug(f\"\\nconfig: {config}\")\n        print(f\"\\ncore_model: {fun_control['core_model']}\")\n        print(f\"config: {config}\")\n        # extract parameters like epochs, batch_size, lr, etc. from config\n        # config_id = generate_config_id(config)\n        try:\n            print(\"fun: Calling train_model\")\n            df_eval = train_model(config, self.fun_control)\n            print(\"fun: train_model returned\")\n        except Exception as err:\n            logger.error(f\"Error in fun(). Call to train_model failed. {err=}, {type(err)=}\")\n            logger.error(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        z_val = self.fun_control[\"weights\"] * df_eval\n        z_res = np.append(z_res, z_val)\n    return z_res\n</code></pre>"},{"location":"reference/spotPython/fun/hypersklearn/","title":"hypersklearn","text":""},{"location":"reference/spotPython/fun/hypersklearn/#spotPython.fun.hypersklearn.HyperSklearn","title":"<code>HyperSklearn</code>","text":"<p>Hyperparameter Tuning for Sklearn.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed. See Numpy Random Sampling</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for logger. Default is 50.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> <code>log_level</code> <code>int</code> <p>log level for logger.</p> <p>Examples:     &gt;&gt;&gt; from spotPython.fun.hypersklearn import HyperSklearn     &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)     &gt;&gt;&gt; print(hyper_sklearn.seed)     126</p> Source code in <code>spotPython/fun/hypersklearn.py</code> <pre><code>class HyperSklearn:\n    \"\"\"\n    Hyperparameter Tuning for Sklearn.\n\n    Args:\n        seed (int): seed.\n            See Numpy Random Sampling\n        log_level (int): log level for logger. Default is 50.\n\n    Attributes:\n        seed (int): seed for random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the function.\n        log_level (int): log level for logger.\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.hypersklearn import HyperSklearn\n        &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n        &gt;&gt;&gt; print(hyper_sklearn.seed)\n        126\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50):\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\n            \"seed\": None,\n            \"data\": None,\n            \"step\": 10_000,\n            \"horizon\": None,\n            \"grace_period\": None,\n            \"metric_river\": None,\n            \"metric_sklearn\": mean_absolute_error,\n            \"weights\": array([1, 0, 0]),\n            \"weight_coeff\": 0.0,\n            \"log_level\": log_level,\n            \"var_name\": [],\n            \"var_type\": [],\n            \"prep_model\": None,\n            \"predict_proba\": False,\n        }\n        self.log_level = self.fun_control[\"log_level\"]\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Check the shape of the input array X.\n\n        Args:\n            X (np.ndarray): input array.\n\n        Raises:\n            Exception: if the second dimension of X does not match the length of var_name in fun_control.\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.hypersklearn import HyperSklearn\n            &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n            &gt;&gt;&gt; hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"]\n            &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]]))\n            &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2]]))\n            Traceback (most recent call last):\n            ...\n            Exception\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(self.fun_control[\"var_name\"]):\n            raise Exception\n\n    def get_sklearn_df_eval_preds(self, model) -&gt; tuple:\n        \"\"\"\n        Get evaluation and prediction dataframes for a given model.\n        Args:\n            model (sklearn model): sklearn model.\n\n        Returns:\n            (tuple): tuple containing evaluation and prediction dataframes.\n\n        Raises:\n            Exception: if call to evaluate_model fails.\n\n        \"\"\"\n        try:\n            df_eval, df_preds = self.evaluate_model(model, self.fun_control)\n        except Exception as err:\n            print(f\"Error in get_sklearn_df_eval_preds(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n            print(\"Setting df_eval and df.preds to np.nan\")\n            df_eval = np.nan\n            df_preds = np.nan\n        return df_eval, df_preds\n\n    def fun_sklearn(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluate a sklearn model using hyperparameters specified in X.\n\n        Args:\n            X (np.ndarray): input array containing hyperparameters.\n            fun_control (dict): dictionary containing control parameters for the function. Default is None.\n\n        Returns:\n            (np.ndarray): array containing evaluation results.\n\n        Raises:\n            Exception: if call to evaluate_model fails.\n\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        self.fun_control.update(fun_control)\n        self.check_X_shape(X)\n        var_dict = assign_values(X, self.fun_control[\"var_name\"])\n        for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n            # config_id = generate_config_id(config)\n            if self.fun_control[\"prep_model\"] is not None:\n                model = make_pipeline(self.fun_control[\"prep_model\"], self.fun_control[\"core_model\"](**config))\n            else:\n                model = self.fun_control[\"core_model\"](**config)\n            try:\n                eval_type = fun_control[\"eval\"]\n                if eval_type == \"eval_oob_score\":\n                    df_eval, _ = evaluate_model_oob(model, self.fun_control)\n                elif eval_type == \"train_cv\":\n                    df_eval, _ = evaluate_cv(model, self.fun_control)\n                else:  # eval_type == \"train_hold_out\":\n                    df_eval, _ = evaluate_hold_out(model, self.fun_control)\n            except Exception as err:\n                print(f\"Error in fun_sklearn(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n                print(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            z_res = np.append(z_res, fun_control[\"weights\"] * df_eval)\n        return z_res\n</code></pre>"},{"location":"reference/spotPython/fun/hypersklearn/#spotPython.fun.hypersklearn.HyperSklearn.check_X_shape","title":"<code>check_X_shape(X)</code>","text":"<p>Check the shape of the input array X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>if the second dimension of X does not match the length of var_name in fun_control.</p> <p>Examples:     &gt;&gt;&gt; from spotPython.fun.hypersklearn import HyperSklearn     &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)     &gt;&gt;&gt; hyper_sklearn.fun_control[\u201cvar_name\u201d] = [\u201ca\u201d, \u201cb\u201d, \u201cc\u201d]     &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]]))     &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2]]))     Traceback (most recent call last):     \u2026     Exception</p> Source code in <code>spotPython/fun/hypersklearn.py</code> <pre><code>def check_X_shape(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Check the shape of the input array X.\n\n    Args:\n        X (np.ndarray): input array.\n\n    Raises:\n        Exception: if the second dimension of X does not match the length of var_name in fun_control.\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.hypersklearn import HyperSklearn\n        &gt;&gt;&gt; hyper_sklearn = HyperSklearn(seed=126, log_level=50)\n        &gt;&gt;&gt; hyper_sklearn.fun_control[\"var_name\"] = [\"a\", \"b\", \"c\"]\n        &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2, 3]]))\n        &gt;&gt;&gt; hyper_sklearn.check_X_shape(X=np.array([[1, 2]]))\n        Traceback (most recent call last):\n        ...\n        Exception\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(self.fun_control[\"var_name\"]):\n        raise Exception\n</code></pre>"},{"location":"reference/spotPython/fun/hypersklearn/#spotPython.fun.hypersklearn.HyperSklearn.fun_sklearn","title":"<code>fun_sklearn(X, fun_control=None)</code>","text":"<p>Evaluate a sklearn model using hyperparameters specified in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array containing hyperparameters.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array containing evaluation results.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if call to evaluate_model fails.</p> Source code in <code>spotPython/fun/hypersklearn.py</code> <pre><code>def fun_sklearn(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluate a sklearn model using hyperparameters specified in X.\n\n    Args:\n        X (np.ndarray): input array containing hyperparameters.\n        fun_control (dict): dictionary containing control parameters for the function. Default is None.\n\n    Returns:\n        (np.ndarray): array containing evaluation results.\n\n    Raises:\n        Exception: if call to evaluate_model fails.\n\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    self.fun_control.update(fun_control)\n    self.check_X_shape(X)\n    var_dict = assign_values(X, self.fun_control[\"var_name\"])\n    for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n        # config_id = generate_config_id(config)\n        if self.fun_control[\"prep_model\"] is not None:\n            model = make_pipeline(self.fun_control[\"prep_model\"], self.fun_control[\"core_model\"](**config))\n        else:\n            model = self.fun_control[\"core_model\"](**config)\n        try:\n            eval_type = fun_control[\"eval\"]\n            if eval_type == \"eval_oob_score\":\n                df_eval, _ = evaluate_model_oob(model, self.fun_control)\n            elif eval_type == \"train_cv\":\n                df_eval, _ = evaluate_cv(model, self.fun_control)\n            else:  # eval_type == \"train_hold_out\":\n                df_eval, _ = evaluate_hold_out(model, self.fun_control)\n        except Exception as err:\n            print(f\"Error in fun_sklearn(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n            print(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        z_res = np.append(z_res, fun_control[\"weights\"] * df_eval)\n    return z_res\n</code></pre>"},{"location":"reference/spotPython/fun/hypersklearn/#spotPython.fun.hypersklearn.HyperSklearn.get_sklearn_df_eval_preds","title":"<code>get_sklearn_df_eval_preds(model)</code>","text":"<p>Get evaluation and prediction dataframes for a given model. Args:     model (sklearn model): sklearn model.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>tuple containing evaluation and prediction dataframes.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>if call to evaluate_model fails.</p> Source code in <code>spotPython/fun/hypersklearn.py</code> <pre><code>def get_sklearn_df_eval_preds(self, model) -&gt; tuple:\n    \"\"\"\n    Get evaluation and prediction dataframes for a given model.\n    Args:\n        model (sklearn model): sklearn model.\n\n    Returns:\n        (tuple): tuple containing evaluation and prediction dataframes.\n\n    Raises:\n        Exception: if call to evaluate_model fails.\n\n    \"\"\"\n    try:\n        df_eval, df_preds = self.evaluate_model(model, self.fun_control)\n    except Exception as err:\n        print(f\"Error in get_sklearn_df_eval_preds(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n        print(\"Setting df_eval and df.preds to np.nan\")\n        df_eval = np.nan\n        df_preds = np.nan\n    return df_eval, df_preds\n</code></pre>"},{"location":"reference/spotPython/fun/hypertorch/","title":"hypertorch","text":""},{"location":"reference/spotPython/fun/hypertorch/#spotPython.fun.hypertorch.HyperTorch","title":"<code>HyperTorch</code>","text":"<p>Hyperparameter Tuning for Torch.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>seed for random number generator. See Numpy Random Sampling</p> <code>126</code> <code>log_level</code> <code>int</code> <p>log level for logger. Default is 50.</p> <code>50</code> <p>Attributes:</p> Name Type Description <code>seed</code> <code>int</code> <p>seed for random number generator.</p> <code>rng</code> <code>Generator</code> <p>random number generator.</p> <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> <code>log_level</code> <code>int</code> <p>log level for logger.</p> Source code in <code>spotPython/fun/hypertorch.py</code> <pre><code>class HyperTorch:\n    \"\"\"\n    Hyperparameter Tuning for Torch.\n\n    Args:\n        seed (int): seed for random number generator.\n            See Numpy Random Sampling\n        log_level (int): log level for logger. Default is 50.\n\n    Attributes:\n        seed (int): seed for random number generator.\n        rng (Generator): random number generator.\n        fun_control (dict): dictionary containing control parameters for the function.\n        log_level (int): log level for logger.\n    \"\"\"\n\n    def __init__(self, seed: int = 126, log_level: int = 50):\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\n            \"seed\": None,\n            \"data\": None,\n            \"step\": 10_000,\n            \"horizon\": None,\n            \"grace_period\": None,\n            \"metric_river\": None,\n            \"metric_sklearn\": None,\n            \"weights\": array([1, 0, 0]),\n            \"weight_coeff\": 0.0,\n            \"log_level\": log_level,\n            \"var_name\": [],\n            \"var_type\": [],\n        }\n        self.log_level = self.fun_control[\"log_level\"]\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n    def check_X_shape(self, X: np.ndarray) -&gt; None:\n        \"\"\"\n        Check the shape of the input array X.\n\n        Args:\n            X (np.ndarray): input array.\n\n        Raises:\n            Exception: if the second dimension of X does not match the length of var_name in fun_control.\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.hypertorch import HyperTorch\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n            &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n            &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]]))\n            &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([1, 2]))\n            Traceback (most recent call last):\n            ...\n            Exception\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != len(self.fun_control[\"var_name\"]):\n            raise Exception\n\n    def fun_torch(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n        \"\"\"\n        Function to be optimized.\n\n        Args:\n            X (np.ndarray): input array.\n            fun_control (dict): dictionary containing control parameters for the function.\n        Returns:\n            np.ndarray: output array.\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.hypertorch import HyperTorch\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n            &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n            &gt;&gt;&gt; hyper_torch.fun_torch(np.array([[1, 2], [3, 4]]))\n\n        \"\"\"\n        z_res = np.array([], dtype=float)\n        self.fun_control.update(fun_control)\n        self.check_X_shape(X)\n        var_dict = assign_values(X, self.fun_control[\"var_name\"])\n        for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n            print(f\"\\nconfig: {config}\")\n            config_id = generate_config_id(config)\n            if self.fun_control[\"prep_model\"] is not None:\n                model = make_pipeline(self.fun_control[\"prep_model\"], self.fun_control[\"core_model\"](**config))\n            else:\n                model = self.fun_control[\"core_model\"](**config)\n            try:\n                if self.fun_control[\"eval\"] == \"train_cv\":\n                    df_eval, _ = evaluate_cv(\n                        model,\n                        dataset=fun_control[\"train\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n                elif self.fun_control[\"eval\"] == \"test_cv\":\n                    df_eval, _ = evaluate_cv(\n                        model,\n                        dataset=fun_control[\"test\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n                elif self.fun_control[\"eval\"] == \"test_hold_out\":\n                    df_eval, _ = evaluate_hold_out(\n                        model,\n                        train_dataset=fun_control[\"train\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        loss_function=self.fun_control[\"loss_function\"],\n                        metric=self.fun_control[\"metric_torch\"],\n                        test_dataset=fun_control[\"test\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        path=self.fun_control[\"path\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n                else:  # eval == \"train_hold_out\"\n                    df_eval, _ = evaluate_hold_out(\n                        model,\n                        train_dataset=fun_control[\"train\"],\n                        shuffle=self.fun_control[\"shuffle\"],\n                        loss_function=self.fun_control[\"loss_function\"],\n                        metric=self.fun_control[\"metric_torch\"],\n                        device=self.fun_control[\"device\"],\n                        show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                        path=self.fun_control[\"path\"],\n                        task=self.fun_control[\"task\"],\n                        writer=self.fun_control[\"spot_writer\"],\n                        writerId=config_id,\n                    )\n            except Exception as err:\n                print(f\"Error in fun_torch(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n                print(\"Setting df_eval to np.nan\")\n                df_eval = np.nan\n            z_val = fun_control[\"weights\"] * df_eval\n            if self.fun_control[\"spot_writer\"] is not None:\n                writer = self.fun_control[\"spot_writer\"]\n                writer.add_hparams(config, {\"fun_torch: loss\": z_val})\n                writer.flush()\n            z_res = np.append(z_res, z_val)\n        return z_res\n</code></pre>"},{"location":"reference/spotPython/fun/hypertorch/#spotPython.fun.hypertorch.HyperTorch.check_X_shape","title":"<code>check_X_shape(X)</code>","text":"<p>Check the shape of the input array X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>if the second dimension of X does not match the length of var_name in fun_control.</p> <p>Examples:     &gt;&gt;&gt; from spotPython.fun.hypertorch import HyperTorch     &gt;&gt;&gt; import numpy as np     &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)     &gt;&gt;&gt; hyper_torch.fun_control[\u201cvar_name\u201d] = [\u201cx1\u201d, \u201cx2\u201d]     &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]]))     &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([1, 2]))     Traceback (most recent call last):     \u2026     Exception</p> Source code in <code>spotPython/fun/hypertorch.py</code> <pre><code>def check_X_shape(self, X: np.ndarray) -&gt; None:\n    \"\"\"\n    Check the shape of the input array X.\n\n    Args:\n        X (np.ndarray): input array.\n\n    Raises:\n        Exception: if the second dimension of X does not match the length of var_name in fun_control.\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.hypertorch import HyperTorch\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n        &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n        &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([[1, 2], [3, 4]]))\n        &gt;&gt;&gt; hyper_torch.check_X_shape(np.array([1, 2]))\n        Traceback (most recent call last):\n        ...\n        Exception\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != len(self.fun_control[\"var_name\"]):\n        raise Exception\n</code></pre>"},{"location":"reference/spotPython/fun/hypertorch/#spotPython.fun.hypertorch.HyperTorch.fun_torch","title":"<code>fun_torch(X, fun_control=None)</code>","text":"<p>Function to be optimized.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>input array.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the function.</p> <code>None</code> <p>Returns:     np.ndarray: output array. Examples:     &gt;&gt;&gt; from spotPython.fun.hypertorch import HyperTorch     &gt;&gt;&gt; import numpy as np     &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)     &gt;&gt;&gt; hyper_torch.fun_control[\u201cvar_name\u201d] = [\u201cx1\u201d, \u201cx2\u201d]     &gt;&gt;&gt; hyper_torch.fun_torch(np.array([[1, 2], [3, 4]]))</p> Source code in <code>spotPython/fun/hypertorch.py</code> <pre><code>def fun_torch(self, X: np.ndarray, fun_control: dict = None) -&gt; np.ndarray:\n    \"\"\"\n    Function to be optimized.\n\n    Args:\n        X (np.ndarray): input array.\n        fun_control (dict): dictionary containing control parameters for the function.\n    Returns:\n        np.ndarray: output array.\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.hypertorch import HyperTorch\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; hyper_torch = HyperTorch(seed=126, log_level=50)\n        &gt;&gt;&gt; hyper_torch.fun_control[\"var_name\"] = [\"x1\", \"x2\"]\n        &gt;&gt;&gt; hyper_torch.fun_torch(np.array([[1, 2], [3, 4]]))\n\n    \"\"\"\n    z_res = np.array([], dtype=float)\n    self.fun_control.update(fun_control)\n    self.check_X_shape(X)\n    var_dict = assign_values(X, self.fun_control[\"var_name\"])\n    for config in generate_one_config_from_var_dict(var_dict, self.fun_control):\n        print(f\"\\nconfig: {config}\")\n        config_id = generate_config_id(config)\n        if self.fun_control[\"prep_model\"] is not None:\n            model = make_pipeline(self.fun_control[\"prep_model\"], self.fun_control[\"core_model\"](**config))\n        else:\n            model = self.fun_control[\"core_model\"](**config)\n        try:\n            if self.fun_control[\"eval\"] == \"train_cv\":\n                df_eval, _ = evaluate_cv(\n                    model,\n                    dataset=fun_control[\"train\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n            elif self.fun_control[\"eval\"] == \"test_cv\":\n                df_eval, _ = evaluate_cv(\n                    model,\n                    dataset=fun_control[\"test\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n            elif self.fun_control[\"eval\"] == \"test_hold_out\":\n                df_eval, _ = evaluate_hold_out(\n                    model,\n                    train_dataset=fun_control[\"train\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    loss_function=self.fun_control[\"loss_function\"],\n                    metric=self.fun_control[\"metric_torch\"],\n                    test_dataset=fun_control[\"test\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    path=self.fun_control[\"path\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n            else:  # eval == \"train_hold_out\"\n                df_eval, _ = evaluate_hold_out(\n                    model,\n                    train_dataset=fun_control[\"train\"],\n                    shuffle=self.fun_control[\"shuffle\"],\n                    loss_function=self.fun_control[\"loss_function\"],\n                    metric=self.fun_control[\"metric_torch\"],\n                    device=self.fun_control[\"device\"],\n                    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n                    path=self.fun_control[\"path\"],\n                    task=self.fun_control[\"task\"],\n                    writer=self.fun_control[\"spot_writer\"],\n                    writerId=config_id,\n                )\n        except Exception as err:\n            print(f\"Error in fun_torch(). Call to evaluate_model failed. {err=}, {type(err)=}\")\n            print(\"Setting df_eval to np.nan\")\n            df_eval = np.nan\n        z_val = fun_control[\"weights\"] * df_eval\n        if self.fun_control[\"spot_writer\"] is not None:\n            writer = self.fun_control[\"spot_writer\"]\n            writer.add_hparams(config, {\"fun_torch: loss\": z_val})\n            writer.flush()\n        z_res = np.append(z_res, z_val)\n    return z_res\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/","title":"objectivefunctions","text":""},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical","title":"<code>analytical</code>","text":"<p>Class for analytical test functions.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>float</code> <p>Offset value. Defaults to 0.0.</p> <code>0.0</code> <code>hz</code> <code>float</code> <p>Horizontal value. Defaults to 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Seed value for random number generation. Defaults to 126.</p> <code>126</code> <p>Note:     See Numpy Random Sampling</p> <p>Attributes:</p> Name Type Description <code>offset</code> <code>float</code> <p>Offset value.</p> <code>hz</code> <code>float</code> <p>Horizontal value.</p> <code>seed</code> <code>int</code> <p>Seed value for random number generation.</p> <code>rng</code> <code>Generator</code> <p>Numpy random number generator object.</p> <code>fun_control</code> <code>dict</code> <p>Dictionary containing control parameters for the function.</p> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>class analytical:\n    \"\"\"\n    Class for analytical test functions.\n\n    Args:\n        offset (float):\n            Offset value. Defaults to 0.0.\n        hz (float):\n            Horizontal value. Defaults to 0.\n        seed (int):\n            Seed value for random number generation. Defaults to 126.\n    Note:\n        See [Numpy Random Sampling](https://numpy.org/doc/stable/reference/random/index.html#random-quick-start)\n\n    Attributes:\n        offset (float):\n            Offset value.\n        hz (float):\n            Horizontal value.\n        seed (int):\n            Seed value for random number generation.\n        rng (Generator):\n            Numpy random number generator object.\n        fun_control (dict):\n            Dictionary containing control parameters for the function.\n    \"\"\"\n\n    def __init__(self, offset: float = 0.0, hz: float = 0, seed: int = 126) -&gt; None:\n        self.offset = offset\n        self.hz = hz\n        self.seed = seed\n        self.rng = default_rng(seed=self.seed)\n        self.fun_control = {\"sigma\": 0, \"seed\": None, \"sel_var\": None}\n\n    def __repr__(self) -&gt; str:\n        return f\"analytical(offset={self.offset}, hz={self.hz}, seed={self.seed})\"\n\n    def add_noise(self, y: List[float]) -&gt; np.ndarray:\n        \"\"\"\n        Adds noise to the input data.\n        This method takes in a list of float values y as input and adds noise to\n        the data using a random number generator. The method returns a numpy array\n        containing the noisy data.\n\n        Args:\n            self (analytical): analytical class object.\n            y (List[float]): Input data.\n\n        Returns:\n            np.ndarray: Noisy data.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.add_noise(y)\n            array([1.        , 2.        , 3.        , 4.        , 5.        ])\n\n        \"\"\"\n        # Use own rng:\n        if self.fun_control[\"seed\"] is not None:\n            rng = default_rng(seed=self.fun_control[\"seed\"])\n        # Use class rng:\n        else:\n            rng = self.rng\n        noise_y = np.array([], dtype=float)\n        for y_i in y:\n            noise_y = np.append(\n                noise_y,\n                y_i + rng.normal(loc=0, scale=self.fun_control[\"sigma\"], size=1),\n            )\n        return noise_y\n\n    def fun_branin_factor(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"\n        Calculates the Branin function with an additional factor based on the value of x3.\n\n        Args:\n            X (np.ndarray):\n                A 2D numpy array with shape (n, 3) where n is the number of samples.\n            fun_control (Optional[Dict]):\n                A dictionary containing control parameters for the function.\n                If None, self.fun_control is used. Defaults to None.\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_branin_factor(X)\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        if len(X.shape) == 1:\n            X = np.array([X])\n        if X.shape[1] != 3:\n            raise Exception(\"X must have shape (n, 3)\")\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        x3 = X[:, 2]\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        r = 6\n        s = 10\n        t = 1 / (8 * np.pi)\n        y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n        for j in range(X.shape[0]):\n            if x3[j] == 1:\n                y[j] = y[j] + 10\n            elif x3[j] == 2:\n                y[j] = y[j] - 10\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_linear(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Linear function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_linear(X)\n            array([ 6., 15.])\n\n        \"\"\"\n        if fun_control is not None:\n            self.fun_control = fun_control\n        try:\n            X.shape[1]\n        except ValueError as err:\n            print(\"error message:\", err)\n            X = np.array(X)\n\n        if len(X.shape) &lt; 2:\n            X = np.array([X])\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, np.sum(X[i]))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_sphere(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Sphere function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_sphere(X)\n            array([14., 77.])\n\n        \"\"\"\n        if fun_control is not None:\n            self.fun_control = fun_control\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array(X)\n\n        if len(X.shape) &lt; 2:\n            X = np.array([X])\n        offset = np.ones(X.shape[1]) * self.offset\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, np.sum((X[i] - offset) ** 2))\n        # TODO: move to a separate function:\n        if self.fun_control[\"sigma\"] &gt; 0:\n            # Use own rng:\n            if self.fun_control[\"seed\"] is not None:\n                rng = default_rng(seed=fun_control[\"seed\"])\n            # Use class rng:\n            else:\n                rng = self.rng\n            noise_y = np.array([], dtype=float)\n            for y_i in y:\n                noise_y = np.append(noise_y, y_i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n            return noise_y\n        else:\n            return y\n\n    def fun_cubed(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Cubed function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_cubed(X)\n            array([ 0., 27.])\n        \"\"\"\n\n        if fun_control is None:\n            fun_control = self.fun_control\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array(X)\n\n        if len(X.shape) &lt; 2:\n            X = np.array([X])\n        offset = np.ones(X.shape[1]) * self.offset\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, np.sum((X[i] - offset) ** 3))\n        # TODO: move to a separate function:\n        if fun_control[\"sigma\"] &gt; 0:\n            # Use own rng:\n            if fun_control[\"seed\"] is not None:\n                rng = default_rng(seed=fun_control[\"seed\"])\n            # Use class rng:\n            else:\n                rng = self.rng\n            noise_y = np.array([], dtype=float)\n            for i in y:\n                # noise_y = np.append(\n                #     noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1)\n                noise_y = np.append(noise_y, i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n            return noise_y\n        else:\n            return y\n\n    def fun_forrester(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Forrester function. Function used by [Forr08a, p.83].\n           f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].\n           Starts with three sample points at x=0, x=0.5, and x=1.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_forrester(X)\n            array([  0.        ,  11.99999999])\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array(X)\n\n        if len(X.shape) &lt; 2:\n            X = np.array([X])\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, (6.0 * X[i] - 2) ** 2 * np.sin(12 * X[i] - 4))\n        # TODO: move to a separate function:\n        if fun_control[\"sigma\"] &gt; 0:\n            # Use own rng:\n            if fun_control[\"seed\"] is not None:\n                rng = default_rng(seed=fun_control[\"seed\"])\n            # Use class rng:\n            else:\n                rng = self.rng\n            noise_y = np.array([], dtype=float)\n            for i in y:\n                # noise_y = np.append(\n                #     noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1)\n                noise_y = np.append(noise_y, i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n            return noise_y\n        else:\n            return y\n\n    def fun_branin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Branin function. The 2-dim Branin function is defined as\n            y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s,\n            where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2),\n            c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi).\n\n            It has three global minima:\n            f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475).\n            Input domain: This function is usually evaluated on the square  x1 in  [-5, 10] x x2 in [0, 15].\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_branin(X)\n            array([  0.        ,  11.99999999])\n\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != 2:\n            raise Exception\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        r = 6\n        s = 10\n        t = 1 / (8 * np.pi)\n        y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n        # TODO: move to a separate function:\n        if fun_control[\"sigma\"] &gt; 0:\n            # Use own rng:\n            if fun_control[\"seed\"] is not None:\n                rng = default_rng(seed=fun_control[\"seed\"])\n            # Use class rng:\n            else:\n                rng = self.rng\n            noise_y = np.array([], dtype=float)\n            for i in y:\n                # noise_y = np.append(\n                #     noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1)\n                noise_y = np.append(noise_y, i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n            return noise_y\n        else:\n            return y\n\n    def fun_branin_modified(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Modified Branin function.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_branin_modified(X)\n            array([  0.        ,  11.99999999])\n\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n\n        if X.shape[1] != 2:\n            raise Exception\n        x = X[:, 0]\n        y = X[:, 1]\n        X1 = 15 * x - 5\n        X2 = 15 * y\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        d = 6\n        e = 10\n        ff = 1 / (8 * np.pi)\n        y = (a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e) + 5 * x\n        # TODO: move to a separate function:\n        if fun_control[\"sigma\"] &gt; 0:\n            # Use own rng:\n            if fun_control[\"seed\"] is not None:\n                rng = default_rng(seed=fun_control[\"seed\"])\n            # Use class rng:\n            else:\n                rng = self.rng\n            noise_y = np.array([], dtype=float)\n            for i in y:\n                noise_y = np.append(noise_y, i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n            return noise_y\n        else:\n            return y\n\n    def branin_noise(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Branin function with noise.\n\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.branin_noise(X)\n            array([  0.        ,  11.99999999])\n\n        \"\"\"\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n\n        if X.shape[1] != 2:\n            raise Exception\n        x = X[:, 0]\n        y = X[:, 1]\n        X1 = 15 * x - 5\n        X2 = 15 * y\n        a = 1\n        b = 5.1 / (4 * np.pi**2)\n        c = 5 / np.pi\n        d = 6\n        e = 10\n        ff = 1 / (8 * np.pi)\n        noiseFree = (a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e) + 5 * x\n        noise_y = []\n        for i in noiseFree:\n            noise_y.append(i + np.random.standard_normal() * 15)\n        return np.array(noise_y)\n\n    def fun_sin_cos(self, X, fun_control=None):\n        \"\"\"Sinusoidal function.\n        Args:\n            X (array):\n                input\n            fun_control (dict):\n                dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_sin_cos(X)\n            array([-1.        , -0.41614684])\n        \"\"\"\n\n        if fun_control is None:\n            fun_control = self.fun_control\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != 2:\n            raise Exception\n        x0 = X[:, 0]\n        x1 = X[:, 1]\n        y = 2.0 * np.sin(x0 + self.hz) + 0.5 * np.cos(x1 + self.hz)\n        # TODO: move to a separate function:\n        if fun_control[\"sigma\"] &gt; 0:\n            # Use own rng:\n            if fun_control[\"seed\"] is not None:\n                rng = default_rng(seed=fun_control[\"seed\"])\n            # Use class rng:\n            else:\n                rng = self.rng\n            noise_y = np.array([], dtype=float)\n            for i in y:\n                noise_y = np.append(noise_y, i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n            return noise_y\n        else:\n            return y\n\n    # def fun_forrester_2(self, X):\n    #     \"\"\"\n    #     Function used by [Forr08a, p.83].\n    #     f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].\n    #     Starts with three sample points at x=0, x=0.5, and x=1.\n\n    #     Args:\n    #         X (flooat): input values (1-dim)\n\n    #     Returns:\n    #         float: function value\n    #     \"\"\"\n    #     try:\n    #         X.shape[1]\n    #     except ValueError:\n    #         X = np.array(X)\n\n    #     if len(X.shape) &lt; 2:\n    #         X = np.array([X])\n    #     # y = X[:, 1]\n    #     y = (6.0 * X - 2) ** 2 * np.sin(12 * X - 4)\n    #     if self.sigma != 0:\n    #         noise_y = np.array([], dtype=float)\n    #         for i in y:\n    #             noise_y = np.append(\n    #                 noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1)\n    #             )\n    #         return noise_y\n    #     else:\n    #         return y\n\n    def fun_runge(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k &gt;= 1.\n           Interval: -5 &lt;= x &lt;= 5\n\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_runge(X)\n            array([0.0625    , 0.015625  , 0.00390625])\n\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array(X)\n\n        if len(X.shape) &lt; 2:\n            X = np.array([X])\n        offset = np.ones(X.shape[1]) * self.offset\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, (1 / (1 + np.sum((X[i] - offset) ** 2))))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_wingwt(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Wing weight function. Example from Forrester et al. to understand the weight\n            of an unpainted light aircraft wing as a function of nine design and operational parameters:\n            W = 0.036 S_W**0.758 * Wfw**0.0035 ( A / (cos**2 Lambda))**0.6 *\n            q**0.006  * lambda**0.04 * ( (100 Rtc)/(cos Lambda) ))**-0.3*(Nz Wdg)**0.49\n\n        | Symbol    | Parameter                              | Baseline | Minimum | Maximum |\n        |-----------|----------------------------------------|----------|---------|---------|\n        | $S_W$     | Wing area ($ft^2$)                     | 174      | 150     | 200     |\n        | $W_{fw}$  | Weight of fuel in wing (lb)            | 252      | 220     | 300     |\n        | $A$       | Aspect ratio                          | 7.52     | 6       | 10      |\n        | $Lambda$ | Quarter-chord sweep (deg)              | 0        | -10     | 10      |\n        | $q$       | Dynamic pressure at cruise ($lb/ft^2$) | 34       | 16      | 45      |\n        | $lambda$ | Taper ratio                            | 0.672    | 0.5     | 1       |\n        | $R_{tc}$  | Aerofoil thickness to chord ratio      | 0.12     | 0.08    | 0.18    |\n        | $N_z$     | Ultimate load factor                   | 3.8      | 2.5     | 6       |\n        | $W_{dg}$  | Flight design gross weight (lb)         | 2000     | 1700    | 2500    |\n        | $W_p$     | paint weight (lb/ft^2)                   | 0.064 |   0.025  | 0.08    |\n\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_wingwt(X)\n            array([0.0625    , 0.015625  , 0.00390625])\n\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array(X)\n        #\n        W_res = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            Sw = X[i, 0] * (200 - 150) + 150\n            Wfw = X[i, 1] * (300 - 220) + 220\n            A = X[i, 2] * (10 - 6) + 6\n            L = (X[i, 3] * (10 - (-10)) - 10) * np.pi / 180\n            q = X[i, 4] * (45 - 16) + 16\n            la = X[i, 5] * (1 - 0.5) + 0.5\n            Rtc = X[i, 6] * (0.18 - 0.08) + 0.08\n            Nz = X[i, 7] * (6 - 2.5) + 2.5\n            Wdg = X[i, 8] * (2500 - 1700) + 1700\n            Wp = X[i, 9] * (0.08 - 0.025) + 0.025\n            # calculation on natural scale\n            W = 0.036 * Sw**0.758 * Wfw**0.0035 * (A / np.cos(L) ** 2) ** 0.6 * q**0.006\n            W = W * la**0.04 * (100 * Rtc / np.cos(L)) ** (-0.3) * (Nz * Wdg) ** (0.49) + Sw * Wp\n            W_res = np.append(W_res, W)\n        return W_res\n\n    def fun_xsin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Example function.\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_xsin(X)\n            array([0.84147098, 0.90929743, 0.14112001])\n\n        \"\"\"\n        if fun_control is None:\n            fun_control = self.fun_control\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array(X)\n\n        if len(X.shape) &lt; 2:\n            X = np.array([X])\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            y = np.append(y, X[i] * np.sin(1.0 / X[i]))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_rosen(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Rosenbrock function.\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_rosen(X)\n            array([24,  0])\n        \"\"\"\n\n        if fun_control is None:\n            fun_control = self.fun_control\n        try:\n            X.shape[1]\n        except ValueError:\n            X = np.array([X])\n        if X.shape[1] != 2:\n            raise Exception\n        x0 = X[:, 0]\n        x1 = X[:, 1]\n        b = 10\n        y = (x0 - 1) ** 2 + b * (x1 - x0**2) ** 2\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            return y\n\n    def fun_random_error(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n        \"\"\"Return errors for testing spot stability.\n        Args:\n            X (array): input\n            fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n        Returns:\n            np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n        Examples:\n            &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n            &gt;&gt;&gt; fun = analytical()\n            &gt;&gt;&gt; fun.fun_random_error(X)\n            array([24,  0])\n\n        \"\"\"\n        if fun_control is not None:\n            self.fun_control = fun_control\n        try:\n            X.shape[1]\n        except ValueError as err:\n            print(\"error message:\", err)\n            X = np.array(X)\n        if len(X.shape) &lt; 2:\n            X = np.array([X])\n        y = np.array([], dtype=float)\n        for i in range(X.shape[0]):\n            # provoke error:\n            if random() &lt; 0.1:\n                y = np.append(y, np.nan)\n            else:\n                y = np.append(y, np.sum(X[i]))\n        if self.fun_control[\"sigma\"] &gt; 0:\n            return self.add_noise(y)\n        else:\n            print(y)\n            return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.add_noise","title":"<code>add_noise(y)</code>","text":"<p>Adds noise to the input data. This method takes in a list of float values y as input and adds noise to the data using a random number generator. The method returns a numpy array containing the noisy data.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>analytical</code> <p>analytical class object.</p> required <code>y</code> <code>List[float]</code> <p>Input data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Noisy data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.add_noise(y)\narray([1.        , 2.        , 3.        , 4.        , 5.        ])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def add_noise(self, y: List[float]) -&gt; np.ndarray:\n    \"\"\"\n    Adds noise to the input data.\n    This method takes in a list of float values y as input and adds noise to\n    the data using a random number generator. The method returns a numpy array\n    containing the noisy data.\n\n    Args:\n        self (analytical): analytical class object.\n        y (List[float]): Input data.\n\n    Returns:\n        np.ndarray: Noisy data.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.add_noise(y)\n        array([1.        , 2.        , 3.        , 4.        , 5.        ])\n\n    \"\"\"\n    # Use own rng:\n    if self.fun_control[\"seed\"] is not None:\n        rng = default_rng(seed=self.fun_control[\"seed\"])\n    # Use class rng:\n    else:\n        rng = self.rng\n    noise_y = np.array([], dtype=float)\n    for y_i in y:\n        noise_y = np.append(\n            noise_y,\n            y_i + rng.normal(loc=0, scale=self.fun_control[\"sigma\"], size=1),\n        )\n    return noise_y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.branin_noise","title":"<code>branin_noise(X, fun_control=None)</code>","text":"<p>Branin function with noise.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.branin_noise(X)\narray([  0.        ,  11.99999999])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def branin_noise(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Branin function with noise.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.branin_noise(X)\n        array([  0.        ,  11.99999999])\n\n    \"\"\"\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n\n    if X.shape[1] != 2:\n        raise Exception\n    x = X[:, 0]\n    y = X[:, 1]\n    X1 = 15 * x - 5\n    X2 = 15 * y\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    d = 6\n    e = 10\n    ff = 1 / (8 * np.pi)\n    noiseFree = (a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e) + 5 * x\n    noise_y = []\n    for i in noiseFree:\n        noise_y.append(i + np.random.standard_normal() * 15)\n    return np.array(noise_y)\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_branin","title":"<code>fun_branin(X, fun_control=None)</code>","text":"<p>Branin function. The 2-dim Branin function is defined as     y = a * (x2 - b * x12 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s,     where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi2),     c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi).</p> <pre><code>It has three global minima:\nf(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475).\nInput domain: This function is usually evaluated on the square  x1 in  [-5, 10] x x2 in [0, 15].\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_branin(X)\narray([  0.        ,  11.99999999])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_branin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Branin function. The 2-dim Branin function is defined as\n        y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s,\n        where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2),\n        c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi).\n\n        It has three global minima:\n        f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475).\n        Input domain: This function is usually evaluated on the square  x1 in  [-5, 10] x x2 in [0, 15].\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_branin(X)\n        array([  0.        ,  11.99999999])\n\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != 2:\n        raise Exception\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    r = 6\n    s = 10\n    t = 1 / (8 * np.pi)\n    y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n    # TODO: move to a separate function:\n    if fun_control[\"sigma\"] &gt; 0:\n        # Use own rng:\n        if fun_control[\"seed\"] is not None:\n            rng = default_rng(seed=fun_control[\"seed\"])\n        # Use class rng:\n        else:\n            rng = self.rng\n        noise_y = np.array([], dtype=float)\n        for i in y:\n            # noise_y = np.append(\n            #     noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1)\n            noise_y = np.append(noise_y, i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n        return noise_y\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_branin_factor","title":"<code>fun_branin_factor(X, fun_control=None)</code>","text":"<p>Calculates the Branin function with an additional factor based on the value of x3.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D numpy array with shape (n, 3) where n is the number of samples.</p> required <code>fun_control</code> <code>Optional[Dict]</code> <p>A dictionary containing control parameters for the function. If None, self.fun_control is used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_branin_factor(X)\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_branin_factor(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the Branin function with an additional factor based on the value of x3.\n\n    Args:\n        X (np.ndarray):\n            A 2D numpy array with shape (n, 3) where n is the number of samples.\n        fun_control (Optional[Dict]):\n            A dictionary containing control parameters for the function.\n            If None, self.fun_control is used. Defaults to None.\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_branin_factor(X)\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    if len(X.shape) == 1:\n        X = np.array([X])\n    if X.shape[1] != 3:\n        raise Exception(\"X must have shape (n, 3)\")\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n    x3 = X[:, 2]\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    r = 6\n    s = 10\n    t = 1 / (8 * np.pi)\n    y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s\n    for j in range(X.shape[0]):\n        if x3[j] == 1:\n            y[j] = y[j] + 10\n        elif x3[j] == 2:\n            y[j] = y[j] - 10\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_branin_modified","title":"<code>fun_branin_modified(X, fun_control=None)</code>","text":"<p>Modified Branin function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_branin_modified(X)\narray([  0.        ,  11.99999999])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_branin_modified(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Modified Branin function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_branin_modified(X)\n        array([  0.        ,  11.99999999])\n\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n\n    if X.shape[1] != 2:\n        raise Exception\n    x = X[:, 0]\n    y = X[:, 1]\n    X1 = 15 * x - 5\n    X2 = 15 * y\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    d = 6\n    e = 10\n    ff = 1 / (8 * np.pi)\n    y = (a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e) + 5 * x\n    # TODO: move to a separate function:\n    if fun_control[\"sigma\"] &gt; 0:\n        # Use own rng:\n        if fun_control[\"seed\"] is not None:\n            rng = default_rng(seed=fun_control[\"seed\"])\n        # Use class rng:\n        else:\n            rng = self.rng\n        noise_y = np.array([], dtype=float)\n        for i in y:\n            noise_y = np.append(noise_y, i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n        return noise_y\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_cubed","title":"<code>fun_cubed(X, fun_control=None)</code>","text":"<p>Cubed function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_cubed(X)\narray([ 0., 27.])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_cubed(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Cubed function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_cubed(X)\n        array([ 0., 27.])\n    \"\"\"\n\n    if fun_control is None:\n        fun_control = self.fun_control\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array(X)\n\n    if len(X.shape) &lt; 2:\n        X = np.array([X])\n    offset = np.ones(X.shape[1]) * self.offset\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, np.sum((X[i] - offset) ** 3))\n    # TODO: move to a separate function:\n    if fun_control[\"sigma\"] &gt; 0:\n        # Use own rng:\n        if fun_control[\"seed\"] is not None:\n            rng = default_rng(seed=fun_control[\"seed\"])\n        # Use class rng:\n        else:\n            rng = self.rng\n        noise_y = np.array([], dtype=float)\n        for i in y:\n            # noise_y = np.append(\n            #     noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1)\n            noise_y = np.append(noise_y, i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n        return noise_y\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_forrester","title":"<code>fun_forrester(X, fun_control=None)</code>","text":"<p>Forrester function. Function used by [Forr08a, p.83].    f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].    Starts with three sample points at x=0, x=0.5, and x=1.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:     np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_forrester(X)\narray([  0.        ,  11.99999999])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_forrester(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Forrester function. Function used by [Forr08a, p.83].\n       f(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].\n       Starts with three sample points at x=0, x=0.5, and x=1.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_forrester(X)\n        array([  0.        ,  11.99999999])\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array(X)\n\n    if len(X.shape) &lt; 2:\n        X = np.array([X])\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, (6.0 * X[i] - 2) ** 2 * np.sin(12 * X[i] - 4))\n    # TODO: move to a separate function:\n    if fun_control[\"sigma\"] &gt; 0:\n        # Use own rng:\n        if fun_control[\"seed\"] is not None:\n            rng = default_rng(seed=fun_control[\"seed\"])\n        # Use class rng:\n        else:\n            rng = self.rng\n        noise_y = np.array([], dtype=float)\n        for i in y:\n            # noise_y = np.append(\n            #     noise_y, i + np.random.normal(loc=0, scale=self.sigma, size=1)\n            noise_y = np.append(noise_y, i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n        return noise_y\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_linear","title":"<code>fun_linear(X, fun_control=None)</code>","text":"<p>Linear function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_linear(X)\narray([ 6., 15.])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_linear(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Linear function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_linear(X)\n        array([ 6., 15.])\n\n    \"\"\"\n    if fun_control is not None:\n        self.fun_control = fun_control\n    try:\n        X.shape[1]\n    except ValueError as err:\n        print(\"error message:\", err)\n        X = np.array(X)\n\n    if len(X.shape) &lt; 2:\n        X = np.array([X])\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, np.sum(X[i]))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_random_error","title":"<code>fun_random_error(X, fun_control=None)</code>","text":"<p>Return errors for testing spot stability. Args:     X (array): input     fun_control (dict): dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed). Returns:     np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples:     &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical     &gt;&gt;&gt; import numpy as np     &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])     &gt;&gt;&gt; fun = analytical()     &gt;&gt;&gt; fun.fun_random_error(X)     array([24,  0])</p> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_random_error(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Return errors for testing spot stability.\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_random_error(X)\n        array([24,  0])\n\n    \"\"\"\n    if fun_control is not None:\n        self.fun_control = fun_control\n    try:\n        X.shape[1]\n    except ValueError as err:\n        print(\"error message:\", err)\n        X = np.array(X)\n    if len(X.shape) &lt; 2:\n        X = np.array([X])\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        # provoke error:\n        if random() &lt; 0.1:\n            y = np.append(y, np.nan)\n        else:\n            y = np.append(y, np.sum(X[i]))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        print(y)\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_rosen","title":"<code>fun_rosen(X, fun_control=None)</code>","text":"<p>Rosenbrock function. Args:     X (array): input     fun_control (dict): dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed). Returns:     np.ndarray: A 1D numpy array with shape (n,) containing the calculated values. Examples:     &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical     &gt;&gt;&gt; import numpy as np     &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])     &gt;&gt;&gt; fun = analytical()     &gt;&gt;&gt; fun.fun_rosen(X)     array([24,  0])</p> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_rosen(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Rosenbrock function.\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2,], [4, 5 ]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_rosen(X)\n        array([24,  0])\n    \"\"\"\n\n    if fun_control is None:\n        fun_control = self.fun_control\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != 2:\n        raise Exception\n    x0 = X[:, 0]\n    x1 = X[:, 1]\n    b = 10\n    y = (x0 - 1) ** 2 + b * (x1 - x0**2) ** 2\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_runge","title":"<code>fun_runge(X, fun_control=None)</code>","text":"<p>Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k &gt;= 1.    Interval: -5 &lt;= x &lt;= 5</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_runge(X)\narray([0.0625    , 0.015625  , 0.00390625])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_runge(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Runge function. Formula: f(x) = 1/ (1 + sum(x_i) - offset)^2. Dim: k &gt;= 1.\n       Interval: -5 &lt;= x &lt;= 5\n\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_runge(X)\n        array([0.0625    , 0.015625  , 0.00390625])\n\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array(X)\n\n    if len(X.shape) &lt; 2:\n        X = np.array([X])\n    offset = np.ones(X.shape[1]) * self.offset\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, (1 / (1 + np.sum((X[i] - offset) ** 2))))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_sin_cos","title":"<code>fun_sin_cos(X, fun_control=None)</code>","text":"<p>Sinusoidal function. Args:     X (array):         input     fun_control (dict):         dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_sin_cos(X)\narray([-1.        , -0.41614684])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_sin_cos(self, X, fun_control=None):\n    \"\"\"Sinusoidal function.\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        (np.ndarray): A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_sin_cos(X)\n        array([-1.        , -0.41614684])\n    \"\"\"\n\n    if fun_control is None:\n        fun_control = self.fun_control\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array([X])\n    if X.shape[1] != 2:\n        raise Exception\n    x0 = X[:, 0]\n    x1 = X[:, 1]\n    y = 2.0 * np.sin(x0 + self.hz) + 0.5 * np.cos(x1 + self.hz)\n    # TODO: move to a separate function:\n    if fun_control[\"sigma\"] &gt; 0:\n        # Use own rng:\n        if fun_control[\"seed\"] is not None:\n            rng = default_rng(seed=fun_control[\"seed\"])\n        # Use class rng:\n        else:\n            rng = self.rng\n        noise_y = np.array([], dtype=float)\n        for i in y:\n            noise_y = np.append(noise_y, i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n        return noise_y\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_sphere","title":"<code>fun_sphere(X, fun_control=None)</code>","text":"<p>Sphere function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_sphere(X)\narray([14., 77.])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_sphere(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Sphere function.\n\n    Args:\n        X (array):\n            input\n        fun_control (dict):\n            dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_sphere(X)\n        array([14., 77.])\n\n    \"\"\"\n    if fun_control is not None:\n        self.fun_control = fun_control\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array(X)\n\n    if len(X.shape) &lt; 2:\n        X = np.array([X])\n    offset = np.ones(X.shape[1]) * self.offset\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, np.sum((X[i] - offset) ** 2))\n    # TODO: move to a separate function:\n    if self.fun_control[\"sigma\"] &gt; 0:\n        # Use own rng:\n        if self.fun_control[\"seed\"] is not None:\n            rng = default_rng(seed=fun_control[\"seed\"])\n        # Use class rng:\n        else:\n            rng = self.rng\n        noise_y = np.array([], dtype=float)\n        for y_i in y:\n            noise_y = np.append(noise_y, y_i + rng.normal(loc=0, scale=fun_control[\"sigma\"], size=1))\n        return noise_y\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_wingwt","title":"<code>fun_wingwt(X, fun_control=None)</code>","text":"<p>Wing weight function. Example from Forrester et al. to understand the weight     of an unpainted light aircraft wing as a function of nine design and operational parameters:     W = 0.036 S_W0.758 * Wfw0.0035 ( A / (cos2 Lambda))0.6 *     q0.006  * lambda0.04 * ( (100 Rtc)/(cos Lambda) ))-0.3*(Nz Wdg)0.49</p> Symbol Parameter Baseline Minimum Maximum $S_W$ Wing area ($ft^2$) 174 150 200 $W_{fw}$ Weight of fuel in wing (lb) 252 220 300 $A$ Aspect ratio 7.52 6 10 $Lambda$ Quarter-chord sweep (deg) 0 -10 10 $q$ Dynamic pressure at cruise ($lb/ft^2$) 34 16 45 $lambda$ Taper ratio 0.672 0.5 1 $R_{tc}$ Aerofoil thickness to chord ratio 0.12 0.08 0.18 $N_z$ Ultimate load factor 3.8 2.5 6 $W_{dg}$ Flight design gross weight (lb) 2000 1700 2500 $W_p$ paint weight (lb/ft^2) 0.064 0.025 0.08 <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>input</p> required <code>fun_control</code> <code>dict</code> <p>dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_wingwt(X)\narray([0.0625    , 0.015625  , 0.00390625])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_wingwt(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Wing weight function. Example from Forrester et al. to understand the weight\n        of an unpainted light aircraft wing as a function of nine design and operational parameters:\n        W = 0.036 S_W**0.758 * Wfw**0.0035 ( A / (cos**2 Lambda))**0.6 *\n        q**0.006  * lambda**0.04 * ( (100 Rtc)/(cos Lambda) ))**-0.3*(Nz Wdg)**0.49\n\n    | Symbol    | Parameter                              | Baseline | Minimum | Maximum |\n    |-----------|----------------------------------------|----------|---------|---------|\n    | $S_W$     | Wing area ($ft^2$)                     | 174      | 150     | 200     |\n    | $W_{fw}$  | Weight of fuel in wing (lb)            | 252      | 220     | 300     |\n    | $A$       | Aspect ratio                          | 7.52     | 6       | 10      |\n    | $Lambda$ | Quarter-chord sweep (deg)              | 0        | -10     | 10      |\n    | $q$       | Dynamic pressure at cruise ($lb/ft^2$) | 34       | 16      | 45      |\n    | $lambda$ | Taper ratio                            | 0.672    | 0.5     | 1       |\n    | $R_{tc}$  | Aerofoil thickness to chord ratio      | 0.12     | 0.08    | 0.18    |\n    | $N_z$     | Ultimate load factor                   | 3.8      | 2.5     | 6       |\n    | $W_{dg}$  | Flight design gross weight (lb)         | 2000     | 1700    | 2500    |\n    | $W_p$     | paint weight (lb/ft^2)                   | 0.064 |   0.025  | 0.08    |\n\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_wingwt(X)\n        array([0.0625    , 0.015625  , 0.00390625])\n\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array(X)\n    #\n    W_res = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        Sw = X[i, 0] * (200 - 150) + 150\n        Wfw = X[i, 1] * (300 - 220) + 220\n        A = X[i, 2] * (10 - 6) + 6\n        L = (X[i, 3] * (10 - (-10)) - 10) * np.pi / 180\n        q = X[i, 4] * (45 - 16) + 16\n        la = X[i, 5] * (1 - 0.5) + 0.5\n        Rtc = X[i, 6] * (0.18 - 0.08) + 0.08\n        Nz = X[i, 7] * (6 - 2.5) + 2.5\n        Wdg = X[i, 8] * (2500 - 1700) + 1700\n        Wp = X[i, 9] * (0.08 - 0.025) + 0.025\n        # calculation on natural scale\n        W = 0.036 * Sw**0.758 * Wfw**0.0035 * (A / np.cos(L) ** 2) ** 0.6 * q**0.006\n        W = W * la**0.04 * (100 * Rtc / np.cos(L)) ** (-0.3) * (Nz * Wdg) ** (0.49) + Sw * Wp\n        W_res = np.append(W_res, W)\n    return W_res\n</code></pre>"},{"location":"reference/spotPython/fun/objectivefunctions/#spotPython.fun.objectivefunctions.analytical.fun_xsin","title":"<code>fun_xsin(X, fun_control=None)</code>","text":"<p>Example function. Args:     X (array): input     fun_control (dict): dict with entries <code>sigma</code> (noise level) and <code>seed</code> (random seed).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n&gt;&gt;&gt; fun = analytical()\n&gt;&gt;&gt; fun.fun_xsin(X)\narray([0.84147098, 0.90929743, 0.14112001])\n</code></pre> Source code in <code>spotPython/fun/objectivefunctions.py</code> <pre><code>def fun_xsin(self, X: np.ndarray, fun_control: Optional[Dict] = None) -&gt; np.ndarray:\n    \"\"\"Example function.\n    Args:\n        X (array): input\n        fun_control (dict): dict with entries `sigma` (noise level) and `seed` (random seed).\n\n    Returns:\n        np.ndarray: A 1D numpy array with shape (n,) containing the calculated values.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.fun.objectivefunctions import analytical\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9, 10, 11, 12]])\n        &gt;&gt;&gt; fun = analytical()\n        &gt;&gt;&gt; fun.fun_xsin(X)\n        array([0.84147098, 0.90929743, 0.14112001])\n\n    \"\"\"\n    if fun_control is None:\n        fun_control = self.fun_control\n    try:\n        X.shape[1]\n    except ValueError:\n        X = np.array(X)\n\n    if len(X.shape) &lt; 2:\n        X = np.array([X])\n    y = np.array([], dtype=float)\n    for i in range(X.shape[0]):\n        y = np.append(y, X[i] * np.sin(1.0 / X[i]))\n    if self.fun_control[\"sigma\"] &gt; 0:\n        return self.add_noise(y)\n    else:\n        return y\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/categorical/","title":"categorical","text":""},{"location":"reference/spotPython/hyperparameters/categorical/#spotPython.hyperparameters.categorical.add_missing_elements","title":"<code>add_missing_elements(a, b)</code>","text":"<p>Add missing elements from list a to list b. Arguments:     a (list): List of elements to check.     b (list): List of elements to add to.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of elements with missing elements from list a added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; a = [1, 4]\n    b = [1, 2]\n    add_missing_elements(a, b)\n    [1, 2, 4]\n</code></pre> Source code in <code>spotPython/hyperparameters/categorical.py</code> <pre><code>def add_missing_elements(a: list, b: list) -&gt; list:\n    \"\"\"Add missing elements from list a to list b.\n    Arguments:\n        a (list): List of elements to check.\n        b (list): List of elements to add to.\n\n    Returns:\n        list: List of elements with missing elements from list a added.\n\n    Examples:\n        &gt;&gt;&gt; a = [1, 4]\n            b = [1, 2]\n            add_missing_elements(a, b)\n            [1, 2, 4]\n    \"\"\"\n    for element in a:\n        if element not in b:\n            b.append(element)\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/categorical/#spotPython.hyperparameters.categorical.find_closest_key","title":"<code>find_closest_key(integer_value, encoding_dict)</code>","text":"<p>Given an integer value and an encoding dictionary that maps keys to binary values, this function finds the key in the dictionary whose binary value is closest to the binary representation of the integer value.</p> <p>Parameters:</p> Name Type Description Default <code>integer_value</code> <code>int</code> <p>The integer value to find the closest key for.</p> required <code>encoding_dict</code> <code>dict</code> <p>The encoding dictionary that maps keys to binary values.</p> required <p>Returns:     str: The key in the encoding dictionary whose binary value is     closest to the binary representation of the integer value. Examples:     &gt;&gt;&gt; encoding_dict = {\u2018A\u2019: [1, 0, 0], \u2018B\u2019: [0, 1, 0], \u2018C\u2019: [0, 0, 1]}         find_closest_key(6, encoding_dict)         \u2018B\u2019</p> Source code in <code>spotPython/hyperparameters/categorical.py</code> <pre><code>def find_closest_key(integer_value: int, encoding_dict: dict) -&gt; str:\n    \"\"\"\n    Given an integer value and an encoding dictionary that maps keys to binary values,\n    this function finds the key in the dictionary whose binary value is closest to the binary\n    representation of the integer value.\n\n    Arguments:\n        integer_value (int): The integer value to find the closest key for.\n        encoding_dict (dict): The encoding dictionary that maps keys to binary values.\n    Returns:\n        str: The key in the encoding dictionary whose binary value is\n        closest to the binary representation of the integer value.\n    Examples:\n        &gt;&gt;&gt; encoding_dict = {'A': [1, 0, 0], 'B': [0, 1, 0], 'C': [0, 0, 1]}\n            find_closest_key(6, encoding_dict)\n            'B'\n    \"\"\"\n    binary_value = [int(x) for x in format(integer_value, f\"0{len(list(encoding_dict.values())[0])}b\")]\n    min_distance = float(\"inf\")\n    closest_key = None\n    for key, encoded_value in encoding_dict.items():\n        distance = sum([x != y for x, y in zip(binary_value, encoded_value)])\n        if distance &lt; min_distance:\n            min_distance = distance\n            closest_key = key\n    return closest_key\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/categorical/#spotPython.hyperparameters.categorical.get_one_hot","title":"<code>get_one_hot(alg, hyper_param, d=None, filename='data.json')</code>","text":"<p>Get one hot encoded values for a hyper parameter of an algorithm. Arguments:     alg (str): Name of the algorithm.     hyper_param (str): Name of the hyper parameter.     d (dict): Dictionary of algorithms and their hyperparameters.     filename (str): Name of the file containing the dictionary. Returns:     dict: Dictionary of hyper parameter values and their one hot encoded values. Examples:     &gt;&gt;&gt; alg = \u201cHoeffdingAdaptiveTreeClassifier\u201d         hyper_param = \u201csplit_criterion\u201d         d = {             \u201cHoeffdingAdaptiveTreeClassifier\u201d: {                 \u201csplit_criterion\u201d: [\u201cgini\u201d, \u201cinfo_gain\u201d, \u201chellinger\u201d],                 \u201cleaf_prediction\u201d: [\u201cmc\u201d, \u201cnb\u201d, \u201cnba\u201d],                 \u201cbootstrap_sampling\u201d: [\u201c0\u201d, \u201c1\u201d]                 },                 \u201cHoeffdingTreeClassifier\u201d: {                     \u201csplit_criterion\u201d: [\u201cgini\u201d, \u201cinfo_gain\u201d, \u201chellinger\u201d],                     \u201cleaf_prediction\u201d: [\u201cmc\u201d, \u201cnb\u201d, \u201cnba\u201d],                     \u201cbinary_split\u201d: [\u201c0\u201d, \u201c1\u201d],                     \u201cstop_mem_management\u201d: [\u201c0\u201d, \u201c1\u201d]                 }             }         get_one_hot(alg, hyper_param, d)         {\u2018gini\u2019: [1, 0, 0], \u2018info_gain\u2019: [0, 1, 0], \u2018hellinger\u2019: [0, 0, 1]}</p> Source code in <code>spotPython/hyperparameters/categorical.py</code> <pre><code>def get_one_hot(alg: str, hyper_param: str, d: dict = None, filename: str = \"data.json\") -&gt; dict:\n    \"\"\"Get one hot encoded values for a hyper parameter of an algorithm.\n    Arguments:\n        alg (str): Name of the algorithm.\n        hyper_param (str): Name of the hyper parameter.\n        d (dict): Dictionary of algorithms and their hyperparameters.\n        filename (str): Name of the file containing the dictionary.\n    Returns:\n        dict: Dictionary of hyper parameter values and their one hot encoded values.\n    Examples:\n        &gt;&gt;&gt; alg = \"HoeffdingAdaptiveTreeClassifier\"\n            hyper_param = \"split_criterion\"\n            d = {\n                \"HoeffdingAdaptiveTreeClassifier\": {\n                    \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"],\n                    \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"],\n                    \"bootstrap_sampling\": [\"0\", \"1\"]\n                    },\n                    \"HoeffdingTreeClassifier\": {\n                        \"split_criterion\": [\"gini\", \"info_gain\", \"hellinger\"],\n                        \"leaf_prediction\": [\"mc\", \"nb\", \"nba\"],\n                        \"binary_split\": [\"0\", \"1\"],\n                        \"stop_mem_management\": [\"0\", \"1\"]\n                    }\n                }\n            get_one_hot(alg, hyper_param, d)\n            {'gini': [1, 0, 0], 'info_gain': [0, 1, 0], 'hellinger': [0, 0, 1]}\n    \"\"\"\n    if d is None:\n        with open(filename, \"r\") as f:\n            d = json.load(f)\n    values = d[alg][hyper_param]\n    one_hot_encoded_values = one_hot_encode(values)\n    return one_hot_encoded_values\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/categorical/#spotPython.hyperparameters.categorical.one_hot_encode","title":"<code>one_hot_encode(strings)</code>","text":"<p>One hot encode a list of strings. Arguments:     strings (list): List of strings to encode. Returns:     dict: Dictionary of strings and their one hot encoded values. Examples:     &gt;&gt;&gt; one_hot_encode([\u2018a\u2019, \u2018b\u2019, \u2018c\u2019])     {\u2018a\u2019: [1, 0, 0], \u2018b\u2019: [0, 1, 0], \u2018c\u2019: [0, 0, 1]}</p> Source code in <code>spotPython/hyperparameters/categorical.py</code> <pre><code>def one_hot_encode(strings) -&gt; dict:\n    \"\"\"One hot encode a list of strings.\n    Arguments:\n        strings (list): List of strings to encode.\n    Returns:\n        dict: Dictionary of strings and their one hot encoded values.\n    Examples:\n        &gt;&gt;&gt; one_hot_encode(['a', 'b', 'c'])\n        {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]}\n    \"\"\"\n    n = len(strings)\n    encoding_dict = {}\n    for i, string in enumerate(strings):\n        one_hot_encoded_value = [0] * n\n        one_hot_encoded_value[i] = 1\n        encoding_dict[string] = one_hot_encoded_value\n    return encoding_dict\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/categorical/#spotPython.hyperparameters.categorical.sum_encoded_values","title":"<code>sum_encoded_values(strings, encoding_dict)</code>","text":"<p>Sum the encoded values of a list of strings.</p> <p>Parameters:</p> Name Type Description Default <code>strings</code> <code>list</code> <p>List of strings to encode.</p> required <code>encoding_dict</code> <code>dict</code> <p>Dictionary of strings and their one hot encoded values.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Decimal value of the sum of the encoded values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; encoding_dict = {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]}\n    sum_encoded_values(['a', 'b', 'c'], encoding_dict)\n    7\n    sum_encoded_values(['a', 'c'], encoding_dict)\n    5\n</code></pre> Source code in <code>spotPython/hyperparameters/categorical.py</code> <pre><code>def sum_encoded_values(strings, encoding_dict) -&gt; int:\n    \"\"\"Sum the encoded values of a list of strings.\n\n    Args:\n        strings (list): List of strings to encode.\n        encoding_dict (dict): Dictionary of strings and their one hot encoded values.\n\n    Returns:\n        int: Decimal value of the sum of the encoded values.\n\n    Examples:\n        &gt;&gt;&gt; encoding_dict = {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]}\n            sum_encoded_values(['a', 'b', 'c'], encoding_dict)\n            7\n            sum_encoded_values(['a', 'c'], encoding_dict)\n            5\n    \"\"\"\n    result = [0] * len(list(encoding_dict.values())[0])\n    for string in strings:\n        encoded_value = encoding_dict.get(string)\n        if encoded_value:\n            result = [sum(x) for x in zip(result, encoded_value)]\n    decimal_result = 0\n    for i, value in enumerate(result[::-1]):\n        decimal_result += value * (2**i)\n    return decimal_result\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/optimizer/","title":"optimizer","text":""},{"location":"reference/spotPython/hyperparameters/optimizer/#spotPython.hyperparameters.optimizer.optimizer_handler","title":"<code>optimizer_handler(optimizer_name, params, lr_mult=1.0, **kwargs)</code>","text":"<p>Returns an instance of the specified optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_name</code> <code>str</code> <p>The name of the optimizer to use.</p> required <code>params</code> <code>list or Tensor</code> <p>The parameters to optimize.</p> required <code>lr_mult</code> <code>float</code> <p>A multiplier for the learning rate. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the optimizer.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optimizer</code> <p>An instance of the specified optimizer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = torch.nn.Linear(10, 1)\n&gt;&gt;&gt; optimizer = optimizer_handler(\"Adadelta\", model.parameters(), lr_mult=0.5)\n&gt;&gt;&gt; print(optimizer)\nAdadelta (\n    Parameter Group 0\n        eps: 1e-06\n        lr: 0.5\n        rho: 0.9\n        weight_decay: 0\n)\n</code></pre> Source code in <code>spotPython/hyperparameters/optimizer.py</code> <pre><code>def optimizer_handler(\n    optimizer_name: str, params: Union[list, torch.Tensor], lr_mult: float = 1.0, **kwargs: Any\n) -&gt; torch.optim.Optimizer:\n    \"\"\"Returns an instance of the specified optimizer.\n\n    Args:\n        optimizer_name (str): The name of the optimizer to use.\n        params (list or torch.Tensor): The parameters to optimize.\n        lr_mult (float, optional): A multiplier for the learning rate. Defaults to 1.0.\n        **kwargs: Additional keyword arguments for the optimizer.\n\n    Returns:\n        (torch.optim.Optimizer):\n            An instance of the specified optimizer.\n\n    Examples:\n        &gt;&gt;&gt; model = torch.nn.Linear(10, 1)\n        &gt;&gt;&gt; optimizer = optimizer_handler(\"Adadelta\", model.parameters(), lr_mult=0.5)\n        &gt;&gt;&gt; print(optimizer)\n        Adadelta (\n            Parameter Group 0\n                eps: 1e-06\n                lr: 0.5\n                rho: 0.9\n                weight_decay: 0\n        )\n    \"\"\"\n    if optimizer_name == \"Adadelta\":\n        return torch.optim.Adadelta(\n            params,\n            lr=lr_mult * 1.0,\n            rho=0.9,\n            eps=1e-06,\n            weight_decay=0,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"Adagrad\":\n        return torch.optim.Adagrad(\n            params,\n            lr=lr_mult * 0.01,\n            lr_decay=0,\n            weight_decay=0,\n            initial_accumulator_value=0,\n            eps=1e-10,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"Adam\":\n        return torch.optim.Adam(\n            params,\n            lr=lr_mult * 0.001,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            amsgrad=False,\n            foreach=None,\n            maximize=False,\n            capturable=False,\n            # differentiable=False,\n            fused=None,\n        )\n    elif optimizer_name == \"AdamW\":\n        return torch.optim.AdamW(\n            params,\n            lr=lr_mult * 0.001,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0.01,\n            amsgrad=False,\n            foreach=None,\n            maximize=False,\n            capturable=False,\n            # differentiable=False,\n            # fused=None,\n        )\n    elif optimizer_name == \"SparseAdam\":\n        return torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, maximize=False)\n    elif optimizer_name == \"Adamax\":\n        return torch.optim.Adamax(\n            params,\n            lr=lr_mult * 0.002,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"ASGD\":\n        return torch.optim.ASGD(\n            params,\n            lr=lr_mult * 0.01,\n            lambd=0.0001,\n            alpha=0.75,\n            t0=1000000.0,\n            weight_decay=0,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"LBFGS\":\n        return torch.optim.LBFGS(\n            params,\n            lr=lr_mult * 1,\n            max_iter=20,\n            max_eval=None,\n            tolerance_grad=1e-07,\n            tolerance_change=1e-09,\n            history_size=100,\n            line_search_fn=None,\n        )\n    elif optimizer_name == \"NAdam\":\n        return torch.optim.NAdam(\n            params,\n            lr=lr_mult * 0.002,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            momentum_decay=0.004,\n            foreach=None,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"RAdam\":\n        return torch.optim.RAdam(\n            params,\n            lr=0.001,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=0,\n            foreach=None,\n            # differentiable=False\n        )\n    elif optimizer_name == \"RMSprop\":\n        return torch.optim.RMSprop(\n            params,\n            lr=lr_mult * 0.01,\n            alpha=0.99,\n            eps=1e-08,\n            weight_decay=0,\n            momentum=0,\n            centered=False,\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"Rprop\":\n        return torch.optim.Rprop(\n            params,\n            lr=lr_mult * 0.01,\n            etas=(0.5, 1.2),\n            step_sizes=(1e-06, 50),\n            foreach=None,\n            maximize=False,\n            # differentiable=False,\n        )\n    elif optimizer_name == \"SGD\":\n        return torch.optim.SGD(\n            params,\n            lr=lr_mult * 1e-3,\n            momentum=0,\n            dampening=0,\n            weight_decay=0,\n            nesterov=False,\n            maximize=False,\n            foreach=None,\n            # differentiable=False,\n        )\n    else:\n        raise ValueError(f\"Optimizer {optimizer_name} not supported\")\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/","title":"values","text":""},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.add_core_model_to_fun_control","title":"<code>add_core_model_to_fun_control(core_model, fun_control, hyper_dict, filename=None)</code>","text":"<p>Add the core model to the function control dictionary. Args:     core_model (class):         The core model.     fun_control (dict):         The function control dictionary.     hyper_dict (dict):         The hyper parameter dictionary.     filename (str):         The name of the json file that contains the hyper parameter dictionary.         Optional. Default is None. Returns:     (dict):         The function control dictionary. Examples:     &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor         from spotRiver.data.river_hyper_dict import RiverHyperDict         fun_control = {}         add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,             fun_control=func_control,             hyper_dict=RiverHyperDict,             filename=None)</p> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def add_core_model_to_fun_control(core_model, fun_control, hyper_dict, filename=None) -&gt; dict:\n    \"\"\"Add the core model to the function control dictionary.\n    Args:\n        core_model (class):\n            The core model.\n        fun_control (dict):\n            The function control dictionary.\n        hyper_dict (dict):\n            The hyper parameter dictionary.\n        filename (str):\n            The name of the json file that contains the hyper parameter dictionary.\n            Optional. Default is None.\n    Returns:\n        (dict):\n            The function control dictionary.\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotRiver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n    \"\"\"\n    fun_control.update({\"core_model\": core_model})\n    if filename is None:\n        new_hyper_dict = hyper_dict().load()\n    else:\n        with open(filename, \"r\") as f:\n            new_hyper_dict = json.load(f)\n    hyper_dict().load()\n    fun_control.update({\"core_model_hyper_dict\": new_hyper_dict[core_model.__name__]})\n    var_type = get_var_type(fun_control)\n    var_name = get_var_name(fun_control)\n    fun_control.update({\"var_type\": var_type, \"var_name\": var_name})\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.assign_values","title":"<code>assign_values(X, var_list)</code>","text":"<p>This function takes an np.array X and a list of variable names as input arguments and returns a dictionary where the keys are the variable names and the values are assigned from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>A 2D numpy array where each column represents a variable.</p> required <code>var_list</code> <code>list</code> <p>A list of strings representing variable names.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are assigned from X.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotPython.utils.prepare import assign_values\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; var_list = ['a', 'b']\n&gt;&gt;&gt; result = assign_values(X, var_list)\n&gt;&gt;&gt; print(result)\n{'a': array([1, 3, 5]), 'b': array([2, 4, 6])}\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def assign_values(X: np.array, var_list: list) -&gt; dict:\n    \"\"\"\n    This function takes an np.array X and a list of variable names as input arguments\n    and returns a dictionary where the keys are the variable names and the values are assigned from X.\n\n    Args:\n        X (np.array):\n            A 2D numpy array where each column represents a variable.\n        var_list (list):\n            A list of strings representing variable names.\n\n    Returns:\n        dict:\n            A dictionary where keys are variable names and values are assigned from X.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotPython.utils.prepare import assign_values\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; var_list = ['a', 'b']\n        &gt;&gt;&gt; result = assign_values(X, var_list)\n        &gt;&gt;&gt; print(result)\n        {'a': array([1, 3, 5]), 'b': array([2, 4, 6])}\n    \"\"\"\n    result = {}\n    for i, var_name in enumerate(var_list):\n        result[var_name] = X[:, i]\n    return result\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.convert_keys","title":"<code>convert_keys(d, var_type)</code>","text":"<p>Convert values in a dictionary to integers based on a list of variable types.</p> <p>This function takes a dictionary <code>d</code> and a list of variable types <code>var_type</code> as arguments. For each key in the dictionary, if the corresponding entry in <code>var_type</code> is not equal to <code>\"num\"</code>, the value associated with that key is converted to an integer.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>The input dictionary.</p> required <code>var_type</code> <code>list</code> <p>A list of variable types. If the entry is not <code>\"num\"</code> the corresponding value will be converted to the type <code>\"int\"</code>.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Union[int, float]]</code> <p>The modified dictionary with values converted to integers based on <code>var_type</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.utils.prepare import convert_keys\n&gt;&gt;&gt; d = {'a': '1.1', 'b': '2', 'c': '3.1'}\n&gt;&gt;&gt; var_type = [\"int\", \"num\", \"int\"]\n&gt;&gt;&gt; convert_keys(d, var_type)\n{'a': 1, 'b': '2', 'c': 3}\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def convert_keys(d: Dict[str, Union[int, float, str]], var_type: List[str]) -&gt; Dict[str, Union[int, float]]:\n    \"\"\"Convert values in a dictionary to integers based on a list of variable types.\n\n    This function takes a dictionary `d` and a list of variable types `var_type` as arguments.\n    For each key in the dictionary,\n    if the corresponding entry in `var_type` is not equal to `\"num\"`,\n    the value associated with that key is converted to an integer.\n\n    Args:\n        d (dict): The input dictionary.\n        var_type (list):\n            A list of variable types. If the entry is not `\"num\"` the corresponding\n            value will be converted to the type `\"int\"`.\n\n    Returns:\n        dict: The modified dictionary with values converted to integers based on `var_type`.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.prepare import convert_keys\n        &gt;&gt;&gt; d = {'a': '1.1', 'b': '2', 'c': '3.1'}\n        &gt;&gt;&gt; var_type = [\"int\", \"num\", \"int\"]\n        &gt;&gt;&gt; convert_keys(d, var_type)\n        {'a': 1, 'b': '2', 'c': 3}\n    \"\"\"\n    keys = list(d.keys())\n    for i in range(len(keys)):\n        if var_type[i] not in [\"num\", \"float\"]:\n            d[keys[i]] = int(d[keys[i]])\n    return d\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.generate_one_config_from_var_dict","title":"<code>generate_one_config_from_var_dict(var_dict, fun_control)</code>","text":"<p>Generate one configuration from a dictionary of variables (as a generator).</p> <p>This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d.</p> required <p>Returns:</p> Type Description <code>Generator[Dict[str, Union[int, float]], None, None]</code> <p>Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotPython.utils.prepare import generate_one_config_from_var_dict\n&gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n&gt;&gt;&gt; fun_control = {\"var_type\": [\"int\", \"num\"]}\n&gt;&gt;&gt; list(generate_one_config_from_var_dict(var_dict, fun_control))\n[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def generate_one_config_from_var_dict(\n    var_dict: Dict[str, np.ndarray], fun_control: Dict[str, Union[List[str], str]]\n) -&gt; Generator[Dict[str, Union[int, float]], None, None]:\n    \"\"\"Generate one configuration from a dictionary of variables (as a generator).\n\n    This function takes a dictionary of variables as input arguments and returns a generator\n    that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Args:\n        var_dict (dict): A dictionary where keys are variable names and values are numpy arrays.\n        fun_control (dict): A dictionary which (at least) has an entry with the following key:\n            \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding\n            value will be converted to the type \"int\".\n\n    Returns:\n        Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotPython.utils.prepare import generate_one_config_from_var_dict\n        &gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n        &gt;&gt;&gt; fun_control = {\"var_type\": [\"int\", \"num\"]}\n        &gt;&gt;&gt; list(generate_one_config_from_var_dict(var_dict, fun_control))\n        [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n    \"\"\"\n    for values in iterate_dict_values(var_dict):\n        values = convert_keys(values, fun_control[\"var_type\"])\n        values = get_dict_with_levels_and_types(fun_control=fun_control, v=values)\n        values = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values)\n        yield values\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_bound_values","title":"<code>get_bound_values(fun_control, bound, as_list=False)</code>","text":"<p>Generate a list or array from a dictionary.</p> <p>This function takes the values from the keys \u201cbound\u201d in the fun_control[\u201ccore_model_hyper_dict\u201d] dictionary and returns a list or array of the values in the same order as the keys in the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>A dictionary containing a key \u201ccore_model_hyper_dict\u201d which is a dictionary with keys that have either an \u201cupper\u201d or \u201clower\u201d value.</p> required <code>bound</code> <code>str</code> <p>Either \u201cupper\u201d or \u201clower\u201d, indicating which value to extract from the inner dictionary.</p> required <code>as_list</code> <code>bool</code> <p>If True, return a list. If False, return a numpy array. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List, ndarray]</code> <p>list or np.ndarray: A list or array of the extracted values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If bound is not \u201cupper\u201d or \u201clower\u201d.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.utils.prepare import get_bound_values\n&gt;&gt;&gt; fun_control = {\"core_model_hyper_dict\": {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}}}\n&gt;&gt;&gt; get_bound_values(fun_control, \"upper\", as_list=True)\n[1, 2]\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_bound_values(fun_control: dict, bound: str, as_list: bool = False) -&gt; Union[List, np.ndarray]:\n    \"\"\"Generate a list or array from a dictionary.\n\n    This function takes the values from the keys \"bound\" in the\n    fun_control[\"core_model_hyper_dict\"] dictionary and returns a list or array of the values\n    in the same order as the keys in the dictionary.\n\n    Args:\n        fun_control (dict):\n            A dictionary containing a key \"core_model_hyper_dict\"\n            which is a dictionary with keys that have either an \"upper\" or \"lower\" value.\n        bound (str):\n            Either \"upper\" or \"lower\",\n            indicating which value to extract from the inner dictionary.\n        as_list (bool):\n            If True, return a list.\n            If False, return a numpy array. Default is False.\n\n    Returns:\n        list or np.ndarray:\n            A list or array of the extracted values.\n\n    Raises:\n        ValueError:\n            If bound is not \"upper\" or \"lower\".\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.prepare import get_bound_values\n        &gt;&gt;&gt; fun_control = {\"core_model_hyper_dict\": {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}}}\n        &gt;&gt;&gt; get_bound_values(fun_control, \"upper\", as_list=True)\n        [1, 2]\n    \"\"\"\n    # Throw value error if bound is not upper or lower:\n    if bound not in [\"upper\", \"lower\"]:\n        raise ValueError(\"bound must be either 'upper' or 'lower'\")\n    d = fun_control[\"core_model_hyper_dict\"]\n    b = []\n    for key, value in d.items():\n        b.append(value[bound])\n    if as_list:\n        return b\n    else:\n        return np.array(b)\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_default_hyperparameters_as_array","title":"<code>get_default_hyperparameters_as_array(fun_control)</code>","text":"<p>Get the default hyper parameters as array. Args:     fun_control (dict):         The function control dictionary.</p> <p>Returns:</p> Type Description <code>array</code> <p>The default hyper parameters as array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotRiver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    get_default_hyperparameters_as_array(fun_control)\n    array([0, 0, 0, 0, 0])\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_default_hyperparameters_as_array(fun_control) -&gt; np.array:\n    \"\"\"Get the default hyper parameters as array.\n    Args:\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (np.array):\n            The default hyper parameters as array.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotRiver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            get_default_hyperparameters_as_array(fun_control)\n            array([0, 0, 0, 0, 0])\n    \"\"\"\n    X0 = get_default_values(fun_control)\n    X0 = replace_levels_with_positions(fun_control[\"core_model_hyper_dict\"], X0)\n    X0 = get_values_from_dict(X0)\n    X0 = np.array([X0])\n    X0.shape[1]\n    return X0\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_default_hyperparameters_for_core_model","title":"<code>get_default_hyperparameters_for_core_model(fun_control)</code>","text":"<p>Get the default hyper parameters for the core model. Args:     fun_control (dict):         The function control dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The default hyper parameters for the core model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotRiver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    get_default_hyperparameters_for_core_model(fun_control)\n    {'leaf_prediction': 'mean',\n    'leaf_model': 'NBAdaptive',\n    'splitter': 'HoeffdingAdaptiveTreeSplitter',\n    'binary_split': 'info_gain',\n    'stop_mem_management': False}\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_default_hyperparameters_for_core_model(fun_control) -&gt; dict:\n    \"\"\"Get the default hyper parameters for the core model.\n    Args:\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (dict):\n            The default hyper parameters for the core model.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotRiver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            get_default_hyperparameters_for_core_model(fun_control)\n            {'leaf_prediction': 'mean',\n            'leaf_model': 'NBAdaptive',\n            'splitter': 'HoeffdingAdaptiveTreeSplitter',\n            'binary_split': 'info_gain',\n            'stop_mem_management': False}\n    \"\"\"\n    values = get_default_values(fun_control)\n    values = get_dict_with_levels_and_types(fun_control=fun_control, v=values)\n    values = convert_keys(values, fun_control[\"var_type\"])\n    values = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values)\n    return values\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_default_values","title":"<code>get_default_values(fun_control)</code>","text":"<p>Get the values from the \u201cdefault\u201d keys from the dictionary fun_control as a dict. If the key of the value has as \u201ctype\u201d the value \u201cint\u201d or \u201cfloat\u201d, convert the value to the corresponding type. Args:     fun_control (dict):         dictionary with levels and types Returns:     new_dict (dict):         dictionary with default values Examples:     &gt;&gt;&gt; from spotPython.utils.prepare import get_default_values         d = {\u201ccore_model_hyper_dict\u201d:{             \u201cleaf_prediction\u201d: {                 \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d],                 \u201ctype\u201d: \u201cfactor\u201d,                 \u201cdefault\u201d: \u201cmean\u201d,                 \u201ccore_model_parameter_type\u201d: \u201cstr\u201d},             \u201cleaf_model\u201d: {                 \u201clevels\u201d: [\u201clinear_model.LinearRegression\u201d, \u201clinear_model.PARegressor\u201d, \u201clinear_model.Perceptron\u201d],                 \u201ctype\u201d: \u201cfactor\u201d,                 \u201cdefault\u201d: \u201cLinearRegression\u201d,                 \u201ccore_model_parameter_type\u201d: \u201cinstance\u201d},             \u201csplitter\u201d: {                 \u201clevels\u201d: [\u201cEBSTSplitter\u201d, \u201cTEBSTSplitter\u201d, \u201cQOSplitter\u201d],                 \u201ctype\u201d: \u201cfactor\u201d,                 \u201cdefault\u201d: \u201cEBSTSplitter\u201d,                 \u201ccore_model_parameter_type\u201d: \u201cinstance()\u201d},             \u201cbinary_split\u201d: {                 \u201clevels\u201d: [0, 1],                 \u201ctype\u201d: \u201cfactor\u201d,                 \u201cdefault\u201d: 0,                 \u201ccore_model_parameter_type\u201d: \u201cbool\u201d},             \u201cstop_mem_management\u201d: {                 \u201clevels\u201d: [0, 1],                 \u201ctype\u201d: \u201cfactor\u201d,                 \u201cdefault\u201d: 0,                 \u201ccore_model_parameter_type\u201d: \u201cbool\u201d}}}         get_default_values_from_dict(d)         {\u2018leaf_prediction\u2019: \u2018mean\u2019,         \u2018leaf_model\u2019: \u2018linear_model.LinearRegression\u2019,         \u2018splitter\u2019: \u2018EBSTSplitter\u2019,         \u2018binary_split\u2019: 0,         \u2018stop_mem_management\u2019: 0}</p> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_default_values(fun_control) -&gt; dict:\n    \"\"\"Get the values from the \"default\" keys from the dictionary fun_control as a dict.\n    If the key of the value has as \"type\" the value \"int\" or \"float\", convert the value to the corresponding type.\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n    Returns:\n        new_dict (dict):\n            dictionary with default values\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.prepare import get_default_values\n            d = {\"core_model_hyper_dict\":{\n                \"leaf_prediction\": {\n                    \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                    \"type\": \"factor\",\n                    \"default\": \"mean\",\n                    \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}}\n            get_default_values_from_dict(d)\n            {'leaf_prediction': 'mean',\n            'leaf_model': 'linear_model.LinearRegression',\n            'splitter': 'EBSTSplitter',\n            'binary_split': 0,\n            'stop_mem_management': 0}\n    \"\"\"\n    d = fun_control[\"core_model_hyper_dict\"]\n    new_dict = {}\n    for key, value in d.items():\n        if value[\"type\"] == \"int\":\n            new_dict[key] = int(value[\"default\"])\n        elif value[\"type\"] == \"float\":\n            new_dict[key] = float(value[\"default\"])\n        else:\n            new_dict[key] = value[\"default\"]\n    return new_dict\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_dict_with_levels_and_types","title":"<code>get_dict_with_levels_and_types(fun_control, v)</code>","text":"<p>Get dictionary with levels and types.</p> <p>The function maps the numerical output of the hyperparameter optimization to the corresponding levels of the hyperparameter needed by the core model, i.e., the tuned algorithm. The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from fun_control. If the key value in the dictionary is 0, it takes the first value from the list, if it is 1, it takes the second and so on. If a key is not in fun_control, it takes the key from v. If the core_model_parameter_type value is instance, it returns the class of the value from the module via getattr(\u201cclass\u201d, value).</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>Dict[str, Any]</code> <p>A dictionary containing information about the core model hyperparameters.</p> required <code>v</code> <code>Dict[str, Any]</code> <p>A dictionary containing the numerical output of the hyperparameter optimization.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fun_control = {\n...     \"core_model_hyper_dict\": {\n...         \"leaf_prediction\": {\n...             \"levels\": [\"mean\", \"model\", \"adaptive\"],\n...             \"type\": \"factor\",\n...             \"default\": \"mean\",\n...             \"core_model_parameter_type\": \"str\"\n...         },\n...         \"leaf_model\": {\n...             \"levels\": [\n...                 \"linear_model.LinearRegression\",\n...                 \"linear_model.PARegressor\",\n...                 \"linear_model.Perceptron\"\n...             ],\n...             \"type\": \"factor\",\n...             \"default\": \"LinearRegression\",\n...             \"core_model_parameter_type\": \"instance\"\n...         },\n...         \"splitter\": {\n...             \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n...             \"type\": \"factor\",\n...             \"default\": \"EBSTSplitter\",\n...             \"core_model_parameter_type\": \"instance()\"\n...         },\n...         \"binary_split\": {\n...             \"levels\": [0, 1],\n...             \"type\": \"factor\",\n...             \"default\": 0,\n...             \"core_model_parameter_type\": \"bool\"\n...         },\n...         \"stop_mem_management\": {\n...             \"levels\": [0, 1],\n...             \"type\": \"factor\",\n...             \"default\": 0,\n...             \"core_model_parameter_type\": \"bool\"\n...         }\n...     }\n... }\n&gt;&gt;&gt; v = {\n...     'grace_period': 200,\n...     'max_depth': 10,\n...     'delta': 1e-07,\n...     'tau': 0.05,\n...     'leaf_prediction': 0,\n...     'leaf_model': 0,\n...     'model_selector_decay': 0.95,\n...     'splitter': 1,\n...     'min_samples_split': 9,\n...     'binary_split': 0,\n...     'max_size': 500.0\n... }\n&gt;&gt;&gt; get_dict_with_levels_and_types(fun_control, v)\n{\n    'grace_period': 200,\n    'max_depth': 10,\n    'delta': 1e-07,\n    'tau': 0.05,\n    'leaf_prediction': 'mean',\n    'leaf_model': linear_model.LinearRegression,\n    'model_selector_decay': 0.95,\n    'splitter': TEBSTSplitter,\n    'min_samples_split': 9,\n    'binary_split': False,\n    'max_size': 500.0\n}\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_dict_with_levels_and_types(fun_control: Dict[str, Any], v: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Get dictionary with levels and types.\n\n    The function maps the numerical output of the hyperparameter optimization to the corresponding levels\n    of the hyperparameter needed by the core model, i.e., the tuned algorithm.\n    The function takes the dictionaries fun_control and v and returns a new dictionary with the same keys as v\n    but with the values of the levels of the keys from fun_control.\n    If the key value in the dictionary is 0, it takes the first value from the list,\n    if it is 1, it takes the second and so on.\n    If a key is not in fun_control, it takes the key from v.\n    If the core_model_parameter_type value is instance, it returns the class of the value from the module\n    via getattr(\"class\", value).\n\n    Args:\n        fun_control (Dict[str, Any]): A dictionary containing information about the core model hyperparameters.\n        v (Dict[str, Any]): A dictionary containing the numerical output of the hyperparameter optimization.\n\n    Returns:\n        Dict[str, Any]:\n            A new dictionary with the same keys as v but with the values of the levels of the keys from fun_control.\n\n    Examples:\n        &gt;&gt;&gt; fun_control = {\n        ...     \"core_model_hyper_dict\": {\n        ...         \"leaf_prediction\": {\n        ...             \"levels\": [\"mean\", \"model\", \"adaptive\"],\n        ...             \"type\": \"factor\",\n        ...             \"default\": \"mean\",\n        ...             \"core_model_parameter_type\": \"str\"\n        ...         },\n        ...         \"leaf_model\": {\n        ...             \"levels\": [\n        ...                 \"linear_model.LinearRegression\",\n        ...                 \"linear_model.PARegressor\",\n        ...                 \"linear_model.Perceptron\"\n        ...             ],\n        ...             \"type\": \"factor\",\n        ...             \"default\": \"LinearRegression\",\n        ...             \"core_model_parameter_type\": \"instance\"\n        ...         },\n        ...         \"splitter\": {\n        ...             \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n        ...             \"type\": \"factor\",\n        ...             \"default\": \"EBSTSplitter\",\n        ...             \"core_model_parameter_type\": \"instance()\"\n        ...         },\n        ...         \"binary_split\": {\n        ...             \"levels\": [0, 1],\n        ...             \"type\": \"factor\",\n        ...             \"default\": 0,\n        ...             \"core_model_parameter_type\": \"bool\"\n        ...         },\n        ...         \"stop_mem_management\": {\n        ...             \"levels\": [0, 1],\n        ...             \"type\": \"factor\",\n        ...             \"default\": 0,\n        ...             \"core_model_parameter_type\": \"bool\"\n        ...         }\n        ...     }\n        ... }\n        &gt;&gt;&gt; v = {\n        ...     'grace_period': 200,\n        ...     'max_depth': 10,\n        ...     'delta': 1e-07,\n        ...     'tau': 0.05,\n        ...     'leaf_prediction': 0,\n        ...     'leaf_model': 0,\n        ...     'model_selector_decay': 0.95,\n        ...     'splitter': 1,\n        ...     'min_samples_split': 9,\n        ...     'binary_split': 0,\n        ...     'max_size': 500.0\n        ... }\n        &gt;&gt;&gt; get_dict_with_levels_and_types(fun_control, v)\n        {\n            'grace_period': 200,\n            'max_depth': 10,\n            'delta': 1e-07,\n            'tau': 0.05,\n            'leaf_prediction': 'mean',\n            'leaf_model': linear_model.LinearRegression,\n            'model_selector_decay': 0.95,\n            'splitter': TEBSTSplitter,\n            'min_samples_split': 9,\n            'binary_split': False,\n            'max_size': 500.0\n        }\n    \"\"\"\n    d = fun_control[\"core_model_hyper_dict\"]\n    new_dict = {}\n    for key, value in v.items():\n        if key in d and d[key][\"type\"] == \"factor\":\n            if d[key][\"core_model_parameter_type\"] == \"instance\":\n                if \"class_name\" in d[key]:\n                    mdl = d[key][\"class_name\"]\n                c = d[key][\"levels\"][value]\n                new_dict[key] = class_for_name(mdl, c)\n            elif d[key][\"core_model_parameter_type\"] == \"instance()\":\n                mdl = d[key][\"class_name\"]\n                c = d[key][\"levels\"][value]\n                k = class_for_name(mdl, c)\n                new_dict[key] = k()\n            else:\n                new_dict[key] = d[key][\"levels\"][value]\n        else:\n            new_dict[key] = v[key]\n    return new_dict\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_one_config_from_X","title":"<code>get_one_config_from_X(X, fun_control=None)</code>","text":"<p>Get one config from X. Args:     X (np.array):         The array with the hyper parameter values.     fun_control (dict):         The function control dictionary. Returns:     (dict):         The config dictionary. Examples:     &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor         from spotRiver.data.river_hyper_dict import RiverHyperDict         fun_control = {}         add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,             fun_control=func_control,             hyper_dict=RiverHyperDict,             filename=None)         X = np.array([0, 0, 0, 0, 0])         get_one_config_from_X(X, fun_control)         {\u2018leaf_prediction\u2019: \u2018mean\u2019,         \u2018leaf_model\u2019: \u2018NBAdaptive\u2019,         \u2018splitter\u2019: \u2018HoeffdingAdaptiveTreeSplitter\u2019,         \u2018binary_split\u2019: \u2018info_gain\u2019,         \u2018stop_mem_management\u2019: False}</p> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_one_config_from_X(X, fun_control=None):\n    \"\"\"Get one config from X.\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n    Returns:\n        (dict):\n            The config dictionary.\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotRiver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_config_from_X(X, fun_control)\n            {'leaf_prediction': 'mean',\n            'leaf_model': 'NBAdaptive',\n            'splitter': 'HoeffdingAdaptiveTreeSplitter',\n            'binary_split': 'info_gain',\n            'stop_mem_management': False}\n    \"\"\"\n    var_dict = assign_values(X, fun_control[\"var_name\"])\n    config = return_conf_list_from_var_dict(var_dict, fun_control)[0]\n    return config\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_one_core_model_from_X","title":"<code>get_one_core_model_from_X(X, fun_control=None)</code>","text":"<p>Get one core model from X. Args:     X (np.array):         The array with the hyper parameter values.     fun_control (dict):         The function control dictionary. Returns:     (class):         The core model. Examples:     &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor         from spotRiver.data.river_hyper_dict import RiverHyperDict         fun_control = {}         add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,             fun_control=func_control,             hyper_dict=RiverHyperDict,             filename=None)         X = np.array([0, 0, 0, 0, 0])         get_one_core_model_from_X(X, fun_control)         HoeffdingAdaptiveTreeRegressor()</p> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_one_core_model_from_X(X, fun_control=None):\n    \"\"\"Get one core model from X.\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n    Returns:\n        (class):\n            The core model.\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotRiver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_core_model_from_X(X, fun_control)\n            HoeffdingAdaptiveTreeRegressor()\n    \"\"\"\n    var_dict = assign_values(X, fun_control[\"var_name\"])\n    config = return_conf_list_from_var_dict(var_dict, fun_control)[0]\n    core_model = fun_control[\"core_model\"](**config)\n    return core_model\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_one_river_model_from_X","title":"<code>get_one_river_model_from_X(X, fun_control=None)</code>","text":"<p>Get one river model from X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>The array with the hyper parameter values.</p> required <code>fun_control</code> <code>dict</code> <p>The function control dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>class</code> <p>The river model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n    from spotRiver.data.river_hyper_dict import RiverHyperDict\n    fun_control = {}\n    add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n        fun_control=func_control,\n        hyper_dict=RiverHyperDict,\n        filename=None)\n    X = np.array([0, 0, 0, 0, 0])\n    get_one_river_model_from_X(X, fun_control)\n    HoeffdingAdaptiveTreeRegressor()\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_one_river_model_from_X(X, fun_control=None):\n    \"\"\"Get one river model from X.\n\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n\n    Returns:\n        (class):\n            The river model.\n\n    Examples:\n        &gt;&gt;&gt; from river.tree import HoeffdingAdaptiveTreeRegressor\n            from spotRiver.data.river_hyper_dict import RiverHyperDict\n            fun_control = {}\n            add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n                fun_control=func_control,\n                hyper_dict=RiverHyperDict,\n                filename=None)\n            X = np.array([0, 0, 0, 0, 0])\n            get_one_river_model_from_X(X, fun_control)\n            HoeffdingAdaptiveTreeRegressor()\n    \"\"\"\n    core_model = get_one_core_model_from_X(X=X, fun_control=fun_control)\n    if fun_control[\"prep_model\"] is not None:\n        model = compose.Pipeline(fun_control[\"prep_model\"], core_model)\n    else:\n        model = core_model\n    return model\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_one_sklearn_model_from_X","title":"<code>get_one_sklearn_model_from_X(X, fun_control=None)</code>","text":"<p>Get one sklearn model from X. Args:     X (np.array):         The array with the hyper parameter values.     fun_control (dict):         The function control dictionary. Returns:     (class):         The sklearn model. Examples:</p> <p>from sklearn.linear_model import LinearRegression     from spotRiver.data.sklearn_hyper_dict import SklearnHyperDict     fun_control = {}     add_core_model_to_fun_control(core_model=LinearRegression,         fun_control=func_control,         hyper_dict=SklearnHyperDict,         filename=None)     X = np.array([0, 0, 0, 0, 0])     get_one_sklearn_model_from_X(X, fun_control)     LinearRegression()</p> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_one_sklearn_model_from_X(X, fun_control=None):\n    \"\"\"Get one sklearn model from X.\n    Args:\n        X (np.array):\n            The array with the hyper parameter values.\n        fun_control (dict):\n            The function control dictionary.\n    Returns:\n        (class):\n            The sklearn model.\n    Examples:\n    &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        from spotRiver.data.sklearn_hyper_dict import SklearnHyperDict\n        fun_control = {}\n        add_core_model_to_fun_control(core_model=LinearRegression,\n            fun_control=func_control,\n            hyper_dict=SklearnHyperDict,\n            filename=None)\n        X = np.array([0, 0, 0, 0, 0])\n        get_one_sklearn_model_from_X(X, fun_control)\n        LinearRegression()\n    \"\"\"\n    core_model = get_one_core_model_from_X(X=X, fun_control=fun_control)\n    if fun_control[\"prep_model\"] is not None:\n        model = make_pipeline(fun_control[\"prep_model\"], core_model)\n    else:\n        model = core_model\n    return model\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_transform","title":"<code>get_transform(fun_control)</code>","text":"<p>Get the transformations of the values from the dictionary fun_control as a list. Args:     fun_control (dict):         dictionary with levels and types Returns:     (list):         list with transformations Examples:     &gt;&gt;&gt; from spotPython.utils.prepare import get_transform         d = {\u201ccore_model_hyper_dict\u201d:{         \u201cleaf_prediction\u201d: {             \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: \u201cmean\u201d,             \u201ctransform\u201d: \u201cNone\u201d,             \u201ccore_model_parameter_type\u201d: \u201cstr\u201d},         \u201cleaf_model\u201d: {             \u201clevels\u201d: [\u201clinear_model.LinearRegression\u201d, \u201clinear_model.PARegressor\u201d, \u201clinear_model.Perceptron\u201d],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: \u201cLinearRegression\u201d,             \u201ctransform\u201d: \u201cNone\u201d,             \u201ccore_model_parameter_type\u201d: \u201cinstance\u201d},         \u201csplitter\u201d: {             \u201clevels\u201d: [\u201cEBSTSplitter\u201d, \u201cTEBSTSplitter\u201d, \u201cQOSplitter\u201d],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: \u201cEBSTSplitter\u201d,             \u201ctransform\u201d: \u201cNone\u201d,             \u201ccore_model_parameter_type\u201d: \u201cinstance()\u201d},         \u201cbinary_split\u201d: {             \u201clevels\u201d: [0, 1],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: 0,             \u201ctransform\u201d: \u201cNone\u201d,             \u201ccore_model_parameter_type\u201d: \u201cbool\u201d},         \u201cstop_mem_management\u201d: {                                                         \u201clevels\u201d: [0, 1],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: 0,             \u201ctransform\u201d: \u201cNone\u201d,             \u201ccore_model_parameter_type\u201d: \u201cbool\u201d}}}</p> <pre><code>    get_transform(d)\n    ['None', 'None', 'None', 'None', 'None']\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_transform(fun_control) -&gt; list:\n    \"\"\"Get the transformations of the values from the dictionary fun_control as a list.\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n    Returns:\n        (list):\n            list with transformations\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.prepare import get_transform\n            d = {\"core_model_hyper_dict\":{\n            \"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"str\"},\n            \"leaf_model\": {\n                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                \"type\": \"factor\",\n                \"default\": \"LinearRegression\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"instance\"},\n            \"splitter\": {\n                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                \"type\": \"factor\",\n                \"default\": \"EBSTSplitter\",\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"instance()\"},\n            \"binary_split\": {\n                \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"bool\"},\n            \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"transform\": \"None\",\n                \"core_model_parameter_type\": \"bool\"}}}\n\n            get_transform(d)\n            ['None', 'None', 'None', 'None', 'None']\n    \"\"\"\n    return list(\n        fun_control[\"core_model_hyper_dict\"][key][\"transform\"] for key in fun_control[\"core_model_hyper_dict\"].keys()\n    )\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_values_from_dict","title":"<code>get_values_from_dict(dictionary)</code>","text":"<p>Get the values from a dictionary as an array. Generate an np.array that contains the values of the keys of a dictionary in the same order as the keys of the dictionary. Args:     dictionary (dict):         dictionary with values Returns:     (np.array):         array with values Examples:     &gt;&gt;&gt; from spotPython.utils.prepare import get_values_from_dict     &gt;&gt;&gt; d = {\u201ca\u201d: 1, \u201cb\u201d: 2, \u201cc\u201d: 3}     &gt;&gt;&gt; get_values_from_dict(d)     array([1, 2, 3])</p> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_values_from_dict(dictionary) -&gt; np.array:\n    \"\"\"Get the values from a dictionary as an array.\n    Generate an np.array that contains the values of the keys of a dictionary\n    in the same order as the keys of the dictionary.\n    Args:\n        dictionary (dict):\n            dictionary with values\n    Returns:\n        (np.array):\n            array with values\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.prepare import get_values_from_dict\n        &gt;&gt;&gt; d = {\"a\": 1, \"b\": 2, \"c\": 3}\n        &gt;&gt;&gt; get_values_from_dict(d)\n        array([1, 2, 3])\n    \"\"\"\n    return np.array(list(dictionary.values()))\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_var_name","title":"<code>get_var_name(fun_control)</code>","text":"<p>Get the names of the values from the dictionary fun_control as a list. Args:     fun_control (dict):         dictionary with names Returns:     (list):         ist with names Examples:     &gt;&gt;&gt; d = {\u201ccore_model_hyper_dict\u201d:{         \u201cleaf_prediction\u201d: {             \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: \u201cmean\u201d,             \u201ccore_model_parameter_type\u201d: \u201cstr\u201d},         \u201cleaf_model\u201d: {             \u201clevels\u201d: [\u201clinear_model.LinearRegression\u201d, \u201clinear_model.PARegressor\u201d, \u201clinear_model.Perceptron\u201d],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: \u201cLinearRegression\u201d,             \u201ccore_model_parameter_type\u201d: \u201cinstance\u201d},         \u201csplitter\u201d: {             \u201clevels\u201d: [\u201cEBSTSplitter\u201d, \u201cTEBSTSplitter\u201d, \u201cQOSplitter\u201d],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: \u201cEBSTSplitter\u201d,             \u201ccore_model_parameter_type\u201d: \u201cinstance()\u201d},         \u201cbinary_split\u201d: {             \u201clevels\u201d: [0, 1],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: 0,             \u201ccore_model_parameter_type\u201d: \u201cbool\u201d},         \u201cstop_mem_management\u201d: {                                                         \u201clevels\u201d: [0, 1],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: 0,             \u201ccore_model_parameter_type\u201d: \u201cbool\u201d}}}</p> <pre><code>get_var_name(d)\n['leaf_prediction', 'leaf_model', 'splitter', 'binary_split', 'stop_mem_management']\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_var_name(fun_control) -&gt; list:\n    \"\"\"Get the names of the values from the dictionary fun_control as a list.\n    Args:\n        fun_control (dict):\n            dictionary with names\n    Returns:\n        (list):\n            ist with names\n    Examples:\n        &gt;&gt;&gt; d = {\"core_model_hyper_dict\":{\n            \"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"core_model_parameter_type\": \"str\"},\n            \"leaf_model\": {\n                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                \"type\": \"factor\",\n                \"default\": \"LinearRegression\",\n                \"core_model_parameter_type\": \"instance\"},\n            \"splitter\": {\n                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                \"type\": \"factor\",\n                \"default\": \"EBSTSplitter\",\n                \"core_model_parameter_type\": \"instance()\"},\n            \"binary_split\": {\n                \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"core_model_parameter_type\": \"bool\"},\n            \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"core_model_parameter_type\": \"bool\"}}}\n\n        get_var_name(d)\n        ['leaf_prediction', 'leaf_model', 'splitter', 'binary_split', 'stop_mem_management']\n    \"\"\"\n    return list(fun_control[\"core_model_hyper_dict\"].keys())\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.get_var_type","title":"<code>get_var_type(fun_control)</code>","text":"<p>Get the types of the values from the dictionary fun_control as a list. Args:     fun_control (dict):         dictionary with levels and types Returns:     (list):         list with types Examples:     &gt;&gt;&gt; from spotPython.utils.prepare import get_var_type         d = {\u201ccore_model_hyper_dict\u201d:{         \u201cleaf_prediction\u201d: {             \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: \u201cmean\u201d,             \u201ccore_model_parameter_type\u201d: \u201cstr\u201d},         \u201cleaf_model\u201d: {             \u201clevels\u201d: [\u201clinear_model.LinearRegression\u201d, \u201clinear_model.PARegressor\u201d, \u201clinear_model.Perceptron\u201d],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: \u201cLinearRegression\u201d,             \u201ccore_model_parameter_type\u201d: \u201cinstance\u201d},         \u201csplitter\u201d: {             \u201clevels\u201d: [\u201cEBSTSplitter\u201d, \u201cTEBSTSplitter\u201d, \u201cQOSplitter\u201d],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: \u201cEBSTSplitter\u201d,             \u201ccore_model_parameter_type\u201d: \u201cinstance()\u201d},         \u201cbinary_split\u201d: {             \u201clevels\u201d: [0, 1],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: 0,             \u201ccore_model_parameter_type\u201d: \u201cbool\u201d},         \u201cstop_mem_management\u201d: {                                                         \u201clevels\u201d: [0, 1],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: 0,             \u201ccore_model_parameter_type\u201d: \u201cbool\u201d}}}</p> <pre><code>    get_var_type(d)\n    ['factor', 'factor', 'factor', 'factor', 'factor']\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def get_var_type(fun_control) -&gt; list:\n    \"\"\"Get the types of the values from the dictionary fun_control as a list.\n    Args:\n        fun_control (dict):\n            dictionary with levels and types\n    Returns:\n        (list):\n            list with types\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.prepare import get_var_type\n            d = {\"core_model_hyper_dict\":{\n            \"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"core_model_parameter_type\": \"str\"},\n            \"leaf_model\": {\n                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                \"type\": \"factor\",\n                \"default\": \"LinearRegression\",\n                \"core_model_parameter_type\": \"instance\"},\n            \"splitter\": {\n                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                \"type\": \"factor\",\n                \"default\": \"EBSTSplitter\",\n                \"core_model_parameter_type\": \"instance()\"},\n            \"binary_split\": {\n                \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"core_model_parameter_type\": \"bool\"},\n            \"stop_mem_management\": {                                                         \"levels\": [0, 1],\n                \"type\": \"factor\",\n                \"default\": 0,\n                \"core_model_parameter_type\": \"bool\"}}}\n\n            get_var_type(d)\n            ['factor', 'factor', 'factor', 'factor', 'factor']\n    \"\"\"\n    return list(\n        fun_control[\"core_model_hyper_dict\"][key][\"type\"] for key in fun_control[\"core_model_hyper_dict\"].keys()\n    )\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.iterate_dict_values","title":"<code>iterate_dict_values(var_dict)</code>","text":"<p>Iterate over the values of a dictionary of variables.</p> <p>This function takes a dictionary of variables as input arguments and returns a generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <p>Returns:</p> Type Description <code>Generator[Dict[str, Union[int, float]], None, None]</code> <p>Generator[dict]: A generator that yields dictionaries with the values from the arrays in the input dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotPython.utils.prepare import iterate_dict_values\n&gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n&gt;&gt;&gt; list(iterate_dict_values(var_dict))\n[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def iterate_dict_values(var_dict: Dict[str, np.ndarray]) -&gt; Generator[Dict[str, Union[int, float]], None, None]:\n    \"\"\"Iterate over the values of a dictionary of variables.\n\n    This function takes a dictionary of variables as input arguments and returns a generator that\n    yields dictionaries with the values from the arrays in the input dictionary.\n\n    Args:\n        var_dict (dict): A dictionary where keys are variable names and values are numpy arrays.\n\n    Returns:\n        Generator[dict]:\n            A generator that yields dictionaries with the values from the arrays in the input dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotPython.utils.prepare import iterate_dict_values\n        &gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n        &gt;&gt;&gt; list(iterate_dict_values(var_dict))\n        [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n    \"\"\"\n    n = len(next(iter(var_dict.values())))\n    for i in range(n):\n        yield {key: value[i] for key, value in var_dict.items()}\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.modify_hyper_parameter_bounds","title":"<code>modify_hyper_parameter_bounds(fun_control, hyperparameter, bounds)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>bounds</code> <code>list</code> <p>list of two bound values. The first value represents the lower bound and the second value represents the upper bound.</p> required <p>Returns:</p> Name Type Description <code>fun_control</code> <code>dict</code> <p>updated fun_control</p> <p>Examples:     &gt;&gt;&gt; from spotPython.utils.prepare import modify_hyper_parameter_levels         fun_control = {}         core_model  = HoeffdingTreeRegressor         fun_control.update({\u201ccore_model\u201d: core_model})         fun_control.update({\u201ccore_model_hyper_dict\u201d: river_hyper_dict[core_model.name]})         bounds = [3, 11]         fun_control = modify_hyper_parameter_levels(fun_control, \u201cmin_samples_split\u201d, bounds)</p> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def modify_hyper_parameter_bounds(fun_control, hyperparameter, bounds) -&gt; dict:\n    \"\"\"\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        bounds (list):\n            list of two bound values. The first value represents the lower bound\n            and the second value represents the upper bound.\n\n    Returns:\n        fun_control (dict):\n            updated fun_control\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.prepare import modify_hyper_parameter_levels\n            fun_control = {}\n            core_model  = HoeffdingTreeRegressor\n            fun_control.update({\"core_model\": core_model})\n            fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n            bounds = [3, 11]\n            fun_control = modify_hyper_parameter_levels(fun_control, \"min_samples_split\", bounds)\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": bounds[0]})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": bounds[1]})\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.modify_hyper_parameter_levels","title":"<code>modify_hyper_parameter_levels(fun_control, hyperparameter, levels)</code>","text":"<p>This function modifies the levels of a hyperparameter in the fun_control dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>fun_control</code> <code>dict</code> <p>fun_control dictionary</p> required <code>hyperparameter</code> <code>str</code> <p>hyperparameter name</p> required <code>levels</code> <code>list</code> <p>list of levels</p> required <p>Returns:</p> Name Type Description <code>fun_control</code> <code>dict</code> <p>updated fun_control</p> <p>Examples:     &gt;&gt;&gt; fun_control = {}         from spotPython.utils.prepare import modify_hyper_parameter_levels         core_model  = HoeffdingTreeRegressor         fun_control.update({\u201ccore_model\u201d: core_model})         fun_control.update({\u201ccore_model_hyper_dict\u201d: river_hyper_dict[core_model.name]})         levels = [\u201cmean\u201d, \u201cmodel\u201d]         fun_control = modify_hyper_parameter_levels(fun_control, \u201cleaf_prediction\u201d, levels)</p> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def modify_hyper_parameter_levels(fun_control, hyperparameter, levels) -&gt; dict:\n    \"\"\"\n    This function modifies the levels of a hyperparameter in the fun_control dictionary.\n\n    Args:\n        fun_control (dict):\n            fun_control dictionary\n        hyperparameter (str):\n            hyperparameter name\n        levels (list):\n            list of levels\n\n    Returns:\n        fun_control (dict):\n            updated fun_control\n    Examples:\n        &gt;&gt;&gt; fun_control = {}\n            from spotPython.utils.prepare import modify_hyper_parameter_levels\n            core_model  = HoeffdingTreeRegressor\n            fun_control.update({\"core_model\": core_model})\n            fun_control.update({\"core_model_hyper_dict\": river_hyper_dict[core_model.__name__]})\n            levels = [\"mean\", \"model\"]\n            fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_prediction\", levels)\n    \"\"\"\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"levels\": levels})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"lower\": 0})\n    fun_control[\"core_model_hyper_dict\"][hyperparameter].update({\"upper\": len(levels) - 1})\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.replace_levels_with_positions","title":"<code>replace_levels_with_positions(hyper_dict, hyper_dict_values)</code>","text":"<p>Replace the levels with the position in the levels list. The function that takes two dictionaries. The first contains as hyperparameters as keys. If the hyperparameter has the key \u201clevels\u201d, then the value of the corresponding hyperparameter in the second dictionary is replaced by the position of the value in the list of levels. The function returns a dictionary with the same keys as the second dictionary. For example, if the second dictionary is {\u201ca\u201d: 1, \u201cb\u201d: \u201cmodel1\u201d, \u201cc\u201d: 3} and the first dictionary is {     \u201ca\u201d: {\u201ctype\u201d: \u201cint\u201d},     \u201cb\u201d: {\u201clevels\u201d: [\u201cmodel4\u201d, \u201cmodel5\u201d, \u201cmodel1\u201d]},     \u201cd\u201d: {\u201ctype\u201d: \u201cfloat\u201d}}, then the function should return {\u201ca\u201d: 1, \u201cb\u201d: 2, \u201cc\u201d: 3}.</p> <p>Parameters:</p> Name Type Description Default <code>hyper_dict</code> <code>dict</code> <p>dictionary with levels</p> required <code>hyper_dict_values</code> <code>dict</code> <p>dictionary with values</p> required <p>Returns:     (dict):         dictionary with values Examples:     &gt;&gt;&gt; from spotPython.utils.prepare import replace_levels_with_positions         hyper_dict = {\u201cleaf_prediction\u201d: {             \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d],             \u201ctype\u201d: \u201cfactor\u201d,             \u201cdefault\u201d: \u201cmean\u201d,             \u201ccore_model_parameter_type\u201d: \u201cstr\u201d},             \u201cleaf_model\u201d: {                 \u201clevels\u201d: [\u201clinear_model.LinearRegression\u201d, \u201clinear_model.PARegressor\u201d, \u201clinear_model.Perceptron\u201d],                 \u201ctype\u201d: \u201cfactor\u201d,                 \u201cdefault\u201d: \u201cLinearRegression\u201d,                 \u201ccore_model_parameter_type\u201d: \u201cinstance\u201d},             \u201csplitter\u201d: {                 \u201clevels\u201d: [\u201cEBSTSplitter\u201d, \u201cTEBSTSplitter\u201d, \u201cQOSplitter\u201d],                 \u201ctype\u201d: \u201cfactor\u201d,                 \u201cdefault\u201d: \u201cEBSTSplitter\u201d,                 \u201ccore_model_parameter_type\u201d: \u201cinstance()\u201d},             \u201cbinary_split\u201d: {                 \u201clevels\u201d: [0, 1],                 \u201ctype\u201d: \u201cfactor\u201d,                 \u201cdefault\u201d: 0,                 \u201ccore_model_parameter_type\u201d: \u201cbool\u201d},             \u201cstop_mem_management\u201d: {                 \u201clevels\u201d: [0, 1],                 \u201ctype\u201d: \u201cfactor\u201d,                 \u201cdefault\u201d: 0,                 \u201ccore_model_parameter_type\u201d: \u201cbool\u201d}}         hyper_dict_values = {\u201cleaf_prediction\u201d: \u201cmean\u201d,             \u201cleaf_model\u201d: \u201clinear_model.LinearRegression\u201d,             \u201csplitter\u201d: \u201cEBSTSplitter\u201d,             \u201cbinary_split\u201d: 0,             \u201cstop_mem_management\u201d: 0}         replace_levels_with_position(hyper_dict, hyper_dict_values)             {\u2018leaf_prediction\u2019: 0,             \u2018leaf_model\u2019: 0,             \u2018splitter\u2019: 0,             \u2018binary_split\u2019: 0,             \u2018stop_mem_management\u2019: 0}</p> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def replace_levels_with_positions(hyper_dict, hyper_dict_values) -&gt; dict:\n    \"\"\"Replace the levels with the position in the levels list.\n    The function that takes two dictionaries.\n    The first contains as hyperparameters as keys.\n    If the hyperparameter has the key \"levels\",\n    then the value of the corresponding hyperparameter in the second dictionary is\n    replaced by the position of the value in the list of levels.\n    The function returns a dictionary with the same keys as the second dictionary.\n    For example, if the second dictionary is {\"a\": 1, \"b\": \"model1\", \"c\": 3}\n    and the first dictionary is {\n        \"a\": {\"type\": \"int\"},\n        \"b\": {\"levels\": [\"model4\", \"model5\", \"model1\"]},\n        \"d\": {\"type\": \"float\"}},\n    then the function should return {\"a\": 1, \"b\": 2, \"c\": 3}.\n\n    Args:\n        hyper_dict (dict):\n            dictionary with levels\n        hyper_dict_values (dict):\n            dictionary with values\n    Returns:\n        (dict):\n            dictionary with values\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.prepare import replace_levels_with_positions\n            hyper_dict = {\"leaf_prediction\": {\n                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                \"type\": \"factor\",\n                \"default\": \"mean\",\n                \"core_model_parameter_type\": \"str\"},\n                \"leaf_model\": {\n                    \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n                    \"type\": \"factor\",\n                    \"default\": \"LinearRegression\",\n                    \"core_model_parameter_type\": \"instance\"},\n                \"splitter\": {\n                    \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n                    \"type\": \"factor\",\n                    \"default\": \"EBSTSplitter\",\n                    \"core_model_parameter_type\": \"instance()\"},\n                \"binary_split\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"},\n                \"stop_mem_management\": {\n                    \"levels\": [0, 1],\n                    \"type\": \"factor\",\n                    \"default\": 0,\n                    \"core_model_parameter_type\": \"bool\"}}\n            hyper_dict_values = {\"leaf_prediction\": \"mean\",\n                \"leaf_model\": \"linear_model.LinearRegression\",\n                \"splitter\": \"EBSTSplitter\",\n                \"binary_split\": 0,\n                \"stop_mem_management\": 0}\n            replace_levels_with_position(hyper_dict, hyper_dict_values)\n                {'leaf_prediction': 0,\n                'leaf_model': 0,\n                'splitter': 0,\n                'binary_split': 0,\n                'stop_mem_management': 0}\n    \"\"\"\n    hyper_dict_values_new = copy.deepcopy(hyper_dict_values)\n    for key, value in hyper_dict_values.items():\n        if key in hyper_dict.keys():\n            if \"levels\" in hyper_dict[key].keys():\n                hyper_dict_values_new[key] = hyper_dict[key][\"levels\"].index(value)\n    return hyper_dict_values_new\n</code></pre>"},{"location":"reference/spotPython/hyperparameters/values/#spotPython.hyperparameters.values.return_conf_list_from_var_dict","title":"<code>return_conf_list_from_var_dict(var_dict, fun_control)</code>","text":"<p>Return a list of configurations from a dictionary of variables.</p> <p>This function takes a dictionary of variables and a dictionary of function control as input arguments. It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries of hyper parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>var_dict</code> <code>dict</code> <p>A dictionary where keys are variable names and values are numpy arrays.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary which (at least) has an entry with the following key: \u201cvar_type\u201d (list): A list of variable types. If the entry is not \u201cnum\u201d the corresponding value will be converted to the type \u201cint\u201d.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, Union[int, float]]]</code> <p>A list of dictionaries of hyper parameter values. Transformations are applied to the values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotPython.utils.prepare import return_conf_list_from_var_dict\n&gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n&gt;&gt;&gt; fun_control = {'var_type': ['int', 'int']}\n&gt;&gt;&gt; return_conf_list_from_var_dict(var_dict, fun_control)\n[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n</code></pre> Source code in <code>spotPython/hyperparameters/values.py</code> <pre><code>def return_conf_list_from_var_dict(\n    var_dict: Dict[str, np.ndarray], fun_control: Dict[str, Union[List[str], str]]\n) -&gt; List[Dict[str, Union[int, float]]]:\n    \"\"\"Return a list of configurations from a dictionary of variables.\n\n    This function takes a dictionary of variables and a dictionary of function control as input arguments.\n    It performs similar steps as generate_one_config_from_var_dict() but returns a list of dictionaries\n    of hyper parameter values.\n\n    Args:\n        var_dict (dict): A dictionary where keys are variable names and values are numpy arrays.\n        fun_control (dict): A dictionary which (at least) has an entry with the following key:\n            \"var_type\" (list): A list of variable types. If the entry is not \"num\" the corresponding\n            value will be converted to the type \"int\".\n\n    Returns:\n        list: A list of dictionaries of hyper parameter values. Transformations are applied to the values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotPython.utils.prepare import return_conf_list_from_var_dict\n        &gt;&gt;&gt; var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n        &gt;&gt;&gt; fun_control = {'var_type': ['int', 'int']}\n        &gt;&gt;&gt; return_conf_list_from_var_dict(var_dict, fun_control)\n        [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n    \"\"\"\n    conf_list = []\n    for values in generate_one_config_from_var_dict(var_dict, fun_control):\n        conf_list.append(values)\n    return conf_list\n</code></pre>"},{"location":"reference/spotPython/light/crossvalidationdatamodule/","title":"crossvalidationdatamodule","text":""},{"location":"reference/spotPython/light/crossvalidationdatamodule/#spotPython.light.crossvalidationdatamodule.CrossValidationDataModule","title":"<code>CrossValidationDataModule</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for handling cross-validation data splits.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The size of the batch. Defaults to 64.</p> <code>64</code> <code>k</code> <code>int</code> <p>The fold number. Defaults to 1.</p> <code>1</code> <code>split_seed</code> <code>int</code> <p>The random seed for splitting the data. Defaults to 42.</p> <code>42</code> <code>num_splits</code> <code>int</code> <p>The number of splits for cross-validation. Defaults to 10.</p> <code>10</code> <code>data_dir</code> <code>str</code> <p>The path to the dataset. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <code>num_workers</code> <code>int</code> <p>The number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory for data loading. Defaults to False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>data_train</code> <code>Optional[Dataset]</code> <p>The training dataset.</p> <code>data_val</code> <code>Optional[Dataset]</code> <p>The validation dataset.</p> <p>Examples:     &gt;&gt;&gt; from spotPython.light import CrossValidationDataModule     &gt;&gt;&gt; data_module = CrossValidationDataModule()     &gt;&gt;&gt; data_module.setup()     &gt;&gt;&gt; print(f\u201dTraining set size: {len(data_module.data_train)}\u201d)     Training set size: 45000     &gt;&gt;&gt; print(f\u201dValidation set size: {len(data_module.data_val)}\u201d)     Validation set size: 5000     &gt;&gt;&gt; print(f\u201dTest set size: {len(data_module.data_test)}\u201d)     Test set size: 10000</p> Source code in <code>spotPython/light/crossvalidationdatamodule.py</code> <pre><code>class CrossValidationDataModule(L.LightningDataModule):\n    \"\"\"\n    A LightningDataModule for handling cross-validation data splits.\n\n    Args:\n        batch_size (int): The size of the batch. Defaults to 64.\n        k (int): The fold number. Defaults to 1.\n        split_seed (int): The random seed for splitting the data. Defaults to 42.\n        num_splits (int): The number of splits for cross-validation. Defaults to 10.\n        data_dir (str): The path to the dataset. Defaults to \"./data\".\n        num_workers (int): The number of workers for data loading. Defaults to 0.\n        pin_memory (bool): Whether to pin memory for data loading. Defaults to False.\n\n    Attributes:\n        data_train (Optional[Dataset]): The training dataset.\n        data_val (Optional[Dataset]): The validation dataset.\n    Examples:\n        &gt;&gt;&gt; from spotPython.light import CrossValidationDataModule\n        &gt;&gt;&gt; data_module = CrossValidationDataModule()\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; print(f\"Training set size: {len(data_module.data_train)}\")\n        Training set size: 45000\n        &gt;&gt;&gt; print(f\"Validation set size: {len(data_module.data_val)}\")\n        Validation set size: 5000\n        &gt;&gt;&gt; print(f\"Test set size: {len(data_module.data_test)}\")\n        Test set size: 10000\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size=64,\n        k: int = 1,\n        split_seed: int = 42,\n        num_splits: int = 10,\n        data_dir: str = \"./data\",\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.data_dir = data_dir\n        self.num_workers = num_workers\n        self.k = k\n        self.split_seed = split_seed\n        self.num_splits = num_splits\n        self.pin_memory = pin_memory\n        self.save_hyperparameters(logger=False)\n        assert 0 &lt;= self.k &lt; self.num_splits, \"incorrect fold number\"\n\n        # no data transformations\n        self.transforms = None\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Prepares the data for use.\"\"\"\n        # download\n        pass\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Sets up the data for use.\n\n        Args:\n            stage (Optional[str]): The current stage. Defaults to None.\n        \"\"\"\n        if not self.data_train and not self.data_val:\n            dataset_full = CSVDataset(csv_file=\"./data/VBDP/train.csv\", train=True)\n            kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n            all_splits = [k for k in kf.split(dataset_full)]\n            train_indexes, val_indexes = all_splits[self.hparams.k]\n            train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n            self.data_train = Subset(dataset_full, train_indexes)\n            print(f\"Train Dataset Size: {len(self.data_train)}\")\n            self.data_val = Subset(dataset_full, val_indexes)\n            print(f\"Val Dataset Size: {len(self.data_val)}\")\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the training dataloader.\n\n        Returns:\n            DataLoader: The training dataloader.\n        Examples:\n            &gt;&gt;&gt; from spotPython.light import CrossValidationDataModule\n            &gt;&gt;&gt; data_module = CrossValidationDataModule()\n            &gt;&gt;&gt; data_module.setup()\n            &gt;&gt;&gt; train_dataloader = data_module.train_dataloader()\n            &gt;&gt;&gt; print(f\"Training set size: {len(train_dataloader.dataset)}\")\n            Training set size: 45000\n\n        \"\"\"\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the validation dataloader.\n\n        Returns:\n            DataLoader: The validation dataloader.\n        Examples:\n            &gt;&gt;&gt; from spotPython.light import CrossValidationDataModule\n            &gt;&gt;&gt; data_module = CrossValidationDataModule()\n            &gt;&gt;&gt; data_module.setup()\n            &gt;&gt;&gt; val_dataloader = data_module.val_dataloader()\n            &gt;&gt;&gt; print(f\"Validation set size: {len(val_dataloader.dataset)}\")\n            Validation set size: 5000\n        \"\"\"\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n        )\n</code></pre>"},{"location":"reference/spotPython/light/crossvalidationdatamodule/#spotPython.light.crossvalidationdatamodule.CrossValidationDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepares the data for use.</p> Source code in <code>spotPython/light/crossvalidationdatamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Prepares the data for use.\"\"\"\n    # download\n    pass\n</code></pre>"},{"location":"reference/spotPython/light/crossvalidationdatamodule/#spotPython.light.crossvalidationdatamodule.CrossValidationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Sets up the data for use.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>The current stage. Defaults to None.</p> <code>None</code> Source code in <code>spotPython/light/crossvalidationdatamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Sets up the data for use.\n\n    Args:\n        stage (Optional[str]): The current stage. Defaults to None.\n    \"\"\"\n    if not self.data_train and not self.data_val:\n        dataset_full = CSVDataset(csv_file=\"./data/VBDP/train.csv\", train=True)\n        kf = KFold(n_splits=self.hparams.num_splits, shuffle=True, random_state=self.hparams.split_seed)\n        all_splits = [k for k in kf.split(dataset_full)]\n        train_indexes, val_indexes = all_splits[self.hparams.k]\n        train_indexes, val_indexes = train_indexes.tolist(), val_indexes.tolist()\n        self.data_train = Subset(dataset_full, train_indexes)\n        print(f\"Train Dataset Size: {len(self.data_train)}\")\n        self.data_val = Subset(dataset_full, val_indexes)\n        print(f\"Val Dataset Size: {len(self.data_val)}\")\n</code></pre>"},{"location":"reference/spotPython/light/crossvalidationdatamodule/#spotPython.light.crossvalidationdatamodule.CrossValidationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The training dataloader.</p> <p>Examples:     &gt;&gt;&gt; from spotPython.light import CrossValidationDataModule     &gt;&gt;&gt; data_module = CrossValidationDataModule()     &gt;&gt;&gt; data_module.setup()     &gt;&gt;&gt; train_dataloader = data_module.train_dataloader()     &gt;&gt;&gt; print(f\u201dTraining set size: {len(train_dataloader.dataset)}\u201d)     Training set size: 45000</p> Source code in <code>spotPython/light/crossvalidationdatamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the training dataloader.\n\n    Returns:\n        DataLoader: The training dataloader.\n    Examples:\n        &gt;&gt;&gt; from spotPython.light import CrossValidationDataModule\n        &gt;&gt;&gt; data_module = CrossValidationDataModule()\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; train_dataloader = data_module.train_dataloader()\n        &gt;&gt;&gt; print(f\"Training set size: {len(train_dataloader.dataset)}\")\n        Training set size: 45000\n\n    \"\"\"\n    return DataLoader(\n        dataset=self.data_train,\n        batch_size=self.hparams.batch_size,\n        num_workers=self.hparams.num_workers,\n        pin_memory=self.hparams.pin_memory,\n        shuffle=True,\n    )\n</code></pre>"},{"location":"reference/spotPython/light/crossvalidationdatamodule/#spotPython.light.crossvalidationdatamodule.CrossValidationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The validation dataloader.</p> <p>Examples:     &gt;&gt;&gt; from spotPython.light import CrossValidationDataModule     &gt;&gt;&gt; data_module = CrossValidationDataModule()     &gt;&gt;&gt; data_module.setup()     &gt;&gt;&gt; val_dataloader = data_module.val_dataloader()     &gt;&gt;&gt; print(f\u201dValidation set size: {len(val_dataloader.dataset)}\u201d)     Validation set size: 5000</p> Source code in <code>spotPython/light/crossvalidationdatamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the validation dataloader.\n\n    Returns:\n        DataLoader: The validation dataloader.\n    Examples:\n        &gt;&gt;&gt; from spotPython.light import CrossValidationDataModule\n        &gt;&gt;&gt; data_module = CrossValidationDataModule()\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; val_dataloader = data_module.val_dataloader()\n        &gt;&gt;&gt; print(f\"Validation set size: {len(val_dataloader.dataset)}\")\n        Validation set size: 5000\n    \"\"\"\n    return DataLoader(\n        dataset=self.data_val,\n        batch_size=self.hparams.batch_size,\n        num_workers=self.hparams.num_workers,\n        pin_memory=self.hparams.pin_memory,\n    )\n</code></pre>"},{"location":"reference/spotPython/light/csvdatamodule/","title":"csvdatamodule","text":""},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule","title":"<code>CSVDataModule</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for handling CSV data.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The size of the batch.</p> required <code>data_dir</code> <code>str</code> <p>The path to the dataset. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <code>num_workers</code> <code>int</code> <p>The number of workers for data loading. Defaults to 0.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>data_train</code> <code>Dataset</code> <p>The training dataset.</p> <code>data_val</code> <code>Dataset</code> <p>The validation dataset.</p> <code>data_test</code> <code>Dataset</code> <p>The test dataset.</p> Source code in <code>spotPython/light/csvdatamodule.py</code> <pre><code>class CSVDataModule(L.LightningDataModule):\n    \"\"\"\n    A LightningDataModule for handling CSV data.\n\n    Args:\n        batch_size (int): The size of the batch.\n        data_dir (str): The path to the dataset. Defaults to \"./data\".\n        num_workers (int): The number of workers for data loading. Defaults to 0.\n\n    Attributes:\n        data_train (Dataset): The training dataset.\n        data_val (Dataset): The validation dataset.\n        data_test (Dataset): The test dataset.\n    \"\"\"\n\n    def __init__(self, batch_size: int, data_dir: str = \"./data\", num_workers: int = 0):\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Prepares the data for use.\"\"\"\n        # download\n        pass\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Sets up the data for use.\n\n        Args:\n            stage (Optional[str]): The current stage. Defaults to None.\n        Examples:\n            &gt;&gt;&gt; from spotPython.light import CSVDataModule\n            &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)\n            &gt;&gt;&gt; data_module.setup()\n            &gt;&gt;&gt; print(f\"Training set size: {len(data_module.data_train)}\")\n            Training set size: 45000\n            &gt;&gt;&gt; print(f\"Validation set size: {len(data_module.data_val)}\")\n            Validation set size: 5000\n            &gt;&gt;&gt; print(f\"Test set size: {len(data_module.data_test)}\")\n            Test set size: 10000\n\n        \"\"\"\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            data_full = CSVDataset(csv_file=\"./data/VBDP/train.csv\", train=True)\n            test_abs = int(len(data_full) * 0.6)\n            self.data_train, self.data_val = random_split(data_full, [test_abs, len(data_full) - test_abs])\n\n        # Assign test dataset for use in dataloader(s)\n        # TODO: Adapt this to the VBDP Situation\n        if stage == \"test\" or stage is None:\n            self.data_test = CSVDataset(csv_file=\"./data/VBDP/train.csv\", train=True)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the training dataloader.\n\n        Returns:\n            DataLoader: The training dataloader.\n        Examples:\n            &gt;&gt;&gt; from spotPython.light import CSVDataModule\n            &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)\n            &gt;&gt;&gt; data_module.setup()\n            &gt;&gt;&gt; train_dataloader = data_module.train_dataloader()\n            &gt;&gt;&gt; print(f\"Training dataloader size: {len(train_dataloader)}\")\n            Training dataloader size: 704\n\n        \"\"\"\n        return DataLoader(self.data_train, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the validation dataloader.\n\n        Returns:\n            DataLoader: The validation dataloader.\n        Examples:\n            &gt;&gt;&gt; from spotPython.light import CSVDataModule\n            &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)\n            &gt;&gt;&gt; data_module.setup()\n            &gt;&gt;&gt; val_dataloader = data_module.val_dataloader()\n            &gt;&gt;&gt; print(f\"Validation dataloader size: {len(val_dataloader)}\")\n            Validation dataloader size: 79\n\n        \"\"\"\n        return DataLoader(self.data_val, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the test dataloader.\n\n        Returns:\n            DataLoader: The test dataloader.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.light import CSVDataModule\n            &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)\n            &gt;&gt;&gt; data_module.setup()\n            &gt;&gt;&gt; test_dataloader = data_module.test_dataloader()\n            &gt;&gt;&gt; print(f\"Test dataloader size: {len(test_dataloader)}\")\n            Test dataloader size: 704\n\n        \"\"\"\n        return DataLoader(self.data_test, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepares the data for use.</p> Source code in <code>spotPython/light/csvdatamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Prepares the data for use.\"\"\"\n    # download\n    pass\n</code></pre>"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Sets up the data for use.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>The current stage. Defaults to None.</p> <code>None</code> <p>Examples:     &gt;&gt;&gt; from spotPython.light import CSVDataModule     &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)     &gt;&gt;&gt; data_module.setup()     &gt;&gt;&gt; print(f\u201dTraining set size: {len(data_module.data_train)}\u201d)     Training set size: 45000     &gt;&gt;&gt; print(f\u201dValidation set size: {len(data_module.data_val)}\u201d)     Validation set size: 5000     &gt;&gt;&gt; print(f\u201dTest set size: {len(data_module.data_test)}\u201d)     Test set size: 10000</p> Source code in <code>spotPython/light/csvdatamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Sets up the data for use.\n\n    Args:\n        stage (Optional[str]): The current stage. Defaults to None.\n    Examples:\n        &gt;&gt;&gt; from spotPython.light import CSVDataModule\n        &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; print(f\"Training set size: {len(data_module.data_train)}\")\n        Training set size: 45000\n        &gt;&gt;&gt; print(f\"Validation set size: {len(data_module.data_val)}\")\n        Validation set size: 5000\n        &gt;&gt;&gt; print(f\"Test set size: {len(data_module.data_test)}\")\n        Test set size: 10000\n\n    \"\"\"\n    # Assign train/val datasets for use in dataloaders\n    if stage == \"fit\" or stage is None:\n        data_full = CSVDataset(csv_file=\"./data/VBDP/train.csv\", train=True)\n        test_abs = int(len(data_full) * 0.6)\n        self.data_train, self.data_val = random_split(data_full, [test_abs, len(data_full) - test_abs])\n\n    # Assign test dataset for use in dataloader(s)\n    # TODO: Adapt this to the VBDP Situation\n    if stage == \"test\" or stage is None:\n        self.data_test = CSVDataset(csv_file=\"./data/VBDP/train.csv\", train=True)\n</code></pre>"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The test dataloader.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.light import CSVDataModule\n&gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)\n&gt;&gt;&gt; data_module.setup()\n&gt;&gt;&gt; test_dataloader = data_module.test_dataloader()\n&gt;&gt;&gt; print(f\"Test dataloader size: {len(test_dataloader)}\")\nTest dataloader size: 704\n</code></pre> Source code in <code>spotPython/light/csvdatamodule.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the test dataloader.\n\n    Returns:\n        DataLoader: The test dataloader.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.light import CSVDataModule\n        &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; test_dataloader = data_module.test_dataloader()\n        &gt;&gt;&gt; print(f\"Test dataloader size: {len(test_dataloader)}\")\n        Test dataloader size: 704\n\n    \"\"\"\n    return DataLoader(self.data_test, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The training dataloader.</p> <p>Examples:     &gt;&gt;&gt; from spotPython.light import CSVDataModule     &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)     &gt;&gt;&gt; data_module.setup()     &gt;&gt;&gt; train_dataloader = data_module.train_dataloader()     &gt;&gt;&gt; print(f\u201dTraining dataloader size: {len(train_dataloader)}\u201d)     Training dataloader size: 704</p> Source code in <code>spotPython/light/csvdatamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the training dataloader.\n\n    Returns:\n        DataLoader: The training dataloader.\n    Examples:\n        &gt;&gt;&gt; from spotPython.light import CSVDataModule\n        &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; train_dataloader = data_module.train_dataloader()\n        &gt;&gt;&gt; print(f\"Training dataloader size: {len(train_dataloader)}\")\n        Training dataloader size: 704\n\n    \"\"\"\n    return DataLoader(self.data_train, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotPython/light/csvdatamodule/#spotPython.light.csvdatamodule.CSVDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The validation dataloader.</p> <p>Examples:     &gt;&gt;&gt; from spotPython.light import CSVDataModule     &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)     &gt;&gt;&gt; data_module.setup()     &gt;&gt;&gt; val_dataloader = data_module.val_dataloader()     &gt;&gt;&gt; print(f\u201dValidation dataloader size: {len(val_dataloader)}\u201d)     Validation dataloader size: 79</p> Source code in <code>spotPython/light/csvdatamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the validation dataloader.\n\n    Returns:\n        DataLoader: The validation dataloader.\n    Examples:\n        &gt;&gt;&gt; from spotPython.light import CSVDataModule\n        &gt;&gt;&gt; data_module = CSVDataModule(batch_size=64)\n        &gt;&gt;&gt; data_module.setup()\n        &gt;&gt;&gt; val_dataloader = data_module.val_dataloader()\n        &gt;&gt;&gt; print(f\"Validation dataloader size: {len(val_dataloader)}\")\n        Validation dataloader size: 79\n\n    \"\"\"\n    return DataLoader(self.data_val, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/spotPython/light/csvdataset/","title":"csvdataset","text":""},{"location":"reference/spotPython/light/csvdataset/#spotPython.light.csvdataset.CSVDataset","title":"<code>CSVDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for handling CSV data.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>The path to the CSV file. Defaults to \u201c./data/VBDP/train.csv\u201d.</p> <code>'./data/VBDP/train.csv'</code> <code>train</code> <code>bool</code> <p>Whether the dataset is for training or not. Defaults to True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>Tensor</code> <p>The data features.</p> <code>targets</code> <code>Tensor</code> <p>The data targets.</p> Source code in <code>spotPython/light/csvdataset.py</code> <pre><code>class CSVDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset for handling CSV data.\n\n    Args:\n        csv_file (str): The path to the CSV file. Defaults to \"./data/VBDP/train.csv\".\n        train (bool): Whether the dataset is for training or not. Defaults to True.\n\n    Attributes:\n        data (Tensor): The data features.\n        targets (Tensor): The data targets.\n    \"\"\"\n\n    def __init__(\n        self,\n        csv_file: str = \"./data/VBDP/train.csv\",\n        train: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.csv_file = csv_file\n        self.train = train\n        self.data, self.targets = self._load_data()\n\n    def _load_data(self) -&gt; tuple:\n        \"\"\"\n        Loads the data from the CSV file.\n\n        Returns:\n            tuple: A tuple containing the features and targets as tensors.\n        Examples:\n            &gt;&gt;&gt; from spotPython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(dataset.data.shape)\n            torch.Size([60000, 784])\n            &gt;&gt;&gt; print(dataset.targets.shape)\n            torch.Size([60000])\n\n        \"\"\"\n        data_df = pd.read_csv(self.csv_file)\n        # drop the id column\n        data_df = data_df.drop(columns=[\"id\"])\n        target_column = \"prognosis\"\n\n        # Encode prognosis labels as integers\n        label_encoder = LabelEncoder()\n        targets = label_encoder.fit_transform(data_df[target_column])\n\n        # Convert features to tensor\n        features = data_df.drop(columns=[target_column]).values\n        features_tensor = torch.tensor(features, dtype=torch.float32)\n\n        # Convert targets to tensor\n        targets_tensor = torch.tensor(targets, dtype=torch.long)\n        return features_tensor, targets_tensor\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"\n        Returns the feature and target at the given index.\n\n        Args:\n            idx (int): The index.\n\n        Returns:\n            tuple: A tuple containing the feature and target at the given index.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; feature, target = dataset[0]\n            &gt;&gt;&gt; print(feature.shape)\n            torch.Size([784])\n            &gt;&gt;&gt; print(target)\n            tensor(0)\n\n        \"\"\"\n        feature = self.data[idx]\n        target = self.targets[idx]\n        return feature, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(len(dataset))\n            60000\n\n        \"\"\"\n        return len(self.data)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the dataset.\n\n        Returns:\n            str: A string representation of the dataset.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.light import CSVDataset\n            &gt;&gt;&gt; dataset = CSVDataset()\n            &gt;&gt;&gt; print(dataset)\n            Split: Train\n\n        \"\"\"\n        split = \"Train\" if self.train else \"Test\"\n        return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotPython/light/csvdataset/#spotPython.light.csvdataset.CSVDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the feature and target at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the feature and target at the given index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; feature, target = dataset[0]\n&gt;&gt;&gt; print(feature.shape)\ntorch.Size([784])\n&gt;&gt;&gt; print(target)\ntensor(0)\n</code></pre> Source code in <code>spotPython/light/csvdataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"\n    Returns the feature and target at the given index.\n\n    Args:\n        idx (int): The index.\n\n    Returns:\n        tuple: A tuple containing the feature and target at the given index.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; feature, target = dataset[0]\n        &gt;&gt;&gt; print(feature.shape)\n        torch.Size([784])\n        &gt;&gt;&gt; print(target)\n        tensor(0)\n\n    \"\"\"\n    feature = self.data[idx]\n    target = self.targets[idx]\n    return feature, target\n</code></pre>"},{"location":"reference/spotPython/light/csvdataset/#spotPython.light.csvdataset.CSVDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(len(dataset))\n60000\n</code></pre> Source code in <code>spotPython/light/csvdataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(len(dataset))\n        60000\n\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/spotPython/light/csvdataset/#spotPython.light.csvdataset.CSVDataset.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Returns a string representation of the dataset.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.light import CSVDataset\n&gt;&gt;&gt; dataset = CSVDataset()\n&gt;&gt;&gt; print(dataset)\nSplit: Train\n</code></pre> Source code in <code>spotPython/light/csvdataset.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the dataset.\n\n    Returns:\n        str: A string representation of the dataset.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.light import CSVDataset\n        &gt;&gt;&gt; dataset = CSVDataset()\n        &gt;&gt;&gt; print(dataset)\n        Split: Train\n\n    \"\"\"\n    split = \"Train\" if self.train else \"Test\"\n    return f\"Split: {split}\"\n</code></pre>"},{"location":"reference/spotPython/light/litmodel/","title":"litmodel","text":""},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel","title":"<code>LitModel</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>A LightningModule class for a simple neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>act_fn</code> <code>str</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>learning_rate</code> <code>float</code> <p>The learning rate for the optimizer.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>model</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n&gt;&gt;&gt; lit_model = LitModel(l1=128, epochs=10, batch_size=BATCH_SIZE, act_fn='relu', optimizer='adam')\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(lit_model, train_loader)\n</code></pre> Source code in <code>spotPython/light/litmodel.py</code> <pre><code>class LitModel(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a simple neural network model.\n\n    Attributes:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        act_fn (str): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        learning_rate (float): The learning rate for the optimizer.\n        _L_in (int): The number of input features.\n        _L_out (int): The number of output classes.\n        model (nn.Sequential): The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; lit_model = LitModel(l1=128, epochs=10, batch_size=BATCH_SIZE, act_fn='relu', optimizer='adam')\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(lit_model, train_loader)\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        act_fn: str,\n        optimizer: str,\n        learning_rate: float = 2e-4,\n        _L_in: int = 28 * 28,\n        _L_out: int = 10,\n    ):\n        \"\"\"\n        Initializes the LitModel object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            act_fn (str): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            learning_rate (float, optional): The learning rate for the optimizer. Defaults to 2e-4.\n            _L_in (int, optional): The number of input features. Defaults to 28 * 28.\n            _L_out (int, optional): The number of output classes. Defaults to 10.\n\n        Returns:\n           (NoneType): None\n        \"\"\"\n        super().__init__()\n\n        # We take in input dimensions as parameters and use those to dynamically build model.\n        self._L_out = _L_out\n        self.l1 = l1\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.act_fn = act_fn\n        self.optimizer = optimizer\n        self.learning_rate = learning_rate\n\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(_L_in, l1),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(l1, l1),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(l1, _L_out),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the log probabilities for each class.\n        \"\"\"\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch: A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int) -&gt; None:\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n\n        Returns:\n            None\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def test_step(self, batch: tuple, batch_idx: int) -&gt; tuple:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n\n        Returns:\n            tuple: A tuple containing the loss and accuracy for this batch.\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n        return loss, acc\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n        \"\"\"\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre>"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.__init__","title":"<code>__init__(l1, epochs, batch_size, act_fn, optimizer, learning_rate=0.0002, _L_in=28 * 28, _L_out=10)</code>","text":"<p>Initializes the LitModel object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>act_fn</code> <code>str</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate for the optimizer. Defaults to 2e-4.</p> <code>0.0002</code> <code>_L_in</code> <code>int</code> <p>The number of input features. Defaults to 28 * 28.</p> <code>28 * 28</code> <code>_L_out</code> <code>int</code> <p>The number of output classes. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotPython/light/litmodel.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    act_fn: str,\n    optimizer: str,\n    learning_rate: float = 2e-4,\n    _L_in: int = 28 * 28,\n    _L_out: int = 10,\n):\n    \"\"\"\n    Initializes the LitModel object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        act_fn (str): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        learning_rate (float, optional): The learning rate for the optimizer. Defaults to 2e-4.\n        _L_in (int, optional): The number of input features. Defaults to 28 * 28.\n        _L_out (int, optional): The number of output classes. Defaults to 10.\n\n    Returns:\n       (NoneType): None\n    \"\"\"\n    super().__init__()\n\n    # We take in input dimensions as parameters and use those to dynamically build model.\n    self._L_out = _L_out\n    self.l1 = l1\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.act_fn = act_fn\n    self.optimizer = optimizer\n    self.learning_rate = learning_rate\n\n    self.model = nn.Sequential(\n        nn.Flatten(),\n        nn.Linear(_L_in, l1),\n        nn.ReLU(),\n        nn.Dropout(0.1),\n        nn.Linear(l1, l1),\n        nn.ReLU(),\n        nn.Dropout(0.1),\n        nn.Linear(l1, _L_out),\n    )\n</code></pre>"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotPython/light/litmodel.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n    \"\"\"\n    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    return optimizer\n</code></pre>"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the log probabilities for each class.</p> Source code in <code>spotPython/light/litmodel.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the log probabilities for each class.\n    \"\"\"\n    x = self.model(x)\n    return F.log_softmax(x, dim=1)\n</code></pre>"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the loss and accuracy for this batch.</p> Source code in <code>spotPython/light/litmodel.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int) -&gt; tuple:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n\n    Returns:\n        tuple: A tuple containing the loss and accuracy for this batch.\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.nll_loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n    self.log(\"val_loss\", loss, prog_bar=True)\n    self.log(\"val_acc\", acc, prog_bar=True)\n    return loss, acc\n</code></pre>"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> Source code in <code>spotPython/light/litmodel.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch: A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.nll_loss(logits, y)\n    return loss\n</code></pre>"},{"location":"reference/spotPython/light/litmodel/#spotPython.light.litmodel.LitModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spotPython/light/litmodel.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int) -&gt; None:\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n\n    Returns:\n        None\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    loss = F.nll_loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n    self.log(\"val_loss\", loss, prog_bar=True)\n    self.log(\"val_acc\", acc, prog_bar=True)\n</code></pre>"},{"location":"reference/spotPython/light/mnistdatamodule/","title":"mnistdatamodule","text":""},{"location":"reference/spotPython/light/netlightbase/","title":"netlightbase","text":""},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase","title":"<code>NetLightBase</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>A LightningModule class for a neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS,\n                       train=True,\n                       download=True,\n                       transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data,\n                              batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                  epochs=10,\n                                  batch_size=BATCH_SIZE,\n                                  initialization='xavier',\n                                  act_fn=nn.ReLU(),\n                                  optimizer='Adam',\n                                  dropout_prob=0.1,\n                                  lr_mult=0.1,\n                                  patience=5)\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n</code></pre> Source code in <code>spotPython/light/netlightbase.py</code> <pre><code>class NetLightBase(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a neural network model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS,\n                               train=True,\n                               download=True,\n                               transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data,\n                                      batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                          epochs=10,\n                                          batch_size=BATCH_SIZE,\n                                          initialization='xavier',\n                                          act_fn=nn.ReLU(),\n                                          optimizer='Adam',\n                                          dropout_prob=0.1,\n                                          lr_mult=0.1,\n                                          patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n    ):\n        \"\"\"\n        Initializes the NetLightBase object.\n\n        Args:\n            l1 (int): The number of neurons in the first hidden layer.\n            epochs (int): The number of epochs to train the model for.\n            batch_size (int): The batch size to use during training.\n            initialization (str): The initialization method to use for the weights.\n            act_fn (nn.Module): The activation function to use in the hidden layers.\n            optimizer (str): The optimizer to use during training.\n            dropout_prob (float): The probability of dropping out a neuron during training.\n            lr_mult (float): The learning rate multiplier for the optimizer.\n            patience (int): The number of epochs to wait before early stopping.\n            _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 4.\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n        \"\"\"\n        super().__init__()\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n        if self.hparams.l1 &lt; 4:\n            raise ValueError(\"l1 must be at least 4\")\n\n        hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n        self.train_mapk = MAPK(k=3)\n        self.valid_mapk = MAPK(k=3)\n        self.test_mapk = MAPK(k=3)\n\n        # Create the network based on the specified hidden sizes\n        layers = []\n        layer_sizes = [self._L_in] + hidden_sizes\n        layer_size_last = layer_sizes[0]\n        for layer_size in layer_sizes[1:]:\n            layers += [\n                nn.Linear(layer_size_last, layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n            layer_size_last = layer_size\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the probabilities for each class.\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                              epochs=10,\n                                              batch_size=BATCH_SIZE,\n                                              initialization='xavier', act_fn=nn.ReLU(),\n                                              optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                              patience=5)\n\n        \"\"\"\n        x = self.layers(x)\n        return F.softmax(x, dim=1)\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = F.cross_entropy(logits, y)\n        # self.train_mapk(logits, y)\n        # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False):\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            (NoneType): None\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())\n            &gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, val_loader)\n\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = F.cross_entropy(logits, y)\n        # loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n        self.valid_mapk(logits, y)\n        self.log(\"valid_mapk\", self.valid_mapk, on_step=False, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", loss, prog_bar=prog_bar)\n        self.log(\"val_acc\", acc, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; tuple:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            tuple: A tuple containing the loss and accuracy for this batch.\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n        self.test_mapk(logits, y)\n        self.log(\"test_mapk\", self.test_mapk, on_step=True, on_epoch=True, prog_bar=prog_bar)\n        self.log(\"val_loss\", loss, prog_bar=prog_bar)\n        self.log(\"val_acc\", acc, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n        return loss, acc\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = optimizer_handler(\n            optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult\n        )\n        return optimizer\n</code></pre>"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.__init__","title":"<code>__init__(l1, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out)</code>","text":"<p>Initializes the NetLightBase object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 4.</p> <p>Examples:     &gt;&gt;&gt; from torch.utils.data import DataLoader     &gt;&gt;&gt; from torchvision.datasets import MNIST     &gt;&gt;&gt; from torchvision.transforms import ToTensor     &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())     &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)     &gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,                                         initialization=\u2019xavier\u2019, act_fn=nn.ReLU(),                                         optimizer=\u2019Adam\u2019, dropout_prob=0.1, lr_mult=0.1,                                         patience=5)     &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)     &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)</p> Source code in <code>spotPython/light/netlightbase.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n):\n    \"\"\"\n    Initializes the NetLightBase object.\n\n    Args:\n        l1 (int): The number of neurons in the first hidden layer.\n        epochs (int): The number of epochs to train the model for.\n        batch_size (int): The batch size to use during training.\n        initialization (str): The initialization method to use for the weights.\n        act_fn (nn.Module): The activation function to use in the hidden layers.\n        optimizer (str): The optimizer to use during training.\n        dropout_prob (float): The probability of dropping out a neuron during training.\n        lr_mult (float): The learning rate multiplier for the optimizer.\n        patience (int): The number of epochs to wait before early stopping.\n        _L_in (int): The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int): The number of output classes. Not a hyperparameter, but needed to create the network.\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 4.\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n    \"\"\"\n    super().__init__()\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\"])\n    if self.hparams.l1 &lt; 4:\n        raise ValueError(\"l1 must be at least 4\")\n\n    hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n    self.train_mapk = MAPK(k=3)\n    self.valid_mapk = MAPK(k=3)\n    self.test_mapk = MAPK(k=3)\n\n    # Create the network based on the specified hidden sizes\n    layers = []\n    layer_sizes = [self._L_in] + hidden_sizes\n    layer_size_last = layer_sizes[0]\n    for layer_size in layer_sizes[1:]:\n        layers += [\n            nn.Linear(layer_size_last, layer_size),\n            self.hparams.act_fn,\n            nn.Dropout(self.hparams.dropout_prob),\n        ]\n        layer_size_last = layer_size\n    layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n    # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n    self.layers = nn.Sequential(*layers)\n</code></pre>"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotPython/light/netlightbase.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    optimizer = optimizer_handler(\n        optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult\n    )\n    return optimizer\n</code></pre>"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the probabilities for each class.</p> <p>Examples:     &gt;&gt;&gt; from torch.utils.data import DataLoader     &gt;&gt;&gt; from torchvision.datasets import MNIST     &gt;&gt;&gt; from torchvision.transforms import ToTensor     &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())     &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)     &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,                                       epochs=10,                                       batch_size=BATCH_SIZE,                                       initialization=\u2019xavier\u2019, act_fn=nn.ReLU(),                                       optimizer=\u2019Adam\u2019, dropout_prob=0.1, lr_mult=0.1,                                       patience=5)</p> Source code in <code>spotPython/light/netlightbase.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the probabilities for each class.\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                          epochs=10,\n                                          batch_size=BATCH_SIZE,\n                                          initialization='xavier', act_fn=nn.ReLU(),\n                                          optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                          patience=5)\n\n    \"\"\"\n    x = self.layers(x)\n    return F.softmax(x, dim=1)\n</code></pre>"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the loss and accuracy for this batch.</p> Source code in <code>spotPython/light/netlightbase.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; tuple:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing the loss and accuracy for this batch.\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = F.cross_entropy(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n    self.test_mapk(logits, y)\n    self.log(\"test_mapk\", self.test_mapk, on_step=True, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", loss, prog_bar=prog_bar)\n    self.log(\"val_acc\", acc, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n    return loss, acc\n</code></pre>"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> <p>Examples:     &gt;&gt;&gt; from torch.utils.data import DataLoader     &gt;&gt;&gt; from torchvision.datasets import MNIST     &gt;&gt;&gt; from torchvision.transforms import ToTensor     &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())     &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)     &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,                                         epochs=10,                                         batch_size=BATCH_SIZE,                                         initialization=\u2019xavier\u2019, act_fn=nn.ReLU(),                                         optimizer=\u2019Adam\u2019, dropout_prob=0.1, lr_mult=0.1,                                         patience=5)     &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)     &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)</p> Source code in <code>spotPython/light/netlightbase.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                            epochs=10,\n                                            batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = F.cross_entropy(logits, y)\n    # self.train_mapk(logits, y)\n    # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False)\n    return loss\n</code></pre>"},{"location":"reference/spotPython/light/netlightbase/#spotPython.light.netlightbase.NetLightBase.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:     &gt;&gt;&gt; from torch.utils.data import DataLoader     &gt;&gt;&gt; from torchvision.datasets import MNIST     &gt;&gt;&gt; from torchvision.transforms import ToTensor     &gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())     &gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)     &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,                                         epochs=10,                                         batch_size=BATCH_SIZE,                                         initialization=\u2019xavier\u2019, act_fn=nn.ReLU(),                                         optimizer=\u2019Adam\u2019, dropout_prob=0.1, lr_mult=0.1,                                         patience=5)     &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)     &gt;&gt;&gt; trainer.fit(net_light_base, val_loader)</p> Source code in <code>spotPython/light/netlightbase.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False):\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        (NoneType): None\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())\n        &gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                            epochs=10,\n                                            batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, val_loader)\n\n    \"\"\"\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = F.cross_entropy(logits, y)\n    # loss = F.nll_loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    acc = accuracy(preds, y, task=\"multiclass\", num_classes=self._L_out)\n    self.valid_mapk(logits, y)\n    self.log(\"valid_mapk\", self.valid_mapk, on_step=False, on_epoch=True, prog_bar=prog_bar)\n    self.log(\"val_loss\", loss, prog_bar=prog_bar)\n    self.log(\"val_acc\", acc, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n</code></pre>"},{"location":"reference/spotPython/light/netlinearbase/","title":"netlinearbase","text":""},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase","title":"<code>NetLinearBase</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>A LightningModule class for a linear (dense) neural network model.</p> <p>Attributes:</p> Name Type Description <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> <code>_L_in</code> <code>int</code> <p>The number of input features.</p> <code>_L_out</code> <code>int</code> <p>The number of output classes.</p> <code>_metric</code> <code>accuracy</code> <p>The metric to use for evaluation.</p> <code>_loss</code> <code>cross_entropy</code> <p>The loss function to use for training.</p> <code>layers</code> <code>Sequential</code> <p>The neural network model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; from torchvision.datasets import MNIST\n&gt;&gt;&gt; from torchvision.transforms import ToTensor\n&gt;&gt;&gt; train_data = MNIST(PATH_DATASETS,\n                       train=True,\n                       download=True,\n                       transform=ToTensor())\n&gt;&gt;&gt; train_loader = DataLoader(train_data,\n                              batch_size=BATCH_SIZE)\n&gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                  epochs=10,\n                                  batch_size=BATCH_SIZE,\n                                  initialization='xavier',\n                                  act_fn=nn.ReLU(),\n                                  optimizer='Adam',\n                                  dropout_prob=0.1,\n                                  lr_mult=0.1,\n                                  patience=5)\n&gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n&gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n</code></pre> Source code in <code>spotPython/light/netlinearbase.py</code> <pre><code>class NetLinearBase(L.LightningModule):\n    \"\"\"\n    A LightningModule class for a linear (dense) neural network model.\n\n    Attributes:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output classes.\n        _metric (torchmetrics.functional.accuracy):\n            The metric to use for evaluation.\n        _loss (torch.nn.functional.cross_entropy):\n            The loss function to use for training.\n        layers (nn.Sequential):\n            The neural network model.\n\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS,\n                               train=True,\n                               download=True,\n                               transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data,\n                                      batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                          epochs=10,\n                                          batch_size=BATCH_SIZE,\n                                          initialization='xavier',\n                                          act_fn=nn.ReLU(),\n                                          optimizer='Adam',\n                                          dropout_prob=0.1,\n                                          lr_mult=0.1,\n                                          patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n    \"\"\"\n\n    def __init__(\n        self,\n        l1: int,\n        epochs: int,\n        batch_size: int,\n        initialization: str,\n        act_fn: nn.Module,\n        optimizer: str,\n        dropout_prob: float,\n        lr_mult: float,\n        patience: int,\n        _L_in: int,\n        _L_out: int,\n        # _metric: torchmetrics.functional = accuracy,\n        # _loss: torch.nn.functional = F.cross_entropy,\n    ):\n        \"\"\"\n        Initializes the NetLightBase object.\n\n        Args:\n            l1 (int):\n                The number of neurons in the first hidden layer.\n            epochs (int):\n                The number of epochs to train the model for.\n            batch_size (int):\n                The batch size to use during training.\n            initialization (str):\n                The initialization method to use for the weights.\n            act_fn (nn.Module):\n                The activation function to use in the hidden layers.\n            optimizer (str):\n                The optimizer to use during training.\n            dropout_prob (float):\n                The probability of dropping out a neuron during training.\n            lr_mult (float):\n                The learning rate multiplier for the optimizer.\n            patience (int):\n                The number of epochs to wait before early stopping.\n            _L_in (int):\n                The number of input features. Not a hyperparameter, but needed to create the network.\n            _L_out (int):\n                The number of output classes. Not a hyperparameter, but needed to create the network.\n            _metric (torchmetrics.functional.accuracy):\n                The metric to use for evaluation. Not a hyperparameter, but needed to create the network.\n            _loss (torch.nn.functional.cross_entropy):\n                The loss function to use for training. Not a hyperparameter, but needed to create the network.\n\n        Returns:\n            (NoneType): None\n\n        Raises:\n            ValueError: If l1 is less than 4.\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n        \"\"\"\n        super().__init__()\n        print(\"NetLinearBase.__init__(): l1\", l1)\n        # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n        # checkpointing. It is recommended to ignore them\n        # using `self.save_hyperparameters(ignore=['act_fn'])`\n        # self.save_hyperparameters(ignore=[\"act_fn\"])\n        #\n        self._L_in = _L_in\n        self._L_out = _L_out\n        # _L_in and _L_out are not hyperparameters, but are needed to create the network\n        self._metric = accuracy\n        self._loss = F.cross_entropy\n        self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_metric\", \"_loss\"])\n        if self.hparams.l1 &lt; 4:\n            raise ValueError(\"l1 must be at least 4\")\n\n        hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n\n        # Create the network based on the specified hidden sizes\n        layers = []\n        layer_sizes = [self._L_in] + hidden_sizes\n        layer_size_last = layer_sizes[0]\n        for layer_size in layer_sizes[1:]:\n            layers += [\n                nn.Linear(layer_size_last, layer_size),\n                self.hparams.act_fn,\n                nn.Dropout(self.hparams.dropout_prob),\n            ]\n            layer_size_last = layer_size\n        layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n        self.layers = nn.Sequential(*layers)\n        print(\"Leaving NetLinearBase.__init__()\")\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the model.\n\n        Args:\n            x (torch.Tensor): A tensor containing a batch of input data.\n\n        Returns:\n            torch.Tensor: A tensor containing the probabilities for each class.\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                              epochs=10,\n                                              batch_size=BATCH_SIZE,\n                                              initialization='xavier', act_fn=nn.ReLU(),\n                                              optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                              patience=5)\n\n        \"\"\"\n        print(\"Entering NetLinearBase.forward()\")\n        x = self.layers(x)\n        return F.softmax(x, dim=1)\n\n    def training_step(self, batch: tuple) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n\n        Returns:\n            torch.Tensor: A tensor containing the loss for this batch.\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n            &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n        \"\"\"\n        print(\"Entering NetLinearBase.training_step()\")\n        # x, y = batch\n        # print(\"NetLinearBase.training_step(): batch\")\n        # logits = self(x)\n        # print(\"NetLinearBase.training_step(): logits\")\n        # # compute loss (default: cross entropy loss) from logits and y\n        # loss = self._loss(logits, y)\n        # # self.train_mapk(logits, y)\n        # # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False)\n        # return loss\n        return 0.1234\n\n    def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False):\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            (NoneType): None\n        Examples:\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; from torchvision.datasets import MNIST\n            &gt;&gt;&gt; from torchvision.transforms import ToTensor\n            &gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())\n            &gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n            &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                                epochs=10,\n                                                batch_size=BATCH_SIZE,\n                                                initialization='xavier', act_fn=nn.ReLU(),\n                                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                                patience=5)\n            &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n            &gt;&gt;&gt; trainer.fit(net_light_base, val_loader)\n\n        \"\"\"\n        print(\"Entering NetLinearBase.validation_step()\")\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = self._loss(logits, y)\n        # loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        metric = self._metric(preds, y, task=\"multiclass\", num_classes=self._L_out)\n        self.log(\"val_loss\", loss, prog_bar=prog_bar)\n        self.log(\"val_metric\", metric, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n\n    def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; tuple:\n        \"\"\"\n        Performs a single test step.\n\n        Args:\n            batch (tuple): A tuple containing a batch of input data and labels.\n            batch_idx (int): The index of the current batch.\n            prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n        Returns:\n            tuple: A tuple containing the loss and accuracy for this batch.\n        \"\"\"\n        print(\"Entering NetLinearBase.test_step()\")\n        x, y = batch\n        logits = self(x)\n        # compute cross entropy loss from logits and y\n        loss = self._loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        metric = self._metric(preds, y, task=\"multiclass\", num_classes=self._L_out)\n        self.log(\"val_loss\", loss, prog_bar=prog_bar)\n        self.log(\"val_metric\", metric, prog_bar=prog_bar)\n        self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n        return loss, metric\n\n    def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n        \"\"\"\n        Configures the optimizer for the model.\n\n        Returns:\n            torch.optim.Optimizer: The optimizer to use during training.\n\n        \"\"\"\n        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        print(\"Entering NetLinearBase.configure_optimizers()\")\n        optimizer = optimizer_handler(\n            optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult\n        )\n        print(\"Leaving NetLinearBase.configure_optimizers()\")\n        return optimizer\n</code></pre>"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.__init__","title":"<code>__init__(l1, epochs, batch_size, initialization, act_fn, optimizer, dropout_prob, lr_mult, patience, _L_in, _L_out)</code>","text":"<p>Initializes the NetLightBase object.</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>int</code> <p>The number of neurons in the first hidden layer.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training.</p> required <code>initialization</code> <code>str</code> <p>The initialization method to use for the weights.</p> required <code>act_fn</code> <code>Module</code> <p>The activation function to use in the hidden layers.</p> required <code>optimizer</code> <code>str</code> <p>The optimizer to use during training.</p> required <code>dropout_prob</code> <code>float</code> <p>The probability of dropping out a neuron during training.</p> required <code>lr_mult</code> <code>float</code> <p>The learning rate multiplier for the optimizer.</p> required <code>patience</code> <code>int</code> <p>The number of epochs to wait before early stopping.</p> required <code>_L_in</code> <code>int</code> <p>The number of input features. Not a hyperparameter, but needed to create the network.</p> required <code>_L_out</code> <code>int</code> <p>The number of output classes. Not a hyperparameter, but needed to create the network.</p> required <code>_metric</code> <code>accuracy</code> <p>The metric to use for evaluation. Not a hyperparameter, but needed to create the network.</p> required <code>_loss</code> <code>cross_entropy</code> <p>The loss function to use for training. Not a hyperparameter, but needed to create the network.</p> required <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If l1 is less than 4.</p> <p>Examples:     &gt;&gt;&gt; from torch.utils.data import DataLoader     &gt;&gt;&gt; from torchvision.datasets import MNIST     &gt;&gt;&gt; from torchvision.transforms import ToTensor     &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())     &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)     &gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,                                         initialization=\u2019xavier\u2019, act_fn=nn.ReLU(),                                         optimizer=\u2019Adam\u2019, dropout_prob=0.1, lr_mult=0.1,                                         patience=5)     &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)     &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)</p> Source code in <code>spotPython/light/netlinearbase.py</code> <pre><code>def __init__(\n    self,\n    l1: int,\n    epochs: int,\n    batch_size: int,\n    initialization: str,\n    act_fn: nn.Module,\n    optimizer: str,\n    dropout_prob: float,\n    lr_mult: float,\n    patience: int,\n    _L_in: int,\n    _L_out: int,\n    # _metric: torchmetrics.functional = accuracy,\n    # _loss: torch.nn.functional = F.cross_entropy,\n):\n    \"\"\"\n    Initializes the NetLightBase object.\n\n    Args:\n        l1 (int):\n            The number of neurons in the first hidden layer.\n        epochs (int):\n            The number of epochs to train the model for.\n        batch_size (int):\n            The batch size to use during training.\n        initialization (str):\n            The initialization method to use for the weights.\n        act_fn (nn.Module):\n            The activation function to use in the hidden layers.\n        optimizer (str):\n            The optimizer to use during training.\n        dropout_prob (float):\n            The probability of dropping out a neuron during training.\n        lr_mult (float):\n            The learning rate multiplier for the optimizer.\n        patience (int):\n            The number of epochs to wait before early stopping.\n        _L_in (int):\n            The number of input features. Not a hyperparameter, but needed to create the network.\n        _L_out (int):\n            The number of output classes. Not a hyperparameter, but needed to create the network.\n        _metric (torchmetrics.functional.accuracy):\n            The metric to use for evaluation. Not a hyperparameter, but needed to create the network.\n        _loss (torch.nn.functional.cross_entropy):\n            The loss function to use for training. Not a hyperparameter, but needed to create the network.\n\n    Returns:\n        (NoneType): None\n\n    Raises:\n        ValueError: If l1 is less than 4.\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128, epochs=10, batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n    \"\"\"\n    super().__init__()\n    print(\"NetLinearBase.__init__(): l1\", l1)\n    # Attribute 'act_fn' is an instance of `nn.Module` and is already saved during\n    # checkpointing. It is recommended to ignore them\n    # using `self.save_hyperparameters(ignore=['act_fn'])`\n    # self.save_hyperparameters(ignore=[\"act_fn\"])\n    #\n    self._L_in = _L_in\n    self._L_out = _L_out\n    # _L_in and _L_out are not hyperparameters, but are needed to create the network\n    self._metric = accuracy\n    self._loss = F.cross_entropy\n    self.save_hyperparameters(ignore=[\"_L_in\", \"_L_out\", \"_metric\", \"_loss\"])\n    if self.hparams.l1 &lt; 4:\n        raise ValueError(\"l1 must be at least 4\")\n\n    hidden_sizes = [self.hparams.l1, self.hparams.l1 // 2, self.hparams.l1 // 2, self.hparams.l1 // 4]\n\n    # Create the network based on the specified hidden sizes\n    layers = []\n    layer_sizes = [self._L_in] + hidden_sizes\n    layer_size_last = layer_sizes[0]\n    for layer_size in layer_sizes[1:]:\n        layers += [\n            nn.Linear(layer_size_last, layer_size),\n            self.hparams.act_fn,\n            nn.Dropout(self.hparams.dropout_prob),\n        ]\n        layer_size_last = layer_size\n    layers += [nn.Linear(layer_sizes[-1], self._L_out)]\n    # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n    self.layers = nn.Sequential(*layers)\n    print(\"Leaving NetLinearBase.__init__()\")\n</code></pre>"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>torch.optim.Optimizer: The optimizer to use during training.</p> Source code in <code>spotPython/light/netlinearbase.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for the model.\n\n    Returns:\n        torch.optim.Optimizer: The optimizer to use during training.\n\n    \"\"\"\n    # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    print(\"Entering NetLinearBase.configure_optimizers()\")\n    optimizer = optimizer_handler(\n        optimizer_name=self.hparams.optimizer, params=self.parameters(), lr_mult=self.hparams.lr_mult\n    )\n    print(\"Leaving NetLinearBase.configure_optimizers()\")\n    return optimizer\n</code></pre>"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.forward","title":"<code>forward(x)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>A tensor containing a batch of input data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the probabilities for each class.</p> <p>Examples:     &gt;&gt;&gt; from torch.utils.data import DataLoader     &gt;&gt;&gt; from torchvision.datasets import MNIST     &gt;&gt;&gt; from torchvision.transforms import ToTensor     &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())     &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)     &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,                                       epochs=10,                                       batch_size=BATCH_SIZE,                                       initialization=\u2019xavier\u2019, act_fn=nn.ReLU(),                                       optimizer=\u2019Adam\u2019, dropout_prob=0.1, lr_mult=0.1,                                       patience=5)</p> Source code in <code>spotPython/light/netlinearbase.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        x (torch.Tensor): A tensor containing a batch of input data.\n\n    Returns:\n        torch.Tensor: A tensor containing the probabilities for each class.\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                          epochs=10,\n                                          batch_size=BATCH_SIZE,\n                                          initialization='xavier', act_fn=nn.ReLU(),\n                                          optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                          patience=5)\n\n    \"\"\"\n    print(\"Entering NetLinearBase.forward()\")\n    x = self.layers(x)\n    return F.softmax(x, dim=1)\n</code></pre>"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.test_step","title":"<code>test_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing the loss and accuracy for this batch.</p> Source code in <code>spotPython/light/netlinearbase.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False) -&gt; tuple:\n    \"\"\"\n    Performs a single test step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        tuple: A tuple containing the loss and accuracy for this batch.\n    \"\"\"\n    print(\"Entering NetLinearBase.test_step()\")\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = self._loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    metric = self._metric(preds, y, task=\"multiclass\", num_classes=self._L_out)\n    self.log(\"val_loss\", loss, prog_bar=prog_bar)\n    self.log(\"val_metric\", metric, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n    return loss, metric\n</code></pre>"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.training_step","title":"<code>training_step(batch)</code>","text":"<p>Performs a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the loss for this batch.</p> <p>Examples:     &gt;&gt;&gt; from torch.utils.data import DataLoader     &gt;&gt;&gt; from torchvision.datasets import MNIST     &gt;&gt;&gt; from torchvision.transforms import ToTensor     &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())     &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)     &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,                                         epochs=10,                                         batch_size=BATCH_SIZE,                                         initialization=\u2019xavier\u2019, act_fn=nn.ReLU(),                                         optimizer=\u2019Adam\u2019, dropout_prob=0.1, lr_mult=0.1,                                         patience=5)     &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)     &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)</p> Source code in <code>spotPython/light/netlinearbase.py</code> <pre><code>def training_step(self, batch: tuple) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a single training step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n\n    Returns:\n        torch.Tensor: A tensor containing the loss for this batch.\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; train_data = MNIST(PATH_DATASETS, train=True, download=True, transform=ToTensor())\n        &gt;&gt;&gt; train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                            epochs=10,\n                                            batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, train_loader)\n\n    \"\"\"\n    print(\"Entering NetLinearBase.training_step()\")\n    # x, y = batch\n    # print(\"NetLinearBase.training_step(): batch\")\n    # logits = self(x)\n    # print(\"NetLinearBase.training_step(): logits\")\n    # # compute loss (default: cross entropy loss) from logits and y\n    # loss = self._loss(logits, y)\n    # # self.train_mapk(logits, y)\n    # # self.log(\"train_mapk\", self.train_mapk, on_step=True, on_epoch=False)\n    # return loss\n    return 0.1234\n</code></pre>"},{"location":"reference/spotPython/light/netlinearbase/#spotPython.light.netlinearbase.NetLinearBase.validation_step","title":"<code>validation_step(batch, batch_idx, prog_bar=False)</code>","text":"<p>Performs a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>A tuple containing a batch of input data and labels.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the current batch.</p> required <code>prog_bar</code> <code>bool</code> <p>Whether to display the progress bar. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:     &gt;&gt;&gt; from torch.utils.data import DataLoader     &gt;&gt;&gt; from torchvision.datasets import MNIST     &gt;&gt;&gt; from torchvision.transforms import ToTensor     &gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())     &gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)     &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,                                         epochs=10,                                         batch_size=BATCH_SIZE,                                         initialization=\u2019xavier\u2019, act_fn=nn.ReLU(),                                         optimizer=\u2019Adam\u2019, dropout_prob=0.1, lr_mult=0.1,                                         patience=5)     &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)     &gt;&gt;&gt; trainer.fit(net_light_base, val_loader)</p> Source code in <code>spotPython/light/netlinearbase.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int, prog_bar: bool = False):\n    \"\"\"\n    Performs a single validation step.\n\n    Args:\n        batch (tuple): A tuple containing a batch of input data and labels.\n        batch_idx (int): The index of the current batch.\n        prog_bar (bool, optional): Whether to display the progress bar. Defaults to False.\n\n    Returns:\n        (NoneType): None\n    Examples:\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; from torchvision.datasets import MNIST\n        &gt;&gt;&gt; from torchvision.transforms import ToTensor\n        &gt;&gt;&gt; val_data = MNIST(PATH_DATASETS, train=False, download=True, transform=ToTensor())\n        &gt;&gt;&gt; val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n        &gt;&gt;&gt; net_light_base = NetLightBase(l1=128,\n                                            epochs=10,\n                                            batch_size=BATCH_SIZE,\n                                            initialization='xavier', act_fn=nn.ReLU(),\n                                            optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n                                            patience=5)\n        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=10)\n        &gt;&gt;&gt; trainer.fit(net_light_base, val_loader)\n\n    \"\"\"\n    print(\"Entering NetLinearBase.validation_step()\")\n    x, y = batch\n    logits = self(x)\n    # compute cross entropy loss from logits and y\n    loss = self._loss(logits, y)\n    # loss = F.nll_loss(logits, y)\n    preds = torch.argmax(logits, dim=1)\n    metric = self._metric(preds, y, task=\"multiclass\", num_classes=self._L_out)\n    self.log(\"val_loss\", loss, prog_bar=prog_bar)\n    self.log(\"val_metric\", metric, prog_bar=prog_bar)\n    self.log(\"hp_metric\", loss, prog_bar=prog_bar)\n</code></pre>"},{"location":"reference/spotPython/light/trainmodel/","title":"trainmodel","text":""},{"location":"reference/spotPython/light/trainmodel/#spotPython.light.trainmodel.train_model","title":"<code>train_model(config, fun_control)</code>","text":"<p>Trains a model for the given configuration and control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>dictionary containing the configuration for the hyperparameter tuning.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <p>Returns:</p> Type Description <code>object</code> <p>model object.</p> <code>dict</code> <p>dictionary containing the evaluation results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.light.trainmodel import train_model\n    config = {\"c_in\": 3,\n                \"c_out\": 10,\n                \"act_fn\": nn.ReLU,\n                \"optimizer_name\": \"Adam\",\n                \"optimizer_hparams\": {\"lr\": 1e-3, \"weight_decay\": 1e-4}}\n    fun_control = {\"core_model\": GoogleNet}\n    model, result = train_model(config, fun_control)\n    result\n    {'test': 0.8772, 'val': 0.8772}\n</code></pre> Source code in <code>spotPython/light/trainmodel.py</code> <pre><code>def train_model(config: dict, fun_control: dict):\n    \"\"\"\n    Trains a model for the given configuration and control parameters.\n\n    Args:\n        config (dict):\n            dictionary containing the configuration for the hyperparameter tuning.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        (object):\n            model object.\n        (dict):\n            dictionary containing the evaluation results.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.light.trainmodel import train_model\n            config = {\"c_in\": 3,\n                        \"c_out\": 10,\n                        \"act_fn\": nn.ReLU,\n                        \"optimizer_name\": \"Adam\",\n                        \"optimizer_hparams\": {\"lr\": 1e-3, \"weight_decay\": 1e-4}}\n            fun_control = {\"core_model\": GoogleNet}\n            model, result = train_model(config, fun_control)\n            result\n            {'test': 0.8772, 'val': 0.8772}\n\n    \"\"\"\n    print(\"train_model: Starting\")\n    print(f\"train_model: config: {config}\")\n    save_name = fun_control[\"core_model\"].__name__\n    # Create PyTorch Lightning data loaders\n    CHECKPOINT_PATH = fun_control[\"CHECKPOINT_PATH\"]\n    DATASET_PATH = fun_control[\"DATASET_PATH\"]\n\n    # Create PyTorch Lightning data loaders\n    # TODO: Replace this by data loaders external to train_model method:\n\n    train_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)\n    DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0, 1, 2))\n    DATA_STD = (train_dataset.data / 255.0).std(axis=(0, 1, 2))\n    print(\"Data mean\", DATA_MEANS)\n    print(\"Data std\", DATA_STD)\n    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(DATA_MEANS, DATA_STD)])\n    # For training, we add some augmentation. Networks are too powerful and would overfit.\n    train_transform = transforms.Compose(\n        [\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n            transforms.ToTensor(),\n            transforms.Normalize(DATA_MEANS, DATA_STD),\n        ]\n    )\n    # Loading the training dataset. We need to split it into a training and validation part\n    # We need to do a little trick because the validation set should not use the augmentation.\n    train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n    val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n    L.seed_everything(42)\n    train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n    L.seed_everything(42)\n    _, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n\n    # Loading the test set\n    test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n\n    # We define a set of data loaders that we can use for various purposes later.\n    train_loader = data.DataLoader(\n        train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4\n    )\n    val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n    test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n    # END TODO\n\n    # Create a PyTorch Lightning trainer with the generation callback\n    print(\"train_model: Creating trainer\")\n    trainer = L.Trainer(\n        default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),  # Where to save models\n        # We run on a single GPU (if possible)\n        accelerator=\"auto\",\n        devices=1,\n        # How many epochs to train for if no patience is set\n        max_epochs=4,\n        callbacks=[\n            ModelCheckpoint(\n                save_weights_only=True, mode=\"max\", monitor=\"val_acc\"\n            ),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n            LearningRateMonitor(\"epoch\"),\n        ],  # Log learning rate every epoch\n    )  # In case your notebook crashes due to the progress bar, consider increasing the refresh rate\n    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n    print(\"train_model: Created trainer\")\n\n    # Check whether pretrained model exists. If yes, load it and skip training\n    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n    if os.path.isfile(pretrained_filename):\n        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n        # Automatically loads the model with the saved hyperparameters\n        model = NetCNNBase.load_from_checkpoint(pretrained_filename)\n    else:\n        L.seed_everything(42)  # To be reproducable\n        print(\"train_model: Creating model\")\n        model = NetCNNBase(\n            model_name=fun_control[\"core_model\"].__name__,\n            model_hparams=config,\n            optimizer_name=\"Adam\",\n            optimizer_hparams={\"lr\": 1e-3, \"weight_decay\": 1e-4},\n        )  # Create model\n        trainer.fit(model, train_loader, val_loader)\n        model = NetCNNBase.load_from_checkpoint(\n            trainer.checkpoint_callback.best_model_path\n        )  # Load best checkpoint after training\n\n    # Test best model on validation and test set\n    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n\n    return model, result\n</code></pre>"},{"location":"reference/spotPython/light/traintest/","title":"traintest","text":""},{"location":"reference/spotPython/light/traintest/#spotPython.light.traintest.cv_model","title":"<code>cv_model(config, fun_control)</code>","text":"<p>Performs k-fold cross-validation on a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean average precision at k (MAP@k) score of the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n...     \"k_folds\": 5,\n... }\n&gt;&gt;&gt; mapk_score = cv_model(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest.py</code> <pre><code>def cv_model(config: dict, fun_control: dict) -&gt; float:\n    \"\"\"\n    Performs k-fold cross-validation on a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        (float): The mean average precision at k (MAP@k) score of the model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ...     \"k_folds\": 5,\n        ... }\n        &gt;&gt;&gt; mapk_score = cv_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"CV\" postfix to config_id\n    config_id = generate_config_id(config) + \"_CV\"\n    results = []\n    num_folds = fun_control[\"k_folds\"]\n    split_seed = 12345\n\n    for k in range(num_folds):\n        print(\"k:\", k)\n\n        model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out)\n        initialization = config[\"initialization\"]\n        if initialization == \"Xavier\":\n            xavier_init(model)\n        elif initialization == \"Kaiming\":\n            kaiming_init(model)\n        else:\n            pass\n        # print(f\"model: {model}\")\n\n        dm = CrossValidationDataModule(\n            k=k,\n            num_splits=num_folds,\n            split_seed=split_seed,\n            batch_size=config[\"batch_size\"],\n            data_dir=fun_control[\"DATASET_PATH\"],\n        )\n        dm.prepare_data()\n        dm.setup()\n\n        # Init trainer\n        trainer = L.Trainer(\n            # Where to save models\n            default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n            max_epochs=model.hparams.epochs,\n            accelerator=\"auto\",\n            devices=1,\n            logger=TensorBoardLogger(\n                save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True\n            ),\n            callbacks=[\n                EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)\n            ],\n            enable_progress_bar=enable_progress_bar,\n        )\n        # Pass the datamodule as arg to trainer.fit to override model hooks :)\n        trainer.fit(model=model, datamodule=dm)\n        # Test best model on validation and test set\n        # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\n        score = trainer.validate(model=model, datamodule=dm)\n        # unlist the result (from a list of one dict)\n        score = score[0]\n        print(f\"train_model result: {score}\")\n\n        results.append(score[\"valid_mapk\"])\n\n    mapk_score = sum(results) / num_folds\n    # print(f\"cv_model mapk result: {mapk_score}\")\n    return mapk_score\n</code></pre>"},{"location":"reference/spotPython/light/traintest/#spotPython.light.traintest.load_light_from_checkpoint","title":"<code>load_light_from_checkpoint(config, fun_control, postfix='_TEST')</code>","text":"<p>Loads a model from a checkpoint using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <code>postfix</code> <code>str</code> <p>The postfix to append to the configuration ID when generating the checkpoint path.</p> <code>'_TEST'</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The loaded model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"core_model\": MyModel,\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; model = load_light_from_checkpoint(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest.py</code> <pre><code>def load_light_from_checkpoint(config: dict, fun_control: dict, postfix: str = \"_TEST\") -&gt; Any:\n    \"\"\"\n    Loads a model from a checkpoint using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n        postfix (str): The postfix to append to the configuration ID when generating the checkpoint path.\n\n    Returns:\n        Any: The loaded model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"core_model\": MyModel,\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; model = load_light_from_checkpoint(config, fun_control)\n    \"\"\"\n    config_id = generate_config_id(config) + postfix\n    # default_root_dir = fun_control[\"TENSORBOARD_PATH\"] + \"lightning_logs/\" + config_id + \"/checkpoints/last.ckpt\"\n    default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id, \"last.ckpt\")\n    # default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id)\n    print(f\"Loading model from {default_root_dir}\")\n    model = fun_control[\"core_model\"].load_from_checkpoint(\n        default_root_dir, _L_in=fun_control[\"_L_in\"], _L_out=fun_control[\"_L_out\"]\n    )\n    # disable randomness, dropout, etc...\n    model.eval()\n    return model\n</code></pre>"},{"location":"reference/spotPython/light/traintest/#spotPython.light.traintest.test_model","title":"<code>test_model(config, fun_control)</code>","text":"<p>Tests a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: The validation loss and accuracy of the tested model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; val_loss, val_acc = test_model(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest.py</code> <pre><code>def test_model(config: dict, fun_control: dict) -&gt; Tuple[float, float]:\n    \"\"\"\n    Tests a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        Tuple[float, float]: The validation loss and accuracy of the tested model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; val_loss, val_acc = test_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"TEST\" postfix to config_id\n    config_id = generate_config_id(config) + \"_TEST\"\n    # Init DataModule\n    dm = CSVDataModule(\n        batch_size=config[\"batch_size\"],\n        num_workers=fun_control[\"num_workers\"],\n        data_dir=fun_control[\"DATASET_PATH\"],\n    )\n    # Init model from datamodule's attributes\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out)\n    initialization = config[\"initialization\"]\n    if initialization == \"Xavier\":\n        xavier_init(model)\n    elif initialization == \"Kaiming\":\n        kaiming_init(model)\n    else:\n        pass\n    # print(f\"model: {model}\")\n    # Init trainer\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=\"auto\",\n        devices=1,\n        logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True),\n        callbacks=[\n            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False),\n            ModelCheckpoint(\n                dirpath=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id), save_last=True\n            ),  # Save the last checkpoint\n        ],\n        enable_progress_bar=enable_progress_bar,\n    )\n    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n    trainer.fit(model=model, datamodule=dm)\n    test_result = trainer.test(datamodule=dm, ckpt_path=\"last\")\n    test_result = test_result[0]\n    # print(f\"test_model result: {test_result}\")\n    return test_result[\"val_loss\"], test_result[\"val_acc\"]\n</code></pre>"},{"location":"reference/spotPython/light/traintest/#spotPython.light.traintest.train_model","title":"<code>train_model(config, fun_control)</code>","text":"<p>Trains a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The validation loss of the trained model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; val_loss = train_model(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest.py</code> <pre><code>def train_model(config: dict, fun_control: dict) -&gt; float:\n    \"\"\"\n    Trains a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        float: The validation loss of the trained model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; val_loss = train_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    # print(f\"_L_in: {_L_in}\")\n    # print(f\"_L_out: {_L_out}\")\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    config_id = generate_config_id(config)\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out)\n    initialization = config[\"initialization\"]\n    if initialization == \"Xavier\":\n        xavier_init(model)\n    elif initialization == \"Kaiming\":\n        kaiming_init(model)\n    else:\n        pass\n    # print(f\"model: {model}\")\n\n    # Init DataModule\n    dm = CSVDataModule(\n        batch_size=config[\"batch_size\"],\n        num_workers=fun_control[\"num_workers\"],\n        data_dir=fun_control[\"DATASET_PATH\"],\n    )\n\n    # Init trainer\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=\"auto\",\n        devices=1,\n        logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True),\n        callbacks=[\n            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)\n        ],\n        enable_progress_bar=enable_progress_bar,\n    )\n    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n    trainer.fit(model=model, datamodule=dm)\n    # Test best model on validation and test set\n    # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\n    result = trainer.validate(model=model, datamodule=dm)\n    # unlist the result (from a list of one dict)\n    result = result[0]\n    # print(f\"train_model result: {result}\")\n    return result[\"val_loss\"]\n</code></pre>"},{"location":"reference/spotPython/light/traintest_NEW/","title":"traintest_NEW","text":""},{"location":"reference/spotPython/light/traintest_NEW/#spotPython.light.traintest_NEW.cv_model","title":"<code>cv_model(config, fun_control)</code>","text":"<p>Performs k-fold cross-validation on a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean average precision at k (MAP@k) score of the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n...     \"k_folds\": 5,\n... }\n&gt;&gt;&gt; mapk_score = cv_model(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest_NEW.py</code> <pre><code>def cv_model(config: dict, fun_control: dict) -&gt; float:\n    \"\"\"\n    Performs k-fold cross-validation on a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        (float): The mean average precision at k (MAP@k) score of the model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ...     \"k_folds\": 5,\n        ... }\n        &gt;&gt;&gt; mapk_score = cv_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"CV\" postfix to config_id\n    config_id = generate_config_id(config) + \"_CV\"\n    results = []\n    num_folds = fun_control[\"k_folds\"]\n    split_seed = 12345\n\n    for k in range(num_folds):\n        print(\"k:\", k)\n\n        model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out)\n        initialization = config[\"initialization\"]\n        if initialization == \"Xavier\":\n            xavier_init(model)\n        elif initialization == \"Kaiming\":\n            kaiming_init(model)\n        else:\n            pass\n        # print(f\"model: {model}\")\n\n        dm = CrossValidationDataModule(\n            k=k,\n            num_splits=num_folds,\n            split_seed=split_seed,\n            batch_size=config[\"batch_size\"],\n            data_dir=fun_control[\"DATASET_PATH\"],\n        )\n        dm.prepare_data()\n        dm.setup()\n\n        # Init trainer\n        trainer = L.Trainer(\n            # Where to save models\n            default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n            max_epochs=model.hparams.epochs,\n            accelerator=\"auto\",\n            devices=1,\n            logger=TensorBoardLogger(\n                save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True\n            ),\n            callbacks=[\n                EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)\n            ],\n            enable_progress_bar=enable_progress_bar,\n        )\n        # Pass the datamodule as arg to trainer.fit to override model hooks :)\n        trainer.fit(model=model, datamodule=dm)\n        # Test best model on validation and test set\n        # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\n        score = trainer.validate(model=model, datamodule=dm)\n        # unlist the result (from a list of one dict)\n        score = score[0]\n        print(f\"train_model result: {score}\")\n\n        results.append(score[\"valid_mapk\"])\n\n    mapk_score = sum(results) / num_folds\n    # print(f\"cv_model mapk result: {mapk_score}\")\n    return mapk_score\n</code></pre>"},{"location":"reference/spotPython/light/traintest_NEW/#spotPython.light.traintest_NEW.load_light_from_checkpoint","title":"<code>load_light_from_checkpoint(config, fun_control, postfix='_TEST')</code>","text":"<p>Loads a model from a checkpoint using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <code>postfix</code> <code>str</code> <p>The postfix to append to the configuration ID when generating the checkpoint path.</p> <code>'_TEST'</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The loaded model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"core_model\": MyModel,\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; model = load_light_from_checkpoint(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest_NEW.py</code> <pre><code>def load_light_from_checkpoint(config: dict, fun_control: dict, postfix: str = \"_TEST\") -&gt; Any:\n    \"\"\"\n    Loads a model from a checkpoint using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n        postfix (str): The postfix to append to the configuration ID when generating the checkpoint path.\n\n    Returns:\n        Any: The loaded model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"core_model\": MyModel,\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; model = load_light_from_checkpoint(config, fun_control)\n    \"\"\"\n    config_id = generate_config_id(config) + postfix\n    default_root_dir = fun_control[\"TENSORBOARD_PATH\"] + \"lightning_logs/\" + config_id + \"/checkpoints/last.ckpt\"\n    # default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id)\n    print(f\"Loading model from {default_root_dir}\")\n    model = fun_control[\"core_model\"].load_from_checkpoint(\n        default_root_dir, _L_in=fun_control[\"_L_in\"], _L_out=fun_control[\"_L_out\"]\n    )\n    # disable randomness, dropout, etc...\n    model.eval()\n    return model\n</code></pre>"},{"location":"reference/spotPython/light/traintest_NEW/#spotPython.light.traintest_NEW.test_model","title":"<code>test_model(config, fun_control)</code>","text":"<p>Tests a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: The validation loss and accuracy of the tested model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; val_loss, val_acc = test_model(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest_NEW.py</code> <pre><code>def test_model(config: dict, fun_control: dict) -&gt; Tuple[float, float]:\n    \"\"\"\n    Tests a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        Tuple[float, float]: The validation loss and accuracy of the tested model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; val_loss, val_acc = test_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"TEST\" postfix to config_id\n    config_id = generate_config_id(config) + \"_TEST\"\n    # Init DataModule\n    dm = CIFAR10DataModule(\n        batch_size=config[\"batch_size\"], data_dir=fun_control[\"DATASET_PATH\"], num_workers=fun_control[\"num_workers\"]\n    )\n    # Init model from datamodule's attributes\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out)\n    initialization = config[\"initialization\"]\n    if initialization == \"Xavier\":\n        xavier_init(model)\n    elif initialization == \"Kaiming\":\n        kaiming_init(model)\n    else:\n        pass\n    # print(f\"model: {model}\")\n    # Init trainer\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=\"auto\",\n        devices=1,\n        logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True),\n        callbacks=[\n            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False),\n            ModelCheckpoint(save_last=True),  # Save the last checkpoint\n        ],\n        enable_progress_bar=enable_progress_bar,\n    )\n    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n    trainer.fit(model=model, datamodule=dm)\n    test_result = trainer.test(datamodule=dm, ckpt_path=\"last\")\n    test_result = test_result[0]\n    # print(f\"test_model result: {test_result}\")\n    return test_result[\"val_loss\"], test_result[\"val_acc\"]\n</code></pre>"},{"location":"reference/spotPython/light/traintest_NEW/#spotPython.light.traintest_NEW.train_model","title":"<code>train_model(config, fun_control)</code>","text":"<p>Trains a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The validation loss of the trained model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; val_loss = train_model(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest_NEW.py</code> <pre><code>def train_model(config: dict, fun_control: dict) -&gt; float:\n    \"\"\"\n    Trains a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        float: The validation loss of the trained model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; val_loss = train_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    # print(f\"_L_in: {_L_in}\")\n    # print(f\"_L_out: {_L_out}\")\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    config_id = generate_config_id(config)\n    # print(f\"config_id: {config_id}\")\n    # print(f\"config: {config}\")\n    # print(f\"fun_control core model: {fun_control['core_model']}\")\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out)\n    # print(f\"model: {model}\")\n    initialization = config[\"initialization\"]\n    if initialization == \"Xavier\":\n        xavier_init(model)\n    elif initialization == \"Kaiming\":\n        kaiming_init(model)\n    else:\n        pass\n    # print(f\"model: {model}\")\n\n    # Init DataModule\n    dm = CIFAR10DataModule(\n        batch_size=config[\"batch_size\"], data_dir=fun_control[\"DATASET_PATH\"], num_workers=fun_control[\"num_workers\"]\n    )\n    dm.prepare_data()\n    dm.setup()\n    print(\"Leaving dm.setup()\")\n    # Init trainer\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=\"auto\",\n        devices=1,\n        logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True),\n        callbacks=[\n            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)\n        ],\n        enable_progress_bar=enable_progress_bar,\n    )\n    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n    print(\"train.model: Entering trainer.fit()\")\n    trainer.fit(model=model, datamodule=dm)\n    print(\"train.model: Leaving trainer.fit()\")\n    # Test best model on validation and test set\n    # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\n    result = trainer.validate(model=model, datamodule=dm)\n    # unlist the result (from a list of one dict)\n    result = result[0]\n    print(f\"train_model result: {result}\")\n    return result[\"val_loss\"]\n</code></pre>"},{"location":"reference/spotPython/light/traintest_OLD/","title":"traintest_OLD","text":""},{"location":"reference/spotPython/light/traintest_OLD/#spotPython.light.traintest_OLD.cv_model","title":"<code>cv_model(config, fun_control)</code>","text":"<p>Performs k-fold cross-validation on a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean average precision at k (MAP@k) score of the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n...     \"k_folds\": 5,\n... }\n&gt;&gt;&gt; mapk_score = cv_model(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest_OLD.py</code> <pre><code>def cv_model(config: dict, fun_control: dict) -&gt; float:\n    \"\"\"\n    Performs k-fold cross-validation on a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        (float): The mean average precision at k (MAP@k) score of the model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ...     \"k_folds\": 5,\n        ... }\n        &gt;&gt;&gt; mapk_score = cv_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"CV\" postfix to config_id\n    config_id = generate_config_id(config) + \"_CV\"\n    results = []\n    num_folds = fun_control[\"k_folds\"]\n    split_seed = 12345\n\n    for k in range(num_folds):\n        print(\"k:\", k)\n\n        model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out)\n        initialization = config[\"initialization\"]\n        if initialization == \"Xavier\":\n            xavier_init(model)\n        elif initialization == \"Kaiming\":\n            kaiming_init(model)\n        else:\n            pass\n        # print(f\"model: {model}\")\n\n        dm = CrossValidationDataModule(\n            k=k,\n            num_splits=num_folds,\n            split_seed=split_seed,\n            batch_size=config[\"batch_size\"],\n            DATASET_PATH=fun_control[\"DATASET_PATH\"],\n        )\n        dm.prepare_data()\n        dm.setup()\n\n        # Init trainer\n        trainer = L.Trainer(\n            # Where to save models\n            default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n            max_epochs=model.hparams.epochs,\n            accelerator=\"auto\",\n            devices=1,\n            logger=TensorBoardLogger(\n                save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True\n            ),\n            callbacks=[\n                EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)\n            ],\n            enable_progress_bar=enable_progress_bar,\n        )\n        # Pass the datamodule as arg to trainer.fit to override model hooks :)\n        trainer.fit(model=model, datamodule=dm)\n        # Test best model on validation and test set\n        # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\n        score = trainer.validate(model=model, datamodule=dm)\n        # unlist the result (from a list of one dict)\n        score = score[0]\n        print(f\"train_model result: {score}\")\n\n        results.append(score[\"valid_mapk\"])\n\n    mapk_score = sum(results) / num_folds\n    # print(f\"cv_model mapk result: {mapk_score}\")\n    return mapk_score\n</code></pre>"},{"location":"reference/spotPython/light/traintest_OLD/#spotPython.light.traintest_OLD.load_light_from_checkpoint","title":"<code>load_light_from_checkpoint(config, fun_control, postfix='_TEST')</code>","text":"<p>Loads a model from a checkpoint using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <code>postfix</code> <code>str</code> <p>The postfix to append to the configuration ID when generating the checkpoint path.</p> <code>'_TEST'</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The loaded model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"core_model\": MyModel,\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; model = load_light_from_checkpoint(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest_OLD.py</code> <pre><code>def load_light_from_checkpoint(config: dict, fun_control: dict, postfix: str = \"_TEST\") -&gt; Any:\n    \"\"\"\n    Loads a model from a checkpoint using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n        postfix (str): The postfix to append to the configuration ID when generating the checkpoint path.\n\n    Returns:\n        Any: The loaded model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"core_model\": MyModel,\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; model = load_light_from_checkpoint(config, fun_control)\n    \"\"\"\n    config_id = generate_config_id(config) + postfix\n    default_root_dir = fun_control[\"TENSORBOARD_PATH\"] + \"lightning_logs/\" + config_id + \"/checkpoints/last.ckpt\"\n    # default_root_dir = os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id)\n    print(f\"Loading model from {default_root_dir}\")\n    model = fun_control[\"core_model\"].load_from_checkpoint(\n        default_root_dir, _L_in=fun_control[\"_L_in\"], _L_out=fun_control[\"_L_out\"]\n    )\n    # disable randomness, dropout, etc...\n    model.eval()\n    return model\n</code></pre>"},{"location":"reference/spotPython/light/traintest_OLD/#spotPython.light.traintest_OLD.test_model","title":"<code>test_model(config, fun_control)</code>","text":"<p>Tests a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: The validation loss and accuracy of the tested model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; val_loss, val_acc = test_model(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest_OLD.py</code> <pre><code>def test_model(config: dict, fun_control: dict) -&gt; Tuple[float, float]:\n    \"\"\"\n    Tests a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        Tuple[float, float]: The validation loss and accuracy of the tested model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; val_loss, val_acc = test_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    # Add \"TEST\" postfix to config_id\n    config_id = generate_config_id(config) + \"_TEST\"\n    # Init DataModule\n    dm = CSVDataModule(\n        batch_size=config[\"batch_size\"],\n        num_workers=fun_control[\"num_workers\"],\n        DATASET_PATH=fun_control[\"DATASET_PATH\"],\n    )\n    # Init model from datamodule's attributes\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out)\n    initialization = config[\"initialization\"]\n    if initialization == \"Xavier\":\n        xavier_init(model)\n    elif initialization == \"Kaiming\":\n        kaiming_init(model)\n    else:\n        pass\n    # print(f\"model: {model}\")\n    # Init trainer\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=\"auto\",\n        devices=1,\n        logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True),\n        callbacks=[\n            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False),\n            ModelCheckpoint(save_last=True),  # Save the last checkpoint\n        ],\n        enable_progress_bar=enable_progress_bar,\n    )\n    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n    trainer.fit(model=model, datamodule=dm)\n    test_result = trainer.test(datamodule=dm, ckpt_path=\"last\")\n    test_result = test_result[0]\n    # print(f\"test_model result: {test_result}\")\n    return test_result[\"val_loss\"], test_result[\"val_acc\"]\n</code></pre>"},{"location":"reference/spotPython/light/traintest_OLD/#spotPython.light.traintest_OLD.train_model","title":"<code>train_model(config, fun_control)</code>","text":"<p>Trains a model using the given configuration and function control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration parameters for the model.</p> required <code>fun_control</code> <code>dict</code> <p>A dictionary containing the function control parameters.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The validation loss of the trained model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"initialization\": \"Xavier\",\n...     \"batch_size\": 32,\n...     \"patience\": 10,\n... }\n&gt;&gt;&gt; fun_control = {\n...     \"_L_in\": 10,\n...     \"_L_out\": 1,\n...     \"enable_progress_bar\": True,\n...     \"core_model\": MyModel,\n...     \"num_workers\": 4,\n...     \"DATASET_PATH\": \"./data\",\n...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n... }\n&gt;&gt;&gt; val_loss = train_model(config, fun_control)\n</code></pre> Source code in <code>spotPython/light/traintest_OLD.py</code> <pre><code>def train_model(config: dict, fun_control: dict) -&gt; float:\n    \"\"\"\n    Trains a model using the given configuration and function control parameters.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters for the model.\n        fun_control (dict): A dictionary containing the function control parameters.\n\n    Returns:\n        float: The validation loss of the trained model.\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     \"initialization\": \"Xavier\",\n        ...     \"batch_size\": 32,\n        ...     \"patience\": 10,\n        ... }\n        &gt;&gt;&gt; fun_control = {\n        ...     \"_L_in\": 10,\n        ...     \"_L_out\": 1,\n        ...     \"enable_progress_bar\": True,\n        ...     \"core_model\": MyModel,\n        ...     \"num_workers\": 4,\n        ...     \"DATASET_PATH\": \"./data\",\n        ...     \"CHECKPOINT_PATH\": \"./checkpoints\",\n        ...     \"TENSORBOARD_PATH\": \"./tensorboard\",\n        ... }\n        &gt;&gt;&gt; val_loss = train_model(config, fun_control)\n    \"\"\"\n    _L_in = fun_control[\"_L_in\"]\n    _L_out = fun_control[\"_L_out\"]\n    # print(f\"_L_in: {_L_in}\")\n    # print(f\"_L_out: {_L_out}\")\n    if fun_control[\"enable_progress_bar\"] is None:\n        enable_progress_bar = False\n    else:\n        enable_progress_bar = fun_control[\"enable_progress_bar\"]\n    config_id = generate_config_id(config)\n    model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out)\n    initialization = config[\"initialization\"]\n    if initialization == \"Xavier\":\n        xavier_init(model)\n    elif initialization == \"Kaiming\":\n        kaiming_init(model)\n    else:\n        pass\n    # print(f\"model: {model}\")\n\n    # Init DataModule\n    dm = CSVDataModule(\n        batch_size=config[\"batch_size\"],\n        num_workers=fun_control[\"num_workers\"],\n        DATASET_PATH=fun_control[\"DATASET_PATH\"],\n    )\n\n    # Init trainer\n    trainer = L.Trainer(\n        # Where to save models\n        default_root_dir=os.path.join(fun_control[\"CHECKPOINT_PATH\"], config_id),\n        max_epochs=model.hparams.epochs,\n        accelerator=\"auto\",\n        devices=1,\n        logger=TensorBoardLogger(save_dir=fun_control[\"TENSORBOARD_PATH\"], version=config_id, default_hp_metric=True),\n        callbacks=[\n            EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\", strict=False, verbose=False)\n        ],\n        enable_progress_bar=enable_progress_bar,\n    )\n    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n    trainer.fit(model=model, datamodule=dm)\n    # Test best model on validation and test set\n    # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\n    result = trainer.validate(model=model, datamodule=dm)\n    # unlist the result (from a list of one dict)\n    result = result[0]\n    # print(f\"train_model result: {result}\")\n    return result[\"val_loss\"]\n</code></pre>"},{"location":"reference/spotPython/light/utils/","title":"utils","text":""},{"location":"reference/spotPython/light/utils/#spotPython.light.utils.create_model","title":"<code>create_model(config, fun_control, **kwargs)</code>","text":"<p>Creates a model for the given configuration and control parameters.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>dictionary containing the configuration for the hyperparameter tuning.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <code>**kwargs</code> <p>additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>object</code> <p>model object.</p> Source code in <code>spotPython/light/utils.py</code> <pre><code>def create_model(config, fun_control, **kwargs) -&gt; object:\n    \"\"\"\n    Creates a model for the given configuration and control parameters.\n\n    Args:\n        config (dict):\n            dictionary containing the configuration for the hyperparameter tuning.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n        **kwargs:\n            additional keyword arguments.\n\n    Returns:\n        (object):\n            model object.\n    \"\"\"\n    return fun_control[\"core_model\"](**config, **kwargs)\n</code></pre>"},{"location":"reference/spotPython/light/utils/#spotPython.light.utils.get_tuned_architecture","title":"<code>get_tuned_architecture(spot_tuner, fun_control)</code>","text":"<p>Returns the tuned architecture.</p> <p>Parameters:</p> Name Type Description Default <code>spot_tuner</code> <code>object</code> <p>spot tuner object.</p> required <code>fun_control</code> <code>dict</code> <p>dictionary containing control parameters for the hyperparameter tuning.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the tuned architecture.</p> Source code in <code>spotPython/light/utils.py</code> <pre><code>def get_tuned_architecture(spot_tuner, fun_control) -&gt; dict:\n    \"\"\"\n    Returns the tuned architecture.\n\n    Args:\n        spot_tuner (object):\n            spot tuner object.\n        fun_control (dict):\n            dictionary containing control parameters for the hyperparameter tuning.\n\n    Returns:\n        (dict):\n            dictionary containing the tuned architecture.\n    \"\"\"\n    X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1, -1))\n    config = get_one_config_from_X(X, fun_control)\n    return config\n</code></pre>"},{"location":"reference/spotPython/light/cifar10/cifar10datamodule/","title":"cifar10datamodule","text":""},{"location":"reference/spotPython/light/cifar10/cifar10datamodule/#spotPython.light.cifar10.cifar10datamodule.CIFAR10DataModule","title":"<code>CIFAR10DataModule</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for handling CIFAR10 data.</p> Torchvision provides many built-in datasets in the torchvision.datasets module, <p>as well as utility classes for building your own datasets. All datasets are subclasses of torch.utils.data.Dataset i.e, they have getitem and len methods implemented. Hence, they can all be passed to a torch.utils.data.DataLoader which can load multiple samples in parallel using torch.multiprocessing workers, see [1].</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The size of the batch.</p> required <code>data_dir</code> <code>str</code> <p>The directory where the data is stored. Defaults to \u201c./data\u201d.</p> <code>'./data'</code> <code>num_workers</code> <code>int</code> <p>The number of workers for data loading. Defaults to 0.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>data_train</code> <code>Dataset</code> <p>The training dataset.</p> <code>data_val</code> <code>Dataset</code> <p>The validation dataset.</p> <code>data_test</code> <code>Dataset</code> <p>The test dataset.</p> References <p>[1] https://pytorch.org/vision/stable/datasets.html</p> Source code in <code>spotPython/light/cifar10/cifar10datamodule.py</code> <pre><code>class CIFAR10DataModule(pl.LightningDataModule):\n    \"\"\"\n    A LightningDataModule for handling CIFAR10 data.\n\n    Note: Torchvision provides many built-in datasets in the torchvision.datasets module,\n        as well as utility classes for building your own datasets. All datasets are subclasses\n        of torch.utils.data.Dataset i.e, they have __getitem__ and __len__ methods implemented.\n        Hence, they can all be passed to a torch.utils.data.DataLoader which can load multiple\n        samples in parallel using torch.multiprocessing workers, see [1].\n\n    Args:\n        batch_size (int): The size of the batch.\n        data_dir (str): The directory where the data is stored. Defaults to \"./data\".\n        num_workers (int): The number of workers for data loading. Defaults to 0.\n\n    Attributes:\n        data_train (Dataset): The training dataset.\n        data_val (Dataset): The validation dataset.\n        data_test (Dataset): The test dataset.\n\n    References:\n        [1] [https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html)\n    \"\"\"\n\n    def __init__(self, batch_size: int, data_dir: str = \"./data\", num_workers: int = 0):\n        super().__init__()\n        self.batch_size = batch_size\n        self.data_dir = data_dir\n        self.num_workers = num_workers\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Prepares the data for use.\"\"\"\n        # download\n        CIFAR10(root=self.data_dir, train=True, download=True)\n        CIFAR10(root=self.data_dir, train=False, download=True)\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Sets up the data for use.\n\n        Args:\n            stage (Optional[str]): The current stage. Defaults to None.\n\n        \"\"\"\n        # Assign appropriate data transforms, see\n        # https://lightning.ai/docs/pytorch/latest/notebooks/course_UvA-DL/04-inception-resnet-densenet.html\n        DATA_MEANS = (0.49139968, 0.48215841, 0.44653091)\n        DATA_STDS = (0.24703223, 0.24348513, 0.26158784)\n        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(DATA_MEANS, DATA_STDS)])\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            data_full = CIFAR10(root=self.data_dir, train=True, transform=transform)\n            test_abs = int(len(data_full) * 0.6)\n            self.data_train, self.data_val = random_split(data_full, [test_abs, len(data_full) - test_abs])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.data_test = CIFAR10(root=self.data_dir, train=False, transform=transform)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the training dataloader.\n\n        Returns:\n            DataLoader: The training dataloader.\n\n        \"\"\"\n        print(\"train_dataloader: self.batch_size\", self.batch_size)\n        return DataLoader(\n            self.data_train, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=self.num_workers\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the validation dataloader.\n\n        Returns:\n            DataLoader: The validation dataloader.\n\n\n        \"\"\"\n        print(\"val_dataloader: self.batch_size\", self.batch_size)\n        return DataLoader(\n            self.data_val, batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.num_workers\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Returns the test dataloader.\n\n        Returns:\n            DataLoader: The test dataloader.\n\n\n        \"\"\"\n        print(\"train_data_loader: self.batch_size\", self.batch_size)\n        return DataLoader(\n            self.data_test, batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.num_workers\n        )\n</code></pre>"},{"location":"reference/spotPython/light/cifar10/cifar10datamodule/#spotPython.light.cifar10.cifar10datamodule.CIFAR10DataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepares the data for use.</p> Source code in <code>spotPython/light/cifar10/cifar10datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Prepares the data for use.\"\"\"\n    # download\n    CIFAR10(root=self.data_dir, train=True, download=True)\n    CIFAR10(root=self.data_dir, train=False, download=True)\n</code></pre>"},{"location":"reference/spotPython/light/cifar10/cifar10datamodule/#spotPython.light.cifar10.cifar10datamodule.CIFAR10DataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Sets up the data for use.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>The current stage. Defaults to None.</p> <code>None</code> Source code in <code>spotPython/light/cifar10/cifar10datamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Sets up the data for use.\n\n    Args:\n        stage (Optional[str]): The current stage. Defaults to None.\n\n    \"\"\"\n    # Assign appropriate data transforms, see\n    # https://lightning.ai/docs/pytorch/latest/notebooks/course_UvA-DL/04-inception-resnet-densenet.html\n    DATA_MEANS = (0.49139968, 0.48215841, 0.44653091)\n    DATA_STDS = (0.24703223, 0.24348513, 0.26158784)\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(DATA_MEANS, DATA_STDS)])\n    # Assign train/val datasets for use in dataloaders\n    if stage == \"fit\" or stage is None:\n        data_full = CIFAR10(root=self.data_dir, train=True, transform=transform)\n        test_abs = int(len(data_full) * 0.6)\n        self.data_train, self.data_val = random_split(data_full, [test_abs, len(data_full) - test_abs])\n\n    # Assign test dataset for use in dataloader(s)\n    if stage == \"test\" or stage is None:\n        self.data_test = CIFAR10(root=self.data_dir, train=False, transform=transform)\n</code></pre>"},{"location":"reference/spotPython/light/cifar10/cifar10datamodule/#spotPython.light.cifar10.cifar10datamodule.CIFAR10DataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The test dataloader.</p> Source code in <code>spotPython/light/cifar10/cifar10datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the test dataloader.\n\n    Returns:\n        DataLoader: The test dataloader.\n\n\n    \"\"\"\n    print(\"train_data_loader: self.batch_size\", self.batch_size)\n    return DataLoader(\n        self.data_test, batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.num_workers\n    )\n</code></pre>"},{"location":"reference/spotPython/light/cifar10/cifar10datamodule/#spotPython.light.cifar10.cifar10datamodule.CIFAR10DataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The training dataloader.</p> Source code in <code>spotPython/light/cifar10/cifar10datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the training dataloader.\n\n    Returns:\n        DataLoader: The training dataloader.\n\n    \"\"\"\n    print(\"train_dataloader: self.batch_size\", self.batch_size)\n    return DataLoader(\n        self.data_train, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=self.num_workers\n    )\n</code></pre>"},{"location":"reference/spotPython/light/cifar10/cifar10datamodule/#spotPython.light.cifar10.cifar10datamodule.CIFAR10DataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The validation dataloader.</p> Source code in <code>spotPython/light/cifar10/cifar10datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Returns the validation dataloader.\n\n    Returns:\n        DataLoader: The validation dataloader.\n\n\n    \"\"\"\n    print(\"val_dataloader: self.batch_size\", self.batch_size)\n    return DataLoader(\n        self.data_val, batch_size=self.batch_size, shuffle=False, drop_last=False, num_workers=self.num_workers\n    )\n</code></pre>"},{"location":"reference/spotPython/light/cnn/googlenet/","title":"googlenet","text":""},{"location":"reference/spotPython/light/cnn/googlenet/#spotPython.light.cnn.googlenet.GoogleNet","title":"<code>GoogleNet</code>","text":"<p>             Bases: <code>Module</code></p> <p>GoogleNet architecture</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of classes for the classification task. Defaults to 10.</p> <code>10</code> <code>act_fn_name</code> <code>str</code> <p>Name of the activation function. Defaults to \u201crelu\u201d.</p> <code>'relu'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>hparams</code> <code>SimpleNamespace</code> <p>Namespace containing the hyperparameters.</p> <code>input_net</code> <code>Sequential</code> <p>Input network.</p> <code>inception_blocks</code> <code>Sequential</code> <p>Inception blocks.</p> <code>output_net</code> <code>Sequential</code> <p>Output network.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of the GoogleNet architecture</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.light.cnn.googlenet import GoogleNet\n    import torch\n    import torch.nn as nn\n    model = GoogleNet()\n    x = torch.randn(1, 3, 32, 32)\n    y = model(x)\n    y.shape\n    torch.Size([1, 10])\n</code></pre> Source code in <code>spotPython/light/cnn/googlenet.py</code> <pre><code>class GoogleNet(nn.Module):\n    \"\"\"GoogleNet architecture\n\n    Args:\n        num_classes (int):\n            Number of classes for the classification task. Defaults to 10.\n        act_fn_name (str):\n            Name of the activation function. Defaults to \"relu\".\n        **kwargs:\n            Additional keyword arguments.\n\n    Attributes:\n        hparams (SimpleNamespace):\n            Namespace containing the hyperparameters.\n        input_net (nn.Sequential):\n            Input network.\n        inception_blocks (nn.Sequential):\n            Inception blocks.\n        output_net (nn.Sequential):\n            Output network.\n\n    Returns:\n        (torch.Tensor):\n            Output tensor of the GoogleNet architecture\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.light.cnn.googlenet import GoogleNet\n            import torch\n            import torch.nn as nn\n            model = GoogleNet()\n            x = torch.randn(1, 3, 32, 32)\n            y = model(x)\n            y.shape\n            torch.Size([1, 10])\n    \"\"\"\n\n    def __init__(self, num_classes: int = 10, act_fn_name: str = \"relu\", **kwargs):\n        super().__init__()\n        # TODO: Replace this by act_fn handlers specified in the config file:\n        act_fn_by_name = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"leakyrelu\": nn.LeakyReLU, \"gelu\": nn.GELU}\n        self.hparams = SimpleNamespace(\n            num_classes=num_classes, act_fn_name=act_fn_name, act_fn=act_fn_by_name[act_fn_name]\n        )\n        self._create_network()\n        self._init_params()\n\n    def _create_network(self):\n        # A first convolution on the original image to scale up the channel size\n        self.input_net = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), self.hparams.act_fn()\n        )\n        # Stacking inception blocks\n        self.inception_blocks = nn.Sequential(\n            InceptionBlock(\n                64,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                64,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 24, \"3x3\": 48, \"5x5\": 12, \"max\": 12},\n                act_fn=self.hparams.act_fn,\n            ),\n            nn.MaxPool2d(3, stride=2, padding=1),  # 32x32 =&gt; 16x16\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 24, \"3x3\": 48, \"5x5\": 12, \"max\": 12},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 16, \"3x3\": 48, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 16, \"3x3\": 48, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                96,\n                c_red={\"3x3\": 32, \"5x5\": 16},\n                c_out={\"1x1\": 32, \"3x3\": 48, \"5x5\": 24, \"max\": 24},\n                act_fn=self.hparams.act_fn,\n            ),\n            nn.MaxPool2d(3, stride=2, padding=1),  # 16x16 =&gt; 8x8\n            InceptionBlock(\n                128,\n                c_red={\"3x3\": 48, \"5x5\": 16},\n                c_out={\"1x1\": 32, \"3x3\": 64, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n            InceptionBlock(\n                128,\n                c_red={\"3x3\": 48, \"5x5\": 16},\n                c_out={\"1x1\": 32, \"3x3\": 64, \"5x5\": 16, \"max\": 16},\n                act_fn=self.hparams.act_fn,\n            ),\n        )\n        # Mapping to classification output\n        self.output_net = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Linear(128, self.hparams.num_classes)\n        )\n\n    def _init_params(self):\n        # We should initialize the\n        # convolutions according to the activation function\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, nonlinearity=self.hparams.act_fn_name)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.input_net(x)\n        x = self.inception_blocks(x)\n        x = self.output_net(x)\n        return x\n</code></pre>"},{"location":"reference/spotPython/light/cnn/inceptionblock/","title":"inceptionblock","text":""},{"location":"reference/spotPython/light/cnn/inceptionblock/#spotPython.light.cnn.inceptionblock.InceptionBlock","title":"<code>InceptionBlock</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>spotPython/light/cnn/inceptionblock.py</code> <pre><code>class InceptionBlock(nn.Module):\n    def __init__(self, c_in, c_red: dict, c_out: dict, act_fn):\n        \"\"\"\n        Inception block as used in GoogLeNet.\n\n        Description from\n        [P. Lippe:INCEPTION, RESNET AND DENSENET](https://lightning.ai/docs/pytorch/stable/)\n        An Inception block applies four convolution blocks separately on the same feature map:\n        a 1x1, 3x3, and 5x5 convolution, and a max pool operation.\n        This allows the network to look at the same data with different receptive fields.\n        Of course, learning only 5x5 convolution would be theoretically more powerful.\n        However, this is not only more computation and memory heavy but also tends to overfit much easier.\n        The 1x1 convolutions are used to reduce the number of input channels to the 3x3 and 5x5 convolutions,\n        which reduces the number of parameters and computation.\n\n        Args:\n            c_in:\n                Number of input feature maps from the previous layers\n            c_red:\n                Dictionary with keys \"3x3\" and \"5x5\" specifying\n                the output of the dimensionality reducing 1x1 convolutions\n            c_out:\n                Dictionary with keys \"1x1\", \"3x3\", \"5x5\", and \"max\"\n            act_fn:\n                Activation class constructor (e.g. nn.ReLU)\n\n        Returns:\n            torch.Tensor:\n                Output tensor of the inception block\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.light.cnn.googlenet import InceptionBlock\n                import torch\n                import torch.nn as nn\n                block = InceptionBlock(3,\n                            {\"3x3\": 32, \"5x5\": 16},\n                            {\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8},\n                            nn.ReLU)\n                x = torch.randn(1, 3, 32, 32)\n                y = block(x)\n                y.shape\n                torch.Size([1, 64, 32, 32])\n\n        \"\"\"\n        super().__init__()\n\n        # 1x1 convolution branch\n        self.conv_1x1 = nn.Sequential(\n            nn.Conv2d(c_in, c_out[\"1x1\"], kernel_size=1), nn.BatchNorm2d(c_out[\"1x1\"]), act_fn()\n        )\n\n        # 3x3 convolution branch\n        self.conv_3x3 = nn.Sequential(\n            nn.Conv2d(c_in, c_red[\"3x3\"], kernel_size=1),\n            nn.BatchNorm2d(c_red[\"3x3\"]),\n            act_fn(),\n            nn.Conv2d(c_red[\"3x3\"], c_out[\"3x3\"], kernel_size=3, padding=1),\n            nn.BatchNorm2d(c_out[\"3x3\"]),\n            act_fn(),\n        )\n\n        # 5x5 convolution branch\n        self.conv_5x5 = nn.Sequential(\n            nn.Conv2d(c_in, c_red[\"5x5\"], kernel_size=1),\n            nn.BatchNorm2d(c_red[\"5x5\"]),\n            act_fn(),\n            nn.Conv2d(c_red[\"5x5\"], c_out[\"5x5\"], kernel_size=5, padding=2),\n            nn.BatchNorm2d(c_out[\"5x5\"]),\n            act_fn(),\n        )\n\n        # Max-pool branch\n        self.max_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n            nn.Conv2d(c_in, c_out[\"max\"], kernel_size=1),\n            nn.BatchNorm2d(c_out[\"max\"]),\n            act_fn(),\n        )\n\n    def forward(self, x):\n        x_1x1 = self.conv_1x1(x)\n        x_3x3 = self.conv_3x3(x)\n        x_5x5 = self.conv_5x5(x)\n        x_max = self.max_pool(x)\n        x_out = torch.cat([x_1x1, x_3x3, x_5x5, x_max], dim=1)\n        return x_out\n</code></pre>"},{"location":"reference/spotPython/light/cnn/inceptionblock/#spotPython.light.cnn.inceptionblock.InceptionBlock.__init__","title":"<code>__init__(c_in, c_red, c_out, act_fn)</code>","text":"<p>Inception block as used in GoogLeNet.</p> <p>Description from P. Lippe:INCEPTION, RESNET AND DENSENET An Inception block applies four convolution blocks separately on the same feature map: a 1x1, 3x3, and 5x5 convolution, and a max pool operation. This allows the network to look at the same data with different receptive fields. Of course, learning only 5x5 convolution would be theoretically more powerful. However, this is not only more computation and memory heavy but also tends to overfit much easier. The 1x1 convolutions are used to reduce the number of input channels to the 3x3 and 5x5 convolutions, which reduces the number of parameters and computation.</p> <p>Parameters:</p> Name Type Description Default <code>c_in</code> <p>Number of input feature maps from the previous layers</p> required <code>c_red</code> <code>dict</code> <p>Dictionary with keys \u201c3x3\u201d and \u201c5x5\u201d specifying the output of the dimensionality reducing 1x1 convolutions</p> required <code>c_out</code> <code>dict</code> <p>Dictionary with keys \u201c1x1\u201d, \u201c3x3\u201d, \u201c5x5\u201d, and \u201cmax\u201d</p> required <code>act_fn</code> <p>Activation class constructor (e.g. nn.ReLU)</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output tensor of the inception block</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.light.cnn.googlenet import InceptionBlock\n    import torch\n    import torch.nn as nn\n    block = InceptionBlock(3,\n                {\"3x3\": 32, \"5x5\": 16},\n                {\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8},\n                nn.ReLU)\n    x = torch.randn(1, 3, 32, 32)\n    y = block(x)\n    y.shape\n    torch.Size([1, 64, 32, 32])\n</code></pre> Source code in <code>spotPython/light/cnn/inceptionblock.py</code> <pre><code>def __init__(self, c_in, c_red: dict, c_out: dict, act_fn):\n    \"\"\"\n    Inception block as used in GoogLeNet.\n\n    Description from\n    [P. Lippe:INCEPTION, RESNET AND DENSENET](https://lightning.ai/docs/pytorch/stable/)\n    An Inception block applies four convolution blocks separately on the same feature map:\n    a 1x1, 3x3, and 5x5 convolution, and a max pool operation.\n    This allows the network to look at the same data with different receptive fields.\n    Of course, learning only 5x5 convolution would be theoretically more powerful.\n    However, this is not only more computation and memory heavy but also tends to overfit much easier.\n    The 1x1 convolutions are used to reduce the number of input channels to the 3x3 and 5x5 convolutions,\n    which reduces the number of parameters and computation.\n\n    Args:\n        c_in:\n            Number of input feature maps from the previous layers\n        c_red:\n            Dictionary with keys \"3x3\" and \"5x5\" specifying\n            the output of the dimensionality reducing 1x1 convolutions\n        c_out:\n            Dictionary with keys \"1x1\", \"3x3\", \"5x5\", and \"max\"\n        act_fn:\n            Activation class constructor (e.g. nn.ReLU)\n\n    Returns:\n        torch.Tensor:\n            Output tensor of the inception block\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.light.cnn.googlenet import InceptionBlock\n            import torch\n            import torch.nn as nn\n            block = InceptionBlock(3,\n                        {\"3x3\": 32, \"5x5\": 16},\n                        {\"1x1\": 16, \"3x3\": 32, \"5x5\": 8, \"max\": 8},\n                        nn.ReLU)\n            x = torch.randn(1, 3, 32, 32)\n            y = block(x)\n            y.shape\n            torch.Size([1, 64, 32, 32])\n\n    \"\"\"\n    super().__init__()\n\n    # 1x1 convolution branch\n    self.conv_1x1 = nn.Sequential(\n        nn.Conv2d(c_in, c_out[\"1x1\"], kernel_size=1), nn.BatchNorm2d(c_out[\"1x1\"]), act_fn()\n    )\n\n    # 3x3 convolution branch\n    self.conv_3x3 = nn.Sequential(\n        nn.Conv2d(c_in, c_red[\"3x3\"], kernel_size=1),\n        nn.BatchNorm2d(c_red[\"3x3\"]),\n        act_fn(),\n        nn.Conv2d(c_red[\"3x3\"], c_out[\"3x3\"], kernel_size=3, padding=1),\n        nn.BatchNorm2d(c_out[\"3x3\"]),\n        act_fn(),\n    )\n\n    # 5x5 convolution branch\n    self.conv_5x5 = nn.Sequential(\n        nn.Conv2d(c_in, c_red[\"5x5\"], kernel_size=1),\n        nn.BatchNorm2d(c_red[\"5x5\"]),\n        act_fn(),\n        nn.Conv2d(c_red[\"5x5\"], c_out[\"5x5\"], kernel_size=5, padding=2),\n        nn.BatchNorm2d(c_out[\"5x5\"]),\n        act_fn(),\n    )\n\n    # Max-pool branch\n    self.max_pool = nn.Sequential(\n        nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n        nn.Conv2d(c_in, c_out[\"max\"], kernel_size=1),\n        nn.BatchNorm2d(c_out[\"max\"]),\n        act_fn(),\n    )\n</code></pre>"},{"location":"reference/spotPython/light/cnn/netcnnbase/","title":"netcnnbase","text":""},{"location":"reference/spotPython/light/cnn/netcnnbase/#spotPython.light.cnn.netcnnbase.NetCNNBase","title":"<code>NetCNNBase</code>","text":"<p>             Bases: <code>LightningModule</code></p> Source code in <code>spotPython/light/cnn/netcnnbase.py</code> <pre><code>class NetCNNBase(L.LightningModule):\n    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n        \"\"\"\n        Initializes the CNN model.\n\n        Args:\n            model_name (str): name of the model.\n            model_hparams (dict): dictionary containing the hyperparameters for the model.\n            optimizer_name (str): name of the optimizer.\n            optimizer_hparams (dict): dictionary containing the hyperparameters for the optimizer.\n\n        Returns:\n            (object): model object.\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.light.cnn.netcnnbase import NetCNNBase\n                from spotPython.light.cnn.googlenet import GoogleNet\n                import torch\n                import torch.nn as nn\n                model_hparams = {\"c_in\": 3, \"c_out\": 10, \"act_fn\": nn.ReLU, \"optimizer_name\": \"Adam\"}\n                fun_control = {\"core_model\": GoogleNet}\n                model = NetCNNBase(model_hparams, fun_control)\n                x = torch.randn(1, 3, 32, 32)\n                y = model(x)\n                y.shape\n                torch.Size([1, 10])\n\n        \"\"\"\n        super().__init__()\n        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n        self.save_hyperparameters()\n        print(f\"model_hparams: {model_hparams}\")\n        print(f\"self.hparams: {self.hparams}\")\n        # Create model\n        self.model = self.create_model(model_name, model_hparams)\n        # self.model = fun_control[\"core_model\"](**model_hparams)\n        print(f\"self.model: {self.model}\")\n        # Create loss module\n        self.loss_module = nn.CrossEntropyLoss()\n        # Example input for visualizing the graph in Tensorboard\n        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n\n    def forward(self, imgs):\n        # Forward function that is run when visualizing the graph\n        return self.model(imgs)\n\n    def configure_optimizers(self):\n        if self.hparams.optimizer_name == \"Adam\":\n            optimizer = optim.AdamW(self.parameters(), **self.hparams.optimizer_hparams)\n        elif self.hparams.optimizer_name == \"SGD\":\n            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n        else:\n            assert False, f'Unknown optimizer: \"{self.hparams.optimizer_name}\"'\n\n        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n        return [optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx):\n        # \"batch\" is the output of the training data loader.\n        imgs, labels = batch\n        preds = self.model(imgs)\n        loss = self.loss_module(preds, labels)\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n\n        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n        self.log(\"train_loss\", loss)\n        return loss  # Return tensor to call \".backward\" on\n\n    def validation_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs).argmax(dim=-1)\n        acc = (labels == preds).float().mean()\n        # By default logs it per epoch (weighted average over batches)\n        self.log(\"val_acc\", acc)\n\n    def test_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs).argmax(dim=-1)\n        acc = (labels == preds).float().mean()\n        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n        self.log(\"test_acc\", acc)\n\n    def create_model(self, model_name, model_hparams):\n        print(\"create_model: Starting\")\n        print(f\"model_name: {model_name}\")\n        print(f\"model_hparams: {model_hparams}\")\n        model_dict = {\"GoogleNet\": GoogleNet}\n        if model_name in model_dict:\n            return model_dict[model_name](**model_hparams)\n        else:\n            assert False, f'Unknown model name \"{model_name}\". Available models are: {str(model_dict.keys())}'\n</code></pre>"},{"location":"reference/spotPython/light/cnn/netcnnbase/#spotPython.light.cnn.netcnnbase.NetCNNBase.__init__","title":"<code>__init__(model_name, model_hparams, optimizer_name, optimizer_hparams)</code>","text":"<p>Initializes the CNN model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>name of the model.</p> required <code>model_hparams</code> <code>dict</code> <p>dictionary containing the hyperparameters for the model.</p> required <code>optimizer_name</code> <code>str</code> <p>name of the optimizer.</p> required <code>optimizer_hparams</code> <code>dict</code> <p>dictionary containing the hyperparameters for the optimizer.</p> required <p>Returns:</p> Type Description <code>object</code> <p>model object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.light.cnn.netcnnbase import NetCNNBase\n    from spotPython.light.cnn.googlenet import GoogleNet\n    import torch\n    import torch.nn as nn\n    model_hparams = {\"c_in\": 3, \"c_out\": 10, \"act_fn\": nn.ReLU, \"optimizer_name\": \"Adam\"}\n    fun_control = {\"core_model\": GoogleNet}\n    model = NetCNNBase(model_hparams, fun_control)\n    x = torch.randn(1, 3, 32, 32)\n    y = model(x)\n    y.shape\n    torch.Size([1, 10])\n</code></pre> Source code in <code>spotPython/light/cnn/netcnnbase.py</code> <pre><code>def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n    \"\"\"\n    Initializes the CNN model.\n\n    Args:\n        model_name (str): name of the model.\n        model_hparams (dict): dictionary containing the hyperparameters for the model.\n        optimizer_name (str): name of the optimizer.\n        optimizer_hparams (dict): dictionary containing the hyperparameters for the optimizer.\n\n    Returns:\n        (object): model object.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.light.cnn.netcnnbase import NetCNNBase\n            from spotPython.light.cnn.googlenet import GoogleNet\n            import torch\n            import torch.nn as nn\n            model_hparams = {\"c_in\": 3, \"c_out\": 10, \"act_fn\": nn.ReLU, \"optimizer_name\": \"Adam\"}\n            fun_control = {\"core_model\": GoogleNet}\n            model = NetCNNBase(model_hparams, fun_control)\n            x = torch.randn(1, 3, 32, 32)\n            y = model(x)\n            y.shape\n            torch.Size([1, 10])\n\n    \"\"\"\n    super().__init__()\n    # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n    self.save_hyperparameters()\n    print(f\"model_hparams: {model_hparams}\")\n    print(f\"self.hparams: {self.hparams}\")\n    # Create model\n    self.model = self.create_model(model_name, model_hparams)\n    # self.model = fun_control[\"core_model\"](**model_hparams)\n    print(f\"self.model: {self.model}\")\n    # Create loss module\n    self.loss_module = nn.CrossEntropyLoss()\n    # Example input for visualizing the graph in Tensorboard\n    self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n</code></pre>"},{"location":"reference/spotPython/plot/contour/","title":"contour","text":""},{"location":"reference/spotPython/plot/contour/#spotPython.plot.contour.simple_contour","title":"<code>simple_contour(fun, min_x=-1, max_x=1, min_y=-1, max_y=1, min_z=None, max_z=None, n_samples=100, n_levels=30)</code>","text":"<p>Simple contour plot</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>_type_</code> <p>description</p> required <code>min_x</code> <code>int</code> <p>description. Defaults to -1.</p> <code>-1</code> <code>max_x</code> <code>int</code> <p>description. Defaults to 1.</p> <code>1</code> <code>min_y</code> <code>int</code> <p>description. Defaults to -1.</p> <code>-1</code> <code>max_y</code> <code>int</code> <p>description. Defaults to 1.</p> <code>1</code> <code>min_z</code> <code>int</code> <p>description. Defaults to 0.</p> <code>None</code> <code>max_z</code> <code>int</code> <p>description. Defaults to 1.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>description. Defaults to 100.</p> <code>100</code> <code>n_levels</code> <code>int</code> <p>description. Defaults to 5.</p> <code>30</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n    import numpy as np\n    from spotPython.fun.objectivefunctions import analytical\n    fun = analytical().fun_branin\n    simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15)\n</code></pre> Source code in <code>spotPython/plot/contour.py</code> <pre><code>def simple_contour(\n    fun,\n    min_x=-1,\n    max_x=1,\n    min_y=-1,\n    max_y=1,\n    min_z=None,\n    max_z=None,\n    n_samples=100,\n    n_levels=30,\n):\n    \"\"\"\n    Simple contour plot\n\n    Args:\n        fun (_type_): _description_\n        min_x (int, optional): _description_. Defaults to -1.\n        max_x (int, optional): _description_. Defaults to 1.\n        min_y (int, optional): _description_. Defaults to -1.\n        max_y (int, optional): _description_. Defaults to 1.\n        min_z (int, optional): _description_. Defaults to 0.\n        max_z (int, optional): _description_. Defaults to 1.\n        n_samples (int, optional): _description_. Defaults to 100.\n        n_levels (int, optional): _description_. Defaults to 5.\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n            import numpy as np\n            from spotPython.fun.objectivefunctions import analytical\n            fun = analytical().fun_branin\n            simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15)\n\n    \"\"\"\n    XX, YY = np.meshgrid(np.linspace(min_x, max_x, n_samples), np.linspace(min_y, max_y, n_samples))\n    zz = np.array([fun(np.array([xi, yi]).reshape(-1, 2)) for xi, yi in zip(np.ravel(XX), np.ravel(YY))]).reshape(\n        n_samples, n_samples\n    )\n    fig, ax = plt.subplots(figsize=(5, 2.7), layout=\"constrained\")\n    if min_z is None:\n        min_z = np.min(zz)\n    if max_z is None:\n        max_z = np.max(zz)\n    plt.contourf(\n        XX,\n        YY,\n        zz,\n        levels=np.linspace(min_z, max_z, n_levels),\n        zorder=1,\n        cmap=\"jet\",\n        vmin=min_z,\n        vmax=max_z,\n    )\n    plt.colorbar()\n</code></pre>"},{"location":"reference/spotPython/plot/validation/","title":"validation","text":""},{"location":"reference/spotPython/plot/validation/#spotPython.plot.validation.plot_actual_vs_predicted","title":"<code>plot_actual_vs_predicted(y_test, y_pred, title=None, show=True)</code>","text":"<p>Plot actual vs. predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>ndarray</code> <p>True values.</p> required <code>y_pred</code> <code>ndarray</code> <p>Predicted values.</p> required <code>title</code> <code>str</code> <p>Title of the plot. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotPython/plot/validation.py</code> <pre><code>def plot_actual_vs_predicted(y_test, y_pred, title=None, show=True) -&gt; None:\n    \"\"\"Plot actual vs. predicted values.\n\n    Args:\n        y_test (np.ndarray): True values.\n        y_pred (np.ndarray): Predicted values.\n        title (str, optional): Title of the plot. Defaults to None.\n        show (bool, optional): If True, the plot is shown. Defaults to True.\n\n    Returns:\n        (NoneType): None\n    \"\"\"\n    fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"actual_vs_predicted\",\n        subsample=100,\n        ax=axs[0],\n        random_state=0,\n        scatter_kwargs={\"alpha\": 0.5},\n    )\n    axs[0].set_title(\"Actual vs. Predicted values\")\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"residual_vs_predicted\",\n        subsample=100,\n        ax=axs[1],\n        random_state=0,\n    )\n    axs[1].set_title(\"Residuals vs. Predicted Values\")\n    if title is not None:\n        fig.suptitle(title)\n    plt.tight_layout()\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotPython/plot/validation/#spotPython.plot.validation.plot_confusion_matrix","title":"<code>plot_confusion_matrix(model=None, fun_control=None, df=None, title=None, target_names=None, y_true_name=None, y_pred_name=None, show=False)</code>","text":"<p>Plotting a confusion matrix. If a model and the fun_control dictionary are passed, the confusion matrix is computed. If a dataframe is passed, the confusion matrix is computed from the dataframe. In this case, the names of the columns with the true and the predicted values must be specified. Default the dataframe is None.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Sklearn model. The model to be used for cross-validation. Defaults to None.</p> <code>None</code> <code>fun_control</code> <code>Dict</code> <p>Dictionary containing the data and the target column. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to None.</p> <code>None</code> <code>df</code> <code>DataFrame</code> <p>Dataframe containing the predictions and the target column. Defaults to None.</p> <code>None</code> <code>target_names</code> <code>List[str]</code> <p>List of target names. Defaults to None.</p> <code>None</code> <code>y_true_name</code> <code>str</code> <p>Name of the column with the true values if a dataframe is specified. Defaults to None.</p> <code>None</code> <code>y_pred_name</code> <code>str</code> <p>Name of the column with the predicted values if a dataframe is specified. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Source code in <code>spotPython/plot/validation.py</code> <pre><code>def plot_confusion_matrix(\n    model=None, fun_control=None, df=None, title=None, target_names=None, y_true_name=None, y_pred_name=None, show=False\n):\n    \"\"\"\n    Plotting a confusion matrix. If a model and the fun_control dictionary are passed,\n    the confusion matrix is computed. If a dataframe is passed, the confusion matrix is\n    computed from the dataframe. In this case, the names of the columns with the true and\n    the predicted values must be specified. Default the dataframe is None.\n\n    Args:\n        model (Any, optional):\n            Sklearn model. The model to be used for cross-validation. Defaults to None.\n        fun_control (Dict, optional):\n            Dictionary containing the data and the target column. Defaults to None.\n        title (str, optional):\n            Title of the plot. Defaults to None.\n        df (pd.DataFrame, optional):\n            Dataframe containing the predictions and the target column. Defaults to None.\n        target_names (List[str], optional):\n            List of target names. Defaults to None.\n        y_true_name (str, optional):\n            Name of the column with the true values if a dataframe is specified. Defaults to None.\n        y_pred_name (str, optional):\n            Name of the column with the predicted values if a dataframe is specified. Defaults to None.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to False.\n\n    Returns:\n        (NoneType): None\n\n    \"\"\"\n    if df is not None:\n        # assign the column y_true_name from df to y_true\n        y_true = df[y_true_name]\n        # assign the column y_pred_name from df to y_pred\n        y_pred = df[y_pred_name]\n    else:\n        X_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n        X_test, y_true = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ConfusionMatrixDisplay.from_predictions(y_true=y_true, y_pred=y_pred, ax=ax)\n    if target_names is not None:\n        ax.xaxis.set_ticklabels(target_names)\n        ax.yaxis.set_ticklabels(target_names)\n    if title is not None:\n        _ = ax.set_title(title)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotPython/plot/validation/#spotPython.plot.validation.plot_cv_predictions","title":"<code>plot_cv_predictions(model, fun_control, show=True)</code>","text":"<p>Plots cross-validated predictions for regression.</p> <p>Uses <code>sklearn.model_selection.cross_val_predict</code> together with <code>sklearn.metrics.PredictionErrorDisplay</code> to visualize prediction errors. It is based on the example from the scikit-learn documentation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-download-auto-examples-model-selection-plot-cv-predict-py</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Sklearn model. The model to be used for cross-validation.</p> required <code>fun_control</code> <code>Dict</code> <p>Dictionary containing the data and the target column.</p> required <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; X, y = load_diabetes(return_X_y=True)\n&gt;&gt;&gt; lr = LinearRegression()\n&gt;&gt;&gt; plot_cv_predictions(lr, fun_control)\n</code></pre> Source code in <code>spotPython/plot/validation.py</code> <pre><code>def plot_cv_predictions(model: Any, fun_control: Dict, show=True) -&gt; None:\n    \"\"\"\n    Plots cross-validated predictions for regression.\n\n    Uses `sklearn.model_selection.cross_val_predict` together with\n    `sklearn.metrics.PredictionErrorDisplay` to visualize prediction errors.\n    It is based on the example from the scikit-learn documentation:\n    https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-download-auto-examples-model-selection-plot-cv-predict-py\n\n    Args:\n        model (Any):\n            Sklearn model. The model to be used for cross-validation.\n        fun_control (Dict):\n            Dictionary containing the data and the target column.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to True.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; X, y = load_diabetes(return_X_y=True)\n        &gt;&gt;&gt; lr = LinearRegression()\n        &gt;&gt;&gt; plot_cv_predictions(lr, fun_control)\n    \"\"\"\n    X_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n    # cross_val_predict returns an array of the same size of y\n    # where each entry is a prediction obtained by cross validation.\n    y_pred = cross_val_predict(model, X_test, y_test, cv=10)\n    fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"actual_vs_predicted\",\n        subsample=100,\n        ax=axs[0],\n        random_state=0,\n    )\n    axs[0].set_title(\"Actual vs. Predicted values\")\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"residual_vs_predicted\",\n        subsample=100,\n        ax=axs[1],\n        random_state=0,\n    )\n    axs[1].set_title(\"Residuals vs. Predicted Values\")\n    fig.suptitle(\"Plotting cross-validated predictions\")\n    plt.tight_layout()\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotPython/plot/validation/#spotPython.plot.validation.plot_roc","title":"<code>plot_roc(model_list, fun_control, alpha=0.8, model_names=None, show=True)</code>","text":"<p>Plots ROC curves for a list of models using the Visualization API from scikit-learn.</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>List[BaseEstimator]</code> <p>A list of scikit-learn models to plot ROC curves for.</p> required <code>fun_control</code> <code>Dict[str, Union[str, DataFrame]]</code> <p>A dictionary containing the train and test dataframes and the target column name.</p> required <code>alpha</code> <code>float</code> <p>The alpha value for the ROC curve. Defaults to 0.8.</p> <code>0.8</code> <code>model_names</code> <code>List[str]</code> <p>A list of names for the models. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n&gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X_train = iris.data[:100]\n&gt;&gt;&gt; y_train = iris.target[:100]\n&gt;&gt;&gt; X_test = iris.data[100:]\n&gt;&gt;&gt; y_test = iris.target[100:]\n&gt;&gt;&gt; train_df = pd.DataFrame(X_train, columns=iris.feature_names)\n&gt;&gt;&gt; train_df['target'] = y_train\n&gt;&gt;&gt; test_df = pd.DataFrame(X_test, columns=iris.feature_names)\n&gt;&gt;&gt; test_df['target'] = y_test\n&gt;&gt;&gt; fun_control = {\"train\": train_df, \"test\": test_df, \"target_column\": \"target\"}\n&gt;&gt;&gt; model_list = [LogisticRegression(), DecisionTreeClassifier()]\n&gt;&gt;&gt; model_names = [\"Logistic Regression\", \"Decision Tree\"]\n&gt;&gt;&gt; plot_roc(model_list, fun_control, model_names=model_names)\n</code></pre> Source code in <code>spotPython/plot/validation.py</code> <pre><code>def plot_roc(\n    model_list: List[BaseEstimator],\n    fun_control: Dict[str, Union[str, pd.DataFrame]],\n    alpha: float = 0.8,\n    model_names: List[str] = None,\n    show=True,\n) -&gt; None:\n    \"\"\"\n    Plots ROC curves for a list of models using the Visualization API from scikit-learn.\n\n    Args:\n        model_list (List[BaseEstimator]):\n            A list of scikit-learn models to plot ROC curves for.\n        fun_control (Dict[str, Union[str, pd.DataFrame]]):\n            A dictionary containing the train and test dataframes and the target column name.\n        alpha (float, optional):\n            The alpha value for the ROC curve. Defaults to 0.8.\n        model_names (List[str], optional):\n            A list of names for the models. Defaults to None.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to True.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n        &gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; X_train = iris.data[:100]\n        &gt;&gt;&gt; y_train = iris.target[:100]\n        &gt;&gt;&gt; X_test = iris.data[100:]\n        &gt;&gt;&gt; y_test = iris.target[100:]\n        &gt;&gt;&gt; train_df = pd.DataFrame(X_train, columns=iris.feature_names)\n        &gt;&gt;&gt; train_df['target'] = y_train\n        &gt;&gt;&gt; test_df = pd.DataFrame(X_test, columns=iris.feature_names)\n        &gt;&gt;&gt; test_df['target'] = y_test\n        &gt;&gt;&gt; fun_control = {\"train\": train_df, \"test\": test_df, \"target_column\": \"target\"}\n        &gt;&gt;&gt; model_list = [LogisticRegression(), DecisionTreeClassifier()]\n        &gt;&gt;&gt; model_names = [\"Logistic Regression\", \"Decision Tree\"]\n        &gt;&gt;&gt; plot_roc(model_list, fun_control, model_names=model_names)\n    \"\"\"\n    X_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n    X_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n    ax = plt.gca()\n    for i, model in enumerate(model_list):\n        model.fit(X_train, y_train)\n        if model_names is not None:\n            model_name = model_names[i]\n        else:\n            model_name = None\n        y_pred = model.predict(X_test)\n        RocCurveDisplay.from_predictions(y_test, y_pred, ax=ax, alpha=alpha, name=model_name)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotPython/plot/validation/#spotPython.plot.validation.plot_roc_from_dataframes","title":"<code>plot_roc_from_dataframes(df_list, alpha=0.8, model_names=None, target_column=None, show=True)</code>","text":"<p>Plot ROC curve for a list of dataframes from model evaluations.</p> <p>Parameters:</p> Name Type Description Default <code>df_list</code> <code>List[DataFrame]</code> <p>List of dataframes with results from models.</p> required <code>alpha</code> <code>float</code> <p>Transparency of the plotted lines.</p> <code>0.8</code> <code>model_names</code> <code>List[str]</code> <p>List of model names.</p> <code>None</code> <code>target_column</code> <code>str</code> <p>Name of the target column.</p> <code>None</code> <code>show</code> <p>If True, the plot is shown.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n    from spotPython.plot.validation import plot_roc_from_dataframes\n    df1 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,0,0]})\n    df2 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,1,1]})\n    df_list = [df1, df2]\n    model_names = [\"Model 1\", \"Model 2\"]\n    plot_roc_from_dataframes(df_list, model_names=model_names, target_column=\"y\")\n</code></pre> Source code in <code>spotPython/plot/validation.py</code> <pre><code>def plot_roc_from_dataframes(\n    df_list: List[pd.DataFrame],\n    alpha: float = 0.8,\n    model_names: List[str] = None,\n    target_column: str = None,\n    show=True,\n) -&gt; None:\n    \"\"\"\n    Plot ROC curve for a list of dataframes from model evaluations.\n\n    Args:\n        df_list:\n            List of dataframes with results from models.\n        alpha:\n            Transparency of the plotted lines.\n        model_names:\n            List of model names.\n        target_column:\n            Name of the target column.\n        show:\n            If True, the plot is shown.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n            from spotPython.plot.validation import plot_roc_from_dataframes\n            df1 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,0,0]})\n            df2 = pd.DataFrame({\"y\": [1, 0, 0, 1], \"Prediction\": [1,0,1,1]})\n            df_list = [df1, df2]\n            model_names = [\"Model 1\", \"Model 2\"]\n            plot_roc_from_dataframes(df_list, model_names=model_names, target_column=\"y\")\n\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(10, 5))\n    for i, df in enumerate(df_list):\n        y_test = df[target_column]\n        y_pred = df[\"Prediction\"]\n        if model_names is not None:\n            model_name = model_names[i]\n        else:\n            model_name = None\n        RocCurveDisplay.from_predictions(y_test, y_pred, ax=ax, alpha=alpha, name=model_name)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotPython/sklearn/traintest/","title":"traintest","text":""},{"location":"reference/spotPython/sklearn/traintest/#spotPython.sklearn.traintest.evaluate_model_oob","title":"<code>evaluate_model_oob(model, fun_control)</code>","text":"<p>Out-of-bag evaluation (Only for RandomForestClassifier). If fun_control[\u201ceval\u201d] == \u201ceval_oob_score\u201d.</p> Source code in <code>spotPython/sklearn/traintest.py</code> <pre><code>def evaluate_model_oob(model, fun_control):\n    \"\"\"Out-of-bag evaluation (Only for RandomForestClassifier).\n    If fun_control[\"eval\"] == \"eval_oob_score\".\n    \"\"\"\n    try:\n        X, y = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n        model.fit(X, y)\n        df_preds = model.oob_decision_function_\n        df_eval = fun_control[\"metric_sklearn\"](y, df_preds, **fun_control[\"metric_params\"])\n    except Exception as err:\n        print(f\"Error in fun_sklearn(). Call to evaluate_model_oob failed. {err=}, {type(err)=}\")\n        df_eval = np.nan\n        df_eval = np.nan\n    return df_eval, df_preds\n</code></pre>"},{"location":"reference/spotPython/spot/spot/","title":"spot","text":""},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot","title":"<code>Spot</code>","text":"<p>Spot base class to handle the following tasks in a uniform manner:</p> <ul> <li>Getting and setting parameters. This is done via the <code>Spot</code> initialization.</li> <li>Running surrogate based hyperparameter optimization. After the class is initialized, hyperparameter tuning runs can be performed via the <code>run</code> method.</li> <li>Displaying information. The <code>plot</code> method can be used for visualizing results. The <code>print</code> methods summarizes information about the tuning run.</li> </ul> <p>The <code>Spot</code> class is built in a modular manner. It combines the following three components:</p> <pre><code>1. Design\n2. Surrogate\n3. Optimizer\n</code></pre> <p>For each of the three components different implementations can be selected and combined. Internal components are selected as default. These can be replaced by components from other packages, e.g., scikit-learn or scikit-optimize.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>objective function</p> required <code>lower</code> <code>array</code> <p>lower bound</p> required <code>upper</code> <code>array</code> <p>upper bound</p> required <code>fun_evals</code> <code>int</code> <p>number of function evaluations</p> <code>15</code> <code>fun_repeats</code> <code>int</code> <p>number of repeats (replicates).</p> <code>1</code> <code>max_time</code> <code>int</code> <p>maximum time (in minutes)</p> <code>inf</code> <code>noise</code> <code>bool</code> <p>deterministic or noisy objective function</p> <code>False</code> <code>tolerance_x</code> <code>float</code> <p>tolerance for new x solutions. Minimum distance of new solutions, generated by <code>suggest_new_X</code>, to already existing solutions. If zero (which is the default), every new solution is accepted.</p> <code>0</code> <code>ocba_delta</code> <code>int</code> <p>OCBA increment (only used if <code>noise==True</code>)</p> <code>0</code> <code>var_type</code> <code>List[str]</code> <p>list of type information, can be either \u201cnum\u201d or \u201cfactor\u201d</p> <code>['num']</code> <code>var_name</code> <code>List[str]</code> <p>list of variable names, e.g., [\u201cx1\u201d, \u201cx2\u201d]</p> <code>None</code> <code>infill_criterion</code> <code>str</code> <p>Can be <code>\"y\"</code>, <code>\"s\"</code>, <code>\"ei\"</code> (negative expected improvement), or <code>\"all\"</code>.</p> <code>'y'</code> <code>n_points</code> <code>int</code> <p>number of infill points</p> <code>1</code> <code>seed</code> <code>int</code> <p>initial seed</p> <code>123</code> <code>log_level</code> <code>int</code> <p>log level with the following settings: <code>NOTSET</code> (<code>0</code>), <code>DEBUG</code> (<code>10</code>: Detailed information, typically of interest only when diagnosing problems.), <code>INFO</code> (<code>20</code>: Confirmation that things are working as expected.), <code>WARNING</code> (<code>30</code>: An indication that something unexpected happened, or indicative of some problem in the near     future (e.g. \u2018disk space low\u2019). The software is still working as expected.), <code>ERROR</code> (<code>40</code>: Due to a more serious problem, the software has not been able to perform some function.), and <code>CRITICAL</code> (<code>50</code>: A serious error, indicating that the program itself may be unable to continue running.)</p> <code>50</code> <code>show_models</code> <code>bool</code> <p>Plot model. Currently only 1-dim functions are supported.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar.</p> <code>True</code> <code>design</code> <code>object</code> <p>experimental design.</p> <code>None</code> <code>design_control</code> <code>Dict[str, Union[int, float]]</code> <p>experimental design information stored as a dictionary with the following entries: \u201cinit_size\u201d: <code>10</code>, \u201crepeats\u201d: <code>1</code>.</p> <code>{}</code> <code>surrogate</code> <code>object</code> <p>surrogate model. If <code>None</code>, spotPython\u2019s <code>kriging</code> is used.</p> <code>None</code> <code>surrogate_control</code> <code>Dict[str, Union[int, float]]</code> <p>surrogate model information stored as a dictionary with the following entries: \u201cmodel_optimizer\u201d: <code>differential_evolution</code>, \u201cmodel_fun_evals\u201d: <code>None</code>, \u201cmin_theta\u201d: <code>-3.</code>,  \u201cmax_theta\u201d: <code>3.</code>, \u201cn_theta\u201d: <code>1</code>, \u201cn_p\u201d: <code>1</code>, \u201coptim_p\u201d: <code>False</code>, \u201ccod_type\u201d: <code>\"norm\"</code>, \u201cvar_type\u201d: <code>self.var_type</code>, \u201cuse_cod_y\u201d: <code>False</code>.</p> <code>{}</code> <code>optimizer</code> <code>object</code> <p>optimizer. If <code>None</code>, <code>scipy.optimize</code>\u2019s <code>differential_evolution</code> is used.</p> <code>None</code> <code>optimizer_control</code> <code>Dict[str, Union[int, float]]</code> <p>information about the optimizer stored as a dictionary with the following entries: \u201cmax_iter\u201d: <code>1000</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> Note <p>Description in the source code refers to [bart21i]: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches. In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67\u2013114.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from math import inf\n&gt;&gt;&gt; from spotpy.spot_setup import Spot\n&gt;&gt;&gt; def objective_function(x):\n&gt;&gt;&gt;     return x[0]**2 + x[1]**2\n&gt;&gt;&gt; lower = np.array([0, 0])\n&gt;&gt;&gt; upper = np.array([10, 10])\n&gt;&gt;&gt; spot = Spot(fun=objective_function,\n&gt;&gt;&gt;             lower=lower,\n&gt;&gt;&gt;             upper=upper,\n&gt;&gt;&gt;             fun_evals=100,\n&gt;&gt;&gt;             fun_repeats=1,\n&gt;&gt;&gt;             max_time=inf,\n&gt;&gt;&gt;             noise=False,\n&gt;&gt;&gt;             tolerance_x=0,\n&gt;&gt;&gt;             ocba_delta=0,\n&gt;&gt;&gt;             var_type=[\"num\", \"num\"],\n&gt;&gt;&gt;             var_name=[\"x1\", \"x2\"],\n&gt;&gt;&gt;             infill_criterion=\"ei\",\n&gt;&gt;&gt;             n_points=10,\n&gt;&gt;&gt;             seed=123,\n&gt;&gt;&gt;             log_level=20,\n&gt;&gt;&gt;             show_models=False,\n&gt;&gt;&gt;             show_progress=True,\n&gt;&gt;&gt;             design=None,\n&gt;&gt;&gt;             design_control={\"init_size\": 10, \"repeats\": 1},\n&gt;&gt;&gt;             surrogate=None,\n&gt;&gt;&gt;             surrogate_control={\"model_optimizer\": \"differential_evolution\",\n&gt;&gt;&gt;                                \"model_fun_evals\": None,\n&gt;&gt;&gt;                                \"min_theta\": -3.,\n&gt;&gt;&gt;                                \"max_theta\": 3.,\n&gt;&gt;&gt;                                \"n_theta\": 1,\n&gt;&gt;&gt;                                \"n_p\": 1,\n&gt;&gt;&gt;                                \"optim_p\": False,\n&gt;&gt;&gt;                                \"cod_type\": \"norm\",\n&gt;&gt;&gt;                                \"var_type\": [\"num\", \"num\"],\n&gt;&gt;&gt;                                \"use_cod_y\": False},\n&gt;&gt;&gt;             optimizer_control={\"max_iter\": 1000})\n&gt;&gt;&gt; spot.run()\n</code></pre> Source code in <code>spotPython/spot/spot.py</code> <pre><code>class Spot:\n    \"\"\"\n    Spot base class to handle the following tasks in a uniform manner:\n\n    * Getting and setting parameters. This is done via the `Spot` initialization.\n    * Running surrogate based hyperparameter optimization. After the class is initialized, hyperparameter tuning\n    runs can be performed via the `run` method.\n    * Displaying information. The `plot` method can be used for visualizing results. The `print` methods summarizes\n    information about the tuning run.\n\n    The `Spot` class is built in a modular manner. It combines the following three components:\n\n        1. Design\n        2. Surrogate\n        3. Optimizer\n\n    For each of the three components different implementations can be selected and combined.\n    Internal components are selected as default.\n    These can be replaced by components from other packages, e.g., scikit-learn or scikit-optimize.\n\n    Args:\n        fun (Callable): objective function\n        lower (np.array): lower bound\n        upper (np.array): upper bound\n        fun_evals (int):\n            number of function evaluations\n        fun_repeats (int):\n            number of repeats (replicates).\n        max_time (int):\n            maximum time (in minutes)\n        noise (bool):\n            deterministic or noisy objective function\n        tolerance_x (float):\n            tolerance for new x solutions. Minimum distance of new solutions,\n            generated by `suggest_new_X`, to already existing solutions.\n            If zero (which is the default), every new solution is accepted.\n        ocba_delta (int):\n            OCBA increment (only used if `noise==True`)\n        var_type (List[str]):\n            list of type information, can be either \"num\" or \"factor\"\n        var_name (List[str]):\n            list of variable names, e.g., [\"x1\", \"x2\"]\n        infill_criterion (str):\n            Can be `\"y\"`, `\"s\"`, `\"ei\"` (negative expected improvement), or `\"all\"`.\n        n_points (int):\n            number of infill points\n        seed (int):\n            initial seed\n        log_level (int):\n            log level with the following settings:\n            `NOTSET` (`0`),\n            `DEBUG` (`10`: Detailed information, typically of interest only when diagnosing problems.),\n            `INFO` (`20`: Confirmation that things are working as expected.),\n            `WARNING` (`30`: An indication that something unexpected happened, or indicative of some problem in the near\n                future (e.g. \u2018disk space low\u2019). The software is still working as expected.),\n            `ERROR` (`40`: Due to a more serious problem, the software has not been able to perform some function.), and\n            `CRITICAL` (`50`: A serious error, indicating that the program itself may be unable to continue running.)\n        show_models (bool):\n            Plot model. Currently only 1-dim functions are supported.\n        show_progress (bool):\n            Show progress bar.\n        design (object):\n            experimental design.\n        design_control (Dict[str, Union[int, float]]):\n            experimental design information stored as a dictionary with the following entries:\n            \"init_size\": `10`, \"repeats\": `1`.\n        surrogate (object):\n            surrogate model. If `None`, spotPython's `kriging` is used.\n        surrogate_control (Dict[str, Union[int, float]]):\n            surrogate model information stored as a dictionary with the following entries:\n            \"model_optimizer\": `differential_evolution`,\n            \"model_fun_evals\": `None`,\n            \"min_theta\": `-3.`,  \"max_theta\": `3.`,\n            \"n_theta\": `1`,\n            \"n_p\": `1`,\n            \"optim_p\": `False`,\n            \"cod_type\": `\"norm\"`,\n            \"var_type\": `self.var_type`,\n            \"use_cod_y\": `False`.\n        optimizer (object):\n            optimizer. If `None`, `scipy.optimize`'s `differential_evolution` is used.\n        optimizer_control (Dict[str, Union[int, float]]):\n            information about the optimizer stored as a dictionary with the following entries:\n            \"max_iter\": `1000`.\n\n    Returns:\n        (NoneType): None\n\n    Note:\n        Description in the source code refers to [bart21i]:\n        Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches.\n        In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide,\n        E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67\u2013114.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from math import inf\n        &gt;&gt;&gt; from spotpy.spot_setup import Spot\n        &gt;&gt;&gt; def objective_function(x):\n        &gt;&gt;&gt;     return x[0]**2 + x[1]**2\n        &gt;&gt;&gt; lower = np.array([0, 0])\n        &gt;&gt;&gt; upper = np.array([10, 10])\n        &gt;&gt;&gt; spot = Spot(fun=objective_function,\n        &gt;&gt;&gt;             lower=lower,\n        &gt;&gt;&gt;             upper=upper,\n        &gt;&gt;&gt;             fun_evals=100,\n        &gt;&gt;&gt;             fun_repeats=1,\n        &gt;&gt;&gt;             max_time=inf,\n        &gt;&gt;&gt;             noise=False,\n        &gt;&gt;&gt;             tolerance_x=0,\n        &gt;&gt;&gt;             ocba_delta=0,\n        &gt;&gt;&gt;             var_type=[\"num\", \"num\"],\n        &gt;&gt;&gt;             var_name=[\"x1\", \"x2\"],\n        &gt;&gt;&gt;             infill_criterion=\"ei\",\n        &gt;&gt;&gt;             n_points=10,\n        &gt;&gt;&gt;             seed=123,\n        &gt;&gt;&gt;             log_level=20,\n        &gt;&gt;&gt;             show_models=False,\n        &gt;&gt;&gt;             show_progress=True,\n        &gt;&gt;&gt;             design=None,\n        &gt;&gt;&gt;             design_control={\"init_size\": 10, \"repeats\": 1},\n        &gt;&gt;&gt;             surrogate=None,\n        &gt;&gt;&gt;             surrogate_control={\"model_optimizer\": \"differential_evolution\",\n        &gt;&gt;&gt;                                \"model_fun_evals\": None,\n        &gt;&gt;&gt;                                \"min_theta\": -3.,\n        &gt;&gt;&gt;                                \"max_theta\": 3.,\n        &gt;&gt;&gt;                                \"n_theta\": 1,\n        &gt;&gt;&gt;                                \"n_p\": 1,\n        &gt;&gt;&gt;                                \"optim_p\": False,\n        &gt;&gt;&gt;                                \"cod_type\": \"norm\",\n        &gt;&gt;&gt;                                \"var_type\": [\"num\", \"num\"],\n        &gt;&gt;&gt;                                \"use_cod_y\": False},\n        &gt;&gt;&gt;             optimizer_control={\"max_iter\": 1000})\n        &gt;&gt;&gt; spot.run()\n    \"\"\"\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __init__(\n        self,\n        fun: Callable,\n        lower: np.array,\n        upper: np.array,\n        fun_evals: int = 15,\n        fun_repeats: int = 1,\n        fun_control: Dict[str, Union[int, float]] = {},\n        max_time: int = inf,\n        noise: bool = False,\n        tolerance_x: float = 0,\n        var_type: List[str] = [\"num\"],\n        var_name: List[str] = None,\n        all_var_name: List[str] = None,\n        infill_criterion: str = \"y\",\n        n_points: int = 1,\n        ocba_delta: int = 0,\n        seed: int = 123,\n        log_level: int = 50,\n        show_models: bool = False,\n        show_progress: bool = True,\n        design: object = None,\n        design_control: Dict[str, Union[int, float]] = {},\n        surrogate: object = None,\n        surrogate_control: Dict[str, Union[int, float]] = {},\n        optimizer: object = None,\n        optimizer_control: Dict[str, Union[int, float]] = {},\n    ):\n        # use x0, x1, ... as default variable names:\n        if var_name is None:\n            var_name = [\"x\" + str(i) for i in range(len(lower))]\n        # small value:\n        self.eps = sqrt(spacing(1))\n        self.fun = fun\n        self.lower = lower\n        self.upper = upper\n        self.var_type = var_type\n        self.var_name = var_name\n        self.all_var_name = all_var_name\n        # Reduce dim based on lower == upper logic:\n        # modifies lower, upper, and var_type\n        self.to_red_dim()\n        self.k = self.lower.size\n        self.fun_evals = fun_evals\n        self.fun_repeats = fun_repeats\n        self.max_time = max_time\n        self.noise = noise\n        self.tolerance_x = tolerance_x\n        self.ocba_delta = ocba_delta\n        self.log_level = log_level\n        self.show_models = show_models\n        self.show_progress = show_progress\n        # Random number generator:\n        self.seed = seed\n        self.rng = default_rng(self.seed)\n        # Force numeric type as default in every dim:\n        # assume all variable types are \"num\" if \"num\" is\n        # specified once:\n        if len(self.var_type) &lt; self.k:\n            self.var_type = self.var_type * self.k\n            logger.warning(\"Warning: All variable types forced to 'num'.\")\n        self.infill_criterion = infill_criterion\n        # Bounds\n        de_bounds = []\n        for j in range(self.k):\n            de_bounds.append([self.lower[j], self.upper[j]])\n        self.de_bounds = de_bounds\n        # Infill points:\n        self.n_points = n_points\n        # Objective function related information:\n        self.fun_control = {\"sigma\": 0, \"seed\": None}\n        self.fun_control.update(fun_control)\n        # Design related information:\n        self.design = design\n        if design is None:\n            self.design = spacefilling(k=self.k, seed=self.seed)\n        self.design_control = {\"init_size\": 10, \"repeats\": 1}\n        self.design_control.update(design_control)\n        # Surrogate related information:\n        self.surrogate = surrogate\n        self.surrogate_control = {\n            \"noise\": self.noise,\n            \"model_optimizer\": differential_evolution,\n            \"model_fun_evals\": None,\n            \"min_theta\": -3.0,\n            \"max_theta\": 3.0,\n            \"n_theta\": 1,\n            \"n_p\": 1,\n            \"optim_p\": False,\n            \"cod_type\": \"norm\",\n            \"var_type\": self.var_type,\n            \"seed\": 124,\n            \"use_cod_y\": False,\n        }\n        # Logging information:\n        self.counter = 0\n        self.min_y = None\n        self.min_X = None\n        self.min_mean_X = None\n        self.min_mean_y = None\n        self.mean_X = None\n        self.mean_y = None\n        self.var_y = None\n        logger.setLevel(self.log_level)\n        logger.info(f\"Starting the logger at level {self.log_level} for module {__name__}:\")\n\n        # if the key \"spot_writer\" is not in the dictionary fun_control,\n        # set self.spot_writer to None else to the value of the key \"spot_writer\"\n        self.spot_writer = fun_control.get(\"spot_writer\", None)\n        self.surrogate_control.update(surrogate_control)\n        # If no surrogate model is specified, use the internal\n        # spotPython kriging surrogate:\n        if self.surrogate is None:\n            # Call kriging with surrogate_control parameters:\n            self.surrogate = Kriging(\n                name=\"kriging\",\n                noise=self.surrogate_control[\"noise\"],\n                model_optimizer=self.surrogate_control[\"model_optimizer\"],\n                model_fun_evals=self.surrogate_control[\"model_fun_evals\"],\n                seed=self.surrogate_control[\"seed\"],\n                log_level=self.log_level,\n                min_theta=self.surrogate_control[\"min_theta\"],\n                max_theta=self.surrogate_control[\"max_theta\"],\n                n_theta=self.surrogate_control[\"n_theta\"],\n                n_p=self.surrogate_control[\"n_p\"],\n                optim_p=self.surrogate_control[\"optim_p\"],\n                cod_type=self.surrogate_control[\"cod_type\"],\n                var_type=self.surrogate_control[\"var_type\"],\n                use_cod_y=self.surrogate_control[\"use_cod_y\"],\n                spot_writer=self.spot_writer,\n                counter=self.design_control[\"init_size\"] * self.design_control[\"repeats\"] - 1,\n            )\n        # Optimizer related information:\n        self.optimizer = optimizer\n        self.optimizer_control = {\"max_iter\": 1000, \"seed\": 125}\n        self.optimizer_control.update(optimizer_control)\n        if self.optimizer is None:\n            self.optimizer = optimize.differential_evolution\n\n    def to_red_dim(self):\n        self.all_lower = self.lower\n        self.all_upper = self.upper\n        self.ident = (self.upper - self.lower) == 0\n        self.lower = self.lower[~self.ident]\n        self.upper = self.upper[~self.ident]\n        self.red_dim = self.ident.any()\n        self.all_var_type = self.var_type\n        self.var_type = [x for x, y in zip(self.all_var_type, self.ident) if not y]\n        if self.var_name is not None:\n            self.all_var_name = self.var_name\n            self.var_name = [x for x, y in zip(self.all_var_name, self.ident) if not y]\n\n    def to_all_dim(self, X0):\n        n = X0.shape[0]\n        k = len(self.ident)\n        X = np.zeros((n, k))\n        j = 0\n        for i in range(k):\n            if self.ident[i]:\n                X[:, i] = self.all_lower[i]\n                j += 1\n            else:\n                X[:, i] = X0[:, i - j]\n        return X\n\n    def to_all_dim_if_needed(self, X):\n        if self.red_dim:\n            return self.to_all_dim(X)\n        else:\n            return X\n\n    def get_X_ocba(self):\n        if self.noise and self.ocba_delta &gt; 0:\n            return get_ocba_X(self.mean_X, self.mean_y, self.var_y, self.ocba_delta)\n        else:\n            return None\n\n    def get_new_X0(self):\n        X0 = self.suggest_new_X()\n        X0 = repair_non_numeric(X0, self.var_type)\n        # (S-16) Duplicate Handling:\n        # Condition: select only X= that have min distance\n        # to existing solutions\n        X0, X0_ind = selectNew(A=X0, X=self.X, tolerance=self.tolerance_x)\n        logger.debug(\"XO values are new: %s %s\", X0_ind, X0)\n        # 1. There are X0 that fullfil the condition.\n        # Note: The number of new X0 can be smaller than self.n_points!\n        if X0.shape[0] &gt; 0:\n            return repeat(X0, self.fun_repeats, axis=0)\n        # 2. No X0 found. Then generate self.n_points new solutions:\n        else:\n            self.design = spacefilling(k=self.k, seed=self.seed + self.counter)\n            X0 = self.generate_design(\n                size=self.n_points, repeats=self.design_control[\"repeats\"], lower=self.lower, upper=self.upper\n            )\n            X0 = repair_non_numeric(X0, self.var_type)\n            logger.warning(\"No new XO found on surrogate. Generate new solution %s\", X0)\n            return X0\n\n    def append_X_ocba(self, X_ocba, X0):\n        if self.noise and self.ocba_delta &gt; 0:\n            return append(X_ocba, X0, axis=0)\n        else:\n            return X0\n\n    def run(self, X_start=None):\n        self.initialize_design(X_start)\n        # New: self.update_stats() moved here:\n        # self.update_stats()\n        # (S-5) Calling the spotLoop Function\n        # and\n        # (S-9) Termination Criteria, Conditions:\n        timeout_start = time.time()\n        while self.should_continue(timeout_start):\n            self.update_design()\n            # (S-10): Subset Selection for the Surrogate:\n            # Not implemented yet.\n            # Update stats\n            self.update_stats()\n            # Update writer:\n            self.update_writer()\n            # (S-11) Surrogate Fit:\n            self.fit_surrogate()\n            # progress bar:\n            self.show_progress_if_needed(timeout_start)\n        if self.spot_writer is not None:\n            writer = self.spot_writer\n            writer.close()\n        return self\n\n    def initialize_design(self, X_start=None):\n        # (S-2) Initial Design:\n        X0 = self.generate_design(\n            size=self.design_control[\"init_size\"],\n            repeats=self.design_control[\"repeats\"],\n            lower=self.lower,\n            upper=self.upper,\n        )\n        if X_start is not None:\n            try:\n                X0 = append(X_start, X0, axis=0)\n            except ValueError:\n                logger.warning(\"X_start has wrong shape. Ignoring it.\")\n        X0 = repair_non_numeric(X0, self.var_type)\n        self.X = X0\n        # (S-3): Eval initial design:\n        X_all = self.to_all_dim_if_needed(X0)\n        self.y = self.fun(X=X_all, fun_control=self.fun_control)\n        # TODO: Error if only nan values are returned\n        logger.debug(\"New y value: %s\", self.y)\n        #\n        self.counter = self.y.size\n        if self.spot_writer is not None:\n            writer = self.spot_writer\n            # range goes to init_size -1 because the last value is added by update_stats(),\n            # which always adds the last value.\n            # Changed in 0.5.9:\n            for j in range(len(self.y)):\n                X_j = self.X[j].copy()\n                y_j = self.y[j].copy()\n                config = {self.var_name[i]: X_j[i] for i in range(self.k)}\n                writer.add_hparams(config, {\"spot_y\": y_j})\n                writer.flush()\n        #\n        self.X, self.y = remove_nan(self.X, self.y)\n        # self.update_stats() moved to run()!\n        # changed in 0.5.9:\n        self.update_stats()\n        # (S-4): Imputation:\n        # Not implemented yet.\n        # (S-11) Surrogate Fit:\n        self.fit_surrogate()\n\n    def should_continue(self, timeout_start):\n        return (self.counter &lt; self.fun_evals) and (time.time() &lt; timeout_start + self.max_time * 60)\n\n    def update_design(self):\n        # OCBA (only if noise)\n        X_ocba = self.get_X_ocba()\n        # (S-15) Compile Surrogate Results:\n        X0 = self.get_new_X0()\n        # (S-18): Evaluating New Solutions:\n        X0 = self.append_X_ocba(X_ocba, X0)\n        X_all = self.to_all_dim_if_needed(X0)\n        y0 = self.fun(X=X_all, fun_control=self.fun_control)\n        X0, y0 = remove_nan(X0, y0)\n        # Append New Solutions:\n        self.X = np.append(self.X, X0, axis=0)\n        self.y = np.append(self.y, y0)\n\n    def fit_surrogate(self):\n        self.surrogate.fit(self.X, self.y)\n        if self.show_models:\n            self.plot_model()\n\n    def show_progress_if_needed(self, timeout_start):\n        if not self.show_progress:\n            return\n        if isfinite(self.fun_evals):\n            progress_bar(progress=self.counter / self.fun_evals, y=self.min_y)\n        else:\n            progress_bar(progress=(time.time() - timeout_start) / (self.max_time * 60), y=self.min_y)\n\n    def generate_design(self, size, repeats, lower, upper):\n        return self.design.scipy_lhd(n=size, repeats=repeats, lower=lower, upper=upper)\n\n    def update_stats(self):\n        \"\"\"\n        Update the following stats: 1. `min_y` 2. `min_X` 3. `counter`\n        If `noise` is `True`, additionally the following stats are computed: 1. `mean_X`\n        2. `mean_y` 3. `min_mean_y` 4. `min_mean_X`.\n\n        \"\"\"\n        self.min_y = min(self.y)\n        self.min_X = self.X[argmin(self.y)]\n        self.counter = self.y.size\n        # Update aggregated x and y values (if noise):\n        if self.noise:\n            Z = aggregate_mean_var(X=self.X, y=self.y)\n            self.mean_X = Z[0]\n            self.mean_y = Z[1]\n            self.var_y = Z[2]\n            # X value of the best mean y value so far:\n            self.min_mean_X = self.mean_X[argmin(self.mean_y)]\n            # variance of the best mean y value so far:\n            self.min_var_y = self.var_y[argmin(self.mean_y)]\n            # best mean y value so far:\n            self.min_mean_y = self.mean_y[argmin(self.mean_y)]\n\n    def update_writer(self):\n        if self.spot_writer is not None:\n            writer = self.spot_writer\n            # get the last y value:\n            y_last = self.y[-1].copy()\n            if self.noise is False:\n                y_min = self.min_y.copy()\n                X_min = self.min_X.copy()\n                # y_min: best y value so far\n                # y_last: last y value, can be worse than y_min\n                writer.add_scalars(\"spot_y\", {\"min\": y_min, \"last\": y_last}, self.counter)\n                # X_min: X value of the best y value so far\n                writer.add_scalars(\"spot_X\", {f\"X_{i}\": X_min[i] for i in range(self.k)}, self.counter)\n            else:\n                # get the last n y values:\n                y_last_n = self.y[-self.fun_repeats :].copy()\n                # y_min_mean: best mean y value so far\n                y_min_mean = self.min_mean_y.copy()\n                # X_min_mean: X value of the best mean y value so far\n                X_min_mean = self.min_mean_X.copy()\n                # y_min_var: variance of the min y value so far\n                y_min_var = self.min_var_y.copy()\n                writer.add_scalar(\"spot_y_min_var\", y_min_var, self.counter)\n                # y_min_mean: best mean y value so far (see above)\n                writer.add_scalar(\"spot_y\", y_min_mean, self.counter)\n                # last n y values (noisy):\n                writer.add_scalars(\n                    \"spot_y\", {f\"y_last_n{i}\": y_last_n[i] for i in range(self.fun_repeats)}, self.counter\n                )\n                # X_min_mean: X value of the best mean y value so far (see above)\n                writer.add_scalars(\n                    \"spot_X_noise\", {f\"X_min_mean{i}\": X_min_mean[i] for i in range(self.k)}, self.counter\n                )\n            # get last value of self.X and convert to dict. take the values from self.var_name as keys:\n            X_last = self.X[-1].copy()\n            config = {self.var_name[i]: X_last[i] for i in range(self.k)}\n            # hyperparameters X and value y of the last configuration:\n            writer.add_hparams(config, {\"spot_y\": y_last})\n            writer.flush()\n\n    def suggest_new_X_old(self):\n        \"\"\"\n        Compute `n_points` new infill points in natural units.\n        The optimizer searches in the ranges from `lower_j` to `upper_j`.\n        The method `infill()` is used as the objective function.\n\n        Returns:\n            (numpy.ndarray): `n_points` infill points in natural units, each of dim k\n\n        Note:\n            This is step (S-14a) in [bart21i].\n        \"\"\"\n        # (S-14a) Optimization on the surrogate:\n        new_X = np.zeros([self.n_points, self.k], dtype=float)\n\n        optimizer_name = self.optimizer.__name__\n        for i in range(self.n_points):\n            if optimizer_name == \"dual_annealing\":\n                result = self.optimizer(func=self.infill, bounds=self.de_bounds)\n            elif optimizer_name == \"differential_evolution\":\n                result = self.optimizer(\n                    func=self.infill,\n                    bounds=self.de_bounds,\n                    maxiter=self.optimizer_control[\"max_iter\"],\n                    seed=self.optimizer_control[\"seed\"],\n                    # popsize=10,\n                    # updating=\"deferred\"\n                )\n            elif optimizer_name == \"direct\":\n                result = self.optimizer(func=self.infill, bounds=self.de_bounds, eps=1e-2)\n            elif optimizer_name == \"shgo\":\n                result = self.optimizer(func=self.infill, bounds=self.de_bounds)\n            elif optimizer_name == \"basinhopping\":\n                result = self.optimizer(func=self.infill, x0=self.min_X)\n            else:\n                result = self.optimizer(func=self.infill, bounds=self.de_bounds)\n            new_X[i][:] = result.x\n        return new_X\n\n    def suggest_new_X(self):\n        \"\"\"\n        Compute `n_points` new infill points in natural units.\n        The optimizer searches in the ranges from `lower_j` to `upper_j`.\n        The method `infill()` is used as the objective function.\n\n        Returns:\n            (numpy.ndarray): `n_points` infill points in natural units, each of dim k\n\n        Note:\n            This is step (S-14a) in [bart21i].\n        \"\"\"\n        # (S-14a) Optimization on the surrogate:\n        new_X = np.zeros([self.n_points, self.k], dtype=float)\n\n        optimizer_name = self.optimizer.__name__\n\n        optimizers = {\n            \"dual_annealing\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n            \"differential_evolution\": lambda: self.optimizer(\n                func=self.infill,\n                bounds=self.de_bounds,\n                maxiter=self.optimizer_control[\"max_iter\"],\n                seed=self.optimizer_control[\"seed\"],\n            ),\n            \"direct\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds, eps=1e-2),\n            \"shgo\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n            \"basinhopping\": lambda: self.optimizer(func=self.infill, x0=self.min_X),\n            \"default\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n        }\n\n        for i in range(self.n_points):\n            result = optimizers.get(optimizer_name, optimizers[\"default\"])()\n            new_X[i][:] = result.x\n        return new_X\n\n    def infill(self, x):\n        \"\"\"\n        Infill (acquisition) function. Evaluates one point on the surrogate via `surrogate.predict(x.reshape(1,-1))`,\n        if `sklearn` surrogates are used or `surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)`\n        if the internal surrogate `kriging` is selected.\n        This method is passed to the optimizer in `suggest_new_X`, i.e., the optimizer is called via\n        `self.optimizer(func=self.infill)`.\n\n        Args:\n            x (array): point in natural units with shape `(1, dim)`.\n\n        Returns:\n            (numpy.ndarray): value based on infill criterion, e.g., `\"ei\"`. Shape `(1,)`.\n                The objective function value `y` that is used as a base value for the\n                infill criterion is calculated in natural units.\n\n        Note:\n            This is step (S-12) in [bart21i].\n        \"\"\"\n        # Reshape x to have shape (1, -1) because the predict method expects a 2D array\n        x_reshaped = x.reshape(1, -1)\n        if isinstance(self.surrogate, Kriging):\n            return self.surrogate.predict(x_reshaped, return_val=self.infill_criterion)\n        else:\n            return self.surrogate.predict(x_reshaped)\n\n    def plot_progress(\n        self, show=True, log_x=False, log_y=False, filename=\"plot.png\", style=[\"ko\", \"k\", \"ro-\"], dpi=300\n    ) -&gt; None:\n        \"\"\"Plot the progress of the hyperparameter tuning (optimization).\n        Args:\n            show (bool):\n                Show the plot.\n            log_x (bool):\n                Use logarithmic scale for x-axis.\n            log_y (bool):\n                Use logarithmic scale for y-axis.\n            filename (str):\n                Filename to save the plot.\n            style (list):\n                Style of the plot. Default: ['k', 'ro-'], i.e., the initial points are plotted as a black line\n                and the subsequent points as red dots connected by a line.\n        Returns:\n            None\n        \"\"\"\n        fig = pylab.figure(figsize=(9, 6))\n        s_y = pd.Series(self.y)\n        s_c = s_y.cummin()\n        n_init = self.design_control[\"init_size\"] * self.design_control[\"repeats\"]\n        ax = fig.add_subplot(211)\n        ax.plot(\n            range(1, n_init + 1),\n            s_y[:n_init],\n            style[0],\n            range(1, n_init + 2),\n            [s_c[:n_init].min()] * (n_init + 1),\n            style[1],\n            range(n_init + 1, len(s_c) + 1),\n            s_c[n_init:],\n            style[2],\n        )\n        if log_x:\n            ax.set_xscale(\"log\")\n        if log_y:\n            ax.set_yscale(\"log\")\n        if filename is not None:\n            pylab.savefig(filename, dpi=dpi, bbox_inches=\"tight\")\n        if show:\n            pylab.show()\n\n    def plot_model(self, y_min=None, y_max=None):\n        \"\"\"\n        Plot the model fit for 1-dim objective functions.\n\n        Args:\n            y_min (float, optional): y range, lower bound.\n            y_max (float, optional): y range, upper bound.\n        \"\"\"\n        if self.k == 1:\n            X_test = np.linspace(self.lower[0], self.upper[0], 100)\n            y_test = self.fun(X=X_test.reshape(-1, 1), fun_control=self.fun_control)\n            if isinstance(self.surrogate, Kriging):\n                y_hat = self.surrogate.predict(X_test[:, np.newaxis], return_val=\"y\")\n            else:\n                y_hat = self.surrogate.predict(X_test[:, np.newaxis])\n            plt.plot(X_test, y_hat, label=\"Model\")\n            plt.plot(X_test, y_test, label=\"True function\")\n            plt.scatter(self.X, self.y, edgecolor=\"b\", s=20, label=\"Samples\")\n            plt.scatter(self.X[-1], self.y[-1], edgecolor=\"r\", s=30, label=\"Last Sample\")\n            if self.noise:\n                plt.scatter(self.min_mean_X, self.min_mean_y, edgecolor=\"g\", s=30, label=\"Best Sample (mean)\")\n            else:\n                plt.scatter(self.min_X, self.min_y, edgecolor=\"g\", s=30, label=\"Best Sample\")\n            plt.xlabel(\"x\")\n            plt.ylabel(\"y\")\n            plt.xlim((self.lower[0], self.upper[0]))\n            if y_min is None:\n                y_min = min(min(self.y), min(y_test))\n            if y_max is None:\n                y_max = max(max(self.y), max(y_test))\n            plt.ylim((y_min, y_max))\n            plt.legend(loc=\"best\")\n            # plt.title(self.surrogate.__class__.__name__ + \". \" + str(self.counter) + \": \" + str(self.min_y))\n            if self.noise:\n                plt.title(\n                    str(self.counter)\n                    + \". y (noise): \"\n                    + str(np.round(self.min_y, 6))\n                    + \" y mean: \"\n                    + str(np.round(self.min_mean_y, 6))\n                )\n            else:\n                plt.title(str(self.counter) + \". y: \" + str(np.round(self.min_y, 6)))\n            plt.show()\n\n    def print_results(self, print_screen=True) -&gt; list[str]:\n        \"\"\"Print results from the run:\n            1. min y\n            2. min X\n            If `noise == True`, additionally the following values are printed:\n            3. min mean y\n            4. min mean X\n        Args:\n            print_screen (bool, optional): print results to screen\n        Returns:\n            output (list): list of results\n        \"\"\"\n        output = []\n        if print_screen:\n            print(f\"min y: {self.min_y}\")\n        res = self.to_all_dim(self.min_X.reshape(1, -1))\n        for i in range(res.shape[1]):\n            var_name = \"x\" + str(i) if self.all_var_name is None else self.all_var_name[i]\n            if print_screen:\n                print(var_name + \":\", res[0][i])\n            output.append([var_name, res[0][i]])\n        if self.noise:\n            res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n            if print_screen:\n                print(f\"min mean y: {self.min_mean_y}\")\n            for i in range(res.shape[1]):\n                var_name = \"x\" + str(i) if self.all_var_name is None else self.all_var_name[i]\n                if print_screen:\n                    print(var_name + \":\", res[0][i])\n                output.append([var_name, res[0][i]])\n        return output\n\n    def chg(self, x, y, z0, i, j):\n        \"\"\"\n        Change the values of elements at indices `i` and `j` in the array `z0` to `x` and `y`, respectively.\n\n        Args:\n            x (int or float): The new value for the element at index `i`.\n            y (int or float): The new value for the element at index `j`.\n            z0 (list or numpy.ndarray): The array to be modified.\n            i (int): The index of the element to be changed to `x`.\n            j (int): The index of the element to be changed to `y`.\n\n        Returns:\n            (list) or (numpy.ndarray): The modified array.\n\n        Examples:\n            &gt;&gt;&gt; z0 = [1, 2, 3]\n            &gt;&gt;&gt; chg(4, 5, z0, 0, 2)\n            [4, 2, 5]\n        \"\"\"\n        z0[i] = x\n        z0[j] = y\n        return z0\n\n    def plot_contour(\n        self, i=0, j=1, min_z=None, max_z=None, show=True, filename=None, n_grid=25, contour_levels=10, dpi=200\n    ) -&gt; None:\n        \"\"\"Plot the contour of any dimension.\n        Args:\n            i (int): the first dimension\n            j (int): the second dimension\n            min_z (float): the minimum value of z\n            max_z (float): the maximum value of z\n            show (bool): show the plot\n            filename (str): save the plot to a file\n            n_grid (int): number of grid points\n            contour_levels (int): number of contour levels\n        Returns:\n            None\n        \"\"\"\n        fig = pylab.figure(figsize=(9, 6))\n        # lower and upper\n        x = np.linspace(self.lower[i], self.upper[i], num=n_grid)\n        y = np.linspace(self.lower[j], self.upper[j], num=n_grid)\n        X, Y = meshgrid(x, y)\n        # Predict based on the optimized results\n        z0 = np.mean(np.array([self.lower, self.upper]), axis=0)\n        zz = array([self.surrogate.predict(array([self.chg(x, y, z0, i, j)])) for x, y in zip(ravel(X), ravel(Y))])\n        zs = zz[:, 0]\n        Z = zs.reshape(X.shape)\n        if min_z is None:\n            min_z = np.min(Z)\n        if max_z is None:\n            max_z = np.max(Z)\n        ax = fig.add_subplot(221)\n        # plot predicted values:\n        plt.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\", vmin=min_z, vmax=max_z)\n        if self.var_name is None:\n            plt.xlabel(\"x\" + str(i))\n            plt.ylabel(\"x\" + str(j))\n        else:\n            plt.xlabel(\"x\" + str(i) + \": \" + self.var_name[i])\n            plt.ylabel(\"x\" + str(j) + \": \" + self.var_name[j])\n        plt.title(\"Surrogate\")\n        pylab.colorbar()\n        ax = fig.add_subplot(222, projection=\"3d\")\n        ax.plot_surface(X, Y, Z, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\", vmin=min_z, vmax=max_z)\n        if self.var_name is None:\n            plt.xlabel(\"x\" + str(i))\n            plt.ylabel(\"x\" + str(j))\n        else:\n            plt.xlabel(\"x\" + str(i) + \": \" + self.var_name[i])\n            plt.ylabel(\"x\" + str(j) + \": \" + self.var_name[j])\n        if filename:\n            pylab.savefig(filename, bbox_inches=\"tight\", dpi=dpi, pad_inches=0),\n        if show:\n            pylab.show()\n\n    def plot_important_hyperparameter_contour(self, threshold=0.025, filename=None, show=True) -&gt; None:\n        impo = self.print_importance(threshold=threshold, print_screen=True)\n        var_plots = [i for i, x in enumerate(impo) if x[1] &gt; threshold]\n        min_z = min(self.y)\n        max_z = max(self.y)\n        for i in var_plots:\n            for j in var_plots:\n                if j &gt; i:\n                    if filename is not None:\n                        filename_full = filename + \"_contour_\" + str(i) + \"_\" + str(j) + \".png\"\n                    else:\n                        filename_full = None\n                    self.plot_contour(i=i, j=j, min_z=min_z, max_z=max_z, filename=filename_full, show=show)\n\n    def get_importance(self) -&gt; list:\n        \"\"\"Get importance of each variable and return the results as a list.\n        Returns:\n            output (list): list of results\n        \"\"\"\n        if self.surrogate.n_theta &gt; 1 and self.var_name is not None:\n            output = [0] * len(self.all_var_name)\n            theta = np.power(10, self.surrogate.theta)\n            imp = 100 * theta / np.max(theta)\n            ind = find_indices(A=self.var_name, B=self.all_var_name)\n            j = 0\n            for i in ind:\n                output[i] = imp[j]\n                j = j + 1\n            return output\n        else:\n            print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n\n    def print_importance(self, threshold=0.1, print_screen=True) -&gt; list:\n        \"\"\"Print importance of each variable and return the results as a list.\n        Args:\n            threshold (float): threshold for printing\n            print_screen (boolean): if `True`, values are also printed on the screen. Default is `True`.\n        Returns:\n            output (list): list of results\n        \"\"\"\n        output = []\n        if self.surrogate.n_theta &gt; 1:\n            theta = np.power(10, self.surrogate.theta)\n            imp = 100 * theta / np.max(theta)\n            # imp = imp[imp &gt;= threshold]\n            if self.var_name is None:\n                for i in range(len(imp)):\n                    if imp[i] &gt;= threshold:\n                        if print_screen:\n                            print(\"x\", i, \": \", imp[i])\n                        output.append(\"x\" + str(i) + \": \" + str(imp[i]))\n            else:\n                var_name = [self.var_name[i] for i in range(len(imp))]\n                for i in range(len(imp)):\n                    if imp[i] &gt;= threshold:\n                        if print_screen:\n                            print(var_name[i] + \": \", imp[i])\n                    output.append([var_name[i], imp[i]])\n        else:\n            print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n        return output\n\n    def plot_importance(self, threshold=0.1, filename=None, dpi=300, show=True) -&gt; None:\n        \"\"\"Plot the importance of each variable.\n        Args:\n            threshold (float):  The threshold of the importance.\n            filename (str): The filename of the plot.\n            dpi (int): The dpi of the plot.\n            show (bool): Show the plot. Default is `True`.\n        Returns:\n            None\n        \"\"\"\n        if self.surrogate.n_theta &gt; 1:\n            theta = np.power(10, self.surrogate.theta)\n            imp = 100 * theta / np.max(theta)\n            idx = np.where(imp &gt; threshold)[0]\n            if self.var_name is None:\n                plt.bar(range(len(imp[idx])), imp[idx])\n                plt.xticks(range(len(imp[idx])), [\"x\" + str(i) for i in idx])\n            else:\n                var_name = [self.var_name[i] for i in idx]\n                plt.bar(range(len(imp[idx])), imp[idx])\n                plt.xticks(range(len(imp[idx])), var_name)\n            if filename is not None:\n                plt.savefig(filename, bbox_inches=\"tight\", dpi=dpi)\n            if show:\n                plt.show()\n\n    def parallel_plot(self, show=True) -&gt; go.Figure:\n        \"\"\"\n        Parallel plot.\n\n        Args:\n            show (bool): show the plot\n\n        Returns:\n                fig (plotly.graph_objects.Figure): figure object\n\n        \"\"\"\n        X = self.X\n        y = self.y\n        df = pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=self.var_name + [\"y\"])\n\n        fig = go.Figure(\n            data=go.Parcoords(\n                line=dict(color=df[\"y\"], colorscale=\"Jet\", showscale=True, cmin=min(df[\"y\"]), cmax=max(df[\"y\"])),\n                dimensions=list(\n                    [\n                        dict(range=[min(df.iloc[:, i]), max(df.iloc[:, i])], label=df.columns[i], values=df.iloc[:, i])\n                        for i in range(len(df.columns) - 1)\n                    ]\n                ),\n            )\n        )\n        if show:\n            fig.show()\n        return fig\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.chg","title":"<code>chg(x, y, z0, i, j)</code>","text":"<p>Change the values of elements at indices <code>i</code> and <code>j</code> in the array <code>z0</code> to <code>x</code> and <code>y</code>, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int or float</code> <p>The new value for the element at index <code>i</code>.</p> required <code>y</code> <code>int or float</code> <p>The new value for the element at index <code>j</code>.</p> required <code>z0</code> <code>list or ndarray</code> <p>The array to be modified.</p> required <code>i</code> <code>int</code> <p>The index of the element to be changed to <code>x</code>.</p> required <code>j</code> <code>int</code> <p>The index of the element to be changed to <code>y</code>.</p> required <p>Returns:</p> Type Description <code>list) or (numpy.ndarray</code> <p>The modified array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; z0 = [1, 2, 3]\n&gt;&gt;&gt; chg(4, 5, z0, 0, 2)\n[4, 2, 5]\n</code></pre> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def chg(self, x, y, z0, i, j):\n    \"\"\"\n    Change the values of elements at indices `i` and `j` in the array `z0` to `x` and `y`, respectively.\n\n    Args:\n        x (int or float): The new value for the element at index `i`.\n        y (int or float): The new value for the element at index `j`.\n        z0 (list or numpy.ndarray): The array to be modified.\n        i (int): The index of the element to be changed to `x`.\n        j (int): The index of the element to be changed to `y`.\n\n    Returns:\n        (list) or (numpy.ndarray): The modified array.\n\n    Examples:\n        &gt;&gt;&gt; z0 = [1, 2, 3]\n        &gt;&gt;&gt; chg(4, 5, z0, 0, 2)\n        [4, 2, 5]\n    \"\"\"\n    z0[i] = x\n    z0[j] = y\n    return z0\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.get_importance","title":"<code>get_importance()</code>","text":"<p>Get importance of each variable and return the results as a list. Returns:     output (list): list of results</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def get_importance(self) -&gt; list:\n    \"\"\"Get importance of each variable and return the results as a list.\n    Returns:\n        output (list): list of results\n    \"\"\"\n    if self.surrogate.n_theta &gt; 1 and self.var_name is not None:\n        output = [0] * len(self.all_var_name)\n        theta = np.power(10, self.surrogate.theta)\n        imp = 100 * theta / np.max(theta)\n        ind = find_indices(A=self.var_name, B=self.all_var_name)\n        j = 0\n        for i in ind:\n            output[i] = imp[j]\n            j = j + 1\n        return output\n    else:\n        print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.infill","title":"<code>infill(x)</code>","text":"<p>Infill (acquisition) function. Evaluates one point on the surrogate via <code>surrogate.predict(x.reshape(1,-1))</code>, if <code>sklearn</code> surrogates are used or <code>surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)</code> if the internal surrogate <code>kriging</code> is selected. This method is passed to the optimizer in <code>suggest_new_X</code>, i.e., the optimizer is called via <code>self.optimizer(func=self.infill)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>point in natural units with shape <code>(1, dim)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>value based on infill criterion, e.g., <code>\"ei\"</code>. Shape <code>(1,)</code>. The objective function value <code>y</code> that is used as a base value for the infill criterion is calculated in natural units.</p> Note <p>This is step (S-12) in [bart21i].</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def infill(self, x):\n    \"\"\"\n    Infill (acquisition) function. Evaluates one point on the surrogate via `surrogate.predict(x.reshape(1,-1))`,\n    if `sklearn` surrogates are used or `surrogate.predict(x.reshape(1,-1), return_val=self.infill_criterion)`\n    if the internal surrogate `kriging` is selected.\n    This method is passed to the optimizer in `suggest_new_X`, i.e., the optimizer is called via\n    `self.optimizer(func=self.infill)`.\n\n    Args:\n        x (array): point in natural units with shape `(1, dim)`.\n\n    Returns:\n        (numpy.ndarray): value based on infill criterion, e.g., `\"ei\"`. Shape `(1,)`.\n            The objective function value `y` that is used as a base value for the\n            infill criterion is calculated in natural units.\n\n    Note:\n        This is step (S-12) in [bart21i].\n    \"\"\"\n    # Reshape x to have shape (1, -1) because the predict method expects a 2D array\n    x_reshaped = x.reshape(1, -1)\n    if isinstance(self.surrogate, Kriging):\n        return self.surrogate.predict(x_reshaped, return_val=self.infill_criterion)\n    else:\n        return self.surrogate.predict(x_reshaped)\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.parallel_plot","title":"<code>parallel_plot(show=True)</code>","text":"<p>Parallel plot.</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>show the plot</p> <code>True</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>figure object</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def parallel_plot(self, show=True) -&gt; go.Figure:\n    \"\"\"\n    Parallel plot.\n\n    Args:\n        show (bool): show the plot\n\n    Returns:\n            fig (plotly.graph_objects.Figure): figure object\n\n    \"\"\"\n    X = self.X\n    y = self.y\n    df = pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=self.var_name + [\"y\"])\n\n    fig = go.Figure(\n        data=go.Parcoords(\n            line=dict(color=df[\"y\"], colorscale=\"Jet\", showscale=True, cmin=min(df[\"y\"]), cmax=max(df[\"y\"])),\n            dimensions=list(\n                [\n                    dict(range=[min(df.iloc[:, i]), max(df.iloc[:, i])], label=df.columns[i], values=df.iloc[:, i])\n                    for i in range(len(df.columns) - 1)\n                ]\n            ),\n        )\n    )\n    if show:\n        fig.show()\n    return fig\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.plot_contour","title":"<code>plot_contour(i=0, j=1, min_z=None, max_z=None, show=True, filename=None, n_grid=25, contour_levels=10, dpi=200)</code>","text":"<p>Plot the contour of any dimension. Args:     i (int): the first dimension     j (int): the second dimension     min_z (float): the minimum value of z     max_z (float): the maximum value of z     show (bool): show the plot     filename (str): save the plot to a file     n_grid (int): number of grid points     contour_levels (int): number of contour levels Returns:     None</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def plot_contour(\n    self, i=0, j=1, min_z=None, max_z=None, show=True, filename=None, n_grid=25, contour_levels=10, dpi=200\n) -&gt; None:\n    \"\"\"Plot the contour of any dimension.\n    Args:\n        i (int): the first dimension\n        j (int): the second dimension\n        min_z (float): the minimum value of z\n        max_z (float): the maximum value of z\n        show (bool): show the plot\n        filename (str): save the plot to a file\n        n_grid (int): number of grid points\n        contour_levels (int): number of contour levels\n    Returns:\n        None\n    \"\"\"\n    fig = pylab.figure(figsize=(9, 6))\n    # lower and upper\n    x = np.linspace(self.lower[i], self.upper[i], num=n_grid)\n    y = np.linspace(self.lower[j], self.upper[j], num=n_grid)\n    X, Y = meshgrid(x, y)\n    # Predict based on the optimized results\n    z0 = np.mean(np.array([self.lower, self.upper]), axis=0)\n    zz = array([self.surrogate.predict(array([self.chg(x, y, z0, i, j)])) for x, y in zip(ravel(X), ravel(Y))])\n    zs = zz[:, 0]\n    Z = zs.reshape(X.shape)\n    if min_z is None:\n        min_z = np.min(Z)\n    if max_z is None:\n        max_z = np.max(Z)\n    ax = fig.add_subplot(221)\n    # plot predicted values:\n    plt.contourf(X, Y, Z, contour_levels, zorder=1, cmap=\"jet\", vmin=min_z, vmax=max_z)\n    if self.var_name is None:\n        plt.xlabel(\"x\" + str(i))\n        plt.ylabel(\"x\" + str(j))\n    else:\n        plt.xlabel(\"x\" + str(i) + \": \" + self.var_name[i])\n        plt.ylabel(\"x\" + str(j) + \": \" + self.var_name[j])\n    plt.title(\"Surrogate\")\n    pylab.colorbar()\n    ax = fig.add_subplot(222, projection=\"3d\")\n    ax.plot_surface(X, Y, Z, rstride=3, cstride=3, alpha=0.9, cmap=\"jet\", vmin=min_z, vmax=max_z)\n    if self.var_name is None:\n        plt.xlabel(\"x\" + str(i))\n        plt.ylabel(\"x\" + str(j))\n    else:\n        plt.xlabel(\"x\" + str(i) + \": \" + self.var_name[i])\n        plt.ylabel(\"x\" + str(j) + \": \" + self.var_name[j])\n    if filename:\n        pylab.savefig(filename, bbox_inches=\"tight\", dpi=dpi, pad_inches=0),\n    if show:\n        pylab.show()\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.plot_importance","title":"<code>plot_importance(threshold=0.1, filename=None, dpi=300, show=True)</code>","text":"<p>Plot the importance of each variable. Args:     threshold (float):  The threshold of the importance.     filename (str): The filename of the plot.     dpi (int): The dpi of the plot.     show (bool): Show the plot. Default is <code>True</code>. Returns:     None</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def plot_importance(self, threshold=0.1, filename=None, dpi=300, show=True) -&gt; None:\n    \"\"\"Plot the importance of each variable.\n    Args:\n        threshold (float):  The threshold of the importance.\n        filename (str): The filename of the plot.\n        dpi (int): The dpi of the plot.\n        show (bool): Show the plot. Default is `True`.\n    Returns:\n        None\n    \"\"\"\n    if self.surrogate.n_theta &gt; 1:\n        theta = np.power(10, self.surrogate.theta)\n        imp = 100 * theta / np.max(theta)\n        idx = np.where(imp &gt; threshold)[0]\n        if self.var_name is None:\n            plt.bar(range(len(imp[idx])), imp[idx])\n            plt.xticks(range(len(imp[idx])), [\"x\" + str(i) for i in idx])\n        else:\n            var_name = [self.var_name[i] for i in idx]\n            plt.bar(range(len(imp[idx])), imp[idx])\n            plt.xticks(range(len(imp[idx])), var_name)\n        if filename is not None:\n            plt.savefig(filename, bbox_inches=\"tight\", dpi=dpi)\n        if show:\n            plt.show()\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.plot_model","title":"<code>plot_model(y_min=None, y_max=None)</code>","text":"<p>Plot the model fit for 1-dim objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>y_min</code> <code>float</code> <p>y range, lower bound.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>y range, upper bound.</p> <code>None</code> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def plot_model(self, y_min=None, y_max=None):\n    \"\"\"\n    Plot the model fit for 1-dim objective functions.\n\n    Args:\n        y_min (float, optional): y range, lower bound.\n        y_max (float, optional): y range, upper bound.\n    \"\"\"\n    if self.k == 1:\n        X_test = np.linspace(self.lower[0], self.upper[0], 100)\n        y_test = self.fun(X=X_test.reshape(-1, 1), fun_control=self.fun_control)\n        if isinstance(self.surrogate, Kriging):\n            y_hat = self.surrogate.predict(X_test[:, np.newaxis], return_val=\"y\")\n        else:\n            y_hat = self.surrogate.predict(X_test[:, np.newaxis])\n        plt.plot(X_test, y_hat, label=\"Model\")\n        plt.plot(X_test, y_test, label=\"True function\")\n        plt.scatter(self.X, self.y, edgecolor=\"b\", s=20, label=\"Samples\")\n        plt.scatter(self.X[-1], self.y[-1], edgecolor=\"r\", s=30, label=\"Last Sample\")\n        if self.noise:\n            plt.scatter(self.min_mean_X, self.min_mean_y, edgecolor=\"g\", s=30, label=\"Best Sample (mean)\")\n        else:\n            plt.scatter(self.min_X, self.min_y, edgecolor=\"g\", s=30, label=\"Best Sample\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.xlim((self.lower[0], self.upper[0]))\n        if y_min is None:\n            y_min = min(min(self.y), min(y_test))\n        if y_max is None:\n            y_max = max(max(self.y), max(y_test))\n        plt.ylim((y_min, y_max))\n        plt.legend(loc=\"best\")\n        # plt.title(self.surrogate.__class__.__name__ + \". \" + str(self.counter) + \": \" + str(self.min_y))\n        if self.noise:\n            plt.title(\n                str(self.counter)\n                + \". y (noise): \"\n                + str(np.round(self.min_y, 6))\n                + \" y mean: \"\n                + str(np.round(self.min_mean_y, 6))\n            )\n        else:\n            plt.title(str(self.counter) + \". y: \" + str(np.round(self.min_y, 6)))\n        plt.show()\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.plot_progress","title":"<code>plot_progress(show=True, log_x=False, log_y=False, filename='plot.png', style=['ko', 'k', 'ro-'], dpi=300)</code>","text":"<p>Plot the progress of the hyperparameter tuning (optimization). Args:     show (bool):         Show the plot.     log_x (bool):         Use logarithmic scale for x-axis.     log_y (bool):         Use logarithmic scale for y-axis.     filename (str):         Filename to save the plot.     style (list):         Style of the plot. Default: [\u2018k\u2019, \u2018ro-\u2018], i.e., the initial points are plotted as a black line         and the subsequent points as red dots connected by a line. Returns:     None</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def plot_progress(\n    self, show=True, log_x=False, log_y=False, filename=\"plot.png\", style=[\"ko\", \"k\", \"ro-\"], dpi=300\n) -&gt; None:\n    \"\"\"Plot the progress of the hyperparameter tuning (optimization).\n    Args:\n        show (bool):\n            Show the plot.\n        log_x (bool):\n            Use logarithmic scale for x-axis.\n        log_y (bool):\n            Use logarithmic scale for y-axis.\n        filename (str):\n            Filename to save the plot.\n        style (list):\n            Style of the plot. Default: ['k', 'ro-'], i.e., the initial points are plotted as a black line\n            and the subsequent points as red dots connected by a line.\n    Returns:\n        None\n    \"\"\"\n    fig = pylab.figure(figsize=(9, 6))\n    s_y = pd.Series(self.y)\n    s_c = s_y.cummin()\n    n_init = self.design_control[\"init_size\"] * self.design_control[\"repeats\"]\n    ax = fig.add_subplot(211)\n    ax.plot(\n        range(1, n_init + 1),\n        s_y[:n_init],\n        style[0],\n        range(1, n_init + 2),\n        [s_c[:n_init].min()] * (n_init + 1),\n        style[1],\n        range(n_init + 1, len(s_c) + 1),\n        s_c[n_init:],\n        style[2],\n    )\n    if log_x:\n        ax.set_xscale(\"log\")\n    if log_y:\n        ax.set_yscale(\"log\")\n    if filename is not None:\n        pylab.savefig(filename, dpi=dpi, bbox_inches=\"tight\")\n    if show:\n        pylab.show()\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.print_importance","title":"<code>print_importance(threshold=0.1, print_screen=True)</code>","text":"<p>Print importance of each variable and return the results as a list. Args:     threshold (float): threshold for printing     print_screen (boolean): if <code>True</code>, values are also printed on the screen. Default is <code>True</code>. Returns:     output (list): list of results</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def print_importance(self, threshold=0.1, print_screen=True) -&gt; list:\n    \"\"\"Print importance of each variable and return the results as a list.\n    Args:\n        threshold (float): threshold for printing\n        print_screen (boolean): if `True`, values are also printed on the screen. Default is `True`.\n    Returns:\n        output (list): list of results\n    \"\"\"\n    output = []\n    if self.surrogate.n_theta &gt; 1:\n        theta = np.power(10, self.surrogate.theta)\n        imp = 100 * theta / np.max(theta)\n        # imp = imp[imp &gt;= threshold]\n        if self.var_name is None:\n            for i in range(len(imp)):\n                if imp[i] &gt;= threshold:\n                    if print_screen:\n                        print(\"x\", i, \": \", imp[i])\n                    output.append(\"x\" + str(i) + \": \" + str(imp[i]))\n        else:\n            var_name = [self.var_name[i] for i in range(len(imp))]\n            for i in range(len(imp)):\n                if imp[i] &gt;= threshold:\n                    if print_screen:\n                        print(var_name[i] + \": \", imp[i])\n                output.append([var_name[i], imp[i]])\n    else:\n        print(\"Importance requires more than one theta values (n_theta&gt;1).\")\n    return output\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.print_results","title":"<code>print_results(print_screen=True)</code>","text":"Print results from the run <ol> <li>min y</li> <li>min X If <code>noise == True</code>, additionally the following values are printed:</li> <li>min mean y</li> <li>min mean X</li> </ol> <p>Args:     print_screen (bool, optional): print results to screen Returns:     output (list): list of results</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def print_results(self, print_screen=True) -&gt; list[str]:\n    \"\"\"Print results from the run:\n        1. min y\n        2. min X\n        If `noise == True`, additionally the following values are printed:\n        3. min mean y\n        4. min mean X\n    Args:\n        print_screen (bool, optional): print results to screen\n    Returns:\n        output (list): list of results\n    \"\"\"\n    output = []\n    if print_screen:\n        print(f\"min y: {self.min_y}\")\n    res = self.to_all_dim(self.min_X.reshape(1, -1))\n    for i in range(res.shape[1]):\n        var_name = \"x\" + str(i) if self.all_var_name is None else self.all_var_name[i]\n        if print_screen:\n            print(var_name + \":\", res[0][i])\n        output.append([var_name, res[0][i]])\n    if self.noise:\n        res = self.to_all_dim(self.min_mean_X.reshape(1, -1))\n        if print_screen:\n            print(f\"min mean y: {self.min_mean_y}\")\n        for i in range(res.shape[1]):\n            var_name = \"x\" + str(i) if self.all_var_name is None else self.all_var_name[i]\n            if print_screen:\n                print(var_name + \":\", res[0][i])\n            output.append([var_name, res[0][i]])\n    return output\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.suggest_new_X","title":"<code>suggest_new_X()</code>","text":"<p>Compute <code>n_points</code> new infill points in natural units. The optimizer searches in the ranges from <code>lower_j</code> to <code>upper_j</code>. The method <code>infill()</code> is used as the objective function.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>n_points</code> infill points in natural units, each of dim k</p> Note <p>This is step (S-14a) in [bart21i].</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def suggest_new_X(self):\n    \"\"\"\n    Compute `n_points` new infill points in natural units.\n    The optimizer searches in the ranges from `lower_j` to `upper_j`.\n    The method `infill()` is used as the objective function.\n\n    Returns:\n        (numpy.ndarray): `n_points` infill points in natural units, each of dim k\n\n    Note:\n        This is step (S-14a) in [bart21i].\n    \"\"\"\n    # (S-14a) Optimization on the surrogate:\n    new_X = np.zeros([self.n_points, self.k], dtype=float)\n\n    optimizer_name = self.optimizer.__name__\n\n    optimizers = {\n        \"dual_annealing\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n        \"differential_evolution\": lambda: self.optimizer(\n            func=self.infill,\n            bounds=self.de_bounds,\n            maxiter=self.optimizer_control[\"max_iter\"],\n            seed=self.optimizer_control[\"seed\"],\n        ),\n        \"direct\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds, eps=1e-2),\n        \"shgo\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n        \"basinhopping\": lambda: self.optimizer(func=self.infill, x0=self.min_X),\n        \"default\": lambda: self.optimizer(func=self.infill, bounds=self.de_bounds),\n    }\n\n    for i in range(self.n_points):\n        result = optimizers.get(optimizer_name, optimizers[\"default\"])()\n        new_X[i][:] = result.x\n    return new_X\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.suggest_new_X_old","title":"<code>suggest_new_X_old()</code>","text":"<p>Compute <code>n_points</code> new infill points in natural units. The optimizer searches in the ranges from <code>lower_j</code> to <code>upper_j</code>. The method <code>infill()</code> is used as the objective function.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p><code>n_points</code> infill points in natural units, each of dim k</p> Note <p>This is step (S-14a) in [bart21i].</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def suggest_new_X_old(self):\n    \"\"\"\n    Compute `n_points` new infill points in natural units.\n    The optimizer searches in the ranges from `lower_j` to `upper_j`.\n    The method `infill()` is used as the objective function.\n\n    Returns:\n        (numpy.ndarray): `n_points` infill points in natural units, each of dim k\n\n    Note:\n        This is step (S-14a) in [bart21i].\n    \"\"\"\n    # (S-14a) Optimization on the surrogate:\n    new_X = np.zeros([self.n_points, self.k], dtype=float)\n\n    optimizer_name = self.optimizer.__name__\n    for i in range(self.n_points):\n        if optimizer_name == \"dual_annealing\":\n            result = self.optimizer(func=self.infill, bounds=self.de_bounds)\n        elif optimizer_name == \"differential_evolution\":\n            result = self.optimizer(\n                func=self.infill,\n                bounds=self.de_bounds,\n                maxiter=self.optimizer_control[\"max_iter\"],\n                seed=self.optimizer_control[\"seed\"],\n                # popsize=10,\n                # updating=\"deferred\"\n            )\n        elif optimizer_name == \"direct\":\n            result = self.optimizer(func=self.infill, bounds=self.de_bounds, eps=1e-2)\n        elif optimizer_name == \"shgo\":\n            result = self.optimizer(func=self.infill, bounds=self.de_bounds)\n        elif optimizer_name == \"basinhopping\":\n            result = self.optimizer(func=self.infill, x0=self.min_X)\n        else:\n            result = self.optimizer(func=self.infill, bounds=self.de_bounds)\n        new_X[i][:] = result.x\n    return new_X\n</code></pre>"},{"location":"reference/spotPython/spot/spot/#spotPython.spot.spot.Spot.update_stats","title":"<code>update_stats()</code>","text":"<p>Update the following stats: 1. <code>min_y</code> 2. <code>min_X</code> 3. <code>counter</code> If <code>noise</code> is <code>True</code>, additionally the following stats are computed: 1. <code>mean_X</code> 2. <code>mean_y</code> 3. <code>min_mean_y</code> 4. <code>min_mean_X</code>.</p> Source code in <code>spotPython/spot/spot.py</code> <pre><code>def update_stats(self):\n    \"\"\"\n    Update the following stats: 1. `min_y` 2. `min_X` 3. `counter`\n    If `noise` is `True`, additionally the following stats are computed: 1. `mean_X`\n    2. `mean_y` 3. `min_mean_y` 4. `min_mean_X`.\n\n    \"\"\"\n    self.min_y = min(self.y)\n    self.min_X = self.X[argmin(self.y)]\n    self.counter = self.y.size\n    # Update aggregated x and y values (if noise):\n    if self.noise:\n        Z = aggregate_mean_var(X=self.X, y=self.y)\n        self.mean_X = Z[0]\n        self.mean_y = Z[1]\n        self.var_y = Z[2]\n        # X value of the best mean y value so far:\n        self.min_mean_X = self.mean_X[argmin(self.mean_y)]\n        # variance of the best mean y value so far:\n        self.min_var_y = self.var_y[argmin(self.mean_y)]\n        # best mean y value so far:\n        self.min_mean_y = self.mean_y[argmin(self.mean_y)]\n</code></pre>"},{"location":"reference/spotPython/torch/activation/","title":"activation","text":""},{"location":"reference/spotPython/torch/dataframedataset/","title":"dataframedataset","text":""},{"location":"reference/spotPython/torch/initialization/","title":"initialization","text":""},{"location":"reference/spotPython/torch/mapk/","title":"mapk","text":""},{"location":"reference/spotPython/torch/mapk/#spotPython.torch.mapk.MAPK","title":"<code>MAPK</code>","text":"<p>             Bases: <code>Metric</code></p> <p>Mean Average Precision at K (MAPK) metric.</p> <p>This class inherits from the <code>Metric</code> class of the <code>torchmetrics</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of top predictions to consider when calculating the metric.</p> <code>10</code> <code>dist_sync_on_step</code> <code>bool</code> <p>Whether to synchronize the metric states across processes during the forward pass.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>total</code> <code>Tensor</code> <p>The cumulative sum of the metric scores across all batches.</p> <code>count</code> <code>Tensor</code> <p>The number of batches processed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.torch.mapk import MAPK\n    import torch\n    mapk = MAPK(k=2)\n    target = torch.tensor([0, 1, 2, 2])\n    preds = torch.tensor(\n        [\n            [0.5, 0.2, 0.2],  # 0 is in top 2\n            [0.3, 0.4, 0.2],  # 1 is in top 2\n            [0.2, 0.4, 0.3],  # 2 is in top 2\n            [0.7, 0.2, 0.1],  # 2 isn't in top 2\n        ]\n    )\n    mapk.update(preds, target)\n    print(mapk.compute()) # tensor(0.6250)\n</code></pre> Source code in <code>spotPython/torch/mapk.py</code> <pre><code>class MAPK(torchmetrics.Metric):\n    \"\"\"\n    Mean Average Precision at K (MAPK) metric.\n\n    This class inherits from the `Metric` class of the `torchmetrics` library.\n\n    Args:\n        k (int):\n            The number of top predictions to consider when calculating the metric.\n        dist_sync_on_step (bool):\n            Whether to synchronize the metric states across processes during the forward pass.\n\n    Attributes:\n        total (torch.Tensor):\n            The cumulative sum of the metric scores across all batches.\n        count (torch.Tensor):\n            The number of batches processed.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.torch.mapk import MAPK\n            import torch\n            mapk = MAPK(k=2)\n            target = torch.tensor([0, 1, 2, 2])\n            preds = torch.tensor(\n                [\n                    [0.5, 0.2, 0.2],  # 0 is in top 2\n                    [0.3, 0.4, 0.2],  # 1 is in top 2\n                    [0.2, 0.4, 0.3],  # 2 is in top 2\n                    [0.7, 0.2, 0.1],  # 2 isn't in top 2\n                ]\n            )\n            mapk.update(preds, target)\n            print(mapk.compute()) # tensor(0.6250)\n    \"\"\"\n\n    def __init__(self, k=10, dist_sync_on_step=False):\n        super().__init__(dist_sync_on_step=dist_sync_on_step)\n        self.k = k\n        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"count\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, predicted: torch.Tensor, actual: torch.Tensor):\n        \"\"\"\n        Update the state variables with a new batch of data.\n\n        Args:\n            predicted (torch.Tensor):\n                A 2D tensor containing the predicted scores for each class.\n            actual (torch.Tensor):\n                A 1D tensor containing the ground truth labels.\n        Returns:\n            (NoneType): None\n\n        Examples:\n            &gt;&gt;&gt; from spotPython.torch.mapk import MAPK\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; mapk = MAPK(k=2)\n            &gt;&gt;&gt; target = torch.tensor([0, 1, 2, 2])\n            &gt;&gt;&gt; preds = torch.tensor(\n            ...     [\n            ...         [0.5, 0.2, 0.2],  # 0 is in top 2\n            ...         [0.3, 0.4, 0.2],  # 1 is in top 2\n            ...         [0.2, 0.4, 0.3],  # 2 is in top 2\n            ...         [0.7, 0.2, 0.1],  # 2 isn't in top 2\n            ...     ]\n            ... )\n            &gt;&gt;&gt; mapk.update(preds, target)\n            &gt;&gt;&gt; print(mapk.compute()) # tensor(0.6250)\n\n        Raises:\n            AssertionError: If the actual tensor is not 1D or the predicted tensor is not 2D.\n            AssertionError: If the number of elements in the actual and predicted tensors are not equal.\n\n        \"\"\"\n        assert len(actual.shape) == 1, \"actual must be a 1D tensor\"\n        assert len(predicted.shape) == 2, \"predicted must be a 2D tensor\"\n        assert actual.shape[0] == predicted.shape[0], \"actual and predicted must have the same number of elements\"\n\n        # Convert actual to list of lists\n        actual = actual.tolist()\n        actual = [[a] for a in actual]\n\n        # Convert predicted to list of lists of indices sorted by confidence score\n        _, predicted = predicted.topk(k=self.k, dim=1)\n        predicted = predicted.tolist()\n        # Code modified according to: \"Inplace update to inference tensor outside InferenceMode\n        # is not allowed. You can make a clone to get a normal tensor before doing inplace update.\"\n        score = np.mean([self.apk(p, a, self.k) for p, a in zip(predicted, actual)])\n        self.total = self.total + score\n        self.count = self.count + 1\n\n    def compute(self) -&gt; float:\n        \"\"\"\n        Compute the mean average precision at k.\n\n        Args:\n            self (MAPK):\n                The current instance of the class.\n\n        Returns:\n            (float):\n                The mean average precision at k.\n\n        Examples:\n            &gt;&gt;&gt; evaluator = Evaluator()\n            &gt;&gt;&gt; evaluator.total = 3.0\n            &gt;&gt;&gt; evaluator.count = 2\n            &gt;&gt;&gt; evaluator.compute()\n            1.5\n        \"\"\"\n        return self.total / self.count\n\n    @staticmethod\n    def apk(predicted: List[int], actual: List[int], k: int = 10) -&gt; float:\n        \"\"\"\n        Calculate the average precision at k for a single pair of actual and predicted labels.\n\n        Args:\n            predicted (list): A list of predicted labels.\n            actual (list): A list of ground truth labels.\n            k (int): The number of top predictions to consider.\n\n        Returns:\n            float: The average precision at k.\n\n        Examples:\n            &gt;&gt;&gt; Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3)\n            0.8888888888888888\n        \"\"\"\n        if not actual:\n            return 0.0\n\n        if len(predicted) &gt; k:\n            predicted = predicted[:k]\n\n        score = 0.0\n        num_hits = 0.0\n\n        for i, p in enumerate(predicted):\n            if p in actual and p not in predicted[:i]:\n                num_hits += 1.0\n                score += num_hits / (i + 1.0)\n\n        return score / min(len(actual), k)\n</code></pre>"},{"location":"reference/spotPython/torch/mapk/#spotPython.torch.mapk.MAPK.apk","title":"<code>apk(predicted, actual, k=10)</code>  <code>staticmethod</code>","text":"<p>Calculate the average precision at k for a single pair of actual and predicted labels.</p> <p>Parameters:</p> Name Type Description Default <code>predicted</code> <code>list</code> <p>A list of predicted labels.</p> required <code>actual</code> <code>list</code> <p>A list of ground truth labels.</p> required <code>k</code> <code>int</code> <p>The number of top predictions to consider.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The average precision at k.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3)\n0.8888888888888888\n</code></pre> Source code in <code>spotPython/torch/mapk.py</code> <pre><code>@staticmethod\ndef apk(predicted: List[int], actual: List[int], k: int = 10) -&gt; float:\n    \"\"\"\n    Calculate the average precision at k for a single pair of actual and predicted labels.\n\n    Args:\n        predicted (list): A list of predicted labels.\n        actual (list): A list of ground truth labels.\n        k (int): The number of top predictions to consider.\n\n    Returns:\n        float: The average precision at k.\n\n    Examples:\n        &gt;&gt;&gt; Evaluator.apk([1, 3, 2, 4], [1, 2, 3], 3)\n        0.8888888888888888\n    \"\"\"\n    if not actual:\n        return 0.0\n\n    if len(predicted) &gt; k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i, p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i + 1.0)\n\n    return score / min(len(actual), k)\n</code></pre>"},{"location":"reference/spotPython/torch/mapk/#spotPython.torch.mapk.MAPK.compute","title":"<code>compute()</code>","text":"<p>Compute the mean average precision at k.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>MAPK</code> <p>The current instance of the class.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean average precision at k.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; evaluator = Evaluator()\n&gt;&gt;&gt; evaluator.total = 3.0\n&gt;&gt;&gt; evaluator.count = 2\n&gt;&gt;&gt; evaluator.compute()\n1.5\n</code></pre> Source code in <code>spotPython/torch/mapk.py</code> <pre><code>def compute(self) -&gt; float:\n    \"\"\"\n    Compute the mean average precision at k.\n\n    Args:\n        self (MAPK):\n            The current instance of the class.\n\n    Returns:\n        (float):\n            The mean average precision at k.\n\n    Examples:\n        &gt;&gt;&gt; evaluator = Evaluator()\n        &gt;&gt;&gt; evaluator.total = 3.0\n        &gt;&gt;&gt; evaluator.count = 2\n        &gt;&gt;&gt; evaluator.compute()\n        1.5\n    \"\"\"\n    return self.total / self.count\n</code></pre>"},{"location":"reference/spotPython/torch/mapk/#spotPython.torch.mapk.MAPK.update","title":"<code>update(predicted, actual)</code>","text":"<p>Update the state variables with a new batch of data.</p> <p>Parameters:</p> Name Type Description Default <code>predicted</code> <code>Tensor</code> <p>A 2D tensor containing the predicted scores for each class.</p> required <code>actual</code> <code>Tensor</code> <p>A 1D tensor containing the ground truth labels.</p> required <p>Returns:     (NoneType): None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.torch.mapk import MAPK\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; mapk = MAPK(k=2)\n&gt;&gt;&gt; target = torch.tensor([0, 1, 2, 2])\n&gt;&gt;&gt; preds = torch.tensor(\n...     [\n...         [0.5, 0.2, 0.2],  # 0 is in top 2\n...         [0.3, 0.4, 0.2],  # 1 is in top 2\n...         [0.2, 0.4, 0.3],  # 2 is in top 2\n...         [0.7, 0.2, 0.1],  # 2 isn't in top 2\n...     ]\n... )\n&gt;&gt;&gt; mapk.update(preds, target)\n&gt;&gt;&gt; print(mapk.compute()) # tensor(0.6250)\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the actual tensor is not 1D or the predicted tensor is not 2D.</p> <code>AssertionError</code> <p>If the number of elements in the actual and predicted tensors are not equal.</p> Source code in <code>spotPython/torch/mapk.py</code> <pre><code>def update(self, predicted: torch.Tensor, actual: torch.Tensor):\n    \"\"\"\n    Update the state variables with a new batch of data.\n\n    Args:\n        predicted (torch.Tensor):\n            A 2D tensor containing the predicted scores for each class.\n        actual (torch.Tensor):\n            A 1D tensor containing the ground truth labels.\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.torch.mapk import MAPK\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; mapk = MAPK(k=2)\n        &gt;&gt;&gt; target = torch.tensor([0, 1, 2, 2])\n        &gt;&gt;&gt; preds = torch.tensor(\n        ...     [\n        ...         [0.5, 0.2, 0.2],  # 0 is in top 2\n        ...         [0.3, 0.4, 0.2],  # 1 is in top 2\n        ...         [0.2, 0.4, 0.3],  # 2 is in top 2\n        ...         [0.7, 0.2, 0.1],  # 2 isn't in top 2\n        ...     ]\n        ... )\n        &gt;&gt;&gt; mapk.update(preds, target)\n        &gt;&gt;&gt; print(mapk.compute()) # tensor(0.6250)\n\n    Raises:\n        AssertionError: If the actual tensor is not 1D or the predicted tensor is not 2D.\n        AssertionError: If the number of elements in the actual and predicted tensors are not equal.\n\n    \"\"\"\n    assert len(actual.shape) == 1, \"actual must be a 1D tensor\"\n    assert len(predicted.shape) == 2, \"predicted must be a 2D tensor\"\n    assert actual.shape[0] == predicted.shape[0], \"actual and predicted must have the same number of elements\"\n\n    # Convert actual to list of lists\n    actual = actual.tolist()\n    actual = [[a] for a in actual]\n\n    # Convert predicted to list of lists of indices sorted by confidence score\n    _, predicted = predicted.topk(k=self.k, dim=1)\n    predicted = predicted.tolist()\n    # Code modified according to: \"Inplace update to inference tensor outside InferenceMode\n    # is not allowed. You can make a clone to get a normal tensor before doing inplace update.\"\n    score = np.mean([self.apk(p, a, self.k) for p, a in zip(predicted, actual)])\n    self.total = self.total + score\n    self.count = self.count + 1\n</code></pre>"},{"location":"reference/spotPython/torch/netcifar10/","title":"netcifar10","text":""},{"location":"reference/spotPython/torch/netcore/","title":"netcore","text":""},{"location":"reference/spotPython/torch/netfashionMNIST/","title":"netfashionMNIST","text":""},{"location":"reference/spotPython/torch/netregression/","title":"netregression","text":""},{"location":"reference/spotPython/torch/netvbdp/","title":"netvbdp","text":""},{"location":"reference/spotPython/torch/traintest/","title":"traintest","text":""},{"location":"reference/spotPython/utils/aggregate/","title":"aggregate","text":""},{"location":"reference/spotPython/utils/aggregate/#spotPython.utils.aggregate.aggregate_mean_var","title":"<code>aggregate_mean_var(X, y, sort=False)</code>","text":"<p>Aggregate array to mean.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array, shape <code>(n, k)</code>.</p> required <code>y</code> <code>ndarray</code> <p>values, shape <code>(n,)</code>.</p> required <code>sort</code> <code>bool</code> <p>Whether to sort the resulting DataFrame by the group keys.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>aggregated <code>X</code> values, shape <code>(n-m, k)</code>, if <code>m</code> duplicates in <code>X</code>.</p> <code>ndarray</code> <p>aggregated (mean per group) <code>y</code> values, shape <code>(1,)</code>, if <code>m</code> duplicates in <code>X</code>.</p> <code>ndarray</code> <p>aggregated (variance per group) <code>y</code> values, shape <code>(1,)</code>, if <code>m</code> duplicates in <code>X</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [1, 2]])\n    y = np.array([1, 2, 3])\n    X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n    print(X_agg)\n    [[1. 2.]\n    [3. 4.]]\n    print(y_mean)\n    [2. 2.]\n    print(y_var)\n    [1. 0.]\n</code></pre> Source code in <code>spotPython/utils/aggregate.py</code> <pre><code>def aggregate_mean_var(X, y, sort=False):\n    \"\"\"\n    Aggregate array to mean.\n\n    Args:\n        X (numpy.ndarray): X array, shape `(n, k)`.\n        y (numpy.ndarray): values, shape `(n,)`.\n        sort (bool): Whether to sort the resulting DataFrame by the group keys.\n\n    Returns:\n        (numpy.ndarray): aggregated `X` values, shape `(n-m, k)`, if `m` duplicates in `X`.\n        (numpy.ndarray): aggregated (mean per group) `y` values, shape `(1,)`, if `m` duplicates in `X`.\n        (numpy.ndarray): aggregated (variance per group) `y` values, shape `(1,)`, if `m` duplicates in `X`.\n\n    Examples:\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [1, 2]])\n            y = np.array([1, 2, 3])\n            X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n            print(X_agg)\n            [[1. 2.]\n            [3. 4.]]\n            print(y_mean)\n            [2. 2.]\n            print(y_var)\n            [1. 0.]\n    \"\"\"\n    # Create a DataFrame from X and y\n    df = pd.DataFrame(X)\n    df[\"y\"] = y\n\n    # Group by all columns except 'y' and calculate the mean and variance of 'y' for each group\n    grouped = df.groupby(list(df.columns.difference([\"y\"])), as_index=False, sort=sort)\n    df_mean = grouped.mean()\n    df_var = grouped.var()\n\n    # Convert the resulting DataFrames to numpy arrays\n    mean_array = df_mean.to_numpy()\n    var_array = df_var.to_numpy()\n\n    # Split the resulting arrays into separate arrays for X and y\n    X_agg = np.delete(mean_array, -1, 1)\n    y_mean = mean_array[:, -1]\n    y_var = var_array[:, -1]\n\n    return X_agg, y_mean, y_var\n</code></pre>"},{"location":"reference/spotPython/utils/aggregate/#spotPython.utils.aggregate.get_ranks","title":"<code>get_ranks(x)</code>","text":"<p>Returns a numpy array containing ranks of numbers within an input numpy array x:</p> <p>Examples:</p> <p>get_ranks([2, 1]) [1, 0]</p> <p>get_ranks([20, 10, 100]) [1, 0, 2]</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>numpy array</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ranks</p> Source code in <code>spotPython/utils/aggregate.py</code> <pre><code>def get_ranks(x):\n    \"\"\"\n    Returns a numpy array containing ranks of numbers within an input numpy array x:\n\n    Examples:\n\n    get_ranks([2, 1])\n    [1, 0]\n\n    get_ranks([20, 10, 100])\n    [1, 0, 2]\n\n    Args:\n        x (numpy.ndarray): numpy array\n\n    Returns:\n        (numpy.ndarray): ranks\n\n    \"\"\"\n    ts = x.argsort()\n    ranks = np.empty_like(ts)\n    ranks[ts] = np.arange(len(x))\n    return ranks\n</code></pre>"},{"location":"reference/spotPython/utils/classes/","title":"classes","text":""},{"location":"reference/spotPython/utils/compare/","title":"compare","text":""},{"location":"reference/spotPython/utils/compare/#spotPython.utils.compare.find_equal_in_lists","title":"<code>find_equal_in_lists(a, b)</code>","text":"<p>Find equal values in two lists.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>list</code> <p>list with a values</p> required <code>b</code> <code>list</code> <p>list with b values</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[int]</code> <p>list with 1 if equal, otherwise 0</p> <p>Examples:     &gt;&gt;&gt; from spotPython.utils.compare import find_equal_in_lists         a = [1, 2, 3, 4, 5]         b = [1, 2, 3, 4, 5]         find_equal_in_lists(a, b)         [1, 1, 1, 1, 1]</p> Source code in <code>spotPython/utils/compare.py</code> <pre><code>def find_equal_in_lists(a: List[int], b: List[int]) -&gt; List[int]:\n    \"\"\"Find equal values in two lists.\n\n    Args:\n        a (list): list with a values\n        b (list): list with b values\n\n    Returns:\n        list: list with 1 if equal, otherwise 0\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.compare import find_equal_in_lists\n            a = [1, 2, 3, 4, 5]\n            b = [1, 2, 3, 4, 5]\n            find_equal_in_lists(a, b)\n            [1, 1, 1, 1, 1]\n    \"\"\"\n    equal = [1 if a[i] == b[i] else 0 for i in range(len(a))]\n    return equal\n</code></pre>"},{"location":"reference/spotPython/utils/compare/#spotPython.utils.compare.selectNew","title":"<code>selectNew(A, X, tolerance=0)</code>","text":"<p>Select rows from A that are not in X.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>A array with new values</p> required <code>X</code> <code>ndarray</code> <p>X array with known values</p> required <code>tolerance</code> <code>float</code> <p>tolerance value for comparison</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array with unknown (new) values</p> <code>ndarray</code> <p>array with <code>True</code> if value is new, otherwise <code>False</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.utils.compare import selectNew\n    A = np.array([[1,2,3],[4,5,6]])\n    X = np.array([[1,2,3],[4,5,6]])\n    selectNew(A, X)\n    (array([], shape=(0, 3), dtype=int64), array([], dtype=bool))\n</code></pre> Source code in <code>spotPython/utils/compare.py</code> <pre><code>def selectNew(A: np.ndarray, X: np.ndarray, tolerance: float = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Select rows from A that are not in X.\n\n    Args:\n        A (numpy.ndarray): A array with new values\n        X (numpy.ndarray): X array with known values\n        tolerance (float): tolerance value for comparison\n\n    Returns:\n        (numpy.ndarray): array with unknown (new) values\n        (numpy.ndarray): array with `True` if value is new, otherwise `False`.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.compare import selectNew\n            A = np.array([[1,2,3],[4,5,6]])\n            X = np.array([[1,2,3],[4,5,6]])\n            selectNew(A, X)\n            (array([], shape=(0, 3), dtype=int64), array([], dtype=bool))\n\n    \"\"\"\n    B = np.abs(A[:, None] - X)\n    ind = np.any(np.all(B &lt;= tolerance, axis=2), axis=1)\n    return A[~ind], ~ind\n</code></pre>"},{"location":"reference/spotPython/utils/convert/","title":"convert","text":""},{"location":"reference/spotPython/utils/convert/#spotPython.utils.convert.add_logical_columns","title":"<code>add_logical_columns(X, arity=2, operations=['and', 'or', 'xor'])</code>","text":"<p>Combines all features in a dataframe with each other using bitwise operations</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>dataframe with features</p> required <code>arity</code> <code>int</code> <p>the number of columns to combine at once</p> <code>2</code> <code>operations</code> <code>list of str</code> <p>the operations to apply. Possible values are \u2018and\u2019, \u2018or\u2019 and \u2018xor\u2019</p> <code>['and', 'or', 'xor']</code> <p>Returns:     X (pd.DataFrame): dataframe with new features Examples:     &gt;&gt;&gt; X = pd.DataFrame({\u201ca\u201d: [True, False, True], \u201cb\u201d: [True, True, False], \u201cc\u201d: [False, False, True]})     &gt;&gt;&gt; add_logical_columns(X)         a      b      c  a_and_b  a_and_c  b_and_c  a_or_b  a_or_c  b_or_c  a_xor_b  a_xor_c  b_xor_c     0  True   True  False     True    False    False    True    True    True    False     True     True     1 False   True  False    False    False    False    True   False    True     True     True    False     2  True  False   True    False     True    False    True    True    True     True    False     True</p> Source code in <code>spotPython/utils/convert.py</code> <pre><code>def add_logical_columns(X, arity=2, operations=[\"and\", \"or\", \"xor\"]):\n    \"\"\"Combines all features in a dataframe with each other using bitwise operations\n\n    Args:\n        X (pd.DataFrame): dataframe with features\n        arity (int): the number of columns to combine at once\n        operations (list of str): the operations to apply. Possible values are 'and', 'or' and 'xor'\n    Returns:\n        X (pd.DataFrame): dataframe with new features\n    Examples:\n        &gt;&gt;&gt; X = pd.DataFrame({\"a\": [True, False, True], \"b\": [True, True, False], \"c\": [False, False, True]})\n        &gt;&gt;&gt; add_logical_columns(X)\n            a      b      c  a_and_b  a_and_c  b_and_c  a_or_b  a_or_c  b_or_c  a_xor_b  a_xor_c  b_xor_c\n        0  True   True  False     True    False    False    True    True    True    False     True     True\n        1 False   True  False    False    False    False    True   False    True     True     True    False\n        2  True  False   True    False     True    False    True    True    True     True    False     True\n\n    \"\"\"\n    new_cols = []\n    # Iterate over all combinations of columns of the given arity\n    for cols in combinations(X.columns, arity):\n        # Create new columns for the specified operations\n        if \"and\" in operations:\n            and_col = X[list(cols)].apply(lambda x: x.all(), axis=1)\n            new_cols.append(and_col)\n        if \"or\" in operations:\n            or_col = X[list(cols)].apply(lambda x: x.any(), axis=1)\n            new_cols.append(or_col)\n        if \"xor\" in operations:\n            xor_col = X[list(cols)].apply(lambda x: x.sum() % 2 == 1, axis=1)\n            new_cols.append(xor_col)\n    # Join all the new columns at once\n    X = pd.concat([X] + new_cols, axis=1)\n    return X\n</code></pre>"},{"location":"reference/spotPython/utils/convert/#spotPython.utils.convert.class_for_name","title":"<code>class_for_name(module_name, class_name)</code>","text":"<p>Returns a class for a given module and class name.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>The name of the module.</p> required <code>class_name</code> <code>str</code> <p>The name of the class.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>The class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotPython.utils.convert import class_for_name\n    from scipy.optimize import rosen\n    bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n    shgo_class = class_for_name(\"scipy.optimize\", \"shgo\")\n    result = shgo_class(rosen, bounds)\n</code></pre> Source code in <code>spotPython/utils/convert.py</code> <pre><code>def class_for_name(module_name, class_name) -&gt; object:\n    \"\"\"Returns a class for a given module and class name.\n\n    Parameters:\n        module_name (str): The name of the module.\n        class_name (str): The name of the class.\n\n    Returns:\n        object: The class.\n\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.convert import class_for_name\n            from scipy.optimize import rosen\n            bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n            shgo_class = class_for_name(\"scipy.optimize\", \"shgo\")\n            result = shgo_class(rosen, bounds)\n    \"\"\"\n    m = importlib.import_module(module_name)\n    c = getattr(m, class_name)\n    return c\n</code></pre>"},{"location":"reference/spotPython/utils/convert/#spotPython.utils.convert.get_Xy_from_df","title":"<code>get_Xy_from_df(df, target_column)</code>","text":"<p>Get X and y from a dataframe. Parameters:     df (pandas.DataFrame): The input dataframe.     target_column (str): The name of the target column. Returns:     tuple: The tuple (X, y). Examples:     &gt;&gt;&gt; from spotPython.utils.convert import get_Xy_from_df     &gt;&gt;&gt; import pandas as pd     &gt;&gt;&gt; df = pd.DataFrame({\u201ca\u201d: [1, 2, 3], \u201cb\u201d: [4, 5, 6], \u201cc\u201d: [7, 8, 9]})     &gt;&gt;&gt; X, y = get_Xy_from_df(df, \u201cc\u201d)</p> Source code in <code>spotPython/utils/convert.py</code> <pre><code>def get_Xy_from_df(df, target_column) -&gt; tuple:\n    \"\"\"Get X and y from a dataframe.\n    Parameters:\n        df (pandas.DataFrame): The input dataframe.\n        target_column (str): The name of the target column.\n    Returns:\n        tuple: The tuple (X, y).\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.convert import get_Xy_from_df\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\n        &gt;&gt;&gt; X, y = get_Xy_from_df(df, \"c\")\n    \"\"\"\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n    # convert to numpy arrays\n    X = X.to_numpy()\n    y = y.to_numpy()\n    return X, y\n</code></pre>"},{"location":"reference/spotPython/utils/convert/#spotPython.utils.convert.series_to_array","title":"<code>series_to_array(series)</code>","text":"<p>Converts a pandas series to a numpy array. Args:     series (pandas.Series): The input series. Returns:     (numpy.ndarray): The output array. Examples:     &gt;&gt;&gt; from spotPython.utils.convert import series_to_array     &gt;&gt;&gt; import pandas as pd     &gt;&gt;&gt; series = pd.Series([1, 2, 3])     &gt;&gt;&gt; series_to_array(series)     array([1, 2, 3])</p> Source code in <code>spotPython/utils/convert.py</code> <pre><code>def series_to_array(series):\n    \"\"\"Converts a pandas series to a numpy array.\n    Args:\n        series (pandas.Series): The input series.\n    Returns:\n        (numpy.ndarray): The output array.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.convert import series_to_array\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; series = pd.Series([1, 2, 3])\n        &gt;&gt;&gt; series_to_array(series)\n        array([1, 2, 3])\n    \"\"\"\n    if isinstance(series, np.ndarray):\n        return series\n    else:\n        return series.to_numpy()\n</code></pre>"},{"location":"reference/spotPython/utils/device/","title":"device","text":""},{"location":"reference/spotPython/utils/device/#spotPython.utils.device.getDevice","title":"<code>getDevice(device=None)</code>","text":"<p>Get cpu, gpu or mps device for training. Args:     device (str):         Device for training. If None or \u201cauto\u201d the device is selected automatically. Returns:     device (str):         Device for training. Examples:     &gt;&gt;&gt; from spotPython.utils.device import getDevice     &gt;&gt;&gt; getDevice()     \u2018cuda:0\u2019</p> Source code in <code>spotPython/utils/device.py</code> <pre><code>def getDevice(device=None):\n    \"\"\"Get cpu, gpu or mps device for training.\n    Args:\n        device (str):\n            Device for training. If None or \"auto\" the device is selected automatically.\n    Returns:\n        device (str):\n            Device for training.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.device import getDevice\n        &gt;&gt;&gt; getDevice()\n        'cuda:0'\n    \"\"\"\n    if device is None or device == \"auto\":\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        elif torch.backends.mps.is_available():\n            device = \"mps\"\n    return device\n</code></pre>"},{"location":"reference/spotPython/utils/eda/","title":"eda","text":""},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.compare_two_tree_models","title":"<code>compare_two_tree_models(model1, model2, headers=['Parameter', 'Default', 'Spot'])</code>","text":"<p>Compares two tree models. Args:     model1 (object):         A tree model.     model2 (object):         A tree model.     headers (list):         A list with the headers of the table. Returns:     (str):         A table with the comparison of the two models. Examples:     &gt;&gt;&gt; from spotPython.utils.eda import compare_two_tree_models     &gt;&gt;&gt; from spotPython.hyperparameters.values import get_default_values     &gt;&gt;&gt; fun_control = {     \u2026     \u201cx1\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx2\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx3\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx4\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx5\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx6\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx7\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx8\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx9\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx10\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026 }     &gt;&gt;&gt; default_values = get_default_values(fun_control)     &gt;&gt;&gt; model1 = spot_tuner.get_model(\u201crf\u201d, default_values)     &gt;&gt;&gt; model2 = spot_tuner.get_model(\u201crf\u201d, default_values)     &gt;&gt;&gt; compare_two_tree_models(model1, model2)</p> Source code in <code>spotPython/utils/eda.py</code> <pre><code>def compare_two_tree_models(model1, model2, headers=[\"Parameter\", \"Default\", \"Spot\"]):\n    \"\"\"Compares two tree models.\n    Args:\n        model1 (object):\n            A tree model.\n        model2 (object):\n            A tree model.\n        headers (list):\n            A list with the headers of the table.\n    Returns:\n        (str):\n            A table with the comparison of the two models.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.eda import compare_two_tree_models\n        &gt;&gt;&gt; from spotPython.hyperparameters.values import get_default_values\n        &gt;&gt;&gt; fun_control = {\n        ...     \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ... }\n        &gt;&gt;&gt; default_values = get_default_values(fun_control)\n        &gt;&gt;&gt; model1 = spot_tuner.get_model(\"rf\", default_values)\n        &gt;&gt;&gt; model2 = spot_tuner.get_model(\"rf\", default_values)\n        &gt;&gt;&gt; compare_two_tree_models(model1, model2)\n    \"\"\"\n    keys = model1.summary.keys()\n    values1 = model1.summary.values()\n    values2 = model2.summary.values()\n    tbl = []\n    for key, value1, value2 in zip(keys, values1, values2):\n        tbl.append([key, value1, value2])\n    return tabulate(tbl, headers=headers, numalign=\"right\", tablefmt=\"github\")\n</code></pre>"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.count_missing_data","title":"<code>count_missing_data(df)</code>","text":"<p>Counts the number of missing values in each column of the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data to be counted.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame containing the number of missing values in each column.</p> Example <p>import pandas as pd df = pd.DataFrame({\u2018A\u2019: [1, 2, None], \u2018B\u2019: [4, None, 6], \u2018C\u2019: [7, 8, 9]}) count_missing_data(df)    column_name  missing_count 0           A              1 1           B              1</p> Source code in <code>spotPython/utils/eda.py</code> <pre><code>def count_missing_data(df):\n    \"\"\"\n    Counts the number of missing values in each column of the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data to be counted.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the number of missing values in each column.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, None], 'B': [4, None, 6], 'C': [7, 8, 9]})\n        &gt;&gt;&gt; count_missing_data(df)\n           column_name  missing_count\n        0           A              1\n        1           B              1\n    \"\"\"\n    missing_df = df.isnull().sum(axis=0).reset_index()\n    missing_df.columns = [\"column_name\", \"missing_count\"]\n    missing_df = missing_df.loc[missing_df[\"missing_count\"] &gt; 0]\n    missing_df = missing_df.sort_values(by=\"missing_count\")\n\n    return missing_df\n</code></pre>"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.filter_highly_correlated","title":"<code>filter_highly_correlated(df, sorted=True, threshold=1 - 1e-05)</code>","text":"<p>Return a new DataFrame with only those columns that are highly correlated.</p> <p>Args: df (DataFrame): The input DataFrame. threshold (float): The correlation threshold. sorted (bool): If True, the columns are sorted by name.</p> <p>Returns: DataFrame: A new DataFrame with only highly correlated columns.</p> <p>Examples:</p> <p>df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list(\u2018ABCD\u2019))     df = filter_highly_correlated(df, sorted=True, threshold=0.99)</p> Source code in <code>spotPython/utils/eda.py</code> <pre><code>def filter_highly_correlated(df: pd.DataFrame, sorted: bool = True, threshold: float = 1 - 1e-5) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new DataFrame with only those columns that are highly correlated.\n\n    Args:\n    df (DataFrame): The input DataFrame.\n    threshold (float): The correlation threshold.\n    sorted (bool): If True, the columns are sorted by name.\n\n    Returns:\n    DataFrame: A new DataFrame with only highly correlated columns.\n\n    Examples:\n    &gt;&gt;&gt; df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n        df = filter_highly_correlated(df, sorted=True, threshold=0.99)\n\n    \"\"\"\n    corr_matrix = df.corr()\n    # Find pairs of columns with correlation greater than threshold\n    corr_pairs = corr_matrix.abs().unstack()\n    corr_pairs = corr_pairs[corr_pairs &lt; 1]  # Remove self-correlations\n    high_corr = corr_pairs[corr_pairs &gt; threshold]\n    high_corr = high_corr[high_corr &lt; 1]  # Remove self-correlations\n\n    # Get the column names of highly correlated columns\n    high_corr_cols = list(set([col[0] for col in high_corr.index]))\n\n    # Create new DataFrame with only highly correlated columns\n    new_df = df[high_corr_cols]\n    # sort the columns by name\n    if sorted:\n        new_df = new_df.sort_index(axis=1)\n\n    return new_df\n</code></pre>"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.gen_design_table","title":"<code>gen_design_table(fun_control, spot=None, tablefmt='github')</code>","text":"<p>Generates a table with the design variables and their bounds. Args:     fun_control (dict):         A dictionary with function design variables.     spot (object):         A spot object. Defaults to None. Returns:     (str):         a table with the design variables, their default values, and their bounds.         If a spot object is provided,         the table will also include the value and the importance of each hyperparameter.         Use the <code>print</code> function to display the table. Examples:     &gt;&gt;&gt; from spotPython.utils.eda import gen_design_table     &gt;&gt;&gt; from spotPython.hyperparameters.values import get_default_values     &gt;&gt;&gt; fun_control = {     \u2026     \u201cx1\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx2\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx3\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx4\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx5\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx6\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx7\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx8\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx9\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026     \u201cx10\u201d: {\u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 1, \u201clower\u201d: 1, \u201cupper\u201d: 10},     \u2026 }</p> Source code in <code>spotPython/utils/eda.py</code> <pre><code>def gen_design_table(fun_control: dict, spot: object = None, tablefmt=\"github\") -&gt; str:\n    \"\"\"Generates a table with the design variables and their bounds.\n    Args:\n        fun_control (dict):\n            A dictionary with function design variables.\n        spot (object):\n            A spot object. Defaults to None.\n    Returns:\n        (str):\n            a table with the design variables, their default values, and their bounds.\n            If a spot object is provided,\n            the table will also include the value and the importance of each hyperparameter.\n            Use the `print` function to display the table.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.eda import gen_design_table\n        &gt;&gt;&gt; from spotPython.hyperparameters.values import get_default_values\n        &gt;&gt;&gt; fun_control = {\n        ...     \"x1\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x2\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x3\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x4\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x5\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x6\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x7\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x8\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x9\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ...     \"x10\": {\"type\": \"int\", \"default\": 1, \"lower\": 1, \"upper\": 10},\n        ... }\n    \"\"\"\n    default_values = get_default_values(fun_control)\n    defaults = list(default_values.values())\n    if spot is None:\n        tab = tabulate(\n            {\n                \"name\": get_var_name(fun_control),\n                \"type\": get_var_type(fun_control),\n                \"default\": defaults,\n                \"lower\": get_bound_values(fun_control, \"lower\", as_list=True),\n                \"upper\": get_bound_values(fun_control, \"upper\", as_list=True),\n                \"transform\": get_transform(fun_control),\n            },\n            headers=\"keys\",\n            tablefmt=tablefmt,\n        )\n    else:\n        res = spot.print_results(print_screen=False)\n        tuned = [item[1] for item in res]\n        # imp = spot.print_importance(threshold=0.0, print_screen=False)\n        # importance = [item[1] for item in imp]\n        importance = spot.get_importance()\n        stars = get_stars(importance)\n        tab = tabulate(\n            {\n                \"name\": get_var_name(fun_control),\n                \"type\": get_var_type(fun_control),\n                \"default\": defaults,\n                \"lower\": get_bound_values(fun_control, \"lower\", as_list=True),\n                \"upper\": get_bound_values(fun_control, \"upper\", as_list=True),\n                \"tuned\": tuned,\n                \"transform\": get_transform(fun_control),\n                \"importance\": importance,\n                \"stars\": stars,\n            },\n            headers=\"keys\",\n            numalign=\"right\",\n            floatfmt=(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \".2f\"),\n            tablefmt=tablefmt,\n        )\n    return tab\n</code></pre>"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.generate_config_id","title":"<code>generate_config_id(config)</code>","text":"<p>Generates a unique id for a configuration. Args:     config (dict):         A dictionary with the configuration. Returns:     (str):         A unique id for the configuration. Examples:     &gt;&gt;&gt; from spotPython.hyperparameters.values import get_one_config_from_X     &gt;&gt;&gt; X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))     &gt;&gt;&gt; config = get_one_config_from_X(X, fun_control)     &gt;&gt;&gt; generate_config_id(config)</p> Source code in <code>spotPython/utils/eda.py</code> <pre><code>def generate_config_id(config):\n    \"\"\"Generates a unique id for a configuration.\n    Args:\n        config (dict):\n            A dictionary with the configuration.\n    Returns:\n        (str):\n            A unique id for the configuration.\n    Examples:\n        &gt;&gt;&gt; from spotPython.hyperparameters.values import get_one_config_from_X\n        &gt;&gt;&gt; X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n        &gt;&gt;&gt; config = get_one_config_from_X(X, fun_control)\n        &gt;&gt;&gt; generate_config_id(config)\n    \"\"\"\n    config_id = \"\"\n    for key in config:\n        config_id += str(config[key]) + \"_\"\n        # hash the config_id to make it shorter and unique\n        config_id = str(hash(config_id)) + \"_\"\n    return config_id[:-1]\n</code></pre>"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.get_stars","title":"<code>get_stars(input_list)</code>","text":"<p>Converts a list of values to a list of stars, which can be used to     visualize the importance of a variable. Args:     input_list (list): A list of values. Returns:     (list):         A list of strings. Examples:     &gt;&gt;&gt; from spotPython.utils.eda import convert_list     &gt;&gt;&gt; get_stars([100, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])     [***, \u2018\u2019, \u2018\u2019, \u2018\u2019, \u2018\u2019, \u2018\u2019, \u2018\u2019, \u2018\u2019, \u2018\u2019]</p> Source code in <code>spotPython/utils/eda.py</code> <pre><code>def get_stars(input_list) -&gt; list:\n    \"\"\"Converts a list of values to a list of stars, which can be used to\n        visualize the importance of a variable.\n    Args:\n        input_list (list): A list of values.\n    Returns:\n        (list):\n            A list of strings.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.eda import convert_list\n        &gt;&gt;&gt; get_stars([100, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n        [***, '', '', '', '', '', '', '', '']\n    \"\"\"\n    output_list = []\n    for value in input_list:\n        if value &gt; 95:\n            output_list.append(\"***\")\n        elif value &gt; 50:\n            output_list.append(\"**\")\n        elif value &gt; 1:\n            output_list.append(\"*\")\n        elif value &gt; 0.1:\n            output_list.append(\".\")\n        else:\n            output_list.append(\"\")\n    return output_list\n</code></pre>"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.plot_missing_data","title":"<code>plot_missing_data(df, relative=False, figsize=(7, 5), color='grey', xlabel='Missing Data', title='Missing Data')</code>","text":"<p>Plots a horizontal bar chart of the number of missing values in each column of the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data to be plotted.</p> required <code>relative</code> <code>bool</code> <p>Whether to plot relative values (percentage) or absolute values.</p> <code>False</code> <code>figsize</code> <code>tuple</code> <p>Size of the figure to be plotted.</p> <code>(7, 5)</code> <code>color</code> <code>str</code> <p>Color of the bars in the bar chart.</p> <code>'grey'</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis.</p> <code>'Missing Data'</code> <code>title</code> <code>str</code> <p>Title for the plot.</p> <code>'Missing Data'</code> <p>Returns:</p> Type Description <p>None</p> Example <p>import pandas as pd df = pd.DataFrame({\u2018A\u2019: [1, 2, np.nan], \u2018B\u2019: [4, np.nan, 6], \u2018C\u2019: [7, 8, 9]}) plot_missing_data(df)</p> Source code in <code>spotPython/utils/eda.py</code> <pre><code>def plot_missing_data(df, relative=False, figsize=(7, 5), color=\"grey\", xlabel=\"Missing Data\", title=\"Missing Data\"):\n    \"\"\"\n    Plots a horizontal bar chart of the number of missing values in each column of the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data to be plotted.\n        relative (bool): Whether to plot relative values (percentage) or absolute values.\n        figsize (tuple): Size of the figure to be plotted.\n        color (str): Color of the bars in the bar chart.\n        xlabel (str): Label for the x-axis.\n        title (str): Title for the plot.\n\n    Returns:\n        None\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, np.nan], 'B': [4, np.nan, 6], 'C': [7, 8, 9]})\n        &gt;&gt;&gt; plot_missing_data(df)\n    \"\"\"\n    missing_df = count_missing_data(df)\n\n    if relative:\n        missing_df[\"missing_count\"] = missing_df[\"missing_count\"] / df.shape[0]\n        xlabel = \"Percentage of \" + xlabel\n        title = \"Percentage of \" + title\n\n    ind = np.arange(missing_df.shape[0])\n    _, ax = plt.subplots(figsize=figsize)\n    _ = ax.barh(ind, missing_df.missing_count.values, color=color)\n    ax.set_yticks(ind)\n    ax.set_yticklabels(missing_df.column_name.values, rotation=\"horizontal\")\n    ax.set_xlabel(xlabel)\n    ax.set_title(title)\n    plt.vlines(1, 0, missing_df.shape[0])\n    plt.vlines(0.97, 0, missing_df.shape[0])\n    plt.vlines(0.5, 0, missing_df.shape[0])\n    plt.show()\n</code></pre>"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.plot_sns_heatmap","title":"<code>plot_sns_heatmap(df_heat, figsize=(16, 12), cmap='vlag', vmin=-1, vmax=1, annot=True, fmt='.5f', linewidths=0.5, annot_kws={'size': 8})</code>","text":"<p>Plots a heatmap of the correlation matrix of the given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df_heat</code> <code>DataFrame</code> <p>DataFrame containing the data to be plotted.</p> required <code>figsize</code> <code>tuple</code> <p>Size of the figure to be plotted.</p> <code>(16, 12)</code> <code>cmap</code> <code>str</code> <p>Color map to be used for the heatmap.</p> <code>'vlag'</code> <code>vmin</code> <code>int</code> <p>Minimum value for the color scale.</p> <code>-1</code> <code>vmax</code> <code>int</code> <p>Maximum value for the color scale.</p> <code>1</code> <code>annot</code> <code>bool</code> <p>Whether to display annotations on the heatmap.</p> <code>True</code> <code>fmt</code> <code>str</code> <p>Format string for annotations.</p> <code>'.5f'</code> <code>linewidths</code> <code>float</code> <p>Width of lines separating cells in the heatmap.</p> <code>0.5</code> <code>annot_kws</code> <code>dict</code> <p>Keyword arguments for annotations.</p> <code>{'size': 8}</code> <p>Returns:</p> Type Description <p>None</p> Example <p>import pandas as pd df = pd.DataFrame({\u2018A\u2019: [1, 2, 3], \u2018B\u2019: [4, 5, 6], \u2018C\u2019: [7, 8, 9]}) plot_heatmap(df)</p> Source code in <code>spotPython/utils/eda.py</code> <pre><code>def plot_sns_heatmap(\n    df_heat,\n    figsize=(16, 12),\n    cmap=\"vlag\",\n    vmin=-1,\n    vmax=1,\n    annot=True,\n    fmt=\".5f\",\n    linewidths=0.5,\n    annot_kws={\"size\": 8},\n):\n    \"\"\"\n    Plots a heatmap of the correlation matrix of the given DataFrame.\n\n    Args:\n        df_heat (pd.DataFrame): DataFrame containing the data to be plotted.\n        figsize (tuple): Size of the figure to be plotted.\n        cmap (str): Color map to be used for the heatmap.\n        vmin (int): Minimum value for the color scale.\n        vmax (int): Maximum value for the color scale.\n        annot (bool): Whether to display annotations on the heatmap.\n        fmt (str): Format string for annotations.\n        linewidths (float): Width of lines separating cells in the heatmap.\n        annot_kws (dict): Keyword arguments for annotations.\n\n    Returns:\n        None\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\n        &gt;&gt;&gt; plot_heatmap(df)\n    \"\"\"\n    plt.figure(figsize=figsize)\n    matrix = np.triu(np.ones_like(df_heat.corr()))\n    sns.heatmap(\n        data=df_heat.corr(),\n        cmap=cmap,\n        vmin=vmin,\n        vmax=vmax,\n        annot=annot,\n        fmt=fmt,\n        linewidths=linewidths,\n        annot_kws=annot_kws,\n        mask=matrix,\n    )\n    plt.show()\n    plt.gcf().clear()\n</code></pre>"},{"location":"reference/spotPython/utils/eda/#spotPython.utils.eda.visualize_activations","title":"<code>visualize_activations(net, device='cpu', color='C0')</code>","text":"<p>Visualizes the activations of a neural network. Code is based on: PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS, Author: Phillip Lippe,     License: CC BY-SA.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>object</code> <p>A neural network.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \u201ccpu\u201d.</p> <code>'cpu'</code> <code>color</code> <code>str</code> <p>The color to use. Defaults to \u201cC0\u201d.</p> <code>'C0'</code> <p>Examples:     &gt;&gt;&gt; from spotPython.hyperparameters.values import get_one_config_from_X     &gt;&gt;&gt; X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))     &gt;&gt;&gt; config = get_one_config_from_X(X, fun_control)     &gt;&gt;&gt; model = fun_control\u201ccore_model\u201d     &gt;&gt;&gt; visualize_activations(model, device=\u201dcpu\u201d, color=f\u201dC{0}\u201d)</p> Source code in <code>spotPython/utils/eda.py</code> <pre><code>def visualize_activations(net, device=\"cpu\", color=\"C0\"):\n    \"\"\"Visualizes the activations of a neural network.\n    Code is based on: PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS, Author: Phillip Lippe,\n        License: CC BY-SA.\n\n    Args:\n        net (object):\n            A neural network.\n        device (str, optional):\n            The device to use. Defaults to \"cpu\".\n        color (str, optional):\n            The color to use. Defaults to \"C0\".\n    Examples:\n        &gt;&gt;&gt; from spotPython.hyperparameters.values import get_one_config_from_X\n        &gt;&gt;&gt; X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n        &gt;&gt;&gt; config = get_one_config_from_X(X, fun_control)\n        &gt;&gt;&gt; model = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11)\n        &gt;&gt;&gt; visualize_activations(model, device=\"cpu\", color=f\"C{0}\")\n    \"\"\"\n    activations = {}\n    net.eval()\n    # Create an instance of CSVDataset\n    dataset = CSVDataset(csv_file=\"./data/VBDP/train.csv\", train=True)\n    # Set batch size for DataLoader\n    batch_size = 128\n    # Create DataLoader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    # for batch in dataloader:\n    #     inputs, targets = batch\n    # small_loader = data.DataLoader(train_set, batch_size=1024)\n    inputs, _ = next(iter(dataloader))\n    with torch.no_grad():\n        layer_index = 0\n        inputs = inputs.to(device)\n        inputs = inputs.view(inputs.size(0), -1)\n        # We need to manually loop through the layers to save all activations\n        for layer_index, layer in enumerate(net.layers[:-1]):\n            inputs = layer(inputs)\n            activations[layer_index] = inputs.view(-1).cpu().numpy()\n\n    # Plotting\n    columns = 4\n    rows = math.ceil(len(activations) / columns)\n    fig, ax = plt.subplots(rows, columns, figsize=(columns * 2.7, rows * 2.5))\n    fig_index = 0\n    for key in activations:\n        key_ax = ax[fig_index // columns][fig_index % columns]\n        sns.histplot(data=activations[key], bins=50, ax=key_ax, color=color, kde=True, stat=\"density\")\n        key_ax.set_title(f\"Layer {key} - {net.layers[key].__class__.__name__}\")\n        fig_index += 1\n    fig.suptitle(f\"Activation distribution for activation function {net.hparams.act_fn}\", fontsize=14)\n    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n    plt.show()\n    plt.close()\n</code></pre>"},{"location":"reference/spotPython/utils/file/","title":"file","text":""},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.get_experiment_name","title":"<code>get_experiment_name(prefix='00')</code>","text":"<p>Returns a unique experiment name with a given prefix. Args:     prefix (str, optional): Prefix for the experiment name. Defaults to \u201c00\u201d. Returns:     str: Unique experiment name. Examples:     &gt;&gt;&gt; from spotPython.utils.file import get_experiment_name     &gt;&gt;&gt; get_experiment_name(prefix=\u201d00\u201d)     00_ubuntu_2021-08-31_14-30-00</p> Source code in <code>spotPython/utils/file.py</code> <pre><code>def get_experiment_name(prefix: str = \"00\") -&gt; str:\n    \"\"\"Returns a unique experiment name with a given prefix.\n    Args:\n        prefix (str, optional): Prefix for the experiment name. Defaults to \"00\".\n    Returns:\n        str: Unique experiment name.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.file import get_experiment_name\n        &gt;&gt;&gt; get_experiment_name(prefix=\"00\")\n        00_ubuntu_2021-08-31_14-30-00\n    \"\"\"\n    start_time = datetime.now(tzlocal())\n    HOSTNAME = socket.gethostname().split(\".\")[0]\n    experiment_name = prefix + \"_\" + HOSTNAME + \"_\" + str(start_time).split(\".\", 1)[0].replace(\" \", \"_\")\n    experiment_name = experiment_name.replace(\":\", \"-\")\n    return experiment_name\n</code></pre>"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.get_spot_tensorboard_path","title":"<code>get_spot_tensorboard_path(experiment_name)</code>","text":"<p>Get the path to the spot tensorboard files. Args:     experiment_name (str): The name of the experiment. Returns:     spot_tensorboard_path (str): The path to the folder where the spot tensorboard files are saved.</p> Source code in <code>spotPython/utils/file.py</code> <pre><code>def get_spot_tensorboard_path(experiment_name):\n    \"\"\"Get the path to the spot tensorboard files.\n    Args:\n        experiment_name (str): The name of the experiment.\n    Returns:\n        spot_tensorboard_path (str): The path to the folder where the spot tensorboard files are saved.\n    \"\"\"\n    spot_tensorboard_path = os.environ.get(\"PATH_TENSORBOARD\", \"runs/spot_logs/\")\n    spot_tensorboard_path = os.path.join(spot_tensorboard_path, experiment_name)\n    return spot_tensorboard_path\n</code></pre>"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.get_tensorboard_path","title":"<code>get_tensorboard_path(fun_control)</code>","text":"<p>Get the path to the tensorboard files. Args:     fun_control (dict): The function control dictionary. Returns:     tensorboard_path (str): The path to the folder where the tensorboard files are saved.</p> Source code in <code>spotPython/utils/file.py</code> <pre><code>def get_tensorboard_path(fun_control):\n    \"\"\"Get the path to the tensorboard files.\n    Args:\n        fun_control (dict): The function control dictionary.\n    Returns:\n        tensorboard_path (str): The path to the folder where the tensorboard files are saved.\n    \"\"\"\n    return fun_control[\"TENSORBOARD_PATH\"]\n</code></pre>"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.load_data","title":"<code>load_data(data_dir='./data')</code>","text":"<p>Loads the CIFAR10 dataset. Args:     data_dir (str, optional): Directory to save the data. Defaults to \u201c./data\u201d. Returns:     trainset (torchvision.datasets.CIFAR10): Training dataset. Examples:     &gt;&gt;&gt; from spotPython.utils.file import load_data     &gt;&gt;&gt; trainset = load_data(data_dir=\u201d./data\u201d)</p> Source code in <code>spotPython/utils/file.py</code> <pre><code>def load_data(data_dir=\"./data\"):\n    \"\"\"Loads the CIFAR10 dataset.\n    Args:\n        data_dir (str, optional): Directory to save the data. Defaults to \"./data\".\n    Returns:\n        trainset (torchvision.datasets.CIFAR10): Training dataset.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.file import load_data\n        &gt;&gt;&gt; trainset = load_data(data_dir=\"./data\")\n\n    \"\"\"\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n    trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n\n    testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n\n    return trainset, testset\n</code></pre>"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.load_pickle","title":"<code>load_pickle(filename)</code>","text":"<p>Loads a pickle file.     Add .pkl to the filename. Args:     filename (str): Name of the pickle file. Returns:     (object): Loaded object. Examples:     &gt;&gt;&gt; from spotPython.utils.file import load_pickle     &gt;&gt;&gt; obj = load_pickle(filename=\u201dobj.pkl\u201d)</p> Source code in <code>spotPython/utils/file.py</code> <pre><code>def load_pickle(filename: str):\n    \"\"\"Loads a pickle file.\n        Add .pkl to the filename.\n    Args:\n        filename (str): Name of the pickle file.\n    Returns:\n        (object): Loaded object.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.file import load_pickle\n        &gt;&gt;&gt; obj = load_pickle(filename=\"obj.pkl\")\n    \"\"\"\n    filename = filename + \".pkl\"\n    with open(filename, \"rb\") as f:\n        obj = pickle.load(f)\n    return obj\n</code></pre>"},{"location":"reference/spotPython/utils/file/#spotPython.utils.file.save_pickle","title":"<code>save_pickle(obj, filename)</code>","text":"<p>Saves an object as a pickle file.     Add .pkl to the filename. Args:     obj (object): Object to be saved.     filename (str): Name of the pickle file. Examples:     &gt;&gt;&gt; from spotPython.utils.file import save_pickle     &gt;&gt;&gt; save_pickle(obj, filename=\u201dobj.pkl\u201d)</p> Source code in <code>spotPython/utils/file.py</code> <pre><code>def save_pickle(obj, filename: str):\n    \"\"\"Saves an object as a pickle file.\n        Add .pkl to the filename.\n    Args:\n        obj (object): Object to be saved.\n        filename (str): Name of the pickle file.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.file import save_pickle\n        &gt;&gt;&gt; save_pickle(obj, filename=\"obj.pkl\")\n    \"\"\"\n    filename = filename + \".pkl\"\n    with open(filename, \"wb\") as f:\n        pickle.dump(obj, f)\n</code></pre>"},{"location":"reference/spotPython/utils/init/","title":"init","text":""},{"location":"reference/spotPython/utils/init/#spotPython.utils.init.check_and_create_dir","title":"<code>check_and_create_dir(path)</code>","text":"<p>Check if the path exists and create it if it does not. Args:     path (str): Path to the directory. Returns:     (noneType): None Examples:     &gt;&gt;&gt; fromspotPy.utils.init import check_and_create_dir     &gt;&gt;&gt; check_and_create_dir(\u201cdata/\u201d)</p> Source code in <code>spotPython/utils/init.py</code> <pre><code>def check_and_create_dir(path):\n    \"\"\"Check if the path exists and create it if it does not.\n    Args:\n        path (str): Path to the directory.\n    Returns:\n        (noneType): None\n    Examples:\n        &gt;&gt;&gt; fromspotPy.utils.init import check_and_create_dir\n        &gt;&gt;&gt; check_and_create_dir(\"data/\")\n    \"\"\"\n    if not isinstance(path, str):\n        raise Exception(\"path must be a string\")\n    if not os.path.exists(path):\n        os.makedirs(path)\n</code></pre>"},{"location":"reference/spotPython/utils/init/#spotPython.utils.init.fun_control_init","title":"<code>fun_control_init(task='classification', _L_in=None, _L_out=None, enable_progress_bar=False, spot_tensorboard_path=None, TENSORBOARD_CLEAN=False, num_workers=0, device=None, seed=1234, sigma=0.0)</code>","text":"<p>Initialize fun_control dictionary. Args:     task (str):         The task to perform. It can be either \u201cclassification\u201d or \u201cregression\u201d.     _L_in (int):         The number of input features.     _L_out (int):         The number of output features.     enable_progress_bar (bool):         Whether to enable the progress bar or not.     spot_tensorboard_path (str):         The path to the folder where the spot tensorboard files are saved.         If None, no spot tensorboard files are saved.     num_workers (int):         The number of workers to use for the data loading.     device (str):         The device to use for the training. It can be either \u201ccpu\u201d, \u201cmps\u201d, or \u201ccuda\u201d. Returns:     fun_control (dict):         A dictionary containing the information about the core model,         loss function, metrics, and the hyperparameters. Examples:     &gt;&gt;&gt; fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)     &gt;&gt;&gt; fun_control     &gt;&gt;&gt; {\u2018CHECKPOINT_PATH\u2019: \u2018saved_models/\u2019,             \u2018DATASET_PATH\u2019: \u2018data/\u2019,             \u2018RESULTS_PATH\u2019: \u2018results/\u2019,             \u2018TENSORBOARD_PATH\u2019: \u2018runs/\u2019,             \u2018_L_in\u2019: 64,             \u2018_L_out\u2019: 11,             \u2018data\u2019: None,             \u2018data_dir\u2019: \u2018./data\u2019,             \u2018device\u2019: None,             \u2018enable_progress_bar\u2019: False,             \u2018eval\u2019: None,             \u2018k_folds\u2019: None,             \u2018loss_function\u2019: None,             \u2018metric_river\u2019: None,             \u2018metric_sklearn\u2019: None,             \u2018metric_torch\u2019: None,             \u2018metric_params\u2019: {},             \u2018model_dict\u2019: {},             \u2018n_samples\u2019: None,             \u2018num_workers\u2019: 0,             \u2018optimizer\u2019: None,             \u2018path\u2019: None,             \u2018prep_model\u2019: None,             \u2018save_model\u2019: False,             \u2018seed\u2019: 1234,             \u2018show_batch_interval\u2019: 1000000,             \u2018shuffle\u2019: None,             \u2018sigma\u2019: 0.0,             \u2018target_column\u2019: None,             \u2018train\u2019: None,             \u2018test\u2019: None,             \u2018task\u2019: \u2018classification\u2019,             \u2018tensorboard_path\u2019: None,             \u2018weights\u2019: 1.0,             \u2018writer\u2019: None}</p> Source code in <code>spotPython/utils/init.py</code> <pre><code>def fun_control_init(\n    task=\"classification\",\n    _L_in=None,\n    _L_out=None,\n    enable_progress_bar=False,\n    spot_tensorboard_path=None,\n    TENSORBOARD_CLEAN=False,\n    num_workers=0,\n    device=None,\n    seed=1234,\n    sigma=0.0,\n):\n    \"\"\"Initialize fun_control dictionary.\n    Args:\n        task (str):\n            The task to perform. It can be either \"classification\" or \"regression\".\n        _L_in (int):\n            The number of input features.\n        _L_out (int):\n            The number of output features.\n        enable_progress_bar (bool):\n            Whether to enable the progress bar or not.\n        spot_tensorboard_path (str):\n            The path to the folder where the spot tensorboard files are saved.\n            If None, no spot tensorboard files are saved.\n        num_workers (int):\n            The number of workers to use for the data loading.\n        device (str):\n            The device to use for the training. It can be either \"cpu\", \"mps\", or \"cuda\".\n    Returns:\n        fun_control (dict):\n            A dictionary containing the information about the core model,\n            loss function, metrics, and the hyperparameters.\n    Examples:\n        &gt;&gt;&gt; fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n        &gt;&gt;&gt; fun_control\n        &gt;&gt;&gt; {'CHECKPOINT_PATH': 'saved_models/',\n                'DATASET_PATH': 'data/',\n                'RESULTS_PATH': 'results/',\n                'TENSORBOARD_PATH': 'runs/',\n                '_L_in': 64,\n                '_L_out': 11,\n                'data': None,\n                'data_dir': './data',\n                'device': None,\n                'enable_progress_bar': False,\n                'eval': None,\n                'k_folds': None,\n                'loss_function': None,\n                'metric_river': None,\n                'metric_sklearn': None,\n                'metric_torch': None,\n                'metric_params': {},\n                'model_dict': {},\n                'n_samples': None,\n                'num_workers': 0,\n                'optimizer': None,\n                'path': None,\n                'prep_model': None,\n                'save_model': False,\n                'seed': 1234,\n                'show_batch_interval': 1000000,\n                'shuffle': None,\n                'sigma': 0.0,\n                'target_column': None,\n                'train': None,\n                'test': None,\n                'task': 'classification',\n                'tensorboard_path': None,\n                'weights': 1.0,\n                'writer': None}\n    \"\"\"\n    # Setting the seed\n    L.seed_everything(42)\n\n    # Path to the folder where the pretrained models are saved\n    CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"runs/saved_models/\")\n    os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n    # Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n    DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n    os.makedirs(DATASET_PATH, exist_ok=True)\n    # Path to the folder where the results (plots, csv, etc.) are saved\n    RESULTS_PATH = os.environ.get(\"PATH_RESULTS\", \"results/\")\n    os.makedirs(RESULTS_PATH, exist_ok=True)\n    # Path to the folder where the tensorboard files are saved\n    TENSORBOARD_PATH = os.environ.get(\"PATH_TENSORBOARD\", \"runs/\")\n    if TENSORBOARD_CLEAN:\n        # if the folder \"runs\"  exists, move it to \"runs_Y_M_D_H_M_S\" to avoid overwriting old tensorboard files\n        if os.path.exists(TENSORBOARD_PATH):\n            now = datetime.datetime.now()\n            os.makedirs(\"runs_OLD\", exist_ok=True)\n            # use [:-1] to remove \"/\" from the end of the path\n            TENSORBOARD_PATH_OLD = \"runs_OLD/\" + TENSORBOARD_PATH[:-1] + \"_\" + now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n            os.rename(TENSORBOARD_PATH[:-1], TENSORBOARD_PATH_OLD)\n    os.makedirs(TENSORBOARD_PATH, exist_ok=True)\n    if spot_tensorboard_path is not None:\n        os.makedirs(spot_tensorboard_path, exist_ok=True)\n        spot_writer = SummaryWriter(spot_tensorboard_path)\n    else:\n        spot_writer = None\n\n    if not os.path.exists(\"./figures\"):\n        os.makedirs(\"./figures\")\n\n    fun_control = {\n        \"CHECKPOINT_PATH\": CHECKPOINT_PATH,\n        \"DATASET_PATH\": DATASET_PATH,\n        \"RESULTS_PATH\": RESULTS_PATH,\n        \"TENSORBOARD_PATH\": TENSORBOARD_PATH,\n        \"_L_in\": _L_in,\n        \"_L_out\": _L_out,\n        \"data\": None,\n        \"data_dir\": \"./data\",\n        \"device\": device,\n        \"enable_progress_bar\": enable_progress_bar,\n        \"eval\": None,\n        \"k_folds\": 3,\n        \"loss_function\": None,\n        \"metric_river\": None,\n        \"metric_sklearn\": None,\n        \"metric_torch\": None,\n        \"metric_params\": {},\n        \"model_dict\": {},\n        \"n_samples\": None,\n        \"num_workers\": num_workers,\n        \"optimizer\": None,\n        \"path\": None,\n        \"prep_model\": None,\n        \"save_model\": False,\n        \"seed\": seed,\n        \"show_batch_interval\": 1_000_000,\n        \"shuffle\": None,\n        \"sigma\": sigma,\n        \"target_column\": None,\n        \"train\": None,\n        \"test\": None,\n        \"task\": task,\n        \"spot_tensorboard_path\": spot_tensorboard_path,\n        \"weights\": 1.0,\n        \"spot_writer\": spot_writer,\n    }\n    return fun_control\n</code></pre>"},{"location":"reference/spotPython/utils/metrics/","title":"metrics","text":""},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.apk","title":"<code>apk(actual, predicted, k=10)</code>","text":"<p>Computes the average precision at k. This function computes the average precision at k between two lists of items. Parameters</p> <p>actual : list          A list of elements that are to be predicted (order doesn\u2019t matter) predicted : list             A list of predicted elements (order does matter) k : int, optional     The maximum number of predicted elements Returns</p> <p>score : double         The average precision at k over the input lists</p> Source code in <code>spotPython/utils/metrics.py</code> <pre><code>def apk(actual, predicted, k=10):\n    \"\"\"\n    Computes the average precision at k.\n    This function computes the average precision at k between two lists of\n    items.\n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n    \"\"\"\n    if len(predicted) &gt; k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i, p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i + 1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n</code></pre>"},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.mapk","title":"<code>mapk(actual, predicted, k=10)</code>","text":"<p>Computes the mean average precision at k. This function computes the mean average precision at k between two lists of lists of items. Parameters</p> <p>actual : list          A list of lists of elements that are to be predicted          (order doesn\u2019t matter in the lists) predicted : list             A list of lists of predicted elements             (order matters in the lists) k : int, optional     The maximum number of predicted elements Returns</p> <p>score : double         The mean average precision at k over the input lists</p> Source code in <code>spotPython/utils/metrics.py</code> <pre><code>def mapk(actual, predicted, k=10):\n    \"\"\"\n    Computes the mean average precision at k.\n    This function computes the mean average precision at k between two lists\n    of lists of items.\n    Parameters\n    ----------\n    actual : list\n             A list of lists of elements that are to be predicted\n             (order doesn't matter in the lists)\n    predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n    \"\"\"\n    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n</code></pre>"},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.mapk_score","title":"<code>mapk_score(y_true, y_pred, k=3)</code>","text":"<p>Wrapper for mapk func using numpy arrays  Args:         y_true (np.array): array of true values         y_pred (np.array): array of predicted values         k (int): number of predictions Returns:         score (float): mean average precision at k Examples:         &gt;&gt;&gt; y_true = np.array([0, 1, 2, 2])         &gt;&gt;&gt; y_pred = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2                  [0.3, 0.4, 0.2],  # 1 is in top 2                  [0.2, 0.4, 0.3],  # 2 is in top 2                  [0.7, 0.2, 0.1]]) # 2 isn\u2019t in top 2         &gt;&gt;&gt; mapk_score(y_true, y_pred, k=1)         0.25         &gt;&gt;&gt; mapk_score(y_true, y_pred, k=2)         0.375         &gt;&gt;&gt; mapk_score(y_true, y_pred, k=3)         0.4583333333333333         &gt;&gt;&gt; mapk_score(y_true, y_pred, k=4)         0.4583333333333333</p> Source code in <code>spotPython/utils/metrics.py</code> <pre><code>def mapk_score(y_true, y_pred, k=3):\n    \"\"\"Wrapper for mapk func using numpy arrays\n     Args:\n            y_true (np.array): array of true values\n            y_pred (np.array): array of predicted values\n            k (int): number of predictions\n    Returns:\n            score (float): mean average precision at k\n    Examples:\n            &gt;&gt;&gt; y_true = np.array([0, 1, 2, 2])\n            &gt;&gt;&gt; y_pred = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n                     [0.3, 0.4, 0.2],  # 1 is in top 2\n                     [0.2, 0.4, 0.3],  # 2 is in top 2\n                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=1)\n            0.25\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=2)\n            0.375\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=3)\n            0.4583333333333333\n            &gt;&gt;&gt; mapk_score(y_true, y_pred, k=4)\n            0.4583333333333333\n    \"\"\"\n    y_true = series_to_array(y_true)\n    sorted_prediction_ids = np.argsort(-y_pred, axis=1)\n    top_k_prediction_ids = sorted_prediction_ids[:, :k]\n    score = mapk(y_true.reshape(-1, 1), top_k_prediction_ids, k=k)\n    return score\n</code></pre>"},{"location":"reference/spotPython/utils/metrics/#spotPython.utils.metrics.mapk_scorer","title":"<code>mapk_scorer(estimator, X, y)</code>","text":"<p>Scorer for mean average precision at k. This function computes the mean average precision at k between two lists of lists of items. Parameters</p> <p>estimator : sklearn estimator             The estimator to be used for prediction. X : array-like of shape (n_samples, n_features)     The input samples. y : array-like of shape (n_samples,)     The target values. Returns</p> <p>score : double         The mean average precision at k over the input lists</p> Source code in <code>spotPython/utils/metrics.py</code> <pre><code>def mapk_scorer(estimator, X, y):\n    \"\"\"\n    Scorer for mean average precision at k.\n    This function computes the mean average precision at k between two lists\n    of lists of items.\n    Parameters\n    ----------\n    estimator : sklearn estimator\n                The estimator to be used for prediction.\n    X : array-like of shape (n_samples, n_features)\n        The input samples.\n    y : array-like of shape (n_samples,)\n        The target values.\n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n    \"\"\"\n    y_pred = estimator.predict_proba(X)\n    score = mapk_score(y, y_pred, k=3)\n    return score\n</code></pre>"},{"location":"reference/spotPython/utils/progress/","title":"progress","text":""},{"location":"reference/spotPython/utils/progress/#spotPython.utils.progress.progress_bar","title":"<code>progress_bar(progress, bar_length=10, message='spotPython tuning:', y=None)</code>","text":"<p>Displays or updates a console progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>progress</code> <code>float</code> <p>a float between 0 and 1. Any int will be converted to a float. A value under 0 represents a halt. A value at 1 or bigger represents 100%.</p> required <code>bar_length</code> <code>int</code> <p>length of the progress bar</p> <code>10</code> <code>message</code> <code>str</code> <p>message text to display</p> <code>'spotPython tuning:'</code> Source code in <code>spotPython/utils/progress.py</code> <pre><code>def progress_bar(progress: float, bar_length: int = 10, message: str = \"spotPython tuning:\", y=None) -&gt; None:\n    \"\"\"\n    Displays or updates a console progress bar.\n\n    Args:\n        progress (float): a float between 0 and 1. Any int will be converted to a float.\n            A value under 0 represents a halt.\n            A value at 1 or bigger represents 100%.\n        bar_length (int): length of the progress bar\n        message (str): message text to display\n    \"\"\"\n    status = \"\"\n    if y is not None:\n        message = f\"{message} {y}\"\n    if progress &lt; 0:\n        progress = 0\n        status = \"Halt...\\r\\n\"\n    elif progress &gt;= 1:\n        progress = 1\n        status = \"Done...\\r\\n\"\n    block = int(round(bar_length * progress))\n    text = f\"{message} [{'#' * block + '-' * (bar_length - block)}] {progress * 100:.2f}% {status}\\r\\n\"\n    stdout.write(text)\n    stdout.flush()\n</code></pre>"},{"location":"reference/spotPython/utils/repair/","title":"repair","text":""},{"location":"reference/spotPython/utils/repair/#spotPython.utils.repair.remove_nan","title":"<code>remove_nan(X, y)</code>","text":"<p>Remove rows from X and y where y contains NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array</p> required <code>y</code> <code>ndarray</code> <p>y array</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[numpy.ndarray, numpy.ndarray]: X and y arrays with rows containing NaN values in y removed</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; y = np.array([1, np.nan, 2])\n&gt;&gt;&gt; remove_nan(X, y)\n(array([[1, 2],\n        [5, 6]]), array([1., 2.]))\n</code></pre> Source code in <code>spotPython/utils/repair.py</code> <pre><code>def remove_nan(X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Remove rows from X and y where y contains NaN values.\n\n    Args:\n        X (numpy.ndarray): X array\n        y (numpy.ndarray): y array\n\n    Returns:\n        Tuple[numpy.ndarray, numpy.ndarray]: X and y arrays with rows containing NaN values in y removed\n\n    Examples:\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; y = np.array([1, np.nan, 2])\n        &gt;&gt;&gt; remove_nan(X, y)\n        (array([[1, 2],\n                [5, 6]]), array([1., 2.]))\n    \"\"\"\n    ind = np.isfinite(y)\n    y = y[ind]\n    X = X[ind, :]\n    return X, y\n</code></pre>"},{"location":"reference/spotPython/utils/repair/#spotPython.utils.repair.repair_non_numeric","title":"<code>repair_non_numeric(X, var_type)</code>","text":"<p>Round non-numeric values to integers. This applies to all variables except for \u201cnum\u201d and \u201cfloat\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>X array</p> required <code>var_type</code> <code>list</code> <p>list with type information</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: X array with non-numeric values rounded to integers</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.array([[1.2, 2.3], [3.4, 4.5]])\n&gt;&gt;&gt; var_type = [\"num\", \"factor\"]\n&gt;&gt;&gt; repair_non_numeric(X, var_type)\narray([[1., 2.],\n       [3., 4.]])\n</code></pre> Source code in <code>spotPython/utils/repair.py</code> <pre><code>def repair_non_numeric(X: np.ndarray, var_type: List[str]) -&gt; np.ndarray:\n    \"\"\"\n    Round non-numeric values to integers.\n    This applies to all variables except for \"num\" and \"float\".\n\n    Args:\n        X (numpy.ndarray): X array\n        var_type (list): list with type information\n\n    Returns:\n        numpy.ndarray: X array with non-numeric values rounded to integers\n\n    Examples:\n        &gt;&gt;&gt; X = np.array([[1.2, 2.3], [3.4, 4.5]])\n        &gt;&gt;&gt; var_type = [\"num\", \"factor\"]\n        &gt;&gt;&gt; repair_non_numeric(X, var_type)\n        array([[1., 2.],\n               [3., 4.]])\n    \"\"\"\n    mask = np.isin(var_type, [\"num\", \"float\"], invert=True)\n    X[:, mask] = np.around(X[:, mask])\n    return X\n</code></pre>"},{"location":"reference/spotPython/utils/tensorboard/","title":"tensorboard","text":""},{"location":"reference/spotPython/utils/tensorboard/#spotPython.utils.tensorboard.start_tensorboard","title":"<code>start_tensorboard()</code>","text":"<p>Starts a tensorboard server in the background.</p> <p>Returns:</p> Name Type Description <code>process</code> <code>Popen</code> <p>The process of the tensorboard server.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; process = start_tensorboard()\n</code></pre> Source code in <code>spotPython/utils/tensorboard.py</code> <pre><code>def start_tensorboard() -&gt; subprocess.Popen:\n    \"\"\"Starts a tensorboard server in the background.\n\n    Returns:\n        process: The process of the tensorboard server.\n\n    Examples:\n        &gt;&gt;&gt; process = start_tensorboard()\n\n    \"\"\"\n    cmd = [\"tensorboard\", \"--logdir=./runs\"]\n    process = subprocess.Popen(cmd)\n    return process\n</code></pre>"},{"location":"reference/spotPython/utils/tensorboard/#spotPython.utils.tensorboard.stop_tensorboard","title":"<code>stop_tensorboard(process)</code>","text":"<p>Stops a tensorboard server.</p> <p>Parameters:</p> Name Type Description Default <code>process</code> <p>The process of the tensorboard server.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; process = start_tensorboard()\n&gt;&gt;&gt; stop_tensorboard(process)\n</code></pre> Source code in <code>spotPython/utils/tensorboard.py</code> <pre><code>def stop_tensorboard(process) -&gt; None:\n    \"\"\"Stops a tensorboard server.\n\n    Args:\n        process: The process of the tensorboard server.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; process = start_tensorboard()\n        &gt;&gt;&gt; stop_tensorboard(process)\n    \"\"\"\n    process.terminate()\n</code></pre>"},{"location":"reference/spotPython/utils/transform/","title":"transform","text":""},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.scale","title":"<code>scale(X, lower, upper)</code>","text":"<p>Sample scaling from unit hypercube to different bounds. Converts a sample from <code>[0, 1)</code> to <code>[a, b)</code>. The following transformation is used: <code>(b - a) * X + a</code></p> Note <p>equal lower and upper bounds are feasible.</p> <p>Args:     X (array):         Sample to scale.     lower (array):         lower bound of transformed data.     upper (array):         upper bounds of transformed data.</p> <p>Returns:</p> Type Description <code>array</code> <p>Scaled sample.</p> <p>Examples:</p> <p>Transform three samples in the unit hypercube to (lower, upper) bounds:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.stats import qmc\n&gt;&gt;&gt; from spotPython.utils.transform import scale\n&gt;&gt;&gt; lower = np.array([6, 0])\n&gt;&gt;&gt; upper = np.array([6, 5])\n&gt;&gt;&gt; sample = np.array([[0.5 , 0.75],\n&gt;&gt;&gt;             [0.5 , 0.5],\n&gt;&gt;&gt;             [0.75, 0.25]])\n&gt;&gt;&gt; scale(sample, lower, upper)\n</code></pre> Source code in <code>spotPython/utils/transform.py</code> <pre><code>def scale(X: np.ndarray, lower: np.ndarray, upper: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Sample scaling from unit hypercube to different bounds. Converts a sample from `[0, 1)` to `[a, b)`.\n    The following transformation is used:\n    `(b - a) * X + a`\n\n    Note:\n        equal lower and upper bounds are feasible.\n    Args:\n        X (array):\n            Sample to scale.\n        lower (array):\n            lower bound of transformed data.\n        upper (array):\n            upper bounds of transformed data.\n\n    Returns:\n        (array):\n            Scaled sample.\n\n    Examples:\n        Transform three samples in the unit hypercube to (lower, upper) bounds:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from scipy.stats import qmc\n        &gt;&gt;&gt; from spotPython.utils.transform import scale\n        &gt;&gt;&gt; lower = np.array([6, 0])\n        &gt;&gt;&gt; upper = np.array([6, 5])\n        &gt;&gt;&gt; sample = np.array([[0.5 , 0.75],\n        &gt;&gt;&gt;             [0.5 , 0.5],\n        &gt;&gt;&gt;             [0.75, 0.25]])\n        &gt;&gt;&gt; scale(sample, lower, upper)\n\n    \"\"\"\n    # Checking that X is within (0,1) interval\n    if (X.max() &gt; 1.0) or (X.min() &lt; 0.0):\n        raise ValueError(\"Sample is not in unit hypercube\")\n    # Vectorized scaling operation\n    X = (upper - lower) * X + lower\n    # Handling case where lower == upper\n    X[:, lower == upper] = lower[lower == upper]\n    return X\n</code></pre>"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_hyper_parameter_values","title":"<code>transform_hyper_parameter_values(fun_control, hyper_parameter_values)</code>","text":"<p>Transform the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \u201cint\u201d, or \u201cfloat\u201d or \u201cnum\u201d. Let fun_control = {\u201ccore_model_hyper_dict\u201d:{ \u201cleaf_prediction\u201d: { \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d], \u201ctype\u201d: \u201cfactor\u201d, \u201cdefault\u201d: \u201cmean\u201d, \u201ccore_model_parameter_type\u201d: \u201cstr\u201d}, \u201cmax_depth\u201d: { \u201ctype\u201d: \u201cint\u201d, \u201cdefault\u201d: 20, \u201ctransform\u201d: \u201ctransform_power_2\u201d, \u201clower\u201d: 2, \u201cupper\u201d: 20}}} and v = {\u2018max_depth\u2019: 20,\u2019leaf_prediction\u2019: \u2018mean\u2019} and def transform_power_2(x): return 2**x. The function takes fun_control and v as input and returns a dictionary with the same structure as v. The function transforms the values of the hyperparameters according to the transform function specified in fun_control if the hyperparameter is of type \u201cint\u201d, or \u201cfloat\u201d or \u201cnum\u201d.</p> <p>For example, transform_hyper_parameter_values(fun_control, v) returns  {\u2018max_depth\u2019: 1048576, \u2018leaf_prediction\u2019: \u2018mean\u2019}. Args:     fun_control (dict):         A dictionary containing the information about the core model and the hyperparameters.     hyper_parameter_values (dict):         A dictionary containing the values of the hyperparameters. Returns:     (dict):         A dictionary containing the values of the hyperparameters. Examples:     &gt;&gt;&gt; import copy         from spotPython.utils.prepare import transform_hyper_parameter_values         fun_control = {         \u201ccore_model_hyper_dict\u201d: {             \u201cleaf_prediction\u201d: {                 \u201clevels\u201d: [\u201cmean\u201d, \u201cmodel\u201d, \u201cadaptive\u201d],                 \u201ctype\u201d: \u201cfactor\u201d,                 \u201cdefault\u201d: \u201cmean\u201d,                 \u201ccore_model_parameter_type\u201d: \u201cstr\u201d},             \u201cmax_depth\u201d: {\u201ctype\u201d: \u201cint\u201d,                           \u201cdefault\u201d: 20                           \u201ctransform\u201d: \u201ctransform_power_2\u201d,                           \u201clower\u201d: 2,                           \u201cupper\u201d: 20}}}         hyper_parameter_values = {\u2018max_depth\u2019: 20,                                   \u2018leaf_prediction\u2019: \u2018mean\u2019}         transform_hyper_parameter_values(fun_control, hyper_parameter_values)         {\u2018max_depth\u2019: 1048576,          \u2018leaf_prediction\u2019: \u2018mean\u2019}</p> Source code in <code>spotPython/utils/transform.py</code> <pre><code>def transform_hyper_parameter_values(fun_control, hyper_parameter_values):\n    \"\"\"\n    Transform the values of the hyperparameters according to the transform function specified in fun_control\n    if the hyperparameter is of type \"int\", or \"float\" or \"num\".\n    Let fun_control = {\"core_model_hyper_dict\":{ \"leaf_prediction\":\n    { \"levels\": [\"mean\", \"model\", \"adaptive\"], \"type\": \"factor\", \"default\": \"mean\", \"core_model_parameter_type\": \"str\"},\n    \"max_depth\": { \"type\": \"int\", \"default\": 20, \"transform\": \"transform_power_2\", \"lower\": 2, \"upper\": 20}}}\n    and v = {'max_depth': 20,'leaf_prediction': 'mean'} and def transform_power_2(x): return 2**x.\n    The function takes fun_control and v as input and returns a dictionary with the same structure as v.\n    The function transforms the values of the hyperparameters according to the transform function\n    specified in fun_control if the hyperparameter is of type \"int\", or \"float\" or \"num\".\n\n    For example, transform_hyper_parameter_values(fun_control, v) returns\n     {'max_depth': 1048576, 'leaf_prediction': 'mean'}.\n    Args:\n        fun_control (dict):\n            A dictionary containing the information about the core model and the hyperparameters.\n        hyper_parameter_values (dict):\n            A dictionary containing the values of the hyperparameters.\n    Returns:\n        (dict):\n            A dictionary containing the values of the hyperparameters.\n    Examples:\n        &gt;&gt;&gt; import copy\n            from spotPython.utils.prepare import transform_hyper_parameter_values\n            fun_control = {\n            \"core_model_hyper_dict\": {\n                \"leaf_prediction\": {\n                    \"levels\": [\"mean\", \"model\", \"adaptive\"],\n                    \"type\": \"factor\",\n                    \"default\": \"mean\",\n                    \"core_model_parameter_type\": \"str\"},\n                \"max_depth\": {\"type\": \"int\",\n                              \"default\": 20\n                              \"transform\": \"transform_power_2\",\n                              \"lower\": 2,\n                              \"upper\": 20}}}\n            hyper_parameter_values = {'max_depth': 20,\n                                      'leaf_prediction': 'mean'}\n            transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n            {'max_depth': 1048576,\n             'leaf_prediction': 'mean'}\n    \"\"\"\n    hyper_parameter_values = copy.deepcopy(hyper_parameter_values)\n    for key, value in hyper_parameter_values.items():\n        if (\n            fun_control[\"core_model_hyper_dict\"][key][\"type\"] in [\"int\", \"float\", \"num\", \"factor\"]\n            and fun_control[\"core_model_hyper_dict\"][key][\"transform\"] != \"None\"\n        ):\n            hyper_parameter_values[key] = eval(fun_control[\"core_model_hyper_dict\"][key][\"transform\"])(value)\n    return hyper_parameter_values\n</code></pre>"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_none_to_None","title":"<code>transform_none_to_None(x)</code>","text":"<p>Transformations for hyperparameters of type None. Args:     x (str): The string to transform. Returns:     (str): The transformed string. Examples:     &gt;&gt;&gt; from spotPython.utils.transform import transform_none_to_None     &gt;&gt;&gt; transform_none_to_None(\u201cnone\u201d)     None Note:     Needed for sklearn.linear_model.LogisticRegression</p> Source code in <code>spotPython/utils/transform.py</code> <pre><code>def transform_none_to_None(x):\n    \"\"\"\n    Transformations for hyperparameters of type None.\n    Args:\n        x (str): The string to transform.\n    Returns:\n        (str): The transformed string.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.transform import transform_none_to_None\n        &gt;&gt;&gt; transform_none_to_None(\"none\")\n        None\n    Note:\n        Needed for sklearn.linear_model.LogisticRegression\n    \"\"\"\n    if x == \"none\":\n        return None\n    else:\n        return x\n</code></pre>"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_power","title":"<code>transform_power(base, x, as_int=False)</code>","text":"<p>Raises a given base to the power of x.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>int</code> <p>The base to raise to the power of x.</p> required <code>x</code> <code>int</code> <p>The exponent.</p> required <code>as_int</code> <code>bool</code> <p>If True, returns the result as an integer.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The result of raising the base to the power of x.</p> <p>Examples:     &gt;&gt;&gt; from spotPython.utils.transform import transform_power     &gt;&gt;&gt; transform_power(2, 3)     8</p> Source code in <code>spotPython/utils/transform.py</code> <pre><code>def transform_power(base: int, x: int, as_int: bool = False) -&gt; float:\n    \"\"\"\n    Raises a given base to the power of x.\n\n    Args:\n        base (int):\n            The base to raise to the power of x.\n        x (int):\n            The exponent.\n        as_int (bool):\n            If True, returns the result as an integer.\n\n    Returns:\n        (float):\n            The result of raising the base to the power of x.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.transform import transform_power\n        &gt;&gt;&gt; transform_power(2, 3)\n        8\n    \"\"\"\n    result = base**x\n    if as_int:\n        result = int(result)\n    return result\n</code></pre>"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_power_10","title":"<code>transform_power_10(x)</code>","text":"<p>Transformations for hyperparameters of type float. Args:     x (float): The exponent. Returns:     (float): The result of raising 10 to the power of x. Examples:     &gt;&gt;&gt; from spotPython.utils.transform import transform_power_10     &gt;&gt;&gt; transform_power_10(3)     1000</p> Source code in <code>spotPython/utils/transform.py</code> <pre><code>def transform_power_10(x):\n    \"\"\"Transformations for hyperparameters of type float.\n    Args:\n        x (float): The exponent.\n    Returns:\n        (float): The result of raising 10 to the power of x.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.transform import transform_power_10\n        &gt;&gt;&gt; transform_power_10(3)\n        1000\n    \"\"\"\n    return 10**x\n</code></pre>"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_power_10_int","title":"<code>transform_power_10_int(x)</code>","text":"<p>Transformations for hyperparameters of type int. Args:     x (int): The exponent. Returns:     (int): The result of raising 10 to the power of x. Examples:     &gt;&gt;&gt; from spotPython.utils.transform import transform_power_10_int     &gt;&gt;&gt; transform_power_10_int(3)     1000</p> Source code in <code>spotPython/utils/transform.py</code> <pre><code>def transform_power_10_int(x: int) -&gt; int:\n    \"\"\"Transformations for hyperparameters of type int.\n    Args:\n        x (int): The exponent.\n    Returns:\n        (int): The result of raising 10 to the power of x.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.transform import transform_power_10_int\n        &gt;&gt;&gt; transform_power_10_int(3)\n        1000\n    \"\"\"\n    return int(10**x)\n</code></pre>"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_power_2","title":"<code>transform_power_2(x)</code>","text":"<p>Transformations for hyperparameters of type float. Args:     x (float): The exponent. Returns:     (float): The result of raising 2 to the power of x. Examples:     &gt;&gt;&gt; from spotPython.utils.transform import transform_power_2     &gt;&gt;&gt; transform_power_2(3)     8</p> Source code in <code>spotPython/utils/transform.py</code> <pre><code>def transform_power_2(x):\n    \"\"\"Transformations for hyperparameters of type float.\n    Args:\n        x (float): The exponent.\n    Returns:\n        (float): The result of raising 2 to the power of x.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.transform import transform_power_2\n        &gt;&gt;&gt; transform_power_2(3)\n        8\n    \"\"\"\n    return 2**x\n</code></pre>"},{"location":"reference/spotPython/utils/transform/#spotPython.utils.transform.transform_power_2_int","title":"<code>transform_power_2_int(x)</code>","text":"<p>Transformations for hyperparameters of type int. Args:     x (int): The exponent. Returns:     (int): The result of raising 2 to the power of x. Examples:     &gt;&gt;&gt; from spotPython.utils.transform import transform_power_2_int     &gt;&gt;&gt; transform_power_2_int(3)     8</p> Source code in <code>spotPython/utils/transform.py</code> <pre><code>def transform_power_2_int(x: int) -&gt; int:\n    \"\"\"Transformations for hyperparameters of type int.\n    Args:\n        x (int): The exponent.\n    Returns:\n        (int): The result of raising 2 to the power of x.\n    Examples:\n        &gt;&gt;&gt; from spotPython.utils.transform import transform_power_2_int\n        &gt;&gt;&gt; transform_power_2_int(3)\n        8\n    \"\"\"\n    return int(2**x)\n</code></pre>"}]}