<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hyperparameter-tuning Cookbook - 14&nbsp; Hyperparameter Tuning for PyTorch With spotPython: Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./25_spot_torch_vbdp.html" rel="next">
<link href="./14_spot_ray_hpt_torch_cifar10.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script type="text/javascript">
window.PlotlyConfig = {MathJaxConfig: 'local'};
if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
if (typeof require !== 'undefined') {
require.undef("plotly");
requirejs.config({
    paths: {
        'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']
    }
});
require(['plotly'], function(Plotly) {
    window._Plotly = Plotly;
});
}
</script>


  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="twitter:title" content="Hyperparameter-tuning Cookbook - 14&nbsp; Hyperparameter Tuning for PyTorch With spotPython: Regression">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="./figures/tensorboard_0.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./24_spot_torch_regression.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning for PyTorch With `spotPython`: Regression</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hyperparameter-tuning Cookbook</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sequential-parameter-optimization/spotPython" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Hyperparameter-tuning-Cookbook.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_spot_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Hyperparameter Tuning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_spot_multidim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multi-dimensional Functions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_spot_anisotropic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Isotropic and Anisotropic Kriging</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_spot_sklearn_surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Using <code>sklearn</code> Surrogates in <code>spotPython</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_spot_sklearn_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Using <code>scipy</code> Optimizers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_spot_gaussian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Sequential Parameter Optimization: Gaussian Process Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_spot_ei.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Expected Improvement</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_spot_noisy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning and Noise</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_spot_ocba.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Handling Noise: Optimal Computational Budget Allocation in <code>Spot</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_spot_hpt_sklearn_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning: sklearn</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_spot_hpt_torch_fashion_mnist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning: PyTorch With fashionMNIST Data Using Hold-out Data Sets</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_spot_hpt_torch_cifar10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning: PyTorch wth cifar10 Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_spot_ray_hpt_torch_cifar10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning for PyTorch With <code>spotPython</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24_spot_torch_regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning for PyTorch With <code>spotPython</code>: Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25_spot_torch_vbdp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning: VBDP</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99_spot_doc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Documentation of the Sequential Parameter Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-setup-24" id="toc-sec-setup-24" class="nav-link active" data-scroll-target="#sec-setup-24"><span class="header-section-number">14.1</span> Setup</a></li>
  <li><a href="#initialization-of-the-fun_control-dictionary" id="toc-initialization-of-the-fun_control-dictionary" class="nav-link" data-scroll-target="#initialization-of-the-fun_control-dictionary"><span class="header-section-number">14.2</span> Initialization of the <code>fun_control</code> Dictionary</a></li>
  <li><a href="#sec-data-loading-24" id="toc-sec-data-loading-24" class="nav-link" data-scroll-target="#sec-data-loading-24"><span class="header-section-number">14.3</span> PyTorch Data Loading</a></li>
  <li><a href="#sec-specification-of-preprocessing-model-24" id="toc-sec-specification-of-preprocessing-model-24" class="nav-link" data-scroll-target="#sec-specification-of-preprocessing-model-24"><span class="header-section-number">14.4</span> Specification of the Preprocessing Model</a></li>
  <li><a href="#sec-selection-of-the-algorithm-24" id="toc-sec-selection-of-the-algorithm-24" class="nav-link" data-scroll-target="#sec-selection-of-the-algorithm-24"><span class="header-section-number">14.5</span> Select <code>algorithm</code> and <code>core_model_hyper_dict</code></a>
  <ul class="collapse">
  <li><a href="#sec-implementation-with-spotpython-24" id="toc-sec-implementation-with-spotpython-24" class="nav-link" data-scroll-target="#sec-implementation-with-spotpython-24"><span class="header-section-number">14.5.1</span> Implementing a Configurable Neural Network With spotPython</a></li>
  </ul></li>
  <li><a href="#sec-search-space-24" id="toc-sec-search-space-24" class="nav-link" data-scroll-target="#sec-search-space-24"><span class="header-section-number">14.6</span> The Search Space</a>
  <ul class="collapse">
  <li><a href="#sec-configuring-the-search-space-with-spotpython-24" id="toc-sec-configuring-the-search-space-with-spotpython-24" class="nav-link" data-scroll-target="#sec-configuring-the-search-space-with-spotpython-24"><span class="header-section-number">14.6.1</span> Configuring the Search Space With spotPython</a></li>
  </ul></li>
  <li><a href="#sec-modification-of-hyperparameters-24" id="toc-sec-modification-of-hyperparameters-24" class="nav-link" data-scroll-target="#sec-modification-of-hyperparameters-24"><span class="header-section-number">14.7</span> Modifying the Hyperparameters</a>
  <ul class="collapse">
  <li><a href="#sec-modification-of-default-values-24" id="toc-sec-modification-of-default-values-24" class="nav-link" data-scroll-target="#sec-modification-of-default-values-24"><span class="header-section-number">14.7.1</span> Modify <code>hyper_dict</code> Hyperparameters for the Selected Algorithm aka <code>core_model</code></a></li>
  <li><a href="#modify-hyperparameters-of-type-numeric-and-integer-boolean" id="toc-modify-hyperparameters-of-type-numeric-and-integer-boolean" class="nav-link" data-scroll-target="#modify-hyperparameters-of-type-numeric-and-integer-boolean"><span class="header-section-number">14.7.2</span> Modify Hyperparameters of Type numeric and integer (boolean)</a></li>
  <li><a href="#modify-hyperparameter-of-type-factor" id="toc-modify-hyperparameter-of-type-factor" class="nav-link" data-scroll-target="#modify-hyperparameter-of-type-factor"><span class="header-section-number">14.7.3</span> Modify Hyperparameter of Type factor</a></li>
  <li><a href="#sec-optimizers-24" id="toc-sec-optimizers-24" class="nav-link" data-scroll-target="#sec-optimizers-24"><span class="header-section-number">14.7.4</span> Optimizers</a></li>
  </ul></li>
  <li><a href="#sec-selection-of-target-function-24" id="toc-sec-selection-of-target-function-24" class="nav-link" data-scroll-target="#sec-selection-of-target-function-24"><span class="header-section-number">14.8</span> Evaluation</a>
  <ul class="collapse">
  <li><a href="#hold-out-data-split-and-cross-validation" id="toc-hold-out-data-split-and-cross-validation" class="nav-link" data-scroll-target="#hold-out-data-split-and-cross-validation"><span class="header-section-number">14.8.1</span> Hold-out Data Split and Cross-Validation</a></li>
  <li><a href="#sec-loss-functions-and-metrics-24" id="toc-sec-loss-functions-and-metrics-24" class="nav-link" data-scroll-target="#sec-loss-functions-and-metrics-24"><span class="header-section-number">14.8.2</span> Loss Functions and Metrics</a></li>
  </ul></li>
  <li><a href="#sec-call-the-hyperparameter-tuner-24" id="toc-sec-call-the-hyperparameter-tuner-24" class="nav-link" data-scroll-target="#sec-call-the-hyperparameter-tuner-24"><span class="header-section-number">14.9</span> Calling the SPOT Function</a></li>
  <li><a href="#sec-tensorboard-24" id="toc-sec-tensorboard-24" class="nav-link" data-scroll-target="#sec-tensorboard-24"><span class="header-section-number">14.10</span> Tensorboard</a>
  <ul class="collapse">
  <li><a href="#tensorboard-start-tensorboard" id="toc-tensorboard-start-tensorboard" class="nav-link" data-scroll-target="#tensorboard-start-tensorboard"><span class="header-section-number">14.10.1</span> Tensorboard: Start Tensorboard</a></li>
  </ul></li>
  <li><a href="#sec-results-tuning-24" id="toc-sec-results-tuning-24" class="nav-link" data-scroll-target="#sec-results-tuning-24"><span class="header-section-number">14.11</span> Results</a></li>
  <li><a href="#sec-get-spot-results-24" id="toc-sec-get-spot-results-24" class="nav-link" data-scroll-target="#sec-get-spot-results-24"><span class="header-section-number">14.12</span> Get the Tuned Architecture</a></li>
  <li><a href="#evaluation-of-the-tuned-architecture" id="toc-evaluation-of-the-tuned-architecture" class="nav-link" data-scroll-target="#evaluation-of-the-tuned-architecture"><span class="header-section-number">14.13</span> Evaluation of the Tuned Architecture</a></li>
  <li><a href="#cross-validated-evaluations" id="toc-cross-validated-evaluations" class="nav-link" data-scroll-target="#cross-validated-evaluations"><span class="header-section-number">14.14</span> Cross-validated Evaluations</a></li>
  <li><a href="#detailed-hyperparameter-plots" id="toc-detailed-hyperparameter-plots" class="nav-link" data-scroll-target="#detailed-hyperparameter-plots"><span class="header-section-number">14.15</span> Detailed Hyperparameter Plots</a></li>
  <li><a href="#sec-summary-24" id="toc-sec-summary-24" class="nav-link" data-scroll-target="#sec-summary-24"><span class="header-section-number">14.16</span> Summary and Outlook</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-hyperparameter-tuning-for-pytorch-24" class="quarto-section-identifier"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning for PyTorch With <code>spotPython</code>: Regression</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this tutorial, we will show how <code>spotPython</code> can be integrated into the <code>PyTorch</code> training workflow.</p>
<p>This document refers to the following software versions:</p>
<ul>
<li><code>python</code>: 3.10.10</li>
<li><code>torch</code>: 2.0.1</li>
<li><code>torchvision</code>: 0.15.0</li>
<li><code>spotPython</code>: 0.2.29</li>
</ul>
<p><code>spotPython</code> can be installed via pip. Alternatively, the source code can be downloaded from gitHub: <a href="https://github.com/sequential-parameter-optimization/spotPython">https://github.com/sequential-parameter-optimization/spotPython</a>.</p>
<pre class="{raw}"><code>!pip install spotPython</code></pre>
<section id="sec-setup-24" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="sec-setup-24"><span class="header-section-number">14.1</span> Setup</h2>
<p>Before we consider the detailed experimental setup, we select the parameters that affect run time, initial design size and the device that is used.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>MAX_TIME <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>INIT_SIZE <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>DEVICE <span class="op">=</span> <span class="va">None</span> <span class="co"># "cpu" # "cuda:0"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.utils.device <span class="im">import</span> getDevice</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>DEVICE <span class="op">=</span> getDevice(DEVICE)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(DEVICE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>mps</code></pre>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> socket</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dateutil.tz <span class="im">import</span> tzlocal</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> datetime.now(tzlocal())</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>HOSTNAME <span class="op">=</span> socket.gethostname().split(<span class="st">"."</span>)[<span class="dv">0</span>]</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>experiment_name <span class="op">=</span> <span class="st">'24-torch'</span> <span class="op">+</span> <span class="st">"_"</span> <span class="op">+</span> HOSTNAME <span class="op">+</span> <span class="st">"_"</span> <span class="op">+</span> <span class="bu">str</span>(MAX_TIME) <span class="op">+</span> <span class="st">"min_"</span> <span class="op">+</span> <span class="bu">str</span>(INIT_SIZE) <span class="op">+</span> <span class="st">"init_"</span> <span class="op">+</span> <span class="bu">str</span>(start_time).split(<span class="st">"."</span>, <span class="dv">1</span>)[<span class="dv">0</span>].replace(<span class="st">' '</span>, <span class="st">'_'</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>experiment_name <span class="op">=</span> experiment_name.replace(<span class="st">':'</span>, <span class="st">'-'</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(experiment_name)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(<span class="st">'./figures'</span>):</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    os.makedirs(<span class="st">'./figures'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>24-torch_bartz09_1min_5init_2023-06-15_05-59-44</code></pre>
</div>
</div>
</section>
<section id="initialization-of-the-fun_control-dictionary" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="initialization-of-the-fun_control-dictionary"><span class="header-section-number">14.2</span> Initialization of the <code>fun_control</code> Dictionary</h2>
<p><code>spotPython</code> uses a Python dictionary for storing the information required for the hyperparameter tuning process. This dictionary is called <code>fun_control</code> and is initialized with the function <code>fun_control_init</code>. The function <code>fun_control_init</code> returns a skeleton dictionary. The dictionary is filled with the required information for the hyperparameter tuning process. It stores the hyperparameter tuning settings, e.g., the deep learning network architecture that should be tuned, the classification (or regression) problem, and the data that is used for the tuning. The dictionary is used as an input for the SPOT function.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.utils.init <span class="im">import</span> fun_control_init</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>fun_control <span class="op">=</span> fun_control_init(task<span class="op">=</span><span class="st">"regression"</span>,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a> tensorboard_path<span class="op">=</span><span class="st">"runs/24_spot_torch_regression"</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a> device<span class="op">=</span>DEVICE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-data-loading-24" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="sec-data-loading-24"><span class="header-section-number">14.3</span> PyTorch Data Loading</h2>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dataset</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets <span class="im">as</span> sklearn_datasets</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> sklearn_datasets.make_regression(</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">10</span>, noise<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">123</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize the data</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>X_scaler <span class="op">=</span> MinMaxScaler()</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> X_scaler.fit_transform(X)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>y_scaler <span class="op">=</span> MinMaxScaler()</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>y_scaled <span class="op">=</span> y_scaler.fit_transform(y)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># combine the features and target into a single dataframe named train_df</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.DataFrame(np.hstack((X_scaled, y_scaled)))</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>target_column <span class="op">=</span> <span class="st">"y"</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> train_df.shape[<span class="dv">0</span>]</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> train_df.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>train_df.columns <span class="op">=</span> [<span class="ss">f"x</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n_features<span class="op">+</span><span class="dv">1</span>)] <span class="op">+</span> [target_column]</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(train_df.drop(target_column,</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    axis<span class="op">=</span><span class="dv">1</span>), </span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    train_df[target_column],</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>trainset <span class="op">=</span> pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))))</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>testset <span class="op">=</span> pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))))</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>trainset.columns <span class="op">=</span> [<span class="ss">f"x</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n_features<span class="op">+</span><span class="dv">1</span>)] <span class="op">+</span> [target_column]</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>testset.columns <span class="op">=</span> [<span class="ss">f"x</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n_features<span class="op">+</span><span class="dv">1</span>)] <span class="op">+</span> [target_column]</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_df.shape)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(trainset.shape)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(testset.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(1000, 11)
(750, 11)
(250, 11)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.torch.dataframedataset <span class="im">import</span> DataFrameDataset</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>dtype_x <span class="op">=</span> torch.float32</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>dtype_y <span class="op">=</span> torch.float32</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> DataFrameDataset(train_df, target_column<span class="op">=</span>target_column,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    dtype_x<span class="op">=</span>dtype_x, dtype_y<span class="op">=</span>dtype_y)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> DataFrameDataset(trainset, target_column<span class="op">=</span>target_column,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    dtype_x<span class="op">=</span>dtype_x, dtype_y<span class="op">=</span>dtype_y)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> DataFrameDataset(testset, target_column<span class="op">=</span>target_column,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    dtype_x<span class="op">=</span>dtype_x, dtype_y<span class="op">=</span>dtype_y)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="bu">len</span>(train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Now we can test the data loading:</li>
</ul>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.torch.traintest <span class="im">import</span> create_train_val_data_loaders</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>trainloader, testloader <span class="op">=</span> create_train_val_data_loaders(train, <span class="dv">2</span>, <span class="va">True</span>, <span class="dv">0</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader, <span class="dv">0</span>):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    inputs, labels <span class="op">=</span> data</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(inputs.shape)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(labels.shape)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(inputs)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(labels)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([2, 10])
torch.Size([2])
tensor([[0.3163, 0.1814, 0.6331, 0.3031, 0.1927, 0.5964, 0.7035, 0.5347, 0.7078,
         0.4439],
        [0.4570, 0.6765, 0.6100, 0.3040, 0.3819, 0.6163, 0.4936, 0.8727, 0.4437,
         0.3648]])
tensor([0.2249, 0.6488])</code></pre>
</div>
</div>
<ul>
<li>Since this works fine, we can add the data loading to the <code>fun_control</code> dictionary:</li>
</ul>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add the dataset to the fun_control</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>fun_control.update({<span class="st">"data"</span>: train_df, <span class="co"># full dataset,</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>               <span class="st">"train"</span>: train,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>               <span class="st">"test"</span>: test,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>               <span class="st">"n_samples"</span>: n_samples,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>               <span class="st">"target_column"</span>: target_column,})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-specification-of-preprocessing-model-24" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="sec-specification-of-preprocessing-model-24"><span class="header-section-number">14.4</span> Specification of the Preprocessing Model</h2>
<p>After the training and test data are specified and added to the <code>fun_control</code> dictionary, <code>spotPython</code> allows the specification of a data preprocessing pipeline, e.g., for the scaling of the data or for the one-hot encoding of categorical variables. The preprocessing model is called <code>prep_model</code> (“preparation” or pre-processing) and includes steps that are not subject to the hyperparameter tuning process. The preprocessing model is specified in the <code>fun_control</code> dictionary. The preprocessing model can be implemented as a <code>sklearn</code> pipeline. The following code shows a typical preprocessing pipeline:</p>
<pre class="{raw}"><code>categorical_columns = ["cities", "colors"]
one_hot_encoder = OneHotEncoder(handle_unknown="ignore",
                                    sparse_output=False)
prep_model = ColumnTransformer(
        transformers=[
             ("categorical", one_hot_encoder, categorical_columns),
         ],
         remainder=StandardScaler(),
     )</code></pre>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>fun_control.update({<span class="st">"prep_model"</span>: <span class="va">None</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-selection-of-the-algorithm-24" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="sec-selection-of-the-algorithm-24"><span class="header-section-number">14.5</span> Select <code>algorithm</code> and <code>core_model_hyper_dict</code></h2>
<section id="sec-implementation-with-spotpython-24" class="level3" data-number="14.5.1">
<h3 data-number="14.5.1" class="anchored" data-anchor-id="sec-implementation-with-spotpython-24"><span class="header-section-number">14.5.1</span> Implementing a Configurable Neural Network With spotPython</h3>
<p><code>spotPython</code> includes the <code>Net_lin_reg</code> class which is implemented in the file <code>netregression.py</code>.</p>
<pre class="{raw}"><code>from torch import nn
import spotPython.torch.netcore as netcore


class Net_lin_reg(netcore.Net_Core):
    def __init__(
        self, _L_in, _L_out, l1, dropout_prob, lr_mult,
        batch_size, epochs, k_folds, patience, optimizer,
        sgd_momentum
    ):
        super(Net_lin_reg, self).__init__(
            lr_mult=lr_mult,
            batch_size=batch_size,
            epochs=epochs,
            k_folds=k_folds,
            patience=patience,
            optimizer=optimizer,
            sgd_momentum=sgd_momentum,
        )
        l2 = max(l1 // 2, 4)
        self.fc1 = nn.Linear(_L_in, l1)
        self.fc2 = nn.Linear(l1, l2)
        self.fc3 = nn.Linear(l2, _L_out)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)
        self.dropout1 = nn.Dropout(p=dropout_prob)
        self.dropout2 = nn.Dropout(p=dropout_prob / 2)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout1(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout2(x)
        x = self.fc3(x)
        return x
</code></pre>
<section id="the-net_core-class" class="level4" data-number="14.5.1.1">
<h4 data-number="14.5.1.1" class="anchored" data-anchor-id="the-net_core-class"><span class="header-section-number">14.5.1.1</span> The <code>Net_Core</code> class</h4>
<p><code>Net_lin_reg</code> inherits from the class <code>Net_Core</code> which is implemented in the file <code>netcore.py</code>. It implements the additional attributes that are common to all neural network models. The <code>Net_Core</code> class is implemented in the file <code>netcore.py</code>. It implements hyperparameters as attributes, that are not used by the <code>core_model</code>, e.g.:</p>
<ul>
<li>optimizer (<code>optimizer</code>),</li>
<li>learning rate (<code>lr</code>),</li>
<li>batch size (<code>batch_size</code>),</li>
<li>epochs (<code>epochs</code>),</li>
<li>k_folds (<code>k_folds</code>), and</li>
<li>early stopping criterion “patience” (<code>patience</code>).</li>
</ul>
<p>Users can add further attributes to the class. The class <code>Net_Core</code> is shown below.</p>
<pre class="{raw}"><code>from torch import nn


class Net_Core(nn.Module):
    def __init__(self, lr_mult, batch_size, epochs, k_folds, patience,
    optimizer, sgd_momentum):
        super(Net_Core, self).__init__()
        self.lr_mult = lr_mult
        self.batch_size = batch_size
        self.epochs = epochs
        self.k_folds = k_folds
        self.patience = patience
        self.optimizer = optimizer
        self.sgd_momentum = sgd_momentum</code></pre>
<p>We see that the class <code>Net_lin_reg</code> has additional attributes and does not inherit from <code>nn</code> directly. It adds an additional class, <code>Net_core</code>, that takes care of additional attributes that are common to all neural network models, e.g., the learning rate multiplier <code>lr_mult</code> or the batch size <code>batch_size</code>.</p>
<p><code>spotPython</code>’s <code>core_model</code> implements an instance of the <code>Net_lin_reg</code> class. In addition to the basic neural network model, the <code>core_model</code> can use these additional attributes. <code>spotPython</code> provides methods for handling these additional attributes to guarantee 100% compatibility with the <code>PyTorch</code> classes. The method <code>add_core_model_to_fun_control</code> adds the hyperparameters and additional attributes to the <code>fun_control</code> dictionary. The method is shown below.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.torch.netregression <span class="im">import</span> Net_lin_reg</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.data.torch_hyper_dict <span class="im">import</span> TorchHyperDict</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.hyperparameters.values <span class="im">import</span> add_core_model_to_fun_control</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>core_model <span class="op">=</span> Net_lin_reg</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>fun_control <span class="op">=</span> add_core_model_to_fun_control(core_model<span class="op">=</span>core_model,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                              fun_control<span class="op">=</span>fun_control,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                              hyper_dict<span class="op">=</span>TorchHyperDict,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>                              filename<span class="op">=</span><span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="sec-search-space-24" class="level2" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="sec-search-space-24"><span class="header-section-number">14.6</span> The Search Space</h2>
<section id="sec-configuring-the-search-space-with-spotpython-24" class="level3" data-number="14.6.1">
<h3 data-number="14.6.1" class="anchored" data-anchor-id="sec-configuring-the-search-space-with-spotpython-24"><span class="header-section-number">14.6.1</span> Configuring the Search Space With spotPython</h3>
<section id="the-hyper_dict-hyperparameters-for-the-selected-algorithm" class="level4" data-number="14.6.1.1">
<h4 data-number="14.6.1.1" class="anchored" data-anchor-id="the-hyper_dict-hyperparameters-for-the-selected-algorithm"><span class="header-section-number">14.6.1.1</span> The <code>hyper_dict</code> Hyperparameters for the Selected Algorithm</h4>
<p><code>spotPython</code> uses <code>JSON</code> files for the specification of the hyperparameters. Users can specify their individual <code>JSON</code> files, or they can use the <code>JSON</code> files provided by <code>spotPython</code>. The <code>JSON</code> file for the <code>core_model</code> is called <code>torch_hyper_dict.json</code>.</p>
<p><code>spotPython</code> can handle numerical, boolean, and categorical hyperparameters. They can be specified in the <code>JSON</code> file in a similar way as the numerical hyperparameters as shown below. Each entry in the <code>JSON</code> file represents one hyperparameter with the following structure: <code>type</code>, <code>default</code>, <code>transform</code>, <code>lower</code>, and <code>upper</code>.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="er">"factor_hyperparameter":</span> <span class="fu">{</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"levels"</span><span class="fu">:</span> <span class="ot">[</span><span class="st">"A"</span><span class="ot">,</span> <span class="st">"B"</span><span class="ot">,</span> <span class="st">"C"</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"factor"</span><span class="fu">,</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"default"</span><span class="fu">:</span> <span class="st">"B"</span><span class="fu">,</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"None"</span><span class="fu">,</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"core_model_parameter_type"</span><span class="fu">:</span> <span class="st">"str"</span><span class="fu">,</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"lower"</span><span class="fu">:</span> <span class="dv">0</span><span class="fu">,</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"upper"</span><span class="fu">:</span> <span class="dv">2</span><span class="fu">}</span><span class="er">,</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The corresponding entries for the <code>Net_lin_reg</code> class are shown below.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>    <span class="er">"Net_lin_reg":</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"_L_in"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"int"</span><span class="fu">,</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="dv">10</span><span class="fu">,</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"None"</span><span class="fu">,</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="dv">10</span><span class="fu">,</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="dv">10</span><span class="fu">},</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"_L_out"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"int"</span><span class="fu">,</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">,</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"None"</span><span class="fu">,</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">,</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">},</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"l1"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"int"</span><span class="fu">,</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="dv">3</span><span class="fu">,</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"transform_power_2_int"</span><span class="fu">,</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="dv">3</span><span class="fu">,</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="dv">8</span><span class="fu">},</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"dropout_prob"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"float"</span><span class="fu">,</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="fl">0.01</span><span class="fu">,</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"None"</span><span class="fu">,</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="fl">0.0</span><span class="fu">,</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="fl">0.9</span><span class="fu">},</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"lr_mult"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"float"</span><span class="fu">,</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="fl">1.0</span><span class="fu">,</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"None"</span><span class="fu">,</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="fl">0.1</span><span class="fu">,</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="fl">10.0</span><span class="fu">},</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"batch_size"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"int"</span><span class="fu">,</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="dv">4</span><span class="fu">,</span></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"transform_power_2_int"</span><span class="fu">,</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">,</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="dv">4</span><span class="fu">},</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"epochs"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"int"</span><span class="fu">,</span></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="dv">4</span><span class="fu">,</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"transform_power_2_int"</span><span class="fu">,</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="dv">4</span><span class="fu">,</span></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="dv">9</span><span class="fu">},</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"k_folds"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"int"</span><span class="fu">,</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">,</span></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"None"</span><span class="fu">,</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">,</span></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">},</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"patience"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"int"</span><span class="fu">,</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="dv">2</span><span class="fu">,</span></span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"transform_power_2_int"</span><span class="fu">,</span></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">,</span></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="dv">5</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>        <span class="fu">},</span></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"optimizer"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"levels"</span><span class="fu">:</span> <span class="ot">[</span><span class="st">"Adadelta"</span><span class="ot">,</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"Adagrad"</span><span class="ot">,</span></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"Adam"</span><span class="ot">,</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"AdamW"</span><span class="ot">,</span></span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"SparseAdam"</span><span class="ot">,</span></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"Adamax"</span><span class="ot">,</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"ASGD"</span><span class="ot">,</span></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"NAdam"</span><span class="ot">,</span></span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"RAdam"</span><span class="ot">,</span></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"RMSprop"</span><span class="ot">,</span></span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"Rprop"</span><span class="ot">,</span></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"SGD"</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"factor"</span><span class="fu">,</span></span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="st">"SGD"</span><span class="fu">,</span></span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"None"</span><span class="fu">,</span></span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"class_name"</span><span class="fu">:</span> <span class="st">"torch.optim"</span><span class="fu">,</span></span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"core_model_parameter_type"</span><span class="fu">:</span> <span class="st">"str"</span><span class="fu">,</span></span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="dv">0</span><span class="fu">,</span></span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="dv">12</span><span class="fu">},</span></span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>        <span class="dt">"sgd_momentum"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"type"</span><span class="fu">:</span> <span class="st">"float"</span><span class="fu">,</span></span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"default"</span><span class="fu">:</span> <span class="fl">0.0</span><span class="fu">,</span></span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"transform"</span><span class="fu">:</span> <span class="st">"None"</span><span class="fu">,</span></span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"lower"</span><span class="fu">:</span> <span class="fl">0.0</span><span class="fu">,</span></span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"upper"</span><span class="fu">:</span> <span class="fl">1.0</span><span class="fu">}</span></span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span><span class="er">,</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="sec-modification-of-hyperparameters-24" class="level2" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="sec-modification-of-hyperparameters-24"><span class="header-section-number">14.7</span> Modifying the Hyperparameters</h2>
<p><code>spotPython</code> provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions are described in the following.</p>
<section id="sec-modification-of-default-values-24" class="level3" data-number="14.7.1">
<h3 data-number="14.7.1" class="anchored" data-anchor-id="sec-modification-of-default-values-24"><span class="header-section-number">14.7.1</span> Modify <code>hyper_dict</code> Hyperparameters for the Selected Algorithm aka <code>core_model</code></h3>
<p>After specifying the model, the corresponding hyperparameters, their types and bounds are loaded from the <code>JSON</code> file <code>torch_hyper_dict.json</code>. After loading, the user can modify the hyperparameters, e.g., the bounds. <code>spotPython</code> provides a simple rule for de-activating hyperparameters: If the lower and the upper bound are set to identical values, the hyperparameter is de-activated. This is useful for the hyperparameter tuning, because it allows to specify a hyperparameter in the <code>JSON</code> file, but to de-activate it in the <code>fun_control</code> dictionary. This is done in the next step.</p>
</section>
<section id="modify-hyperparameters-of-type-numeric-and-integer-boolean" class="level3" data-number="14.7.2">
<h3 data-number="14.7.2" class="anchored" data-anchor-id="modify-hyperparameters-of-type-numeric-and-integer-boolean"><span class="header-section-number">14.7.2</span> Modify Hyperparameters of Type numeric and integer (boolean)</h3>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># modify the hyperparameter levels</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.hyperparameters.values <span class="im">import</span> modify_hyper_parameter_bounds</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>fun_control <span class="op">=</span> modify_hyper_parameter_bounds(fun_control, <span class="st">"epochs"</span>, bounds<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">16</span>])</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>fun_control <span class="op">=</span> modify_hyper_parameter_bounds(fun_control, <span class="st">"patience"</span>, bounds<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">7</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="modify-hyperparameter-of-type-factor" class="level3" data-number="14.7.3">
<h3 data-number="14.7.3" class="anchored" data-anchor-id="modify-hyperparameter-of-type-factor"><span class="header-section-number">14.7.3</span> Modify Hyperparameter of Type factor</h3>
<p>In a similar manner as for the numerical hyperparameters, the categorical hyperparameters can be modified. New configurations can be chosen by adding or deleting levels. For example, the hyperparameter <code>optimizer</code> can be re-configured as follows:</p>
<p>In the following setting, two optimizers (<code>"SGD"</code> and <code>"Adam"</code>) will be compared during the <code>spotPython</code> hyperparameter tuning. The hyperparameter <code>optimizer</code> is active.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.hyperparameters.values <span class="im">import</span> modify_hyper_parameter_levels</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>fun_control <span class="op">=</span> modify_hyper_parameter_levels(fun_control, <span class="st">"optimizer"</span>,</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"SGD"</span>, <span class="st">"Adam"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The hyperparameter <code>optimizer</code> can be de-activated by choosing only one value (level), here: <code>"SGD"</code>.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>fun_control <span class="op">=</span> modify_hyper_parameter_levels(fun_control, <span class="st">"optimizer"</span>, [<span class="st">"SGD"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As discussed in <a href="#sec-optimizers-24"><span>Section&nbsp;14.7.4</span></a>, there are some issues with the LBFGS optimizer. Therefore, the usage of the LBFGS optimizer is not deactivated in <code>spotPython</code> by default. However, the LBFGS optimizer can be activated by adding it to the list of optimizers. <code>Rprop</code> was removed, because it does perform very poorly (as some pre-tests have shown). However, it can also be activated by adding it to the list of optimizers. Since <code>SparseAdam</code> does not support dense gradients, <code>Adam</code> was used instead. Therefore, there are 10 default optimizers:</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>fun_control <span class="op">=</span> modify_hyper_parameter_levels(fun_control, <span class="st">"optimizer"</span>,</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Adadelta"</span>, <span class="st">"Adagrad"</span>, <span class="st">"Adam"</span>, <span class="st">"AdamW"</span>, <span class="st">"Adamax"</span>, <span class="st">"ASGD"</span>, <span class="st">"NAdam"</span>])</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>fun_control.update({</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>               <span class="st">"_L_in"</span>: n_features,</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>               <span class="st">"_L_out"</span>: <span class="dv">1</span>,})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-optimizers-24" class="level3" data-number="14.7.4">
<h3 data-number="14.7.4" class="anchored" data-anchor-id="sec-optimizers-24"><span class="header-section-number">14.7.4</span> Optimizers</h3>
<p><a href="#tbl-optimizers">Table&nbsp;<span>14.1</span></a> shows some of the optimizers available in <code>PyTorch</code>:</p>
<div id="tbl-optimizers" class="anchored">
<table class="table">
<caption>Table&nbsp;14.1: Optimizers available in PyTorch (selection). “mom” denotes <code>momentum</code>, “weight” <code>weight_decay</code>, “damp” <code>dampening</code>, “nest” <code>nesterov</code>, “lr_sc” <code>learning rate for scaling delta</code>, “mom_dec” for <code>momentum_decay</code>, and “step_s” for <code>step_sizes</code>. The default values are shown in the table.</caption>
<colgroup>
<col style="width: 11%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 5%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Optimizer</th>
<th style="text-align: left;">lr</th>
<th style="text-align: left;">mom</th>
<th style="text-align: left;">weight</th>
<th style="text-align: left;">damp</th>
<th style="text-align: left;">nest</th>
<th style="text-align: left;">rho</th>
<th style="text-align: left;">lr_sc</th>
<th style="text-align: left;">lr_decay</th>
<th style="text-align: left;">betas</th>
<th style="text-align: left;">lambd</th>
<th style="text-align: left;">alpha</th>
<th style="text-align: left;">mom_decay</th>
<th style="text-align: left;">etas</th>
<th style="text-align: left;">step_s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Adadelta</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adagrad</td>
<td style="text-align: left;">1e-2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adam</td>
<td style="text-align: left;">1e-3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">(0.9,0.999)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">AdamW</td>
<td style="text-align: left;">1e-3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">1e-2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">(0.9,0.999)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SparseAdam</td>
<td style="text-align: left;">1e-3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">(0.9,0.999)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adamax</td>
<td style="text-align: left;">2e-3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">(0.9, 0.999)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ASGD</td>
<td style="text-align: left;">1e-2</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">False</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">1e-4</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">LBFGS</td>
<td style="text-align: left;">1.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NAdam</td>
<td style="text-align: left;">2e-3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">(0.9,0.999)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">RAdam</td>
<td style="text-align: left;">1e-3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">(0.9,0.999)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RMSprop</td>
<td style="text-align: left;">1e-2</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">(0.9,0.999)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">Rprop</td>
<td style="text-align: left;">1e-2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">(0.5,1.2)</td>
<td style="text-align: left;">(1e-6, 50)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SGD</td>
<td style="text-align: left;">required</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">0.</td>
<td style="text-align: left;">False</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
</div>
<p><code>spotPython</code> implements an <code>optimization</code> handler that maps the optimizer names to the corresponding <code>PyTorch</code> optimizers.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A note on LBFGS
</div>
</div>
<div class="callout-body-container callout-body">
<p>We recommend deactivating <code>PyTorch</code>’s LBFGS optimizer, because it does not perform very well. The <code>PyTorch</code> documentation, see <a href="https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS">https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS</a>, states:</p>
<blockquote class="blockquote">
<p>This is a very memory intensive optimizer (it requires additional <code>param_bytes * (history_size + 1)</code> bytes). If it doesn’t fit in memory try reducing the history size, or use a different algorithm.</p>
</blockquote>
<p>Furthermore, the LBFGS optimizer is not compatible with the <code>PyTorch</code> tutorial. The reason is that the LBFGS optimizer requires the <code>closure</code> function, which is not implemented in the <code>PyTorch</code> tutorial. Therefore, the <code>LBFGS</code> optimizer is recommended here.</p>
</div>
</div>
<p>Since there are 10 optimizers in the portfolio, it is not recommended tuning the hyperparameters that effect one single optimizer only.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A note on the learning rate
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>spotPython</code> provides a multiplier for the default learning rates, <code>lr_mult</code>, because optimizers use different learning rates. Using a multiplier for the learning rates might enable a simultaneous tuning of the learning rates for all optimizers. However, this is not recommended, because the learning rates are not comparable across optimizers. Therefore, we recommend fixing the learning rate for all optimizers if multiple optimizers are used. This can be done by setting the lower and upper bounds of the learning rate multiplier to the same value as shown below.</p>
</div>
</div>
<p>Thus, the learning rate, which affects the <code>SGD</code> optimizer, will be set to a fixed value. We choose the default value of <code>1e-3</code> for the learning rate, because it is used in other <code>PyTorch</code> examples (it is also the default value used by <code>spotPython</code> as defined in the <code>optimizer_handler()</code> method). We recommend tuning the learning rate later, when a reduced set of optimizers is fixed. Here, we will demonstrate how to select in a screening phase the optimizers that should be used for the hyperparameter tuning.</p>
<p>For the same reason, we will fix the <code>sgd_momentum</code> to <code>0.9</code>.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>fun_control <span class="op">=</span> modify_hyper_parameter_bounds(fun_control,</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"lr_mult"</span>, bounds<span class="op">=</span>[<span class="fl">1e-3</span>, <span class="fl">1e-3</span>])</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>fun_control <span class="op">=</span> modify_hyper_parameter_bounds(fun_control,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"sgd_momentum"</span>, bounds<span class="op">=</span>[<span class="fl">0.9</span>, <span class="fl">0.9</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="sec-selection-of-target-function-24" class="level2" data-number="14.8">
<h2 data-number="14.8" class="anchored" data-anchor-id="sec-selection-of-target-function-24"><span class="header-section-number">14.8</span> Evaluation</h2>
<p>The evaluation procedure requires the specification of two elements:</p>
<ol type="1">
<li>the way how the data is split into a train and a test set and</li>
<li>the loss function (and a metric).</li>
</ol>
<section id="hold-out-data-split-and-cross-validation" class="level3" data-number="14.8.1">
<h3 data-number="14.8.1" class="anchored" data-anchor-id="hold-out-data-split-and-cross-validation"><span class="header-section-number">14.8.1</span> Hold-out Data Split and Cross-Validation</h3>
<p>As a default, <code>spotPython</code> provides a standard hold-out data split and cross validation.</p>
<section id="hold-out-data-split" class="level4" data-number="14.8.1.1">
<h4 data-number="14.8.1.1" class="anchored" data-anchor-id="hold-out-data-split"><span class="header-section-number">14.8.1.1</span> Hold-out Data Split</h4>
<p>If a hold-out data split is used, the data will be partitioned into a training, a validation, and a test data set. The split depends on the setting of the <code>eval</code> parameter. If <code>eval</code> is set to <code>train_hold_out</code>, one data set, usually the original training data set, is split into a new training and a validation data set. The training data set is used for training the model. The validation data set is used for the evaluation of the hyperparameter configuration and early stopping to prevent overfitting. In this case, the original test data set is not used. The following splits are performed in the hold-out setting: <span class="math inline">\(\{\text{train}_0, \text{test}\} \rightarrow \{\text{train}_1, \text{validation}_1, \text{test}\}\)</span>, where <span class="math inline">\(\text{train}_1 \cup \text{validation}_1 = \text{train}_0\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>spotPython</code> returns the hyperparameters of the machine learning and deep learning models, e.g., number of layers, learning rate, or optimizer, but not the model weights. Therefore, after the SPOT run is finished, the corresponding model with the optimized architecture has to be trained again with the best hyperparameter configuration. The training is performed on the training data set. The test data set is used for the final evaluation of the model.</p>
<p>Summarizing, the following splits are performed in the hold-out setting:</p>
<ol type="1">
<li>Run <code>spotPython</code> with <code>eval</code> set to <code>train_hold_out</code> to determine the best hyperparameter configuration.</li>
<li>Train the model with the best hyperparameter configuration (“architecture”) on the training data set:
<ul>
<li><code>train_tuned(model_spot, train, "model_spot.pt")</code>.</li>
</ul></li>
<li>Test the model on the test data:
<ul>
<li><code>test_tuned(model_spot, test, "model_spot.pt")</code></li>
</ul></li>
</ol>
<p>These steps will be exemplified in the following sections.</p>
</div>
</div>
<p>In addition to this <code>hold-out</code> setting, <code>spotPython</code> provides another hold-out setting, where an explicit test data is specified by the user that will be used as the validation set. To choose this option, the <code>eval</code> parameter is set to <code>test_hold_out</code>. In this case, the training data set is used for the model training. Then, the explicitly defined test data set is used for the evaluation of the hyperparameter configuration (the validation).</p>
</section>
<section id="cross-validation" class="level4" data-number="14.8.1.2">
<h4 data-number="14.8.1.2" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">14.8.1.2</span> Cross-Validation</h4>
<p>The cross validation setting is used by setting the <code>eval</code> parameter to <code>train_cv</code> or <code>test_cv</code>. In both cases, the data set is split into <span class="math inline">\(k\)</span> folds. The model is trained on <span class="math inline">\(k-1\)</span> folds and evaluated on the remaining fold. This is repeated <span class="math inline">\(k\)</span> times, so that each fold is used exactly once for evaluation. The final evaluation is performed on the test data set. The cross validation setting is useful for small data sets, because it allows to use all data for training and evaluation. However, it is computationally expensive, because the model has to be trained <span class="math inline">\(k\)</span> times.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Combinations of the above settings are possible, e.g., cross validation can be used for training and hold-out for evaluation or <em>vice versa</em>. Also, cross validation can be used for training and testing. Because cross validation is not used in the <code>PyTorch</code> tutorial <span class="citation" data-cites="pyto23a">(<a href="references.html#ref-pyto23a" role="doc-biblioref">PyTorch 2023</a>)</span>, it is not considered further here.</p>
</div>
</div>
</section>
<section id="overview-of-the-evaluation-settings" class="level4" data-number="14.8.1.3">
<h4 data-number="14.8.1.3" class="anchored" data-anchor-id="overview-of-the-evaluation-settings"><span class="header-section-number">14.8.1.3</span> Overview of the Evaluation Settings</h4>
<section id="settings-for-the-hyperparameter-tuning" class="level5" data-number="14.8.1.3.1">
<h5 data-number="14.8.1.3.1" class="anchored" data-anchor-id="settings-for-the-hyperparameter-tuning"><span class="header-section-number">14.8.1.3.1</span> Settings for the Hyperparameter Tuning</h5>
<p><a href="#tbl-eval-settings">Table&nbsp;<span>14.2</span></a> provides an overview of the training evaluations.</p>
<div id="tbl-eval-settings" class="anchored">
<table class="table">
<caption>Table&nbsp;14.2: Overview of the evaluation settings.</caption>
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 28%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th><code>eval</code></th>
<th style="text-align: center;"><code>train</code></th>
<th style="text-align: center;"><code>test</code></th>
<th style="text-align: left;">function</th>
<th style="text-align: left;">comment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>"train_hold_out"</code></td>
<td style="text-align: center;"><span class="math inline">\(\checkmark\)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>train_one_epoch()</code>, <code>validate_one_epoch()</code> for early stopping</td>
<td style="text-align: left;">splits the <code>train</code> data set internally</td>
</tr>
<tr class="even">
<td><code>"test_hold_out"</code></td>
<td style="text-align: center;"><span class="math inline">\(\checkmark\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\checkmark\)</span></td>
<td style="text-align: left;"><code>train_one_epoch()</code>, <code>validate_one_epoch()</code> for early stopping</td>
<td style="text-align: left;">use the <code>test data set</code> for <code>validate_one_epoch()</code></td>
</tr>
<tr class="odd">
<td><code>"train_cv"</code></td>
<td style="text-align: center;"><span class="math inline">\(\checkmark\)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>evaluate_cv(net, train)</code></td>
<td style="text-align: left;">CV using the <code>train</code> data set</td>
</tr>
<tr class="even">
<td><code>"test_cv"</code></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span class="math inline">\(\checkmark\)</span></td>
<td style="text-align: left;"><code>evaluate_cv(net, test)</code></td>
<td style="text-align: left;">CV using the <code>test</code> data set . Identical to <code>"train_cv"</code>, uses only test data.</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><code>"train_cv"</code> and <code>"test_cv"</code> use <code>sklearn.model_selection.KFold()</code> internally.</li>
</ul>
</section>
</section>
<section id="settings-for-the-final-evaluation-of-the-tuned-architecture" class="level4" data-number="14.8.1.4">
<h4 data-number="14.8.1.4" class="anchored" data-anchor-id="settings-for-the-final-evaluation-of-the-tuned-architecture"><span class="header-section-number">14.8.1.4</span> Settings for the Final Evaluation of the Tuned Architecture</h4>
<section id="training-of-the-tuned-architecture" class="level5" data-number="14.8.1.4.1">
<h5 data-number="14.8.1.4.1" class="anchored" data-anchor-id="training-of-the-tuned-architecture"><span class="header-section-number">14.8.1.4.1</span> Training of the Tuned Architecture</h5>
<p><code>train_tuned(model, train)</code>: train the model with the best hyperparameter configuration (or simply the default) on the training data set. It splits the <code>train</code>data into new <code>train</code> and <code>validation</code> sets using <code>create_train_val_data_loaders()</code>, which calls <code>torch.utils.data.random_split()</code> internally. Currently, 60% of the data is used for training and 40% for validation. The <code>train</code> data is used for training the model with <code>train_one_epoch()</code>. The <code>validation</code> data is used for early stopping using <code>validate_one_epoch()</code> on the <code>validation</code> data set.</p>
</section>
<section id="testing-of-the-tuned-architecture" class="level5" data-number="14.8.1.4.2">
<h5 data-number="14.8.1.4.2" class="anchored" data-anchor-id="testing-of-the-tuned-architecture"><span class="header-section-number">14.8.1.4.2</span> Testing of the Tuned Architecture</h5>
<p><code>test_tuned(model, test)</code>: test the model on the test data set. No data splitting is performed. The (trained) model is evaluated using the <code>validate_one_epoch()</code> function.</p>
<p>Note: During training, <code>shuffle</code> is set to <code>True</code>, whereas during testing, <code>shuffle</code> is set to <code>False</code>.</p>
</section>
</section>
</section>
<section id="sec-loss-functions-and-metrics-24" class="level3" data-number="14.8.2">
<h3 data-number="14.8.2" class="anchored" data-anchor-id="sec-loss-functions-and-metrics-24"><span class="header-section-number">14.8.2</span> Loss Functions and Metrics</h3>
<p>The key <code>"loss_function"</code> specifies the loss function which is used during the optimization. There are several different loss functions under <code>PyTorch</code>’s <code>nn</code> package. For example, a simple loss is <code>MSELoss</code>, which computes the mean-squared error between the output and the target. In this tutorial we will use <code>CrossEntropyLoss</code>, because it is also used in the <code>PyTorch</code> tutorial.</p>
<section id="loss-function" class="level4" data-number="14.8.2.1">
<h4 data-number="14.8.2.1" class="anchored" data-anchor-id="loss-function"><span class="header-section-number">14.8.2.1</span> Loss Function</h4>
<p>The loss function is specified by the key <code>"loss_function"</code>. We will use MSE loss for the regression task.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> MSELoss</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>loss_torch <span class="op">=</span> MSELoss()</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>fun_control.update({<span class="st">"loss_function"</span>: loss_torch})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In addition to the loss functions, <code>spotPython</code> provides access to a large number of metrics.</p>
<ul>
<li>The key <code>"metric_sklearn"</code> is used for metrics that follow the <code>scikit-learn</code> conventions.</li>
<li>The key <code>"river_metric"</code> is used for the river based evaluation <span class="citation" data-cites="mont20a">(<a href="references.html#ref-mont20a" role="doc-biblioref">Montiel et al. 2021</a>)</span> via <code>eval_oml_iter_progressive</code>, and</li>
<li>the key <code>"metric_torch"</code> is used for the metrics from <code>TorchMetrics</code>.</li>
</ul>
<p><code>TorchMetrics</code> is a collection of more than 90 PyTorch metrics<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchmetrics <span class="im">import</span> MeanAbsoluteError</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>metric_torch <span class="op">=</span> MeanAbsoluteError(device<span class="op">=</span>fun_control[<span class="st">"device"</span>])</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>fun_control.update({<span class="st">"metric_torch"</span>: metric_torch})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="sec-call-the-hyperparameter-tuner-24" class="level2" data-number="14.9">
<h2 data-number="14.9" class="anchored" data-anchor-id="sec-call-the-hyperparameter-tuner-24"><span class="header-section-number">14.9</span> Calling the SPOT Function</h2>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the variable types, names, and bounds</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.hyperparameters.values <span class="im">import</span> (get_bound_values,</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    get_var_name,</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    get_var_type,)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>var_type <span class="op">=</span> get_var_type(fun_control)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>var_name <span class="op">=</span> get_var_name(fun_control)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>fun_control.update({<span class="st">"var_type"</span>: var_type,</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"var_name"</span>: var_name})</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>lower <span class="op">=</span> get_bound_values(fun_control, <span class="st">"lower"</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>upper <span class="op">=</span> get_bound_values(fun_control, <span class="st">"upper"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, the dictionary <code>fun_control</code> contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method <code>gen_design_table</code> generates a design table as follows:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.utils.eda <span class="im">import</span> gen_design_table</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(gen_design_table(fun_control))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>| name         | type   | default   |   lower |   upper | transform             |
|--------------|--------|-----------|---------|---------|-----------------------|
| _L_in        | int    | 10        |  10     |  10     | None                  |
| _L_out       | int    | 1         |   1     |   1     | None                  |
| l1           | int    | 3         |   3     |   8     | transform_power_2_int |
| dropout_prob | float  | 0.01      |   0     |   0.9   | None                  |
| lr_mult      | float  | 1.0       |   0.001 |   0.001 | None                  |
| batch_size   | int    | 4         |   1     |   4     | transform_power_2_int |
| epochs       | int    | 4         |   2     |  16     | transform_power_2_int |
| k_folds      | int    | 1         |   1     |   1     | None                  |
| patience     | int    | 2         |   3     |   7     | transform_power_2_int |
| optimizer    | factor | SGD       |   0     |   6     | None                  |
| sgd_momentum | float  | 0.0       |   0.9   |   0.9   | None                  |</code></pre>
</div>
</div>
<p>This allows to check if all information is available and if the information is correct. <a href="#tbl-design">Table&nbsp;<span>14.3</span></a> shows the experimental design for the hyperparameter tuning. Hyperparameter transformations are shown in the column “transform”, e.g., the <code>l1</code> default is <code>5</code>, which results in the value <span class="math inline">\(2^5 = 32\)</span> for the network, because the transformation <code>transform_power_2_int</code> was selected in the <code>JSON</code> file. The default value of the <code>batch_size</code> is set to <code>4</code>, which results in a batch size of <span class="math inline">\(2^4 = 16\)</span>.</p>
<div id="tbl-design" class="anchored">
<table class="table">
<caption>Table&nbsp;14.3: Experimental design for the hyperparameter tuning. The table shows the hyperparameters, their types, default values, lower and upper bounds, and the transformation function. The transformation function is used to transform the hyperparameter values from the unit hypercube to the original domain. The transformation function is applied to the hyperparameter values before the evaluation of the objective function.</caption>
<colgroup>
<col style="width: 18%">
<col style="width: 10%">
<col style="width: 14%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>name</th>
<th>type</th>
<th>default</th>
<th>lower</th>
<th>upper</th>
<th>transform</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>_L_in</td>
<td>int</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>None</td>
</tr>
<tr class="even">
<td>_L_out</td>
<td>int</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>None</td>
</tr>
<tr class="odd">
<td>l1</td>
<td>int</td>
<td>3</td>
<td>3</td>
<td>8</td>
<td>transform_power_2_int</td>
</tr>
<tr class="even">
<td>dropout_prob</td>
<td>float</td>
<td>0.01</td>
<td>0</td>
<td>0.9</td>
<td>None</td>
</tr>
<tr class="odd">
<td>lr_mult</td>
<td>float</td>
<td>1.0</td>
<td>0.001</td>
<td>0.001</td>
<td>None</td>
</tr>
<tr class="even">
<td>batch_size</td>
<td>int</td>
<td>4</td>
<td>1</td>
<td>4</td>
<td>transform_power_2_int</td>
</tr>
<tr class="odd">
<td>epochs</td>
<td>int</td>
<td>4</td>
<td>2</td>
<td>16</td>
<td>transform_power_2_int</td>
</tr>
<tr class="even">
<td>k_folds</td>
<td>int</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>None</td>
</tr>
<tr class="odd">
<td>patience</td>
<td>int</td>
<td>2</td>
<td>3</td>
<td>7</td>
<td>transform_power_2_int</td>
</tr>
<tr class="even">
<td>optimizer</td>
<td>factor</td>
<td>SGD</td>
<td>0</td>
<td>6</td>
<td>None</td>
</tr>
<tr class="odd">
<td>sgd_momentum</td>
<td>float</td>
<td>0.0</td>
<td>0.9</td>
<td>0.9</td>
<td>None</td>
</tr>
</tbody>
</table>
</div>
<p>The objective function <code>fun_torch</code> is selected next. It implements an interface from <code>PyTorch</code>’s training, validation, and testing methods to <code>spotPython</code>.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.fun.hypertorch <span class="im">import</span> HyperTorch</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>fun <span class="op">=</span> HyperTorch().fun_torch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.hyperparameters.values <span class="im">import</span> get_default_hyperparameters_as_array</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>hyper_dict<span class="op">=</span>TorchHyperDict().load()</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>X_start <span class="op">=</span> get_default_hyperparameters_as_array(fun_control, hyper_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>fun_control.update({</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>               <span class="st">"device"</span>: <span class="st">"cpu"</span>,</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>               })</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>spotPython</code> hyperparameter tuning is started by calling the <code>Spot</code> function. Here, we will run the tuner for approximately 30 minutes (<code>max_time</code>). Note: the initial design is always evaluated in the <code>spotPython</code> run. As a consequence, the run may take longer than specified by <code>max_time</code>, because the evaluation time of initial design (here: <code>init_size</code>, 10 points) is performed independently of <code>max_time</code>.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.spot <span class="im">import</span> spot</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> inf</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>spot_tuner <span class="op">=</span> spot.Spot(fun<span class="op">=</span>fun,</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>                   lower <span class="op">=</span> lower,</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>                   upper <span class="op">=</span> upper,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>                   fun_evals <span class="op">=</span> inf,</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>                   fun_repeats <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>                   max_time <span class="op">=</span> MAX_TIME,</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>                   noise <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>                   tolerance_x <span class="op">=</span> np.sqrt(np.spacing(<span class="dv">1</span>)),</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>                   var_type <span class="op">=</span> var_type,</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>                   var_name <span class="op">=</span> var_name,</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>                   infill_criterion <span class="op">=</span> <span class="st">"y"</span>,</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>                   n_points <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>                   seed<span class="op">=</span><span class="dv">123</span>,</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>                   log_level <span class="op">=</span> <span class="dv">50</span>,</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>                   show_models<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>                   show_progress<span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>                   fun_control <span class="op">=</span> fun_control,</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>                   design_control<span class="op">=</span>{<span class="st">"init_size"</span>: INIT_SIZE,</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>                                   <span class="st">"repeats"</span>: <span class="dv">1</span>},</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>                   surrogate_control<span class="op">=</span>{<span class="st">"noise"</span>: <span class="va">True</span>,</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>                                      <span class="st">"cod_type"</span>: <span class="st">"norm"</span>,</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>                                      <span class="st">"min_theta"</span>: <span class="op">-</span><span class="dv">4</span>,</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>                                      <span class="st">"max_theta"</span>: <span class="dv">3</span>,</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>                                      <span class="st">"n_theta"</span>: <span class="bu">len</span>(var_name),</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>                                      <span class="st">"model_fun_evals"</span>: <span class="dv">10_000</span>,</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>                                      <span class="st">"log_level"</span>: <span class="dv">50</span></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>                                      })</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>spot_tuner.run(X_start<span class="op">=</span>X_start)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
config: {'_L_in': 10, '_L_out': 1, 'l1': 128, 'dropout_prob': 0.7103122166156, 'lr_mult': 0.001, 'batch_size': 4, 'epochs': 128, 'k_folds': 1, 'patience': 128, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}
Epoch: 1
Loss on hold-out set: 0.3880799110730489
MeanAbsoluteError value on hold-out data: 0.5899776816368103
Epoch: 2
Loss on hold-out set: 0.3836869841814041
MeanAbsoluteError value on hold-out data: 0.5859453678131104
Epoch: 3
Loss on hold-out set: 0.37800121009349824
MeanAbsoluteError value on hold-out data: 0.5838974118232727
Epoch: 4</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.37562453736861545
MeanAbsoluteError value on hold-out data: 0.5799493193626404
Epoch: 5
Loss on hold-out set: 0.36837248464425404
MeanAbsoluteError value on hold-out data: 0.5729854106903076
Epoch: 6
Loss on hold-out set: 0.36849121779203414
MeanAbsoluteError value on hold-out data: 0.5718793272972107
Epoch: 7
Loss on hold-out set: 0.3593301492929459
MeanAbsoluteError value on hold-out data: 0.5673142075538635
Epoch: 8</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.35647780607144036
MeanAbsoluteError value on hold-out data: 0.5659938454627991
Epoch: 9
Loss on hold-out set: 0.3682290816307068
MeanAbsoluteError value on hold-out data: 0.572464644908905
Epoch: 10
Loss on hold-out set: 0.3438674708207448
MeanAbsoluteError value on hold-out data: 0.5554881691932678
Epoch: 11
Loss on hold-out set: 0.35077977160612744
MeanAbsoluteError value on hold-out data: 0.5578439235687256
Epoch: 12</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.32907806505759557
MeanAbsoluteError value on hold-out data: 0.5416633486747742
Epoch: 13
Loss on hold-out set: 0.3389214011033376
MeanAbsoluteError value on hold-out data: 0.5456399321556091
Epoch: 14
Loss on hold-out set: 0.33186697870492937
MeanAbsoluteError value on hold-out data: 0.5429821610450745
Epoch: 15
Loss on hold-out set: 0.32186263173818586
MeanAbsoluteError value on hold-out data: 0.5337691307067871
Epoch: 16</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.32638982792695365
MeanAbsoluteError value on hold-out data: 0.5339400768280029
Epoch: 17
Loss on hold-out set: 0.34150955428679786
MeanAbsoluteError value on hold-out data: 0.5488361120223999
Epoch: 18
Loss on hold-out set: 0.3181079350908597
MeanAbsoluteError value on hold-out data: 0.5279594659805298
Epoch: 19
Loss on hold-out set: 0.3101205576459567
MeanAbsoluteError value on hold-out data: 0.5216462016105652
Epoch: 20</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.31272549013296763
MeanAbsoluteError value on hold-out data: 0.5277438759803772
Epoch: 21
Loss on hold-out set: 0.31258161957065267
MeanAbsoluteError value on hold-out data: 0.5266361236572266
Epoch: 22
Loss on hold-out set: 0.3085262535015742
MeanAbsoluteError value on hold-out data: 0.5225566625595093
Epoch: 23
Loss on hold-out set: 0.2937054855624835
MeanAbsoluteError value on hold-out data: 0.5049911737442017
Epoch: 24</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.29786009723941487
MeanAbsoluteError value on hold-out data: 0.505726158618927
Epoch: 25
Loss on hold-out set: 0.29567268838485083
MeanAbsoluteError value on hold-out data: 0.5087849497795105
Epoch: 26
Loss on hold-out set: 0.2813201687733332
MeanAbsoluteError value on hold-out data: 0.4963820278644562
Epoch: 27
Loss on hold-out set: 0.2902079832553863
MeanAbsoluteError value on hold-out data: 0.5025125741958618
Epoch: 28</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.27412380596001945
MeanAbsoluteError value on hold-out data: 0.4855813682079315
Epoch: 29
Loss on hold-out set: 0.27381120532751085
MeanAbsoluteError value on hold-out data: 0.4837619960308075
Epoch: 30
Loss on hold-out set: 0.2753249797721704
MeanAbsoluteError value on hold-out data: 0.48615604639053345
Epoch: 31
Loss on hold-out set: 0.2756906555593014
MeanAbsoluteError value on hold-out data: 0.4903359115123749
Epoch: 32</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.26016402393579485
MeanAbsoluteError value on hold-out data: 0.4773153066635132
Epoch: 33
Loss on hold-out set: 0.2694820617636045
MeanAbsoluteError value on hold-out data: 0.48563048243522644
Epoch: 34
Loss on hold-out set: 0.2616444872319698
MeanAbsoluteError value on hold-out data: 0.47832727432250977
Epoch: 35
Loss on hold-out set: 0.2511261120438576
MeanAbsoluteError value on hold-out data: 0.4622211158275604
Epoch: 36</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.26056523924072583
MeanAbsoluteError value on hold-out data: 0.47433289885520935
Epoch: 37
Loss on hold-out set: 0.24974486788113912
MeanAbsoluteError value on hold-out data: 0.4631868898868561
Epoch: 38
Loss on hold-out set: 0.24516535783807436
MeanAbsoluteError value on hold-out data: 0.4578583240509033
Epoch: 39
Loss on hold-out set: 0.2462805862724781
MeanAbsoluteError value on hold-out data: 0.4543892443180084
Epoch: 40</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.23850424150625865
MeanAbsoluteError value on hold-out data: 0.4484350085258484
Epoch: 41
Loss on hold-out set: 0.2375045409301917
MeanAbsoluteError value on hold-out data: 0.44821715354919434
Epoch: 42
Loss on hold-out set: 0.2434506955742836
MeanAbsoluteError value on hold-out data: 0.4549056589603424
Epoch: 43
Loss on hold-out set: 0.23038897509376208
MeanAbsoluteError value on hold-out data: 0.44086360931396484
Epoch: 44</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2311189151306947
MeanAbsoluteError value on hold-out data: 0.43871328234672546
Epoch: 45
Loss on hold-out set: 0.23182736575603485
MeanAbsoluteError value on hold-out data: 0.44097092747688293
Epoch: 46
Loss on hold-out set: 0.21953868828713893
MeanAbsoluteError value on hold-out data: 0.4279610216617584
Epoch: 47
Loss on hold-out set: 0.23960840448737145
MeanAbsoluteError value on hold-out data: 0.44631215929985046
Epoch: 48</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.21432177330056826
MeanAbsoluteError value on hold-out data: 0.42518240213394165
Epoch: 49
Loss on hold-out set: 0.22572503065069516
MeanAbsoluteError value on hold-out data: 0.4385136365890503
Epoch: 50
Loss on hold-out set: 0.21196399688720702
MeanAbsoluteError value on hold-out data: 0.42245790362358093
Epoch: 51
Loss on hold-out set: 0.21647593803703785
MeanAbsoluteError value on hold-out data: 0.42338305711746216
Epoch: 52</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.21441839839021365
MeanAbsoluteError value on hold-out data: 0.42293089628219604
Epoch: 53
Loss on hold-out set: 0.20614871313174565
MeanAbsoluteError value on hold-out data: 0.41088417172431946
Epoch: 54
Loss on hold-out set: 0.21418154728909333
MeanAbsoluteError value on hold-out data: 0.4205377697944641
Epoch: 55
Loss on hold-out set: 0.19838518684109052
MeanAbsoluteError value on hold-out data: 0.4055808186531067
Epoch: 56</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.19423130945612987
MeanAbsoluteError value on hold-out data: 0.4013197720050812
Epoch: 57
Loss on hold-out set: 0.19603884605069954
MeanAbsoluteError value on hold-out data: 0.40485307574272156
Epoch: 58
Loss on hold-out set: 0.191354534526666
MeanAbsoluteError value on hold-out data: 0.3967095613479614
Epoch: 59
Loss on hold-out set: 0.18607441939413547
MeanAbsoluteError value on hold-out data: 0.389576256275177
Epoch: 60</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.19079403246442478
MeanAbsoluteError value on hold-out data: 0.3925374746322632
Epoch: 61
Loss on hold-out set: 0.1960036384811004
MeanAbsoluteError value on hold-out data: 0.4035680592060089
Epoch: 62
Loss on hold-out set: 0.18160789246360462
MeanAbsoluteError value on hold-out data: 0.380288690328598
Epoch: 63
Loss on hold-out set: 0.1989179414510727
MeanAbsoluteError value on hold-out data: 0.3994502127170563
Epoch: 64
Loss on hold-out set: 0.19254313580691815
MeanAbsoluteError value on hold-out data: 0.39306581020355225
Epoch: 65</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1771948327869177
MeanAbsoluteError value on hold-out data: 0.3773879110813141
Epoch: 66
Loss on hold-out set: 0.1760231030980746
MeanAbsoluteError value on hold-out data: 0.3762041926383972
Epoch: 67
Loss on hold-out set: 0.17199590936303139
MeanAbsoluteError value on hold-out data: 0.3691391050815582
Epoch: 68
Loss on hold-out set: 0.16509607955813407
MeanAbsoluteError value on hold-out data: 0.3647073805332184
Epoch: 69
Loss on hold-out set: 0.17139507159590722
MeanAbsoluteError value on hold-out data: 0.3691309094429016
Epoch: 70</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.16666551979879538
MeanAbsoluteError value on hold-out data: 0.3621125817298889
Epoch: 71
Loss on hold-out set: 0.1694364886979262
MeanAbsoluteError value on hold-out data: 0.36576178669929504
Epoch: 72
Loss on hold-out set: 0.1707458314920465
MeanAbsoluteError value on hold-out data: 0.36526861786842346
Epoch: 73
Loss on hold-out set: 0.158644323994716
MeanAbsoluteError value on hold-out data: 0.35473546385765076
Epoch: 74
Loss on hold-out set: 0.15925804947813352
MeanAbsoluteError value on hold-out data: 0.35808253288269043
Epoch: 75</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14871751541271805
MeanAbsoluteError value on hold-out data: 0.3411659300327301
Epoch: 76
Loss on hold-out set: 0.15881584075589975
MeanAbsoluteError value on hold-out data: 0.35057008266448975
Epoch: 77
Loss on hold-out set: 0.15353250943124294
MeanAbsoluteError value on hold-out data: 0.3414638340473175
Epoch: 78
Loss on hold-out set: 0.13879319821794828
MeanAbsoluteError value on hold-out data: 0.3278297185897827
Epoch: 79
Loss on hold-out set: 0.1432836973418792
MeanAbsoluteError value on hold-out data: 0.3311055302619934
Epoch: 80</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14388307603696981
MeanAbsoluteError value on hold-out data: 0.3319510519504547
Epoch: 81
Loss on hold-out set: 0.14630789950489997
MeanAbsoluteError value on hold-out data: 0.3349020481109619
Epoch: 82
Loss on hold-out set: 0.1358308787892262
MeanAbsoluteError value on hold-out data: 0.3195601999759674
Epoch: 83
Loss on hold-out set: 0.13304505594074725
MeanAbsoluteError value on hold-out data: 0.31500616669654846
Epoch: 84
Loss on hold-out set: 0.14229338797430197
MeanAbsoluteError value on hold-out data: 0.3295309841632843
Epoch: 85</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13413121630748112
MeanAbsoluteError value on hold-out data: 0.32050973176956177
Epoch: 86
Loss on hold-out set: 0.12837606566647689
MeanAbsoluteError value on hold-out data: 0.3109428882598877
Epoch: 87
Loss on hold-out set: 0.1336614220837752
MeanAbsoluteError value on hold-out data: 0.31675392389297485
Epoch: 88
Loss on hold-out set: 0.12711885467171669
MeanAbsoluteError value on hold-out data: 0.3072037696838379
Epoch: 89
Loss on hold-out set: 0.13057974826544524
MeanAbsoluteError value on hold-out data: 0.31409522891044617
Epoch: 90</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1297481312106053
MeanAbsoluteError value on hold-out data: 0.31132158637046814
Epoch: 91
Loss on hold-out set: 0.11912523386379083
MeanAbsoluteError value on hold-out data: 0.30101707577705383
Epoch: 92
Loss on hold-out set: 0.12311545960605144
MeanAbsoluteError value on hold-out data: 0.30496668815612793
Epoch: 93
Loss on hold-out set: 0.11927016124129296
MeanAbsoluteError value on hold-out data: 0.2962914705276489
Epoch: 94
Loss on hold-out set: 0.12144774296631415
MeanAbsoluteError value on hold-out data: 0.30237501859664917
Epoch: 95</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11746331736445427
MeanAbsoluteError value on hold-out data: 0.2950419485569
Epoch: 96
Loss on hold-out set: 0.11121155418455601
MeanAbsoluteError value on hold-out data: 0.2861716151237488
Epoch: 97
Loss on hold-out set: 0.10882653936743736
MeanAbsoluteError value on hold-out data: 0.2833530306816101
Epoch: 98
Loss on hold-out set: 0.10903889745473862
MeanAbsoluteError value on hold-out data: 0.2800130844116211
Epoch: 99
Loss on hold-out set: 0.12341873566309611
MeanAbsoluteError value on hold-out data: 0.29833653569221497
Epoch: 100</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1054092987626791
MeanAbsoluteError value on hold-out data: 0.2739351987838745
Epoch: 101
Loss on hold-out set: 0.1053611833974719
MeanAbsoluteError value on hold-out data: 0.27997511625289917
Epoch: 102
Loss on hold-out set: 0.10180222911139329
MeanAbsoluteError value on hold-out data: 0.27020105719566345
Epoch: 103
Loss on hold-out set: 0.11045179172108571
MeanAbsoluteError value on hold-out data: 0.28186503052711487
Epoch: 104
Loss on hold-out set: 0.09439728098611037
MeanAbsoluteError value on hold-out data: 0.26401928067207336
Epoch: 105</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09351931383212407
MeanAbsoluteError value on hold-out data: 0.26154905557632446
Epoch: 106
Loss on hold-out set: 0.09713801949595412
MeanAbsoluteError value on hold-out data: 0.2638161778450012
Epoch: 107
Loss on hold-out set: 0.09345814928412438
MeanAbsoluteError value on hold-out data: 0.2608989179134369
Epoch: 108
Loss on hold-out set: 0.10183271319294969
MeanAbsoluteError value on hold-out data: 0.2747385799884796
Epoch: 109
Loss on hold-out set: 0.09911684604982535
MeanAbsoluteError value on hold-out data: 0.2694036066532135
Epoch: 110</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09799518172939618
MeanAbsoluteError value on hold-out data: 0.2628406286239624
Epoch: 111
Loss on hold-out set: 0.10030249775697787
MeanAbsoluteError value on hold-out data: 0.26927390694618225
Epoch: 112
Loss on hold-out set: 0.089515517236044
MeanAbsoluteError value on hold-out data: 0.24437321722507477
Epoch: 113
Loss on hold-out set: 0.08700421212861935
MeanAbsoluteError value on hold-out data: 0.2446918934583664
Epoch: 114
Loss on hold-out set: 0.09243666367605329
MeanAbsoluteError value on hold-out data: 0.2530674636363983
Epoch: 115</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08728596359491349
MeanAbsoluteError value on hold-out data: 0.2474481761455536
Epoch: 116
Loss on hold-out set: 0.09119456669936578
MeanAbsoluteError value on hold-out data: 0.2502688467502594
Epoch: 117
Loss on hold-out set: 0.08733472774426142
MeanAbsoluteError value on hold-out data: 0.2512621283531189
Epoch: 118
Loss on hold-out set: 0.08680794619644681
MeanAbsoluteError value on hold-out data: 0.2450418621301651
Epoch: 119
Loss on hold-out set: 0.08382196660464009
MeanAbsoluteError value on hold-out data: 0.24181099236011505
Epoch: 120</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08907857824116945
MeanAbsoluteError value on hold-out data: 0.24978691339492798
Epoch: 121
Loss on hold-out set: 0.08207449068625768
MeanAbsoluteError value on hold-out data: 0.24045430123806
Epoch: 122
Loss on hold-out set: 0.09053841241945823
MeanAbsoluteError value on hold-out data: 0.24808357656002045
Epoch: 123
Loss on hold-out set: 0.07853908311575651
MeanAbsoluteError value on hold-out data: 0.23402859270572662
Epoch: 124</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08898908268039425
MeanAbsoluteError value on hold-out data: 0.24805593490600586
Epoch: 125
Loss on hold-out set: 0.09254279808762172
MeanAbsoluteError value on hold-out data: 0.2544447183609009
Epoch: 126
Loss on hold-out set: 0.08263650688032309
MeanAbsoluteError value on hold-out data: 0.23900435864925385
Epoch: 127
Loss on hold-out set: 0.07984504015495379
MeanAbsoluteError value on hold-out data: 0.23947305977344513
Epoch: 128</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0862145904575785
MeanAbsoluteError value on hold-out data: 0.2431158423423767
Returned to Spot: Validation loss: 0.0862145904575785
----------------------------------------------

config: {'_L_in': 10, '_L_out': 1, 'l1': 32, 'dropout_prob': 0.13021660839652088, 'lr_mult': 0.001, 'batch_size': 8, 'epochs': 256, 'k_folds': 1, 'patience': 64, 'optimizer': 'Adagrad', 'sgd_momentum': 0.9}
Epoch: 1
Loss on hold-out set: 0.4627989726631265
MeanAbsoluteError value on hold-out data: 0.6565921306610107
Epoch: 2
Loss on hold-out set: 0.4589531892224362
MeanAbsoluteError value on hold-out data: 0.6534714102745056
Epoch: 3
Loss on hold-out set: 0.4593477570696881
MeanAbsoluteError value on hold-out data: 0.6539322137832642
Epoch: 4
Loss on hold-out set: 0.46001733133667394
MeanAbsoluteError value on hold-out data: 0.6547444462776184
Epoch: 5
Loss on hold-out set: 0.4608977029198094
MeanAbsoluteError value on hold-out data: 0.6551709175109863
Epoch: 6
Loss on hold-out set: 0.4647337535494252
MeanAbsoluteError value on hold-out data: 0.6578038334846497
Epoch: 7
Loss on hold-out set: 0.4612514509966499
MeanAbsoluteError value on hold-out data: 0.654595136642456
Epoch: 8
Loss on hold-out set: 0.4579661174824363
MeanAbsoluteError value on hold-out data: 0.6533114910125732
Epoch: 9
Loss on hold-out set: 0.4593767343383086
MeanAbsoluteError value on hold-out data: 0.6537655591964722
Epoch: 10
Loss on hold-out set: 0.46053999348690633
MeanAbsoluteError value on hold-out data: 0.6547128558158875
Epoch: 11</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4595827977908285
MeanAbsoluteError value on hold-out data: 0.6539943218231201
Epoch: 12
Loss on hold-out set: 0.45962194470982803
MeanAbsoluteError value on hold-out data: 0.6543705463409424
Epoch: 13
Loss on hold-out set: 0.45736330512322876
MeanAbsoluteError value on hold-out data: 0.6524419784545898
Epoch: 14
Loss on hold-out set: 0.4599130185026872
MeanAbsoluteError value on hold-out data: 0.6543874144554138
Epoch: 15
Loss on hold-out set: 0.4579157084226608
MeanAbsoluteError value on hold-out data: 0.6527398228645325
Epoch: 16
Loss on hold-out set: 0.4562674541222422
MeanAbsoluteError value on hold-out data: 0.6513288617134094
Epoch: 17
Loss on hold-out set: 0.4579960154859643
MeanAbsoluteError value on hold-out data: 0.6523417234420776
Epoch: 18
Loss on hold-out set: 0.45789690080441925
MeanAbsoluteError value on hold-out data: 0.6533842086791992
Epoch: 19
Loss on hold-out set: 0.4583052561471337
MeanAbsoluteError value on hold-out data: 0.6532690525054932
Epoch: 20
Loss on hold-out set: 0.45698324316426325
MeanAbsoluteError value on hold-out data: 0.6522902846336365
Epoch: 21
Loss on hold-out set: 0.4576420674198552
MeanAbsoluteError value on hold-out data: 0.6519546508789062
Epoch: 22</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.45693942826045186
MeanAbsoluteError value on hold-out data: 0.6517285108566284
Epoch: 23
Loss on hold-out set: 0.4544865826242848
MeanAbsoluteError value on hold-out data: 0.6501532793045044
Epoch: 24
Loss on hold-out set: 0.45744823860494715
MeanAbsoluteError value on hold-out data: 0.6524167656898499
Epoch: 25
Loss on hold-out set: 0.45570837823968185
MeanAbsoluteError value on hold-out data: 0.6511331796646118
Epoch: 26
Loss on hold-out set: 0.4553325913454357
MeanAbsoluteError value on hold-out data: 0.6506592631340027
Epoch: 27
Loss on hold-out set: 0.45445852452202845
MeanAbsoluteError value on hold-out data: 0.6499620676040649
Epoch: 28
Loss on hold-out set: 0.4542893304636604
MeanAbsoluteError value on hold-out data: 0.6509696245193481
Epoch: 29
Loss on hold-out set: 0.45320791398224075
MeanAbsoluteError value on hold-out data: 0.6491036415100098
Epoch: 30
Loss on hold-out set: 0.45692677090042516
MeanAbsoluteError value on hold-out data: 0.6519822478294373
Epoch: 31
Loss on hold-out set: 0.4532112212557542
MeanAbsoluteError value on hold-out data: 0.6492712497711182
Epoch: 32
Loss on hold-out set: 0.4557249985243145
MeanAbsoluteError value on hold-out data: 0.6510018706321716
Epoch: 33</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4588989474271473
MeanAbsoluteError value on hold-out data: 0.6533785462379456
Epoch: 34
Loss on hold-out set: 0.45636854516832454
MeanAbsoluteError value on hold-out data: 0.6515430212020874
Epoch: 35
Loss on hold-out set: 0.4528054547937293
MeanAbsoluteError value on hold-out data: 0.6489641070365906
Epoch: 36
Loss on hold-out set: 0.45358750851530777
MeanAbsoluteError value on hold-out data: 0.6493239998817444
Epoch: 37
Loss on hold-out set: 0.4529598269023393
MeanAbsoluteError value on hold-out data: 0.6488447785377502
Epoch: 38
Loss on hold-out set: 0.45496590121796254
MeanAbsoluteError value on hold-out data: 0.6508914232254028
Epoch: 39
Loss on hold-out set: 0.4567106726922487
MeanAbsoluteError value on hold-out data: 0.6524199843406677
Epoch: 40
Loss on hold-out set: 0.45581202914840296
MeanAbsoluteError value on hold-out data: 0.6514948010444641
Epoch: 41
Loss on hold-out set: 0.4554611115079177
MeanAbsoluteError value on hold-out data: 0.6506933569908142
Epoch: 42
Loss on hold-out set: 0.4532644387922789
MeanAbsoluteError value on hold-out data: 0.6492961645126343
Epoch: 43
Loss on hold-out set: 0.45394236790506465
MeanAbsoluteError value on hold-out data: 0.6494168639183044
Epoch: 44</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4535429516905232
MeanAbsoluteError value on hold-out data: 0.6494808197021484
Epoch: 45
Loss on hold-out set: 0.4547828020233857
MeanAbsoluteError value on hold-out data: 0.6497856974601746
Epoch: 46
Loss on hold-out set: 0.4545118989128816
MeanAbsoluteError value on hold-out data: 0.6500173211097717
Epoch: 47
Loss on hold-out set: 0.4523406075803857
MeanAbsoluteError value on hold-out data: 0.6484460234642029
Epoch: 48
Loss on hold-out set: 0.45267852434986516
MeanAbsoluteError value on hold-out data: 0.6489351391792297
Epoch: 49
Loss on hold-out set: 0.4535845941618869
MeanAbsoluteError value on hold-out data: 0.6492393016815186
Epoch: 50
Loss on hold-out set: 0.4537069475964496
MeanAbsoluteError value on hold-out data: 0.6499642729759216
Epoch: 51
Loss on hold-out set: 0.45489380783156347
MeanAbsoluteError value on hold-out data: 0.6506103277206421
Epoch: 52
Loss on hold-out set: 0.452504049790533
MeanAbsoluteError value on hold-out data: 0.6487504243850708
Epoch: 53
Loss on hold-out set: 0.45203583020912974
MeanAbsoluteError value on hold-out data: 0.648635745048523
Epoch: 54
Loss on hold-out set: 0.4535038831986879
MeanAbsoluteError value on hold-out data: 0.6492515206336975
Epoch: 55</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.45310733977117035
MeanAbsoluteError value on hold-out data: 0.6495614647865295
Epoch: 56
Loss on hold-out set: 0.453528745394004
MeanAbsoluteError value on hold-out data: 0.6494340300559998
Epoch: 57
Loss on hold-out set: 0.4530581337840934
MeanAbsoluteError value on hold-out data: 0.6490804553031921
Epoch: 58
Loss on hold-out set: 0.4540705453408392
MeanAbsoluteError value on hold-out data: 0.6495937705039978
Epoch: 59
Loss on hold-out set: 0.4543669427696027
MeanAbsoluteError value on hold-out data: 0.6501813530921936
Epoch: 60
Loss on hold-out set: 0.45246423231927974
MeanAbsoluteError value on hold-out data: 0.6484190821647644
Epoch: 61
Loss on hold-out set: 0.45014405799539464
MeanAbsoluteError value on hold-out data: 0.6469748020172119
Epoch: 62
Loss on hold-out set: 0.4557330953447442
MeanAbsoluteError value on hold-out data: 0.6506618857383728
Epoch: 63
Loss on hold-out set: 0.4508426573715712
MeanAbsoluteError value on hold-out data: 0.6475402116775513
Epoch: 64
Loss on hold-out set: 0.45314121011056396
MeanAbsoluteError value on hold-out data: 0.6491293907165527
Epoch: 65
Loss on hold-out set: 0.45261439916334656
MeanAbsoluteError value on hold-out data: 0.6486645936965942
Epoch: 66</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4515979501761888
MeanAbsoluteError value on hold-out data: 0.6481348872184753
Epoch: 67
Loss on hold-out set: 0.45172497316410665
MeanAbsoluteError value on hold-out data: 0.648300290107727
Epoch: 68
Loss on hold-out set: 0.4508030885144284
MeanAbsoluteError value on hold-out data: 0.6472572088241577
Epoch: 69
Loss on hold-out set: 0.4539273004782827
MeanAbsoluteError value on hold-out data: 0.6490530967712402
Epoch: 70
Loss on hold-out set: 0.4526115687269914
MeanAbsoluteError value on hold-out data: 0.6488827466964722
Epoch: 71
Loss on hold-out set: 0.4541605852152172
MeanAbsoluteError value on hold-out data: 0.6494703888893127
Epoch: 72
Loss on hold-out set: 0.45270411042790665
MeanAbsoluteError value on hold-out data: 0.6489551663398743
Epoch: 73
Loss on hold-out set: 0.4520396590232849
MeanAbsoluteError value on hold-out data: 0.6479901075363159
Epoch: 74
Loss on hold-out set: 0.45378051306072037
MeanAbsoluteError value on hold-out data: 0.6492975354194641
Epoch: 75
Loss on hold-out set: 0.450921799007215
MeanAbsoluteError value on hold-out data: 0.6474267840385437
Epoch: 76
Loss on hold-out set: 0.4542545684074101
MeanAbsoluteError value on hold-out data: 0.650068998336792
Epoch: 77</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4513097781884043
MeanAbsoluteError value on hold-out data: 0.6477612853050232
Epoch: 78
Loss on hold-out set: 0.4510274945121062
MeanAbsoluteError value on hold-out data: 0.6470417380332947
Epoch: 79
Loss on hold-out set: 0.4519781650681245
MeanAbsoluteError value on hold-out data: 0.6483672857284546
Epoch: 80
Loss on hold-out set: 0.45214405106870753
MeanAbsoluteError value on hold-out data: 0.6486579775810242
Epoch: 81
Loss on hold-out set: 0.45095630068528025
MeanAbsoluteError value on hold-out data: 0.6472180485725403
Epoch: 82
Loss on hold-out set: 0.4504508564346715
MeanAbsoluteError value on hold-out data: 0.6472318768501282
Epoch: 83
Loss on hold-out set: 0.45189395939048965
MeanAbsoluteError value on hold-out data: 0.6481188535690308
Epoch: 84
Loss on hold-out set: 0.44999781878370987
MeanAbsoluteError value on hold-out data: 0.646296501159668
Epoch: 85
Loss on hold-out set: 0.45152992872815384
MeanAbsoluteError value on hold-out data: 0.6477870941162109
Epoch: 86
Loss on hold-out set: 0.4488496145135478
MeanAbsoluteError value on hold-out data: 0.6458466053009033
Epoch: 87
Loss on hold-out set: 0.4495000760806234
MeanAbsoluteError value on hold-out data: 0.6469359993934631
Epoch: 88</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4504060298204422
MeanAbsoluteError value on hold-out data: 0.6467682123184204
Epoch: 89
Loss on hold-out set: 0.4479993079838
MeanAbsoluteError value on hold-out data: 0.645456075668335
Epoch: 90
Loss on hold-out set: 0.4511445031354302
MeanAbsoluteError value on hold-out data: 0.6470165252685547
Epoch: 91
Loss on hold-out set: 0.4507835648561779
MeanAbsoluteError value on hold-out data: 0.6472745537757874
Epoch: 92
Loss on hold-out set: 0.45019401609897614
MeanAbsoluteError value on hold-out data: 0.6470003128051758
Epoch: 93
Loss on hold-out set: 0.451634932505457
MeanAbsoluteError value on hold-out data: 0.6483078002929688
Epoch: 94
Loss on hold-out set: 0.4503865140049081
MeanAbsoluteError value on hold-out data: 0.6471909284591675
Epoch: 95
Loss on hold-out set: 0.4515090365158884
MeanAbsoluteError value on hold-out data: 0.6474428176879883
Epoch: 96
Loss on hold-out set: 0.44951579680568293
MeanAbsoluteError value on hold-out data: 0.6459831595420837
Epoch: 97
Loss on hold-out set: 0.45158388583283676
MeanAbsoluteError value on hold-out data: 0.6478838920593262
Epoch: 98
Loss on hold-out set: 0.44928121017782313
MeanAbsoluteError value on hold-out data: 0.6458011269569397
Epoch: 99</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4469640592211171
MeanAbsoluteError value on hold-out data: 0.6448303461074829
Epoch: 100
Loss on hold-out set: 0.44917522684523936
MeanAbsoluteError value on hold-out data: 0.6462762951850891
Epoch: 101
Loss on hold-out set: 0.4470257359115701
MeanAbsoluteError value on hold-out data: 0.6440426111221313
Epoch: 102
Loss on hold-out set: 0.44675874474801514
MeanAbsoluteError value on hold-out data: 0.6443750262260437
Epoch: 103
Loss on hold-out set: 0.4480649797539962
MeanAbsoluteError value on hold-out data: 0.6452566385269165
Epoch: 104
Loss on hold-out set: 0.44959760731772375
MeanAbsoluteError value on hold-out data: 0.6465162634849548
Epoch: 105
Loss on hold-out set: 0.44940275424405146
MeanAbsoluteError value on hold-out data: 0.6460399627685547
Epoch: 106
Loss on hold-out set: 0.44725412444064494
MeanAbsoluteError value on hold-out data: 0.6448283791542053
Epoch: 107
Loss on hold-out set: 0.4493673545749564
MeanAbsoluteError value on hold-out data: 0.646686851978302
Epoch: 108
Loss on hold-out set: 0.4491570239004336
MeanAbsoluteError value on hold-out data: 0.6460201740264893
Epoch: 109
Loss on hold-out set: 0.4494960457086563
MeanAbsoluteError value on hold-out data: 0.6462743878364563
Epoch: 110</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.44923827993242366
MeanAbsoluteError value on hold-out data: 0.6455119252204895
Epoch: 111
Loss on hold-out set: 0.4480784237384796
MeanAbsoluteError value on hold-out data: 0.6451811790466309
Epoch: 112
Loss on hold-out set: 0.4494582022491254
MeanAbsoluteError value on hold-out data: 0.6459540724754333
Epoch: 113
Loss on hold-out set: 0.4489768970953791
MeanAbsoluteError value on hold-out data: 0.6459988355636597
Epoch: 114
Loss on hold-out set: 0.44773849217515244
MeanAbsoluteError value on hold-out data: 0.6452873349189758
Epoch: 115
Loss on hold-out set: 0.4495871067047119
MeanAbsoluteError value on hold-out data: 0.6465311050415039
Epoch: 116
Loss on hold-out set: 0.4493013996826975
MeanAbsoluteError value on hold-out data: 0.6457881927490234
Epoch: 117
Loss on hold-out set: 0.4466182142496109
MeanAbsoluteError value on hold-out data: 0.6442474126815796
Epoch: 118
Loss on hold-out set: 0.44765500332179825
MeanAbsoluteError value on hold-out data: 0.6448273062705994
Epoch: 119
Loss on hold-out set: 0.4508525854662845
MeanAbsoluteError value on hold-out data: 0.647330105304718
Epoch: 120
Loss on hold-out set: 0.4494436853810361
MeanAbsoluteError value on hold-out data: 0.646956741809845
Epoch: 121</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4461610309387508
MeanAbsoluteError value on hold-out data: 0.6438244581222534
Epoch: 122
Loss on hold-out set: 0.44799703594885376
MeanAbsoluteError value on hold-out data: 0.6448424458503723
Epoch: 123
Loss on hold-out set: 0.44731765122790085
MeanAbsoluteError value on hold-out data: 0.6447571516036987
Epoch: 124
Loss on hold-out set: 0.444920246538363
MeanAbsoluteError value on hold-out data: 0.6431748867034912
Epoch: 125
Loss on hold-out set: 0.45183343557935013
MeanAbsoluteError value on hold-out data: 0.6477536559104919
Epoch: 126
Loss on hold-out set: 0.4456331259325931
MeanAbsoluteError value on hold-out data: 0.6437221169471741
Epoch: 127
Loss on hold-out set: 0.4480566264767396
MeanAbsoluteError value on hold-out data: 0.644836962223053
Epoch: 128
Loss on hold-out set: 0.44945975353843287
MeanAbsoluteError value on hold-out data: 0.6465322971343994
Epoch: 129
Loss on hold-out set: 0.4483418966594495
MeanAbsoluteError value on hold-out data: 0.6456575989723206
Epoch: 130
Loss on hold-out set: 0.44642814209586695
MeanAbsoluteError value on hold-out data: 0.6436738967895508
Epoch: 131
Loss on hold-out set: 0.4477359221169823
MeanAbsoluteError value on hold-out data: 0.6445709466934204
Epoch: 132</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4494143528373618
MeanAbsoluteError value on hold-out data: 0.646141767501831
Epoch: 133
Loss on hold-out set: 0.4477832960455041
MeanAbsoluteError value on hold-out data: 0.6449248194694519
Epoch: 134
Loss on hold-out set: 0.4475400283148414
MeanAbsoluteError value on hold-out data: 0.644883394241333
Epoch: 135
Loss on hold-out set: 0.44694959646777105
MeanAbsoluteError value on hold-out data: 0.6440098285675049
Epoch: 136
Loss on hold-out set: 0.44745009588567836
MeanAbsoluteError value on hold-out data: 0.6444754600524902
Epoch: 137
Loss on hold-out set: 0.4491337506394637
MeanAbsoluteError value on hold-out data: 0.6458517909049988
Epoch: 138
Loss on hold-out set: 0.4462098887092189
MeanAbsoluteError value on hold-out data: 0.6438659429550171
Epoch: 139
Loss on hold-out set: 0.4469732822556245
MeanAbsoluteError value on hold-out data: 0.6444832682609558
Epoch: 140
Loss on hold-out set: 0.4474699944257736
MeanAbsoluteError value on hold-out data: 0.6446747183799744
Epoch: 141
Loss on hold-out set: 0.4471552270023446
MeanAbsoluteError value on hold-out data: 0.6447279453277588
Epoch: 142
Loss on hold-out set: 0.4460928479307576
MeanAbsoluteError value on hold-out data: 0.6436082720756531
Epoch: 143</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.44769378947584254
MeanAbsoluteError value on hold-out data: 0.6452122926712036
Epoch: 144
Loss on hold-out set: 0.4452583836881738
MeanAbsoluteError value on hold-out data: 0.6430895328521729
Epoch: 145
Loss on hold-out set: 0.44615233650333
MeanAbsoluteError value on hold-out data: 0.6435401439666748
Epoch: 146
Loss on hold-out set: 0.44720305659269033
MeanAbsoluteError value on hold-out data: 0.6444634795188904
Epoch: 147
Loss on hold-out set: 0.4465066704310869
MeanAbsoluteError value on hold-out data: 0.6435976624488831
Epoch: 148
Loss on hold-out set: 0.44701012890589864
MeanAbsoluteError value on hold-out data: 0.6445486545562744
Epoch: 149
Loss on hold-out set: 0.4481064614496733
MeanAbsoluteError value on hold-out data: 0.6452943682670593
Epoch: 150
Loss on hold-out set: 0.4473157603489725
MeanAbsoluteError value on hold-out data: 0.6443766355514526
Epoch: 151
Loss on hold-out set: 0.44725596983181803
MeanAbsoluteError value on hold-out data: 0.6446670293807983
Epoch: 152
Loss on hold-out set: 0.44760294101740183
MeanAbsoluteError value on hold-out data: 0.6446340084075928
Epoch: 153
Loss on hold-out set: 0.4489342931069826
MeanAbsoluteError value on hold-out data: 0.6455261707305908
Epoch: 154</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.44462757832125616
MeanAbsoluteError value on hold-out data: 0.6422571539878845
Epoch: 155
Loss on hold-out set: 0.4439422001964168
MeanAbsoluteError value on hold-out data: 0.6425483226776123
Epoch: 156
Loss on hold-out set: 0.44435900841888626
MeanAbsoluteError value on hold-out data: 0.642622709274292
Epoch: 157
Loss on hold-out set: 0.4466083010560588
MeanAbsoluteError value on hold-out data: 0.6440661549568176
Epoch: 158
Loss on hold-out set: 0.4454694123644578
MeanAbsoluteError value on hold-out data: 0.6435724496841431
Epoch: 159
Loss on hold-out set: 0.44492243779333013
MeanAbsoluteError value on hold-out data: 0.6430906653404236
Epoch: 160
Loss on hold-out set: 0.4472875508822893
MeanAbsoluteError value on hold-out data: 0.6441663503646851
Epoch: 161
Loss on hold-out set: 0.44644741165010554
MeanAbsoluteError value on hold-out data: 0.6444502472877502
Epoch: 162
Loss on hold-out set: 0.4432135930186824
MeanAbsoluteError value on hold-out data: 0.6417235732078552
Epoch: 163
Loss on hold-out set: 0.44659182194032165
MeanAbsoluteError value on hold-out data: 0.6442509889602661
Epoch: 164
Loss on hold-out set: 0.4454674689393294
MeanAbsoluteError value on hold-out data: 0.6434155702590942
Epoch: 165</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4450047337695172
MeanAbsoluteError value on hold-out data: 0.6424617767333984
Epoch: 166
Loss on hold-out set: 0.44542016245816884
MeanAbsoluteError value on hold-out data: 0.6432127356529236
Epoch: 167
Loss on hold-out set: 0.44552093587423625
MeanAbsoluteError value on hold-out data: 0.6433373689651489
Epoch: 168
Loss on hold-out set: 0.4436937476459302
MeanAbsoluteError value on hold-out data: 0.6412121057510376
Epoch: 169
Loss on hold-out set: 0.44578569970632853
MeanAbsoluteError value on hold-out data: 0.643306314945221
Epoch: 170
Loss on hold-out set: 0.44649074971675873
MeanAbsoluteError value on hold-out data: 0.6440760493278503
Epoch: 171
Loss on hold-out set: 0.44641782029678945
MeanAbsoluteError value on hold-out data: 0.6443111300468445
Epoch: 172
Loss on hold-out set: 0.44727252110054616
MeanAbsoluteError value on hold-out data: 0.6446658372879028
Epoch: 173
Loss on hold-out set: 0.4454742654373771
MeanAbsoluteError value on hold-out data: 0.6435269117355347
Epoch: 174
Loss on hold-out set: 0.44522760651613535
MeanAbsoluteError value on hold-out data: 0.6431772112846375
Epoch: 175
Loss on hold-out set: 0.4436156632084596
MeanAbsoluteError value on hold-out data: 0.641654372215271
Epoch: 176</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4454385740192313
MeanAbsoluteError value on hold-out data: 0.64328932762146
Epoch: 177
Loss on hold-out set: 0.4471203943616466
MeanAbsoluteError value on hold-out data: 0.6444379687309265
Epoch: 178
Loss on hold-out set: 0.4487398831467879
MeanAbsoluteError value on hold-out data: 0.6461318135261536
Epoch: 179
Loss on hold-out set: 0.44412824668382345
MeanAbsoluteError value on hold-out data: 0.6427754163742065
Epoch: 180
Loss on hold-out set: 0.44386516825148936
MeanAbsoluteError value on hold-out data: 0.6423103213310242
Epoch: 181
Loss on hold-out set: 0.4465391463355014
MeanAbsoluteError value on hold-out data: 0.6438944935798645
Epoch: 182
Loss on hold-out set: 0.4466170488219512
MeanAbsoluteError value on hold-out data: 0.6440552473068237
Epoch: 183
Loss on hold-out set: 0.4459905898884723
MeanAbsoluteError value on hold-out data: 0.643480658531189
Epoch: 184
Loss on hold-out set: 0.4445395893172214
MeanAbsoluteError value on hold-out data: 0.6425812840461731
Epoch: 185
Loss on hold-out set: 0.44426003569050837
MeanAbsoluteError value on hold-out data: 0.6424267292022705
Epoch: 186
Loss on hold-out set: 0.44273590022011805
MeanAbsoluteError value on hold-out data: 0.6411311626434326
Epoch: 187</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.44620092134726674
MeanAbsoluteError value on hold-out data: 0.6434767246246338
Epoch: 188
Loss on hold-out set: 0.4455458717910867
MeanAbsoluteError value on hold-out data: 0.6436594128608704
Epoch: 189
Loss on hold-out set: 0.44562535458489466
MeanAbsoluteError value on hold-out data: 0.6434465050697327
Epoch: 190
Loss on hold-out set: 0.44387538731098175
MeanAbsoluteError value on hold-out data: 0.6418722867965698
Epoch: 191
Loss on hold-out set: 0.44489880376740504
MeanAbsoluteError value on hold-out data: 0.6430019736289978
Epoch: 192
Loss on hold-out set: 0.4452018996602611
MeanAbsoluteError value on hold-out data: 0.6431523561477661
Epoch: 193
Loss on hold-out set: 0.44444681390335683
MeanAbsoluteError value on hold-out data: 0.642634391784668
Epoch: 194
Loss on hold-out set: 0.44494262416111796
MeanAbsoluteError value on hold-out data: 0.643007218837738
Epoch: 195
Loss on hold-out set: 0.443953170588142
MeanAbsoluteError value on hold-out data: 0.6419922113418579
Epoch: 196
Loss on hold-out set: 0.4445572620943973
MeanAbsoluteError value on hold-out data: 0.6425214409828186
Epoch: 197
Loss on hold-out set: 0.44385663694457006
MeanAbsoluteError value on hold-out data: 0.6418905258178711
Epoch: 198</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.44591419555638967
MeanAbsoluteError value on hold-out data: 0.6438300013542175
Epoch: 199
Loss on hold-out set: 0.4439554983063748
MeanAbsoluteError value on hold-out data: 0.6418514847755432
Epoch: 200
Loss on hold-out set: 0.4451851131100404
MeanAbsoluteError value on hold-out data: 0.6431715488433838
Epoch: 201
Loss on hold-out set: 0.44343702651952444
MeanAbsoluteError value on hold-out data: 0.6414101123809814
Epoch: 202
Loss on hold-out set: 0.4440472369131289
MeanAbsoluteError value on hold-out data: 0.6423078179359436
Epoch: 203
Loss on hold-out set: 0.4447176574092162
MeanAbsoluteError value on hold-out data: 0.6428780555725098
Epoch: 204
Loss on hold-out set: 0.44440154652846486
MeanAbsoluteError value on hold-out data: 0.6419203281402588
Epoch: 205
Loss on hold-out set: 0.4421763522060294
MeanAbsoluteError value on hold-out data: 0.6404064297676086
Epoch: 206
Loss on hold-out set: 0.4457006313298878
MeanAbsoluteError value on hold-out data: 0.6436251401901245
Epoch: 207
Loss on hold-out set: 0.44454751359788997
MeanAbsoluteError value on hold-out data: 0.642621636390686
Epoch: 208
Loss on hold-out set: 0.44438493408654867
MeanAbsoluteError value on hold-out data: 0.6424214839935303
Epoch: 209</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.44334235081547185
MeanAbsoluteError value on hold-out data: 0.6415068507194519
Epoch: 210
Loss on hold-out set: 0.44537024199962616
MeanAbsoluteError value on hold-out data: 0.642821729183197
Epoch: 211
Loss on hold-out set: 0.44398138319191177
MeanAbsoluteError value on hold-out data: 0.6423727869987488
Epoch: 212
Loss on hold-out set: 0.44434016942977905
MeanAbsoluteError value on hold-out data: 0.6425963044166565
Epoch: 213
Loss on hold-out set: 0.4416431925798717
MeanAbsoluteError value on hold-out data: 0.6402907967567444
Epoch: 214
Loss on hold-out set: 0.44373079193265813
MeanAbsoluteError value on hold-out data: 0.6419054269790649
Epoch: 215
Loss on hold-out set: 0.44539355917980794
MeanAbsoluteError value on hold-out data: 0.6431086659431458
Epoch: 216
Loss on hold-out set: 0.4422860004399952
MeanAbsoluteError value on hold-out data: 0.6410336494445801
Epoch: 217
Loss on hold-out set: 0.443346967822627
MeanAbsoluteError value on hold-out data: 0.6411227583885193
Epoch: 218
Loss on hold-out set: 0.4438603598820536
MeanAbsoluteError value on hold-out data: 0.6418833136558533
Epoch: 219
Loss on hold-out set: 0.44261356874516133
MeanAbsoluteError value on hold-out data: 0.6411417126655579
Epoch: 220</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.44273753621076284
MeanAbsoluteError value on hold-out data: 0.6411751508712769
Epoch: 221
Loss on hold-out set: 0.44178157338970586
MeanAbsoluteError value on hold-out data: 0.6405621767044067
Epoch: 222
Loss on hold-out set: 0.44207231073003067
MeanAbsoluteError value on hold-out data: 0.6404234766960144
Epoch: 223
Loss on hold-out set: 0.44361892656276103
MeanAbsoluteError value on hold-out data: 0.6414068341255188
Epoch: 224
Loss on hold-out set: 0.4446931991137956
MeanAbsoluteError value on hold-out data: 0.642447829246521
Epoch: 225
Loss on hold-out set: 0.44398548963822815
MeanAbsoluteError value on hold-out data: 0.6420159935951233
Epoch: 226
Loss on hold-out set: 0.44065564635552856
MeanAbsoluteError value on hold-out data: 0.6397602558135986
Epoch: 227
Loss on hold-out set: 0.4425684339121768
MeanAbsoluteError value on hold-out data: 0.6409175395965576
Epoch: 228
Loss on hold-out set: 0.44313475489616394
MeanAbsoluteError value on hold-out data: 0.6416362524032593
Epoch: 229
Loss on hold-out set: 0.44173709577635717
MeanAbsoluteError value on hold-out data: 0.6405831575393677
Epoch: 230
Loss on hold-out set: 0.4451717091234107
MeanAbsoluteError value on hold-out data: 0.6431043744087219
Epoch: 231</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.44248127780462565
MeanAbsoluteError value on hold-out data: 0.6409093141555786
Epoch: 232
Loss on hold-out set: 0.4414456737668891
MeanAbsoluteError value on hold-out data: 0.6401420831680298
Epoch: 233
Loss on hold-out set: 0.4415855454771142
MeanAbsoluteError value on hold-out data: 0.6400477886199951
Epoch: 234
Loss on hold-out set: 0.44236734274186584
MeanAbsoluteError value on hold-out data: 0.6411596536636353
Epoch: 235
Loss on hold-out set: 0.4407345723164709
MeanAbsoluteError value on hold-out data: 0.6393563151359558
Epoch: 236
Loss on hold-out set: 0.4439612195680016
MeanAbsoluteError value on hold-out data: 0.6420528888702393
Epoch: 237
Loss on hold-out set: 0.4431019354807703
MeanAbsoluteError value on hold-out data: 0.6414679884910583
Epoch: 238
Loss on hold-out set: 0.44145549206357254
MeanAbsoluteError value on hold-out data: 0.6400941610336304
Epoch: 239
Loss on hold-out set: 0.4439332869492079
MeanAbsoluteError value on hold-out data: 0.6423124074935913
Epoch: 240
Loss on hold-out set: 0.44171480128639623
MeanAbsoluteError value on hold-out data: 0.6405342221260071
Epoch: 241
Loss on hold-out set: 0.44312768860867147
MeanAbsoluteError value on hold-out data: 0.6416229009628296
Epoch: 242</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4426751583814621
MeanAbsoluteError value on hold-out data: 0.6409239768981934
Epoch: 243
Loss on hold-out set: 0.4426503055974057
MeanAbsoluteError value on hold-out data: 0.6409813165664673
Epoch: 244
Loss on hold-out set: 0.4426125217425196
MeanAbsoluteError value on hold-out data: 0.6410447955131531
Epoch: 245
Loss on hold-out set: 0.44307820028380346
MeanAbsoluteError value on hold-out data: 0.6418214440345764
Epoch: 246
Loss on hold-out set: 0.4421319443928568
MeanAbsoluteError value on hold-out data: 0.6402639746665955
Epoch: 247
Loss on hold-out set: 0.4418956362887433
MeanAbsoluteError value on hold-out data: 0.6409250497817993
Epoch: 248
Loss on hold-out set: 0.44191325024554606
MeanAbsoluteError value on hold-out data: 0.6407129764556885
Epoch: 249
Loss on hold-out set: 0.4434626674965808
MeanAbsoluteError value on hold-out data: 0.6419398784637451
Epoch: 250
Loss on hold-out set: 0.44221042253469167
MeanAbsoluteError value on hold-out data: 0.640393853187561
Epoch: 251
Loss on hold-out set: 0.4429532893394169
MeanAbsoluteError value on hold-out data: 0.6408767700195312
Epoch: 252
Loss on hold-out set: 0.4416337805358987
MeanAbsoluteError value on hold-out data: 0.6402255892753601
Epoch: 253</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.44213949849731043
MeanAbsoluteError value on hold-out data: 0.6404075026512146
Epoch: 254
Loss on hold-out set: 0.4434988028124759
MeanAbsoluteError value on hold-out data: 0.6415325403213501
Epoch: 255
Loss on hold-out set: 0.4415612009010817
MeanAbsoluteError value on hold-out data: 0.6398729681968689
Epoch: 256
Loss on hold-out set: 0.4409936603746916
MeanAbsoluteError value on hold-out data: 0.6400665044784546
Returned to Spot: Validation loss: 0.4409936603746916
----------------------------------------------

config: {'_L_in': 10, '_L_out': 1, 'l1': 8, 'dropout_prob': 0.5015226665924828, 'lr_mult': 0.001, 'batch_size': 16, 'epochs': 16384, 'k_folds': 1, 'patience': 8, 'optimizer': 'NAdam', 'sgd_momentum': 0.9}
Epoch: 1
Loss on hold-out set: 0.0907599024082485
MeanAbsoluteError value on hold-out data: 0.24761947989463806
Epoch: 2
Loss on hold-out set: 0.10860289790128407
MeanAbsoluteError value on hold-out data: 0.26934292912483215
Epoch: 3
Loss on hold-out set: 0.10374700062369045
MeanAbsoluteError value on hold-out data: 0.2640213668346405
Epoch: 4
Loss on hold-out set: 0.10466699400230457
MeanAbsoluteError value on hold-out data: 0.26435166597366333
Epoch: 5
Loss on hold-out set: 0.09770887031366951
MeanAbsoluteError value on hold-out data: 0.26091867685317993
Epoch: 6
Loss on hold-out set: 0.10279451124370098
MeanAbsoluteError value on hold-out data: 0.2613462805747986
Epoch: 7
Loss on hold-out set: 0.10051078074856808
MeanAbsoluteError value on hold-out data: 0.2552301585674286
Epoch: 8
Loss on hold-out set: 0.0946777564914603
MeanAbsoluteError value on hold-out data: 0.2504766285419464
Epoch: 9
Loss on hold-out set: 0.10294534658130847
MeanAbsoluteError value on hold-out data: 0.26373469829559326
Early stopping at epoch 8
Returned to Spot: Validation loss: 0.10294534658130847
----------------------------------------------

config: {'_L_in': 10, '_L_out': 1, 'l1': 16, 'dropout_prob': 0.26673029336651144, 'lr_mult': 0.001, 'batch_size': 8, 'epochs': 16, 'k_folds': 1, 'patience': 16, 'optimizer': 'Adamax', 'sgd_momentum': 0.9}
Epoch: 1
Loss on hold-out set: 0.5555819633759951
MeanAbsoluteError value on hold-out data: 0.7212883234024048
Epoch: 2</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.5540642087396822
MeanAbsoluteError value on hold-out data: 0.7214484810829163
Epoch: 3
Loss on hold-out set: 0.556898232353361
MeanAbsoluteError value on hold-out data: 0.7224522829055786
Epoch: 4
Loss on hold-out set: 0.5594656977214312
MeanAbsoluteError value on hold-out data: 0.72368985414505
Epoch: 5
Loss on hold-out set: 0.5529431826189944
MeanAbsoluteError value on hold-out data: 0.7202922701835632
Epoch: 6
Loss on hold-out set: 0.5517641094170118
MeanAbsoluteError value on hold-out data: 0.7199397683143616
Epoch: 7
Loss on hold-out set: 0.5546157179694426
MeanAbsoluteError value on hold-out data: 0.7200890183448792
Epoch: 8
Loss on hold-out set: 0.5550972231124577
MeanAbsoluteError value on hold-out data: 0.7193150520324707
Epoch: 9
Loss on hold-out set: 0.5524471347269259
MeanAbsoluteError value on hold-out data: 0.7186596989631653
Epoch: 10
Loss on hold-out set: 0.5537106959443343
MeanAbsoluteError value on hold-out data: 0.7196913957595825
Epoch: 11
Loss on hold-out set: 0.5398693155301245
MeanAbsoluteError value on hold-out data: 0.7107371687889099
Epoch: 12</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.5489669324536073
MeanAbsoluteError value on hold-out data: 0.7162120938301086
Epoch: 13
Loss on hold-out set: 0.5486662270207154
MeanAbsoluteError value on hold-out data: 0.717420220375061
Epoch: 14
Loss on hold-out set: 0.5455439576977178
MeanAbsoluteError value on hold-out data: 0.7140594720840454
Epoch: 15
Loss on hold-out set: 0.5542420391973696
MeanAbsoluteError value on hold-out data: 0.7209203839302063
Epoch: 16
Loss on hold-out set: 0.5465334559741774
MeanAbsoluteError value on hold-out data: 0.7153280973434448
Returned to Spot: Validation loss: 0.5465334559741774
----------------------------------------------

config: {'_L_in': 10, '_L_out': 1, 'l1': 128, 'dropout_prob': 0.8973189149831583, 'lr_mult': 0.001, 'batch_size': 2, 'epochs': 2048, 'k_folds': 1, 'patience': 32, 'optimizer': 'Adam', 'sgd_momentum': 0.9}
Epoch: 1
Loss on hold-out set: 0.35942203680829454
MeanAbsoluteError value on hold-out data: 0.5473533868789673
Epoch: 2</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.340117316551817
MeanAbsoluteError value on hold-out data: 0.5312947630882263
Epoch: 3
Loss on hold-out set: 0.3288393857081731
MeanAbsoluteError value on hold-out data: 0.5259130001068115
Epoch: 4
Loss on hold-out set: 0.3357589885592461
MeanAbsoluteError value on hold-out data: 0.533232569694519
Epoch: 5</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3190875634054343
MeanAbsoluteError value on hold-out data: 0.5129112005233765
Epoch: 6
Loss on hold-out set: 0.3325934727489948
MeanAbsoluteError value on hold-out data: 0.5251381397247314
Epoch: 7
Loss on hold-out set: 0.3181713222960631
MeanAbsoluteError value on hold-out data: 0.5162366032600403
Epoch: 8</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.32848209273070095
MeanAbsoluteError value on hold-out data: 0.515327513217926
Epoch: 9
Loss on hold-out set: 0.32387858025729654
MeanAbsoluteError value on hold-out data: 0.5157452821731567
Epoch: 10
Loss on hold-out set: 0.32641185755841434
MeanAbsoluteError value on hold-out data: 0.5214706659317017
Epoch: 11</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3068516902656605
MeanAbsoluteError value on hold-out data: 0.49719303846359253
Epoch: 12
Loss on hold-out set: 0.28916342412897694
MeanAbsoluteError value on hold-out data: 0.4829237461090088
Epoch: 13
Loss on hold-out set: 0.29739811081749695
MeanAbsoluteError value on hold-out data: 0.49592065811157227
Epoch: 14</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3047691786661744
MeanAbsoluteError value on hold-out data: 0.4996480345726013
Epoch: 15
Loss on hold-out set: 0.3049532512660759
MeanAbsoluteError value on hold-out data: 0.49422740936279297
Epoch: 16
Loss on hold-out set: 0.29377639580518006
MeanAbsoluteError value on hold-out data: 0.48742273449897766
Epoch: 17</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2914588878427943
MeanAbsoluteError value on hold-out data: 0.4858555495738983
Epoch: 18
Loss on hold-out set: 0.29059613360712927
MeanAbsoluteError value on hold-out data: 0.487634539604187
Epoch: 19
Loss on hold-out set: 0.2918362336481611
MeanAbsoluteError value on hold-out data: 0.48726415634155273
Epoch: 20</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.289655896712405
MeanAbsoluteError value on hold-out data: 0.4864009618759155
Epoch: 21
Loss on hold-out set: 0.27464142425606647
MeanAbsoluteError value on hold-out data: 0.4757027328014374
Epoch: 22
Loss on hold-out set: 0.2765393829656144
MeanAbsoluteError value on hold-out data: 0.4726284146308899
Epoch: 23</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.27149822041780375
MeanAbsoluteError value on hold-out data: 0.46224120259284973
Epoch: 24
Loss on hold-out set: 0.2678913458126287
MeanAbsoluteError value on hold-out data: 0.4624612033367157
Epoch: 25
Loss on hold-out set: 0.2523702985420823
MeanAbsoluteError value on hold-out data: 0.45226627588272095
Epoch: 26</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2567014840990305
MeanAbsoluteError value on hold-out data: 0.45393162965774536
Epoch: 27
Loss on hold-out set: 0.29154734456290804
MeanAbsoluteError value on hold-out data: 0.49090439081192017
Epoch: 28
Loss on hold-out set: 0.28075619449838995
MeanAbsoluteError value on hold-out data: 0.4758870303630829
Epoch: 29</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2599267060759788
MeanAbsoluteError value on hold-out data: 0.4552158713340759
Epoch: 30
Loss on hold-out set: 0.23721521511363486
MeanAbsoluteError value on hold-out data: 0.43247750401496887
Epoch: 31
Loss on hold-out set: 0.25486929510099193
MeanAbsoluteError value on hold-out data: 0.44886133074760437
Epoch: 32</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.24667249428108334
MeanAbsoluteError value on hold-out data: 0.44277894496917725
Epoch: 33
Loss on hold-out set: 0.23841586945268015
MeanAbsoluteError value on hold-out data: 0.43295714259147644
Epoch: 34
Loss on hold-out set: 0.23702610011639383
MeanAbsoluteError value on hold-out data: 0.42894449830055237
Epoch: 35</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.24268234237407646
MeanAbsoluteError value on hold-out data: 0.4402633607387543
Epoch: 36
Loss on hold-out set: 0.24817818319735427
MeanAbsoluteError value on hold-out data: 0.43655073642730713
Epoch: 37
Loss on hold-out set: 0.23161101255876324
MeanAbsoluteError value on hold-out data: 0.4279007613658905
Epoch: 38</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.22981314857645582
MeanAbsoluteError value on hold-out data: 0.427016943693161
Epoch: 39
Loss on hold-out set: 0.21386192453171438
MeanAbsoluteError value on hold-out data: 0.4096103012561798
Epoch: 40
Loss on hold-out set: 0.2272342791687697
MeanAbsoluteError value on hold-out data: 0.4223040044307709
Epoch: 41</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2266827812915047
MeanAbsoluteError value on hold-out data: 0.4165513813495636
Epoch: 42
Loss on hold-out set: 0.22464013841934502
MeanAbsoluteError value on hold-out data: 0.42042240500450134
Epoch: 43
Loss on hold-out set: 0.21538425634770345
MeanAbsoluteError value on hold-out data: 0.40679508447647095
Epoch: 44</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.22766628672834485
MeanAbsoluteError value on hold-out data: 0.41749778389930725
Epoch: 45
Loss on hold-out set: 0.22404516187186044
MeanAbsoluteError value on hold-out data: 0.4167960584163666
Epoch: 46
Loss on hold-out set: 0.20977662979159503
MeanAbsoluteError value on hold-out data: 0.398589551448822
Epoch: 47</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.22256495270257196
MeanAbsoluteError value on hold-out data: 0.41217002272605896
Epoch: 48
Loss on hold-out set: 0.20653583007243773
MeanAbsoluteError value on hold-out data: 0.39590930938720703
Epoch: 49
Loss on hold-out set: 0.22486225354174774
MeanAbsoluteError value on hold-out data: 0.41829031705856323
Epoch: 50</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.21296398308865416
MeanAbsoluteError value on hold-out data: 0.39846551418304443
Epoch: 51
Loss on hold-out set: 0.20458407807163895
MeanAbsoluteError value on hold-out data: 0.3958100378513336
Epoch: 52
Loss on hold-out set: 0.19535328616853803
MeanAbsoluteError value on hold-out data: 0.38979271054267883
Epoch: 53</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.18599648925475776
MeanAbsoluteError value on hold-out data: 0.38307785987854004
Epoch: 54
Loss on hold-out set: 0.19595590042027955
MeanAbsoluteError value on hold-out data: 0.3857477605342865
Epoch: 55
Loss on hold-out set: 0.1949584838654846
MeanAbsoluteError value on hold-out data: 0.3878266513347626
Epoch: 56</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.20247651587240398
MeanAbsoluteError value on hold-out data: 0.3897246718406677
Epoch: 57
Loss on hold-out set: 0.19522921663398543
MeanAbsoluteError value on hold-out data: 0.3829149901866913
Epoch: 58
Loss on hold-out set: 0.18953906812394658
MeanAbsoluteError value on hold-out data: 0.3836454749107361
Epoch: 59</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1983502767390261
MeanAbsoluteError value on hold-out data: 0.38693109154701233
Epoch: 60
Loss on hold-out set: 0.20998683478683233
MeanAbsoluteError value on hold-out data: 0.39239799976348877
Epoch: 61
Loss on hold-out set: 0.1881432940174515
MeanAbsoluteError value on hold-out data: 0.37518787384033203
Epoch: 62</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1937331968266517
MeanAbsoluteError value on hold-out data: 0.38558271527290344
Epoch: 63
Loss on hold-out set: 0.17847651544647913
MeanAbsoluteError value on hold-out data: 0.3642744719982147
Epoch: 64
Loss on hold-out set: 0.17596024135292584
MeanAbsoluteError value on hold-out data: 0.3617781698703766
Epoch: 65</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.17375697774191698
MeanAbsoluteError value on hold-out data: 0.36626288294792175
Epoch: 66
Loss on hold-out set: 0.1912630452293282
MeanAbsoluteError value on hold-out data: 0.37759149074554443
Epoch: 67
Loss on hold-out set: 0.17724436698985907
MeanAbsoluteError value on hold-out data: 0.3535984456539154
Epoch: 68</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1815174949976305
MeanAbsoluteError value on hold-out data: 0.36903032660484314
Epoch: 69
Loss on hold-out set: 0.15509856992090743
MeanAbsoluteError value on hold-out data: 0.33933332562446594
Epoch: 70
Loss on hold-out set: 0.1698333578063951
MeanAbsoluteError value on hold-out data: 0.3500637710094452
Epoch: 71</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.16520904455411559
MeanAbsoluteError value on hold-out data: 0.35265275835990906
Epoch: 72
Loss on hold-out set: 0.17313115367045004
MeanAbsoluteError value on hold-out data: 0.35542982816696167
Epoch: 73
Loss on hold-out set: 0.17152695803437382
MeanAbsoluteError value on hold-out data: 0.3525194525718689
Epoch: 74</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1648854251243271
MeanAbsoluteError value on hold-out data: 0.3435210585594177
Epoch: 75
Loss on hold-out set: 0.17384557359308625
MeanAbsoluteError value on hold-out data: 0.34949058294296265
Epoch: 76
Loss on hold-out set: 0.1489562683357993
MeanAbsoluteError value on hold-out data: 0.3293082118034363
Epoch: 77</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.17002671528141947
MeanAbsoluteError value on hold-out data: 0.3535289466381073
Epoch: 78
Loss on hold-out set: 0.16128799548450237
MeanAbsoluteError value on hold-out data: 0.3403277099132538
Epoch: 79
Loss on hold-out set: 0.1608771170862019
MeanAbsoluteError value on hold-out data: 0.34000638127326965
Epoch: 80</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.17172873428401847
MeanAbsoluteError value on hold-out data: 0.355564683675766
Epoch: 81
Loss on hold-out set: 0.16013986746220327
MeanAbsoluteError value on hold-out data: 0.34206536412239075
Epoch: 82
Loss on hold-out set: 0.16699964294736372
MeanAbsoluteError value on hold-out data: 0.3495270609855652
Epoch: 83</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.15567113618017175
MeanAbsoluteError value on hold-out data: 0.3439956605434418
Epoch: 84
Loss on hold-out set: 0.13422383261960932
MeanAbsoluteError value on hold-out data: 0.3051757514476776
Epoch: 85
Loss on hold-out set: 0.16424098841923598
MeanAbsoluteError value on hold-out data: 0.34788593649864197
Epoch: 86</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14712993120071285
MeanAbsoluteError value on hold-out data: 0.3263108432292938
Epoch: 87
Loss on hold-out set: 0.15233454243794162
MeanAbsoluteError value on hold-out data: 0.33332711458206177
Epoch: 88
Loss on hold-out set: 0.15584410762259115
MeanAbsoluteError value on hold-out data: 0.33454030752182007
Epoch: 89</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14892084651238596
MeanAbsoluteError value on hold-out data: 0.328662246465683
Epoch: 90
Loss on hold-out set: 0.14601623078963408
MeanAbsoluteError value on hold-out data: 0.3201962411403656
Epoch: 91
Loss on hold-out set: 0.13716077289311215
MeanAbsoluteError value on hold-out data: 0.3127284348011017
Epoch: 92</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14047898249700666
MeanAbsoluteError value on hold-out data: 0.31289368867874146
Epoch: 93
Loss on hold-out set: 0.1444991973368451
MeanAbsoluteError value on hold-out data: 0.32299304008483887
Epoch: 94
Loss on hold-out set: 0.15144212607216712
MeanAbsoluteError value on hold-out data: 0.3261747360229492
Epoch: 95</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13740186965287043
MeanAbsoluteError value on hold-out data: 0.3165481984615326
Epoch: 96
Loss on hold-out set: 0.1335838402295485
MeanAbsoluteError value on hold-out data: 0.31013938784599304
Epoch: 97
Loss on hold-out set: 0.1502707689604722
MeanAbsoluteError value on hold-out data: 0.3266625702381134
Epoch: 98</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14957713865675032
MeanAbsoluteError value on hold-out data: 0.32706141471862793
Epoch: 99
Loss on hold-out set: 0.13765214182746907
MeanAbsoluteError value on hold-out data: 0.31613442301750183
Epoch: 100
Loss on hold-out set: 0.12786531066250367
MeanAbsoluteError value on hold-out data: 0.3060765564441681
Epoch: 101</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13616271160858257
MeanAbsoluteError value on hold-out data: 0.31073009967803955
Epoch: 102
Loss on hold-out set: 0.12764892609668702
MeanAbsoluteError value on hold-out data: 0.296493798494339
Epoch: 103
Loss on hold-out set: 0.13361851508370212
MeanAbsoluteError value on hold-out data: 0.3034282326698303
Epoch: 104</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12137454038563494
MeanAbsoluteError value on hold-out data: 0.28970322012901306
Epoch: 105
Loss on hold-out set: 0.1356013491936028
MeanAbsoluteError value on hold-out data: 0.31113725900650024
Epoch: 106
Loss on hold-out set: 0.11262589131753581
MeanAbsoluteError value on hold-out data: 0.28187623620033264
Epoch: 107</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14472605059777077
MeanAbsoluteError value on hold-out data: 0.32358163595199585
Epoch: 108
Loss on hold-out set: 0.123876127943707
MeanAbsoluteError value on hold-out data: 0.2893526554107666
Epoch: 109
Loss on hold-out set: 0.12229839983241012
MeanAbsoluteError value on hold-out data: 0.29284319281578064
Epoch: 110</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13049372204579412
MeanAbsoluteError value on hold-out data: 0.3062885105609894
Epoch: 111
Loss on hold-out set: 0.12387571594988306
MeanAbsoluteError value on hold-out data: 0.2890383005142212
Epoch: 112
Loss on hold-out set: 0.11327295808121562
MeanAbsoluteError value on hold-out data: 0.2815669775009155
Epoch: 113</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11762650122245152
MeanAbsoluteError value on hold-out data: 0.288284569978714
Epoch: 114
Loss on hold-out set: 0.11521096272025412
MeanAbsoluteError value on hold-out data: 0.2822805643081665
Epoch: 115
Loss on hold-out set: 0.12106607303178558
MeanAbsoluteError value on hold-out data: 0.28616422414779663
Epoch: 116</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12355651274401074
MeanAbsoluteError value on hold-out data: 0.29332292079925537
Epoch: 117
Loss on hold-out set: 0.11728608721556763
MeanAbsoluteError value on hold-out data: 0.28626954555511475
Epoch: 118
Loss on hold-out set: 0.12032038739533163
MeanAbsoluteError value on hold-out data: 0.29031500220298767
Epoch: 119</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12818754196943094
MeanAbsoluteError value on hold-out data: 0.3011290729045868
Epoch: 120
Loss on hold-out set: 0.1333511966051689
MeanAbsoluteError value on hold-out data: 0.3091905415058136
Epoch: 121
Loss on hold-out set: 0.1091590139331917
MeanAbsoluteError value on hold-out data: 0.27781668305397034
Epoch: 122</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12181491513906319
MeanAbsoluteError value on hold-out data: 0.2912302017211914
Epoch: 123
Loss on hold-out set: 0.11089463963871822
MeanAbsoluteError value on hold-out data: 0.2732159495353699
Epoch: 124
Loss on hold-out set: 0.1021037278328246
MeanAbsoluteError value on hold-out data: 0.26803335547447205
Epoch: 125</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11500476690208113
MeanAbsoluteError value on hold-out data: 0.27739861607551575
Epoch: 126
Loss on hold-out set: 0.11748044946541389
MeanAbsoluteError value on hold-out data: 0.28494641184806824
Epoch: 127
Loss on hold-out set: 0.11358133622134725
MeanAbsoluteError value on hold-out data: 0.2807934284210205
Epoch: 128</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11022103113510336
MeanAbsoluteError value on hold-out data: 0.2737456262111664
Epoch: 129
Loss on hold-out set: 0.11364755001171337
MeanAbsoluteError value on hold-out data: 0.280472993850708
Epoch: 130
Loss on hold-out set: 0.11112292560438315
MeanAbsoluteError value on hold-out data: 0.2729261815547943
Epoch: 131</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10050627921591512
MeanAbsoluteError value on hold-out data: 0.26512157917022705
Epoch: 132
Loss on hold-out set: 0.10828669071391535
MeanAbsoluteError value on hold-out data: 0.2710760533809662
Epoch: 133
Loss on hold-out set: 0.11690342619782314
MeanAbsoluteError value on hold-out data: 0.2823551893234253
Epoch: 134</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10715428820549278
MeanAbsoluteError value on hold-out data: 0.2714904844760895
Epoch: 135
Loss on hold-out set: 0.10407921475668748
MeanAbsoluteError value on hold-out data: 0.26757198572158813
Epoch: 136
Loss on hold-out set: 0.12601793787674978
MeanAbsoluteError value on hold-out data: 0.28674396872520447
Epoch: 137</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12784489011391997
MeanAbsoluteError value on hold-out data: 0.2860984206199646
Epoch: 138
Loss on hold-out set: 0.10832093603729542
MeanAbsoluteError value on hold-out data: 0.2646139860153198
Epoch: 139
Loss on hold-out set: 0.10902379850313688
MeanAbsoluteError value on hold-out data: 0.27351465821266174
Epoch: 140</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10812491706960524
MeanAbsoluteError value on hold-out data: 0.2723095715045929
Epoch: 141
Loss on hold-out set: 0.10438801815500483
MeanAbsoluteError value on hold-out data: 0.2687864601612091
Epoch: 142
Loss on hold-out set: 0.10449539088644087
MeanAbsoluteError value on hold-out data: 0.2637953460216522
Epoch: 143</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10966987963416613
MeanAbsoluteError value on hold-out data: 0.27203136682510376
Epoch: 144
Loss on hold-out set: 0.0995169809924361
MeanAbsoluteError value on hold-out data: 0.2614741921424866
Epoch: 145
Loss on hold-out set: 0.09745104723648788
MeanAbsoluteError value on hold-out data: 0.25474119186401367
Epoch: 146</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09868114036275073
MeanAbsoluteError value on hold-out data: 0.26060736179351807
Epoch: 147
Loss on hold-out set: 0.10214638291838735
MeanAbsoluteError value on hold-out data: 0.25949224829673767
Epoch: 148
Loss on hold-out set: 0.09370269942019756
MeanAbsoluteError value on hold-out data: 0.2538761794567108
Epoch: 149</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10047829707214988
MeanAbsoluteError value on hold-out data: 0.2612086236476898
Epoch: 150
Loss on hold-out set: 0.10250811297698723
MeanAbsoluteError value on hold-out data: 0.2650384306907654
Epoch: 151
Loss on hold-out set: 0.11306162872778562
MeanAbsoluteError value on hold-out data: 0.2802852392196655
Epoch: 152</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09915911229212118
MeanAbsoluteError value on hold-out data: 0.2598768472671509
Epoch: 153
Loss on hold-out set: 0.108122091886277
MeanAbsoluteError value on hold-out data: 0.2716710865497589
Epoch: 154
Loss on hold-out set: 0.11257303280018581
MeanAbsoluteError value on hold-out data: 0.264741986989975
Epoch: 155</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10850306998395051
MeanAbsoluteError value on hold-out data: 0.26513752341270447
Epoch: 156
Loss on hold-out set: 0.10669659160717856
MeanAbsoluteError value on hold-out data: 0.271679550409317
Epoch: 157
Loss on hold-out set: 0.10071512242662721
MeanAbsoluteError value on hold-out data: 0.25757279992103577
Epoch: 158</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10535549942869693
MeanAbsoluteError value on hold-out data: 0.26586610078811646
Epoch: 159
Loss on hold-out set: 0.10030520771164447
MeanAbsoluteError value on hold-out data: 0.2569221556186676
Epoch: 160
Loss on hold-out set: 0.10361332301593697
MeanAbsoluteError value on hold-out data: 0.26323434710502625
Epoch: 161</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1035293353368373
MeanAbsoluteError value on hold-out data: 0.2631157636642456
Epoch: 162
Loss on hold-out set: 0.10177684444390858
MeanAbsoluteError value on hold-out data: 0.2630542516708374
Epoch: 163
Loss on hold-out set: 0.09976425169191012
MeanAbsoluteError value on hold-out data: 0.2658640444278717
Epoch: 164</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10808157974854112
MeanAbsoluteError value on hold-out data: 0.269523024559021
Epoch: 165
Loss on hold-out set: 0.11317426273909707
MeanAbsoluteError value on hold-out data: 0.2825244069099426
Epoch: 166
Loss on hold-out set: 0.0928951215154181
MeanAbsoluteError value on hold-out data: 0.24613481760025024
Epoch: 167</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10246243470969299
MeanAbsoluteError value on hold-out data: 0.26763007044792175
Epoch: 168
Loss on hold-out set: 0.1026789629967728
MeanAbsoluteError value on hold-out data: 0.26824951171875
Epoch: 169
Loss on hold-out set: 0.09321827076423991
MeanAbsoluteError value on hold-out data: 0.24754276871681213
Epoch: 170</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09705553084767113
MeanAbsoluteError value on hold-out data: 0.2551393508911133
Epoch: 171
Loss on hold-out set: 0.09775005352683365
MeanAbsoluteError value on hold-out data: 0.2540559768676758
Epoch: 172
Loss on hold-out set: 0.10172915168261776
MeanAbsoluteError value on hold-out data: 0.2600668966770172
Epoch: 173</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09290545317111537
MeanAbsoluteError value on hold-out data: 0.25384628772735596
Epoch: 174
Loss on hold-out set: 0.0944040242691214
MeanAbsoluteError value on hold-out data: 0.2519071400165558
Epoch: 175
Loss on hold-out set: 0.08100815595438084
MeanAbsoluteError value on hold-out data: 0.23731131851673126
Epoch: 176</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09681892585160919
MeanAbsoluteError value on hold-out data: 0.2513824999332428
Epoch: 177
Loss on hold-out set: 0.09306872028391808
MeanAbsoluteError value on hold-out data: 0.2511293888092041
Epoch: 178
Loss on hold-out set: 0.1157253296355096
MeanAbsoluteError value on hold-out data: 0.27343544363975525
Epoch: 179</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0870953202546419
MeanAbsoluteError value on hold-out data: 0.23618169128894806
Epoch: 180
Loss on hold-out set: 0.10462116401176899
MeanAbsoluteError value on hold-out data: 0.2608072757720947
Epoch: 181
Loss on hold-out set: 0.10727453477370243
MeanAbsoluteError value on hold-out data: 0.26881644129753113
Epoch: 182</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10238373705564299
MeanAbsoluteError value on hold-out data: 0.2632840871810913
Epoch: 183
Loss on hold-out set: 0.10366135513099531
MeanAbsoluteError value on hold-out data: 0.26454928517341614
Epoch: 184
Loss on hold-out set: 0.08551485699446251
MeanAbsoluteError value on hold-out data: 0.23792852461338043
Epoch: 185</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10159660036287581
MeanAbsoluteError value on hold-out data: 0.25938475131988525
Epoch: 186
Loss on hold-out set: 0.1068465962074697
MeanAbsoluteError value on hold-out data: 0.27154839038848877
Epoch: 187
Loss on hold-out set: 0.08486659216978296
MeanAbsoluteError value on hold-out data: 0.23020921647548676
Epoch: 188</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0984190525712135
MeanAbsoluteError value on hold-out data: 0.2594861686229706
Epoch: 189
Loss on hold-out set: 0.09891394621867221
MeanAbsoluteError value on hold-out data: 0.2513481676578522
Epoch: 190
Loss on hold-out set: 0.10134982038289309
MeanAbsoluteError value on hold-out data: 0.25807029008865356
Epoch: 191</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09101210748155912
MeanAbsoluteError value on hold-out data: 0.2523293197154999
Epoch: 192
Loss on hold-out set: 0.09606388407448928
MeanAbsoluteError value on hold-out data: 0.2545988857746124
Epoch: 193
Loss on hold-out set: 0.10236769631699039
MeanAbsoluteError value on hold-out data: 0.2602272927761078
Epoch: 194</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08585137002364111
MeanAbsoluteError value on hold-out data: 0.23038868606090546
Epoch: 195
Loss on hold-out set: 0.09980433501147976
MeanAbsoluteError value on hold-out data: 0.2552485764026642
Epoch: 196
Loss on hold-out set: 0.09635448274590695
MeanAbsoluteError value on hold-out data: 0.2510274648666382
Epoch: 197</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10223913447659773
MeanAbsoluteError value on hold-out data: 0.2605172097682953
Epoch: 198
Loss on hold-out set: 0.09489625854378876
MeanAbsoluteError value on hold-out data: 0.254561185836792
Epoch: 199
Loss on hold-out set: 0.0868308769466239
MeanAbsoluteError value on hold-out data: 0.2401658147573471
Epoch: 200</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08554651412181556
MeanAbsoluteError value on hold-out data: 0.24150747060775757
Epoch: 201
Loss on hold-out set: 0.08309459253136689
MeanAbsoluteError value on hold-out data: 0.22918586432933807
Epoch: 202
Loss on hold-out set: 0.10159056303285373
MeanAbsoluteError value on hold-out data: 0.26179012656211853
Epoch: 203</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0802979029873677
MeanAbsoluteError value on hold-out data: 0.22802868485450745
Epoch: 204
Loss on hold-out set: 0.10086305494691866
MeanAbsoluteError value on hold-out data: 0.2549743056297302
Epoch: 205
Loss on hold-out set: 0.09130810380641681
MeanAbsoluteError value on hold-out data: 0.24415336549282074
Epoch: 206</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09874599172306868
MeanAbsoluteError value on hold-out data: 0.2598705589771271
Epoch: 207
Loss on hold-out set: 0.09546209547358254
MeanAbsoluteError value on hold-out data: 0.24915343523025513
Epoch: 208
Loss on hold-out set: 0.08752690909275164
MeanAbsoluteError value on hold-out data: 0.2387186735868454
Epoch: 209</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09195197360755022
MeanAbsoluteError value on hold-out data: 0.24200940132141113
Epoch: 210
Loss on hold-out set: 0.09661983530871415
MeanAbsoluteError value on hold-out data: 0.2523827850818634
Epoch: 211
Loss on hold-out set: 0.09746486369651393
MeanAbsoluteError value on hold-out data: 0.24535971879959106
Epoch: 212</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09593248407618375
MeanAbsoluteError value on hold-out data: 0.24527201056480408
Epoch: 213
Loss on hold-out set: 0.08762997447357823
MeanAbsoluteError value on hold-out data: 0.24818865954875946
Epoch: 214
Loss on hold-out set: 0.09062037191334336
MeanAbsoluteError value on hold-out data: 0.2487483024597168
Epoch: 215</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08981889275378005
MeanAbsoluteError value on hold-out data: 0.24424625933170319
Epoch: 216
Loss on hold-out set: 0.08029537183271411
MeanAbsoluteError value on hold-out data: 0.22789987921714783
Epoch: 217
Loss on hold-out set: 0.09135659327497705
MeanAbsoluteError value on hold-out data: 0.24170203506946564
Epoch: 218</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09148098391635964
MeanAbsoluteError value on hold-out data: 0.2430833876132965
Epoch: 219
Loss on hold-out set: 0.10050653902503351
MeanAbsoluteError value on hold-out data: 0.2615955173969269
Epoch: 220
Loss on hold-out set: 0.09528928042234232
MeanAbsoluteError value on hold-out data: 0.24324889481067657
Epoch: 221</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0922065845564551
MeanAbsoluteError value on hold-out data: 0.2450760453939438
Epoch: 222
Loss on hold-out set: 0.09155014637392014
MeanAbsoluteError value on hold-out data: 0.25402215123176575
Epoch: 223
Loss on hold-out set: 0.09702969212356645
MeanAbsoluteError value on hold-out data: 0.256540447473526
Epoch: 224</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10398892153083579
MeanAbsoluteError value on hold-out data: 0.2546880543231964
Epoch: 225
Loss on hold-out set: 0.0905076963895893
MeanAbsoluteError value on hold-out data: 0.242879718542099
Epoch: 226
Loss on hold-out set: 0.09558403013089749
MeanAbsoluteError value on hold-out data: 0.24483685195446014
Epoch: 227</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08162066148128361
MeanAbsoluteError value on hold-out data: 0.2303251326084137
Epoch: 228
Loss on hold-out set: 0.10206926486736241
MeanAbsoluteError value on hold-out data: 0.2560073733329773
Epoch: 229
Loss on hold-out set: 0.07413971603848041
MeanAbsoluteError value on hold-out data: 0.219350203871727
Epoch: 230</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09404376904810002
MeanAbsoluteError value on hold-out data: 0.2447337657213211
Epoch: 231
Loss on hold-out set: 0.07021813147468492
MeanAbsoluteError value on hold-out data: 0.21451835334300995
Epoch: 232
Loss on hold-out set: 0.10799305157735944
MeanAbsoluteError value on hold-out data: 0.27033549547195435
Epoch: 233</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09353746463195421
MeanAbsoluteError value on hold-out data: 0.24237845838069916
Epoch: 234
Loss on hold-out set: 0.09533997783049321
MeanAbsoluteError value on hold-out data: 0.2515796720981598
Epoch: 235
Loss on hold-out set: 0.087083221412225
MeanAbsoluteError value on hold-out data: 0.2318393439054489
Epoch: 236</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09121541720582173
MeanAbsoluteError value on hold-out data: 0.24456067383289337
Epoch: 237
Loss on hold-out set: 0.0929722089949064
MeanAbsoluteError value on hold-out data: 0.23718376457691193
Epoch: 238
Loss on hold-out set: 0.08488025765870892
MeanAbsoluteError value on hold-out data: 0.22975881397724152
Epoch: 239</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10443284192471765
MeanAbsoluteError value on hold-out data: 0.26233816146850586
Epoch: 240
Loss on hold-out set: 0.09751650258889034
MeanAbsoluteError value on hold-out data: 0.25210845470428467
Epoch: 241
Loss on hold-out set: 0.08799825655529275
MeanAbsoluteError value on hold-out data: 0.23622667789459229
Epoch: 242</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09737830657317924
MeanAbsoluteError value on hold-out data: 0.25393882393836975
Epoch: 243
Loss on hold-out set: 0.09241264024244931
MeanAbsoluteError value on hold-out data: 0.2452748566865921
Epoch: 244
Loss on hold-out set: 0.09117106129454139
MeanAbsoluteError value on hold-out data: 0.23803527653217316
Epoch: 245</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08925814898218959
MeanAbsoluteError value on hold-out data: 0.24430857598781586
Epoch: 246
Loss on hold-out set: 0.10177093826040315
MeanAbsoluteError value on hold-out data: 0.2575643062591553
Epoch: 247
Loss on hold-out set: 0.0777476977884847
MeanAbsoluteError value on hold-out data: 0.2174939215183258
Epoch: 248</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08332410309485264
MeanAbsoluteError value on hold-out data: 0.2338300347328186
Epoch: 249
Loss on hold-out set: 0.08193271123959373
MeanAbsoluteError value on hold-out data: 0.22591537237167358
Epoch: 250
Loss on hold-out set: 0.08771158823510632
MeanAbsoluteError value on hold-out data: 0.2387746423482895
Epoch: 251</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08126815342189123
MeanAbsoluteError value on hold-out data: 0.22467567026615143
Epoch: 252
Loss on hold-out set: 0.09404783456469885
MeanAbsoluteError value on hold-out data: 0.2411993145942688
Epoch: 253
Loss on hold-out set: 0.08702320758835412
MeanAbsoluteError value on hold-out data: 0.2345535010099411
Epoch: 254</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0999036842243125
MeanAbsoluteError value on hold-out data: 0.2553742825984955
Epoch: 255
Loss on hold-out set: 0.09006271986135592
MeanAbsoluteError value on hold-out data: 0.23417551815509796
Epoch: 256
Loss on hold-out set: 0.08867481812136248
MeanAbsoluteError value on hold-out data: 0.24698998034000397
Epoch: 257</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07442789652695259
MeanAbsoluteError value on hold-out data: 0.22076138854026794
Epoch: 258
Loss on hold-out set: 0.07324298803966182
MeanAbsoluteError value on hold-out data: 0.2221785932779312
Epoch: 259
Loss on hold-out set: 0.09793501357703159
MeanAbsoluteError value on hold-out data: 0.2547050416469574
Epoch: 260</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09600132483212898
MeanAbsoluteError value on hold-out data: 0.24808631837368011
Epoch: 261
Loss on hold-out set: 0.0877704827322547
MeanAbsoluteError value on hold-out data: 0.24204887449741364
Epoch: 262
Loss on hold-out set: 0.08901351906902467
MeanAbsoluteError value on hold-out data: 0.2341025471687317
Epoch: 263</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0800885943544563
MeanAbsoluteError value on hold-out data: 0.22656016051769257
Early stopping at epoch 262
Returned to Spot: Validation loss: 0.0800885943544563
----------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
config: {'_L_in': 10, '_L_out': 1, 'l1': 128, 'dropout_prob': 0.8941737037376589, 'lr_mult': 0.001, 'batch_size': 2, 'epochs': 2048, 'k_folds': 1, 'patience': 32, 'optimizer': 'Adam', 'sgd_momentum': 0.9}
Epoch: 1
Loss on hold-out set: 0.6092797444264094
MeanAbsoluteError value on hold-out data: 0.735221803188324
Epoch: 2
Loss on hold-out set: 0.6252139050265153
MeanAbsoluteError value on hold-out data: 0.7479813694953918
Epoch: 3</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.6169953216363986
MeanAbsoluteError value on hold-out data: 0.7423505187034607
Epoch: 4
Loss on hold-out set: 0.6043034598231316
MeanAbsoluteError value on hold-out data: 0.7324802279472351
Epoch: 5
Loss on hold-out set: 0.593123180915912
MeanAbsoluteError value on hold-out data: 0.7228860259056091
Epoch: 6</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.6306304063399633
MeanAbsoluteError value on hold-out data: 0.7507378458976746
Epoch: 7
Loss on hold-out set: 0.6268791493028403
MeanAbsoluteError value on hold-out data: 0.7491353750228882
Epoch: 8
Loss on hold-out set: 0.6119830322017272
MeanAbsoluteError value on hold-out data: 0.7376520037651062
Epoch: 9</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.5535879143079122
MeanAbsoluteError value on hold-out data: 0.702744722366333
Epoch: 10
Loss on hold-out set: 0.5660805068040887
MeanAbsoluteError value on hold-out data: 0.7106385231018066
Epoch: 11
Loss on hold-out set: 0.5772583024700483
MeanAbsoluteError value on hold-out data: 0.7124667167663574
Epoch: 12</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.5899955174823602
MeanAbsoluteError value on hold-out data: 0.7205853462219238
Epoch: 13
Loss on hold-out set: 0.5785451020797093
MeanAbsoluteError value on hold-out data: 0.7146211862564087
Epoch: 14
Loss on hold-out set: 0.5509399648259083
MeanAbsoluteError value on hold-out data: 0.7001301050186157
Epoch: 15</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.5669047906498115
MeanAbsoluteError value on hold-out data: 0.7074636220932007
Epoch: 16
Loss on hold-out set: 0.5356857539961736
MeanAbsoluteError value on hold-out data: 0.6848175525665283
Epoch: 17
Loss on hold-out set: 0.5295113852868477
MeanAbsoluteError value on hold-out data: 0.6805733442306519
Epoch: 18</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.5307178633411725
MeanAbsoluteError value on hold-out data: 0.6794905662536621
Epoch: 19
Loss on hold-out set: 0.5253999585906665
MeanAbsoluteError value on hold-out data: 0.6801208853721619
Epoch: 20
Loss on hold-out set: 0.5115564171969891
MeanAbsoluteError value on hold-out data: 0.6736929416656494
Epoch: 21</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.5512571655710539
MeanAbsoluteError value on hold-out data: 0.6966819763183594
Epoch: 22
Loss on hold-out set: 0.5329030809303125
MeanAbsoluteError value on hold-out data: 0.679682731628418
Epoch: 23
Loss on hold-out set: 0.5224943852921327
MeanAbsoluteError value on hold-out data: 0.676544189453125
Epoch: 24</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.48807271003723146
MeanAbsoluteError value on hold-out data: 0.6553690433502197
Epoch: 25
Loss on hold-out set: 0.5167311435192823
MeanAbsoluteError value on hold-out data: 0.6660130620002747
Epoch: 26
Loss on hold-out set: 0.5177141772387162
MeanAbsoluteError value on hold-out data: 0.6755843162536621
Epoch: 27</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.49164313269158205
MeanAbsoluteError value on hold-out data: 0.6576112508773804
Epoch: 28
Loss on hold-out set: 0.48059559663447243
MeanAbsoluteError value on hold-out data: 0.6471665501594543
Epoch: 29
Loss on hold-out set: 0.47651040559013685
MeanAbsoluteError value on hold-out data: 0.6451191902160645
Epoch: 30</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.48794069776932397
MeanAbsoluteError value on hold-out data: 0.6519941091537476
Epoch: 31
Loss on hold-out set: 0.48096685404578843
MeanAbsoluteError value on hold-out data: 0.6445003151893616
Epoch: 32
Loss on hold-out set: 0.48326448261737825
MeanAbsoluteError value on hold-out data: 0.6446928381919861
Epoch: 33</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.45480635846654577
MeanAbsoluteError value on hold-out data: 0.6273816227912903
Epoch: 34
Loss on hold-out set: 0.4497255781789621
MeanAbsoluteError value on hold-out data: 0.622063398361206
Epoch: 35
Loss on hold-out set: 0.4619014692058166
MeanAbsoluteError value on hold-out data: 0.6351042985916138
Epoch: 36</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4573685630659262
MeanAbsoluteError value on hold-out data: 0.6295377612113953
Epoch: 37
Loss on hold-out set: 0.4284983143707116
MeanAbsoluteError value on hold-out data: 0.612689733505249
Epoch: 38
Loss on hold-out set: 0.4615280417104562
MeanAbsoluteError value on hold-out data: 0.6241000294685364
Epoch: 39</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.42544095593194164
MeanAbsoluteError value on hold-out data: 0.6086981892585754
Epoch: 40
Loss on hold-out set: 0.4626134578883648
MeanAbsoluteError value on hold-out data: 0.6320423483848572
Epoch: 41
Loss on hold-out set: 0.4431185081849496
MeanAbsoluteError value on hold-out data: 0.616335928440094
Epoch: 42</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.4345830073952675
MeanAbsoluteError value on hold-out data: 0.6046192646026611
Epoch: 43
Loss on hold-out set: 0.4229515016078949
MeanAbsoluteError value on hold-out data: 0.6034857630729675
Epoch: 44
Loss on hold-out set: 0.42025726914405825
MeanAbsoluteError value on hold-out data: 0.5941047072410583
Epoch: 45</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.41616627094646297
MeanAbsoluteError value on hold-out data: 0.5928467512130737
Epoch: 46
Loss on hold-out set: 0.4303734613209963
MeanAbsoluteError value on hold-out data: 0.6097280383110046
Epoch: 47
Loss on hold-out set: 0.4399117068698009
MeanAbsoluteError value on hold-out data: 0.6127111911773682
Epoch: 48</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.38362244366668163
MeanAbsoluteError value on hold-out data: 0.570936918258667
Epoch: 49
Loss on hold-out set: 0.425918795162191
MeanAbsoluteError value on hold-out data: 0.6023797392845154
Epoch: 50
Loss on hold-out set: 0.37944095775485037
MeanAbsoluteError value on hold-out data: 0.5657235383987427
Epoch: 51</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.41909450207526483
MeanAbsoluteError value on hold-out data: 0.5988107919692993
Epoch: 52
Loss on hold-out set: 0.38058179163684447
MeanAbsoluteError value on hold-out data: 0.5681595206260681
Epoch: 53
Loss on hold-out set: 0.38593288699785866
MeanAbsoluteError value on hold-out data: 0.5740458965301514
Epoch: 54</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3818231924623251
MeanAbsoluteError value on hold-out data: 0.5708422660827637
Epoch: 55
Loss on hold-out set: 0.37087017672757305
MeanAbsoluteError value on hold-out data: 0.5588675737380981
Epoch: 56
Loss on hold-out set: 0.3573351814846198
MeanAbsoluteError value on hold-out data: 0.5470030307769775
Epoch: 57</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3434672603507837
MeanAbsoluteError value on hold-out data: 0.542571485042572
Epoch: 58
Loss on hold-out set: 0.35915327524145446
MeanAbsoluteError value on hold-out data: 0.5502406358718872
Epoch: 59
Loss on hold-out set: 0.362757097814853
MeanAbsoluteError value on hold-out data: 0.5469200611114502
Epoch: 60</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.34862515702843666
MeanAbsoluteError value on hold-out data: 0.5439870357513428
Epoch: 61
Loss on hold-out set: 0.35205483246594665
MeanAbsoluteError value on hold-out data: 0.5397988557815552
Epoch: 62
Loss on hold-out set: 0.3416006386093795
MeanAbsoluteError value on hold-out data: 0.5357463359832764
Epoch: 63</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.360205436895291
MeanAbsoluteError value on hold-out data: 0.5493037700653076
Epoch: 64
Loss on hold-out set: 0.37136754327764115
MeanAbsoluteError value on hold-out data: 0.5640350580215454
Epoch: 65
Loss on hold-out set: 0.3257768782724937
MeanAbsoluteError value on hold-out data: 0.5197075009346008
Epoch: 66</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3183700472985705
MeanAbsoluteError value on hold-out data: 0.5180015563964844
Epoch: 67
Loss on hold-out set: 0.3064657030813396
MeanAbsoluteError value on hold-out data: 0.501442015171051
Epoch: 68
Loss on hold-out set: 0.3315387768919269
MeanAbsoluteError value on hold-out data: 0.5204872488975525
Epoch: 69</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3213877599189679
MeanAbsoluteError value on hold-out data: 0.5176078081130981
Epoch: 70
Loss on hold-out set: 0.32324237955734136
MeanAbsoluteError value on hold-out data: 0.5120276808738708
Epoch: 71
Loss on hold-out set: 0.3242294270793597
MeanAbsoluteError value on hold-out data: 0.519134521484375
Epoch: 72</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3297145739042511
MeanAbsoluteError value on hold-out data: 0.5156093239784241
Epoch: 73
Loss on hold-out set: 0.3148142039217055
MeanAbsoluteError value on hold-out data: 0.5133469700813293
Epoch: 74
Loss on hold-out set: 0.3068324515471856
MeanAbsoluteError value on hold-out data: 0.5004062652587891
Epoch: 75</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2933913297081987
MeanAbsoluteError value on hold-out data: 0.49451735615730286
Epoch: 76
Loss on hold-out set: 0.3148229472587506
MeanAbsoluteError value on hold-out data: 0.5052622556686401
Epoch: 77
Loss on hold-out set: 0.3124183105987807
MeanAbsoluteError value on hold-out data: 0.5068725347518921
Epoch: 78</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2927109773736447
MeanAbsoluteError value on hold-out data: 0.4815131723880768
Epoch: 79
Loss on hold-out set: 0.2910944478089611
MeanAbsoluteError value on hold-out data: 0.4837327003479004
Epoch: 80
Loss on hold-out set: 0.28482427101582286
MeanAbsoluteError value on hold-out data: 0.48100516200065613
Epoch: 81</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.255353768489634
MeanAbsoluteError value on hold-out data: 0.4441581666469574
Epoch: 82
Loss on hold-out set: 0.28620949341449886
MeanAbsoluteError value on hold-out data: 0.4771195352077484
Epoch: 83
Loss on hold-out set: 0.27753106674800315
MeanAbsoluteError value on hold-out data: 0.47622033953666687
Epoch: 84</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2872464527872701
MeanAbsoluteError value on hold-out data: 0.4758186340332031
Epoch: 85
Loss on hold-out set: 0.27936273060700234
MeanAbsoluteError value on hold-out data: 0.47478607296943665
Epoch: 86
Loss on hold-out set: 0.28135068562813104
MeanAbsoluteError value on hold-out data: 0.4790174961090088
Epoch: 87</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.268540409989655
MeanAbsoluteError value on hold-out data: 0.4588207006454468
Epoch: 88
Loss on hold-out set: 0.29486672264523806
MeanAbsoluteError value on hold-out data: 0.4817069470882416
Epoch: 89
Loss on hold-out set: 0.26274818256652605
MeanAbsoluteError value on hold-out data: 0.4546467065811157
Epoch: 90</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.27618946696942054
MeanAbsoluteError value on hold-out data: 0.47169172763824463
Epoch: 91
Loss on hold-out set: 0.2568877574180563
MeanAbsoluteError value on hold-out data: 0.4547283351421356
Epoch: 92
Loss on hold-out set: 0.2625684273809505
MeanAbsoluteError value on hold-out data: 0.4504115879535675
Epoch: 93</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.24136565074014166
MeanAbsoluteError value on hold-out data: 0.4346344470977783
Epoch: 94
Loss on hold-out set: 0.2431881833340352
MeanAbsoluteError value on hold-out data: 0.43469518423080444
Epoch: 95
Loss on hold-out set: 0.26168598445132374
MeanAbsoluteError value on hold-out data: 0.44678643345832825
Epoch: 96</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2357846260551984
MeanAbsoluteError value on hold-out data: 0.4227188527584076
Epoch: 97
Loss on hold-out set: 0.24118846137697497
MeanAbsoluteError value on hold-out data: 0.4355393350124359
Epoch: 98
Loss on hold-out set: 0.23401787146305045
MeanAbsoluteError value on hold-out data: 0.4264860153198242
Epoch: 99</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2628121180801342
MeanAbsoluteError value on hold-out data: 0.45335590839385986
Epoch: 100
Loss on hold-out set: 0.23090530426707118
MeanAbsoluteError value on hold-out data: 0.4227588176727295
Epoch: 101
Loss on hold-out set: 0.2481928726270174
MeanAbsoluteError value on hold-out data: 0.43762511014938354
Epoch: 102</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2371425759367412
MeanAbsoluteError value on hold-out data: 0.4289780557155609
Epoch: 103
Loss on hold-out set: 0.23892016130809982
MeanAbsoluteError value on hold-out data: 0.43181344866752625
Epoch: 104
Loss on hold-out set: 0.22248011558006206
MeanAbsoluteError value on hold-out data: 0.41746649146080017
Epoch: 105</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.23059281295999728
MeanAbsoluteError value on hold-out data: 0.4215588867664337
Epoch: 106
Loss on hold-out set: 0.21838034548796714
MeanAbsoluteError value on hold-out data: 0.4029487371444702
Epoch: 107
Loss on hold-out set: 0.21990225184553613
MeanAbsoluteError value on hold-out data: 0.40839922428131104
Epoch: 108</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2023099168362872
MeanAbsoluteError value on hold-out data: 0.3893437087535858
Epoch: 109
Loss on hold-out set: 0.21456891510635615
MeanAbsoluteError value on hold-out data: 0.40147995948791504
Epoch: 110
Loss on hold-out set: 0.20247146303455035
MeanAbsoluteError value on hold-out data: 0.39099937677383423
Epoch: 111</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.20842790705365283
MeanAbsoluteError value on hold-out data: 0.393929123878479
Epoch: 112
Loss on hold-out set: 0.21008238877169788
MeanAbsoluteError value on hold-out data: 0.4006597101688385
Epoch: 113
Loss on hold-out set: 0.21634326549867788
MeanAbsoluteError value on hold-out data: 0.407402366399765
Epoch: 114</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.21707706163947782
MeanAbsoluteError value on hold-out data: 0.40155139565467834
Epoch: 115
Loss on hold-out set: 0.20621493414665262
MeanAbsoluteError value on hold-out data: 0.39383184909820557
Epoch: 116
Loss on hold-out set: 0.19600173471185067
MeanAbsoluteError value on hold-out data: 0.3818179965019226
Epoch: 117</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.20354320403964568
MeanAbsoluteError value on hold-out data: 0.3922022879123688
Epoch: 118
Loss on hold-out set: 0.18981659584057828
MeanAbsoluteError value on hold-out data: 0.3794736862182617
Epoch: 119
Loss on hold-out set: 0.20937593007770677
MeanAbsoluteError value on hold-out data: 0.39442914724349976
Epoch: 120</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1923049230252703
MeanAbsoluteError value on hold-out data: 0.3877710700035095
Epoch: 121
Loss on hold-out set: 0.18493319551926107
MeanAbsoluteError value on hold-out data: 0.3668733835220337
Epoch: 122
Loss on hold-out set: 0.20633006950219474
MeanAbsoluteError value on hold-out data: 0.39187178015708923
Epoch: 123</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.18566461597879727
MeanAbsoluteError value on hold-out data: 0.36166077852249146
Epoch: 124
Loss on hold-out set: 0.1832149511653309
MeanAbsoluteError value on hold-out data: 0.36804524064064026
Epoch: 125
Loss on hold-out set: 0.19267334413404266
MeanAbsoluteError value on hold-out data: 0.37318480014801025
Epoch: 126</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1806177544996414
MeanAbsoluteError value on hold-out data: 0.36181947588920593
Epoch: 127
Loss on hold-out set: 0.1822307922846327
MeanAbsoluteError value on hold-out data: 0.36949291825294495
Epoch: 128
Loss on hold-out set: 0.18956777266905797
MeanAbsoluteError value on hold-out data: 0.3679845333099365
Epoch: 129</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.19075591767517228
MeanAbsoluteError value on hold-out data: 0.36986127495765686
Epoch: 130
Loss on hold-out set: 0.17182010455677907
MeanAbsoluteError value on hold-out data: 0.35150131583213806
Epoch: 131
Loss on hold-out set: 0.18131672617979347
MeanAbsoluteError value on hold-out data: 0.3679136335849762
Epoch: 132</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.16570507530122996
MeanAbsoluteError value on hold-out data: 0.3519062399864197
Epoch: 133
Loss on hold-out set: 0.17122932986084682
MeanAbsoluteError value on hold-out data: 0.35133272409439087
Epoch: 134
Loss on hold-out set: 0.17267855714696148
MeanAbsoluteError value on hold-out data: 0.3538814187049866
Epoch: 135</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14941454908034454
MeanAbsoluteError value on hold-out data: 0.3311252295970917
Epoch: 136
Loss on hold-out set: 0.16239871056071328
MeanAbsoluteError value on hold-out data: 0.3358157277107239
Epoch: 137
Loss on hold-out set: 0.1719312709585453
MeanAbsoluteError value on hold-out data: 0.355365514755249
Epoch: 138</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.16336392629581192
MeanAbsoluteError value on hold-out data: 0.33444103598594666
Epoch: 139
Loss on hold-out set: 0.1831935465351368
MeanAbsoluteError value on hold-out data: 0.3619583249092102
Epoch: 140
Loss on hold-out set: 0.16711582879070191
MeanAbsoluteError value on hold-out data: 0.34871265292167664
Epoch: 141</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.16435480381439752
MeanAbsoluteError value on hold-out data: 0.33741888403892517
Epoch: 142
Loss on hold-out set: 0.1707935170425723
MeanAbsoluteError value on hold-out data: 0.35266032814979553
Epoch: 143
Loss on hold-out set: 0.16456656720334042
MeanAbsoluteError value on hold-out data: 0.3332984447479248
Epoch: 144</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14553915120040378
MeanAbsoluteError value on hold-out data: 0.3260967433452606
Epoch: 145
Loss on hold-out set: 0.1617712262214627
MeanAbsoluteError value on hold-out data: 0.3380987346172333
Epoch: 146
Loss on hold-out set: 0.16666443177809317
MeanAbsoluteError value on hold-out data: 0.3498612940311432
Epoch: 147</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14959406814615553
MeanAbsoluteError value on hold-out data: 0.3298910856246948
Epoch: 148
Loss on hold-out set: 0.14579603141212524
MeanAbsoluteError value on hold-out data: 0.320661336183548
Epoch: 149
Loss on hold-out set: 0.14921755488282845
MeanAbsoluteError value on hold-out data: 0.3263871371746063
Epoch: 150</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13905596874727053
MeanAbsoluteError value on hold-out data: 0.308691143989563
Epoch: 151
Loss on hold-out set: 0.15830944856240725
MeanAbsoluteError value on hold-out data: 0.33644118905067444
Epoch: 152
Loss on hold-out set: 0.1524787088483572
MeanAbsoluteError value on hold-out data: 0.33552899956703186
Epoch: 153</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.15514301101056238
MeanAbsoluteError value on hold-out data: 0.32895728945732117
Epoch: 154
Loss on hold-out set: 0.14678748417568083
MeanAbsoluteError value on hold-out data: 0.32654762268066406
Epoch: 155
Loss on hold-out set: 0.1446841979661258
MeanAbsoluteError value on hold-out data: 0.3194190561771393
Epoch: 156</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13453042154743647
MeanAbsoluteError value on hold-out data: 0.29950129985809326
Epoch: 157
Loss on hold-out set: 0.1368563998878623
MeanAbsoluteError value on hold-out data: 0.30520495772361755
Epoch: 158
Loss on hold-out set: 0.14822875382611528
MeanAbsoluteError value on hold-out data: 0.324532151222229
Epoch: 159</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.15180579933570698
MeanAbsoluteError value on hold-out data: 0.3296441435813904
Epoch: 160
Loss on hold-out set: 0.14939323871629312
MeanAbsoluteError value on hold-out data: 0.3195478320121765
Epoch: 161
Loss on hold-out set: 0.14210320755839348
MeanAbsoluteError value on hold-out data: 0.31314677000045776
Epoch: 162</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1324589574189546
MeanAbsoluteError value on hold-out data: 0.3006356358528137
Epoch: 163
Loss on hold-out set: 0.14296881408818687
MeanAbsoluteError value on hold-out data: 0.31706905364990234
Epoch: 164
Loss on hold-out set: 0.1390774669071349
MeanAbsoluteError value on hold-out data: 0.3077169358730316
Epoch: 165</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12986988760894747
MeanAbsoluteError value on hold-out data: 0.3068736791610718
Epoch: 166
Loss on hold-out set: 0.1442980692004009
MeanAbsoluteError value on hold-out data: 0.3159750699996948
Epoch: 167
Loss on hold-out set: 0.13395696219056843
MeanAbsoluteError value on hold-out data: 0.31104007363319397
Epoch: 168</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1293982978044854
MeanAbsoluteError value on hold-out data: 0.29706016182899475
Epoch: 169
Loss on hold-out set: 0.13091147698588126
MeanAbsoluteError value on hold-out data: 0.29830092191696167
Epoch: 170
Loss on hold-out set: 0.14163064129262543
MeanAbsoluteError value on hold-out data: 0.31534403562545776
Epoch: 171</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13452841292407053
MeanAbsoluteError value on hold-out data: 0.30332934856414795
Epoch: 172
Loss on hold-out set: 0.12893682377102475
MeanAbsoluteError value on hold-out data: 0.2972424626350403
Epoch: 173
Loss on hold-out set: 0.12414802142729361
MeanAbsoluteError value on hold-out data: 0.28858938813209534
Epoch: 174</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13073896465430151
MeanAbsoluteError value on hold-out data: 0.2957199513912201
Epoch: 175
Loss on hold-out set: 0.1388825770234689
MeanAbsoluteError value on hold-out data: 0.30314692854881287
Epoch: 176
Loss on hold-out set: 0.13438146860416358
MeanAbsoluteError value on hold-out data: 0.3034878671169281
Epoch: 177</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14018097519622338
MeanAbsoluteError value on hold-out data: 0.31378376483917236
Epoch: 178
Loss on hold-out set: 0.14569199509414224
MeanAbsoluteError value on hold-out data: 0.3140948712825775
Epoch: 179
Loss on hold-out set: 0.12223341701086611
MeanAbsoluteError value on hold-out data: 0.2875300943851471
Epoch: 180</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12802706429055738
MeanAbsoluteError value on hold-out data: 0.29078951478004456
Epoch: 181
Loss on hold-out set: 0.11765267673996277
MeanAbsoluteError value on hold-out data: 0.28107398748397827
Epoch: 182
Loss on hold-out set: 0.12138738076668233
MeanAbsoluteError value on hold-out data: 0.2924133241176605
Epoch: 183</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13456517158076167
MeanAbsoluteError value on hold-out data: 0.30516549944877625
Epoch: 184
Loss on hold-out set: 0.12323771110153757
MeanAbsoluteError value on hold-out data: 0.2901429831981659
Epoch: 185
Loss on hold-out set: 0.12997359291650354
MeanAbsoluteError value on hold-out data: 0.29987841844558716
Epoch: 186</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11112675958565281
MeanAbsoluteError value on hold-out data: 0.2714468240737915
Epoch: 187
Loss on hold-out set: 0.1233990285693047
MeanAbsoluteError value on hold-out data: 0.29297155141830444
Epoch: 188
Loss on hold-out set: 0.12467859138657028
MeanAbsoluteError value on hold-out data: 0.29067572951316833
Epoch: 189</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11561460139385114
MeanAbsoluteError value on hold-out data: 0.2814320921897888
Epoch: 190
Loss on hold-out set: 0.10702503674683006
MeanAbsoluteError value on hold-out data: 0.2682780921459198
Epoch: 191
Loss on hold-out set: 0.11870015875125925
MeanAbsoluteError value on hold-out data: 0.28101983666419983
Epoch: 192</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12994380115531384
MeanAbsoluteError value on hold-out data: 0.29277655482292175
Epoch: 193
Loss on hold-out set: 0.12441150189843028
MeanAbsoluteError value on hold-out data: 0.2874244749546051
Epoch: 194
Loss on hold-out set: 0.1170346553515022
MeanAbsoluteError value on hold-out data: 0.2815093696117401
Epoch: 195</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12402973317308351
MeanAbsoluteError value on hold-out data: 0.29110464453697205
Epoch: 196
Loss on hold-out set: 0.11100082461877415
MeanAbsoluteError value on hold-out data: 0.2726960778236389
Epoch: 197
Loss on hold-out set: 0.12442110761223982
MeanAbsoluteError value on hold-out data: 0.29151543974876404
Epoch: 198</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11905467564084878
MeanAbsoluteError value on hold-out data: 0.2772753834724426
Epoch: 199
Loss on hold-out set: 0.11170080007985234
MeanAbsoluteError value on hold-out data: 0.27187830209732056
Epoch: 200
Loss on hold-out set: 0.11827823230647481
MeanAbsoluteError value on hold-out data: 0.27912166714668274
Epoch: 201</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1273876304489871
MeanAbsoluteError value on hold-out data: 0.2912565767765045
Epoch: 202
Loss on hold-out set: 0.11441332788012612
MeanAbsoluteError value on hold-out data: 0.27606791257858276
Epoch: 203
Loss on hold-out set: 0.11732313039537985
MeanAbsoluteError value on hold-out data: 0.2804623544216156
Epoch: 204</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1052548195363488
MeanAbsoluteError value on hold-out data: 0.26462191343307495
Epoch: 205
Loss on hold-out set: 0.1037378652083377
MeanAbsoluteError value on hold-out data: 0.2644864022731781
Epoch: 206
Loss on hold-out set: 0.11351324831911673
MeanAbsoluteError value on hold-out data: 0.27195534110069275
Epoch: 207</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.117408575299584
MeanAbsoluteError value on hold-out data: 0.28666576743125916
Epoch: 208
Loss on hold-out set: 0.11630733806795131
MeanAbsoluteError value on hold-out data: 0.27779433131217957
Epoch: 209
Loss on hold-out set: 0.11941406763081129
MeanAbsoluteError value on hold-out data: 0.28898417949676514
Epoch: 210</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10203547740238719
MeanAbsoluteError value on hold-out data: 0.25968343019485474
Epoch: 211
Loss on hold-out set: 0.11376385914695372
MeanAbsoluteError value on hold-out data: 0.27381497621536255
Epoch: 212
Loss on hold-out set: 0.10762641798860083
MeanAbsoluteError value on hold-out data: 0.2720147371292114
Epoch: 213</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11157778111456233
MeanAbsoluteError value on hold-out data: 0.2710396945476532
Epoch: 214
Loss on hold-out set: 0.12616038509693075
MeanAbsoluteError value on hold-out data: 0.2953992784023285
Epoch: 215
Loss on hold-out set: 0.12358418749082678
MeanAbsoluteError value on hold-out data: 0.2884238362312317
Epoch: 216</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11261017877686148
MeanAbsoluteError value on hold-out data: 0.2735517621040344
Epoch: 217
Loss on hold-out set: 0.10661012568026006
MeanAbsoluteError value on hold-out data: 0.26306188106536865
Epoch: 218
Loss on hold-out set: 0.11410644911772883
MeanAbsoluteError value on hold-out data: 0.28086844086647034
Epoch: 219</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09814537869448638
MeanAbsoluteError value on hold-out data: 0.25657013058662415
Epoch: 220
Loss on hold-out set: 0.110827275220848
MeanAbsoluteError value on hold-out data: 0.2700057029724121
Epoch: 221
Loss on hold-out set: 0.1115623736285003
MeanAbsoluteError value on hold-out data: 0.269846111536026
Epoch: 222</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11293645952828228
MeanAbsoluteError value on hold-out data: 0.27556946873664856
Epoch: 223
Loss on hold-out set: 0.12128427668843263
MeanAbsoluteError value on hold-out data: 0.2844138443470001
Epoch: 224
Loss on hold-out set: 0.11593177580400758
MeanAbsoluteError value on hold-out data: 0.28207769989967346
Epoch: 225</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1098499128218585
MeanAbsoluteError value on hold-out data: 0.2693658173084259
Epoch: 226
Loss on hold-out set: 0.12257650067874541
MeanAbsoluteError value on hold-out data: 0.2881489098072052
Epoch: 227
Loss on hold-out set: 0.10459783508908004
MeanAbsoluteError value on hold-out data: 0.25916165113449097
Epoch: 228</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10454357961692343
MeanAbsoluteError value on hold-out data: 0.2603591978549957
Epoch: 229
Loss on hold-out set: 0.10579852929183592
MeanAbsoluteError value on hold-out data: 0.26861849427223206
Epoch: 230
Loss on hold-out set: 0.11216929766655084
MeanAbsoluteError value on hold-out data: 0.2642347812652588
Epoch: 231</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09793799663486424
MeanAbsoluteError value on hold-out data: 0.254209965467453
Epoch: 232
Loss on hold-out set: 0.0994921269978416
MeanAbsoluteError value on hold-out data: 0.2580265998840332
Epoch: 233
Loss on hold-out set: 0.10993614310010646
MeanAbsoluteError value on hold-out data: 0.2726841866970062
Epoch: 234</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11940068406440939
MeanAbsoluteError value on hold-out data: 0.2860063314437866
Epoch: 235
Loss on hold-out set: 0.09977136691566557
MeanAbsoluteError value on hold-out data: 0.2539578378200531
Epoch: 236
Loss on hold-out set: 0.10793695023749024
MeanAbsoluteError value on hold-out data: 0.266926646232605
Epoch: 237</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10396265209186822
MeanAbsoluteError value on hold-out data: 0.2653251588344574
Epoch: 238
Loss on hold-out set: 0.09698769961988243
MeanAbsoluteError value on hold-out data: 0.25559747219085693
Epoch: 239
Loss on hold-out set: 0.10933795867681814
MeanAbsoluteError value on hold-out data: 0.270397812128067
Epoch: 240</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12070848843470837
MeanAbsoluteError value on hold-out data: 0.28686249256134033
Epoch: 241
Loss on hold-out set: 0.11311898909974843
MeanAbsoluteError value on hold-out data: 0.27308160066604614
Epoch: 242
Loss on hold-out set: 0.09608247455587843
MeanAbsoluteError value on hold-out data: 0.25195223093032837
Epoch: 243</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10653069351625163
MeanAbsoluteError value on hold-out data: 0.26486891508102417
Epoch: 244
Loss on hold-out set: 0.11213087675472101
MeanAbsoluteError value on hold-out data: 0.2723686397075653
Epoch: 245
Loss on hold-out set: 0.10119126121785181
MeanAbsoluteError value on hold-out data: 0.2580011487007141
Epoch: 246</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10710555158594313
MeanAbsoluteError value on hold-out data: 0.2634083926677704
Epoch: 247
Loss on hold-out set: 0.1097838488019382
MeanAbsoluteError value on hold-out data: 0.27352288365364075
Epoch: 248
Loss on hold-out set: 0.0928323593950093
MeanAbsoluteError value on hold-out data: 0.24971280992031097
Epoch: 249</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10237539400967459
MeanAbsoluteError value on hold-out data: 0.2591789960861206
Epoch: 250
Loss on hold-out set: 0.10270137551939115
MeanAbsoluteError value on hold-out data: 0.26178404688835144
Epoch: 251
Loss on hold-out set: 0.11320523696873958
MeanAbsoluteError value on hold-out data: 0.27131593227386475
Epoch: 252</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09344288539025002
MeanAbsoluteError value on hold-out data: 0.24887937307357788
Epoch: 253
Loss on hold-out set: 0.10080159158523505
MeanAbsoluteError value on hold-out data: 0.25651681423187256
Epoch: 254
Loss on hold-out set: 0.09952241350411593
MeanAbsoluteError value on hold-out data: 0.25471633672714233
Epoch: 255</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11188829597985205
MeanAbsoluteError value on hold-out data: 0.26121315360069275
Epoch: 256
Loss on hold-out set: 0.11157296582202737
MeanAbsoluteError value on hold-out data: 0.2741689383983612
Epoch: 257
Loss on hold-out set: 0.10693139626295306
MeanAbsoluteError value on hold-out data: 0.2586847245693207
Epoch: 258</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10253511922589192
MeanAbsoluteError value on hold-out data: 0.2632488012313843
Epoch: 259
Loss on hold-out set: 0.09878493816572397
MeanAbsoluteError value on hold-out data: 0.2501940131187439
Epoch: 260
Loss on hold-out set: 0.10756558999690848
MeanAbsoluteError value on hold-out data: 0.26451513171195984
Epoch: 261</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0957344484476683
MeanAbsoluteError value on hold-out data: 0.24153712391853333
Epoch: 262
Loss on hold-out set: 0.0998890501773955
MeanAbsoluteError value on hold-out data: 0.25346696376800537
Epoch: 263
Loss on hold-out set: 0.1025741405839411
MeanAbsoluteError value on hold-out data: 0.2505628168582916
Epoch: 264</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11263571697714117
MeanAbsoluteError value on hold-out data: 0.2624308168888092
Epoch: 265
Loss on hold-out set: 0.09514570575517912
MeanAbsoluteError value on hold-out data: 0.24312980473041534
Epoch: 266
Loss on hold-out set: 0.09444879973734108
MeanAbsoluteError value on hold-out data: 0.2557958662509918
Epoch: 267</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10773611680022441
MeanAbsoluteError value on hold-out data: 0.25973981618881226
Epoch: 268
Loss on hold-out set: 0.10204802326547603
MeanAbsoluteError value on hold-out data: 0.2606217861175537
Epoch: 269
Loss on hold-out set: 0.10537981502711773
MeanAbsoluteError value on hold-out data: 0.2634865939617157
Epoch: 270</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12040448564725617
MeanAbsoluteError value on hold-out data: 0.28431135416030884
Epoch: 271
Loss on hold-out set: 0.10517760288824017
MeanAbsoluteError value on hold-out data: 0.26414674520492554
Epoch: 272
Loss on hold-out set: 0.1012012026963445
MeanAbsoluteError value on hold-out data: 0.2622660994529724
Epoch: 273</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10065333780056486
MeanAbsoluteError value on hold-out data: 0.2499065399169922
Epoch: 274
Loss on hold-out set: 0.11135984077079532
MeanAbsoluteError value on hold-out data: 0.2738182842731476
Epoch: 275
Loss on hold-out set: 0.0949684236291796
MeanAbsoluteError value on hold-out data: 0.25055187940597534
Epoch: 276</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10502542970275196
MeanAbsoluteError value on hold-out data: 0.26307055354118347
Epoch: 277
Loss on hold-out set: 0.1084093532897532
MeanAbsoluteError value on hold-out data: 0.26594460010528564
Epoch: 278
Loss on hold-out set: 0.10748029934475198
MeanAbsoluteError value on hold-out data: 0.26291629672050476
Epoch: 279</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08760224367006837
MeanAbsoluteError value on hold-out data: 0.23794907331466675
Epoch: 280
Loss on hold-out set: 0.09017044020971904
MeanAbsoluteError value on hold-out data: 0.23657330870628357
Epoch: 281
Loss on hold-out set: 0.09996001053814932
MeanAbsoluteError value on hold-out data: 0.2538253962993622
Epoch: 282</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10718361851371204
MeanAbsoluteError value on hold-out data: 0.259360671043396
Epoch: 283
Loss on hold-out set: 0.0950390564436869
MeanAbsoluteError value on hold-out data: 0.24873362481594086
Epoch: 284
Loss on hold-out set: 0.11007666932302528
MeanAbsoluteError value on hold-out data: 0.26567062735557556
Epoch: 285</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09455608140910045
MeanAbsoluteError value on hold-out data: 0.2481108158826828
Epoch: 286
Loss on hold-out set: 0.10700388830698405
MeanAbsoluteError value on hold-out data: 0.26076266169548035
Epoch: 287
Loss on hold-out set: 0.10029199966132486
MeanAbsoluteError value on hold-out data: 0.2490522712469101
Epoch: 288</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09684204175854878
MeanAbsoluteError value on hold-out data: 0.24821129441261292
Epoch: 289
Loss on hold-out set: 0.10775551912762846
MeanAbsoluteError value on hold-out data: 0.2651016116142273
Epoch: 290
Loss on hold-out set: 0.08830466168622175
MeanAbsoluteError value on hold-out data: 0.23658724129199982
Epoch: 291</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10247778885435158
MeanAbsoluteError value on hold-out data: 0.2553093135356903
Epoch: 292
Loss on hold-out set: 0.11475749734090641
MeanAbsoluteError value on hold-out data: 0.27823343873023987
Epoch: 293
Loss on hold-out set: 0.09877235082016947
MeanAbsoluteError value on hold-out data: 0.2523566782474518
Epoch: 294</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09167593136119345
MeanAbsoluteError value on hold-out data: 0.25344789028167725
Epoch: 295
Loss on hold-out set: 0.08824003827525302
MeanAbsoluteError value on hold-out data: 0.24452310800552368
Epoch: 296
Loss on hold-out set: 0.10261189106696596
MeanAbsoluteError value on hold-out data: 0.26267775893211365
Epoch: 297</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10781884406334333
MeanAbsoluteError value on hold-out data: 0.2683103382587433
Epoch: 298
Loss on hold-out set: 0.10426949493276576
MeanAbsoluteError value on hold-out data: 0.2586097717285156
Epoch: 299
Loss on hold-out set: 0.10537566358533998
MeanAbsoluteError value on hold-out data: 0.2557260990142822
Epoch: 300</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08738622416703341
MeanAbsoluteError value on hold-out data: 0.24414344131946564
Epoch: 301
Loss on hold-out set: 0.09665848407357772
MeanAbsoluteError value on hold-out data: 0.245344340801239
Epoch: 302
Loss on hold-out set: 0.08282973314907091
MeanAbsoluteError value on hold-out data: 0.22727198898792267
Epoch: 303</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09989605954460179
MeanAbsoluteError value on hold-out data: 0.2557981312274933
Epoch: 304
Loss on hold-out set: 0.0925513205890699
MeanAbsoluteError value on hold-out data: 0.24605034291744232
Epoch: 305
Loss on hold-out set: 0.10034550740616396
MeanAbsoluteError value on hold-out data: 0.25274527072906494
Epoch: 306</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11117899555092056
MeanAbsoluteError value on hold-out data: 0.26835957169532776
Epoch: 307
Loss on hold-out set: 0.11202395385790927
MeanAbsoluteError value on hold-out data: 0.27055710554122925
Epoch: 308
Loss on hold-out set: 0.10808761098557929
MeanAbsoluteError value on hold-out data: 0.26208776235580444
Epoch: 309</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09936847148589247
MeanAbsoluteError value on hold-out data: 0.2573624849319458
Epoch: 310
Loss on hold-out set: 0.0977121713272451
MeanAbsoluteError value on hold-out data: 0.24702222645282745
Epoch: 311
Loss on hold-out set: 0.1058589113674437
MeanAbsoluteError value on hold-out data: 0.26486942172050476
Epoch: 312</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10776058464388673
MeanAbsoluteError value on hold-out data: 0.2638428211212158
Epoch: 313
Loss on hold-out set: 0.10241565763407076
MeanAbsoluteError value on hold-out data: 0.25814372301101685
Epoch: 314
Loss on hold-out set: 0.10158040363069934
MeanAbsoluteError value on hold-out data: 0.2581714987754822
Epoch: 315</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09905272210430363
MeanAbsoluteError value on hold-out data: 0.24983125925064087
Epoch: 316
Loss on hold-out set: 0.09869196314190049
MeanAbsoluteError value on hold-out data: 0.25106459856033325
Epoch: 317
Loss on hold-out set: 0.08492101464148921
MeanAbsoluteError value on hold-out data: 0.23470449447631836
Epoch: 318</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08082625684110704
MeanAbsoluteError value on hold-out data: 0.23300544917583466
Epoch: 319
Loss on hold-out set: 0.09998670940170996
MeanAbsoluteError value on hold-out data: 0.2526736855506897
Epoch: 320
Loss on hold-out set: 0.10330127284435245
MeanAbsoluteError value on hold-out data: 0.2530248165130615
Epoch: 321</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08889454584297103
MeanAbsoluteError value on hold-out data: 0.23512178659439087
Epoch: 322
Loss on hold-out set: 0.09904408855965206
MeanAbsoluteError value on hold-out data: 0.24953068792819977
Epoch: 323
Loss on hold-out set: 0.09806190613967677
MeanAbsoluteError value on hold-out data: 0.2511763870716095
Epoch: 324</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11064364952306883
MeanAbsoluteError value on hold-out data: 0.26591789722442627
Epoch: 325
Loss on hold-out set: 0.10611489981097597
MeanAbsoluteError value on hold-out data: 0.25993797183036804
Epoch: 326
Loss on hold-out set: 0.1017166347114835
MeanAbsoluteError value on hold-out data: 0.2555135488510132
Epoch: 327</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08625796181246793
MeanAbsoluteError value on hold-out data: 0.23895512521266937
Epoch: 328
Loss on hold-out set: 0.0859048586069063
MeanAbsoluteError value on hold-out data: 0.23900479078292847
Epoch: 329
Loss on hold-out set: 0.09173768926567087
MeanAbsoluteError value on hold-out data: 0.2451430708169937
Epoch: 330</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10071083870483562
MeanAbsoluteError value on hold-out data: 0.25503429770469666
Epoch: 331
Loss on hold-out set: 0.10285029636928812
MeanAbsoluteError value on hold-out data: 0.25369587540626526
Epoch: 332
Loss on hold-out set: 0.10915245198654398
MeanAbsoluteError value on hold-out data: 0.2658632695674896
Epoch: 333</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09434498528406644
MeanAbsoluteError value on hold-out data: 0.2512151598930359
Epoch: 334
Loss on hold-out set: 0.09711567693312342
MeanAbsoluteError value on hold-out data: 0.24561317265033722
Epoch: 335
Loss on hold-out set: 0.09787947815394242
MeanAbsoluteError value on hold-out data: 0.2490416020154953
Epoch: 336</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.101184158573548
MeanAbsoluteError value on hold-out data: 0.26090437173843384
Epoch: 337
Loss on hold-out set: 0.09833413654846178
MeanAbsoluteError value on hold-out data: 0.24916350841522217
Epoch: 338
Loss on hold-out set: 0.08373180114664137
MeanAbsoluteError value on hold-out data: 0.2346848100423813
Epoch: 339</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09779657736418206
MeanAbsoluteError value on hold-out data: 0.25268253684043884
Epoch: 340
Loss on hold-out set: 0.1041769944767778
MeanAbsoluteError value on hold-out data: 0.2615896761417389
Epoch: 341
Loss on hold-out set: 0.09335110782441916
MeanAbsoluteError value on hold-out data: 0.24314476549625397
Epoch: 342</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09935948323924094
MeanAbsoluteError value on hold-out data: 0.25608178973197937
Epoch: 343
Loss on hold-out set: 0.09963900986612619
MeanAbsoluteError value on hold-out data: 0.2600635886192322
Epoch: 344
Loss on hold-out set: 0.09135411145563314
MeanAbsoluteError value on hold-out data: 0.24105992913246155
Epoch: 345</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09921329752464468
MeanAbsoluteError value on hold-out data: 0.25551357865333557
Epoch: 346
Loss on hold-out set: 0.07318983603734523
MeanAbsoluteError value on hold-out data: 0.21736004948616028
Epoch: 347
Loss on hold-out set: 0.11195757158939766
MeanAbsoluteError value on hold-out data: 0.26756972074508667
Epoch: 348</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09966473340561303
MeanAbsoluteError value on hold-out data: 0.25599175691604614
Epoch: 349
Loss on hold-out set: 0.10932659663764449
MeanAbsoluteError value on hold-out data: 0.26033493876457214
Epoch: 350
Loss on hold-out set: 0.09074653106178933
MeanAbsoluteError value on hold-out data: 0.2423654943704605
Epoch: 351</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10148459664313123
MeanAbsoluteError value on hold-out data: 0.24934455752372742
Epoch: 352
Loss on hold-out set: 0.09969751699788806
MeanAbsoluteError value on hold-out data: 0.2503238022327423
Epoch: 353
Loss on hold-out set: 0.09501732416101731
MeanAbsoluteError value on hold-out data: 0.24920687079429626
Epoch: 354</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08880804930406157
MeanAbsoluteError value on hold-out data: 0.2403876781463623
Epoch: 355
Loss on hold-out set: 0.09059457238220299
MeanAbsoluteError value on hold-out data: 0.2421310693025589
Epoch: 356
Loss on hold-out set: 0.0927249404283551
MeanAbsoluteError value on hold-out data: 0.2420823872089386
Epoch: 357</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07854535228728006
MeanAbsoluteError value on hold-out data: 0.22445891797542572
Epoch: 358
Loss on hold-out set: 0.09739767363450179
MeanAbsoluteError value on hold-out data: 0.25648584961891174
Epoch: 359
Loss on hold-out set: 0.09340052747633308
MeanAbsoluteError value on hold-out data: 0.24761049449443817
Epoch: 360</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09756276916828938
MeanAbsoluteError value on hold-out data: 0.24980947375297546
Epoch: 361
Loss on hold-out set: 0.08028327326532841
MeanAbsoluteError value on hold-out data: 0.22423797845840454
Epoch: 362
Loss on hold-out set: 0.10220092518255114
MeanAbsoluteError value on hold-out data: 0.2481101006269455
Epoch: 363</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09796740549926956
MeanAbsoluteError value on hold-out data: 0.2546393871307373
Epoch: 364
Loss on hold-out set: 0.09693305348744616
MeanAbsoluteError value on hold-out data: 0.2445785254240036
Epoch: 365
Loss on hold-out set: 0.0945350938901538
MeanAbsoluteError value on hold-out data: 0.24791938066482544
Epoch: 366</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10337185089786848
MeanAbsoluteError value on hold-out data: 0.25611430406570435
Epoch: 367
Loss on hold-out set: 0.0924411107879132
MeanAbsoluteError value on hold-out data: 0.24887405335903168
Epoch: 368
Loss on hold-out set: 0.10371094587705253
MeanAbsoluteError value on hold-out data: 0.2529853880405426
Epoch: 369</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11040098678381885
MeanAbsoluteError value on hold-out data: 0.26649895310401917
Epoch: 370
Loss on hold-out set: 0.09963117587224891
MeanAbsoluteError value on hold-out data: 0.24891260266304016
Epoch: 371
Loss on hold-out set: 0.09739644156036471
MeanAbsoluteError value on hold-out data: 0.24881510436534882
Epoch: 372</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09486391849136756
MeanAbsoluteError value on hold-out data: 0.24813519418239594
Epoch: 373
Loss on hold-out set: 0.08762856728862971
MeanAbsoluteError value on hold-out data: 0.2406335175037384
Epoch: 374
Loss on hold-out set: 0.07840883122116793
MeanAbsoluteError value on hold-out data: 0.22102010250091553
Epoch: 375</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08073819870022514
MeanAbsoluteError value on hold-out data: 0.2251976579427719
Epoch: 376
Loss on hold-out set: 0.09052288512835124
MeanAbsoluteError value on hold-out data: 0.2344832420349121
Epoch: 377
Loss on hold-out set: 0.0997913699860995
MeanAbsoluteError value on hold-out data: 0.24682092666625977
Epoch: 378</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09770269146111484
MeanAbsoluteError value on hold-out data: 0.24954809248447418
Early stopping at epoch 377
Returned to Spot: Validation loss: 0.09770269146111484
----------------------------------------------
spotPython tuning: 0.0800885943544563 [######----] 56.62% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
config: {'_L_in': 10, '_L_out': 1, 'l1': 128, 'dropout_prob': 0.7135267315833492, 'lr_mult': 0.001, 'batch_size': 4, 'epochs': 128, 'k_folds': 1, 'patience': 128, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}
Epoch: 1
Loss on hold-out set: 0.3869756199916204
MeanAbsoluteError value on hold-out data: 0.5870682597160339
Epoch: 2
Loss on hold-out set: 0.38313414047161737
MeanAbsoluteError value on hold-out data: 0.5820286870002747
Epoch: 3
Loss on hold-out set: 0.3906909700234731
MeanAbsoluteError value on hold-out data: 0.5906230807304382
Epoch: 4</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3827437058091164
MeanAbsoluteError value on hold-out data: 0.5804731845855713
Epoch: 5
Loss on hold-out set: 0.3830435069402059
MeanAbsoluteError value on hold-out data: 0.5821036100387573
Epoch: 6
Loss on hold-out set: 0.3705615818500519
MeanAbsoluteError value on hold-out data: 0.5722843408584595
Epoch: 7
Loss on hold-out set: 0.3831476257244746
MeanAbsoluteError value on hold-out data: 0.5831624865531921
Epoch: 8
Loss on hold-out set: 0.3615207358201345
MeanAbsoluteError value on hold-out data: 0.5643625855445862
Epoch: 9</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.37241503775119783
MeanAbsoluteError value on hold-out data: 0.5778858661651611
Epoch: 10
Loss on hold-out set: 0.36311961660782494
MeanAbsoluteError value on hold-out data: 0.5636604428291321
Epoch: 11
Loss on hold-out set: 0.3416154684623082
MeanAbsoluteError value on hold-out data: 0.546600878238678
Epoch: 12
Loss on hold-out set: 0.3564212131500244
MeanAbsoluteError value on hold-out data: 0.5608441233634949
Epoch: 13
Loss on hold-out set: 0.3534911479552587
MeanAbsoluteError value on hold-out data: 0.554561197757721
Epoch: 14</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.34345155527194343
MeanAbsoluteError value on hold-out data: 0.5482335686683655
Epoch: 15
Loss on hold-out set: 0.3479899846514066
MeanAbsoluteError value on hold-out data: 0.5528331995010376
Epoch: 16
Loss on hold-out set: 0.3463412819306056
MeanAbsoluteError value on hold-out data: 0.5526225566864014
Epoch: 17
Loss on hold-out set: 0.3250648838281631
MeanAbsoluteError value on hold-out data: 0.5319401025772095
Epoch: 18
Loss on hold-out set: 0.3255272462964058
MeanAbsoluteError value on hold-out data: 0.5324758887290955
Epoch: 19</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3282787432273229
MeanAbsoluteError value on hold-out data: 0.5365384221076965
Epoch: 20
Loss on hold-out set: 0.3291706364353498
MeanAbsoluteError value on hold-out data: 0.5347241759300232
Epoch: 21
Loss on hold-out set: 0.3334917724132538
MeanAbsoluteError value on hold-out data: 0.5377479791641235
Epoch: 22
Loss on hold-out set: 0.3209902127583822
MeanAbsoluteError value on hold-out data: 0.5262932777404785
Epoch: 23
Loss on hold-out set: 0.30595049654444056
MeanAbsoluteError value on hold-out data: 0.5156698822975159
Epoch: 24</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3114659249782562
MeanAbsoluteError value on hold-out data: 0.5175855159759521
Epoch: 25
Loss on hold-out set: 0.297397866944472
MeanAbsoluteError value on hold-out data: 0.5091090798377991
Epoch: 26
Loss on hold-out set: 0.29587476551532743
MeanAbsoluteError value on hold-out data: 0.5052630305290222
Epoch: 27
Loss on hold-out set: 0.30521112461884814
MeanAbsoluteError value on hold-out data: 0.5161533951759338
Epoch: 28
Loss on hold-out set: 0.304279192785422
MeanAbsoluteError value on hold-out data: 0.5089150667190552
Epoch: 29</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2994864018758138
MeanAbsoluteError value on hold-out data: 0.5069103240966797
Epoch: 30
Loss on hold-out set: 0.2976625599463781
MeanAbsoluteError value on hold-out data: 0.5057229399681091
Epoch: 31
Loss on hold-out set: 0.28092212080955503
MeanAbsoluteError value on hold-out data: 0.4907175600528717
Epoch: 32
Loss on hold-out set: 0.298759198486805
MeanAbsoluteError value on hold-out data: 0.5107173919677734
Epoch: 33
Loss on hold-out set: 0.29305677781502404
MeanAbsoluteError value on hold-out data: 0.5034879446029663
Epoch: 34</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.29342260549465815
MeanAbsoluteError value on hold-out data: 0.5016157031059265
Epoch: 35
Loss on hold-out set: 0.27411746472120285
MeanAbsoluteError value on hold-out data: 0.4836919605731964
Epoch: 36
Loss on hold-out set: 0.2853075847029686
MeanAbsoluteError value on hold-out data: 0.49342748522758484
Epoch: 37
Loss on hold-out set: 0.2718330389261246
MeanAbsoluteError value on hold-out data: 0.47869592905044556
Epoch: 38
Loss on hold-out set: 0.2654052045941353
MeanAbsoluteError value on hold-out data: 0.4733573794364929
Epoch: 39</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.26779606511195503
MeanAbsoluteError value on hold-out data: 0.4768977761268616
Epoch: 40
Loss on hold-out set: 0.2669440055886904
MeanAbsoluteError value on hold-out data: 0.47624388337135315
Epoch: 41
Loss on hold-out set: 0.2694266374905904
MeanAbsoluteError value on hold-out data: 0.4792735278606415
Epoch: 42
Loss on hold-out set: 0.2569913506507874
MeanAbsoluteError value on hold-out data: 0.4686296582221985
Epoch: 43
Loss on hold-out set: 0.2596273785829544
MeanAbsoluteError value on hold-out data: 0.4691150486469269
Epoch: 44</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.24682560354471206
MeanAbsoluteError value on hold-out data: 0.4571458399295807
Epoch: 45
Loss on hold-out set: 0.25502285530169805
MeanAbsoluteError value on hold-out data: 0.46080470085144043
Epoch: 46
Loss on hold-out set: 0.24449285392959913
MeanAbsoluteError value on hold-out data: 0.45041128993034363
Epoch: 47
Loss on hold-out set: 0.24736673146486282
MeanAbsoluteError value on hold-out data: 0.45714065432548523
Epoch: 48
Loss on hold-out set: 0.25246167689561844
MeanAbsoluteError value on hold-out data: 0.46010899543762207
Epoch: 49</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.23893017957607904
MeanAbsoluteError value on hold-out data: 0.44388553500175476
Epoch: 50
Loss on hold-out set: 0.22690321306387584
MeanAbsoluteError value on hold-out data: 0.4364560544490814
Epoch: 51
Loss on hold-out set: 0.23262501100699107
MeanAbsoluteError value on hold-out data: 0.44028300046920776
Epoch: 52
Loss on hold-out set: 0.2418971021970113
MeanAbsoluteError value on hold-out data: 0.4471893906593323
Epoch: 53
Loss on hold-out set: 0.2319105086227258
MeanAbsoluteError value on hold-out data: 0.43450552225112915
Epoch: 54</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2151485891143481
MeanAbsoluteError value on hold-out data: 0.4192073345184326
Epoch: 55
Loss on hold-out set: 0.21055910115440687
MeanAbsoluteError value on hold-out data: 0.4218425452709198
Epoch: 56
Loss on hold-out set: 0.21049771050612132
MeanAbsoluteError value on hold-out data: 0.417337566614151
Epoch: 57
Loss on hold-out set: 0.23432016770044964
MeanAbsoluteError value on hold-out data: 0.4409906566143036
Epoch: 58
Loss on hold-out set: 0.2164562034110228
MeanAbsoluteError value on hold-out data: 0.4216163158416748
Epoch: 59</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.21254760513703028
MeanAbsoluteError value on hold-out data: 0.4171876907348633
Epoch: 60
Loss on hold-out set: 0.20591877897580466
MeanAbsoluteError value on hold-out data: 0.4149273931980133
Epoch: 61
Loss on hold-out set: 0.2137659126520157
MeanAbsoluteError value on hold-out data: 0.41922417283058167
Epoch: 62
Loss on hold-out set: 0.20737884908914567
MeanAbsoluteError value on hold-out data: 0.4130358099937439
Epoch: 63
Loss on hold-out set: 0.20009706631302834
MeanAbsoluteError value on hold-out data: 0.40343111753463745
Epoch: 64</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1958049263060093
MeanAbsoluteError value on hold-out data: 0.39097821712493896
Epoch: 65
Loss on hold-out set: 0.19227868886043628
MeanAbsoluteError value on hold-out data: 0.38735613226890564
Epoch: 66
Loss on hold-out set: 0.19808255324761073
MeanAbsoluteError value on hold-out data: 0.4015730321407318
Epoch: 67
Loss on hold-out set: 0.19545658101638158
MeanAbsoluteError value on hold-out data: 0.39829179644584656
Epoch: 68
Loss on hold-out set: 0.19100918566187222
MeanAbsoluteError value on hold-out data: 0.39029014110565186
Epoch: 69</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.19314903035759926
MeanAbsoluteError value on hold-out data: 0.39322444796562195
Epoch: 70
Loss on hold-out set: 0.19346959655483564
MeanAbsoluteError value on hold-out data: 0.3892301321029663
Epoch: 71
Loss on hold-out set: 0.1900162475804488
MeanAbsoluteError value on hold-out data: 0.3880562484264374
Epoch: 72
Loss on hold-out set: 0.19600291873017947
MeanAbsoluteError value on hold-out data: 0.3986428678035736
Epoch: 73
Loss on hold-out set: 0.1750566628575325
MeanAbsoluteError value on hold-out data: 0.37301361560821533
Epoch: 74</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.17688922877113025
MeanAbsoluteError value on hold-out data: 0.37185800075531006
Epoch: 75
Loss on hold-out set: 0.1690332696090142
MeanAbsoluteError value on hold-out data: 0.3657897710800171
Epoch: 76
Loss on hold-out set: 0.1749995634953181
MeanAbsoluteError value on hold-out data: 0.37585771083831787
Epoch: 77
Loss on hold-out set: 0.16371362486233315
MeanAbsoluteError value on hold-out data: 0.35611769556999207
Epoch: 78
Loss on hold-out set: 0.16637839968005816
MeanAbsoluteError value on hold-out data: 0.35835030674934387
Epoch: 79</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.17788092826803525
MeanAbsoluteError value on hold-out data: 0.37445178627967834
Epoch: 80
Loss on hold-out set: 0.16605780221521854
MeanAbsoluteError value on hold-out data: 0.36173465847969055
Epoch: 81
Loss on hold-out set: 0.1621774427096049
MeanAbsoluteError value on hold-out data: 0.3594120442867279
Epoch: 82
Loss on hold-out set: 0.1653184154133002
MeanAbsoluteError value on hold-out data: 0.35824349522590637
Epoch: 83
Loss on hold-out set: 0.17046693911155064
MeanAbsoluteError value on hold-out data: 0.3633912205696106
Epoch: 84</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.15943498025337854
MeanAbsoluteError value on hold-out data: 0.3490849733352661
Epoch: 85
Loss on hold-out set: 0.14815448619425298
MeanAbsoluteError value on hold-out data: 0.33296552300453186
Epoch: 86
Loss on hold-out set: 0.15260865289717912
MeanAbsoluteError value on hold-out data: 0.3377334177494049
Epoch: 87
Loss on hold-out set: 0.1601749376455943
MeanAbsoluteError value on hold-out data: 0.35260093212127686
Epoch: 88
Loss on hold-out set: 0.15377231419086457
MeanAbsoluteError value on hold-out data: 0.3440048098564148
Epoch: 89</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.15071369368582965
MeanAbsoluteError value on hold-out data: 0.33814525604248047
Epoch: 90
Loss on hold-out set: 0.15570017072061698
MeanAbsoluteError value on hold-out data: 0.34425511956214905
Epoch: 91
Loss on hold-out set: 0.15076902526120345
MeanAbsoluteError value on hold-out data: 0.33209431171417236
Epoch: 92
Loss on hold-out set: 0.1467506339897712
MeanAbsoluteError value on hold-out data: 0.33648020029067993
Epoch: 93
Loss on hold-out set: 0.16019500623146693
MeanAbsoluteError value on hold-out data: 0.34666094183921814
Epoch: 94</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14654857084155082
MeanAbsoluteError value on hold-out data: 0.32720711827278137
Epoch: 95
Loss on hold-out set: 0.13435004904866218
MeanAbsoluteError value on hold-out data: 0.31809550523757935
Epoch: 96
Loss on hold-out set: 0.13349472482999167
MeanAbsoluteError value on hold-out data: 0.3145252764225006
Epoch: 97
Loss on hold-out set: 0.13434532813727856
MeanAbsoluteError value on hold-out data: 0.3195493519306183
Epoch: 98
Loss on hold-out set: 0.13138174841801326
MeanAbsoluteError value on hold-out data: 0.3172186315059662
Epoch: 99</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12474809244275092
MeanAbsoluteError value on hold-out data: 0.3036729693412781
Epoch: 100
Loss on hold-out set: 0.1286641858021418
MeanAbsoluteError value on hold-out data: 0.308639794588089
Epoch: 101
Loss on hold-out set: 0.1189276294534405
MeanAbsoluteError value on hold-out data: 0.29511377215385437
Epoch: 102
Loss on hold-out set: 0.12927589024106662
MeanAbsoluteError value on hold-out data: 0.30656683444976807
Epoch: 103
Loss on hold-out set: 0.1158743541687727
MeanAbsoluteError value on hold-out data: 0.2820979356765747
Epoch: 104</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11820991722246012
MeanAbsoluteError value on hold-out data: 0.2980431914329529
Epoch: 105
Loss on hold-out set: 0.12251925345510245
MeanAbsoluteError value on hold-out data: 0.29504626989364624
Epoch: 106
Loss on hold-out set: 0.11218651612599691
MeanAbsoluteError value on hold-out data: 0.28777819871902466
Epoch: 107
Loss on hold-out set: 0.11998797796666621
MeanAbsoluteError value on hold-out data: 0.2978927195072174
Epoch: 108
Loss on hold-out set: 0.1134148528923591
MeanAbsoluteError value on hold-out data: 0.29213693737983704
Epoch: 109</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11728699127833049
MeanAbsoluteError value on hold-out data: 0.2941322326660156
Epoch: 110
Loss on hold-out set: 0.12247113092492024
MeanAbsoluteError value on hold-out data: 0.293978214263916
Epoch: 111
Loss on hold-out set: 0.10924962304532528
MeanAbsoluteError value on hold-out data: 0.2820943593978882
Epoch: 112
Loss on hold-out set: 0.10735259465873241
MeanAbsoluteError value on hold-out data: 0.2749733626842499
Epoch: 113
Loss on hold-out set: 0.10723513007164001
MeanAbsoluteError value on hold-out data: 0.2795497477054596
Epoch: 114</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11348840991655985
MeanAbsoluteError value on hold-out data: 0.28201863169670105
Epoch: 115
Loss on hold-out set: 0.10368381564815839
MeanAbsoluteError value on hold-out data: 0.2694663405418396
Epoch: 116
Loss on hold-out set: 0.10201366027196249
MeanAbsoluteError value on hold-out data: 0.26623353362083435
Epoch: 117
Loss on hold-out set: 0.09643271669745446
MeanAbsoluteError value on hold-out data: 0.25793516635894775
Epoch: 118
Loss on hold-out set: 0.10216283114006122
MeanAbsoluteError value on hold-out data: 0.2681961953639984
Epoch: 119</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10593739688396454
MeanAbsoluteError value on hold-out data: 0.27138879895210266
Epoch: 120
Loss on hold-out set: 0.09387460425496101
MeanAbsoluteError value on hold-out data: 0.2504810690879822
Epoch: 121
Loss on hold-out set: 0.09489070390661558
MeanAbsoluteError value on hold-out data: 0.2611473500728607
Epoch: 122
Loss on hold-out set: 0.0950024422009786
MeanAbsoluteError value on hold-out data: 0.2627328336238861
Epoch: 123
Loss on hold-out set: 0.09121562522525589
MeanAbsoluteError value on hold-out data: 0.25149184465408325
Epoch: 124</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09430642290661732
MeanAbsoluteError value on hold-out data: 0.24901829659938812
Epoch: 125
Loss on hold-out set: 0.10134427398443221
MeanAbsoluteError value on hold-out data: 0.2713977098464966
Epoch: 126
Loss on hold-out set: 0.10126182333255808
MeanAbsoluteError value on hold-out data: 0.2669573426246643
Epoch: 127
Loss on hold-out set: 0.09552455986539522
MeanAbsoluteError value on hold-out data: 0.2555724084377289
Epoch: 128
Loss on hold-out set: 0.09144544609511893
MeanAbsoluteError value on hold-out data: 0.24809971451759338
Returned to Spot: Validation loss: 0.09144544609511893
----------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>spotPython tuning: 0.0800885943544563 [#######---] 67.71% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
config: {'_L_in': 10, '_L_out': 1, 'l1': 128, 'dropout_prob': 0.6872970752802665, 'lr_mult': 0.001, 'batch_size': 4, 'epochs': 128, 'k_folds': 1, 'patience': 128, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}
Epoch: 1
Loss on hold-out set: 0.3093730019529661
MeanAbsoluteError value on hold-out data: 0.5163847208023071
Epoch: 2</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.3043309567372004
MeanAbsoluteError value on hold-out data: 0.5128142833709717
Epoch: 3
Loss on hold-out set: 0.2892606331904729
MeanAbsoluteError value on hold-out data: 0.49284306168556213
Epoch: 4</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2956153804063797
MeanAbsoluteError value on hold-out data: 0.506260097026825
Epoch: 5
Loss on hold-out set: 0.2982572265466054
MeanAbsoluteError value on hold-out data: 0.5000772476196289
Epoch: 6</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2903728391726812
MeanAbsoluteError value on hold-out data: 0.5022059082984924
Epoch: 7
Loss on hold-out set: 0.2867248164614042
MeanAbsoluteError value on hold-out data: 0.49675309658050537
Epoch: 8</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2833206707239151
MeanAbsoluteError value on hold-out data: 0.48824548721313477
Epoch: 9
Loss on hold-out set: 0.28698371509710946
MeanAbsoluteError value on hold-out data: 0.49150294065475464
Epoch: 10
Loss on hold-out set: 0.27008480340242386
MeanAbsoluteError value on hold-out data: 0.4800601601600647</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 11
Loss on hold-out set: 0.2763152082761129
MeanAbsoluteError value on hold-out data: 0.48124343156814575
Epoch: 12
Loss on hold-out set: 0.27962409416834516
MeanAbsoluteError value on hold-out data: 0.4877188503742218
Epoch: 13</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2752793069680532
MeanAbsoluteError value on hold-out data: 0.4836302101612091
Epoch: 14
Loss on hold-out set: 0.2650074648857117
MeanAbsoluteError value on hold-out data: 0.4742633104324341
Epoch: 15</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.27022735511263213
MeanAbsoluteError value on hold-out data: 0.47723668813705444
Epoch: 16
Loss on hold-out set: 0.26510672355691595
MeanAbsoluteError value on hold-out data: 0.46879512071609497
Epoch: 17
Loss on hold-out set: 0.26533392737309136
MeanAbsoluteError value on hold-out data: 0.4708438217639923
Epoch: 18</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.24797597100337346
MeanAbsoluteError value on hold-out data: 0.45472976565361023
Epoch: 19
Loss on hold-out set: 0.2636282495657603
MeanAbsoluteError value on hold-out data: 0.4684503674507141
Epoch: 20</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.24650216301282246
MeanAbsoluteError value on hold-out data: 0.4509059190750122
Epoch: 21
Loss on hold-out set: 0.2462476028005282
MeanAbsoluteError value on hold-out data: 0.45154622197151184
Epoch: 22</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.26354690353075666
MeanAbsoluteError value on hold-out data: 0.466514527797699
Epoch: 23
Loss on hold-out set: 0.2398198905090491
MeanAbsoluteError value on hold-out data: 0.4494021236896515
Epoch: 24</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.23737950752178827
MeanAbsoluteError value on hold-out data: 0.44630351662635803
Epoch: 25
Loss on hold-out set: 0.22785047550996146
MeanAbsoluteError value on hold-out data: 0.4335591495037079
Epoch: 26</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2198425723860661
MeanAbsoluteError value on hold-out data: 0.42470577359199524
Epoch: 27
Loss on hold-out set: 0.23363647247354188
MeanAbsoluteError value on hold-out data: 0.439311683177948
Epoch: 28</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.22323018004496892
MeanAbsoluteError value on hold-out data: 0.43381693959236145
Epoch: 29
Loss on hold-out set: 0.22316028734048207
MeanAbsoluteError value on hold-out data: 0.4279538094997406
Epoch: 30
Loss on hold-out set: 0.2244773550828298
MeanAbsoluteError value on hold-out data: 0.429585725069046
Epoch: 31</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.22087402363618214
MeanAbsoluteError value on hold-out data: 0.41916054487228394
Epoch: 32
Loss on hold-out set: 0.2150897400577863
MeanAbsoluteError value on hold-out data: 0.41463977098464966
Epoch: 33</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.21376403542856376
MeanAbsoluteError value on hold-out data: 0.4174366891384125
Epoch: 34
Loss on hold-out set: 0.20941990574200947
MeanAbsoluteError value on hold-out data: 0.40838131308555603
Epoch: 35</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.213296466867129
MeanAbsoluteError value on hold-out data: 0.41091400384902954
Epoch: 36
Loss on hold-out set: 0.2040355108429988
MeanAbsoluteError value on hold-out data: 0.4051058888435364
Epoch: 37
Loss on hold-out set: 0.19671465237935384
MeanAbsoluteError value on hold-out data: 0.39892664551734924
Epoch: 38</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.20732417086760202
MeanAbsoluteError value on hold-out data: 0.4086630046367645
Epoch: 39
Loss on hold-out set: 0.20723472811281682
MeanAbsoluteError value on hold-out data: 0.4072141647338867
Epoch: 40</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.19073714718222617
MeanAbsoluteError value on hold-out data: 0.3875740170478821
Epoch: 41
Loss on hold-out set: 0.19302094886700313
MeanAbsoluteError value on hold-out data: 0.3902650773525238
Epoch: 42
Loss on hold-out set: 0.1810104578733444
MeanAbsoluteError value on hold-out data: 0.38195082545280457
Epoch: 43</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.17722262879212697
MeanAbsoluteError value on hold-out data: 0.37704339623451233
Epoch: 44
Loss on hold-out set: 0.17555132006605467
MeanAbsoluteError value on hold-out data: 0.37133336067199707
Epoch: 45</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.18518391162157058
MeanAbsoluteError value on hold-out data: 0.38185837864875793
Epoch: 46
Loss on hold-out set: 0.1775117226690054
MeanAbsoluteError value on hold-out data: 0.37295350432395935
Epoch: 47
Loss on hold-out set: 0.1755660196642081
MeanAbsoluteError value on hold-out data: 0.37099820375442505
Epoch: 48</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.16435564746459325
MeanAbsoluteError value on hold-out data: 0.355783075094223
Epoch: 49
Loss on hold-out set: 0.17605029804011185
MeanAbsoluteError value on hold-out data: 0.37331533432006836
Epoch: 50</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.17001703863342604
MeanAbsoluteError value on hold-out data: 0.363822340965271
Epoch: 51
Loss on hold-out set: 0.16834478030602137
MeanAbsoluteError value on hold-out data: 0.3602946400642395
Epoch: 52</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1688321003317833
MeanAbsoluteError value on hold-out data: 0.358140230178833
Epoch: 53
Loss on hold-out set: 0.1690439114222924
MeanAbsoluteError value on hold-out data: 0.36147579550743103
Epoch: 54
Loss on hold-out set: 0.17233786692221958
MeanAbsoluteError value on hold-out data: 0.36319273710250854
Epoch: 55</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.15612428764502206
MeanAbsoluteError value on hold-out data: 0.34534478187561035
Epoch: 56
Loss on hold-out set: 0.15970078493158021
MeanAbsoluteError value on hold-out data: 0.3480951189994812
Epoch: 57</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14833041434486707
MeanAbsoluteError value on hold-out data: 0.33384695649147034
Epoch: 58
Loss on hold-out set: 0.1547037283082803
MeanAbsoluteError value on hold-out data: 0.33957478404045105
Epoch: 59
Loss on hold-out set: 0.15590120861927667
MeanAbsoluteError value on hold-out data: 0.34220293164253235
Epoch: 60</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14528140726188818
MeanAbsoluteError value on hold-out data: 0.329572856426239
Epoch: 61
Loss on hold-out set: 0.1407043079038461
MeanAbsoluteError value on hold-out data: 0.3287371098995209
Epoch: 62</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14064204399784405
MeanAbsoluteError value on hold-out data: 0.3256448805332184
Epoch: 63
Loss on hold-out set: 0.14086911151806514
MeanAbsoluteError value on hold-out data: 0.3317888379096985
Epoch: 64
Loss on hold-out set: 0.14231873807807763
MeanAbsoluteError value on hold-out data: 0.3299867808818817
Epoch: 65</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1405111609896024
MeanAbsoluteError value on hold-out data: 0.32386481761932373
Epoch: 66
Loss on hold-out set: 0.14093283583720526
MeanAbsoluteError value on hold-out data: 0.3251991868019104
Epoch: 67</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.14125210255384446
MeanAbsoluteError value on hold-out data: 0.3283649682998657
Epoch: 68
Loss on hold-out set: 0.1310523422062397
MeanAbsoluteError value on hold-out data: 0.3146977424621582
Epoch: 69
Loss on hold-out set: 0.13187499749163786
MeanAbsoluteError value on hold-out data: 0.3083782196044922</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 70
Loss on hold-out set: 0.12808902685840926
MeanAbsoluteError value on hold-out data: 0.30536088347435
Epoch: 71</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13056844539940357
MeanAbsoluteError value on hold-out data: 0.3074202835559845
Epoch: 72
Loss on hold-out set: 0.1273248118410508
MeanAbsoluteError value on hold-out data: 0.30493399500846863
Epoch: 73
Loss on hold-out set: 0.12665398235122363
MeanAbsoluteError value on hold-out data: 0.30516472458839417
Epoch: 74</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12638736044367155
MeanAbsoluteError value on hold-out data: 0.30006399750709534
Epoch: 75
Loss on hold-out set: 0.12005186726649603
MeanAbsoluteError value on hold-out data: 0.29853153228759766
Epoch: 76</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.12208623925844829
MeanAbsoluteError value on hold-out data: 0.2982252538204193
Epoch: 77
Loss on hold-out set: 0.12079677678644657
MeanAbsoluteError value on hold-out data: 0.2959551215171814
Epoch: 78
Loss on hold-out set: 0.10747716079155605
MeanAbsoluteError value on hold-out data: 0.2764030992984772
Epoch: 79</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10953840747475624
MeanAbsoluteError value on hold-out data: 0.2865781784057617
Epoch: 80
Loss on hold-out set: 0.10935370010634263
MeanAbsoluteError value on hold-out data: 0.28362318873405457
Epoch: 81</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11928601277371248
MeanAbsoluteError value on hold-out data: 0.2880234718322754
Epoch: 82
Loss on hold-out set: 0.11854493162284295
MeanAbsoluteError value on hold-out data: 0.2969725430011749
Epoch: 83
Loss on hold-out set: 0.10517810409267743
MeanAbsoluteError value on hold-out data: 0.27639082074165344
Epoch: 84</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11008137544617057
MeanAbsoluteError value on hold-out data: 0.28012001514434814
Epoch: 85
Loss on hold-out set: 0.1039415238921841
MeanAbsoluteError value on hold-out data: 0.26928946375846863
Epoch: 86</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10163634361078341
MeanAbsoluteError value on hold-out data: 0.2660697400569916
Epoch: 87
Loss on hold-out set: 0.1091098544249932
MeanAbsoluteError value on hold-out data: 0.270637184381485
Epoch: 88
Loss on hold-out set: 0.10200368742148082
MeanAbsoluteError value on hold-out data: 0.26451191306114197
Epoch: 89</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1000593410183986
MeanAbsoluteError value on hold-out data: 0.2684409022331238
Epoch: 90
Loss on hold-out set: 0.10720110350598891
MeanAbsoluteError value on hold-out data: 0.27091628313064575
Epoch: 91</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10280907599255443
MeanAbsoluteError value on hold-out data: 0.26310276985168457
Epoch: 92
Loss on hold-out set: 0.10491369916747013
MeanAbsoluteError value on hold-out data: 0.27053147554397583
Epoch: 93
Loss on hold-out set: 0.09874883926163117
MeanAbsoluteError value on hold-out data: 0.2670591473579407
Epoch: 94</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10284560178716977
MeanAbsoluteError value on hold-out data: 0.2678760588169098
Epoch: 95
Loss on hold-out set: 0.08599148411303759
MeanAbsoluteError value on hold-out data: 0.24770614504814148
Epoch: 96</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09390623973061642
MeanAbsoluteError value on hold-out data: 0.2582496702671051
Epoch: 97
Loss on hold-out set: 0.09833253871028622
MeanAbsoluteError value on hold-out data: 0.26200324296951294
Epoch: 98
Loss on hold-out set: 0.08606096401810646
MeanAbsoluteError value on hold-out data: 0.2406795769929886
Epoch: 99</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09668089436988035
MeanAbsoluteError value on hold-out data: 0.2590382695198059
Epoch: 100
Loss on hold-out set: 0.0866709676819543
MeanAbsoluteError value on hold-out data: 0.23808886110782623
Epoch: 101</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08305572015543779
MeanAbsoluteError value on hold-out data: 0.24122609198093414
Epoch: 102
Loss on hold-out set: 0.08929613741735618
MeanAbsoluteError value on hold-out data: 0.24686568975448608
Epoch: 103
Loss on hold-out set: 0.08168599487592777
MeanAbsoluteError value on hold-out data: 0.23282310366630554
Epoch: 104</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08819124396890402
MeanAbsoluteError value on hold-out data: 0.23965975642204285
Epoch: 105
Loss on hold-out set: 0.08362914698198437
MeanAbsoluteError value on hold-out data: 0.24069923162460327
Epoch: 106</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07802170542844881
MeanAbsoluteError value on hold-out data: 0.22414584457874298
Epoch: 107
Loss on hold-out set: 0.08144638801614444
MeanAbsoluteError value on hold-out data: 0.23551124334335327
Epoch: 108
Loss on hold-out set: 0.07860119260847569
MeanAbsoluteError value on hold-out data: 0.2280121147632599
Epoch: 109</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08298396760597825
MeanAbsoluteError value on hold-out data: 0.2384718507528305
Epoch: 110
Loss on hold-out set: 0.0778092085570097
MeanAbsoluteError value on hold-out data: 0.2322790026664734
Epoch: 111</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07890863329172135
MeanAbsoluteError value on hold-out data: 0.22819264233112335
Epoch: 112
Loss on hold-out set: 0.08328013386577368
MeanAbsoluteError value on hold-out data: 0.237274169921875
Epoch: 113
Loss on hold-out set: 0.07420680133004984
MeanAbsoluteError value on hold-out data: 0.22451630234718323
Epoch: 114</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07956076804548502
MeanAbsoluteError value on hold-out data: 0.2344481647014618
Epoch: 115
Loss on hold-out set: 0.07017390753918638
MeanAbsoluteError value on hold-out data: 0.2166406810283661
Epoch: 116</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07663844416538874
MeanAbsoluteError value on hold-out data: 0.22669216990470886
Epoch: 117
Loss on hold-out set: 0.0744463819762071
MeanAbsoluteError value on hold-out data: 0.21890632808208466
Epoch: 118
Loss on hold-out set: 0.0728543790554007
MeanAbsoluteError value on hold-out data: 0.22035939991474152
Epoch: 119</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07349796436727046
MeanAbsoluteError value on hold-out data: 0.22258326411247253
Epoch: 120
Loss on hold-out set: 0.07524941165621082
MeanAbsoluteError value on hold-out data: 0.22681327164173126
Epoch: 121</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0727734740326802
MeanAbsoluteError value on hold-out data: 0.2178013175725937
Epoch: 122
Loss on hold-out set: 0.06819305568002164
MeanAbsoluteError value on hold-out data: 0.20833991467952728
Epoch: 123
Loss on hold-out set: 0.06812061826388041
MeanAbsoluteError value on hold-out data: 0.21062560379505157
Epoch: 124</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07322782378643751
MeanAbsoluteError value on hold-out data: 0.22108067572116852
Epoch: 125
Loss on hold-out set: 0.07005781129002571
MeanAbsoluteError value on hold-out data: 0.21741162240505219
Epoch: 126</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.06706245828419924
MeanAbsoluteError value on hold-out data: 0.20643749833106995
Epoch: 127
Loss on hold-out set: 0.07251376112302145
MeanAbsoluteError value on hold-out data: 0.22158604860305786
Epoch: 128
Loss on hold-out set: 0.06696127450714509
MeanAbsoluteError value on hold-out data: 0.21114708483219147
Returned to Spot: Validation loss: 0.06696127450714509
----------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>spotPython tuning: 0.06696127450714509 [########--] 78.80% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
config: {'_L_in': 10, '_L_out': 1, 'l1': 128, 'dropout_prob': 0.5252411128304804, 'lr_mult': 0.001, 'batch_size': 4, 'epochs': 128, 'k_folds': 1, 'patience': 128, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}
Epoch: 1</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2635013208786647
MeanAbsoluteError value on hold-out data: 0.479971319437027
Epoch: 2
Loss on hold-out set: 0.2593793775637945
MeanAbsoluteError value on hold-out data: 0.4751169979572296
Epoch: 3
Loss on hold-out set: 0.25128434111674625
MeanAbsoluteError value on hold-out data: 0.46925705671310425
Epoch: 4</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2548475821812948
MeanAbsoluteError value on hold-out data: 0.47130101919174194
Epoch: 5
Loss on hold-out set: 0.24688415020704269
MeanAbsoluteError value on hold-out data: 0.46639537811279297
Epoch: 6</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.24667746697862944
MeanAbsoluteError value on hold-out data: 0.4630577564239502
Epoch: 7
Loss on hold-out set: 0.23475457241137823
MeanAbsoluteError value on hold-out data: 0.44877028465270996
Epoch: 8
Loss on hold-out set: 0.2320116427540779
MeanAbsoluteError value on hold-out data: 0.4470086097717285
Epoch: 9</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.23451914191246032
MeanAbsoluteError value on hold-out data: 0.45043569803237915
Epoch: 10
Loss on hold-out set: 0.22582463115453721
MeanAbsoluteError value on hold-out data: 0.440849244594574
Epoch: 11</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2333037029703458
MeanAbsoluteError value on hold-out data: 0.44878870248794556
Epoch: 12
Loss on hold-out set: 0.21374910538395245
MeanAbsoluteError value on hold-out data: 0.4290929138660431
Epoch: 13
Loss on hold-out set: 0.2188911708196004
MeanAbsoluteError value on hold-out data: 0.4318825900554657
Epoch: 14</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.21271408041318257
MeanAbsoluteError value on hold-out data: 0.4275589883327484
Epoch: 15
Loss on hold-out set: 0.21449944814046223
MeanAbsoluteError value on hold-out data: 0.4265899658203125
Epoch: 16</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.2092850827674071
MeanAbsoluteError value on hold-out data: 0.42211002111434937
Epoch: 17
Loss on hold-out set: 0.20493940432866414
MeanAbsoluteError value on hold-out data: 0.41466718912124634
Epoch: 18
Loss on hold-out set: 0.1976477434237798
MeanAbsoluteError value on hold-out data: 0.40514999628067017
Epoch: 19</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.20016110613942145
MeanAbsoluteError value on hold-out data: 0.4088449478149414
Epoch: 20
Loss on hold-out set: 0.19388459424177806
MeanAbsoluteError value on hold-out data: 0.4049430787563324
Epoch: 21</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.18500346193710962
MeanAbsoluteError value on hold-out data: 0.39550530910491943
Epoch: 22
Loss on hold-out set: 0.18796198318401972
MeanAbsoluteError value on hold-out data: 0.3953830599784851
Epoch: 23
Loss on hold-out set: 0.17877769316236178
MeanAbsoluteError value on hold-out data: 0.3866729438304901
Epoch: 24</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.18619969854752222
MeanAbsoluteError value on hold-out data: 0.3932362496852875
Epoch: 25
Loss on hold-out set: 0.17280966639518738
MeanAbsoluteError value on hold-out data: 0.377464234828949
Epoch: 26</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.17806998620430628
MeanAbsoluteError value on hold-out data: 0.3848539888858795
Epoch: 27
Loss on hold-out set: 0.17569886629780135
MeanAbsoluteError value on hold-out data: 0.38205036520957947
Epoch: 28
Loss on hold-out set: 0.1713029460608959
MeanAbsoluteError value on hold-out data: 0.3756510615348816
Epoch: 29</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.16547263875603677
MeanAbsoluteError value on hold-out data: 0.36767908930778503
Epoch: 30
Loss on hold-out set: 0.1692646616200606
MeanAbsoluteError value on hold-out data: 0.37181854248046875
Epoch: 31</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.15985325286785762
MeanAbsoluteError value on hold-out data: 0.36465293169021606
Epoch: 32
Loss on hold-out set: 0.16219363994896413
MeanAbsoluteError value on hold-out data: 0.3656635582447052
Epoch: 33
Loss on hold-out set: 0.16270208487908044
MeanAbsoluteError value on hold-out data: 0.36274173855781555
Epoch: 34</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.155665252233545
MeanAbsoluteError value on hold-out data: 0.3537687361240387
Epoch: 35
Loss on hold-out set: 0.155173355837663
MeanAbsoluteError value on hold-out data: 0.35567763447761536
Epoch: 36</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.15566179253160953
MeanAbsoluteError value on hold-out data: 0.3538636267185211
Epoch: 37
Loss on hold-out set: 0.14743970066308976
MeanAbsoluteError value on hold-out data: 0.34539541602134705
Epoch: 38
Loss on hold-out set: 0.1451820057630539
MeanAbsoluteError value on hold-out data: 0.34210970997810364
Epoch: 39</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13924171425402165
MeanAbsoluteError value on hold-out data: 0.33370426297187805
Epoch: 40
Loss on hold-out set: 0.14451856171091398
MeanAbsoluteError value on hold-out data: 0.3416542112827301
Epoch: 41</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1302067215492328
MeanAbsoluteError value on hold-out data: 0.32197555899620056
Epoch: 42
Loss on hold-out set: 0.13349131102363268
MeanAbsoluteError value on hold-out data: 0.32359859347343445
Epoch: 43
Loss on hold-out set: 0.1266732486585776
MeanAbsoluteError value on hold-out data: 0.31763583421707153
Epoch: 44</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13253302534421285
MeanAbsoluteError value on hold-out data: 0.3232727348804474
Epoch: 45
Loss on hold-out set: 0.12709495916962624
MeanAbsoluteError value on hold-out data: 0.31583172082901
Epoch: 46</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1275240768243869
MeanAbsoluteError value on hold-out data: 0.31358009576797485
Epoch: 47
Loss on hold-out set: 0.12337811963011822
MeanAbsoluteError value on hold-out data: 0.3090101480484009
Epoch: 48
Loss on hold-out set: 0.11811547147730986
MeanAbsoluteError value on hold-out data: 0.30135583877563477
Epoch: 49</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11331404949227969
MeanAbsoluteError value on hold-out data: 0.29726627469062805
Epoch: 50
Loss on hold-out set: 0.1195528511951367
MeanAbsoluteError value on hold-out data: 0.30134546756744385
Epoch: 51</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.11175416048616171
MeanAbsoluteError value on hold-out data: 0.2950301170349121
Epoch: 52
Loss on hold-out set: 0.11181936360895633
MeanAbsoluteError value on hold-out data: 0.2932000160217285
Epoch: 53</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10890007187922796
MeanAbsoluteError value on hold-out data: 0.28715211153030396
Epoch: 54
Loss on hold-out set: 0.11120939628531536
MeanAbsoluteError value on hold-out data: 0.2886425852775574
Epoch: 55</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10887354719142119
MeanAbsoluteError value on hold-out data: 0.2805563807487488
Epoch: 56
Loss on hold-out set: 0.10344864533593258
MeanAbsoluteError value on hold-out data: 0.2795654535293579
Epoch: 57</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0965714746961991
MeanAbsoluteError value on hold-out data: 0.26713594794273376
Epoch: 58
Loss on hold-out set: 0.1018772548933824
MeanAbsoluteError value on hold-out data: 0.2759571671485901
Epoch: 59
Loss on hold-out set: 0.09961865367988745
MeanAbsoluteError value on hold-out data: 0.2724776566028595
Epoch: 60</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09895426987359922
MeanAbsoluteError value on hold-out data: 0.2701159417629242
Epoch: 61
Loss on hold-out set: 0.09215714697726071
MeanAbsoluteError value on hold-out data: 0.25785335898399353
Epoch: 62</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09574323879554868
MeanAbsoluteError value on hold-out data: 0.2681463360786438
Epoch: 63
Loss on hold-out set: 0.08903963334858417
MeanAbsoluteError value on hold-out data: 0.25491827726364136
Epoch: 64
Loss on hold-out set: 0.08646201601872841
MeanAbsoluteError value on hold-out data: 0.24891649186611176
Epoch: 65</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08604182699074348
MeanAbsoluteError value on hold-out data: 0.24990658462047577
Epoch: 66
Loss on hold-out set: 0.08979712460190058
MeanAbsoluteError value on hold-out data: 0.2558192312717438
Epoch: 67</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08801769281427066
MeanAbsoluteError value on hold-out data: 0.25591784715652466
Epoch: 68
Loss on hold-out set: 0.09033511425058047
MeanAbsoluteError value on hold-out data: 0.25314420461654663
Epoch: 69
Loss on hold-out set: 0.08009708101550737
MeanAbsoluteError value on hold-out data: 0.23584891855716705
Epoch: 70</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08215375461305181
MeanAbsoluteError value on hold-out data: 0.24046602845191956
Epoch: 71</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07885155086716016
MeanAbsoluteError value on hold-out data: 0.23486541211605072
Epoch: 72
Loss on hold-out set: 0.07684332869326074
MeanAbsoluteError value on hold-out data: 0.2300887554883957
Epoch: 73
Loss on hold-out set: 0.076019918260475
MeanAbsoluteError value on hold-out data: 0.22896479070186615
Epoch: 74</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07494671192020178
MeanAbsoluteError value on hold-out data: 0.22935114800930023
Epoch: 75</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07236433725804091
MeanAbsoluteError value on hold-out data: 0.22079508006572723
Epoch: 76
Loss on hold-out set: 0.0737537012497584
MeanAbsoluteError value on hold-out data: 0.22682294249534607
Epoch: 77
Loss on hold-out set: 0.06532424308359623
MeanAbsoluteError value on hold-out data: 0.21409937739372253
Epoch: 78
Loss on hold-out set: 0.06875808468709389
MeanAbsoluteError value on hold-out data: 0.21651944518089294
Epoch: 79</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07181884995351236
MeanAbsoluteError value on hold-out data: 0.22433766722679138
Epoch: 80</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.06852930568158627
MeanAbsoluteError value on hold-out data: 0.21596264839172363
Epoch: 81
Loss on hold-out set: 0.06857993476092815
MeanAbsoluteError value on hold-out data: 0.21373839676380157
Epoch: 82
Loss on hold-out set: 0.06677640572190285
MeanAbsoluteError value on hold-out data: 0.21250949800014496
Epoch: 83
Loss on hold-out set: 0.06549795098913212
MeanAbsoluteError value on hold-out data: 0.21346469223499298
Epoch: 84</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.06327593960178395</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MeanAbsoluteError value on hold-out data: 0.2101280391216278
Epoch: 85
Loss on hold-out set: 0.06412599558631579
MeanAbsoluteError value on hold-out data: 0.20715530216693878
Epoch: 86
Loss on hold-out set: 0.06766407878448566
MeanAbsoluteError value on hold-out data: 0.21332459151744843
Epoch: 87
Loss on hold-out set: 0.06368203685929377
MeanAbsoluteError value on hold-out data: 0.20729418098926544
Epoch: 88
Loss on hold-out set: 0.05666155653074384
MeanAbsoluteError value on hold-out data: 0.19562605023384094
Epoch: 89</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.053633391093462705
MeanAbsoluteError value on hold-out data: 0.18920518457889557
Epoch: 90
Loss on hold-out set: 0.057612722590565685
MeanAbsoluteError value on hold-out data: 0.19791217148303986
Epoch: 91
Loss on hold-out set: 0.05162541115035613
MeanAbsoluteError value on hold-out data: 0.18763355910778046
Epoch: 92
Loss on hold-out set: 0.059441778833667434
MeanAbsoluteError value on hold-out data: 0.20336101949214935
Epoch: 93
Loss on hold-out set: 0.05635029090568423
MeanAbsoluteError value on hold-out data: 0.1915336549282074
Epoch: 94</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.04775271443029245
MeanAbsoluteError value on hold-out data: 0.180833637714386
Epoch: 95
Loss on hold-out set: 0.054517797405521075
MeanAbsoluteError value on hold-out data: 0.18655820190906525
Epoch: 96
Loss on hold-out set: 0.05171272729833921
MeanAbsoluteError value on hold-out data: 0.1877531111240387
Epoch: 97
Loss on hold-out set: 0.053856865186244246
MeanAbsoluteError value on hold-out data: 0.1875869780778885
Epoch: 98
Loss on hold-out set: 0.05104046472037832
MeanAbsoluteError value on hold-out data: 0.18053534626960754
Epoch: 99</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.05182227627684673
MeanAbsoluteError value on hold-out data: 0.19037264585494995
Epoch: 100
Loss on hold-out set: 0.04903062644104163
MeanAbsoluteError value on hold-out data: 0.18178553879261017
Epoch: 101
Loss on hold-out set: 0.048388442808451754
MeanAbsoluteError value on hold-out data: 0.17761030793190002
Epoch: 102
Loss on hold-out set: 0.04856813476420939
MeanAbsoluteError value on hold-out data: 0.17713838815689087
Epoch: 103
Loss on hold-out set: 0.051205978865424794
MeanAbsoluteError value on hold-out data: 0.18671873211860657
Epoch: 104</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.04682874931643406
MeanAbsoluteError value on hold-out data: 0.1755015254020691
Epoch: 105
Loss on hold-out set: 0.04900848327204585
MeanAbsoluteError value on hold-out data: 0.1796758770942688
Epoch: 106
Loss on hold-out set: 0.04870067108732959
MeanAbsoluteError value on hold-out data: 0.17869362235069275
Epoch: 107
Loss on hold-out set: 0.04440015239020189
MeanAbsoluteError value on hold-out data: 0.16839070618152618
Epoch: 108
Loss on hold-out set: 0.04626780560084929
MeanAbsoluteError value on hold-out data: 0.17204980552196503
Epoch: 109</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.046247685191531974
MeanAbsoluteError value on hold-out data: 0.1730698198080063
Epoch: 110
Loss on hold-out set: 0.046187324052055674
MeanAbsoluteError value on hold-out data: 0.17561113834381104
Epoch: 111
Loss on hold-out set: 0.04825203123812874
MeanAbsoluteError value on hold-out data: 0.1737978607416153
Epoch: 112
Loss on hold-out set: 0.04543189679582914
MeanAbsoluteError value on hold-out data: 0.1694442182779312
Epoch: 113
Loss on hold-out set: 0.04560430628558
MeanAbsoluteError value on hold-out data: 0.16889603435993195
Epoch: 114</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.04472786628330747
MeanAbsoluteError value on hold-out data: 0.17384807765483856
Epoch: 115
Loss on hold-out set: 0.04430019474277894
MeanAbsoluteError value on hold-out data: 0.16936467587947845
Epoch: 116
Loss on hold-out set: 0.04092795062189301
MeanAbsoluteError value on hold-out data: 0.16380727291107178
Epoch: 117
Loss on hold-out set: 0.04118863672328492
MeanAbsoluteError value on hold-out data: 0.15853825211524963
Epoch: 118
Loss on hold-out set: 0.040608120902131
MeanAbsoluteError value on hold-out data: 0.16039541363716125
Epoch: 119</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.041709505217149855
MeanAbsoluteError value on hold-out data: 0.16655294597148895
Epoch: 120
Loss on hold-out set: 0.04429606245830655
MeanAbsoluteError value on hold-out data: 0.1700417399406433
Epoch: 121
Loss on hold-out set: 0.04259021884140869
MeanAbsoluteError value on hold-out data: 0.1653982251882553
Epoch: 122
Loss on hold-out set: 0.04301686386888226
MeanAbsoluteError value on hold-out data: 0.1613122820854187
Epoch: 123
Loss on hold-out set: 0.04237945056054741
MeanAbsoluteError value on hold-out data: 0.16540229320526123
Epoch: 124</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.04100692452241977
MeanAbsoluteError value on hold-out data: 0.1613406389951706
Epoch: 125
Loss on hold-out set: 0.03733738190184037
MeanAbsoluteError value on hold-out data: 0.15597224235534668
Epoch: 126
Loss on hold-out set: 0.03741471623303369
MeanAbsoluteError value on hold-out data: 0.1568743884563446
Epoch: 127
Loss on hold-out set: 0.040717475190758703
MeanAbsoluteError value on hold-out data: 0.16316409409046173
Epoch: 128
Loss on hold-out set: 0.040394650309657054
MeanAbsoluteError value on hold-out data: 0.1641305536031723
Returned to Spot: Validation loss: 0.040394650309657054
----------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>spotPython tuning: 0.040394650309657054 [#########-] 89.98% </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
config: {'_L_in': 10, '_L_out': 1, 'l1': 128, 'dropout_prob': 0.16343155599701642, 'lr_mult': 0.001, 'batch_size': 4, 'epochs': 128, 'k_folds': 1, 'patience': 128, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}
Epoch: 1
Loss on hold-out set: 0.18669108813007673
MeanAbsoluteError value on hold-out data: 0.39615553617477417
Epoch: 2
Loss on hold-out set: 0.18589762563506762
MeanAbsoluteError value on hold-out data: 0.3948878049850464
Epoch: 3
Loss on hold-out set: 0.18437372033794722
MeanAbsoluteError value on hold-out data: 0.3925729990005493
Epoch: 4
Loss on hold-out set: 0.17808965727686882
MeanAbsoluteError value on hold-out data: 0.3859124779701233
Epoch: 5</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.17559666881958644
MeanAbsoluteError value on hold-out data: 0.3819428086280823
Epoch: 6
Loss on hold-out set: 0.17389261906345685
MeanAbsoluteError value on hold-out data: 0.37977930903434753
Epoch: 7
Loss on hold-out set: 0.17078580766916274
MeanAbsoluteError value on hold-out data: 0.37617242336273193
Epoch: 8
Loss on hold-out set: 0.16927964275081953
MeanAbsoluteError value on hold-out data: 0.3741391897201538
Epoch: 9
Loss on hold-out set: 0.1659952910244465
MeanAbsoluteError value on hold-out data: 0.37045586109161377
Epoch: 10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1603252813220024
MeanAbsoluteError value on hold-out data: 0.3639180660247803
Epoch: 11
Loss on hold-out set: 0.16000037245452403
MeanAbsoluteError value on hold-out data: 0.3618090748786926
Epoch: 12
Loss on hold-out set: 0.1588505470752716
MeanAbsoluteError value on hold-out data: 0.36204537749290466
Epoch: 13
Loss on hold-out set: 0.15498818223675093
MeanAbsoluteError value on hold-out data: 0.3559843599796295
Epoch: 14
Loss on hold-out set: 0.15333570808172226
MeanAbsoluteError value on hold-out data: 0.3535970151424408
Epoch: 15</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.15189454729358356
MeanAbsoluteError value on hold-out data: 0.3511638939380646
Epoch: 16
Loss on hold-out set: 0.14913595922291278
MeanAbsoluteError value on hold-out data: 0.3483405113220215
Epoch: 17
Loss on hold-out set: 0.14464758279422918
MeanAbsoluteError value on hold-out data: 0.3423997461795807
Epoch: 18
Loss on hold-out set: 0.1430986241499583
MeanAbsoluteError value on hold-out data: 0.3402748107910156
Epoch: 19
Loss on hold-out set: 0.13958272953828177
MeanAbsoluteError value on hold-out data: 0.3345904052257538
Epoch: 20</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.13744721656044323
MeanAbsoluteError value on hold-out data: 0.33258557319641113
Epoch: 21
Loss on hold-out set: 0.13465398631989955
MeanAbsoluteError value on hold-out data: 0.32839956879615784
Epoch: 22
Loss on hold-out set: 0.1323085835079352
MeanAbsoluteError value on hold-out data: 0.3252319395542145
Epoch: 23
Loss on hold-out set: 0.12963979914784432
MeanAbsoluteError value on hold-out data: 0.321574330329895
Epoch: 24
Loss on hold-out set: 0.12732306788365047
MeanAbsoluteError value on hold-out data: 0.31717967987060547
Epoch: 25</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1255515613158544
MeanAbsoluteError value on hold-out data: 0.3149213194847107
Epoch: 26
Loss on hold-out set: 0.1224898711591959
MeanAbsoluteError value on hold-out data: 0.31111547350883484
Epoch: 27
Loss on hold-out set: 0.11960337422788143
MeanAbsoluteError value on hold-out data: 0.3057972192764282
Epoch: 28
Loss on hold-out set: 0.11782273481289546
MeanAbsoluteError value on hold-out data: 0.3041980564594269
Epoch: 29
Loss on hold-out set: 0.11467898751298587
MeanAbsoluteError value on hold-out data: 0.30063217878341675
Epoch: 30</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.1140014531215032
MeanAbsoluteError value on hold-out data: 0.2983565926551819
Epoch: 31
Loss on hold-out set: 0.11038443607588609
MeanAbsoluteError value on hold-out data: 0.2933163642883301
Epoch: 32
Loss on hold-out set: 0.10893003102391959
MeanAbsoluteError value on hold-out data: 0.29096558690071106
Epoch: 33
Loss on hold-out set: 0.10769404145578543
MeanAbsoluteError value on hold-out data: 0.28817740082740784
Epoch: 34
Loss on hold-out set: 0.10563814048965772
MeanAbsoluteError value on hold-out data: 0.28539445996284485
Epoch: 35</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.10142973692466815
MeanAbsoluteError value on hold-out data: 0.27889198064804077
Epoch: 36
Loss on hold-out set: 0.10070148691534996
MeanAbsoluteError value on hold-out data: 0.2775111794471741
Epoch: 37
Loss on hold-out set: 0.09955558499942223
MeanAbsoluteError value on hold-out data: 0.2747519314289093
Epoch: 38
Loss on hold-out set: 0.09539372141162554
MeanAbsoluteError value on hold-out data: 0.26722097396850586
Epoch: 39
Loss on hold-out set: 0.09245328726867835
MeanAbsoluteError value on hold-out data: 0.26373299956321716
Epoch: 40</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.09267232803006967
MeanAbsoluteError value on hold-out data: 0.26429328322410583
Epoch: 41
Loss on hold-out set: 0.08880806729818384
MeanAbsoluteError value on hold-out data: 0.25795042514801025
Epoch: 42
Loss on hold-out set: 0.0894575858488679
MeanAbsoluteError value on hold-out data: 0.25775858759880066
Epoch: 43
Loss on hold-out set: 0.08437781562407812
MeanAbsoluteError value on hold-out data: 0.25135156512260437
Epoch: 44
Loss on hold-out set: 0.08550942907730738
MeanAbsoluteError value on hold-out data: 0.25164762139320374
Epoch: 45</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.08331248474617799
MeanAbsoluteError value on hold-out data: 0.2479211986064911
Epoch: 46
Loss on hold-out set: 0.08171022387842337
MeanAbsoluteError value on hold-out data: 0.24526359140872955
Epoch: 47
Loss on hold-out set: 0.07813216861337423
MeanAbsoluteError value on hold-out data: 0.23789110779762268
Epoch: 48
Loss on hold-out set: 0.07682195692012707
MeanAbsoluteError value on hold-out data: 0.23570683598518372
Epoch: 49
Loss on hold-out set: 0.07472024315968157
MeanAbsoluteError value on hold-out data: 0.23331013321876526
Epoch: 50</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.07216364827627937
MeanAbsoluteError value on hold-out data: 0.22839218378067017
Epoch: 51
Loss on hold-out set: 0.0710804018502434
MeanAbsoluteError value on hold-out data: 0.2263496071100235
Epoch: 52
Loss on hold-out set: 0.07031039477636417
MeanAbsoluteError value on hold-out data: 0.22316040098667145
Epoch: 53
Loss on hold-out set: 0.06895541642792523
MeanAbsoluteError value on hold-out data: 0.22190739214420319
Epoch: 54
Loss on hold-out set: 0.06638235338653127
MeanAbsoluteError value on hold-out data: 0.2161777764558792
Epoch: 55</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.06430478832994899
MeanAbsoluteError value on hold-out data: 0.21353866159915924
Epoch: 56
Loss on hold-out set: 0.06313725400095184
MeanAbsoluteError value on hold-out data: 0.211651012301445
Epoch: 57
Loss on hold-out set: 0.06267830402279893
MeanAbsoluteError value on hold-out data: 0.21037624776363373
Epoch: 58
Loss on hold-out set: 0.061010961290448903
MeanAbsoluteError value on hold-out data: 0.20554755628108978
Epoch: 59
Loss on hold-out set: 0.05887546905626853
MeanAbsoluteError value on hold-out data: 0.20431308448314667
Epoch: 60</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.056461512794097266
MeanAbsoluteError value on hold-out data: 0.1994888186454773
Epoch: 61
Loss on hold-out set: 0.05565481768921018
MeanAbsoluteError value on hold-out data: 0.1974475383758545
Epoch: 62
Loss on hold-out set: 0.05507119580482443
MeanAbsoluteError value on hold-out data: 0.19607779383659363
Epoch: 63
Loss on hold-out set: 0.05476851986721158
MeanAbsoluteError value on hold-out data: 0.196028470993042
Epoch: 64
Loss on hold-out set: 0.052519595362246034
MeanAbsoluteError value on hold-out data: 0.1901305466890335
Epoch: 65</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.053234470722575984
MeanAbsoluteError value on hold-out data: 0.18963147699832916
Epoch: 66
Loss on hold-out set: 0.049935822977374
MeanAbsoluteError value on hold-out data: 0.1844327598810196
Epoch: 67
Loss on hold-out set: 0.047801838566859566
MeanAbsoluteError value on hold-out data: 0.17805862426757812
Epoch: 68
Loss on hold-out set: 0.04637363961587349
MeanAbsoluteError value on hold-out data: 0.17807161808013916
Epoch: 69
Loss on hold-out set: 0.04744552372644345
MeanAbsoluteError value on hold-out data: 0.17996296286582947
Epoch: 70</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.04480662712206443
MeanAbsoluteError value on hold-out data: 0.17463774979114532
Epoch: 71
Loss on hold-out set: 0.04374726469938954
MeanAbsoluteError value on hold-out data: 0.17090027034282684
Epoch: 72
Loss on hold-out set: 0.04193584790453315
MeanAbsoluteError value on hold-out data: 0.1674395054578781
Epoch: 73
Loss on hold-out set: 0.043119663391262296
MeanAbsoluteError value on hold-out data: 0.16949845850467682
Epoch: 74
Loss on hold-out set: 0.040818812350432075
MeanAbsoluteError value on hold-out data: 0.16504129767417908
Epoch: 75</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.03964812502885858
MeanAbsoluteError value on hold-out data: 0.16349762678146362
Epoch: 76
Loss on hold-out set: 0.04019248845676581
MeanAbsoluteError value on hold-out data: 0.16517476737499237
Epoch: 77
Loss on hold-out set: 0.03967009207233787
MeanAbsoluteError value on hold-out data: 0.1618732511997223
Epoch: 78
Loss on hold-out set: 0.03809533555681507
MeanAbsoluteError value on hold-out data: 0.15879255533218384
Epoch: 79
Loss on hold-out set: 0.038045286219567064
MeanAbsoluteError value on hold-out data: 0.15656998753547668
Epoch: 80</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.036074635423719886
MeanAbsoluteError value on hold-out data: 0.15672698616981506
Epoch: 81
Loss on hold-out set: 0.034253137689083814
MeanAbsoluteError value on hold-out data: 0.1502525359392166
Epoch: 82
Loss on hold-out set: 0.03607516969554126
MeanAbsoluteError value on hold-out data: 0.15392452478408813
Epoch: 83
Loss on hold-out set: 0.036486981430401404
MeanAbsoluteError value on hold-out data: 0.153493732213974
Epoch: 84
Loss on hold-out set: 0.033216136277963715
MeanAbsoluteError value on hold-out data: 0.1470402181148529
Epoch: 85</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.03392506419060131
MeanAbsoluteError value on hold-out data: 0.1476212441921234
Epoch: 86
Loss on hold-out set: 0.032279409334684414
MeanAbsoluteError value on hold-out data: 0.14573466777801514
Epoch: 87
Loss on hold-out set: 0.03340025421697646
MeanAbsoluteError value on hold-out data: 0.14467650651931763
Epoch: 88
Loss on hold-out set: 0.03133754008915275
MeanAbsoluteError value on hold-out data: 0.14475755393505096
Epoch: 89
Loss on hold-out set: 0.033290586477766435
MeanAbsoluteError value on hold-out data: 0.14758041501045227
Epoch: 90</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.03045099730913838
MeanAbsoluteError value on hold-out data: 0.14075855910778046
Epoch: 91
Loss on hold-out set: 0.030169096862276396
MeanAbsoluteError value on hold-out data: 0.1370185911655426
Epoch: 92
Loss on hold-out set: 0.031505483736594515
MeanAbsoluteError value on hold-out data: 0.14226728677749634
Epoch: 93
Loss on hold-out set: 0.029662128320584694
MeanAbsoluteError value on hold-out data: 0.136855810880661
Epoch: 94
Loss on hold-out set: 0.03031885971936087
MeanAbsoluteError value on hold-out data: 0.1390964239835739
Epoch: 95</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.02928745190612972
MeanAbsoluteError value on hold-out data: 0.13796421885490417
Epoch: 96
Loss on hold-out set: 0.028277527627845606
MeanAbsoluteError value on hold-out data: 0.13474783301353455
Epoch: 97
Loss on hold-out set: 0.02893527052132413
MeanAbsoluteError value on hold-out data: 0.13518694043159485
Epoch: 98
Loss on hold-out set: 0.028185868735114732
MeanAbsoluteError value on hold-out data: 0.13539659976959229
Epoch: 99
Loss on hold-out set: 0.028244599224999545
MeanAbsoluteError value on hold-out data: 0.13427846133708954
Epoch: 100</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.02859475358389318
MeanAbsoluteError value on hold-out data: 0.13411881029605865
Epoch: 101
Loss on hold-out set: 0.02897264141589403
MeanAbsoluteError value on hold-out data: 0.13553369045257568
Epoch: 102
Loss on hold-out set: 0.027507960287233194
MeanAbsoluteError value on hold-out data: 0.1324528306722641
Epoch: 103
Loss on hold-out set: 0.028679890263204773
MeanAbsoluteError value on hold-out data: 0.13437701761722565
Epoch: 104</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.027638483146826428
MeanAbsoluteError value on hold-out data: 0.13206452131271362
Epoch: 105
Loss on hold-out set: 0.026344778987889488
MeanAbsoluteError value on hold-out data: 0.1281900852918625
Epoch: 106
Loss on hold-out set: 0.028922165991583218
MeanAbsoluteError value on hold-out data: 0.133317232131958
Epoch: 107
Loss on hold-out set: 0.028511003679595887
MeanAbsoluteError value on hold-out data: 0.13387852907180786
Epoch: 108
Loss on hold-out set: 0.026580023627417782
MeanAbsoluteError value on hold-out data: 0.1307424157857895
Epoch: 109</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.026124688630613187
MeanAbsoluteError value on hold-out data: 0.1276836097240448
Epoch: 110
Loss on hold-out set: 0.028029047618620097
MeanAbsoluteError value on hold-out data: 0.1341014802455902
Epoch: 111
Loss on hold-out set: 0.026698427178586524
MeanAbsoluteError value on hold-out data: 0.12857307493686676
Epoch: 112
Loss on hold-out set: 0.02786889144529899
MeanAbsoluteError value on hold-out data: 0.13390354812145233
Epoch: 113
Loss on hold-out set: 0.027766782914598782
MeanAbsoluteError value on hold-out data: 0.133920356631279
Epoch: 114</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.028331349422223866
MeanAbsoluteError value on hold-out data: 0.13078436255455017
Epoch: 115
Loss on hold-out set: 0.026398231154307724
MeanAbsoluteError value on hold-out data: 0.1285991221666336
Epoch: 116
Loss on hold-out set: 0.02646889109785358
MeanAbsoluteError value on hold-out data: 0.1303335726261139
Epoch: 117
Loss on hold-out set: 0.02692136257266005
MeanAbsoluteError value on hold-out data: 0.1295207291841507
Epoch: 118
Loss on hold-out set: 0.02618577053770423
MeanAbsoluteError value on hold-out data: 0.12808531522750854</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 119
Loss on hold-out set: 0.026384370640541117
MeanAbsoluteError value on hold-out data: 0.12766137719154358
Epoch: 120
Loss on hold-out set: 0.027246563209531206
MeanAbsoluteError value on hold-out data: 0.12836961448192596
Epoch: 121
Loss on hold-out set: 0.026148140837127964
MeanAbsoluteError value on hold-out data: 0.1266038566827774
Epoch: 122
Loss on hold-out set: 0.02668789173476398
MeanAbsoluteError value on hold-out data: 0.12806914746761322
Epoch: 123</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.0254686074781542
MeanAbsoluteError value on hold-out data: 0.12439107149839401
Epoch: 124
Loss on hold-out set: 0.026681452486664058
MeanAbsoluteError value on hold-out data: 0.12987904250621796
Epoch: 125
Loss on hold-out set: 0.02754835154550771
MeanAbsoluteError value on hold-out data: 0.13120846450328827
Epoch: 126
Loss on hold-out set: 0.025480538588017226
MeanAbsoluteError value on hold-out data: 0.12695585191249847
Epoch: 127
Loss on hold-out set: 0.027587213404476642
MeanAbsoluteError value on hold-out data: 0.13168422877788544
Epoch: 128</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss on hold-out set: 0.024931732943902413
MeanAbsoluteError value on hold-out data: 0.12593317031860352
Returned to Spot: Validation loss: 0.024931732943902413
----------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>spotPython tuning: 0.024931732943902413 [##########] 100.00% Done...
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>&lt;spotPython.spot.spot.Spot at 0x2bb29fd00&gt;</code></pre>
</div>
</div>
<p>During the run, the following output is shown:</p>
<pre class="{raw}"><code>config: {'_L_in': 10, '_L_out': 1, 'l1': 64, 'dropout_prob': 0.4475780541539,
    'lr_mult': 0.001, 'batch_size': 16, 'epochs': 512, 'k_folds': 1,
    'patience': 32,'optimizer': 'Adagrad', 'sgd_momentum': 0.9}
Epoch: 1
...
Epoch: 7002
Loss on hold-out set: 1.6959798782529844e-05
MeanAbsoluteError value on hold-out data: 0.0018855303060263395
Epoch: 7003
Loss on hold-out set: 1.6984027051769603e-05
MeanAbsoluteError value on hold-out data: 0.001883985591121018
Early stopping at epoch 7002
Returned to Spot: Validation loss: 1.6984027051769603e-05</code></pre>
</section>
<section id="sec-tensorboard-24" class="level2" data-number="14.10">
<h2 data-number="14.10" class="anchored" data-anchor-id="sec-tensorboard-24"><span class="header-section-number">14.10</span> Tensorboard</h2>
<p>The textual output shown in the console (or code cell) can be visualized with Tensorboard.</p>
<section id="tensorboard-start-tensorboard" class="level3" data-number="14.10.1">
<h3 data-number="14.10.1" class="anchored" data-anchor-id="tensorboard-start-tensorboard"><span class="header-section-number">14.10.1</span> Tensorboard: Start Tensorboard</h3>
<p>Start TensorBoard through the command line to visualize data you logged. Specify the root log directory as used in <code>fun_control = fun_control_init(task="regression", tensorboard_path="runs/24_spot_torch_regression")</code> as the <code>tensorboard_path</code>. The argument logdir points to directory where TensorBoard will look to find event files that it can display. TensorBoard will recursively walk the directory structure rooted at logdir, looking for .<em>tfevents.</em> files.</p>
<p>tensorboard –logdir=runs</p>
<p>Go to the URL it provides OR to http://localhost:6006/.</p>
<p>The following figures show some screenshots of Tensorboard.</p>
<div id="fig-tensorboard_0" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figures/tensorboard_0.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;14.1: Tensorboard</figcaption>
</figure>
</div>
<div id="fig-tensorboard_hdparams" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figures/tensorboard_hdparams.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;14.2: Tensorboard</figcaption>
</figure>
</div>
<div id="fig-tensorboard_parallel" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figures/tensorboard_parallel.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;14.3: Tensorboard</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-results-tuning-24" class="level2" data-number="14.11">
<h2 data-number="14.11" class="anchored" data-anchor-id="sec-results-tuning-24"><span class="header-section-number">14.11</span> Results</h2>
<p>After the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The following code generates the progress plot from <a href="#fig-progress">Figure&nbsp;<span>14.4</span></a>.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb463"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb463-1"><a href="#cb463-1" aria-hidden="true" tabindex="-1"></a>spot_tuner.plot_progress(log_y<span class="op">=</span><span class="va">False</span>, filename<span class="op">=</span><span class="st">"./figures/"</span> <span class="op">+</span> experiment_name<span class="op">+</span><span class="st">"_progress.png"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="24_spot_torch_regression_files/figure-html/cell-25-output-1.png" width="719" height="243"></p>
</div>
</div>
<div id="fig-progress" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figures/figures24-torch_bartz09_10min_20init_2023-06-06_22-50-23_progress.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;14.4: Progress plot. <code>Black</code> dots denote results from the initial design. <code>Red</code> dots illustrate the improvement found by the surrogate model based optimization (surrogate model based optimization).</figcaption>
</figure>
</div>
<p><a href="#fig-progress">Figure&nbsp;<span>14.4</span></a> shows a typical behaviour that can be observed in many hyperparameter studies <span class="citation" data-cites="bart21i">(<a href="references.html#ref-bart21i" role="doc-biblioref">Bartz et al. 2022</a>)</span>: the largest improvement is obtained during the evaluation of the initial design. The surrogate model based optimization-optimization with the surrogate refines the results. <a href="#fig-progress">Figure&nbsp;<span>14.4</span></a> also illustrates one major difference between <code>ray[tune]</code> as used in <span class="citation" data-cites="pyto23a">PyTorch (<a href="references.html#ref-pyto23a" role="doc-biblioref">2023</a>)</span> and <code>spotPython</code>: the <code>ray[tune]</code> uses a random search and will generate results similar to the <em>black</em> dots, whereas <code>spotPython</code> uses a surrogate model based optimization and presents results represented by <em>red</em> dots in <a href="#fig-progress">Figure&nbsp;<span>14.4</span></a>. The surrogate model based optimization is considered to be more efficient than a random search, because the surrogate model guides the search towards promising regions in the hyperparameter space.</p>
<p>In addition to the improved (“optimized”) hyperparameter values, <code>spotPython</code> allows a statistical analysis, e.g., a sensitivity analysis, of the results. We can print the results of the hyperparameter tuning, see <a href="#tbl-results">Table&nbsp;<span>14.4</span></a>.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb464"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb464-1"><a href="#cb464-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(gen_design_table(fun_control<span class="op">=</span>fun_control, spot<span class="op">=</span>spot_tuner))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>| name         | type   | default   |   lower |   upper |               tuned | transform             |   importance | stars   |
|--------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|
| _L_in        | int    | 10        |    10.0 |    10.0 |                10.0 | None                  |         0.00 |         |
| _L_out       | int    | 1         |     1.0 |     1.0 |                 1.0 | None                  |         0.00 |         |
| l1           | int    | 3         |     3.0 |     8.0 |                 7.0 | transform_power_2_int |         0.02 |         |
| dropout_prob | float  | 0.01      |     0.0 |     0.9 | 0.16343155599701642 | None                  |         0.12 | .       |
| lr_mult      | float  | 1.0       |   0.001 |   0.001 |               0.001 | None                  |         0.00 |         |
| batch_size   | int    | 4         |     1.0 |     4.0 |                 2.0 | transform_power_2_int |       100.00 | ***     |
| epochs       | int    | 4         |     2.0 |    16.0 |                 7.0 | transform_power_2_int |         0.00 |         |
| k_folds      | int    | 1         |     1.0 |     1.0 |                 1.0 | None                  |         0.00 |         |
| patience     | int    | 2         |     3.0 |     7.0 |                 7.0 | transform_power_2_int |         0.01 |         |
| optimizer    | factor | SGD       |     0.0 |     6.0 |                 3.0 | None                  |         0.00 |         |
| sgd_momentum | float  | 0.0       |     0.9 |     0.9 |                 0.9 | None                  |         0.00 |         |</code></pre>
</div>
</div>
<div id="tbl-results" class="anchored">
<table class="table">
<caption>Table&nbsp;14.4: Results of the hyperparameter tuning. The table shows the hyperparameters, their types, default values, lower and upper bounds, and the transformation function. The column “tuned” shows the tuned values. The column “importance” shows the importance of the hyperparameters. The column “stars” shows the importance of the hyperparameters in stars. The importance is computed by the SPOT software.</caption>
<colgroup>
<col style="width: 11%">
<col style="width: 7%">
<col style="width: 12%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 18%">
<col style="width: 14%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>name</th>
<th>type</th>
<th>default</th>
<th style="text-align: right;">lower</th>
<th style="text-align: right;">upper</th>
<th style="text-align: right;">tuned</th>
<th>transform</th>
<th style="text-align: right;">importance</th>
<th>stars</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>_L_in</td>
<td>int</td>
<td>10</td>
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">10.0</td>
<td>None</td>
<td style="text-align: right;">0.00</td>
<td></td>
</tr>
<tr class="even">
<td>_L_out</td>
<td>int</td>
<td>1</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">1.0</td>
<td>None</td>
<td style="text-align: right;">0.00</td>
<td></td>
</tr>
<tr class="odd">
<td>l1</td>
<td>int</td>
<td>3</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">6.0</td>
<td>power_2_int</td>
<td style="text-align: right;">1.42</td>
<td>*</td>
</tr>
<tr class="even">
<td>drop_p</td>
<td>float</td>
<td>0.01</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">0.0</td>
<td>None</td>
<td style="text-align: right;">0.00</td>
<td></td>
</tr>
<tr class="odd">
<td>lr_mult</td>
<td>float</td>
<td>1.0</td>
<td style="text-align: right;">0.001</td>
<td style="text-align: right;">0.001</td>
<td style="text-align: right;">0.001</td>
<td>None</td>
<td style="text-align: right;">0.00</td>
<td></td>
</tr>
<tr class="even">
<td>batch_s</td>
<td>int</td>
<td>4</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">1.0</td>
<td>power_2_int</td>
<td style="text-align: right;">0.01</td>
<td></td>
</tr>
<tr class="odd">
<td>epochs</td>
<td>int</td>
<td>4</td>
<td style="text-align: right;">2.0</td>
<td style="text-align: right;">16.0</td>
<td style="text-align: right;">13.0</td>
<td>power_2_int</td>
<td style="text-align: right;">100.00</td>
<td>***</td>
</tr>
<tr class="even">
<td>k_folds</td>
<td>int</td>
<td>1</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">1.0</td>
<td>None</td>
<td style="text-align: right;">0.00</td>
<td></td>
</tr>
<tr class="odd">
<td>patience</td>
<td>int</td>
<td>2</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">7.0</td>
<td style="text-align: right;">4.0</td>
<td>power_2_int</td>
<td style="text-align: right;">0.00</td>
<td></td>
</tr>
<tr class="even">
<td>optim</td>
<td>factor</td>
<td>SGD</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">6.0</td>
<td style="text-align: right;">4.0</td>
<td>None</td>
<td style="text-align: right;">0.00</td>
<td></td>
</tr>
<tr class="odd">
<td>sgd_mom</td>
<td>float</td>
<td>0.0</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">0.9</td>
<td>None</td>
<td style="text-align: right;">0.00</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>To visualize the most important hyperparameters, <code>spotPython</code> provides the function <code>plot_importance</code>. The following code generates the importance plot from <a href="#fig-importance">Figure&nbsp;<span>14.5</span></a>.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb466"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb466-1"><a href="#cb466-1" aria-hidden="true" tabindex="-1"></a>spot_tuner.plot_importance(threshold<span class="op">=</span><span class="fl">0.025</span>, filename<span class="op">=</span><span class="st">"./figures/"</span> <span class="op">+</span> experiment_name<span class="op">+</span><span class="st">"_importance.png"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="24_spot_torch_regression_files/figure-html/cell-27-output-1.png" width="575" height="411"></p>
</div>
</div>
<div id="fig-importance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figures/figures24-torch_bartz09_10min_20init_2023-06-06_22-50-23_importance.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;14.5: Variable importance</figcaption>
</figure>
</div>
</section>
<section id="sec-get-spot-results-24" class="level2" data-number="14.12">
<h2 data-number="14.12" class="anchored" data-anchor-id="sec-get-spot-results-24"><span class="header-section-number">14.12</span> Get the Tuned Architecture</h2>
<p>The architecture of the <code>spotPython</code> model can be obtained by the following code:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb467"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb467-1"><a href="#cb467-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.hyperparameters.values <span class="im">import</span> get_one_core_model_from_X</span>
<span id="cb467-2"><a href="#cb467-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> spot_tuner.to_all_dim(spot_tuner.min_X.reshape(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb467-3"><a href="#cb467-3" aria-hidden="true" tabindex="-1"></a>model_spot <span class="op">=</span> get_one_core_model_from_X(X, fun_control)</span>
<span id="cb467-4"><a href="#cb467-4" aria-hidden="true" tabindex="-1"></a>model_spot</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>Net_lin_reg(
  (fc1): Linear(in_features=10, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=1, bias=True)
  (relu): ReLU()
  (softmax): Softmax(dim=1)
  (dropout1): Dropout(p=0.16343155599701642, inplace=False)
  (dropout2): Dropout(p=0.08171577799850821, inplace=False)
)</code></pre>
</div>
</div>
<p>First, the numerical representation of the hyperparameters are obtained, i.e., the numpy array <code>X</code> is generated. This array is then used to generate the model <code>model_spot</code> by the function <code>get_one_core_model_from_X</code>. The model <code>model_spot</code> has the following architecture:</p>
<pre class="{raw}"><code>Net_lin_reg(
  (fc1): Linear(in_features=10, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=32, bias=True)
  (fc3): Linear(in_features=32, out_features=1, bias=True)
  (relu): ReLU()
  (softmax): Softmax(dim=1)
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
)</code></pre>
</section>
<section id="evaluation-of-the-tuned-architecture" class="level2" data-number="14.13">
<h2 data-number="14.13" class="anchored" data-anchor-id="evaluation-of-the-tuned-architecture"><span class="header-section-number">14.13</span> Evaluation of the Tuned Architecture</h2>
<p>The method <code>train_tuned</code> takes a model architecture without trained weights and trains this model with the train data. The train data is split into train and validation data. The validation data is used for early stopping. The trained model weights are saved as a dictionary.</p>
<p>The following code trains the model <code>model_spot</code>. If <code>path</code> is set to a filename, e.g., <code>path = "model_spot_trained.pt"</code>, the weights of the trained model will be saved to this file.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb470"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb470-1"><a href="#cb470-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.torch.traintest <span class="im">import</span> (</span>
<span id="cb470-2"><a href="#cb470-2" aria-hidden="true" tabindex="-1"></a>    train_tuned,</span>
<span id="cb470-3"><a href="#cb470-3" aria-hidden="true" tabindex="-1"></a>    test_tuned,</span>
<span id="cb470-4"><a href="#cb470-4" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb470-5"><a href="#cb470-5" aria-hidden="true" tabindex="-1"></a>train_tuned(net<span class="op">=</span>model_spot, train_dataset<span class="op">=</span>train,</span>
<span id="cb470-6"><a href="#cb470-6" aria-hidden="true" tabindex="-1"></a>        loss_function<span class="op">=</span>fun_control[<span class="st">"loss_function"</span>],</span>
<span id="cb470-7"><a href="#cb470-7" aria-hidden="true" tabindex="-1"></a>        metric<span class="op">=</span>fun_control[<span class="st">"metric_torch"</span>],</span>
<span id="cb470-8"><a href="#cb470-8" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb470-9"><a href="#cb470-9" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> DEVICE,</span>
<span id="cb470-10"><a href="#cb470-10" aria-hidden="true" tabindex="-1"></a>        path<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb470-11"><a href="#cb470-11" aria-hidden="true" tabindex="-1"></a>        task<span class="op">=</span>fun_control[<span class="st">"task"</span>],)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 1</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Error in Net_Core. Call to evaluate_hold_out() failed. err=RuntimeError('Encountered different devices in metric calculation (see stacktrace for details). This could be due to the metric class not being on the same device as input. Instead of `metric=MeanAbsoluteError(...)` try to do `metric=MeanAbsoluteError(...).to(device)` where device corresponds to the device of the input.'), type(err)=&lt;class 'RuntimeError'&gt;
Returned to Spot: Validation loss: nan
----------------------------------------------</code></pre>
</div>
</div>
<pre class="{raw}"><code>Epoch: 1
Loss on hold-out set: 0.17853929138431945
MeanAbsoluteError value on hold-out data: 0.3907899856567383
Epoch: 2
Loss on hold-out set: 0.17439044278115035
MeanAbsoluteError value on hold-out data: 0.38570401072502136</code></pre>
<p>If <code>path</code> is set to a filename, e.g., <code>path = "model_spot_trained.pt"</code>, the weights of the trained model will be loaded from this file.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb474"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb474-1"><a href="#cb474-1" aria-hidden="true" tabindex="-1"></a>test_tuned(net<span class="op">=</span>model_spot, test_dataset<span class="op">=</span>test,</span>
<span id="cb474-2"><a href="#cb474-2" aria-hidden="true" tabindex="-1"></a>            shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb474-3"><a href="#cb474-3" aria-hidden="true" tabindex="-1"></a>            loss_function<span class="op">=</span>fun_control[<span class="st">"loss_function"</span>],</span>
<span id="cb474-4"><a href="#cb474-4" aria-hidden="true" tabindex="-1"></a>            metric<span class="op">=</span>fun_control[<span class="st">"metric_torch"</span>],</span>
<span id="cb474-5"><a href="#cb474-5" aria-hidden="true" tabindex="-1"></a>            device <span class="op">=</span> DEVICE,</span>
<span id="cb474-6"><a href="#cb474-6" aria-hidden="true" tabindex="-1"></a>            task<span class="op">=</span>fun_control[<span class="st">"task"</span>],)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Error in Net_Core. Call to test_tuned() failed. err=RuntimeError('Encountered different devices in metric calculation (see stacktrace for details). This could be due to the metric class not being on the same device as input. Instead of `metric=MeanAbsoluteError(...)` try to do `metric=MeanAbsoluteError(...).to(device)` where device corresponds to the device of the input.'), type(err)=&lt;class 'RuntimeError'&gt;
Final evaluation: Validation loss: nan
Final evaluation: Validation metric: nan
----------------------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>(nan, nan, nan)</code></pre>
</div>
</div>
<pre class="{raw}"><code>Loss on hold-out set: 1.85966069472272e-05
MeanAbsoluteError value on hold-out data: 0.0021022311411798
Final evaluation: Validation loss: 1.85966069472272e-05
Final evaluation: Validation metric: 0.0021022311411798
----------------------------------------------
(1.85966069472272e-05, nan, tensor(0.0021))</code></pre>
</section>
<section id="cross-validated-evaluations" class="level2" data-number="14.14">
<h2 data-number="14.14" class="anchored" data-anchor-id="cross-validated-evaluations"><span class="header-section-number">14.14</span> Cross-validated Evaluations</h2>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb478"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb478-1"><a href="#cb478-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spotPython.torch.traintest <span class="im">import</span> evaluate_cv</span>
<span id="cb478-2"><a href="#cb478-2" aria-hidden="true" tabindex="-1"></a><span class="co"># modify k-kolds:</span></span>
<span id="cb478-3"><a href="#cb478-3" aria-hidden="true" tabindex="-1"></a><span class="bu">setattr</span>(model_spot, <span class="st">"k_folds"</span>,  <span class="dv">10</span>)</span>
<span id="cb478-4"><a href="#cb478-4" aria-hidden="true" tabindex="-1"></a>evaluate_cv(net<span class="op">=</span>model_spot,</span>
<span id="cb478-5"><a href="#cb478-5" aria-hidden="true" tabindex="-1"></a>            dataset<span class="op">=</span>fun_control[<span class="st">"data"</span>],</span>
<span id="cb478-6"><a href="#cb478-6" aria-hidden="true" tabindex="-1"></a>            loss_function<span class="op">=</span>fun_control[<span class="st">"loss_function"</span>],</span>
<span id="cb478-7"><a href="#cb478-7" aria-hidden="true" tabindex="-1"></a>            metric<span class="op">=</span>fun_control[<span class="st">"metric_torch"</span>],</span>
<span id="cb478-8"><a href="#cb478-8" aria-hidden="true" tabindex="-1"></a>            task<span class="op">=</span>fun_control[<span class="st">"task"</span>],</span>
<span id="cb478-9"><a href="#cb478-9" aria-hidden="true" tabindex="-1"></a>            writer<span class="op">=</span>fun_control[<span class="st">"writer"</span>],</span>
<span id="cb478-10"><a href="#cb478-10" aria-hidden="true" tabindex="-1"></a>            writerId<span class="op">=</span><span class="st">"model_spot_cv"</span>, device<span class="op">=</span>DEVICE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fold: 1
Epoch: 1</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Error in Net_Core. Call to evaluate_cv() failed. err=RuntimeError('Encountered different devices in metric calculation (see stacktrace for details). This could be due to the metric class not being on the same device as input. Instead of `metric=MeanAbsoluteError(...)` try to do `metric=MeanAbsoluteError(...).to(device)` where device corresponds to the device of the input.'), type(err)=&lt;class 'RuntimeError'&gt;</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>(nan, nan, nan)</code></pre>
</div>
</div>
<pre class="{raw}"><code>Fold: 1
Epoch: 1
Loss on hold-out set: 0.36993918985128404
MeanAbsoluteError value on hold-out data: 0.5827060341835022
Epoch: 2
Loss on hold-out set: 0.3583159705996513

(0.0027241395250238156, nan, tensor(0.0147))</code></pre>
<p><a href="#tbl-comparison">Table&nbsp;<span>14.5</span></a> shows the loss and meric value (MAE) of the model with the tuned hyperparameters from SPOT.</p>
<div id="tbl-comparison" class="anchored">
<table class="table">
<caption>Table&nbsp;14.5: Comparison of the loss and metric values.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Loss</th>
<th style="text-align: right;">Metric (MAE)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>Validation</code></td>
<td style="text-align: right;">1.8597e-05</td>
<td style="text-align: right;">0.0021</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>10-fold CV</code></td>
<td style="text-align: right;">0.00272</td>
<td style="text-align: right;">0.0147</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="detailed-hyperparameter-plots" class="level2" data-number="14.15">
<h2 data-number="14.15" class="anchored" data-anchor-id="detailed-hyperparameter-plots"><span class="header-section-number">14.15</span> Detailed Hyperparameter Plots</h2>
<p>The contour plot in this section visualize the interactions of the two most important hyperparameters, <code>l1</code>, and <code>epochs</code> of the surrogate model used to optimize the hyperparameters. Since some of these hyperparameters take fatorial or integer values, sometimes step-like fitness landcapes (or response surfaces) are generated. SPOT draws the interactions of the main hyperparameters by default. It is also possible to visualize all interactions. For this, again refer to the notebook <span class="citation" data-cites="bart23e">(<a href="references.html#ref-bart23e" role="doc-biblioref">Bartz-Beielstein 2023</a>)</span>.</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb483"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb483-1"><a href="#cb483-1" aria-hidden="true" tabindex="-1"></a>filename <span class="op">=</span> <span class="st">"./figures/"</span> <span class="op">+</span> experiment_name</span>
<span id="cb483-2"><a href="#cb483-2" aria-hidden="true" tabindex="-1"></a>spot_tuner.plot_important_hyperparameter_contour(filename<span class="op">=</span>filename)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>dropout_prob:  0.12318354432897388
batch_size:  100.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="24_spot_torch_regression_files/figure-html/cell-32-output-2.png" width="712" height="281"></p>
</div>
</div>
<div id="fig-contour-0-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figures/figures24-torch_bartz09_10min_20init_2023-06-06_22-50-23_contour_0_3.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;14.6: Contour plot of the loss as a function of <code>epochs</code> and <code>l1</code>, i.e., the number of neurons in the layers.</figcaption>
</figure>
</div>
<p><a href="#fig-contour-0-3">Figure&nbsp;<span>14.6</span></a> shows a contour plot of the loss as a function of the hyperparameters. These plots are very helpful for benchmark studies and for understanding neural networks. <code>spotPython</code> provides additional tools for a visual inspection of the results and give valuable insights into the hyperparameter tuning process. This is especially useful for model explainability, transparency, and trustworthiness. In addition to the contour plots, <a href="#fig-parallel">Figure&nbsp;<span>14.7</span></a> shows the parallel plot of the hyperparameters.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb485"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb485-1"><a href="#cb485-1" aria-hidden="true" tabindex="-1"></a>spot_tuner.parallel_plot()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<div>                            <div id="1ddf3d10-eb7d-494f-8001-4b26bf223731" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("1ddf3d10-eb7d-494f-8001-4b26bf223731")) {                    Plotly.newPlot(                        "1ddf3d10-eb7d-494f-8001-4b26bf223731",                        [{"dimensions":[{"label":"l1","range":[3.0,7.0],"values":[7.0,5.0,3.0,4.0,7.0,7.0,7.0,7.0,7.0,7.0]},{"label":"dropout_prob","range":[0.13021660839652088,0.8973189149831583],"values":[0.7103122166156,0.13021660839652088,0.5015226665924828,0.26673029336651144,0.8973189149831583,0.8941737037376589,0.7135267315833492,0.6872970752802665,0.5252411128304804,0.16343155599701642]},{"label":"batch_size","range":[1.0,4.0],"values":[2.0,3.0,4.0,3.0,1.0,1.0,2.0,2.0,2.0,2.0]},{"label":"epochs","range":[4.0,14.0],"values":[7.0,8.0,14.0,4.0,11.0,11.0,7.0,7.0,7.0,7.0]},{"label":"patience","range":[3.0,7.0],"values":[7.0,6.0,3.0,4.0,5.0,5.0,7.0,7.0,7.0,7.0]},{"label":"optimizer","range":[1.0,6.0],"values":[3.0,1.0,6.0,4.0,2.0,2.0,3.0,3.0,3.0,3.0]}],"line":{"cmax":0.5465334559741774,"cmin":0.024931732943902413,"color":[0.0862145904575785,0.4409936603746916,0.10294534658130847,0.5465334559741774,0.0800885943544563,0.09770269146111484,0.09144544609511893,0.06696127450714509,0.040394650309657054,0.024931732943902413],"colorscale":[[0.0,"rgb(0,0,131)"],[0.2,"rgb(0,60,170)"],[0.4,"rgb(5,255,255)"],[0.6,"rgb(255,255,0)"],[0.8,"rgb(250,0,0)"],[1.0,"rgb(128,0,0)"]],"showscale":true},"type":"parcoords"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('1ddf3d10-eb7d-494f-8001-4b26bf223731');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<div id="fig-parallel" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./figures/figures24-torch_bartz09_10min_20init_2023-06-06_22-50-23_parallel.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;14.7: Parallel plot</figcaption>
</figure>
</div>
</section>
<section id="sec-summary-24" class="level2" data-number="14.16">
<h2 data-number="14.16" class="anchored" data-anchor-id="sec-summary-24"><span class="header-section-number">14.16</span> Summary and Outlook</h2>
<p>This tutorial presents the hyperparameter tuning open source software <code>spotPython</code> for <code>PyTorch</code>. Some of the advantages of <code>spotPython</code> are:</p>
<ul>
<li>Numerical and categorical hyperparameters.</li>
<li>Powerful surrogate models.</li>
<li>Flexible approach and easy to use.</li>
<li>Simple JSON files for the specification of the hyperparameters.</li>
<li>Extension of default and user specified network classes.</li>
<li>Noise handling techniques.</li>
<li>Online visualization of the hyperparameter tuning process with <code>tensorboard</code>.</li>
</ul>
<p>Currently, only rudimentary parallel and distributed neural network training is possible, but these capabilities will be extended in the future. The next version of <code>spotPython</code> will also include a more detailed documentation and more examples.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Important: This tutorial does not present a complete benchmarking study <span class="citation" data-cites="bart20gArxiv">(<a href="references.html#ref-bart20gArxiv" role="doc-biblioref">Bartz-Beielstein et al. 2020</a>)</span>. The results are only preliminary and highly dependent on the local configuration (hard- and software). Our goal is to provide a first impression of the performance of the hyperparameter tuning package <code>spotPython</code>. The results should be interpreted with care.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-bart21i" class="csl-entry" role="listitem">
Bartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, eds. 2022. <em><span class="nocase">Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide</span></em>. Springer.
</div>
<div id="ref-bart23e" class="csl-entry" role="listitem">
Bartz-Beielstein, Thomas. 2023. <span>“<span>PyTorch</span> Hyperparameter Tuning with <span>SPOT</span>: Comparison with <span>Ray Tuner</span> and Default Hyperparameters on <span>CIFAR10</span>.”</span> <a href="https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb">https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb</a>.
</div>
<div id="ref-bart20gArxiv" class="csl-entry" role="listitem">
Bartz-Beielstein, Thomas, Carola Doerr, Jakob Bossek, Sowmya Chandrasekaran, Tome Eftimov, Andreas Fischbach, Pascal Kerschke, et al. 2020. <span>“Benchmarking in Optimization: Best Practice and Open Issues.”</span> arXiv. <a href="https://arxiv.org/abs/2007.03488">https://arxiv.org/abs/2007.03488</a>.
</div>
<div id="ref-mont20a" class="csl-entry" role="listitem">
Montiel, Jacob, Max Halford, Saulo Martiello Mastelini, Geoffrey Bolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, et al. 2021. <span>“River: Machine Learning for Streaming Data in Python.”</span>
</div>
<div id="ref-pyto23a" class="csl-entry" role="listitem">
PyTorch. 2023. <span>“Hyperparameter Tuning with Ray Tune.”</span> <a href="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html">https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="https://torchmetrics.readthedocs.io/en/latest/">https://torchmetrics.readthedocs.io/en/latest/.</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./14_spot_ray_hpt_torch_cifar10.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning for PyTorch With <code>spotPython</code></span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./25_spot_torch_vbdp.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Hyperparameter Tuning: VBDP</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>