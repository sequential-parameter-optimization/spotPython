[
  {
    "objectID": "bart23e.html",
    "href": "bart23e.html",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "",
    "text": "Hyperparameter tuning is an important, but often difficult and computationally intensive task. Changing the architecture of a neural network or the learning rate of an optimizer can have a significant impact on the performance.\nThe goal of hyperparameter tuning is to optimize the hyperparameters in a way that improves the performance of the machine learning or deep learning model. The simplest, but also most computationally expensive, approach uses manual search (or trial-and-error (Meignan et al. 2015)). Commonly encountered is simple random search, i.e., random and repeated selection of hyperparameters for evaluation, and lattice search (“grid search”). In addition, methods that perform directed search and other model-free algorithms, i.e., algorithms that do not explicitly rely on a model, e.g., evolution strategies (Bartz-Beielstein et al. 2014) or pattern search (Lewis, Torczon, and Trosset 2000) play an important role. Also, “hyperband”, i.e., a multi-armed bandit strategy that dynamically allocates resources to a set of random configurations and uses successive bisections to stop configurations with poor performance (Li et al. 2016), is very common in hyperparameter tuning. The most sophisticated and efficient approaches are the Bayesian optimization and surrogate model based optimization methods, which are based on the optimization of cost functions determined by simulations or experiments.\nWe consider below a surrogate model based optimization-based hyperparameter tuning approach based on the Python version of the SPOT (“Sequential Parameter Optimization Toolbox”) (Bartz-Beielstein, Lasarczyk, and Preuss 2005), which is suitable for situations where only limited resources are available. This may be due to limited availability and cost of hardware, or due to the fact that confidential data may only be processed locally, e.g., due to legal requirements. Furthermore, in our approach, the understanding of algorithms is seen as a key tool for enabling transparency and explainability. This can be enabled, for example, by quantifying the contribution of machine learning and deep learning components (nodes, layers, split decisions, activation functions, etc.). Understanding the importance of hyperparameters and the interactions between multiple hyperparameters plays a major role in the interpretability and explainability of machine learning models. SPOT provides statistical tools for understanding hyperparameters and their interactions. Last but not least, it should be noted that the SPOT software code is available in the open source spotPython package on github1, allowing replicability of the results. This tutorial descries the Python variant of SPOT, which is called spotPython. The R implementation is described in Bartz et al. (2022). SPOT is an established open source software that has been maintained for more than 15 years (Bartz-Beielstein, Lasarczyk, and Preuss 2005) (Bartz et al. 2022).\nThis tutorial is structured as follows. The concept of the hyperparameter tuning software spotPython is described in Section 2. Section 3 (“Quickstart”) describes the execution of the example from the tutorial “Hyperparameter Tuning with Ray Tune” (PyTorch 2023a). Section 4 describes the integration of spotPython into the PyTorch training workflow in detail and presents the results. Finally, Section 5 presents a summary and an outlook.\n\n\n\n\n\n\nNote\n\n\n\nThe corresponding .ipynb notebook (Bartz-Beielstein 2023) is updated regularly and reflects updates and changes in the spotPython package. It can be downloaded from https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb."
  },
  {
    "objectID": "bart23e.html#sec-setup",
    "href": "bart23e.html#sec-setup",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Setup",
    "text": "Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size and the device that is used.\n\nMAX_TIME = 60\nINIT_SIZE = 20\nDEVICE = \"cpu\" # \"cuda:0\""
  },
  {
    "objectID": "bart23e.html#initialization-of-the-fun_control-dictionary",
    "href": "bart23e.html#initialization-of-the-fun_control-dictionary",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Initialization of the fun_control Dictionary",
    "text": "Initialization of the fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process. This dictionary is called fun_control and is initialized with the function fun_control_init. The function fun_control_init returns a skeleton dictionary. The dictionary is filled with the required information for the hyperparameter tuning process. It stores the hyperparameter tuning settings, e.g., the deep learning network architecture that should be tuned, the classification (or regression) problem, and the data that is used for the tuning. The dictionary is used as an input for the SPOT function.\n\nfun_control = fun_control_init()"
  },
  {
    "objectID": "bart23e.html#sec-data-loading",
    "href": "bart23e.html#sec-data-loading",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Data Loading",
    "text": "Data Loading\nThe data loading process is implemented in the same manner as described in the Section “Data loaders” in PyTorch (2023a). The data loaders are wrapped into the function load_data_cifar10 which is identical to the function load_data in PyTorch (2023a). A global data directory is used, which allows sharing the data directory between different trials.\n\ndef load_data_cifar10(data_dir=\"./data\"):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    trainset = torchvision.datasets.CIFAR10(\n        root=data_dir, train=True, download=True, transform=transform)\n\n    testset = torchvision.datasets.CIFAR10(\n        root=data_dir, train=False, download=True, transform=transform)\n\n    return trainset, testset\n\nThe method load_data_cifar10 is part of the spotPython package and can be imported from spotPython.data.torchdata.\nIn the following step, the test and train data are added to the dictionary fun_control.\n\ntrain, test = load_data_cifar10()\nn_samples = len(train)\n# add the dataset to the fun_control\nfun_control.update({\n    \"train\": train,\n    \"test\": test,\n    \"n_samples\": n_samples})"
  },
  {
    "objectID": "bart23e.html#sec-specification-of-preprocessing-model",
    "href": "bart23e.html#sec-specification-of-preprocessing-model",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Specification of the Preprocessing Model",
    "text": "Specification of the Preprocessing Model\nAfter the training and test data are specified and added to the fun_control dictionary, spotPython allows the specification of a data preprocessing pipeline, e.g., for the scaling of the data or for the one-hot encoding of categorical variables. The preprocessing model is called prep_model (“preparation” or pre-processing) and includes steps that are not subject to the hyperparameter tuning process. The preprocessing model is specified in the fun_control dictionary. The preprocessing model can be implemented as a sklearn pipeline. The following code shows a typical preprocessing pipeline:\ncategorical_columns = [\"cities\", \"colors\"]\none_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\",\n                                    sparse_output=False)\nprep_model = ColumnTransformer(\n        transformers=[\n             (\"categorical\", one_hot_encoder, categorical_columns),\n         ],\n         remainder=StandardScaler(),\n     )\nBecause the Ray Tune (ray[tune]) hyperparameter tuning as described in PyTorch (2023a) does not use a preprocessing model, the preprocessing model is set to None here.\n\nprep_model = None\nfun_control.update({\"prep_model\": prep_model})"
  },
  {
    "objectID": "bart23e.html#sec-selection-of-the-algorithm",
    "href": "bart23e.html#sec-selection-of-the-algorithm",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Select algorithm and core_model_hyper_dict",
    "text": "Select algorithm and core_model_hyper_dict\nThe same neural network model as implemented in the section “Configurable neural network” of the PyTorch tutorial (PyTorch 2023a) is used here. We will show the implementation from PyTorch (2023a) in Section 4.5.1 first, before the extended implementation with spotPython is shown in Section 4.5.2.\n\nImplementing a Configurable Neural Network With Ray Tune\nWe used the same hyperparameters that are implemented as configurable in the PyTorch tutorial. We specify the layer sizes, namely l1 and l2, of the fully connected layers:\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nThe learning rate, i.e., lr, of the optimizer is made configurable, too:\noptimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n\n\nImplementing a Configurable Neural Network With spotPython\nspotPython implements a class which is similar to the class described in the PyTorch tutorial. The class is called Net_CIFAR10 and is implemented in the file netcifar10.py.\nfrom torch import nn\nimport torch.nn.functional as F\nimport spotPython.torch.netcore as netcore\n\n\nclass Net_CIFAR10(netcore.Net_Core):\n    def __init__(self, l1, l2, lr_mult, batch_size, epochs, k_folds, patience, optimizer, sgd_momentum):\n        super(Net_CIFAR10, self).__init__(\n            lr_mult=lr_mult,\n            batch_size=batch_size,\n            epochs=epochs,\n            k_folds=k_folds,\n            patience=patience,\n            optimizer=optimizer,\n            sgd_momentum=sgd_momentum,\n        )\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\nNet_CIFAR10 inherits from the class Net_Core which is implemented in the file netcore.py. It implements the additional attributes that are common to all neural network models. The attributes are the learning rate multiplier lr_mult, the batch size batch_size, the number of epochs epochs, the number of folds k_folds for the cross validation, and the patience patience for the early stopping. The class Net_Core is shown below.\nfrom torch import nn\n\n\nclass Net_Core(nn.Module):\n    def __init__(self, lr_mult, batch_size, epochs, k_folds, patience, optimizer, sgd_momentum):\n        super(Net_Core, self).__init__()\n        self.lr_mult = lr_mult\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.k_folds = k_folds\n        self.patience = patience\n        self.optimizer = optimizer\n        self.sgd_momentum = sgd_momentum\n\n\n\n\n\n\nThe Net_Core class\n\n\n\nThe Net_Core class is implemented in the file netcore.py. It implements hyperparameters as attributes, that are not used by the core_model, e.g.:\n\noptimizer (optimizer),\nlearning rate (lr),\nbatch size (batch_size),\nepochs (epochs),\nk_folds (k_folds), and\nearly stopping criterion “patience” (patience).\n\nUsers can add further attributes to the class.\n\n\n\n\nComparison of the Approach Described in the PyTorch Tutorial With spotPython\nComparing the class Net from the PyTorch tutorial and the class Net_CIFAR10 from spotPython, we see that the class Net_CIFAR10 has additional attributes and does not inherit from nn directly. It adds an additional class, Net_core, that takes care of additional attributes that are common to all neural network models, e.g., the learning rate multiplier lr_mult or the batch size batch_size.\nspotPython’s core_model implements an instance of the Net_CIFAR10 class. In addition to the basic neural network model, the core_model can use these additional attributes. spotPython provides methods for handling these additional attributes to guarantee 100% compatibility with the PyTorch classes. The method add_core_model_to_fun_control adds the hyperparameters and additional attributes to the fun_control dictionary. The method is shown below.\n\ncore_model = Net_CIFAR10\nfun_control = add_core_model_to_fun_control(core_model=core_model,\n                              fun_control=fun_control,\n                              hyper_dict=TorchHyperDict,\n                              filename=None)"
  },
  {
    "objectID": "bart23e.html#sec-search-space",
    "href": "bart23e.html#sec-search-space",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "The Search Space",
    "text": "The Search Space\nIn Section 4.6.1, we first describe how to configure the search space with ray[tune] (as shown in PyTorch (2023a)) and then how to configure the search space with spotPython in Section 4.6.2.\n\nConfiguring the Search Space With Ray Tune\nRay Tune’s search space can be configured as follows (PyTorch 2023a):\nconfig = {\n    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"batch_size\": tune.choice([2, 4, 8, 16])\n}\nThe tune.sample_from() function enables the user to define sample methods to obtain hyperparameters. In this example, the l1 and l2 parameters should be powers of 2 between 4 and 256, so either 4, 8, 16, 32, 64, 128, or 256. The lr (learning rate) should be uniformly sampled between 0.0001 and 0.1. Lastly, the batch size is a choice between 2, 4, 8, and 16.\nAt each trial, ray[tune] will randomly sample a combination of parameters from these search spaces. It will then train a number of models in parallel and find the best performing one among these. ray[tune] uses the ASHAScheduler which will terminate bad performing trials early.\n\n\nConfiguring the Search Space With spotPython\n\nThe hyper_dict Hyperparameters for the Selected Algorithm\nspotPython uses JSON files for the specification of the hyperparameters. Users can specify their individual JSON files, or they can use the JSON files provided by spotPython. The JSON file for the core_model is called torch_hyper_dict.json.\nIn contrast to ray[tune], spotPython can handle numerical, boolean, and categorical hyperparameters. They can be specified in the JSON file in a similar way as the numerical hyperparameters as shown below. Each entry in the JSON file represents one hyperparameter with the following structure: type, default, transform, lower, and upper.\n\"factor_hyperparameter\": {\n    \"levels\": [\"A\", \"B\", \"C\"],\n    \"type\": \"factor\",\n    \"default\": \"B\",\n    \"transform\": \"None\",\n    \"core_model_parameter_type\": \"str\",\n    \"lower\": 0,\n    \"upper\": 2},\nThe corresponding entries for the Net_CIFAR10 class are shown below.\n{\"Net_CIFAR10\":\n    {\n        \"l1\": {\n            \"type\": \"int\",\n            \"default\": 5,\n            \"transform\": \"transform_power_2_int\",\n            \"lower\": 2,\n            \"upper\": 9},\n        \"l2\": {\n            \"type\": \"int\",\n            \"default\": 5,\n            \"transform\": \"transform_power_2_int\",\n            \"lower\": 2,\n            \"upper\": 9},\n        \"lr_mult\": {\n            \"type\": \"float\",\n            \"default\": 1.0,\n            \"transform\": \"None\",\n            \"lower\": 0.1,\n            \"upper\": 10},\n        \"batch_size\": {\n            \"type\": \"int\",\n            \"default\": 4,\n            \"transform\": \"transform_power_2_int\",\n            \"lower\": 1,\n            \"upper\": 4},\n        \"epochs\": {\n            \"type\": \"int\",\n            \"default\": 3,\n            \"transform\": \"transform_power_2_int\",\n            \"lower\": 1,\n            \"upper\": 4},\n        \"k_folds\": {\n            \"type\": \"int\",\n            \"default\": 2,\n            \"transform\": \"None\",\n            \"lower\": 2,\n            \"upper\": 3},\n        \"patience\": {\n            \"type\": \"int\",\n            \"default\": 5,\n            \"transform\": \"None\",\n            \"lower\": 2,\n            \"upper\": 10},\n        \"optimizer\": {\n            \"levels\": [\"Adadelta\",\n                       \"Adagrad\",\n                       \"Adam\",\n                       \"AdamW\",\n                       \"SparseAdam\",\n                       \"Adamax\",\n                       \"ASGD\",\n                       \"LBFGS\",\n                       \"NAdam\",\n                       \"RAdam\",\n                       \"RMSprop\",\n                       \"Rprop\",\n                       \"SGD\"],\n            \"type\": \"factor\",\n            \"default\": \"SGD\",\n            \"transform\": \"None\",\n            \"class_name\": \"torch.optim\",\n            \"core_model_parameter_type\": \"str\",\n            \"lower\": 0,\n            \"upper\": 12},\n        \"sgd_momentum\": {\n            \"type\": \"float\",\n            \"default\": 0.0,\n            \"transform\": \"None\",\n            \"lower\": 0.0,\n            \"upper\": 1.0}\n    }\n}"
  },
  {
    "objectID": "bart23e.html#sec-modification-of-hyperparameters",
    "href": "bart23e.html#sec-modification-of-hyperparameters",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Modifying the Hyperparameters",
    "text": "Modifying the Hyperparameters\nRay tune (PyTorch 2023a) does not provide a way to change the specified hyperparameters without re-compilation. However, spotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions are described in the following.\n\nModify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nAfter specifying the model, the corresponding hyperparameters, their types and bounds are loaded from the JSON file torch_hyper_dict.json. After loading, the user can modify the hyperparameters, e.g., the bounds. spotPython provides a simple rule for de-activating hyperparameters: If the lower and the upper bound are set to identical values, the hyperparameter is de-activated. This is useful for the hyperparameter tuning, because it allows to specify a hyperparameter in the JSON file, but to de-activate it in the fun_control dictionary. This is done in the next step.\n\n\nModify Hyperparameters of Type numeric and integer (boolean)\nSince the hyperparameter k_folds is not used in the PyTorch tutorial, it is de-activated here by setting the lower and upper bound to the same value. Note, k_folds is of type “integer”.\n\nfun_control = modify_hyper_parameter_bounds(fun_control, \"batch_size\", bounds=[1, 5])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"k_folds\", bounds=[0, 0])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[3, 3])\nfun_control[\"core_model_hyper_dict\"]\n\n\n\nModify Hyperparameter of Type factor\nIn a similar manner as for the numerical hyperparameters, the categorical hyperparameters can be modified. New configurations can be chosen by adding or deleting levels. For example, the hyperparameter optimizer can be re-configured as follows:\nIn the following setting, two optimizers (\"SGD\" and \"Adam\") will be compared during the spotPython hyperparameter tuning. The hyperparameter optimizer is active.\n\nfun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\", [\"SGD\", \"Adam\"])\n\nThe hyperparameter optimizer can be de-activated by choosing only one value (level), here: \"SGD\".\n\nfun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\", [\"SGD\"])\n\nAs discussed in Section 4.7.4, there are some issues with the LBFGS optimizer. Therefore, the usage of the LBFGS optimizer is not deactivated in spotPython by default. However, the LBFGS optimizer can be activated by adding it to the list of optimizers. Rprop was removed, because it does perform very poorly (as some pre-tests have shown). However, it can also be activated by adding it to the list of optimizers. Since SparseAdam does not support dense gradients, Adam was used instead. Therefore, there are 10 default optimizers:\n\nfun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adadelta\",\n     \"Adagrad\", \"Adam\", \"AdamW\", \"Adamax\", \"ASGD\", \"NAdam\", \"RAdam\",\n      \"RMSprop\", \"SGD\"])\n\n\n\nOptimizers\nTable 1 shows some of the optimizers available in PyTorch:\n\n\nTable 1: Optimizers available in PyTorch (selection). “mom” denotes momentum, “weight” weight_decay, “damp” dampening, “nest” nesterov, “lr_sc” learning rate for scaling delta, “mom_dec” for momentum_decay, and “step_s” for step_sizes. The default values are shown in the table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizer\nlr\nmom\nweight\ndamp\nnest\nrho\nlr_sc\nlr_decay\nbetas\nlambd\nalpha\nmom_decay\netas\nstep_s\n\n\n\n\nAdadelta\n-\n-\n0.\n-\n-\n0.9\n1.0\n-\n-\n-\n-\n-\n-\n-\n\n\nAdagrad\n1e-2\n-\n0.\n-\n-\n-\n-\n0.\n-\n-\n-\n-\n-\n-\n\n\nAdam\n1e-3\n-\n0.\n-\n-\n-\n-\n-\n(0.9,0.999)\n-\n-\n-\n-\n-\n\n\nAdamW\n1e-3\n-\n1e-2\n-\n-\n-\n-\n-\n(0.9,0.999)\n-\n-\n-\n-\n-\n\n\nSparseAdam\n1e-3\n-\n-\n-\n-\n-\n-\n-\n(0.9,0.999)\n-\n-\n-\n-\n-\n\n\nAdamax\n2e-3\n-\n0.\n-\n-\n-\n-\n-\n(0.9, 0.999)\n-\n-\n-\n-\n-\n\n\nASGD\n1e-2\n0.9\n0.\n-\nFalse\n-\n-\n-\n-\n1e-4\n0.75\n-\n-\n-\n\n\nLBFGS\n1.\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nNAdam\n2e-3\n-\n0.\n-\n-\n-\n-\n-\n(0.9,0.999)\n-\n-\n0\n-\n-\n\n\nRAdam\n1e-3\n-\n0.\n-\n-\n-\n-\n-\n(0.9,0.999)\n-\n-\n-\n-\n-\n\n\nRMSprop\n1e-2\n0.\n0.\n-\n-\n-\n-\n-\n(0.9,0.999)\n-\n-\n-\n-\n-\n\n\nRprop\n1e-2\n-\n-\n-\n-\n-\n-\n-\n-\n-\n(0.5,1.2)\n(1e-6, 50)\n-\n-\n\n\nSGD\nrequired\n0.\n0.\n0.\nFalse\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n\n\nspotPython implements an optimization handler that maps the optimizer names to the corresponding PyTorch optimizers.\n\n\n\n\n\n\nA note on LBFGS\n\n\n\nWe recommend deactivating PyTorch’s LBFGS optimizer, because it does not perform very well. The PyTorch documentation, see https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS, states:\n\nThis is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes). If it doesn’t fit in memory try reducing the history size, or use a different algorithm.\n\nFurthermore, the LBFGS optimizer is not compatible with the PyTorch tutorial. The reason is that the LBFGS optimizer requires the closure function, which is not implemented in the PyTorch tutorial. Therefore, the LBFGS optimizer is recommended here.\n\n\nSince there are 10 optimizers in the portfolio, it is not recommended tuning the hyperparameters that effect one single optimizer only.\n\n\n\n\n\n\nA note on the learning rate\n\n\n\nspotPython provides a multiplier for the default learning rates, lr_mult, because optimizers use different learning rates. Using a multiplier for the learning rates might enable a simultaneous tuning of the learning rates for all optimizers. However, this is not recommended, because the learning rates are not comparable across optimizers. Therefore, we recommend fixing the learning rate for all optimizers if multiple optimizers are used. This can be done by setting the lower and upper bounds of the learning rate multiplier to the same value as shown below.\n\n\nThus, the learning rate, which affects the SGD optimizer, will be set to a fixed value. We choose the default value of 1e-3 for the learning rate, because it is used in other PyTorch examples (it is also the default value used by spotPython as defined in the optimizer_handler() method). We recommend tuning the learning rate later, when a reduced set of optimizers is fixed. Here, we will demonstrate how to select in a screening phase the optimizers that should be used for the hyperparameter tuning.\nFor the same reason, we will fix the sgd_momentum to 0.9.\n\nfun_control = modify_hyper_parameter_bounds(fun_control, \"lr_mult\", bounds=[1.0, 1.0])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"sgd_momentum\", bounds=[0.9, 0.9])"
  },
  {
    "objectID": "bart23e.html#sec-selection-of-target-function",
    "href": "bart23e.html#sec-selection-of-target-function",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Evaluation",
    "text": "Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set and\nthe loss function (and a metric).\n\n\nHold-out Data Split and Cross-Validation\nAs a default, spotPython provides a standard hold-out data split and cross validation.\n\nHold-out Data Split\nIf a hold-out data split is used, the data will be partitioned into a training, a validation, and a test data set. The split depends on the setting of the eval parameter. If eval is set to train_hold_out, one data set, usually the original training data set, is split into a new training and a validation data set. The training data set is used for training the model. The validation data set is used for the evaluation of the hyperparameter configuration and early stopping to prevent overfitting. In this case, the original test data set is not used. The following splits are performed in the hold-out setting: \\(\\{\\text{train}_0, \\text{test}\\} \\rightarrow \\{\\text{train}_1, \\text{validation}_1, \\text{test}\\}\\), where \\(\\text{train}_1 \\cup \\text{validation}_1 = \\text{train}_0\\).\n\n\n\n\n\n\nNote\n\n\n\nspotPython returns the hyperparameters of the machine learning and deep learning models, e.g., number of layers, learning rate, or optimizer, but not the model weights. Therefore, after the SPOT run is finished, the corresponding model with the optimized architecture has to be trained again with the best hyperparameter configuration. The training is performed on the training data set. The test data set is used for the final evaluation of the model.\nSummarizing, the following splits are performed in the hold-out setting:\n\nRun spotPython with eval set to train_hold_out to determine the best hyperparameter configuration.\nTrain the model with the best hyperparameter configuration (“architecture”) on the training data set:\n\ntrain_tuned(model_spot, train, \"model_spot.pt\").\n\nTest the model on the test data:\n\ntest_tuned(model_spot, test, \"model_spot.pt\")\n\n\nThese steps will be exemplified in the following sections.\n\n\nIn addition to this hold-out setting, spotPython provides another hold-out setting, where an explicit test data is specified by the user that will be used as the validation set. To choose this option, the eval parameter is set to test_hold_out. In this case, the training data set is used for the model training. Then, the explicitly defined test data set is used for the evaluation of the hyperparameter configuration (the validation).\n\n\nCross-Validation\nThe cross validation setting is used by setting the eval parameter to train_cv or test_cv. In both cases, the data set is split into \\(k\\) folds. The model is trained on \\(k-1\\) folds and evaluated on the remaining fold. This is repeated \\(k\\) times, so that each fold is used exactly once for evaluation. The final evaluation is performed on the test data set. The cross validation setting is useful for small data sets, because it allows to use all data for training and evaluation. However, it is computationally expensive, because the model has to be trained \\(k\\) times.\n\n\n\n\n\n\nNote\n\n\n\nCombinations of the above settings are possible, e.g., cross validation can be used for training and hold-out for evaluation or vice versa. Also, cross validation can be used for training and testing. Because cross validation is not used in the PyTorch tutorial (PyTorch 2023a), it is not considered further here.\n\n\n\n\nOverview of the Evaluation Settings\n\nSettings for the Hyperparameter Tuning\nTable 2 provides an overview of the training evaluations.\n\n\nTable 2: Overview of the evaluation settings.\n\n\n\n\n\n\n\n\n\neval\ntrain\ntest\nfunction\ncomment\n\n\n\n\n\"train_hold_out\"\n\\(\\checkmark\\)\n\ntrain_hold_out(), validate_fold_or_hold_out() for early stopping\nsplits the train data set internally\n\n\n\"test_hold_out\"\n\\(\\checkmark\\)\n\\(\\checkmark\\)\ntrain_hold_out(), validate_fold_or_hold_out() for early stopping\nuse the test data set for validate_fold_or_hold_out()\n\n\n\"train_cv\"\n\\(\\checkmark\\)\n\nevaluate_cv(net, train)\nCV using the train data set\n\n\n\"test_cv\"\n\n\\(\\checkmark\\)\nevaluate_cv(net, test)\nCV using the test data set . Identical to \"train_cv\", uses only test data.\n\n\n\n\n\n\"train_cv\" and \"test_cv\" use sklearn.model_selection.KFold() internally.\n\nSection 6.2 (in the Appendix) provides more details on the data splitting.\n\n\n\nSettings for the Final Evaluation of the Tuned Architecture\n\nTraining of the Tuned Architecture\ntrain_tuned(model, train): train the model with the best hyperparameter configuration (or simply the default) on the training data set. It splits the traindata into new train and validation sets using create_train_val_data_loaders(), which calls torch.utils.data.random_split() internally. Currently, 60% of the data is used for training and 40% for validation. The train data is used for training the model with train_hold_out(). The validation data is used for early stopping using validate_fold_or_hold_out() on the validation data set.\n\n\nTesting of the Tuned Architecture\ntest_tuned(model, test): test the model on the test data set. No data splitting is performed. The (trained) model is evaluated using the validate_fold_or_hold_out() function.\nNote: During training, shuffle is set to True, whereas during testing, shuffle is set to False.\nSection 6.2.5 describes the final evaluation of the tuned architecture.\n\n\n\n\nLoss Functions and Metrics\nThe key \"loss_function\" specifies the loss function which is used during the optimization. There are several different loss functions under PyTorch’s nn package. For example, a simple loss is MSELoss, which computes the mean-squared error between the output and the target. In this tutorial we will use CrossEntropyLoss, because it is also used in the PyTorch tutorial.\n\nfrom torch.nn import CrossEntropyLoss\nloss_function = CrossEntropyLoss()\nfun_control.update({\"loss_function\": loss_function})\n\nIn addition to the loss functions, spotPython provides access to a large number of metrics. The key \"metric_sklearn\" is used for metrics that follow the scikit-learn conventions. The key \"river_metric\" is used for the river based evaluation (Montiel et al. 2021) via eval_oml_iter_progressive, and the key \"metric_torch\" is used for the metrics from TorchMetrics. TorchMetrics is a collection of more than 90 PyTorch metrics4. Because the PyTorch tutorial uses the accuracy as metric, we use the same metric here. Currently, accuracy is computed in the tutorial’s example code. We will use TorchMetrics instead, because it offers more flexibilty, e.g., it can be used for regression and classification. Furthermore, TorchMetrics offers the following advantages:\n\nA standardized interface to increase reproducibility\nReduces Boilerplate\nDistributed-training compatible\nRigorously tested\nAutomatic accumulation over batches\nAutomatic synchronization between multiple devices\n\nTherefore, we set\n\nmetric_torch = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n\n\nloss_function = CrossEntropyLoss()\nweights = 1.0\nmetric_torch = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\nshuffle = True\neval = \"train_hold_out\"\ndevice = DEVICE\nshow_batch_interval = 100_000\npath=\"torch_model.pt\"\n\nfun_control.update({\n               \"data_dir\": None,\n               \"checkpoint_dir\": None,\n               \"horizon\": None,\n               \"oml_grace_period\": None,\n               \"weights\": weights,\n               \"step\": None,\n               \"log_level\": 50,\n               \"weight_coeff\": None,\n               \"metric_torch\": metric_torch,\n               \"metric_river\": None,\n               \"metric_sklearn\": None,\n               \"loss_function\": loss_function,\n               \"shuffle\": shuffle,\n               \"eval\": eval,\n               \"device\": device,\n               \"show_batch_interval\": show_batch_interval,\n               \"path\": path,\n               })"
  },
  {
    "objectID": "bart23e.html#sec-call-the-hyperparameter-tuner",
    "href": "bart23e.html#sec-call-the-hyperparameter-tuner",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Calling the SPOT Function",
    "text": "Calling the SPOT Function\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table generates a design table as follows:\n\nprint(gen_design_table(fun_control))\n\nThis allows to check if all information is available and if the information is correct. Table 3 shows the experimental design for the hyperparameter tuning. Hyperparameter transformations are shown in the column “transform”, e.g., the l1 default is 5, which results in the value \\(2^5 = 32\\) for the network, because the transformation transform_power_2_int was selected in the JSON file. The default value of the batch_size is set to 4, which results in a batch size of \\(2^4 = 16\\).\n\n\nTable 3: Experimental design for the hyperparameter tuning. The table shows the hyperparameters, their types, default values, lower and upper bounds, and the transformation function. The transformation function is used to transform the hyperparameter values from the unit hypercube to the original domain. The transformation function is applied to the hyperparameter values before the evaluation of the objective function.\n\n\n\n\n\n\n\n\n\n\nname\ntype\ndefault\nlower\nupper\ntransform\n\n\n\n\nl1\nint\n5\n2\n9\ntransform_power_2_int\n\n\nl2\nint\n5\n2\n9\ntransform_power_2_int\n\n\nlr\nfloat\n0.001\n0.001\n0.001\nNone\n\n\nbatch_size\nint\n4\n1\n5\ntransform_power_2_int\n\n\nepochs\nint\n3\n3\n4\ntransform_power_2_int\n\n\nk_folds\nint\n2\n0\n0\nNone\n\n\npatience\nint\n5\n3\n3\nNone\n\n\noptimizer\nfactor\nSGD\n0\n9\nNone\n\n\n\n\nThe objective function fun_torch is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfun = HyperTorch().fun_torch\n\nThe spotPython hyperparameter tuning is started by calling the Spot function. Here, we will run the tuner for approximately 30 minutes (max_time). Note: the initial design is always evaluated in the spotPython run. As a consequence, the run may take longer than specified by max_time, because the evaluation time of initial design (here: init_size, 10 points) is performed independently of max_time.\n\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_optimizer\": differential_evolution,\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\nDuring the run, the following output is shown:\nconfig: {'l1': 4, 'l2': 64, 'lr_mult': 1.0, 'batch_size': 16, 'epochs': 16, 'k_folds': 0, 'patience': 3, 'optimizer': 'Adadelta', 'sgd_momentum': 0.9}\nEpoch: 1\nLoss on hold-out set: 1.602842689704895\nAccuracy on hold-out set: 0.4006\nMetric value on hold-out data: 0.40059998631477356\nEpoch: 2\nLoss on hold-out set: 1.4648857820034027\nAccuracy on hold-out set: 0.47685\nMetric value on hold-out data: 0.4768500030040741\nEpoch: 3\nLoss on hold-out set: 1.403354868555069\nAccuracy on hold-out set: 0.482\nMetric value on hold-out data: 0.4819999933242798\nEpoch: 4\nLoss on hold-out set: 1.384560032081604\nAccuracy on hold-out set: 0.49065\nMetric value on hold-out data: 0.4906499981880188\nEpoch: 5\nLoss on hold-out set: 1.4326466094970702\nAccuracy on hold-out set: 0.4809\nMetric value on hold-out data: 0.48089998960494995\nEpoch: 6\nLoss on hold-out set: 1.3759961807250976\nAccuracy on hold-out set: 0.4995\nMetric value on hold-out data: 0.49950000643730164\nEpoch: 7\nLoss on hold-out set: 1.3684927892208099\nAccuracy on hold-out set: 0.50695\nMetric value on hold-out data: 0.5069500207901001\nEpoch: 8\nLoss on hold-out set: 1.3642385012149811\nAccuracy on hold-out set: 0.506\nMetric value on hold-out data: 0.5059999823570251\nEpoch: 9\nLoss on hold-out set: 1.3157437609672546\nAccuracy on hold-out set: 0.5304\nMetric value on hold-out data: 0.5303999781608582\nEpoch: 10\nLoss on hold-out set: 1.3481314319610596\nAccuracy on hold-out set: 0.5268\nMetric value on hold-out data: 0.5267999768257141\nEpoch: 11\nLoss on hold-out set: 1.3608774542331696\nAccuracy on hold-out set: 0.51525\nMetric value on hold-out data: 0.515250027179718\nEpoch: 12\nLoss on hold-out set: 1.359324642753601\nAccuracy on hold-out set: 0.52355\nMetric value on hold-out data: 0.5235499739646912\nEarly stopping at epoch 11\nReturned to Spot: Validation loss: 1.359324642753601\n----------------------------------------------"
  },
  {
    "objectID": "bart23e.html#sec-results-tuning",
    "href": "bart23e.html#sec-results-tuning",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Results",
    "text": "Results\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The following code generates the progress plot from Figure 2.\n\n\nspot_tuner.plot_progress(log_y=False, filename=\"./figures\" + experiment_name+\"_progress.png\")\nFigure 1: ?(caption)\n\n\n\n\n\nFigure 2: Progress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization (surrogate model based optimization).\n\n\nFigure 2 shows a typical behaviour that can be observed in many hyperparameter studies (Bartz et al. 2022): the largest improvement is obtained during the evaluation of the initial design. The surrogate model based optimization-optimization with the surrogate refines the results. Figure 2 also illustrates one major difference between ray[tune] as used in PyTorch (2023a) and spotPython: the ray[tune] uses a random search and will generate results similar to the black dots, whereas spotPython uses a surrogate model based optimization and presents results represented by red dots in Figure 2. The surrogate model based optimization is considered to be more efficient than a random search, because the surrogate model guides the search towards promising regions in the hyperparameter space.\nIn addition to the improved (“optimized”) hyperparameter values, spotPython allows a statistical analysis, e.g., a sensitivity analysis, of the results. We can print the results of the hyperparameter tuning, see Table 4.\n\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n\n\nTable 4: Results of the hyperparameter tuning. The table shows the hyperparameters, their types, default values, lower and upper bounds, and the transformation function. The column “tuned” shows the tuned values. The column “importance” shows the importance of the hyperparameters. The column “stars” shows the importance of the hyperparameters in stars. The importance is computed by the SPOT software.\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\ntype\ndefault\nlower\nupper\ntuned\ntransform\nimportance\nstars\n\n\n\n\nl1\nint\n5\n2.0\n9.0\n7.0\npow_2_int\n100.00\n***\n\n\nl2\nint\n5\n2.0\n9.0\n3.0\npow_2_int\n96.29\n***\n\n\nlr_mult\nfloat\n1.0\n0.1\n10.0\n0.1\nNone\n0.00\n\n\n\nbatchsize\nint\n4\n1.0\n5.0\n4.0\npow_2_int\n0.00\n\n\n\nepochs\nint\n3\n3.0\n4.0\n4.0\npow_2_int\n4.18\n*\n\n\nk_folds\nint\n2\n0.0\n0.0\n0.0\nNone\n0.00\n\n\n\npatience\nint\n5\n3.0\n3.0\n3.0\nNone\n0.00\n\n\n\noptimizer\nfactor\nSGD\n0.0\n9.0\n3.0\nNone\n0.16\n.\n\n\n\n\nTo visualize the most important hyperparameters, spotPython provides the function plot_importance. The following code generates the importance plot from Figure 3.\n\nspot_tuner.plot_importance(threshold=0.025, filename=\"./figures\" + experiment_name+\"_importance.png\")\n\n\n\n\nFigure 3: Variable importance"
  },
  {
    "objectID": "bart23e.html#sec-get-spot-results",
    "href": "bart23e.html#sec-get-spot-results",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Get SPOT Results",
    "text": "Get SPOT Results\nThe architecture of the spotPython model can be obtained by the following code:\n\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\nmodel_spot\n\nFirst, the numerical representation of the hyperparameters are obtained, i.e., the numpy array X is generated. This array is then used to generate the model model_spot by the function get_one_core_model_from_X. The model model_spot has the following architecture:\nNet_CIFAR10(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=32, bias=True)\n  (fc3): Linear(in_features=32, out_features=10, bias=True)\n)"
  },
  {
    "objectID": "bart23e.html#get-default-hyperparameters",
    "href": "bart23e.html#get-default-hyperparameters",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Get Default Hyperparameters",
    "text": "Get Default Hyperparameters\nIn a similar manner as in Section 4.11, the default hyperparameters can be obtained.\n\n# fun_control was modified, we generate a new one with the original default hyperparameters\nfc = copy.deepcopy(fun_control)\nfc.update({\"core_model_hyper_dict\": hyper_dict[fun_control[\"core_model\"].__name__]})\nmodel_default = get_one_core_model_from_X(X_start, fun_control=fc)\n\nThe corresponding default model has the following architecture:\nNet_CIFAR10(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=32, bias=True)\n  (fc2): Linear(in_features=32, out_features=32, bias=True)\n  (fc3): Linear(in_features=32, out_features=10, bias=True)\n)"
  },
  {
    "objectID": "bart23e.html#evaluation-of-the-tuned-architecture",
    "href": "bart23e.html#evaluation-of-the-tuned-architecture",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Evaluation of the Tuned Architecture",
    "text": "Evaluation of the Tuned Architecture\nThe method train_tuned takes a model architecture without trained weights and trains this model with the train data. The train data is split into train and validation data. The validation data is used for early stopping. The trained model weights are saved as a dictionary.\nThis evaluation is similar to the final evaluation in PyTorch (2023a).\n\ntrain_tuned(net=model_default, train_dataset=train, shuffle=True,\n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        device = DEVICE, show_batch_interval=1_000,)\ntest_tuned(net=model_default, test_dataset=test, \n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        shuffle=False, \n        device = DEVICE)\n\nThe following code trains the model model_spot. If path is set to a filename, e.g., path = \"model_spot_trained.pt\", the weights of the trained model will be saved to this file.\n\ntrain_tuned(net=model_spot, train_dataset=train,\n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        shuffle=True,\n        device = DEVICE,\n        path=None)\n\nLoss on hold-out set: 1.2267619131326675\nAccuracy on hold-out set: 0.58955\nEarly stopping at epoch 13\nIf path is set to a filename, e.g., path = \"model_spot_trained.pt\", the weights of the trained model will be loaded from this file.\n\ntest_tuned(net=model_spot, test_dataset=test,\n            shuffle=False,\n            loss_function=fun_control[\"loss_function\"],\n            metric=fun_control[\"metric_torch\"],\n            device = DEVICE)\n\nLoss on hold-out set: 1.242568492603302\nAccuracy on hold-out set: 0.5957"
  },
  {
    "objectID": "bart23e.html#comparison-with-default-hyperparameters-and-ray-tune",
    "href": "bart23e.html#comparison-with-default-hyperparameters-and-ray-tune",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Comparison with Default Hyperparameters and Ray Tune",
    "text": "Comparison with Default Hyperparameters and Ray Tune\nTable 5 shows the loss and accuracy of the default model, the model with the hyperparameters from SPOT, and the model with the hyperparameters from ray[tune].\n\n\nTable 5: Comparison of the loss and accuracy of the default model, the model with the hyperparameters from SPOT, and the model with the hyperparameters from ray[tune]. ray[tune] only shows the validation loss, because training loss is not reported by ray[tune].\n\n\n\n\n\n\n\n\n\nModel\nValidation Loss\nValidation Accuracy\nLoss\nAccuracy\n\n\n\n\nDefault\n2.1221\n0.2452\n2.1182\n0.2425\n\n\nspotPython\n1.2268\n0.5896\n1.2426\n0.5957\n\n\nray[tune]\n1.1815\n0.5836\n-\n0.5806"
  },
  {
    "objectID": "bart23e.html#detailed-hyperparameter-plots",
    "href": "bart23e.html#detailed-hyperparameter-plots",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Detailed Hyperparameter Plots",
    "text": "Detailed Hyperparameter Plots\nThe contour plots in this section visualize the interactions of the three most important hyperparameters, l1, l2, and epochs, and optimizer of the surrogate model used to optimize the hyperparameters. Since some of these hyperparameters take fatorial or integer values, sometimes step-like fitness landcapes (or response surfaces) are generated. SPOT draws the interactions of the main hyperparameters by default. It is also possible to visualize all interactions. For this, again refer to the notebook (Bartz-Beielstein 2023).\n\nfilename = \"./figures\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\n\n\n\nFigure 4: Contour plot of the loss as a function of l1 and l2, i.e., the number of neurons in the layers.\n\n\n\n\n\nFigure 5: Contour plot of the loss as a function of the number of epochs and the neurons in layer l1.\n\n\n\n\n\nFigure 6: Contour plot of the loss as a function of the optimizer and the neurons in layer l1.\n\n\n\n\n\nFigure 7: Contour plot of the loss as a function of the number of epochs and the neurons in layer l2.\n\n\n\n\n\nFigure 8: Contour plot of the loss as a function of the optimizer and the neurons in layer l2.\n\n\n\n\n\nFigure 9: Contour plot of the loss as a function of the optimizer and the number of epochs.\n\n\nFigure 4 to Figure 9 show the contour plots of the loss as a function of the hyperparameters. These plots are very helpful for benchmark studies and for understanding neural networks. spotPython provides additional tools for a visual inspection of the results and give valuable insights into the hyperparameter tuning process. This is especially useful for model explainability, transparency, and trustworthiness. In addition to the contour plots, Figure 10 shows the parallel plot of the hyperparameters.\n\nspot_tuner.parallel_plot()\n\n\n\n\nFigure 10: Parallel plot"
  },
  {
    "objectID": "bart23e.html#sample-output-from-ray-tunes-run",
    "href": "bart23e.html#sample-output-from-ray-tunes-run",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Sample Output From Ray Tune’s Run",
    "text": "Sample Output From Ray Tune’s Run\nThe output from ray[tune] could look like this (PyTorch 2023b):\nNumber of trials: 10 (10 TERMINATED)\n------+------+-------------+--------------+---------+------------+--------------------+\n|   l1 |   l2 |          lr |   batch_size |    loss |   accuracy | training_iteration |\n+------+------+-------------+--------------+---------+------------+--------------------|\n|   64 |    4 | 0.00011629  |            2 | 1.87273 |     0.244  |                  2 |\n|   32 |   64 | 0.000339763 |            8 | 1.23603 |     0.567  |                  8 |\n|    8 |   16 | 0.00276249  |           16 | 1.1815  |     0.5836 |                 10 |\n|    4 |   64 | 0.000648721 |            4 | 1.31131 |     0.5224 |                  8 |\n|   32 |   16 | 0.000340753 |            8 | 1.26454 |     0.5444 |                  8 |\n|    8 |    4 | 0.000699775 |            8 | 1.99594 |     0.1983 |                  2 |\n|  256 |    8 | 0.0839654   |           16 | 2.3119  |     0.0993 |                  1 |\n|   16 |  128 | 0.0758154   |           16 | 2.33575 |     0.1327 |                  1 |\n|   16 |    8 | 0.0763312   |           16 | 2.31129 |     0.1042 |                  4 |\n|  128 |   16 | 0.000124903 |            4 | 2.26917 |     0.1945 |                  1 |\n+-----+------+------+-------------+--------------+---------+------------+--------------------+\nBest trial config: {'l1': 8, 'l2': 16, 'lr': 0.00276249, 'batch_size': 16, 'data_dir': '...'}\nBest trial final validation loss: 1.181501\nBest trial final validation accuracy: 0.5836\nBest trial test set accuracy: 0.5806"
  },
  {
    "objectID": "bart23e.html#sec-data-splitting",
    "href": "bart23e.html#sec-data-splitting",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Detailed Description of the Data Splitting",
    "text": "Detailed Description of the Data Splitting\n\nDescription of the \"train_hold_out\" Setting\nThe \"train_hold_out\" setting is used by default. It uses the loss function specfied in fun_control and the metric specified in fun_control.\n\nFirst, the method HyperTorch().fun_torch is called.\nfun_torc() calls spotPython.torch.traintest.evaluate_hold_out() as follows:\n\n\ndf_eval, _ = evaluate_hold_out(\n    model,\n    train_dataset=fun_control[\"train\"],\n    shuffle=self.fun_control[\"shuffle\"],\n    loss_function=self.fun_control[\"loss_function\"],\n    metric=self.fun_control[\"metric_torch\"],\n    device=self.fun_control[\"device\"],\n    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n    path=self.fun_control[\"path\"],\n) \n\nNote: Only the data set fun_control[\"train\"] is used for training and validation. It is used as follows:\n\ntrainloader, valloader = create_train_val_data_loaders(\n                dataset=train_dataset, batch_size=batch_size_instance, shuffle=shuffle\n            )\n\ncreate_train_val_data_loaders() splits the train_dataset into trainloader and valloader using torch.utils.data.random_split() as follows:\n\ndef create_train_val_data_loaders(dataset, batch_size, shuffle, num_workers=0):\n    test_abs = int(len(dataset) * 0.6)\n    train_subset, val_subset = random_split(dataset, [test_abs, len(dataset) - test_abs])\n    trainloader = torch.utils.data.DataLoader(\n        train_subset, batch_size=int(batch_size), shuffle=shuffle, num_workers=num_workers\n    )\n    valloader = torch.utils.data.DataLoader(\n        val_subset, batch_size=int(batch_size), shuffle=shuffle, num_workers=num_workers\n    )\n    return trainloader, valloader\n\nThe optimizer is set up as follows:\n\nlr_mult_instance = net.lr_mult\noptimizer = optimizer_handler(optimizer_name=optimizer_instance, params=net.parameters(), lr_mult=lr_mult_instance)\n\n\nevaluate_hold_out() sets the net attributes such as epochs, batch_size, optimizer, and patience. For each epoch, the methods train_hold_out() and validate_fold_or_hold_out() are called, the former for training and the latter for validation and early stopping. The validation loss from the last epoch (not the best validation loss) is returned from evaluate_hold_out.\nThe method train_hold_out() is implemented as follows:\n\n\ndef train_hold_out(net, trainloader, batch_size, loss_function, optimizer, device, show_batch_interval=10_000):\n    running_loss = 0.0\n    epoch_steps = 0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n        optimizer.step()\n        running_loss += loss.item()\n        epoch_steps += 1\n        if i % show_batch_interval == (show_batch_interval - 1):  # print every show_batch_interval mini-batches\n            print(\n                \"Batch: %5d. Batch Size: %d. Training Loss (running): %.3f\"\n                % (i + 1, int(batch_size), running_loss / epoch_steps)\n            )\n            running_loss = 0.0\n    return loss.item()\n\n\nThe method validate_fold_or_hold_out() is implemented as follows:\n\n\ndef validate_fold_or_hold_out(net, valloader, loss_function, metric, device):\n    val_loss = 0.0\n    val_steps = 0\n    metric.reset()\n    for i, data in enumerate(valloader, 0):\n        with torch.no_grad():\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = net(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            metric_value = metric(predicted, labels).to(device)\n            loss = loss_function(outputs, labels)\n            val_loss += loss.cpu().numpy()\n            val_steps += 1\n    loss = val_loss / val_steps\n    metric_value = metric.compute()\n    return metric_value, loss\n\n\n\nDescription of the \"test_hold_out\" Setting\nIt uses the loss function specfied in fun_control and the metric specified in fun_control.\n\nFirst, the method HyperTorch().fun_torch is called.\nfun_torc() calls spotPython.torch.traintest.evaluate_hold_out() similar to the \"train_hold_out\" setting with one exception: It passes an additional test data set to evaluate_hold_out() as follows:\n\n\ntest_dataset=fun_control[\"test\"]\n\nevaluate_hold_out() calls create_train_test_data_loaders instead of create_train_val_data_loaders as follows: The two data sets are used in create_train_test_data_loaders as follows:\n\ndef create_train_test_data_loaders(dataset, batch_size, shuffle, test_dataset, num_workers=0):\n    trainloader = torch.utils.data.DataLoader(\n        dataset, batch_size=int(batch_size), shuffle=shuffle, num_workers=num_workers\n    )\n    testloader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=int(batch_size), shuffle=shuffle, num_workers=num_workers\n    )\n    return trainloader, testloader\n\n\nThe following steps are identical to the \"train_hold_out\" setting. Only a different data loader is used for testing.\n\n\n\nDetailed Description of the \"train_cv\" Setting\nIt uses the loss function specfied in fun_control and the metric specified in fun_control.\n\nFirst, the method HyperTorch().fun_torch is called.\nfun_torc() calls spotPython.torch.traintest.evaluate_cv() as follows:\n\n\ndf_eval, _ = evaluate_cv(\n    model,\n    dataset=fun_control[\"train\"],\n    shuffle=self.fun_control[\"shuffle\"],\n    device=self.fun_control[\"device\"],\n    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n)\n\nNote: Only the data set fun_control[\"train\"] is used for CV. 3. In `evaluate_cv(), the following steps are performed: The optimizer is set up as follows:\n\nlr_instance = net.lr\noptimizer = optimizer_handler(optimizer_name=optimizer_instance, params=net.parameters(), lr_mult=lr_mult_instance)\n\nevaluate_cv() sets the net attributes such as epochs, batch_size, optimizer, and patience. CV is implemented as follows:\n\nkfold = KFold(n_splits=k_folds_instance, shuffle=shuffle)\nfor fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n    trainloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size_instance, sampler=train_subsampler, num_workers=num_workers\n    )\n    valloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size_instance, sampler=val_subsampler, num_workers=num_workers\n    )\n    reset_weights(net)\n    # Train fold for several epochs:\n    train_fold(\n        net,\n        trainloader,\n        epochs_instance,\n        loss_function,\n        optimizer,\n        device,\n        show_batch_interval=show_batch_interval,\n    )\n    # Validate fold: use only loss for tuning\n    metric_values[fold], loss_values[fold] = validate_fold_or_hold_out(net, valloader, loss_function, device)\ndf_eval = sum(loss_values.values()) / len(loss_values.values())\n\n\nThe method train_fold() is implemented as follows:\n\n\ndef train_fold(net, trainloader, epochs, loss_function, optimizer, device, show_batch_interval=10_000):\n    for epoch in range(epochs):\n        print(f\"Epoch: {epoch + 1}\")\n        running_loss = 0.0\n        epoch_steps = 0\n        for i, data in enumerate(trainloader, 0):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = loss_function(outputs, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n            optimizer.step()\n            # the following is for printing the statistic only\n            running_loss += loss.item()\n            epoch_steps += 1\n            if i % show_batch_interval == (show_batch_interval - 1):  # print every show_batch_interval mini-batches\n                print(\"Batch: %5d. Training Loss (running): %.3f\" % (i + 1, running_loss / epoch_steps))\n                running_loss = 0.0\n\n\nThe method validate_fold_or_hold_out() is implemented as shown above. In contrast to the hold-out setting, it is called for each of the \\(k\\) folds. The results are stored in a dictionaries metric_values and loss_values as follows:\n\n\n# Validate fold: use only loss for tuning\n    metric_values[fold], loss_values[fold] = validate_fold_or_hold_out(net, valloader, loss_function, device)\ndf_eval = sum(loss_values.values()) / len(loss_values.values())\n\nThe results are averaged over the \\(k\\) folds and returned as df_eval.\n\n\nDetailed Description of the \"test_cv\" Setting\nIt uses the loss function specfied in fun_control and the metric specified in fun_control.\n\nFirst, the method HyperTorch().fun_torch is called.\nfun_torc() calls spotPython.torch.traintest.evaluate_cv() as follows:\n\n\ndf_eval, _ = evaluate_cv(\n    model,\n    dataset=fun_control[\"test\"],\n    shuffle=self.fun_control[\"shuffle\"],\n    device=self.fun_control[\"device\"],\n    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n)\n\nNote: The data set fun_control[\"test\"] is used for CV. The rest is the same as for the \"train_cv\" setting.\n\n\nDetailed Description of the Final Model Training and Evaluation\n\nDetailed Description of the \"train_tuned Procedure\ntrain_tuned() is just a wrapper to evaluate_hold_out using the train data set. It is implemented as follows:\n\ndef train_tuned(net, train_dataset, shuffle, loss_function, metric, device=None, show_batch_interval=10_000, path=None):\n    evaluate_hold_out(\n        net=net,\n        train_dataset=train_dataset,\n        shuffle=shuffle,\n        test_dataset=None,\n        loss_function=loss_function,\n        metric=metric,\n        device=device,\n        show_batch_interval=show_batch_interval,\n        path=path,\n    )\n\nThe test_tuned() procedure is implemented as follows:\n\ndef test_tuned(net, shuffle, test_dataset=None, loss_function=None, metric=None, device=None, path=None):\n    batch_size_instance = net.batch_size\n    removed_attributes, net = get_removed_attributes_and_base_net(net)\n    if path is not None:\n        net.load_state_dict(torch.load(path))\n        net.eval()\n    try:\n        device = getDevice(device=device)\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n            if torch.cuda.device_count() &gt; 1:\n                print(\"We will use\", torch.cuda.device_count(), \"GPUs!\")\n                net = nn.DataParallel(net)\n        net.to(device)\n        valloader = torch.utils.data.DataLoader(\n            test_dataset, batch_size=int(batch_size_instance), shuffle=shuffle, num_workers=0\n        )\n        metric_value, loss = validate_fold_or_hold_out(\n            net, valloader=valloader, loss_function=loss_function, metric=metric, device=device\n        )\n        df_eval = loss\n        df_metric = metric_value\n        df_preds = np.nan\n    except Exception as err:\n        print(f\"Error in Net_Core. Call to test_tuned() failed. {err=}, {type(err)=}\")\n        df_eval = np.nan\n        df_metric = np.nan\n        df_preds = np.nan\n    add_attributes(net, removed_attributes)\n    print(f\"Final evaluation: Validation loss: {df_eval}\")\n    print(f\"Final evaluation: Validation metric: {df_metric}\")\n    print(\"----------------------------------------------\")\n    return df_eval, df_preds, df_metric"
  },
  {
    "objectID": "bart23e.html#footnotes",
    "href": "bart23e.html#footnotes",
    "title": "PyTorch Hyperparameter Tuning — A Tutorial for spotPython",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://github.com/sequential-parameter-optimization↩︎\nAlternatively, the source code can be downloaded from gitHub: https://github.com/sequential-parameter-optimization/spotPython.↩︎\nWe were not able to install Ray Tune on our system. Therefore, we used the results from the PyTorch tutorial.↩︎\nhttps://torchmetrics.readthedocs.io/en/latest/.↩︎"
  }
]