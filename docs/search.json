[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hyperparameter Tuning Cookbook",
    "section": "",
    "text": "Preface\nThe goal of hyperparameter tuning (or hyperparameter optimization) is to optimize the hyperparameters to improve the performance of the machine or deep learning model.\nspotPython (“Sequential Parameter Optimization Toolbox in Python”) is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. The related open-access book is available here: Hyperparameter Tuning for Machine and Deep Learning with R—A Practical Guide.\nscikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license. The project was started in 2007 by David Cournapeau as a Google Summer of Code project, and since then many volunteers have contributed.\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\nRiver is a Python library for online machine learning. It is designed to be used in real-world environments, where not all data is available at once, but streaming in."
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Hyperparameter Tuning Cookbook",
    "section": "Citation",
    "text": "Citation\nIf this document has been useful to you and you wish to cite it in a scientific publication, please refer to the following paper, which can be found on arXiv: https://arxiv.org/abs/2305.11930.\n@ARTICLE{bart23earxiv,\n       author = {{Bartz-Beielstein}, Thomas},\n        title = \"{PyTorch Hyperparameter Tuning -- A Tutorial for spotPython}\",\n      journal = {arXiv e-prints},\n     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Numerical Analysis, 68T07, A.1, B.8.0, G.1.6, G.4, I.2.8},\n         year = 2023,\n        month = may,\n          eid = {arXiv:2305.11930},\n        pages = {arXiv:2305.11930},\n          doi = {10.48550/arXiv.2305.11930},\narchivePrefix = {arXiv},\n       eprint = {2305.11930},\n primaryClass = {cs.LG},\n       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230511930B},\n      adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}"
  },
  {
    "objectID": "01_spot_intro.html#sec-spot",
    "href": "01_spot_intro.html#sec-spot",
    "title": "1  Introduction: Hyperparameter Tuning",
    "section": "1.1 The Hyperparameter Tuning Software SPOT",
    "text": "1.1 The Hyperparameter Tuning Software SPOT\nSurrogate model based optimization methods are common approaches in simulation and optimization. SPOT was developed because there is a great need for sound statistical analysis of simulation and optimization algorithms. SPOT includes methods for tuning based on classical regression and analysis of variance techniques. It presents tree-based models such as classification and regression trees and random forests as well as Bayesian optimization (Gaussian process models, also known as Kriging). Combinations of different meta-modeling approaches are possible. SPOT comes with a sophisticated surrogate model based optimization method, that can handle discrete and continuous inputs. Furthermore, any model implemented in scikit-learn can be used out-of-the-box as a surrogate in spotPython.\nSPOT implements key techniques such as exploratory fitness landscape analysis and sensitivity analysis. It can be used to understand the performance of various algorithms, while simultaneously giving insights into their algorithmic behavior. In addition, SPOT can be used as an optimizer and for automatic and interactive tuning. Details on SPOT and its use in practice are given by Bartz et al. (2022).\nA typical hyperparameter tuning process with spotPython consists of the following steps:\n\nLoading the data (training and test datasets), see Section 14.3.\nSpecification of the preprocessing model, see Section 14.4. This model is called prep_model (“preparation” or pre-processing). The information required for the hyperparameter tuning is stored in the dictionary fun_control. Thus, the information needed for the execution of the hyperparameter tuning is available in a readable form.\nSelection of the machine learning or deep learning model to be tuned, see Section 14.5. This is called the core_model. Once the core_model is defined, then the associated hyperparameters are stored in the fun_control dictionary. First, the hyperparameters of the core_model are initialized with the default values of the core_model. As default values we use the default values contained in the spotPython package for the algorithms of the torch package.\nModification of the default values for the hyperparameters used in core_model, see Section 14.6.0.1. This step is optional.\n\nnumeric parameters are modified by changing the bounds.\ncategorical parameters are modified by changing the categories (“levels”).\n\nSelection of target function (loss function) for the optimizer, see Section 14.7.5.\nCalling SPOT with the corresponding parameters, see Section 14.8.4. The results are stored in a dictionary and are available for further analysis.\nPresentation, visualization and interpretation of the results, see Section 14.10."
  },
  {
    "objectID": "01_spot_intro.html#spot-as-an-optimizer",
    "href": "01_spot_intro.html#spot-as-an-optimizer",
    "title": "1  Introduction: Hyperparameter Tuning",
    "section": "1.2 Spot as an Optimizer",
    "text": "1.2 Spot as an Optimizer\nThe spot loop consists of the following steps:\n\nInit: Build initial design \\(X\\)\nEvaluate initial design on real objective \\(f\\): \\(y = f(X)\\)\nBuild surrogate: \\(S = S(X,y)\\)\nOptimize on surrogate: \\(X_0 = \\text{optimize}(S)\\)\nEvaluate on real objective: \\(y_0 = f(X_0)\\)\nImpute (Infill) new points: \\(X = X \\cup X_0\\), \\(y = y \\cup y_0\\).\nGot 3.\n\nCentral Idea: Evaluation of the surrogate model S is much cheaper (or / and much faster) than running the real-world experiment \\(f\\). We start with a small example."
  },
  {
    "objectID": "01_spot_intro.html#example-spot-and-the-sphere-function",
    "href": "01_spot_intro.html#example-spot-and-the-sphere-function",
    "title": "1  Introduction: Hyperparameter Tuning",
    "section": "1.3 Example: Spot and the Sphere Function",
    "text": "1.3 Example: Spot and the Sphere Function\n\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\n\n\n1.3.1 The Objective Function: Sphere\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[f(x) = x^2\\]\n\nfun = analytical().fun_sphere\n\nWe can apply the function fun to input values and plot the result:\n\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x)\nplt.figure()\nplt.plot(x, y, \"k\")\nplt.show()\n\n\n\n\n\nspot_0 = spot.Spot(fun=fun,\n                   lower = np.array([-1]),\n                   upper = np.array([1]))\n\n\nspot_0.run()\n\n&lt;spotPython.spot.spot.Spot at 0x15f23bc10&gt;\n\n\n\nspot_0.print_results()\n\nmin y: 5.69019918867849e-10\nx0: 2.3854138401288967e-05\n\n\n[['x0', 2.3854138401288967e-05]]\n\n\n\nspot_0.plot_progress(log_y=True)\n\n\n\n\n\nspot_0.plot_model()"
  },
  {
    "objectID": "01_spot_intro.html#spot-parameters-fun_evals-init_size-and-show_models",
    "href": "01_spot_intro.html#spot-parameters-fun_evals-init_size-and-show_models",
    "title": "1  Introduction: Hyperparameter Tuning",
    "section": "1.4 Spot Parameters: fun_evals, init_size and show_models",
    "text": "1.4 Spot Parameters: fun_evals, init_size and show_models\nWe will modify three parameters:\n\nThe number of function evaluations (fun_evals)\nThe size of the initial design (init_size)\nThe parameter show_models, which visualizes the search process for 1-dim functions.\n\nThe full list of the Spot parameters is shown in the Help System and in the notebook spot_doc.ipynb.\n\nspot_1 = spot.Spot(fun=fun,\n                   lower = np.array([-1]),\n                   upper = np.array([2]),\n                   fun_evals= 10,\n                   seed=123,\n                   show_models=True,\n                   design_control={\"init_size\": 9})\nspot_1.run()\n\n\n\n\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x15f3ea7a0&gt;"
  },
  {
    "objectID": "01_spot_intro.html#print-the-results",
    "href": "01_spot_intro.html#print-the-results",
    "title": "1  Introduction: Hyperparameter Tuning",
    "section": "1.5 Print the Results",
    "text": "1.5 Print the Results\n\nspot_1.print_results()\n\nmin y: 3.6858846844978905e-07\nx0: -0.0006071148725321997\n\n\n[['x0', -0.0006071148725321997]]"
  },
  {
    "objectID": "01_spot_intro.html#show-the-progress",
    "href": "01_spot_intro.html#show-the-progress",
    "title": "1  Introduction: Hyperparameter Tuning",
    "section": "1.6 Show the Progress",
    "text": "1.6 Show the Progress\n\nspot_1.plot_progress()\n\n\n\n\n\n\n\n\nBartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, eds. 2022. Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide. Springer.\n\n\nBartz-Beielstein, Thomas. 2023. “PyTorch Hyperparameter Tuning with SPOT: Comparison with Ray Tuner and Default Hyperparameters on CIFAR10.” https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb.\n\n\nBartz-Beielstein, Thomas, Jürgen Branke, Jörn Mehnen, and Olaf Mersmann. 2014. “Evolutionary Algorithms.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 4 (3): 178–95.\n\n\nBartz-Beielstein, Thomas, Christian Lasarczyk, and Mike Preuss. 2005. “Sequential Parameter Optimization.” In Proceedings 2005 Congress on Evolutionary Computation (CEC’05), Edinburgh, Scotland, edited by B McKay et al., 773–80. Piscataway NJ: IEEE Press.\n\n\nLewis, R M, V Torczon, and M W Trosset. 2000. “Direct search methods: Then and now.” Journal of Computational and Applied Mathematics 124 (1–2): 191–207.\n\n\nLi, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2016. “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.” arXiv e-Prints, March, arXiv:1603.06560.\n\n\nMeignan, David, Sigrid Knust, Jean-Marc Frayet, Gilles Pesant, and Nicolas Gaud. 2015. “A Review and Taxonomy of Interactive Optimization Methods in Operations Research.” ACM Transactions on Interactive Intelligent Systems, September.\n\n\nPyTorch. 2023. “Hyperparameter Tuning with Ray Tune.” https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html."
  },
  {
    "objectID": "01_spot_intro.html#footnotes",
    "href": "01_spot_intro.html#footnotes",
    "title": "1  Introduction: Hyperparameter Tuning",
    "section": "",
    "text": "https://github.com/sequential-parameter-optimization↩︎"
  },
  {
    "objectID": "02_spot_multidim.html#example-spot-and-the-3-dim-sphere-function",
    "href": "02_spot_multidim.html#example-spot-and-the-3-dim-sphere-function",
    "title": "2  Multi-dimensional Functions",
    "section": "2.1 Example: Spot and the 3-dim Sphere Function",
    "text": "2.1 Example: Spot and the 3-dim Sphere Function\n\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\nimport pylab\nfrom numpy import append, ndarray, multiply, isinf, linspace, meshgrid, ravel\nfrom numpy import array\n\n\n2.1.1 The Objective Function: 3-dim Sphere\n\nThe spotPython package provides several classes of objective functions.\nWe will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[f(x) = \\sum_i^n x_i^2 \\]\nHere we will use \\(n=3\\).\n\n\nfun = analytical().fun_sphere\n\n\nThe size of the lower bound vector determines the problem dimension.\nHere we will use np.array([-1, -1, -1]), i.e., a three-dim function.\nWe will use three different theta values (one for each dimension), i.e., we set\nsurrogate_control={\"n_theta\": 3}.\n\n\nspot_3 = spot.Spot(fun=fun,\n                   lower = -1.0*np.ones(3),\n                   upper = np.ones(3),\n                   var_name=[\"Pressure\", \"Temp\", \"Lambda\"],\n                   show_progress=True,\n                   surrogate_control={\"n_theta\": 3})\n\nspot_3.run()\n\nspotPython tuning: 0.03443399805488846 [#######---] 73.33% \n\n\nspotPython tuning: 0.03134895672225177 [########--] 80.00% \n\n\nspotPython tuning: 0.0009630555620661592 [#########-] 86.67% \n\n\nspotPython tuning: 8.567364874637509e-05 [#########-] 93.33% \n\n\nspotPython tuning: 6.0300780324366926e-05 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x11103b550&gt;\n\n\n\n\n2.1.2 Results\n\nspot_3.print_results()\n\nmin y: 6.0300780324366926e-05\nPressure: 0.00514742089151478\nTemp: 0.001954003740617489\nLambda: 0.005476012040857559\n\n\n[['Pressure', 0.00514742089151478],\n ['Temp', 0.001954003740617489],\n ['Lambda', 0.005476012040857559]]\n\n\n\nspot_3.plot_progress()\n\n\n\n\n\n\n2.1.3 A Contour Plot\n\nWe can select two dimensions, say \\(i=0\\) and \\(j=1\\), and generate a contour plot as follows.\n\nNote: We have specified identical min_z and max_z values to generate comparable plots!\n\n\n\nspot_3.plot_contour(i=0, j=1, min_z=0, max_z=2.25)\n\n\n\n\n\nIn a similar manner, we can plot dimension \\(i=0\\) and \\(j=2\\):\n\n\nspot_3.plot_contour(i=0, j=2, min_z=0, max_z=2.25)\n\n\n\n\n\nThe final combination is \\(i=1\\) and \\(j=2\\):\n\n\nspot_3.plot_contour(i=1, j=2, min_z=0, max_z=2.25)\n\n\n\n\n\nThe three plots look very similar, because the fun_sphere is symmetric.\nThis can also be seen from the variable importance:\n\n\nspot_3.print_importance()\n\nPressure:  100.0\nTemp:  99.69922253450551\nLambda:  93.68147774373058\n\n\n[['Pressure', 100.0],\n ['Temp', 99.69922253450551],\n ['Lambda', 93.68147774373058]]"
  },
  {
    "objectID": "02_spot_multidim.html#conclusion",
    "href": "02_spot_multidim.html#conclusion",
    "title": "2  Multi-dimensional Functions",
    "section": "2.2 Conclusion",
    "text": "2.2 Conclusion\nBased on this quick analysis, we can conclude that all three dimensions are equally important (as expected, because the analytical function is known)."
  },
  {
    "objectID": "02_spot_multidim.html#exercises",
    "href": "02_spot_multidim.html#exercises",
    "title": "2  Multi-dimensional Functions",
    "section": "2.3 Exercises",
    "text": "2.3 Exercises\n\nImportant:\n\nResults from these exercises should be added to this document, i.e., you should submit an updated version of this notebook.\nPlease combine your results using this notebook.\nOnly one notebook from each group!\nPresentation is based on this notebook. No addtional slides are required!\nspotPython version 0.16.11 (or greater) is required\n\n\n\n2.3.1 The Three Dimensional fun_cubed\n\nThe input dimension is 3. The search range is \\(-1 \\leq x \\leq 1\\) for all dimensions.\nGenerate contour plots\nCalculate the variable importance.\nDiscuss the variable importance:\n\nAre all variables equally important?\nIf not:\n\nWhich is the most important variable?\nWhich is the least important variable?\n\n\n\n\n\n2.3.2 The Ten Dimensional fun_wing_wt\n\nThe input dimension is 10. The search range is \\(0 \\leq x \\leq 1\\) for all dimensions.\nCalculate the variable importance.\nDiscuss the variable importance:\n\nAre all variables equally important?\nIf not:\n\nWhich is the most important variable?\nWhich is the least important variable?\n\nGenerate contour plots for the three most important variables. Do they confirm your selection?\n\n\n\n\n2.3.3 The Three Dimensional fun_runge\n\nThe input dimension is 3. The search range is \\(-5 \\leq x \\leq 5\\) for all dimensions.\nGenerate contour plots\nCalculate the variable importance.\nDiscuss the variable importance:\n\nAre all variables equally important?\nIf not:\n\nWhich is the most important variable?\nWhich is the least important variable?\n\n\n\n\n\n2.3.4 The Three Dimensional fun_linear\n\nThe input dimension is 3. The search range is \\(-5 \\leq x \\leq 5\\) for all dimensions.\nGenerate contour plots\nCalculate the variable importance.\nDiscuss the variable importance:\n\nAre all variables equally important?\nIf not:\n\nWhich is the most important variable?\nWhich is the least important variable?"
  },
  {
    "objectID": "03_spot_anisotropic.html#example-isotropic-spot-surrogate-and-the-2-dim-sphere-function",
    "href": "03_spot_anisotropic.html#example-isotropic-spot-surrogate-and-the-2-dim-sphere-function",
    "title": "3  Isotropic and Anisotropic Kriging",
    "section": "3.1 Example: Isotropic Spot Surrogate and the 2-dim Sphere Function",
    "text": "3.1 Example: Isotropic Spot Surrogate and the 2-dim Sphere Function\n\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\n\n\n3.1.1 The Objective Function: 2-dim Sphere\n\nThe spotPython package provides several classes of objective functions.\nWe will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[f(x, y) = x^2 + y^2\\]\n\n\nfun = analytical().fun_sphere\nfun_control = {\"sigma\": 0,\n               \"seed\": 123}\n\n\nThe size of the lower bound vector determines the problem dimension.\nHere we will use np.array([-1, -1]), i.e., a two-dim function.\n\n\nspot_2 = spot.Spot(fun=fun,\n                   lower = np.array([-1, -1]),\n                   upper = np.array([1, 1]))\n\nspot_2.run()\n\n&lt;spotPython.spot.spot.Spot at 0x1518f6890&gt;\n\n\n\n\n3.1.2 Results\n\nspot_2.print_results()\n\nmin y: 2.020789135198605e-05\nx0: 0.0015751963468338146\nx1: 0.004210302580683181\n\n\n[['x0', 0.0015751963468338146], ['x1', 0.004210302580683181]]\n\n\n\nspot_2.plot_progress(log_y=True)"
  },
  {
    "objectID": "03_spot_anisotropic.html#example-with-anisotropic-kriging",
    "href": "03_spot_anisotropic.html#example-with-anisotropic-kriging",
    "title": "3  Isotropic and Anisotropic Kriging",
    "section": "3.2 Example With Anisotropic Kriging",
    "text": "3.2 Example With Anisotropic Kriging\n\nThe default parameter setting of spotPython’s Kriging surrogate uses the same theta value for every dimension.\nThis is referred to as “using an isotropic kernel”.\nIf different theta values are used for each dimension, then an anisotropic kernel is used\nTo enable anisotropic models in spotPython, the number of theta values should be larger than one.\nWe can use surrogate_control={\"n_theta\": 2} to enable this behavior (2 is the problem dimension).\n\n\nspot_2_anisotropic = spot.Spot(fun=fun,\n                   lower = np.array([-1, -1]),\n                   upper = np.array([1, 1]),\n                   surrogate_control={\"n_theta\": 2})\nspot_2_anisotropic.run()\n\n&lt;spotPython.spot.spot.Spot at 0x153d2cdf0&gt;\n\n\n\n3.2.1 Taking a Look at the theta Values\n\nWe can check, whether one or several theta values were used.\nThe theta values from the surrogate can be printed as follows:\n\n\nspot_2_anisotropic.surrogate.theta\n\narray([0.24805857, 0.35713614])\n\n\n\nSince the surrogate from the isotropic setting was stored as spot_2, we can also take a look at the theta value from this model:\n\n\nspot_2.surrogate.theta\n\narray([0.26287446])\n\n\n\nNext, the search progress of the optimization with the anisotropic model can be visualized:\n\n\nspot_2_anisotropic.plot_progress(log_y=True)\n\n\n\n\n\nspot_2_anisotropic.print_results()\n\nmin y: 3.898914658670152e-06\nx0: -0.0008031859004420657\nx1: -0.0018038312193775835\n\n\n[['x0', -0.0008031859004420657], ['x1', -0.0018038312193775835]]\n\n\n\nspot_2_anisotropic.surrogate.plot()"
  },
  {
    "objectID": "03_spot_anisotropic.html#exercises",
    "href": "03_spot_anisotropic.html#exercises",
    "title": "3  Isotropic and Anisotropic Kriging",
    "section": "3.3 Exercises",
    "text": "3.3 Exercises\n\n3.3.1 fun_branin\n\nDescribe the function.\n\nThe input dimension is 2. The search range is \\(-5 \\leq x_1 \\leq 10\\) and \\(0 \\leq x_2 \\leq 15\\).\n\nCompare the results from spotPython run a) with isotropic and b) anisotropic surrogate models.\nModify the termination criterion: instead of the number of evaluations (which is specified via fun_evals), the time should be used as the termination criterion. This can be done as follows (max_time=1 specifies a run time of one minute):\n\n\nfun_evals=inf,\nmax_time=1,\n\n\n\n3.3.2 fun_sin_cos\n\nDescribe the function.\n\nThe input dimension is 2. The search range is \\(-2\\pi \\leq x_1 \\leq 2\\pi\\) and \\(-2\\pi \\leq x_2 \\leq 2\\pi\\).\n\nCompare the results from spotPython run a) with isotropic and b) anisotropic surrogate models.\nModify the termination criterion (max_time instead of fun_evals) as described for fun_branin.\n\n\n\n3.3.3 fun_runge\n\nDescribe the function.\n\nThe input dimension is 2. The search range is \\(-5 \\leq x_1 \\leq 5\\) and \\(-5 \\leq x_2 \\leq 5\\).\n\nCompare the results from spotPython run a) with isotropic and b) anisotropic surrogate models.\nModify the termination criterion (max_time instead of fun_evals) as described for fun_branin.\n\n\n\n3.3.4 fun_wingwt\n\nDescribe the function.\n\nThe input dimension is 10. The search ranges are between 0 and 1 (values are mapped internally to their natural bounds).\n\nCompare the results from spotPython run a) with isotropic and b) anisotropic surrogate models.\nModify the termination criterion (max_time instead of fun_evals) as described for fun_branin."
  },
  {
    "objectID": "04_spot_sklearn_surrogate.html#example-branin-function-with-spotpythons-internal-kriging-surrogate",
    "href": "04_spot_sklearn_surrogate.html#example-branin-function-with-spotpythons-internal-kriging-surrogate",
    "title": "4  Using sklearn Surrogates in spotPython",
    "section": "4.1 Example: Branin Function with spotPython’s Internal Kriging Surrogate",
    "text": "4.1 Example: Branin Function with spotPython’s Internal Kriging Surrogate\n\n4.1.1 The Objective Function Branin\n\nThe spotPython package provides several classes of objective functions.\nWe will use an analytical objective function, i.e., a function that can be described by a (closed) formula.\nHere we will use the Branin function:\n  y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s,\n  where values of a, b, c, r, s and t are: a = 1, b = 5.1 / (4*pi**2),\n  c = 5 / pi, r = 6, s = 10 and t = 1 / (8*pi).\nIt has three global minima:\n  f(x) = 0.397887 at (-pi, 12.275), (pi, 2.275), and (9.42478, 2.475).\n\n\nfrom spotPython.fun.objectivefunctions import analytical\nlower = np.array([-5,-0])\nupper = np.array([10,15])\n\n\nfun = analytical().fun_branin\n\n\n\n4.1.2 Running the surrogate model based optimizer Spot:\n\nspot_2 = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = 20,\n                   max_time = inf,\n                   seed=123,\n                   design_control={\"init_size\": 10})\n\n\nspot_2.run()\n\n&lt;spotPython.spot.spot.Spot at 0x159816560&gt;\n\n\n\n\n4.1.3 Print the Results\n\nspot_2.print_results()\n\nmin y: 0.3982296851228586\nx0: 3.135563584477711\nx1: 2.2926607128616965\n\n\n[['x0', 3.135563584477711], ['x1', 2.2926607128616965]]\n\n\n\n\n4.1.4 Show the Progress and the Surrogate\n\nspot_2.plot_progress(log_y=True)\n\n\n\n\n\nspot_2.surrogate.plot()"
  },
  {
    "objectID": "04_spot_sklearn_surrogate.html#example-using-surrogates-from-scikit-learn",
    "href": "04_spot_sklearn_surrogate.html#example-using-surrogates-from-scikit-learn",
    "title": "4  Using sklearn Surrogates in spotPython",
    "section": "4.2 Example: Using Surrogates From scikit-learn",
    "text": "4.2 Example: Using Surrogates From scikit-learn\n\nDefault is the spotPython (i.e., the internal) kriging surrogate.\nIt can be called explicitely and passed to Spot.\n\n\nfrom spotPython.build.kriging import Kriging\nS_0 = Kriging(name='kriging', seed=123)\n\n\nAlternatively, models from scikit-learn can be selected, e.g., Gaussian Process, RBFs, Regression Trees, etc.\n\n\n# Needed for the sklearn surrogates:\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn import tree\nimport pandas as pd\n\n\nHere are some additional models that might be useful later:\n\n\nS_Tree = DecisionTreeRegressor(random_state=0)\nS_LM = linear_model.LinearRegression()\nS_Ridge = linear_model.Ridge()\nS_RF = RandomForestRegressor(max_depth=2, random_state=0)\n\n\n4.2.1 GaussianProcessRegressor as a Surrogate\n\nTo use a Gaussian Process model from sklearn, that is similar to spotPython’s Kriging, we can proceed as follows:\n\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\nS_GP = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n\n\nThe scikit-learn GP model S_GP is selected for Spot as follows:\nsurrogate = S_GP\nWe can check the kind of surogate model with the command isinstance:\n\n\nisinstance(S_GP, GaussianProcessRegressor)\n\nTrue\n\n\n\nisinstance(S_0, Kriging)\n\nTrue\n\n\n\nSimilar to the Spot run with the internal Kriging model, we can call the run with the scikit-learn surrogate:\n\n\nfun = analytical(seed=123).fun_branin\nspot_2_GP = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = 20,\n                   seed=123,\n                   design_control={\"init_size\": 10},\n                   surrogate = S_GP)\nspot_2_GP.run()\n\n&lt;spotPython.spot.spot.Spot at 0x15d6ebc70&gt;\n\n\n\nspot_2_GP.plot_progress()\n\n\n\n\n\nspot_2_GP.print_results()\n\nmin y: 0.3981999166631418\nx0: 3.1493455599412683\nx1: 2.273858589855384\n\n\n[['x0', 3.1493455599412683], ['x1', 2.273858589855384]]"
  },
  {
    "objectID": "04_spot_sklearn_surrogate.html#example-one-dimensional-sphere-function-with-spotpythons-kriging",
    "href": "04_spot_sklearn_surrogate.html#example-one-dimensional-sphere-function-with-spotpythons-kriging",
    "title": "4  Using sklearn Surrogates in spotPython",
    "section": "4.3 Example: One-dimensional Sphere Function With spotPython’s Kriging",
    "text": "4.3 Example: One-dimensional Sphere Function With spotPython’s Kriging\n\nIn this example, we will use an one-dimensional function, which allows us to visualize the optimization process.\n\nshow_models= True is added to the argument list.\n\n\n\nfrom spotPython.fun.objectivefunctions import analytical\nlower = np.array([-1])\nupper = np.array([1])\nfun = analytical(seed=123).fun_sphere\n\n\nspot_1 = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = 10,\n                   max_time = inf,\n                   seed=123,\n                   show_models= True,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   design_control={\"init_size\": 3},)\nspot_1.run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x15d947f40&gt;\n\n\n\n4.3.1 Results\n\nspot_1.print_results()\n\nmin y: 4.487739040061708e-08\nx0: -0.0002118428436379598\n\n\n[['x0', -0.0002118428436379598]]\n\n\n\nspot_1.plot_progress(log_y=True)\n\n\n\n\n\nThe method plot_model plots the final surrogate:\n\n\nspot_1.plot_model()"
  },
  {
    "objectID": "04_spot_sklearn_surrogate.html#example-sklearn-model-gaussianprocess",
    "href": "04_spot_sklearn_surrogate.html#example-sklearn-model-gaussianprocess",
    "title": "4  Using sklearn Surrogates in spotPython",
    "section": "4.4 Example: Sklearn Model GaussianProcess",
    "text": "4.4 Example: Sklearn Model GaussianProcess\n\nThis example visualizes the search process on the GaussianProcessRegression surrogate from sklearn.\nTherefore surrogate = S_GP is added to the argument list.\n\n\nfun = analytical(seed=123).fun_sphere\nspot_1_GP = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = 10,\n                   max_time = inf,\n                   seed=123,\n                   show_models= True,\n                   design_control={\"init_size\": 3},\n                   surrogate = S_GP)\nspot_1_GP.run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x1598156c0&gt;\n\n\n\nspot_1_GP.print_results()\n\nmin y: 3.3006830558021305e-10\nx0: 1.8167782076528027e-05\n\n\n[['x0', 1.8167782076528027e-05]]\n\n\n\nspot_1_GP.plot_progress(log_y=True)\n\n\n\n\n\nspot_1_GP.plot_model()"
  },
  {
    "objectID": "04_spot_sklearn_surrogate.html#exercises",
    "href": "04_spot_sklearn_surrogate.html#exercises",
    "title": "4  Using sklearn Surrogates in spotPython",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\n\n4.5.1 DecisionTreeRegressor\n\nDescribe the surrogate model.\nUse the surrogate as the model for optimization.\n\n\n\n4.5.2 RandomForestRegressor\n\nDescribe the surrogate model.\nUse the surrogate as the model for optimization.\n\n\n\n4.5.3 linear_model.LinearRegression\n\nDescribe the surrogate model.\nUse the surrogate as the model for optimization.\n\n\n\n4.5.4 linear_model.Ridge\n\nDescribe the surrogate model.\nUse the surrogate as the model for optimization."
  },
  {
    "objectID": "04_spot_sklearn_surrogate.html#exercise-2",
    "href": "04_spot_sklearn_surrogate.html#exercise-2",
    "title": "4  Using sklearn Surrogates in spotPython",
    "section": "4.6 Exercise 2",
    "text": "4.6 Exercise 2\n\nCompare the performance of the five different surrogates on both objective functions:\n\nspotPython’s internal Kriging\nDecisionTreeRegressor\nRandomForestRegressor\nlinear_model.LinearRegression\nlinear_model.Ridge"
  },
  {
    "objectID": "05_spot_sklearn_optimization.html#the-objective-function-branin",
    "href": "05_spot_sklearn_optimization.html#the-objective-function-branin",
    "title": "5  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "5.1 The Objective Function Branin",
    "text": "5.1 The Objective Function Branin\n\nThe spotPython package provides several classes of objective functions.\nWe will use an analytical objective function, i.e., a function that can be described by a (closed) formula.\nHere we will use the Branin function. The 2-dim Branin function is\n\\[y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * cos(x1) + s,\\] where values of a, b, c, r, s and t are: \\(a = 1, b = 5.1 / (4*pi**2), c = 5 / pi, r = 6, s = 10\\) and \\(t = 1 / (8*pi)\\).\nIt has three global minima:\n\\(f(x) = 0.397887\\) at \\((-\\pi, 12.275)\\), \\((\\pi, 2.275)\\), and \\((9.42478, 2.475)\\).\nInput Domain: This function is usually evaluated on the square x1 in [-5, 10] x x2 in [0, 15].\n\n\nfrom spotPython.fun.objectivefunctions import analytical\nlower = np.array([-5,-0])\nupper = np.array([10,15])\n\n\nfun = analytical(seed=123).fun_branin"
  },
  {
    "objectID": "05_spot_sklearn_optimization.html#the-optimizer",
    "href": "05_spot_sklearn_optimization.html#the-optimizer",
    "title": "5  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "5.2 The Optimizer",
    "text": "5.2 The Optimizer\n\nDifferential Evalution from the scikit.optimize package, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution is the default optimizer for the search on the surrogate.\nOther optimiers that are available in spotPython:\n\ndual_annealing\ndirect\nshgo\nbasinhopping, see https://docs.scipy.org/doc/scipy/reference/optimize.html#global-optimization.\n\nThese can be selected as follows:\nsurrogate_control = \"model_optimizer\": differential_evolution\nWe will use differential_evolution.\nThe optimizer can use 1000 evaluations. This value will be passed to the differential_evolution method, which has the argument maxiter (int). It defines the maximum number of generations over which the entire differential evolution population is evolved, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution\n\n\nspot_de = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = 20,\n                   max_time = inf,\n                   seed=125,\n                   noise=False,\n                   show_models= False,\n                   design_control={\"init_size\": 10},\n                   surrogate_control={\"n_theta\": 2,\n                                      \"model_optimizer\": differential_evolution,\n                                      \"model_fun_evals\": 1000,\n                                      })\nspot_de.run()\n\n&lt;spotPython.spot.spot.Spot at 0x10e2a75e0&gt;"
  },
  {
    "objectID": "05_spot_sklearn_optimization.html#print-the-results",
    "href": "05_spot_sklearn_optimization.html#print-the-results",
    "title": "5  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "5.3 Print the Results",
    "text": "5.3 Print the Results\n\nspot_de.print_results()\n\nmin y: 0.39955172035863207\nx0: -3.1571400084867083\nx1: 12.289948119375225\n\n\n[['x0', -3.1571400084867083], ['x1', 12.289948119375225]]"
  },
  {
    "objectID": "05_spot_sklearn_optimization.html#show-the-progress",
    "href": "05_spot_sklearn_optimization.html#show-the-progress",
    "title": "5  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "5.4 Show the Progress",
    "text": "5.4 Show the Progress\n\nspot_de.plot_progress(log_y=True)\n\n\n\n\n\nspot_de.surrogate.plot()"
  },
  {
    "objectID": "05_spot_sklearn_optimization.html#exercises",
    "href": "05_spot_sklearn_optimization.html#exercises",
    "title": "5  Sequential Parameter Optimization: Using scipy Optimizers",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\n5.5.1 dual_annealing\n\nDescribe the optimization algorithm\nUse the algorithm as an optimizer on the surrogate\n\n\n\n5.5.2 direct\n\nDescribe the optimization algorithm\nUse the algorithm as an optimizer on the surrogate\n\n\n\n5.5.3 shgo\n\nDescribe the optimization algorithm\nUse the algorithm as an optimizer on the surrogate\n\n\n\n5.5.4 basinhopping\n\nDescribe the optimization algorithm\nUse the algorithm as an optimizer on the surrogate\n\n\n\n5.5.5 Performance Comparison\nCompare the performance and run time of the 5 different optimizers:\n* `differential_evolution`\n* `dual_annealing`\n*  `direct`\n* `shgo`\n* `basinhopping`.\nThe Branin function has three global minima:\n\n\\(f(x) = 0.397887\\) at\n\n\\((-\\pi, 12.275)\\),\n\\((\\pi, 2.275)\\), and\n\\((9.42478, 2.475)\\).\n\n\nWhich optima are found by the optimizers? Does the seed change this behavior?"
  },
  {
    "objectID": "06_spot_gaussian.html#gaussian-processes-regression-basic-introductory-scikit-learn-example",
    "href": "06_spot_gaussian.html#gaussian-processes-regression-basic-introductory-scikit-learn-example",
    "title": "6  Sequential Parameter Optimization: Gaussian Process Models",
    "section": "6.1 Gaussian Processes Regression: Basic Introductory scikit-learn Example",
    "text": "6.1 Gaussian Processes Regression: Basic Introductory scikit-learn Example\n\nThis is the example from scikit-learn: https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html\nAfter fitting our model, we see that the hyperparameters of the kernel have been optimized.\nNow, we will use our kernel to compute the mean prediction of the full dataset and plot the 95% confidence interval.\n\n\n6.1.1 Train and Test Data\n\nX = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\ny = np.squeeze(X * np.sin(X))\nrng = np.random.RandomState(1)\ntraining_indices = rng.choice(np.arange(y.size), size=6, replace=False)\nX_train, y_train = X[training_indices], y[training_indices]\n\n\n\n6.1.2 Building the Surrogate With Sklearn\n\nThe model building with sklearn consisits of three steps:\n\nInstantiating the model, then\nfitting the model (using fit), and\nmaking predictions (using predict)\n\n\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\ngaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\ngaussian_process.fit(X_train, y_train)\nmean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n\n\n\n6.1.3 Plotting the SklearnModel\n\nplt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\nplt.fill_between(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"sk-learn Version: Gaussian process regression on noise-free dataset\")\n\n\n\n\n\n\n6.1.4 The spotPython Version\n\nThe spotPython version is very similar:\n\nInstantiating the model, then\nfitting the model and\nmaking predictions (using predict).\n\n\n\nS = Kriging(name='kriging',  seed=123, log_level=50, cod_type=\"norm\")\nS.fit(X_train, y_train)\nS_mean_prediction, S_std_prediction, S_ei = S.predict(X, return_val=\"all\")\n\n\nplt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, S_mean_prediction, label=\"Mean prediction\")\nplt.fill_between(\n    X.ravel(),\n    S_mean_prediction - 1.96 * S_std_prediction,\n    S_mean_prediction + 1.96 * S_std_prediction,\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"spotPython Version: Gaussian process regression on noise-free dataset\")\n\n\n\n\n\n\n6.1.5 Visualizing the Differences Between the spotPython and the sklearn Model Fits\n\nplt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, S_mean_prediction, label=\"spotPython Mean prediction\")\nplt.plot(X, mean_prediction, label=\"Sklearn Mean Prediction\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Comparing Mean Predictions\")"
  },
  {
    "objectID": "06_spot_gaussian.html#exercises",
    "href": "06_spot_gaussian.html#exercises",
    "title": "6  Sequential Parameter Optimization: Gaussian Process Models",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\n\n6.2.1 Schonlau Example Function\n\nThe Schonlau Example Function is based on sample points only (there is no analytical function description available):\n\n\nX = np.linspace(start=0, stop=13, num=1_000).reshape(-1, 1)\nX_train = np.array([1., 2., 3., 4., 12.]).reshape(-1,1)\ny_train = np.array([0., -1.75, -2, -0.5, 5.])\n\n\nDescribe the function.\nCompare the two models that were build using the spotPython and the sklearn surrogate.\nNote: Since there is no analytical function available, you might be interested in adding some points and describe the effects.\n\n\n\n6.2.2 Forrester Example Function\n\nThe Forrester Example Function is defined as follows:\nf(x) = (6x- 2)^2 sin(12x-4) for x in [0,1].\nData points are generated as follows:\n\n\nX = np.linspace(start=-0.5, stop=1.5, num=1_000).reshape(-1, 1)\nX_train = np.array([0.0, 0.175, 0.225, 0.3, 0.35, 0.375, 0.5,1]).reshape(-1,1)\nfun = analytical().fun_forrester\nfun_control = {\"sigma\": 0.1,\n               \"seed\": 123}\ny = fun(X, fun_control=fun_control)\ny_train = fun(X_train, fun_control=fun_control)\n\n\nDescribe the function.\nCompare the two models that were build using the spotPython and the sklearn surrogate.\nNote: Modify the noise level (\"sigma\"), e.g., use a value of 0.2, and compare the two models.\n\n\nfun_control = {\"sigma\": 0.2}\n\n\n\n6.2.3 fun_runge Function (1-dim)\n\nThe Runge function is defined as follows:\nf(x) = 1/ (1 + sum(x_i))^2\nData points are generated as follows:\n\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_runge\nfun_control = {\"sigma\": 0.025,\n               \"seed\": 123}\nX_train = gen.scipy_lhd(10, lower=lower, upper = upper).reshape(-1,1)\ny_train = fun(X, fun_control=fun_control)\nX = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\ny = fun(X, fun_control=fun_control)\n\n\nDescribe the function.\nCompare the two models that were build using the spotPython and the sklearn surrogate.\nNote: Modify the noise level (\"sigma\"), e.g., use a value of 0.05, and compare the two models.\n\n\nfun_control = {\"sigma\": 0.5}\n\n\n\n6.2.4 fun_cubed (1-dim)\n\nThe Cubed function is defined as follows:\nnp.sum(X[i]** 3)\nData points are generated as follows:\n\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_cubed\nfun_control = {\"sigma\": 0.025,\n               \"seed\": 123}\nX_train = gen.scipy_lhd(10, lower=lower, upper = upper).reshape(-1,1)\ny_train = fun(X, fun_control=fun_control)\nX = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\ny = fun(X, fun_control=fun_control)\n\n\nDescribe the function.\nCompare the two models that were build using the spotPython and the sklearn surrogate.\nNote: Modify the noise level (\"sigma\"), e.g., use a value of 0.05, and compare the two models.\n\n\nfun_control = {\"sigma\": 0.05}\n\n\n\n6.2.5 The Effect of Noise\nHow does the behavior of the spotPython fit changes when the argument noise is set to True, i.e.,\nS = Kriging(name='kriging',  seed=123, n_theta=1, noise=True)\nis used?"
  },
  {
    "objectID": "07_spot_ei.html#example-spot-and-the-1-dim-sphere-function",
    "href": "07_spot_ei.html#example-spot-and-the-1-dim-sphere-function",
    "title": "7  Expected Improvement",
    "section": "7.1 Example: Spot and the 1-dim Sphere Function",
    "text": "7.1 Example: Spot and the 1-dim Sphere Function\n\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\n\n\n7.1.1 The Objective Function: 1-dim Sphere\n\nThe spotPython package provides several classes of objective functions.\nWe will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[f(x) = x^2 \\]\n\n\nfun = analytical().fun_sphere\n\n\nfun = analytical().fun_sphere\nfun_control = {\"sigma\": 0,\n               \"seed\": 123}\n\n\nThe size of the lower bound vector determines the problem dimension.\nHere we will use np.array([-1]), i.e., a one-dim function.\n\n\nspot_1 = spot.Spot(fun=fun,\n                   lower = np.array([-1]),\n                   upper = np.array([1]))\n\nspot_1.run()\n\n&lt;spotPython.spot.spot.Spot at 0x15335a950&gt;\n\n\n\n\n7.1.2 Results\n\nspot_1.print_results()\n\nmin y: 5.69019918867849e-10\nx0: 2.3854138401288967e-05\n\n\n[['x0', 2.3854138401288967e-05]]\n\n\n\nspot_1.plot_progress(log_y=True)"
  },
  {
    "objectID": "07_spot_ei.html#same-but-with-ei-as-infill_criterion",
    "href": "07_spot_ei.html#same-but-with-ei-as-infill_criterion",
    "title": "7  Expected Improvement",
    "section": "7.2 Same, but with EI as infill_criterion",
    "text": "7.2 Same, but with EI as infill_criterion\n\nspot_1_ei = spot.Spot(fun=fun,\n                   lower = np.array([-1]),\n                   upper = np.array([1]),\n                   infill_criterion = \"ei\")\nspot_1_ei.run()\n\n&lt;spotPython.spot.spot.Spot at 0x15586d990&gt;\n\n\n\nspot_1_ei.plot_progress(log_y=True)\n\n\n\n\n\nspot_1_ei.print_results()\n\nmin y: 1.0703048868228972e-11\nx0: -3.271551446673118e-06\n\n\n[['x0', -3.271551446673118e-06]]"
  },
  {
    "objectID": "07_spot_ei.html#non-isotropic-kriging",
    "href": "07_spot_ei.html#non-isotropic-kriging",
    "title": "7  Expected Improvement",
    "section": "7.3 Non-isotropic Kriging",
    "text": "7.3 Non-isotropic Kriging\n\nspot_2_ei_noniso = spot.Spot(fun=fun,\n                   lower = np.array([-1, -1]),\n                   upper = np.array([1, 1]),\n                   fun_evals = 20,\n                   fun_repeats = 1,\n                   max_time = inf,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type=[\"num\"],\n                   infill_criterion = \"ei\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models=True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": 10,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": False,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": 2,\n                                      \"model_optimizer\": differential_evolution,\n                                      \"model_fun_evals\": 1000,\n                                      })\nspot_2_ei_noniso.run()\n\n&lt;spotPython.spot.spot.Spot at 0x15372bc70&gt;\n\n\n\nspot_2_ei_noniso.plot_progress(log_y=True)\n\n\n\n\n\nspot_2_ei_noniso.print_results()\n\nmin y: 3.546757229720085e-07\nx0: 0.0005304763214324646\nx1: 0.0002706854177296757\n\n\n[['x0', 0.0005304763214324646], ['x1', 0.0002706854177296757]]\n\n\n\nspot_2_ei_noniso.surrogate.plot()"
  },
  {
    "objectID": "07_spot_ei.html#using-sklearn-surrogates",
    "href": "07_spot_ei.html#using-sklearn-surrogates",
    "title": "7  Expected Improvement",
    "section": "7.4 Using sklearn Surrogates",
    "text": "7.4 Using sklearn Surrogates\n\n7.4.1 The spot Loop\nThe spot loop consists of the following steps:\n\nInit: Build initial design \\(X\\)\nEvaluate initial design on real objective \\(f\\): \\(y = f(X)\\)\nBuild surrogate: \\(S = S(X,y)\\)\nOptimize on surrogate: \\(X_0 = \\text{optimize}(S)\\)\nEvaluate on real objective: \\(y_0 = f(X_0)\\)\nImpute (Infill) new points: \\(X = X \\cup X_0\\), \\(y = y \\cup y_0\\).\nGot 3.\n\nThe spot loop is implemented in R as follows:\n\n\n\nVisual representation of the model based search with SPOT. Taken from: Bartz-Beielstein, T., and Zaefferer, M. Hyperparameter tuning approaches. In Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide, E. Bartz, T. Bartz-Beielstein, M. Zaefferer, and O. Mersmann, Eds. Springer, 2022, ch. 4, pp. 67–114.\n\n\n\n\n7.4.2 spot: The Initial Model\n\n7.4.2.1 Example: Modifying the initial design size\nThis is the “Example: Modifying the initial design size” from Chapter 4.5.1 in [bart21i].\n\nspot_ei = spot.Spot(fun=fun,\n               lower = np.array([-1,-1]),\n               upper= np.array([1,1]),\n               design_control={\"init_size\": 5})\nspot_ei.run()\n\n&lt;spotPython.spot.spot.Spot at 0x155c32ef0&gt;\n\n\n\nspot_ei.plot_progress()\n\n\n\n\n\nnp.min(spot_1.y), np.min(spot_ei.y)\n\n(5.69019918867849e-10, 1.9041417425886357e-05)\n\n\n\n\n\n7.4.3 Init: Build Initial Design\n\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nfrom spotPython.fun.objectivefunctions import analytical\ngen = spacefilling(2)\nrng = np.random.RandomState(1)\nlower = np.array([-5,-0])\nupper = np.array([10,15])\nfun = analytical().fun_branin\nfun_control = {\"sigma\": 0,\n               \"seed\": 123}\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\n\n[[ 8.97647221 13.41926847]\n [ 0.66946019  1.22344228]\n [ 5.23614115 13.78185824]\n [ 5.6149825  11.5851384 ]\n [-1.72963184  1.66516096]\n [-4.26945568  7.1325531 ]\n [ 1.26363761 10.17935555]\n [ 2.88779942  8.05508969]\n [-3.39111089  4.15213772]\n [ 7.30131231  5.22275244]]\n[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975\n  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]\n\n\n\nS = Kriging(name='kriging',  seed=123)\nS.fit(X, y)\nS.plot()\n\n\n\n\n\ngen = spacefilling(2, seed=123)\nX0 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=345)\nX1 = gen.scipy_lhd(3)\nX2 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=123)\nX3 = gen.scipy_lhd(3)\nX0, X1, X2, X3\n\n(array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]),\n array([[0.78373509, 0.86811887],\n        [0.06692621, 0.6058029 ],\n        [0.41374778, 0.00525456]]),\n array([[0.121357  , 0.69043832],\n        [0.41906219, 0.32838498],\n        [0.86742658, 0.52910374]]),\n array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]))\n\n\n\n\n7.4.4 Evaluate\n\n\n7.4.5 Build Surrogate\n\n\n7.4.6 A Simple Predictor\nThe code below shows how to use a simple model for prediction.\n\nAssume that only two (very costly) measurements are available:\n\nf(0) = 0.5\nf(2) = 2.5\n\nWe are interested in the value at \\(x_0 = 1\\), i.e., \\(f(x_0 = 1)\\), but cannot run an additional, third experiment.\n\n\nfrom sklearn import linear_model\nX = np.array([[0], [2]])\ny = np.array([0.5, 2.5])\nS_lm = linear_model.LinearRegression()\nS_lm = S_lm.fit(X, y)\nX0 = np.array([[1]])\ny0 = S_lm.predict(X0)\nprint(y0)\n\n[1.5]\n\n\n\nCentral Idea:\n\nEvaluation of the surrogate model S_lm is much cheaper (or / and much faster) than running the real-world experiment \\(f\\)."
  },
  {
    "objectID": "07_spot_ei.html#gaussian-processes-regression-basic-introductory-example",
    "href": "07_spot_ei.html#gaussian-processes-regression-basic-introductory-example",
    "title": "7  Expected Improvement",
    "section": "7.5 Gaussian Processes regression: basic introductory example",
    "text": "7.5 Gaussian Processes regression: basic introductory example\nThis example was taken from scikit-learn. After fitting our model, we see that the hyperparameters of the kernel have been optimized. Now, we will use our kernel to compute the mean prediction of the full dataset and plot the 95% confidence interval.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math as m\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\nX = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\ny = np.squeeze(X * np.sin(X))\nrng = np.random.RandomState(1)\ntraining_indices = rng.choice(np.arange(y.size), size=6, replace=False)\nX_train, y_train = X[training_indices], y[training_indices]\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\ngaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\ngaussian_process.fit(X_train, y_train)\ngaussian_process.kernel_\n\nmean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n\nplt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\nplt.fill_between(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"sk-learn Version: Gaussian process regression on noise-free dataset\")\n\n\n\n\n\nfrom spotPython.build.kriging import Kriging\nimport numpy as np\nimport matplotlib.pyplot as plt\nrng = np.random.RandomState(1)\nX = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\ny = np.squeeze(X * np.sin(X))\ntraining_indices = rng.choice(np.arange(y.size), size=6, replace=False)\nX_train, y_train = X[training_indices], y[training_indices]\n\n\nS = Kriging(name='kriging',  seed=123, log_level=50, cod_type=\"norm\")\nS.fit(X_train, y_train)\n\nmean_prediction, std_prediction, ei = S.predict(X, return_val=\"all\")\n\nstd_prediction\n\nplt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\nplt.fill_between(\n    X.ravel(),\n    mean_prediction - 1.96 * std_prediction,\n    mean_prediction + 1.96 * std_prediction,\n    alpha=0.5,\n    label=r\"95% confidence interval\",\n)\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"spotPython Version: Gaussian process regression on noise-free dataset\")"
  },
  {
    "objectID": "07_spot_ei.html#the-surrogate-using-scikit-learn-models",
    "href": "07_spot_ei.html#the-surrogate-using-scikit-learn-models",
    "title": "7  Expected Improvement",
    "section": "7.6 The Surrogate: Using scikit-learn models",
    "text": "7.6 The Surrogate: Using scikit-learn models\nDefault is the internal kriging surrogate.\n\nS_0 = Kriging(name='kriging', seed=123)\n\nModels from scikit-learn can be selected, e.g., Gaussian Process:\n\n# Needed for the sklearn surrogates:\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn import tree\nimport pandas as pd\n\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\nS_GP = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n\n\nand many more:\n\n\nS_Tree = DecisionTreeRegressor(random_state=0)\nS_LM = linear_model.LinearRegression()\nS_Ridge = linear_model.Ridge()\nS_RF = RandomForestRegressor(max_depth=2, random_state=0) \n\n\nThe scikit-learn GP model S_GP is selected.\n\n\nS = S_GP\n\n\nisinstance(S, GaussianProcessRegressor)\n\nTrue\n\n\n\nfrom spotPython.fun.objectivefunctions import analytical\nfun = analytical().fun_branin\nlower = np.array([-5,-0])\nupper = np.array([10,15])\ndesign_control={\"init_size\": 5}\nsurrogate_control={\n            \"infill_criterion\": None,\n            \"n_points\": 1,\n        }\nspot_GP = spot.Spot(fun=fun, lower = lower, upper= upper, surrogate=S, \n                    fun_evals = 15, noise = False, log_level = 50,\n                    design_control=design_control,\n                    surrogate_control=surrogate_control)\n\nspot_GP.run()\n\n&lt;spotPython.spot.spot.Spot at 0x15650ded0&gt;\n\n\n\nspot_GP.y\n\narray([ 69.32459936, 152.38491454, 107.92560483,  24.51465459,\n        76.73500031,  86.3042622 ,  11.00310861,  16.11744447,\n         7.28147024,  21.82317624,  10.96088904,   2.95206547,\n         3.02912556,   2.10497775,   1.94317213])\n\n\n\nspot_GP.plot_progress()\n\n\n\n\n\nspot_GP.print_results()\n\nmin y: 1.943172131852375\nx0: 10.0\nx1: 2.9973468845009013\n\n\n[['x0', 10.0], ['x1', 2.9973468845009013]]"
  },
  {
    "objectID": "07_spot_ei.html#additional-examples",
    "href": "07_spot_ei.html#additional-examples",
    "title": "7  Expected Improvement",
    "section": "7.7 Additional Examples",
    "text": "7.7 Additional Examples\n\n# Needed for the sklearn surrogates:\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn import tree\nimport pandas as pd\n\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\nS_GP = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n\n\nfrom spotPython.build.kriging import Kriging\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\n\nS_K = Kriging(name='kriging',\n              seed=123,\n              log_level=50,\n              infill_criterion = \"y\",\n              n_theta=1,\n              noise=False,\n              cod_type=\"norm\")\nfun = analytical().fun_sphere\nlower = np.array([-1,-1])\nupper = np.array([1,1])\n\ndesign_control={\"init_size\": 10}\nsurrogate_control={\n            \"n_points\": 1,\n        }\nspot_S_K = spot.Spot(fun=fun,\n                     lower = lower,\n                     upper= upper,\n                     surrogate=S_K,\n                     fun_evals = 25,\n                     noise = False,\n                     log_level = 50,\n                     design_control=design_control,\n                     surrogate_control=surrogate_control)\n\nspot_S_K.run()\n\n&lt;spotPython.spot.spot.Spot at 0x15700e7a0&gt;\n\n\n\nspot_S_K.plot_progress(log_y=True)\n\n\n\n\n\nspot_S_K.surrogate.plot()\n\n\n\n\n\nspot_S_K.print_results()\n\nmin y: 1.7353966126200755e-06\nx0: -0.0013044145475565347\nx1: 0.00018411762746395718\n\n\n[['x0', -0.0013044145475565347], ['x1', 0.00018411762746395718]]\n\n\n\n7.7.1 Optimize on Surrogate\n\n\n7.7.2 Evaluate on Real Objective\n\n\n7.7.3 Impute / Infill new Points"
  },
  {
    "objectID": "07_spot_ei.html#tests",
    "href": "07_spot_ei.html#tests",
    "title": "7  Expected Improvement",
    "section": "7.8 Tests",
    "text": "7.8 Tests\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom spotPython.fun.objectivefunctions import analytical\n\nfun_sphere = analytical().fun_sphere\nspot_1 = spot.Spot(\n    fun=fun_sphere,\n    lower=np.array([-1, -1]),\n    upper=np.array([1, 1]),\n    n_points = 2\n)\n\n# (S-2) Initial Design:\nspot_1.X = spot_1.design.scipy_lhd(\n    spot_1.design_control[\"init_size\"], lower=spot_1.lower, upper=spot_1.upper\n)\nprint(spot_1.X)\n\n# (S-3): Eval initial design:\nspot_1.y = spot_1.fun(spot_1.X)\nprint(spot_1.y)\n\nspot_1.surrogate.fit(spot_1.X, spot_1.y)\nX0 = spot_1.suggest_new_X()\nprint(X0)\nassert X0.size == spot_1.n_points * spot_1.k\n\n[[ 0.86352963  0.7892358 ]\n [-0.24407197 -0.83687436]\n [ 0.36481882  0.8375811 ]\n [ 0.415331    0.54468512]\n [-0.56395091 -0.77797854]\n [-0.90259409 -0.04899292]\n [-0.16484832  0.35724741]\n [ 0.05170659  0.07401196]\n [-0.78548145 -0.44638164]\n [ 0.64017497 -0.30363301]]\n[1.36857656 0.75992983 0.83463487 0.46918172 0.92329124 0.8170764\n 0.15480068 0.00815134 0.81623768 0.502017  ]\n[[0.0015752  0.0042103 ]\n [0.00153684 0.00421284]]"
  },
  {
    "objectID": "07_spot_ei.html#ei-the-famous-schonlau-example",
    "href": "07_spot_ei.html#ei-the-famous-schonlau-example",
    "title": "7  Expected Improvement",
    "section": "7.9 EI: The Famous Schonlau Example",
    "text": "7.9 EI: The Famous Schonlau Example\n\nX_train0 = np.array([1, 2, 3, 4, 12]).reshape(-1,1)\nX_train = np.linspace(start=0, stop=10, num=5).reshape(-1, 1)\n\n\nfrom spotPython.build.kriging import Kriging\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX_train = np.array([1., 2., 3., 4., 12.]).reshape(-1,1)\ny_train = np.array([0., -1.75, -2, -0.5, 5.])\n\nS = Kriging(name='kriging',  seed=123, log_level=50, n_theta=1, noise=False, cod_type=\"norm\")\nS.fit(X_train, y_train)\n\nX = np.linspace(start=0, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\nif True:\n    plt.fill_between(\n        X.ravel(),\n        mean_prediction - 2 * std_prediction,\n        mean_prediction + 2 * std_prediction,\n        alpha=0.5,\n        label=r\"95% confidence interval\",\n    )\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n\n\n\n\n\n#plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n# plt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, -ei, label=\"Expected Improvement\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n\n\n\n\n\nS.log\n\n{'negLnLike': array([1.20788205]),\n 'theta': array([1.09276015]),\n 'p': array([2.]),\n 'Lambda': array([None], dtype=object)}"
  },
  {
    "objectID": "07_spot_ei.html#ei-the-forrester-example",
    "href": "07_spot_ei.html#ei-the-forrester-example",
    "title": "7  Expected Improvement",
    "section": "7.10 EI: The Forrester Example",
    "text": "7.10 EI: The Forrester Example\n\nfrom spotPython.build.kriging import Kriging\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\n\n# exact x locations are unknown:\nX_train = np.array([0.0, 0.175, 0.225, 0.3, 0.35, 0.375, 0.5,1]).reshape(-1,1)\n\nfun = analytical().fun_forrester\nfun_control = {\"sigma\": 1.0,\n               \"seed\": 123}\ny_train = fun(X_train, fun_control=fun_control)\n\nS = Kriging(name='kriging',  seed=123, log_level=50, n_theta=1, noise=False, cod_type=\"norm\")\nS.fit(X_train, y_train)\n\nX = np.linspace(start=0, stop=1, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, mean_prediction, label=\"Mean prediction\")\nif True:\n    plt.fill_between(\n        X.ravel(),\n        mean_prediction - 2 * std_prediction,\n        mean_prediction + 2 * std_prediction,\n        alpha=0.5,\n        label=r\"95% confidence interval\",\n    )\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n\n\n\n\n\n#plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n# plt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X, -ei, label=\"Expected Improvement\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression on noise-free dataset\")"
  },
  {
    "objectID": "07_spot_ei.html#noise",
    "href": "07_spot_ei.html#noise",
    "title": "7  Expected Improvement",
    "section": "7.11 Noise",
    "text": "7.11 Noise\n\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nimport matplotlib.pyplot as plt\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_sphere\nfun_control = {\"sigma\": 2,\n               \"seed\": 125}\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\ny.shape\nX_train = X.reshape(-1,1)\ny_train = y\n\nS = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=False)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\n#plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression on noisy dataset\")\n\n[[ 0.63529627]\n [-4.10764204]\n [-0.44071975]\n [ 9.63125638]\n [-8.3518118 ]\n [-3.62418901]\n [ 4.15331   ]\n [ 3.4468512 ]\n [ 6.36049088]\n [-7.77978539]]\n[-4.61635371 11.44873209 -0.19988024 91.92791676 68.05926244 12.02926818\n 16.2470957   9.12729929 38.4987029  58.38469104]\n\n\n\n\n\n\nS.log\n\n{'negLnLike': array([24.69806131]),\n 'theta': array([1.31023969]),\n 'p': array([2.]),\n 'Lambda': array([None], dtype=object)}\n\n\n\nS = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=True)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\n#plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression with nugget on noisy dataset\")\n\n\n\n\n\nS.log\n\n{'negLnLike': array([22.14095646]),\n 'theta': array([-0.32527397]),\n 'p': array([2.]),\n 'Lambda': array([9.0881501e-05])}"
  },
  {
    "objectID": "07_spot_ei.html#cubic-function",
    "href": "07_spot_ei.html#cubic-function",
    "title": "7  Expected Improvement",
    "section": "7.12 Cubic Function",
    "text": "7.12 Cubic Function\n\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nimport matplotlib.pyplot as plt\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_cubed\nfun_control = {\"sigma\": 10,\n               \"seed\": 123}\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\ny.shape\nX_train = X.reshape(-1,1)\ny_train = y\n\nS = Kriging(name='kriging',  seed=123, log_level=50, n_theta=1, noise=False)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Cubed: Gaussian process regression on noisy dataset\")\n\n[[ 0.63529627]\n [-4.10764204]\n [-0.44071975]\n [ 9.63125638]\n [-8.3518118 ]\n [-3.62418901]\n [ 4.15331   ]\n [ 3.4468512 ]\n [ 6.36049088]\n [-7.77978539]]\n[  -9.63480707  -72.98497325   12.7936499   895.34567477 -573.35961837\n  -41.83176425   65.27989461   46.37081417  254.1530734  -474.09587355]\n\n\n\n\n\n\nS = Kriging(name='kriging',  seed=123, log_level=0, n_theta=1, noise=True)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Cubed: Gaussian process with nugget regression on noisy dataset\")\n\n\n\n\n\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nimport matplotlib.pyplot as plt\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_runge\nfun_control = {\"sigma\": 0.25,\n               \"seed\": 123}\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\ny.shape\nX_train = X.reshape(-1,1)\ny_train = y\n\nS = Kriging(name='kriging',  seed=123, log_level=50, n_theta=1, noise=False)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression on noisy dataset\")\n\n[[ 0.63529627]\n [-4.10764204]\n [-0.44071975]\n [ 9.63125638]\n [-8.3518118 ]\n [-3.62418901]\n [ 4.15331   ]\n [ 3.4468512 ]\n [ 6.36049088]\n [-7.77978539]]\n[ 0.46517267 -0.03599548  1.15933822  0.05915901  0.24419145  0.21502359\n -0.10432134  0.21312309 -0.05502681 -0.06434374]\n\n\n\n\n\n\nS = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=True)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\n#plt.plot(X, ei, label=\"Expected Improvement\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Gaussian process regression with nugget on noisy dataset\")"
  },
  {
    "objectID": "07_spot_ei.html#factors",
    "href": "07_spot_ei.html#factors",
    "title": "7  Expected Improvement",
    "section": "7.13 Factors",
    "text": "7.13 Factors\n\n[\"num\"] * 3\n\n['num', 'num', 'num']\n\n\n\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nfrom spotPython.fun.objectivefunctions import analytical\nimport numpy as np\n\n\ngen = spacefilling(2)\nn = 30\nrng = np.random.RandomState(1)\nlower = np.array([-5,-0])\nupper = np.array([10,15])\nfun = analytical().fun_branin_factor\n#fun = analytical(sigma=0).fun_sphere\n\nX0 = gen.scipy_lhd(n, lower=lower, upper = upper)\nX1 = np.random.randint(low=1, high=3, size=(n,))\nX = np.c_[X0, X1]\ny = fun(X)\nS = Kriging(name='kriging',  seed=123, log_level=50, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\nS.fit(X, y)\nSf = Kriging(name='kriging',  seed=123, log_level=50, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"factor\"])\nSf.fit(X, y)\nn = 50\nX0 = gen.scipy_lhd(n, lower=lower, upper = upper)\nX1 = np.random.randint(low=1, high=3, size=(n,))\nX = np.c_[X0, X1]\ny = fun(X)\ns=np.sum(np.abs(S.predict(X)[0] - y))\nsf=np.sum(np.abs(Sf.predict(X)[0] - y))\nsf - s\n\n-22.78851643888538\n\n\n\n# vars(S)\n\n\n# vars(Sf)"
  },
  {
    "objectID": "08_spot_noisy.html#example-spot-and-the-noisy-sphere-function",
    "href": "08_spot_noisy.html#example-spot-and-the-noisy-sphere-function",
    "title": "8  Hyperparameter Tuning and Noise",
    "section": "8.1 Example: Spot and the Noisy Sphere Function",
    "text": "8.1 Example: Spot and the Noisy Sphere Function\n\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport socket\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\n\n\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '10-sklearn' + \"_\" + HOSTNAME + \"_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\n\n10-sklearn_maans03_2023-06-28_01-12-15\n\n\n\n8.1.1 The Objective Function: Noisy Sphere\n\nThe spotPython package provides several classes of objective functions.\nWe will use an analytical objective function with noise, i.e., a function that can be described by a (closed) formula: \\[f(x) = x^2 + \\epsilon\\]\nSince sigma is set to 0.1, noise is added to the function:\n\n\nfun = analytical().fun_sphere\nfun_control = {\"sigma\": 0.1,\n               \"seed\": 123}\n\n\nA plot illustrates the noise:\n\n\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x, fun_control=fun_control)\nplt.figure()\nplt.plot(x,y, \"k\")\nplt.show()\n\n\n\n\nSpot is adopted as follows to cope with noisy functions:\n\nfun_repeats is set to a value larger than 1 (here: 2)\nnoise is set to true. Therefore, a nugget (Lambda) term is added to the correlation matrix\ninit size (of the design_control dictionary) is set to a value larger than 1 (here: 2)\n\n\nspot_1_noisy = spot.Spot(fun=fun,\n                   lower = np.array([-1]),\n                   upper = np.array([1]),\n                   fun_evals = 10,\n                   fun_repeats = 2,\n                   noise = True,\n                   seed=123,\n                   show_models=True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": 3,\n                                   \"repeats\": 2},\n                   surrogate_control={\"noise\": True})\n\n\nspot_1_noisy.run()\n\n\n\n\n\n\n\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x15a3ad8d0&gt;"
  },
  {
    "objectID": "08_spot_noisy.html#print-the-results",
    "href": "08_spot_noisy.html#print-the-results",
    "title": "8  Hyperparameter Tuning and Noise",
    "section": "8.2 Print the Results",
    "text": "8.2 Print the Results\n\nspot_1_noisy.print_results()\n\nmin y: -0.06415721563564872\nx0: 0.18642671321228718\nmin mean y: -0.03309048069165033\nx0: 0.18642671321228718\n\n\n[['x0', 0.18642671321228718], ['x0', 0.18642671321228718]]\n\n\n\nspot_1_noisy.plot_progress(log_y=False,\n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization."
  },
  {
    "objectID": "08_spot_noisy.html#noise-and-surrogates-the-nugget-effect",
    "href": "08_spot_noisy.html#noise-and-surrogates-the-nugget-effect",
    "title": "8  Hyperparameter Tuning and Noise",
    "section": "8.3 Noise and Surrogates: The Nugget Effect",
    "text": "8.3 Noise and Surrogates: The Nugget Effect\n\n8.3.1 The Noisy Sphere\n\n8.3.1.1 The Data\n\nWe prepare some data first:\n\n\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nimport matplotlib.pyplot as plt\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_sphere\nfun_control = {\"sigma\": 2,\n               \"seed\": 125}\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\ny = fun(X, fun_control=fun_control)\nX_train = X.reshape(-1,1)\ny_train = y\n\n\nA surrogate without nugget is fitted to these data:\n\n\nS = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=False)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression on noisy dataset\")\n\n\n\n\n\nIn comparison to the surrogate without nugget, we fit a surrogate with nugget to the data:\n\n\nS_nug = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=True)\nS_nug.fit(X_train, y_train)\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S_nug.predict(X_axis, return_val=\"all\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression with nugget on noisy dataset\")\n\n\n\n\n\nThe value of the nugget term can be extracted from the model as follows:\n\n\nS.Lambda\n\n\nS_nug.Lambda\n\n9.088150096649695e-05\n\n\n\nWe see:\n\nthe first model S has no nugget,\nwhereas the second model has a nugget value (Lambda) larger than zero."
  },
  {
    "objectID": "08_spot_noisy.html#exercises",
    "href": "08_spot_noisy.html#exercises",
    "title": "8  Hyperparameter Tuning and Noise",
    "section": "8.4 Exercises",
    "text": "8.4 Exercises\n\n8.4.1 Noisy fun_cubed\n\nAnalyse the effect of noise on the fun_cubed function with the following settings:\n\n\nfun = analytical().fun_cubed\nfun_control = {\"sigma\": 10,\n               \"seed\": 123}\nlower = np.array([-10])\nupper = np.array([10])\n\n\n\n8.4.2 fun_runge\n\nAnalyse the effect of noise on the fun_runge function with the following settings:\n\n\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_runge\nfun_control = {\"sigma\": 0.25,\n               \"seed\": 123}\n\n\n\n8.4.3 fun_forrester\n\nAnalyse the effect of noise on the fun_forrester function with the following settings:\n\n\nlower = np.array([0])\nupper = np.array([1])\nfun = analytical().fun_forrester\nfun_control = {\"sigma\": 5,\n               \"seed\": 123}\n\n\n\n8.4.4 fun_xsin\n\nAnalyse the effect of noise on the fun_xsin function with the following settings:\n\n\nlower = np.array([-1.])\nupper = np.array([1.])\nfun = analytical().fun_xsin\nfun_control = {\"sigma\": 0.5,\n               \"seed\": 123}"
  },
  {
    "objectID": "09_spot_ocba.html#example-spot-ocba-and-the-noisy-sphere-function",
    "href": "09_spot_ocba.html#example-spot-ocba-and-the-noisy-sphere-function",
    "title": "9  Handling Noise: Optimal Computational Budget Allocation in Spot",
    "section": "9.1 Example: Spot, OCBA, and the Noisy Sphere Function",
    "text": "9.1 Example: Spot, OCBA, and the Noisy Sphere Function\n\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\n\n\n9.1.1 The Objective Function: Noisy Sphere\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function with noise, i.e., a function that can be described by a (closed) formula: \\[f(x) = x^2 + \\epsilon\\]\nSince sigma is set to 0.1, noise is added to the function:\n\nfun = analytical().fun_sphere\nfun_control = {\"sigma\": 0.1,\n               \"seed\": 123}\n\nA plot illustrates the noise:\n\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x, fun_control=fun_control)\nplt.figure()\nplt.plot(x,y, \"k\")\nplt.show()\n\n\n\n\nSpot is adopted as follows to cope with noisy functions:\n\nfun_repeats is set to a value larger than 1 (here: 2)\nnoise is set to true. Therefore, a nugget (Lambda) term is added to the correlation matrix\ninit size (of the design_control dictionary) is set to a value larger than 1 (here: 2)\n\n\nspot_1_noisy = spot.Spot(fun=fun,\n                   lower = np.array([-1]),\n                   upper = np.array([1]),\n                   fun_evals = 50,\n                   fun_repeats = 2,\n                   infill_criterion=\"ei\",\n                   noise = True,\n                   tolerance_x=0.0,\n                   ocba_delta = 1,\n                   seed=123,\n                   show_models=True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": 3,\n                                   \"repeats\": 2},\n                   surrogate_control={\"noise\": True})\n\n\nspot_1_noisy.run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x1600f1960&gt;"
  },
  {
    "objectID": "09_spot_ocba.html#print-the-results",
    "href": "09_spot_ocba.html#print-the-results",
    "title": "9  Handling Noise: Optimal Computational Budget Allocation in Spot",
    "section": "9.2 Print the Results",
    "text": "9.2 Print the Results\n\nspot_1_noisy.print_results()\n\nmin y: -0.08106318976988831\nx0: 0.13359994485364424\nmin mean y: -0.06294830657915665\nx0: 0.13359994485364424\n\n\n[['x0', 0.13359994485364424], ['x0', 0.13359994485364424]]\n\n\n\nspot_1_noisy.plot_progress(log_y=False)"
  },
  {
    "objectID": "09_spot_ocba.html#noise-and-surrogates-the-nugget-effect",
    "href": "09_spot_ocba.html#noise-and-surrogates-the-nugget-effect",
    "title": "9  Handling Noise: Optimal Computational Budget Allocation in Spot",
    "section": "9.3 Noise and Surrogates: The Nugget Effect",
    "text": "9.3 Noise and Surrogates: The Nugget Effect\n\n9.3.1 The Noisy Sphere\n\n9.3.1.1 The Data\nWe prepare some data first:\n\nimport numpy as np\nimport spotPython\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nimport matplotlib.pyplot as plt\n\ngen = spacefilling(1)\nrng = np.random.RandomState(1)\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_sphere\nfun_control = {\"sigma\": 2,\n               \"seed\": 125}\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\ny = fun(X, fun_control=fun_control)\nX_train = X.reshape(-1,1)\ny_train = y\n\nA surrogate without nugget is fitted to these data:\n\nS = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=False)\nS.fit(X_train, y_train)\n\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S.predict(X_axis, return_val=\"all\")\n\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression on noisy dataset\")\n\n\n\n\nIn comparison to the surrogate without nugget, we fit a surrogate with nugget to the data:\n\nS_nug = Kriging(name='kriging',\n            seed=123,\n            log_level=50,\n            n_theta=1,\n            noise=True)\nS_nug.fit(X_train, y_train)\nX_axis = np.linspace(start=-13, stop=13, num=1000).reshape(-1, 1)\nmean_prediction, std_prediction, ei = S_nug.predict(X_axis, return_val=\"all\")\nplt.scatter(X_train, y_train, label=\"Observations\")\nplt.plot(X_axis, mean_prediction, label=\"mue\")\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\n_ = plt.title(\"Sphere: Gaussian process regression with nugget on noisy dataset\")\n\n\n\n\nThe value of the nugget term can be extracted from the model as follows:\n\nS.Lambda\n\n\nS_nug.Lambda\n\n9.088150104540512e-05\n\n\nWe see:\n\nthe first model S has no nugget,\nwhereas the second model has a nugget value (Lambda) larger than zero."
  },
  {
    "objectID": "09_spot_ocba.html#exercises",
    "href": "09_spot_ocba.html#exercises",
    "title": "9  Handling Noise: Optimal Computational Budget Allocation in Spot",
    "section": "9.4 Exercises",
    "text": "9.4 Exercises\n\n9.4.1 Noisy fun_cubed\nAnalyse the effect of noise on the fun_cubed function with the following settings:\n\nfun = analytical().fun_cubed\nfun_control = {\"sigma\": 10,\n               \"seed\": 123}\nlower = np.array([-10])\nupper = np.array([10])\n\n\n\n9.4.2 fun_runge\nAnalyse the effect of noise on the fun_runge function with the following settings:\n\nlower = np.array([-10])\nupper = np.array([10])\nfun = analytical().fun_runge\nfun_control = {\"sigma\": 0.25,\n               \"seed\": 123}\n\n\n\n9.4.3 fun_forrester\nAnalyse the effect of noise on the fun_forrester function with the following settings:\n\nlower = np.array([0])\nupper = np.array([1])\nfun = analytical().fun_forrester\nfun_control = {\"sigma\": 5,\n               \"seed\": 123}\n\n\n\n9.4.4 fun_xsin\nAnalyse the effect of noise on the fun_xsin function with the following settings:\n\nlower = np.array([-1.])\nupper = np.array([1.])\nfun = analytical().fun_xsin\nfun_control = {\"sigma\": 0.5,\n               \"seed\": 123}\n\n\nspot_1_noisy.mean_y.shape[0]\n\n18"
  },
  {
    "objectID": "10_spot_hpt_sklearn_classification.html#sec-setup-10",
    "href": "10_spot_hpt_sklearn_classification.html#sec-setup-10",
    "title": "10  HPT: sklearn SVC on Moons Data",
    "section": "10.1 Step 1: Setup",
    "text": "10.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time and the initial design size.\n\nMAX_TIME = 1\nINIT_SIZE = 5\n\n\nimport os\nimport copy\nimport socket\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '10-sklearn' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\n\n10-sklearn_maans03_1min_5init_2023-06-28_01-12-56"
  },
  {
    "objectID": "10_spot_hpt_sklearn_classification.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "href": "10_spot_hpt_sklearn_classification.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "title": "10  HPT: sklearn SVC on Moons Data",
    "section": "10.2 Step 2: Initialization of the Empty fun_control Dictionary",
    "text": "10.2 Step 2: Initialization of the Empty fun_control Dictionary\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"classification\",\n    tensorboard_path=\"runs/10_spot_hpt_sklearn_classification\")"
  },
  {
    "objectID": "10_spot_hpt_sklearn_classification.html#sec-data-loading-10",
    "href": "10_spot_hpt_sklearn_classification.html#sec-data-loading-10",
    "title": "10  HPT: sklearn SVC on Moons Data",
    "section": "10.3 Step 3: SKlearn Load Data (Classification)",
    "text": "10.3 Step 3: SKlearn Load Data (Classification)\nRandomly generate classification data.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nn_features = 2\nn_samples = 250\ntarget_column = \"y\"\nds =  make_moons(n_samples, noise=0.5, random_state=0)\nX, y = ds\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.4, random_state=42\n)\ntrain = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1))))\ntest = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1))))\ntrain.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\ntest.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\ntrain.head()\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n1.083978\n-1.246111\n1.0\n\n\n1\n0.074916\n0.868104\n0.0\n\n\n2\n-1.668535\n0.751752\n0.0\n\n\n3\n1.286597\n1.454165\n0.0\n\n\n4\n1.387021\n0.448355\n1.0\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\ncm = plt.cm.RdBu\ncm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\nax = plt.subplot(1, 1, 1)\nax.set_title(\"Input data\")\n# Plot the training points\nax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n# Plot the testing points\nax.scatter(\n    X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n)\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xticks(())\nax.set_yticks(())\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nn_samples = len(train)\n# add the dataset to the fun_control\nfun_control.update({\"data\": None, # dataset,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": target_column})"
  },
  {
    "objectID": "10_spot_hpt_sklearn_classification.html#sec-specification-of-preprocessing-model-10",
    "href": "10_spot_hpt_sklearn_classification.html#sec-specification-of-preprocessing-model-10",
    "title": "10  HPT: sklearn SVC on Moons Data",
    "section": "10.4 Step 4: Specification of the Preprocessing Model",
    "text": "10.4 Step 4: Specification of the Preprocessing Model\nData preprocesssing can be very simple, e.g., you can ignore it. Then you would choose the prep_model “None”:\n\nprep_model = None\nfun_control.update({\"prep_model\": prep_model})\n\nA default approach for numerical data is the StandardScaler (mean 0, variance 1). This can be selected as follows:\n\nfrom sklearn.preprocessing import StandardScaler\nprep_model = StandardScaler()\nfun_control.update({\"prep_model\": prep_model})\n\nEven more complicated pre-processing steps are possible, e.g., the follwing pipeline:\n\n# categorical_columns = []\n# one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n# prep_model = ColumnTransformer(\n#         transformers=[\n#             (\"categorical\", one_hot_encoder, categorical_columns),\n#         ],\n#         remainder=StandardScaler(),\n#     )"
  },
  {
    "objectID": "10_spot_hpt_sklearn_classification.html#step-5-select-model-algorithm-and-core_model_hyper_dict",
    "href": "10_spot_hpt_sklearn_classification.html#step-5-select-model-algorithm-and-core_model_hyper_dict",
    "title": "10  HPT: sklearn SVC on Moons Data",
    "section": "10.5 Step 5: Select Model (algorithm) and core_model_hyper_dict",
    "text": "10.5 Step 5: Select Model (algorithm) and core_model_hyper_dict\nThe selection of the algorithm (ML model) that should be tuned is done by specifying the its name from the sklearn implementation. For example, the SVC support vector machine classifier is selected as follows:\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNet\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfrom spotPython.data.sklearn_hyper_dict import SklearnHyperDict\nfrom spotPython.fun.hypersklearn import HyperSklearn\n\n\n# core_model  = RidgeCV\n# core_model = GradientBoostingRegressor\n# core_model = ElasticNet\n# core_model = RandomForestClassifier\ncore_model = SVC\n# core_model = LogisticRegression\n# core_model = KNeighborsClassifier\n# core_model = GradientBoostingClassifier\nfun_control = add_core_model_to_fun_control(core_model=core_model,\n                              fun_control=fun_control,\n                              hyper_dict=SklearnHyperDict,\n                              filename=None)\n\nNow fun_control has the information from the JSON file. The corresponding entries for the core_model class are shown below.\n\nfun_control['core_model_hyper_dict']\n\n{'C': {'type': 'float',\n  'default': 1.0,\n  'transform': 'None',\n  'lower': 0.1,\n  'upper': 10.0},\n 'kernel': {'levels': ['linear', 'poly', 'rbf', 'sigmoid'],\n  'type': 'factor',\n  'default': 'rbf',\n  'transform': 'None',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 3},\n 'degree': {'type': 'int',\n  'default': 3,\n  'transform': 'None',\n  'lower': 3,\n  'upper': 3},\n 'gamma': {'levels': ['scale', 'auto'],\n  'type': 'factor',\n  'default': 'scale',\n  'transform': 'None',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 1},\n 'coef0': {'type': 'float',\n  'default': 0.0,\n  'transform': 'None',\n  'lower': 0.0,\n  'upper': 0.0},\n 'shrinking': {'levels': [0, 1],\n  'type': 'factor',\n  'default': 0,\n  'transform': 'None',\n  'core_model_parameter_type': 'bool',\n  'lower': 0,\n  'upper': 1},\n 'probability': {'levels': [0, 1],\n  'type': 'factor',\n  'default': 0,\n  'transform': 'None',\n  'core_model_parameter_type': 'bool',\n  'lower': 0,\n  'upper': 1},\n 'tol': {'type': 'float',\n  'default': 0.001,\n  'transform': 'None',\n  'lower': 0.0001,\n  'upper': 0.01},\n 'cache_size': {'type': 'float',\n  'default': 200,\n  'transform': 'None',\n  'lower': 100,\n  'upper': 400},\n 'break_ties': {'levels': [0, 1],\n  'type': 'factor',\n  'default': 0,\n  'transform': 'None',\n  'core_model_parameter_type': 'bool',\n  'lower': 0,\n  'upper': 1}}"
  },
  {
    "objectID": "10_spot_hpt_sklearn_classification.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "href": "10_spot_hpt_sklearn_classification.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "title": "10  HPT: sklearn SVC on Moons Data",
    "section": "10.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "10.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in Section 14.6.\n\n10.6.1 Modify hyperparameter of type numeric and integer (boolean)\nNumeric and boolean values can be modified using the modify_hyper_parameter_bounds method. For example, to change the tol hyperparameter of the SVC model to the interval [1e-3, 1e-2], the following code can be used:\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\nfun_control = modify_hyper_parameter_bounds(fun_control, \"tol\", bounds=[1e-3, 1e-2])\n# fun_control = modify_hyper_parameter_bounds(fun_control, \"min_samples_split\", bounds=[3, 20])\n#fun_control = modify_hyper_parameter_bounds(fun_control, \"merit_preprune\", bounds=[0, 0])\nfun_control[\"core_model_hyper_dict\"][\"tol\"]\n\n{'type': 'float',\n 'default': 0.001,\n 'transform': 'None',\n 'lower': 0.001,\n 'upper': 0.01}\n\n\n\n\n10.6.2 Modify hyperparameter of type factor\nFactors can be modified with the modify_hyper_parameter_levels function. For example, to exclude the sigmoid kernel from the tuning, the kernel hyperparameter of the SVC model can be modified as follows:\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_levels\nfun_control = modify_hyper_parameter_levels(fun_control, \"kernel\", [\"linear\", \"poly\", \"rbf\"])\nfun_control[\"core_model_hyper_dict\"][\"kernel\"]\n\n{'levels': ['linear', 'poly', 'rbf'],\n 'type': 'factor',\n 'default': 'rbf',\n 'transform': 'None',\n 'core_model_parameter_type': 'str',\n 'lower': 0,\n 'upper': 2}\n\n\n\n\n10.6.3 Optimizers\nOptimizers are described in Section 14.6.1."
  },
  {
    "objectID": "10_spot_hpt_sklearn_classification.html#step-7-selection-of-the-objective-loss-function",
    "href": "10_spot_hpt_sklearn_classification.html#step-7-selection-of-the-objective-loss-function",
    "title": "10  HPT: sklearn SVC on Moons Data",
    "section": "10.7 Step 7: Selection of the Objective (Loss) Function",
    "text": "10.7 Step 7: Selection of the Objective (Loss) Function\nThere are two metrics:\n\nmetric_river is used for the river based evaluation via eval_oml_iter_progressive.\nmetric_sklearn is used for the sklearn based evaluation.\n\n\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, roc_curve, roc_auc_score, log_loss, mean_squared_error\nfun_control.update({\n               \"metric_sklearn\": log_loss,\n               })\n\n\n10.7.1 Predict Classes or Class Probabilities\nIf the key \"predict_proba\" is set to True, the class probabilities are predicted. False is the default, i.e., the classes are predicted.\n\nfun_control.update({\n               \"predict_proba\": False,\n               })"
  },
  {
    "objectID": "10_spot_hpt_sklearn_classification.html#step-8-calling-the-spot-function",
    "href": "10_spot_hpt_sklearn_classification.html#step-8-calling-the-spot-function",
    "title": "10  HPT: sklearn SVC on Moons Data",
    "section": "10.8 Step 8: Calling the SPOT Function",
    "text": "10.8 Step 8: Calling the SPOT Function\n\n10.8.1 Preparing the SPOT Call\nThe following code passes the information about the parameter ranges and bounds to spot.\n\n# extract the variable types, names, and bounds\nfrom spotPython.hyperparameters.values import (get_bound_values,\n    get_var_name,\n    get_var_type,)\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name        | type   | default   |   lower |   upper | transform   |\n|-------------|--------|-----------|---------|---------|-------------|\n| C           | float  | 1.0       |   0.1   |   10    | None        |\n| kernel      | factor | rbf       |   0     |    2    | None        |\n| degree      | int    | 3         |   3     |    3    | None        |\n| gamma       | factor | scale     |   0     |    1    | None        |\n| coef0       | float  | 0.0       |   0     |    0    | None        |\n| shrinking   | factor | 0         |   0     |    1    | None        |\n| probability | factor | 0         |   0     |    1    | None        |\n| tol         | float  | 0.001     |   0.001 |    0.01 | None        |\n| cache_size  | float  | 200.0     | 100     |  400    | None        |\n| break_ties  | factor | 0         |   0     |    1    | None        |\n\n\n\n\n10.8.2 The Objective Function\nThe objective function is selected next. It implements an interface from sklearn’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hypersklearn import HyperSklearn\nfun = HyperSklearn().fun_sklearn\n\n\n\n10.8.3 Run the Spot Optimizer\n\nRun SPOT for approx. x mins (max_time).\nNote: the run takes longer, because the evaluation time of initial design (here: initi_size, 20 points) is not considered.\n\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nhyper_dict=SklearnHyperDict().load()\nX_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\nX_start\n\narray([[1.e+00, 2.e+00, 3.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-03,\n        2.e+02, 0.e+00]])\n\n\n\n\n10.8.4 Starting the Hyperparameter Tuning\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom math import inf\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\nspotPython tuning: 5.691103166702708 [#---------] 7.34% \n\n\nspotPython tuning: 4.7425859722522565 [#---------] 10.99% \n\n\nspotPython tuning: 4.7425859722522565 [#---------] 14.61% \n\n\nspotPython tuning: 4.7425859722522565 [##--------] 17.94% \n\n\nspotPython tuning: 4.7425859722522565 [##--------] 20.99% \n\n\nspotPython tuning: 4.7425859722522565 [##--------] 23.60% \n\n\nspotPython tuning: 4.7425859722522565 [###-------] 34.04% \n\n\nspotPython tuning: 4.7425859722522565 [####------] 37.78% \n\n\nspotPython tuning: 4.7425859722522565 [#####-----] 49.45% \n\n\nspotPython tuning: 4.7425859722522565 [#####-----] 53.36% \n\n\nspotPython tuning: 4.7425859722522565 [######----] 56.35% \n\n\nspotPython tuning: 4.7425859722522565 [#######---] 71.80% \n\n\nspotPython tuning: 4.7425859722522565 [#########-] 89.76% \n\n\nspotPython tuning: 4.7425859722522565 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x1847d0730&gt;"
  },
  {
    "objectID": "10_spot_hpt_sklearn_classification.html#sec-results-tuning-10",
    "href": "10_spot_hpt_sklearn_classification.html#sec-results-tuning-10",
    "title": "10  HPT: sklearn SVC on Moons Data",
    "section": "10.9 Step 9: Results",
    "text": "10.9 Step 9: Results\n\nSAVE = False\nLOAD = False\n\nif SAVE:\n    result_file_name = \"res_\" + experiment_name + \".pkl\"\n    with open(result_file_name, 'wb') as f:\n        pickle.dump(spot_tuner, f)\n\nif LOAD:\n    result_file_name = \"res_ch10-friedman-hpt-0_maans03_60min_20init_1K_2023-04-14_10-11-19.pkl\"\n    with open(result_file_name, 'rb') as f:\n        spot_tuner =  pickle.load(f)\n\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The following code generates the progress plot from ?fig-progress.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nPrint the results\n\n\nprint(gen_design_table(fun_control=fun_control,\n    spot=spot_tuner))\n\n| name        | type   | default   |   lower |   upper |                tuned | transform   |   importance | stars   |\n|-------------|--------|-----------|---------|---------|----------------------|-------------|--------------|---------|\n| C           | float  | 1.0       |     0.1 |    10.0 |  0.23258412447782734 | None        |       100.00 | ***     |\n| kernel      | factor | rbf       |     0.0 |     2.0 |                  1.0 | None        |         0.32 | .       |\n| degree      | int    | 3         |     3.0 |     3.0 |                  3.0 | None        |         0.00 |         |\n| gamma       | factor | scale     |     0.0 |     1.0 |                  0.0 | None        |       100.00 | ***     |\n| coef0       | float  | 0.0       |     0.0 |     0.0 |                  0.0 | None        |         0.00 |         |\n| shrinking   | factor | 0         |     0.0 |     1.0 |                  1.0 | None        |       100.00 | ***     |\n| probability | factor | 0         |     0.0 |     1.0 |                  1.0 | None        |         0.00 |         |\n| tol         | float  | 0.001     |   0.001 |    0.01 | 0.003757085413122674 | None        |         0.00 |         |\n| cache_size  | float  | 200.0     |   100.0 |   400.0 |   214.29269330654913 | None        |         0.00 |         |\n| break_ties  | factor | 0         |     0.0 |     1.0 |                  1.0 | None        |         0.00 |         |\n\n\n\n10.9.1 Show variable importance\n\nspot_tuner.plot_importance(threshold=0.025, filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n\n10.9.2 Get Default Hyperparameters\n\nfrom spotPython.hyperparameters.values import get_default_values, transform_hyper_parameter_values\nvalues_default = get_default_values(fun_control)\nvalues_default = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values_default)\nvalues_default\n\n{'C': 1.0,\n 'kernel': 'rbf',\n 'degree': 3,\n 'gamma': 'scale',\n 'coef0': 0.0,\n 'shrinking': 0,\n 'probability': 0,\n 'tol': 0.001,\n 'cache_size': 200.0,\n 'break_ties': 0}\n\n\n\nfrom sklearn.pipeline import make_pipeline\nmodel_default = make_pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"](**values_default))\nmodel_default\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc',\n                 SVC(break_ties=0, cache_size=200.0, probability=0,\n                     shrinking=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc',\n                 SVC(break_ties=0, cache_size=200.0, probability=0,\n                     shrinking=0))])StandardScalerStandardScaler()SVCSVC(break_ties=0, cache_size=200.0, probability=0, shrinking=0)\n\n\n\n\n10.9.3 Get SPOT Results\n\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nprint(X)\n\n[[2.32584124e-01 1.00000000e+00 3.00000000e+00 0.00000000e+00\n  0.00000000e+00 1.00000000e+00 1.00000000e+00 3.75708541e-03\n  2.14292693e+02 1.00000000e+00]]\n\n\n\nfrom spotPython.hyperparameters.values import assign_values, return_conf_list_from_var_dict\nv_dict = assign_values(X, fun_control[\"var_name\"])\nreturn_conf_list_from_var_dict(var_dict=v_dict, fun_control=fun_control)\n\n[{'C': 0.23258412447782734,\n  'kernel': 'poly',\n  'degree': 3,\n  'gamma': 'scale',\n  'coef0': 0.0,\n  'shrinking': 1,\n  'probability': 1,\n  'tol': 0.003757085413122674,\n  'cache_size': 214.29269330654913,\n  'break_ties': 1}]\n\n\n\nfrom spotPython.hyperparameters.values import get_one_sklearn_model_from_X\nmodel_spot = get_one_sklearn_model_from_X(X, fun_control)\nmodel_spot\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc',\n                 SVC(C=0.23258412447782734, break_ties=1,\n                     cache_size=214.29269330654913, kernel='poly',\n                     probability=1, shrinking=1, tol=0.003757085413122674))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc',\n                 SVC(C=0.23258412447782734, break_ties=1,\n                     cache_size=214.29269330654913, kernel='poly',\n                     probability=1, shrinking=1, tol=0.003757085413122674))])StandardScalerStandardScaler()SVCSVC(C=0.23258412447782734, break_ties=1, cache_size=214.29269330654913,\n    kernel='poly', probability=1, shrinking=1, tol=0.003757085413122674)\n\n\n\n\n10.9.4 Plot: Compare Predictions\n\nfrom spotPython.plot.validation import plot_roc\nplot_roc([model_default, model_spot], fun_control, model_names=[\"Default\", \"Spot\"])\n\n\n\n\n\nfrom spotPython.plot.validation import plot_confusion_matrix\nplot_confusion_matrix(model_default, fun_control, title = \"Default\")\n\n\n\n\n\nplot_confusion_matrix(model_spot, fun_control, title=\"SPOT\")\n\n\n\n\n\nmin(spot_tuner.y), max(spot_tuner.y)\n\n(4.7425859722522565, 9.485171944504513)\n\n\n\n\n10.9.5 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nC:  100.0\nkernel:  0.32481000470286914\ngamma:  100.0\nshrinking:  100.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.9.6 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n                                                \n\n\n\n\n10.9.7 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
  },
  {
    "objectID": "11_spot_hpt_torch_fashion_mnist.html#sec-setup-11",
    "href": "11_spot_hpt_torch_fashion_mnist.html#sec-setup-11",
    "title": "11  HPT: PyTorch With fashionMNIST",
    "section": "11.1 Step 1: Setup",
    "text": "11.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size and the device that is used.\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nThe device can be selected by setting the variable DEVICE.\nSince we are using a simple neural net, the setting \"cpu\" is preferred (on Mac).\nIf you have a GPU, you can use \"cuda:0\" instead.\nIf DEVICE is set to None, spotPython will automatically select the device.\n\nThis might result in \"mps\" on Macs, which is not the best choice for simple neural nets.\n\n\n\n\n\nMAX_TIME = 1\nINIT_SIZE = 5\nDEVICE = \"cpu\" # \"cuda:0\"\n\n\nfrom spotPython.utils.device import getDevice\nDEVICE = getDevice(DEVICE)\nprint(DEVICE)\n\ncpu\n\n\n\nimport os\nimport copy\nimport socket\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '11-torch' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\n\n11-torch_maans03_1min_5init_2023-06-28_01-16-05"
  },
  {
    "objectID": "11_spot_hpt_torch_fashion_mnist.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "href": "11_spot_hpt_torch_fashion_mnist.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "title": "11  HPT: PyTorch With fashionMNIST",
    "section": "11.2 Step 2: Initialization of the Empty fun_control Dictionary",
    "text": "11.2 Step 2: Initialization of the Empty fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process, which was described in Section 14.2.\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"classification\",\n    tensorboard_path=\"runs/11_spot_hpt_torch_fashion_mnist\",\n    device=DEVICE)"
  },
  {
    "objectID": "11_spot_hpt_torch_fashion_mnist.html#sec-data-loading-11",
    "href": "11_spot_hpt_torch_fashion_mnist.html#sec-data-loading-11",
    "title": "11  HPT: PyTorch With fashionMNIST",
    "section": "11.3 Step 3: PyTorch Data Loading",
    "text": "11.3 Step 3: PyTorch Data Loading\n\n11.3.1 Load fashionMNIST Data\n\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\ndef load_data(data_dir=\"./data\"):\n    # Download training data from open datasets.\n    training_data = datasets.FashionMNIST(\n        root=data_dir,\n        train=True,\n        download=True,\n        transform=ToTensor(),\n    )\n    # Download test data from open datasets.\n    test_data = datasets.FashionMNIST(\n        root=data_dir,\n        train=False,\n        download=True,\n        transform=ToTensor(),\n    )\n    return training_data, test_data\n\n\ntrain, test = load_data()\ntrain.data.shape, test.data.shape\n\n(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))\n\n\n\nn_samples = len(train)\n# add the dataset to the fun_control\nfun_control.update({\"data\": None,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": None})"
  },
  {
    "objectID": "11_spot_hpt_torch_fashion_mnist.html#sec-specification-of-preprocessing-model-11",
    "href": "11_spot_hpt_torch_fashion_mnist.html#sec-specification-of-preprocessing-model-11",
    "title": "11  HPT: PyTorch With fashionMNIST",
    "section": "11.4 Step 4: Specification of the Preprocessing Model",
    "text": "11.4 Step 4: Specification of the Preprocessing Model\nAfter the training and test data are specified and added to the fun_control dictionary, spotPython allows the specification of a data preprocessing pipeline, e.g., for the scaling of the data or for the one-hot encoding of categorical variables, see Section 14.4. This feature is not used here, so we do not change the default value (which is None)."
  },
  {
    "objectID": "11_spot_hpt_torch_fashion_mnist.html#sec-selection-of-the-algorithm-11",
    "href": "11_spot_hpt_torch_fashion_mnist.html#sec-selection-of-the-algorithm-11",
    "title": "11  HPT: PyTorch With fashionMNIST",
    "section": "11.5 Step 5: Select Model (algorithm) and core_model_hyper_dict",
    "text": "11.5 Step 5: Select Model (algorithm) and core_model_hyper_dict\nspotPython implements a class which is similar to the class described in the PyTorch tutorial. The class is called Net_fashionMNIST and is implemented in the file netfashionMNIST.py. The class is imported here.\n\nfrom torch import nn\nimport spotPython.torch.netcore as netcore\n\n\nclass Net_fashionMNIST(netcore.Net_Core):\n    def __init__(self, l1, l2, lr_mult, batch_size, epochs, k_folds, patience, optimizer, sgd_momentum):\n        super(Net_fashionMNIST, self).__init__(\n            lr_mult=lr_mult,\n            batch_size=batch_size,\n            epochs=epochs,\n            k_folds=k_folds,\n            patience=patience,\n            optimizer=optimizer,\n            sgd_momentum=sgd_momentum,\n        )\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, l1),\n            nn.ReLU(),\n            nn.Linear(l1, l2),\n            nn.ReLU(),\n            nn.Linear(l2, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nThis class inherits from the class Net_Core which is implemented in the file netcore.py, see Section 14.5.1.\n\nfrom spotPython.data.torch_hyper_dict import TorchHyperDict\nfrom spotPython.torch.netfashionMNIST import Net_fashionMNIST\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfun_control = add_core_model_to_fun_control(core_model=Net_fashionMNIST,\n                              fun_control=fun_control,\n                              hyper_dict=TorchHyperDict,\n                              filename=None)\n\n\n11.5.1 The Search Space\n\n\n11.5.2 Configuring the Search Space With spotPython\n\n11.5.2.1 The hyper_dict Hyperparameters for the Selected Algorithm\nspotPython uses JSON files for the specification of the hyperparameters, which were described in Section 14.5.5.\nThe corresponding entries for the core_model class are shown below.\n\nfun_control['core_model_hyper_dict']\n\n{'l1': {'type': 'int',\n  'default': 5,\n  'transform': 'transform_power_2_int',\n  'lower': 2,\n  'upper': 9},\n 'l2': {'type': 'int',\n  'default': 5,\n  'transform': 'transform_power_2_int',\n  'lower': 2,\n  'upper': 9},\n 'lr_mult': {'type': 'float',\n  'default': 1.0,\n  'transform': 'None',\n  'lower': 0.1,\n  'upper': 10.0},\n 'batch_size': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 1,\n  'upper': 4},\n 'epochs': {'type': 'int',\n  'default': 3,\n  'transform': 'transform_power_2_int',\n  'lower': 3,\n  'upper': 4},\n 'k_folds': {'type': 'int',\n  'default': 1,\n  'transform': 'None',\n  'lower': 1,\n  'upper': 1},\n 'patience': {'type': 'int',\n  'default': 5,\n  'transform': 'None',\n  'lower': 2,\n  'upper': 10},\n 'optimizer': {'levels': ['Adadelta',\n   'Adagrad',\n   'Adam',\n   'AdamW',\n   'SparseAdam',\n   'Adamax',\n   'ASGD',\n   'NAdam',\n   'RAdam',\n   'RMSprop',\n   'Rprop',\n   'SGD'],\n  'type': 'factor',\n  'default': 'SGD',\n  'transform': 'None',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 12},\n 'sgd_momentum': {'type': 'float',\n  'default': 0.0,\n  'transform': 'None',\n  'lower': 0.0,\n  'upper': 1.0}}"
  },
  {
    "objectID": "11_spot_hpt_torch_fashion_mnist.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "href": "11_spot_hpt_torch_fashion_mnist.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "title": "11  HPT: PyTorch With fashionMNIST",
    "section": "11.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "11.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in Section 14.6.\n\n11.6.1 Modify hyperparameter of type numeric and integer (boolean)\nThe hyperparameter k_folds is not used, it is de-activated here by setting the lower and upper bound to the same value.\n\n\n\n\n\n\nCaution: Small net size, number of epochs, and patience for demonstration purposes\n\n\n\n\nNet sizes l1 and l2 as well as epochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nfun_control = modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[2, 7])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[7, 9]) and\nfun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[2, 7])\n\n\n\n\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\nfun_control = modify_hyper_parameter_bounds(fun_control, \"k_folds\", bounds=[0, 0])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[2, 2])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[2, 3])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[2, 5])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"l2\", bounds=[2, 5])\n\n\n\n11.6.2 Modify hyperparameter of type factor\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_levels\nfun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n\n\n\n11.6.3 Optimizers\nOptimizers are described in Section 14.6.1.\n\nfun_control = modify_hyper_parameter_bounds(fun_control,\n    \"lr_mult\", bounds=[1e-3, 1e-3])\nfun_control = modify_hyper_parameter_bounds(fun_control,\n    \"sgd_momentum\", bounds=[0.9, 0.9])"
  },
  {
    "objectID": "11_spot_hpt_torch_fashion_mnist.html#step-7-selection-of-the-objective-loss-function",
    "href": "11_spot_hpt_torch_fashion_mnist.html#step-7-selection-of-the-objective-loss-function",
    "title": "11  HPT: PyTorch With fashionMNIST",
    "section": "11.7 Step 7: Selection of the Objective (Loss) Function",
    "text": "11.7 Step 7: Selection of the Objective (Loss) Function\n\n11.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set and\nthe loss function (and a metric).\n\nThese are described in Section 19.7.1.\nThe key \"loss_function\" specifies the loss function which is used during the optimization, see Section 14.7.5.\nWe will use CrossEntropy loss for the multiclass-classification task.\n\nfrom torch.nn import CrossEntropyLoss\nloss_function = CrossEntropyLoss()\nfun_control.update({\n        \"loss_function\": loss_function,\n        \"shuffle\": True,\n        \"eval\":  \"train_hold_out\"\n        })\n\n\n\n11.7.2 Metric\n\nfrom torchmetrics import Accuracy\nmetric_torch = Accuracy(task=\"multiclass\", num_classes=10).to(fun_control[\"device\"])\nfun_control.update({\"metric_torch\": metric_torch})"
  },
  {
    "objectID": "11_spot_hpt_torch_fashion_mnist.html#step-8-calling-the-spot-function",
    "href": "11_spot_hpt_torch_fashion_mnist.html#step-8-calling-the-spot-function",
    "title": "11  HPT: PyTorch With fashionMNIST",
    "section": "11.8 Step 8: Calling the SPOT Function",
    "text": "11.8 Step 8: Calling the SPOT Function\n\n11.8.1 Preparing the SPOT Call\nThe following code passes the information about the parameter ranges and bounds to spot.\n\n# extract the variable types, names, and bounds\nfrom spotPython.hyperparameters.values import (get_bound_values,\n    get_var_name,\n    get_var_type,)\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name         | type   | default   |   lower |   upper | transform             |\n|--------------|--------|-----------|---------|---------|-----------------------|\n| l1           | int    | 5         |   2     |   5     | transform_power_2_int |\n| l2           | int    | 5         |   2     |   5     | transform_power_2_int |\n| lr_mult      | float  | 1.0       |   0.001 |   0.001 | None                  |\n| batch_size   | int    | 4         |   1     |   4     | transform_power_2_int |\n| epochs       | int    | 3         |   2     |   3     | transform_power_2_int |\n| k_folds      | int    | 1         |   0     |   0     | None                  |\n| patience     | int    | 5         |   2     |   2     | None                  |\n| optimizer    | factor | SGD       |   0     |   3     | None                  |\n| sgd_momentum | float  | 0.0       |   0.9   |   0.9   | None                  |\n\n\n\n\n11.8.2 The Objective Function fun_torch\nThe objective function fun_torch is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hypertorch import HyperTorch\nfun = HyperTorch().fun_torch\n\n\n\n11.8.3 Starting the Hyperparameter Tuning\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom math import inf\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\n\nconfig: {'l1': 16, 'l2': 8, 'lr_mult': 0.001, 'batch_size': 16, 'epochs': 8, 'k_folds': 0, 'patience': 2, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.1127916648983955 | Loss: 2.3084362339973450 | Acc: 0.1127916666666667.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1799583286046982 | Loss: 2.2982254233360289 | Acc: 0.1799583333333333.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.2217916697263718 | Loss: 2.2845832149187726 | Acc: 0.2217916666666667.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.2259166687726974 | Loss: 2.2671113478342693 | Acc: 0.2259166666666667.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.2225833386182785 | Loss: 2.2479916563034057 | Acc: 0.2225833333333333.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.2185833305120468 | Loss: 2.2290305687586467 | Acc: 0.2185833333333333.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.2202499955892563 | Loss: 2.2097116058667501 | Acc: 0.2202500000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.2203750014305115 | Loss: 2.1899441710313163 | Acc: 0.2203750000000000.\nReturned to Spot: Validation loss: 2.1899441710313163\n\nconfig: {'l1': 8, 'l2': 8, 'lr_mult': 0.001, 'batch_size': 8, 'epochs': 4, 'k_folds': 0, 'patience': 2, 'optimizer': 'Adamax', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.0990416631102562 | Loss: 2.3016376163164773 | Acc: 0.0990416666666667.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1334166675806046 | Loss: 2.2889192294279734 | Acc: 0.1334166666666667.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.1624583303928375 | Loss: 2.2782037033239999 | Acc: 0.1624583333333333.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.1758333295583725 | Loss: 2.2682299453417460 | Acc: 0.1758333333333333.\nReturned to Spot: Validation loss: 2.268229945341746\n\nconfig: {'l1': 32, 'l2': 16, 'lr_mult': 0.001, 'batch_size': 2, 'epochs': 8, 'k_folds': 0, 'patience': 2, 'optimizer': 'NAdam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.1028750017285347 | Loss: 2.1266853316823640 | Acc: 0.1028750000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.2412499934434891 | Loss: 1.9127329909801483 | Acc: 0.2412500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.4292083382606506 | Loss: 1.6879353840549787 | Acc: 0.4292083333333334.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.5465000271797180 | Loss: 1.4741579729293783 | Acc: 0.5465000000000000.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.5914583206176758 | Loss: 1.2921813233730695 | Acc: 0.5914583333333333.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.6236666440963745 | Loss: 1.1535102395123491 | Acc: 0.6236666666666667.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.6440416574478149 | Loss: 1.0508935779292756 | Acc: 0.6440416666666666.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.6568333506584167 | Loss: 0.9772411707276478 | Acc: 0.6568333333333334.\nReturned to Spot: Validation loss: 0.9772411707276478\n\nconfig: {'l1': 4, 'l2': 8, 'lr_mult': 0.001, 'batch_size': 4, 'epochs': 4, 'k_folds': 0, 'patience': 2, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.1547500044107437 | Loss: 2.2885712475776674 | Acc: 0.1547500000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1829583346843719 | Loss: 2.2726075224479039 | Acc: 0.1829583333333333.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.1968750059604645 | Loss: 2.2590436285336812 | Acc: 0.1968750000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.2182916700839996 | Loss: 2.2458393644889196 | Acc: 0.2182916666666667.\nReturned to Spot: Validation loss: 2.2458393644889196\n\nconfig: {'l1': 16, 'l2': 32, 'lr_mult': 0.001, 'batch_size': 8, 'epochs': 8, 'k_folds': 0, 'patience': 2, 'optimizer': 'Adam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.1119166687130928 | Loss: 2.2292482392390567 | Acc: 0.1119166666666667.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1229583323001862 | Loss: 2.1705849734147389 | Acc: 0.1229583333333333.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.1270000040531158 | Loss: 2.1184123195012412 | Acc: 0.1270000000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.1449583321809769 | Loss: 2.0690761794646582 | Acc: 0.1449583333333333.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.2176250070333481 | Loss: 2.0184855046272276 | Acc: 0.2176250000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.2930833399295807 | Loss: 1.9656850552558900 | Acc: 0.2930833333333333.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.3683750033378601 | Loss: 1.9095736619631449 | Acc: 0.3683750000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.4314583241939545 | Loss: 1.8520497523148856 | Acc: 0.4314583333333333.\nReturned to Spot: Validation loss: 1.8520497523148856\n\n\n\nconfig: {'l1': 8, 'l2': 16, 'lr_mult': 0.001, 'batch_size': 8, 'epochs': 8, 'k_folds': 0, 'patience': 2, 'optimizer': 'NAdam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.1011250019073486 | Loss: 2.2754788013299305 | Acc: 0.1011250000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1067499965429306 | Loss: 2.2424170796473821 | Acc: 0.1067500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.1271666735410690 | Loss: 2.2120091686248777 | Acc: 0.1271666666666667.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.1388333290815353 | Loss: 2.1793690120379132 | Acc: 0.1388333333333333.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.1484166681766510 | Loss: 2.1439755277236303 | Acc: 0.1484166666666667.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.1517916619777679 | Loss: 2.1053277701536812 | Acc: 0.1517916666666667.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.1670416593551636 | Loss: 2.0640378013451892 | Acc: 0.1670416666666667.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.2350416630506516 | Loss: 2.0202311071554822 | Acc: 0.2350416666666667.\nReturned to Spot: Validation loss: 2.020231107155482\n\n\nspotPython tuning: 0.9772411707276478 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x18a5b2350&gt;"
  },
  {
    "objectID": "11_spot_hpt_torch_fashion_mnist.html#sec-tensorboard-11",
    "href": "11_spot_hpt_torch_fashion_mnist.html#sec-tensorboard-11",
    "title": "11  HPT: PyTorch With fashionMNIST",
    "section": "11.9 Step 9: Tensorboard",
    "text": "11.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard as described in Section 14.9, see also the description in the documentation: Tensorboard."
  },
  {
    "objectID": "11_spot_hpt_torch_fashion_mnist.html#sec-results-tuning-11",
    "href": "11_spot_hpt_torch_fashion_mnist.html#sec-results-tuning-11",
    "title": "11  HPT: PyTorch With fashionMNIST",
    "section": "11.10 Step 10: Results",
    "text": "11.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed as described in Section 14.10.\n\nSAVE = False\nLOAD = False\n\nif SAVE:\n    result_file_name = \"res_\" + experiment_name + \".pkl\"\n    with open(result_file_name, 'wb') as f:\n        pickle.dump(spot_tuner, f)\n\nif LOAD:\n    result_file_name = \"ADD THE NAME here, e.g.: res_ch10-friedman-hpt-0_maans03_60min_20init_1K_2023-04-14_10-11-19.pkl\"\n    with open(result_file_name, 'rb') as f:\n        spot_tuner =  pickle.load(f)\n\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The following code generates the progress plot from ?fig-progress.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nPrint the results\n\n\nprint(gen_design_table(fun_control=fun_control,\n    spot=spot_tuner))\n\n| name         | type   | default   |   lower |   upper |   tuned | transform             |   importance | stars   |\n|--------------|--------|-----------|---------|---------|---------|-----------------------|--------------|---------|\n| l1           | int    | 5         |     2.0 |     5.0 |     5.0 | transform_power_2_int |       100.00 | ***     |\n| l2           | int    | 5         |     2.0 |     5.0 |     4.0 | transform_power_2_int |         0.00 |         |\n| lr_mult      | float  | 1.0       |   0.001 |   0.001 |   0.001 | None                  |         0.00 |         |\n| batch_size   | int    | 4         |     1.0 |     4.0 |     1.0 | transform_power_2_int |        66.19 | **      |\n| epochs       | int    | 3         |     2.0 |     3.0 |     3.0 | transform_power_2_int |         0.00 |         |\n| k_folds      | int    | 1         |     0.0 |     0.0 |     0.0 | None                  |         0.00 |         |\n| patience     | int    | 5         |     2.0 |     2.0 |     2.0 | None                  |         0.00 |         |\n| optimizer    | factor | SGD       |     0.0 |     3.0 |     3.0 | None                  |         0.01 |         |\n| sgd_momentum | float  | 0.0       |     0.9 |     0.9 |     0.9 | None                  |         0.00 |         |\n\n\n\n11.10.1 Show variable importance\n\nspot_tuner.plot_importance(threshold=0.025, filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n\n11.10.2 Get the Tuned Architecture (SPOT Results)\nThe architecture of the spotPython model can be obtained by the following code:\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\nmodel_spot\n\nNet_fashionMNIST(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=32, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=32, out_features=16, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=16, out_features=10, bias=True)\n  )\n)\n\n\n\n\n11.10.3 Get Default Hyperparameters\n\nfc = fun_control\nfc.update({\"core_model_hyper_dict\":\n    hyper_dict[fun_control[\"core_model\"].__name__]})\nmodel_default = get_one_core_model_from_X(X_start, fun_control=fc)\nmodel_default\n\nNet_fashionMNIST(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=32, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=32, out_features=32, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=32, out_features=10, bias=True)\n  )\n)\n\n\n\n\n11.10.4 Evaluation of the Default and the Tuned Architectures\nThe method train_tuned takes a model architecture without trained weights and trains this model with the train data. The train data is split into train and validation data. The validation data is used for early stopping. The trained model weights are saved as a dictionary.\n\nfrom spotPython.torch.traintest import train_tuned\ntrain_tuned(net=model_default, train_dataset=train, shuffle=True,\n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        device = fun_control[\"device\"],\n        show_batch_interval=1_000_000,\n        path=None,\n        task=fun_control[\"task\"])\n\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.2706249952316284 | Loss: 2.0137663456598918 | Acc: 0.2706250000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.5299583077430725 | Loss: 1.5507222069104512 | Acc: 0.5299583333333333.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.5678750276565552 | Loss: 1.2516097009181977 | Acc: 0.5678750000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.6405833363533020 | Loss: 1.0870934034188589 | Acc: 0.6405833333333333.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.6556249856948853 | Loss: 0.9865004358688990 | Acc: 0.6556250000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.6712499856948853 | Loss: 0.9189888942241669 | Acc: 0.6712500000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.6834999918937683 | Loss: 0.8704074752330780 | Acc: 0.6835000000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.6937083601951599 | Loss: 0.8311609677672386 | Acc: 0.6937083333333334.\nReturned to Spot: Validation loss: 0.8311609677672386\n\n\n\nfrom spotPython.torch.traintest import test_tuned\ntest_tuned(net=model_default, test_dataset=test, \n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        shuffle=False, \n        device = fun_control[\"device\"],\n        task=fun_control[\"task\"])\n\nMulticlassAccuracy: 0.6790999770164490 | Loss: 0.8542572569370270 | Acc: 0.6791000000000000.\nFinal evaluation: Validation loss: 0.854257256937027\nFinal evaluation: Validation metric: 0.679099977016449\n----------------------------------------------\n\n\n(0.854257256937027, nan, tensor(0.6791))\n\n\nThe following code trains the model model_spot. If path is set to a filename, e.g., path = \"model_spot_trained.pt\", the weights of the trained model will be saved to this file.\n\ntrain_tuned(net=model_spot, train_dataset=train,\n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        shuffle=True,\n        device = fun_control[\"device\"],\n        path=None,\n        task=fun_control[\"task\"])\n\nEpoch: 1 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 2.263\n\n\nMulticlassAccuracy: 0.3281250000000000 | Loss: 2.1010061793824035 | Acc: 0.3281250000000000.\nEpoch: 2 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 2.028\n\n\nMulticlassAccuracy: 0.5154583454132080 | Loss: 1.8540721909999847 | Acc: 0.5154583333333334.\nEpoch: 3 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 1.777\n\n\nMulticlassAccuracy: 0.5368333458900452 | Loss: 1.6208453026513259 | Acc: 0.5368333333333334.\nEpoch: 4 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 1.561\n\n\nMulticlassAccuracy: 0.5482916831970215 | Loss: 1.4171262048482896 | Acc: 0.5482916666666666.\nEpoch: 5 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 1.358\n\n\nMulticlassAccuracy: 0.5593749880790710 | Loss: 1.2455760520622134 | Acc: 0.5593750000000000.\nEpoch: 6 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 1.196\n\n\nMulticlassAccuracy: 0.5910833477973938 | Loss: 1.1079711391432210 | Acc: 0.5910833333333333.\nEpoch: 7 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 1.070\n\n\nMulticlassAccuracy: 0.6344583630561829 | Loss: 1.0059068728210405 | Acc: 0.6344583333333333.\nEpoch: 8 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 0.979\n\n\nMulticlassAccuracy: 0.6506666541099548 | Loss: 0.9361676749548254 | Acc: 0.6506666666666666.\nReturned to Spot: Validation loss: 0.9361676749548254\n\n\n\ntest_tuned(net=model_spot, test_dataset=test,\n            shuffle=False,\n            loss_function=fun_control[\"loss_function\"],\n            metric=fun_control[\"metric_torch\"],\n            device = fun_control[\"device\"],\n            task=fun_control[\"task\"])\n\nMulticlassAccuracy: 0.6460000276565552 | Loss: 0.9461929223641753 | Acc: 0.6460000000000000.\nFinal evaluation: Validation loss: 0.9461929223641753\nFinal evaluation: Validation metric: 0.6460000276565552\n----------------------------------------------\n\n\n(0.9461929223641753, nan, tensor(0.6460))\n\n\n\n\n11.10.5 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nl1:  100.00000000000001\nbatch_size:  66.19173444065022\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n11.10.6 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n                                                \nParallel coordinates plots\n\n\n\n\n11.10.7 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
  },
  {
    "objectID": "12_spot_hpt_torch_cifar10.html#sec-setup-12",
    "href": "12_spot_hpt_torch_cifar10.html#sec-setup-12",
    "title": "12  HPT: PyTorch With cifar10 Data",
    "section": "12.1 Step 1: Setup",
    "text": "12.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size and the device that is used.\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nThe device can be selected by setting the variable DEVICE.\nSince we are using a simple neural net, the setting \"cpu\" is preferred (on Mac).\nIf you have a GPU, you can use \"cuda:0\" instead.\nIf DEVICE is set to None, spotPython will automatically select the device.\n\nThis might result in \"mps\" on Macs, which is not the best choice for simple neural nets.\n\n\n\n\n\nMAX_TIME = 1\nINIT_SIZE = 5\nDEVICE = \"cpu\" # \"cuda:0\" None\n\n\nfrom spotPython.utils.device import getDevice\nDEVICE = getDevice(DEVICE)\nprint(DEVICE)\n\ncpu\n\n\n\nimport os\nimport copy\nimport socket\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '12-torch' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\n\n12-torch_maans03_1min_5init_2023-06-28_01-47-07"
  },
  {
    "objectID": "12_spot_hpt_torch_cifar10.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "href": "12_spot_hpt_torch_cifar10.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "title": "12  HPT: PyTorch With cifar10 Data",
    "section": "12.2 Step 2: Initialization of the Empty fun_control Dictionary",
    "text": "12.2 Step 2: Initialization of the Empty fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process, which was described in Section 14.2.\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"classification\",\n    tensorboard_path=\"runs/12_spot_hpt_torch_cifar10\",\n    device=DEVICE)"
  },
  {
    "objectID": "12_spot_hpt_torch_cifar10.html#sec-data-loading-12",
    "href": "12_spot_hpt_torch_cifar10.html#sec-data-loading-12",
    "title": "12  HPT: PyTorch With cifar10 Data",
    "section": "12.3 Step 3: PyTorch Data Loading",
    "text": "12.3 Step 3: PyTorch Data Loading\n\n12.3.1 Load Data Cifar10 Data\n\nfrom torchvision import datasets, transforms\nimport torchvision\ndef load_data(data_dir=\"./data\"):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    trainset = torchvision.datasets.CIFAR10(\n        root=data_dir, train=True, download=True, transform=transform)\n\n    testset = torchvision.datasets.CIFAR10(\n        root=data_dir, train=False, download=True, transform=transform)\n\n    return trainset, testset\ntrain, test = load_data()\n\nFiles already downloaded and verified\n\n\nFiles already downloaded and verified\n\n\n\nSince this works fine, we can add the data loading to the fun_control dictionary:\n\n\nn_samples = len(train)\n# add the dataset to the fun_control\nfun_control.update({\"data\": None, # dataset,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": None})"
  },
  {
    "objectID": "12_spot_hpt_torch_cifar10.html#sec-specification-of-preprocessing-model-12",
    "href": "12_spot_hpt_torch_cifar10.html#sec-specification-of-preprocessing-model-12",
    "title": "12  HPT: PyTorch With cifar10 Data",
    "section": "12.4 Step 4: Specification of the Preprocessing Model",
    "text": "12.4 Step 4: Specification of the Preprocessing Model\nAfter the training and test data are specified and added to the fun_control dictionary, spotPython allows the specification of a data preprocessing pipeline, e.g., for the scaling of the data or for the one-hot encoding of categorical variables, see Section 14.4. This feature is not used here, so we do not change the default value (which is None)."
  },
  {
    "objectID": "12_spot_hpt_torch_cifar10.html#sec-selection-of-the-algorithm-12",
    "href": "12_spot_hpt_torch_cifar10.html#sec-selection-of-the-algorithm-12",
    "title": "12  HPT: PyTorch With cifar10 Data",
    "section": "12.5 Step 5: Select Model (algorithm) and core_model_hyper_dict",
    "text": "12.5 Step 5: Select Model (algorithm) and core_model_hyper_dict\n\n12.5.1 Implementing a Configurable Neural Network With spotPython\nspotPython includes the Net_CIFAR10 class which is implemented in the file netcifar10.py. The class is imported here.\nThis class inherits from the class Net_Core which is implemented in the file netcore.py, see Section 14.5.1.\n\nfrom spotPython.torch.netcifar10 import Net_CIFAR10\nfrom spotPython.data.torch_hyper_dict import TorchHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfun_control = add_core_model_to_fun_control(core_model=Net_CIFAR10,\n                              fun_control=fun_control,\n                              hyper_dict=TorchHyperDict,\n                              filename=None)\n\n\n\n12.5.2 The Search Space\n\n\n12.5.3 Configuring the Search Space With spotPython\n\n12.5.3.1 The hyper_dict Hyperparameters for the Selected Algorithm\nspotPython uses JSON files for the specification of the hyperparameters, which were described in Section 14.5.5.\nThe corresponding entries for the core_model class are shown below.\n\nfun_control['core_model_hyper_dict']\n\n{'l1': {'type': 'int',\n  'default': 5,\n  'transform': 'transform_power_2_int',\n  'lower': 2,\n  'upper': 9},\n 'l2': {'type': 'int',\n  'default': 5,\n  'transform': 'transform_power_2_int',\n  'lower': 2,\n  'upper': 9},\n 'lr_mult': {'type': 'float',\n  'default': 1.0,\n  'transform': 'None',\n  'lower': 0.1,\n  'upper': 10.0},\n 'batch_size': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 1,\n  'upper': 4},\n 'epochs': {'type': 'int',\n  'default': 3,\n  'transform': 'transform_power_2_int',\n  'lower': 3,\n  'upper': 4},\n 'k_folds': {'type': 'int',\n  'default': 1,\n  'transform': 'None',\n  'lower': 1,\n  'upper': 1},\n 'patience': {'type': 'int',\n  'default': 5,\n  'transform': 'None',\n  'lower': 2,\n  'upper': 10},\n 'optimizer': {'levels': ['Adadelta',\n   'Adagrad',\n   'Adam',\n   'AdamW',\n   'SparseAdam',\n   'Adamax',\n   'ASGD',\n   'NAdam',\n   'RAdam',\n   'RMSprop',\n   'Rprop',\n   'SGD'],\n  'type': 'factor',\n  'default': 'SGD',\n  'transform': 'None',\n  'class_name': 'torch.optim',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 12},\n 'sgd_momentum': {'type': 'float',\n  'default': 0.0,\n  'transform': 'None',\n  'lower': 0.0,\n  'upper': 1.0}}"
  },
  {
    "objectID": "12_spot_hpt_torch_cifar10.html#sec-modification-of-hyperparameters-12",
    "href": "12_spot_hpt_torch_cifar10.html#sec-modification-of-hyperparameters-12",
    "title": "12  HPT: PyTorch With cifar10 Data",
    "section": "12.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "12.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in Section 14.6.\n\n12.6.1 Step 5: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\n\n12.6.1.1 Modify Hyperparameters of Type numeric and integer (boolean)\nThe hyperparameter k_folds is not used, it is de-activated here by setting the lower and upper bound to the same value.\n\n\n\n\n\n\nCaution: Small net size, number of epochs, and patience for demonstration purposes\n\n\n\n\nNet sizes l1 and l2 as well as epochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nfun_control = modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[2, 7])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[7, 9]) and\nfun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[2, 7])\n\n\n\n\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\nfun_control = modify_hyper_parameter_bounds(fun_control, \"k_folds\", bounds=[0, 0])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[2, 2])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[2, 3])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[2, 5])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"l2\", bounds=[2, 5])\n\n\n\n\n12.6.2 Modify hyperparameter of type factor\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_levels\nfun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n\n\n\n12.6.3 Optimizers\nOptimizers can be selected as described in Section 19.6.2.\nOptimizers are described in Section 14.6.1.\n\nfun_control = modify_hyper_parameter_bounds(fun_control,\n    \"lr_mult\", bounds=[1e-3, 1e-3])\nfun_control = modify_hyper_parameter_bounds(fun_control,\n    \"sgd_momentum\", bounds=[0.9, 0.9])"
  },
  {
    "objectID": "12_spot_hpt_torch_cifar10.html#step-7-selection-of-the-objective-loss-function",
    "href": "12_spot_hpt_torch_cifar10.html#step-7-selection-of-the-objective-loss-function",
    "title": "12  HPT: PyTorch With cifar10 Data",
    "section": "12.7 Step 7: Selection of the Objective (Loss) Function",
    "text": "12.7 Step 7: Selection of the Objective (Loss) Function\n\n12.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set and\nthe loss function (and a metric).\n\nThese are described in Section 19.7.1.\nThe key \"loss_function\" specifies the loss function which is used during the optimization, see Section 14.7.5.\nWe will use CrossEntropy loss for the multiclass-classification task.\n\nfrom torch.nn import CrossEntropyLoss\nloss_function = CrossEntropyLoss()\nfun_control.update({\n        \"loss_function\": loss_function,\n        \"shuffle\": True,\n        \"eval\":  \"train_hold_out\"\n        })\n\n\n\n12.7.2 Metric\n\nimport torchmetrics\nmetric_torch = torchmetrics.Accuracy(task=\"multiclass\",\n     num_classes=10).to(fun_control[\"device\"])\nfun_control.update({\"metric_torch\": metric_torch})"
  },
  {
    "objectID": "12_spot_hpt_torch_cifar10.html#step-8-calling-the-spot-function",
    "href": "12_spot_hpt_torch_cifar10.html#step-8-calling-the-spot-function",
    "title": "12  HPT: PyTorch With cifar10 Data",
    "section": "12.8 Step 8: Calling the SPOT Function",
    "text": "12.8 Step 8: Calling the SPOT Function\n\n12.8.1 Preparing the SPOT Call\nThe following code passes the information about the parameter ranges and bounds to spot.\n\n# extract the variable types, names, and bounds\nfrom spotPython.hyperparameters.values import (get_bound_values,\n    get_var_name,\n    get_var_type,)\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name         | type   | default   |   lower |   upper | transform             |\n|--------------|--------|-----------|---------|---------|-----------------------|\n| l1           | int    | 5         |   2     |   5     | transform_power_2_int |\n| l2           | int    | 5         |   2     |   5     | transform_power_2_int |\n| lr_mult      | float  | 1.0       |   0.001 |   0.001 | None                  |\n| batch_size   | int    | 4         |   1     |   4     | transform_power_2_int |\n| epochs       | int    | 3         |   2     |   3     | transform_power_2_int |\n| k_folds      | int    | 1         |   0     |   0     | None                  |\n| patience     | int    | 5         |   2     |   2     | None                  |\n| optimizer    | factor | SGD       |   0     |   3     | None                  |\n| sgd_momentum | float  | 0.0       |   0.9   |   0.9   | None                  |\n\n\n\n\n12.8.2 The Objective Function fun_torch\nThe objective function fun_torch is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hypertorch import HyperTorch\nfun = HyperTorch().fun_torch\n\n\n\n12.8.3 Starting the Hyperparameter Tuning\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom math import inf\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\n\nconfig: {'l1': 16, 'l2': 8, 'lr_mult': 0.001, 'batch_size': 16, 'epochs': 8, 'k_folds': 0, 'patience': 2, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.1023000031709671 | Loss: 2.3132927709579469 | Acc: 0.1023000000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1023499965667725 | Loss: 2.3128030347824096 | Acc: 0.1023500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.1025499999523163 | Loss: 2.3122524883270263 | Acc: 0.1025500000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.1032999977469444 | Loss: 2.3116444206237792 | Acc: 0.1033000000000000.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.1043500006198883 | Loss: 2.3109940061569212 | Acc: 0.1043500000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.1056500002741814 | Loss: 2.3103290613174439 | Acc: 0.1056500000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.1086999997496605 | Loss: 2.3096735733032228 | Acc: 0.1087000000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.1108999997377396 | Loss: 2.3090176872253418 | Acc: 0.1109000000000000.\nReturned to Spot: Validation loss: 2.309017687225342\n\nconfig: {'l1': 8, 'l2': 8, 'lr_mult': 0.001, 'batch_size': 8, 'epochs': 4, 'k_folds': 0, 'patience': 2, 'optimizer': 'Adamax', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.0996500030159950 | Loss: 2.3211361103057859 | Acc: 0.0996500000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.0993999987840652 | Loss: 2.3205232890129088 | Acc: 0.0994000000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.0999500006437302 | Loss: 2.3198855847358701 | Acc: 0.0999500000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.0993999987840652 | Loss: 2.3191987520217894 | Acc: 0.0994000000000000.\nReturned to Spot: Validation loss: 2.3191987520217894\n\nconfig: {'l1': 32, 'l2': 16, 'lr_mult': 0.001, 'batch_size': 2, 'epochs': 8, 'k_folds': 0, 'patience': 2, 'optimizer': 'NAdam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.1107499971985817 | Loss: 2.3015510084629058 | Acc: 0.1107500000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1621000021696091 | Loss: 2.2826788492679597 | Acc: 0.1621000000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.1717000007629395 | Loss: 2.2445420648455618 | Acc: 0.1717000000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.1966000050306320 | Loss: 2.1953941403150559 | Acc: 0.1966000000000000.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.2189999967813492 | Loss: 2.1503316965460777 | Acc: 0.2190000000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.2285500019788742 | Loss: 2.1127929374575616 | Acc: 0.2285500000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.2369499951601028 | Loss: 2.0803561464428904 | Acc: 0.2369500000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.2454500049352646 | Loss: 2.0499279996871946 | Acc: 0.2454500000000000.\nReturned to Spot: Validation loss: 2.0499279996871946\n\nconfig: {'l1': 4, 'l2': 8, 'lr_mult': 0.001, 'batch_size': 4, 'epochs': 4, 'k_folds': 0, 'patience': 2, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.0988000035285950 | Loss: 2.3305476430177690 | Acc: 0.0988000000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.0988000035285950 | Loss: 2.3284217825889586 | Acc: 0.0988000000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.0988000035285950 | Loss: 2.3259105927228929 | Acc: 0.0988000000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.0988000035285950 | Loss: 2.3230893067836762 | Acc: 0.0988000000000000.\nReturned to Spot: Validation loss: 2.323089306783676\n\nconfig: {'l1': 16, 'l2': 32, 'lr_mult': 0.001, 'batch_size': 8, 'epochs': 8, 'k_folds': 0, 'patience': 2, 'optimizer': 'Adam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.1044000014662743 | Loss: 2.3046556637763977 | Acc: 0.1044000000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1035000011324883 | Loss: 2.3038212775230407 | Acc: 0.1035000000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.1031000018119812 | Loss: 2.3028934612274168 | Acc: 0.1031000000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.1031000018119812 | Loss: 2.3018476922988893 | Acc: 0.1031000000000000.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.1033999994397163 | Loss: 2.3005531357765197 | Acc: 0.1034000000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.1041999980807304 | Loss: 2.2988326036453248 | Acc: 0.1042000000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.1052500009536743 | Loss: 2.2967097506523131 | Acc: 0.1052500000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.1070000007748604 | Loss: 2.2941469674110411 | Acc: 0.1070000000000000.\nReturned to Spot: Validation loss: 2.294146967411041\n\n\n\nconfig: {'l1': 8, 'l2': 16, 'lr_mult': 0.001, 'batch_size': 8, 'epochs': 8, 'k_folds': 0, 'patience': 2, 'optimizer': 'NAdam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.1002499982714653 | Loss: 2.3088926066398621 | Acc: 0.1002500000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1002499982714653 | Loss: 2.3064979212760925 | Acc: 0.1002500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.1002499982714653 | Loss: 2.3042280316352843 | Acc: 0.1002500000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.1022000014781952 | Loss: 2.3013036189079283 | Acc: 0.1022000000000000.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.1187999993562698 | Loss: 2.2966349051475525 | Acc: 0.1188000000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.1427499949932098 | Loss: 2.2929255472183225 | Acc: 0.1427500000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.1569499969482422 | Loss: 2.2892267224311831 | Acc: 0.1569500000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.1626500040292740 | Loss: 2.2848128066062929 | Acc: 0.1626500000000000.\nReturned to Spot: Validation loss: 2.284812806606293\n\n\nspotPython tuning: 2.0499279996871946 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x187e83b20&gt;"
  },
  {
    "objectID": "12_spot_hpt_torch_cifar10.html#sec-tensorboard-10",
    "href": "12_spot_hpt_torch_cifar10.html#sec-tensorboard-10",
    "title": "12  HPT: PyTorch With cifar10 Data",
    "section": "12.9 Step 9: Tensorboard",
    "text": "12.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard as described in Section 14.9, see also the description in the documentation: Tensorboard."
  },
  {
    "objectID": "12_spot_hpt_torch_cifar10.html#sec-results-tuning-12",
    "href": "12_spot_hpt_torch_cifar10.html#sec-results-tuning-12",
    "title": "12  HPT: PyTorch With cifar10 Data",
    "section": "12.10 Step 10: Results",
    "text": "12.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed as described in Section 14.10.\n\nSAVE = False\nLOAD = False\n\nif SAVE:\n    result_file_name = \"res_\" + experiment_name + \".pkl\"\n    with open(result_file_name, 'wb') as f:\n        pickle.dump(spot_tuner, f)\n\nif LOAD:\n    result_file_name = \"ADD THE NAME here, e.g.: res_ch10-friedman-hpt-0_maans03_60min_20init_1K_2023-04-14_10-11-19.pkl\"\n    with open(result_file_name, 'rb') as f:\n        spot_tuner =  pickle.load(f)\n\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The following code generates the progress plot from ?fig-progress.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nPrint the results\n\n\nprint(gen_design_table(fun_control=fun_control,\n    spot=spot_tuner))\n\n| name         | type   | default   |   lower |   upper |   tuned | transform             |   importance | stars   |\n|--------------|--------|-----------|---------|---------|---------|-----------------------|--------------|---------|\n| l1           | int    | 5         |     2.0 |     5.0 |     5.0 | transform_power_2_int |        11.70 | *       |\n| l2           | int    | 5         |     2.0 |     5.0 |     4.0 | transform_power_2_int |         0.00 |         |\n| lr_mult      | float  | 1.0       |   0.001 |   0.001 |   0.001 | None                  |         0.00 |         |\n| batch_size   | int    | 4         |     1.0 |     4.0 |     1.0 | transform_power_2_int |        18.47 | *       |\n| epochs       | int    | 3         |     2.0 |     3.0 |     3.0 | transform_power_2_int |         0.00 |         |\n| k_folds      | int    | 1         |     0.0 |     0.0 |     0.0 | None                  |         0.00 |         |\n| patience     | int    | 5         |     2.0 |     2.0 |     2.0 | None                  |         0.00 |         |\n| optimizer    | factor | SGD       |     0.0 |     3.0 |     3.0 | None                  |       100.00 | ***     |\n| sgd_momentum | float  | 0.0       |     0.9 |     0.9 |     0.9 | None                  |         0.00 |         |\n\n\n\n12.10.1 Show variable importance\n\nspot_tuner.plot_importance(threshold=0.025, filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n\n12.10.2 Get the Tuned Architecture (SPOT Results)\nThe architecture of the spotPython model can be obtained by the following code:\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\nmodel_spot\n\nNet_CIFAR10(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=32, bias=True)\n  (fc2): Linear(in_features=32, out_features=16, bias=True)\n  (fc3): Linear(in_features=16, out_features=10, bias=True)\n)\n\n\n\n\n12.10.3 Evaluation of the Tuned Architecture\n\nfrom spotPython.torch.traintest import (\n    train_tuned,\n    test_tuned,\n    )\n\n\ntrain_tuned(net=model_spot, train_dataset=train,\n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        shuffle=True,\n        device = fun_control[\"device\"],\n        path=None,\n        task=fun_control[\"task\"],)\n\nEpoch: 1 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 2.316\n\n\nMulticlassAccuracy: 0.1581999957561493 | Loss: 2.3097913815736772 | Acc: 0.1582000000000000.\nEpoch: 2 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 2.309\n\n\nMulticlassAccuracy: 0.1313000023365021 | Loss: 2.2971530404686926 | Acc: 0.1313000000000000.\nEpoch: 3 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 2.293\n\n\nMulticlassAccuracy: 0.1748500019311905 | Loss: 2.2717991746068003 | Acc: 0.1748500000000000.\nEpoch: 4 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 2.262\n\n\nMulticlassAccuracy: 0.1747500002384186 | Loss: 2.2315638593673706 | Acc: 0.1747500000000000.\nEpoch: 5 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 2.219\n\n\nMulticlassAccuracy: 0.1750999987125397 | Loss: 2.1826935840725898 | Acc: 0.1751000000000000.\nEpoch: 6 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 2.167\n\n\nMulticlassAccuracy: 0.1771000027656555 | Loss: 2.1364990820407868 | Acc: 0.1771000000000000.\nEpoch: 7 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 2.125\n\n\nMulticlassAccuracy: 0.1833499968051910 | Loss: 2.0992731188535689 | Acc: 0.1833500000000000.\nEpoch: 8 | \n\n\nBatch: 10000. Batch Size: 2. Training Loss (running): 2.095\n\n\nMulticlassAccuracy: 0.1932000070810318 | Loss: 2.0683008516907693 | Acc: 0.1932000000000000.\nReturned to Spot: Validation loss: 2.0683008516907693\n\n\nIf path is set to a filename, e.g., path = \"model_spot_trained.pt\", the weights of the trained model will be loaded from this file.\n\ntest_tuned(net=model_spot, test_dataset=test,\n            shuffle=False,\n            loss_function=fun_control[\"loss_function\"],\n            metric=fun_control[\"metric_torch\"],\n            device = fun_control[\"device\"],\n            task=fun_control[\"task\"],)\n\nMulticlassAccuracy: 0.1932000070810318 | Loss: 2.0655270670652390 | Acc: 0.1932000000000000.\nFinal evaluation: Validation loss: 2.065527067065239\nFinal evaluation: Validation metric: 0.1932000070810318\n----------------------------------------------\n\n\n(2.065527067065239, nan, tensor(0.1932))\n\n\n\n\n12.10.4 Cross-validated Evaluations\n\n\n\n\n\n\nCaution: Cross-validated Evaluations\n\n\n\n\nThe number of folds is set to 1 by default.\nHere it was changed to 3 for demonstration purposes.\nSet the number of folds to a reasonable value, e.g., 10.\nThis can be done by setting the k_folds attribute of the model as follows:\nsetattr(model_spot, \"k_folds\",  10)\n\n\n\n\nfrom spotPython.torch.traintest import evaluate_cv\n# modify k-kolds:\nsetattr(model_spot, \"k_folds\",  3)\ndf_eval, df_preds, df_metrics = evaluate_cv(net=model_spot,\n            dataset=fun_control[\"data\"],\n            loss_function=fun_control[\"loss_function\"],\n            metric=fun_control[\"metric_torch\"],\n            task=fun_control[\"task\"],\n            writer=fun_control[\"writer\"],\n            writerId=\"model_spot_cv\",\n            device = fun_control[\"device\"])\n\nError in Net_Core. Call to evaluate_cv() failed. err=TypeError(\"Expected sequence or array-like, got &lt;class 'NoneType'&gt;\"), type(err)=&lt;class 'TypeError'&gt;\n\n\n\nmetric_name = type(fun_control[\"metric_torch\"]).__name__\nprint(f\"loss: {df_eval}, Cross-validated {metric_name}: {df_metrics}\")\n\nloss: nan, Cross-validated MulticlassAccuracy: nan\n\n\n\n\n12.10.5 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nl1:  11.697909905761664\nbatch_size:  18.468997595549197\noptimizer:  100.0\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n\n\n\n\n\n\n12.10.6 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n                                                \nParallel coordinates plots\n\n\n\n\n12.10.7 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
  },
  {
    "objectID": "13_spot_hpt_river.html#sec-setup-24",
    "href": "13_spot_hpt_river.html#sec-setup-24",
    "title": "13  HPT: River",
    "section": "13.1 Step 1: Setup",
    "text": "13.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size and the device that is used.\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nK is set to 0.1 for demonstration purposes. For real experiments, this should be increased to at least 1.\n\n\n\n\nMAX_TIME = 1\nINIT_SIZE = 5\nK = .1\n\n\n\n10-river_maans03_1min_5init_2023-06-28_02-34-38\n\n\n\n13.1.1 river Hyperparameter Tuning: HATR with Friedman Drift Data\n\nThis notebook exemplifies hyperparameter tuning with SPOT (spotPython and spotRiver).\nThe hyperparameter software SPOT was developed in R (statistical programming language), see Open Access book “Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide”, available here: https://link.springer.com/book/10.1007/978-981-19-5170-1.\nThis notebook demonstrates hyperparameter tuning for river. It is based on the notebook “Incremental decision trees in river: the Hoeffding Tree case”, see: https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/#42-regression-tree-splitters.\nHere we will use the river HTR and HATR functions as in “Incremental decision trees in river: the Hoeffding Tree case”, see: https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/#42-regression-tree-splitters.\n\n\npip list | grep  \"spot[RiverPython]\"\n\nspotPython                        0.2.50\nspotRiver                         0.0.94\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n# import sys\n# !{sys.executable} -m pip install --upgrade build\n# !{sys.executable} -m pip install --upgrade --force-reinstall spotPython"
  },
  {
    "objectID": "13_spot_hpt_river.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "13_spot_hpt_river.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "13  HPT: River",
    "section": "13.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "13.2 Step 2: Initialization of the fun_control Dictionary\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"regression\",\n tensorboard_path=None)"
  },
  {
    "objectID": "13_spot_hpt_river.html#step-3-load-the-friedman-drift-data",
    "href": "13_spot_hpt_river.html#step-3-load-the-friedman-drift-data",
    "title": "13  HPT: River",
    "section": "13.3 Step 3: Load the Friedman Drift Data",
    "text": "13.3 Step 3: Load the Friedman Drift Data\n\nhorizon = 7*24\nk = K\nn_total = int(k*100_000)\nn_samples = n_total\np_1 = int(k*25_000)\np_2 = int(k*50_000)\nposition=(p_1, p_2)\nn_train = 1_000\na = n_train + p_1 - 12\nb = a + 12\n\n\nSince we also need a river version of the data below for plotting the model, the corresponding data set is generated here. Note: spotRiver uses the train and test data sets, while river uses the X and y data sets\n\n\nfrom river.datasets import synth\nimport pandas as pd\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=position,\n     seed=123\n)\ndata_dict = {key: [] for key in list(dataset.take(1))[0][0].keys()}\ndata_dict[\"y\"] = []\nfor x, y in dataset.take(n_total):\n    for key, value in x.items():\n        data_dict[key].append(value)\n    data_dict[\"y\"].append(y)\ndf = pd.DataFrame(data_dict)\n# Add column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column\ndf.columns = [f\"x{i}\" for i in range(1, 11)] + [\"y\"]\n\ntrain = df[:n_train]\ntest = df[n_train:]\ntarget_column = \"y\"\n#\nfun_control.update({\"data\": None, # dataset,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": target_column})"
  },
  {
    "objectID": "13_spot_hpt_river.html#step-4-specification-of-the-preprocessing-model",
    "href": "13_spot_hpt_river.html#step-4-specification-of-the-preprocessing-model",
    "title": "13  HPT: River",
    "section": "13.4 Step 4: Specification of the Preprocessing Model",
    "text": "13.4 Step 4: Specification of the Preprocessing Model\n\nfrom river import preprocessing\nprep_model = preprocessing.StandardScaler()\nfun_control.update({\"prep_model\": prep_model})"
  },
  {
    "objectID": "13_spot_hpt_river.html#step-5-select-algorithm-and-core_model_hyper_dict",
    "href": "13_spot_hpt_river.html#step-5-select-algorithm-and-core_model_hyper_dict",
    "title": "13  HPT: River",
    "section": "13.5 Step 5: Select algorithm and core_model_hyper_dict",
    "text": "13.5 Step 5: Select algorithm and core_model_hyper_dict\n\nThe river model (HATR) is selected.\nFurthermore, the corresponding hyperparameters, see: https://riverml.xyz/0.15.0/api/tree/HoeffdingTreeRegressor/ are selected (incl. type information, names, and bounds).\nThe corresponding hyperparameter dictionary is added to the fun_control dictionary.\nAlternatively, you can load a local hyper_dict. Simply set river_hyper_dict.json as the filename. If filenameis set to None, the hyper_dict is loaded from the spotRiver package.\n\n\nfrom river.tree import HoeffdingAdaptiveTreeRegressor\nfrom spotRiver.data.river_hyper_dict import RiverHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\ncore_model  = HoeffdingAdaptiveTreeRegressor\nfun_control = add_core_model_to_fun_control(core_model=core_model,\n                              fun_control=fun_control,\n                              hyper_dict=RiverHyperDict,\n                              filename=None)\n\nThe corresponding entries for the core_model class are shown below.\n\nfun_control['core_model_hyper_dict']\n\n{'grace_period': {'type': 'int',\n  'default': 200,\n  'transform': 'None',\n  'lower': 10,\n  'upper': 1000},\n 'max_depth': {'type': 'int',\n  'default': 20,\n  'transform': 'transform_power_2_int',\n  'lower': 2,\n  'upper': 20},\n 'delta': {'type': 'float',\n  'default': 1e-07,\n  'transform': 'None',\n  'lower': 1e-08,\n  'upper': 1e-06},\n 'tau': {'type': 'float',\n  'default': 0.05,\n  'transform': 'None',\n  'lower': 0.01,\n  'upper': 0.1},\n 'leaf_prediction': {'levels': ['mean', 'model', 'adaptive'],\n  'type': 'factor',\n  'default': 'mean',\n  'transform': 'None',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 2},\n 'leaf_model': {'levels': ['LinearRegression', 'PARegressor', 'Perceptron'],\n  'type': 'factor',\n  'default': 'LinearRegression',\n  'transform': 'None',\n  'class_name': 'river.linear_model',\n  'core_model_parameter_type': 'instance()',\n  'lower': 0,\n  'upper': 2},\n 'model_selector_decay': {'type': 'float',\n  'default': 0.95,\n  'transform': 'None',\n  'lower': 0.9,\n  'upper': 0.99},\n 'splitter': {'levels': ['EBSTSplitter', 'TEBSTSplitter', 'QOSplitter'],\n  'type': 'factor',\n  'default': 'EBSTSplitter',\n  'transform': 'None',\n  'class_name': 'river.tree.splitter',\n  'core_model_parameter_type': 'instance()',\n  'lower': 0,\n  'upper': 2},\n 'min_samples_split': {'type': 'int',\n  'default': 5,\n  'transform': 'None',\n  'lower': 2,\n  'upper': 10},\n 'bootstrap_sampling': {'levels': [0, 1],\n  'type': 'factor',\n  'default': 0,\n  'transform': 'None',\n  'core_model_parameter_type': 'bool',\n  'lower': 0,\n  'upper': 1},\n 'drift_window_threshold': {'type': 'int',\n  'default': 300,\n  'transform': 'None',\n  'lower': 100,\n  'upper': 500},\n 'switch_significance': {'type': 'float',\n  'default': 0.05,\n  'transform': 'None',\n  'lower': 0.01,\n  'upper': 0.1},\n 'binary_split': {'levels': [0, 1],\n  'type': 'factor',\n  'default': 0,\n  'transform': 'None',\n  'core_model_parameter_type': 'bool',\n  'lower': 0,\n  'upper': 1},\n 'max_size': {'type': 'float',\n  'default': 500.0,\n  'transform': 'None',\n  'lower': 100.0,\n  'upper': 1000.0},\n 'memory_estimate_period': {'type': 'int',\n  'default': 1000000,\n  'transform': 'None',\n  'lower': 100000,\n  'upper': 1000000},\n 'stop_mem_management': {'levels': [0, 1],\n  'type': 'factor',\n  'default': 0,\n  'transform': 'None',\n  'core_model_parameter_type': 'bool',\n  'lower': 0,\n  'upper': 1},\n 'remove_poor_attrs': {'levels': [0, 1],\n  'type': 'factor',\n  'default': 0,\n  'transform': 'None',\n  'core_model_parameter_type': 'bool',\n  'lower': 0,\n  'upper': 1},\n 'merit_preprune': {'levels': [0, 1],\n  'type': 'factor',\n  'default': 0,\n  'transform': 'None',\n  'core_model_parameter_type': 'bool',\n  'lower': 0,\n  'upper': 1}}"
  },
  {
    "objectID": "13_spot_hpt_river.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "href": "13_spot_hpt_river.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "title": "13  HPT: River",
    "section": "13.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "13.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\n\n13.6.1 Modify hyperparameter of type factor\n\n# fun_control = modify_hyper_parameter_levels(fun_control, \"leaf_model\", [\"LinearRegression\"])\n# fun_control[\"core_model_hyper_dict\"]\n\n\n\n13.6.2 Modify hyperparameter of type numeric and integer (boolean)\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\nfun_control = modify_hyper_parameter_bounds(fun_control, \"delta\", bounds=[1e-10, 1e-6])\n# fun_control = modify_hyper_parameter_bounds(fun_control, \"min_samples_split\", bounds=[3, 20])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"merit_preprune\", [0, 0])"
  },
  {
    "objectID": "13_spot_hpt_river.html#step-7-selection-of-the-objective-loss-function",
    "href": "13_spot_hpt_river.html#step-7-selection-of-the-objective-loss-function",
    "title": "13  HPT: River",
    "section": "13.7 Step 7: Selection of the Objective (Loss) Function",
    "text": "13.7 Step 7: Selection of the Objective (Loss) Function\nThere are three metrics:\n1. `metric_river` is used for the river based evaluation via `eval_oml_iter_progressive`.\n2. `metric_sklearn` is used for the sklearn based evaluation via `eval_oml_horizon`.\n3. `metric_torch` is used for the pytorch based evaluation.\n\nimport numpy as np\nfrom river import metrics\nfrom sklearn.metrics import mean_absolute_error\n\n\nfrom spotRiver.fun.hyperriver import HyperRiver\nfun = HyperRiver(seed=123, log_level=50).fun_oml_horizon\nweights = np.array([1, 1/1000, 1/1000])*10_000.0\nhorizon = 7*24\noml_grace_period = 2\nstep = 100\nweight_coeff = 1.0\n\nfun_control.update({\n               \"horizon\": horizon,\n               \"oml_grace_period\": oml_grace_period,\n               \"weights\": weights,\n               \"step\": step,\n               \"log_level\": 50,\n               \"weight_coeff\": weight_coeff,\n               \"metric_river\": metrics.MAE(),\n               \"metric_sklearn\": mean_absolute_error\n               })"
  },
  {
    "objectID": "13_spot_hpt_river.html#step-8-calling-the-spot-function",
    "href": "13_spot_hpt_river.html#step-8-calling-the-spot-function",
    "title": "13  HPT: River",
    "section": "13.8 Step 8: Calling the SPOT Function",
    "text": "13.8 Step 8: Calling the SPOT Function\n\n13.8.1 Prepare the SPOT Parameters\n\nGet types and variable names as well as lower and upper bounds for the hyperparameters.\n\n\nfrom spotPython.hyperparameters.values import (\n    get_var_type,\n    get_var_name,\n    get_bound_values\n    )\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\n\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name                   | type   | default          |      lower |    upper | transform             |\n|------------------------|--------|------------------|------------|----------|-----------------------|\n| grace_period           | int    | 200              |     10     | 1000     | None                  |\n| max_depth              | int    | 20               |      2     |   20     | transform_power_2_int |\n| delta                  | float  | 1e-07            |      1e-10 |    1e-06 | None                  |\n| tau                    | float  | 0.05             |      0.01  |    0.1   | None                  |\n| leaf_prediction        | factor | mean             |      0     |    2     | None                  |\n| leaf_model             | factor | LinearRegression |      0     |    2     | None                  |\n| model_selector_decay   | float  | 0.95             |      0.9   |    0.99  | None                  |\n| splitter               | factor | EBSTSplitter     |      0     |    2     | None                  |\n| min_samples_split      | int    | 5                |      2     |   10     | None                  |\n| bootstrap_sampling     | factor | 0                |      0     |    1     | None                  |\n| drift_window_threshold | int    | 300              |    100     |  500     | None                  |\n| switch_significance    | float  | 0.05             |      0.01  |    0.1   | None                  |\n| binary_split           | factor | 0                |      0     |    1     | None                  |\n| max_size               | float  | 500.0            |    100     | 1000     | None                  |\n| memory_estimate_period | int    | 1000000          | 100000     |    1e+06 | None                  |\n| stop_mem_management    | factor | 0                |      0     |    1     | None                  |\n| remove_poor_attrs      | factor | 0                |      0     |    1     | None                  |\n| merit_preprune         | factor | 0                |      0     |    0     | None                  |\n\n\n\n\n13.8.2 Run the Spot Optimizer\n\nRun SPOT for approx. x mins (max_time).\nNote: the run takes longer, because the evaluation time of initial design (here: initi_size, 20 points) is not considered.\n\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nhyper_dict=RiverHyperDict().load()\nX_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\n\n\nfrom spotPython.spot import spot\nfrom math import inf\nimport numpy as np\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\nspotPython tuning: 2.199671152176264 [####------] 44.84% \n\n\nspotPython tuning: 2.140348328852409 [#########-] 92.29% \n\n\nspotPython tuning: 2.140348328852409 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x189bcd810&gt;"
  },
  {
    "objectID": "13_spot_hpt_river.html#step-9-results",
    "href": "13_spot_hpt_river.html#step-9-results",
    "title": "13  HPT: River",
    "section": "13.9 Step 9: Results",
    "text": "13.9 Step 9: Results\n\nimport pickle\nSAVE = False\nLOAD = False\n\nif SAVE:\n    result_file_name = \"res_\" + experiment_name + \".pkl\"\n    with open(result_file_name, 'wb') as f:\n        pickle.dump(spot_tuner, f)\n\nif LOAD:\n    result_file_name = \"res_ch10-friedman-hpt-0_maans03_60min_20init_1K_2023-04-14_10-11-19.pkl\"\n    with open(result_file_name, 'rb') as f:\n        spot_tuner =  pickle.load(f)\n\n\nShow the Progress of the hyperparameter tuning:\n\n\nspot_tuner.plot_progress(log_y=True, filename=\"./figures/\" + experiment_name+\"_progress.pdf\")\n\n\n\n\n\nPrint the Results\n\n\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name                   | type   | default          |    lower |     upper |              tuned | transform             |   importance | stars   |\n|------------------------|--------|------------------|----------|-----------|--------------------|-----------------------|--------------|---------|\n| grace_period           | int    | 200              |     10.0 |    1000.0 |              966.0 | None                  |         0.00 |         |\n| max_depth              | int    | 20               |      2.0 |      20.0 |                2.0 | transform_power_2_int |         0.00 |         |\n| delta                  | float  | 1e-07            |    1e-10 |     1e-06 |              1e-10 | None                  |         0.00 |         |\n| tau                    | float  | 0.05             |     0.01 |       0.1 |               0.01 | None                  |         0.00 |         |\n| leaf_prediction        | factor | mean             |      0.0 |       2.0 |                2.0 | None                  |         2.36 | *       |\n| leaf_model             | factor | LinearRegression |      0.0 |       2.0 |                0.0 | None                  |         0.53 | .       |\n| model_selector_decay   | float  | 0.95             |      0.9 |      0.99 |               0.99 | None                  |         0.00 |         |\n| splitter               | factor | EBSTSplitter     |      0.0 |       2.0 |                2.0 | None                  |       100.00 | ***     |\n| min_samples_split      | int    | 5                |      2.0 |      10.0 |                2.0 | None                  |         0.00 |         |\n| bootstrap_sampling     | factor | 0                |      0.0 |       1.0 |                0.0 | None                  |         0.00 |         |\n| drift_window_threshold | int    | 300              |    100.0 |     500.0 |              268.0 | None                  |         0.00 |         |\n| switch_significance    | float  | 0.05             |     0.01 |       0.1 |                0.1 | None                  |         0.00 |         |\n| binary_split           | factor | 0                |      0.0 |       1.0 |                0.0 | None                  |         0.06 |         |\n| max_size               | float  | 500.0            |    100.0 |    1000.0 | 123.77308808574179 | None                  |         0.00 |         |\n| memory_estimate_period | int    | 1000000          | 100000.0 | 1000000.0 |           331395.0 | None                  |         0.00 |         |\n| stop_mem_management    | factor | 0                |      0.0 |       1.0 |                1.0 | None                  |         0.00 |         |\n| remove_poor_attrs      | factor | 0                |      0.0 |       1.0 |                0.0 | None                  |         0.00 |         |\n| merit_preprune         | factor | 0                |      0.0 |       0.0 |                0.0 | None                  |         0.00 |         |\n\n\n\n13.9.1 Show variable importance\n\nspot_tuner.plot_importance(threshold=0.0025, filename=\"./figures/\" + experiment_name+\"_importance.pdf\")\n\n\n\n\n\n\n13.9.2 Build and Evaluate HTR Model with Tuned Hyperparameters\n\nm = test.shape[0]\na = int(m/2)-50\nb = int(m/2)\n\n\n\n13.9.3 The Large Data Set (k=0.2)\n\n\n\n\n\n\nCaution: Increased Friedman-Drift Data Set\n\n\n\n\nThe Friedman-Drift Data Set is increased by a factor of two to show the transferability of the hyperparameter tuning results.\nLarger values of k lead to a longer run time.\n\n\n\n\nhorizon = 7*24\nk = .2\nn_total = int(k*100_000)\nn_samples = n_total\np_1 = int(k*25_000)\np_2 = int(k*50_000)\nposition=(p_1, p_2)\nn_train = 1_000\na = n_train + p_1 - 12\nb = a + 12\ndataset = synth.FriedmanDrift(\n   drift_type='gra',\n   position=position,\n     seed=123\n)\ndata_dict = {key: [] for key in list(dataset.take(1))[0][0].keys()}\ndata_dict[\"y\"] = []\nfor x, y in dataset.take(n_total):\n    for key, value in x.items():\n        data_dict[key].append(value)\n    data_dict[\"y\"].append(y)\ndf = pd.DataFrame(data_dict)\n# Add column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column\ndf.columns = [f\"x{i}\" for i in range(1, 11)] + [\"y\"]\n\ntrain = df[:n_train]\ntest = df[n_train:]\ntarget_column = \"y\"\n#\nfun_control.update({\"data\": None, # dataset,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": target_column})\n\n\n\n13.9.4 Get Default Hyperparameters\n\n# fun_control was modified, we generate a new one with the original \n# default hyperparameters\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nfc = fun_control\nfc.update({\"core_model_hyper_dict\":\n    hyper_dict[fun_control[\"core_model\"].__name__]})\nmodel_default = get_one_core_model_from_X(X_start, fun_control=fc)\nmodel_default\n\nHoeffdingAdaptiveTreeRegressor (\n  grace_period=200\n  max_depth=1048576\n  delta=1e-07\n  tau=0.05\n  leaf_prediction=\"mean\"\n  leaf_model=LinearRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.01\n      )\n    )\n    loss=Squared ()\n    l2=0.\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n  model_selector_decay=0.95\n  nominal_attributes=None\n  splitter=EBSTSplitter ()\n  min_samples_split=5\n  bootstrap_sampling=0\n  drift_window_threshold=300\n  drift_detector=ADWIN (\n    delta=0.002\n    clock=32\n    max_buckets=5\n    min_window_length=5\n    grace_period=10\n  )\n  switch_significance=0.05\n  binary_split=0\n  max_size=500.\n  memory_estimate_period=1000000\n  stop_mem_management=0\n  remove_poor_attrs=0\n  merit_preprune=0\n  seed=None\n)\n\n\n\nfrom spotRiver.evaluation.eval_bml import eval_oml_horizon\n\ndf_eval_default, df_true_default = eval_oml_horizon(\n                    model=model_default,\n                    train=fun_control[\"train\"],\n                    test=fun_control[\"test\"],\n                    target_column=fun_control[\"target_column\"],\n                    horizon=fun_control[\"horizon\"],\n                    oml_grace_period=fun_control[\"oml_grace_period\"],\n                    metric=fun_control[\"metric_sklearn\"],\n                )\n\n\nfrom spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics, plot_bml_oml_horizon_predictions\ndf_labels=[\"default\"]\nplot_bml_oml_horizon_metrics(df_eval = [df_eval_default], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"])\nplot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b]], target_column=target_column,  df_labels=df_labels)\n\n\n\n\n\n\n\n\n\n13.9.5 Get SPOT Results\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\nmodel_spot\n\nHoeffdingAdaptiveTreeRegressor (\n  grace_period=966\n  max_depth=4\n  delta=1e-10\n  tau=0.01\n  leaf_prediction=\"adaptive\"\n  leaf_model=LinearRegression (\n    optimizer=SGD (\n      lr=Constant (\n        learning_rate=0.01\n      )\n    )\n    loss=Squared ()\n    l2=0.\n    l1=0.\n    intercept_init=0.\n    intercept_lr=Constant (\n      learning_rate=0.01\n    )\n    clip_gradient=1e+12\n    initializer=Zeros ()\n  )\n  model_selector_decay=0.99\n  nominal_attributes=None\n  splitter=QOSplitter (\n    radius=0.25\n    allow_multiway_splits=False\n  )\n  min_samples_split=2\n  bootstrap_sampling=0\n  drift_window_threshold=268\n  drift_detector=ADWIN (\n    delta=0.002\n    clock=32\n    max_buckets=5\n    min_window_length=5\n    grace_period=10\n  )\n  switch_significance=0.1\n  binary_split=0\n  max_size=123.773088\n  memory_estimate_period=331395\n  stop_mem_management=1\n  remove_poor_attrs=0\n  merit_preprune=0\n  seed=None\n)\n\n\n\ndf_eval_spot, df_true_spot = eval_oml_horizon(\n                    model=model_spot,\n                    train=fun_control[\"train\"],\n                    test=fun_control[\"test\"],\n                    target_column=fun_control[\"target_column\"],\n                    horizon=fun_control[\"horizon\"],\n                    oml_grace_period=fun_control[\"oml_grace_period\"],\n                    metric=fun_control[\"metric_sklearn\"],\n                )\n\n\ndf_labels=[\"default\", \"spot\"]\nplot_bml_oml_horizon_metrics(df_eval = [df_eval_default, df_eval_spot], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"], filename=\"./figures/\" + experiment_name+\"_metrics.pdf\")\n\n\n\n\n\na = int(m/2)+20\nb = int(m/2)+50\nplot_bml_oml_horizon_predictions(df_true = [df_true_default[a:b], df_true_spot[a:b]], target_column=target_column,  df_labels=df_labels, filename=\"./figures/\" + experiment_name+\"_predictions.pdf\")\n\n\n\n\n\nfrom spotPython.plot.validation import plot_actual_vs_predicted\nplot_actual_vs_predicted(y_test=df_true_default[\"y\"], y_pred=df_true_default[\"Prediction\"], title=\"Default\")\nplot_actual_vs_predicted(y_test=df_true_spot[\"y\"], y_pred=df_true_spot[\"Prediction\"], title=\"SPOT\")\n\n\n\n\n\n\n\n\n\n13.9.6 Visualize Regression Trees\n\ndataset_f = dataset.take(n_total)\nfor x, y in dataset_f:\n    model_default.learn_one(x, y)\n\n\n\n\n\n\n\nCaution: Large Trees\n\n\n\n\nSince the trees are large, the visualization is suppressed by default.\nTo visualize the trees, uncomment the following line.\n\n\n\n\n# model_default.draw()\n\n\nmodel_default.summary\n\n{'n_nodes': 35,\n 'n_branches': 17,\n 'n_leaves': 18,\n 'n_active_leaves': 96,\n 'n_inactive_leaves': 0,\n 'height': 6,\n 'total_observed_weight': 39002.0,\n 'n_alternate_trees': 21,\n 'n_pruned_alternate_trees': 6,\n 'n_switch_alternate_trees': 2}\n\n\n\n\n13.9.7 Spot Model\n\ndataset_f = dataset.take(n_total)\nfor x, y in dataset_f:\n    model_spot.learn_one(x, y)\n\n\n\n\n\n\n\nCaution: Large Trees\n\n\n\n\nSince the trees are large, the visualization is suppressed by default.\nTo visualize the trees, uncomment the following line.\n\n\n\n\n# model_spot.draw()\n\n\nmodel_spot.summary\n\n{'n_nodes': 17,\n 'n_branches': 8,\n 'n_leaves': 9,\n 'n_active_leaves': -18594,\n 'n_inactive_leaves': 18604,\n 'height': 5,\n 'total_observed_weight': 39002.0,\n 'n_alternate_trees': 14,\n 'n_pruned_alternate_trees': 11,\n 'n_switch_alternate_trees': 1}\n\n\n\nfrom spotPython.utils.eda import compare_two_tree_models\nprint(compare_two_tree_models(model_default, model_spot))\n\n| Parameter                |   Default |   Spot |\n|--------------------------|-----------|--------|\n| n_nodes                  |        35 |     17 |\n| n_branches               |        17 |      8 |\n| n_leaves                 |        18 |      9 |\n| n_active_leaves          |        96 | -18594 |\n| n_inactive_leaves        |         0 |  18604 |\n| height                   |         6 |      5 |\n| total_observed_weight    |     39002 |  39002 |\n| n_alternate_trees        |        21 |     14 |\n| n_pruned_alternate_trees |         6 |     11 |\n| n_switch_alternate_trees |         2 |      1 |\n\n\n\nmin(spot_tuner.y), max(spot_tuner.y)\n\n(2.140348328852409, 13.363421535293801)\n\n\n\n\n13.9.8 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nleaf_prediction:  2.362308986395233\nleaf_model:  0.5333799073303701\nsplitter:  100.0\nbinary_split:  0.055674378910427334\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.9.9 Parallel Coordinates Plots\n\nspot_tuner.parallel_plot()\n\n\n                                                \n\n\n\n\n13.9.10 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)\n\n\n\n\n\nMontiel, Jacob, Max Halford, Saulo Martiello Mastelini, Geoffrey Bolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, et al. 2021. “River: Machine Learning for Streaming Data in Python.”"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#sec-setup-14",
    "href": "14_spot_ray_hpt_torch_cifar10.html#sec-setup-14",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.1 Step 1: Setup",
    "text": "14.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size and the device that is used.\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nThe device can be selected by setting the variable DEVICE.\nSince we are using a simple neural net, the setting \"cpu\" is preferred (on Mac).\nIf you have a GPU, you can use \"cuda:0\" instead.\nIf DEVICE is set to None, spotPython will automatically select the device.\n\nThis might result in \"mps\" on Macs, which is not the best choice for simple neural nets.\n\n\n\n\n\nMAX_TIME = 10\nINIT_SIZE = 5\nDEVICE = \"cpu\" # \"cuda:0\"\n\n\nfrom spotPython.utils.device import getDevice\nDEVICE = getDevice(DEVICE)\nprint(DEVICE)\n\ncpu\n\n\n\nimport os\nimport copy\nimport socket\nimport warnings\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '14-torch' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\nwarnings.filterwarnings(\"ignore\")\n\n14-torch_maans03_10min_5init_2023-06-28_02-43-46"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#sec-initialization-fun-control-14",
    "href": "14_spot_ray_hpt_torch_cifar10.html#sec-initialization-fun-control-14",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "14.2 Step 2: Initialization of the fun_control Dictionary\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process. This dictionary is called fun_control and is initialized with the function fun_control_init. The function fun_control_init returns a skeleton dictionary. The dictionary is filled with the required information for the hyperparameter tuning process. It stores the hyperparameter tuning settings, e.g., the deep learning network architecture that should be tuned, the classification (or regression) problem, and the data that is used for the tuning. The dictionary is used as an input for the SPOT function.\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"classification\",\n    tensorboard_path=\"runs/14_spot_ray_hpt_torch_cifar10\",\n    device=DEVICE,)"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#sec-data-loading-14",
    "href": "14_spot_ray_hpt_torch_cifar10.html#sec-data-loading-14",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.3 Step 3: PyTorch Data Loading",
    "text": "14.3 Step 3: PyTorch Data Loading\nThe data loading process is implemented in the same manner as described in the Section “Data loaders” in PyTorch (2023a). The data loaders are wrapped into the function load_data_cifar10 which is identical to the function load_data in PyTorch (2023a). A global data directory is used, which allows sharing the data directory between different trials. The method load_data_cifar10 is part of the spotPython package and can be imported from spotPython.data.torchdata.\nIn the following step, the test and train data are added to the dictionary fun_control.\n\nfrom spotPython.data.torchdata import load_data_cifar10\ntrain, test = load_data_cifar10()\nn_samples = len(train)\n# add the dataset to the fun_control\nfun_control.update({\n    \"train\": train,\n    \"test\": test,\n    \"n_samples\": n_samples})\n\nFiles already downloaded and verified\n\n\nFiles already downloaded and verified"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#sec-specification-of-preprocessing-model-14",
    "href": "14_spot_ray_hpt_torch_cifar10.html#sec-specification-of-preprocessing-model-14",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.4 Step 4: Specification of the Preprocessing Model",
    "text": "14.4 Step 4: Specification of the Preprocessing Model\nAfter the training and test data are specified and added to the fun_control dictionary, spotPython allows the specification of a data preprocessing pipeline, e.g., for the scaling of the data or for the one-hot encoding of categorical variables. The preprocessing model is called prep_model (“preparation” or pre-processing) and includes steps that are not subject to the hyperparameter tuning process. The preprocessing model is specified in the fun_control dictionary. The preprocessing model can be implemented as a sklearn pipeline. The following code shows a typical preprocessing pipeline:\ncategorical_columns = [\"cities\", \"colors\"]\none_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\",\n                                    sparse_output=False)\nprep_model = ColumnTransformer(\n        transformers=[\n             (\"categorical\", one_hot_encoder, categorical_columns),\n         ],\n         remainder=StandardScaler(),\n     )\nBecause the Ray Tune (ray[tune]) hyperparameter tuning as described in PyTorch (2023a) does not use a preprocessing model, the preprocessing model is set to None here.\n\nprep_model = None\nfun_control.update({\"prep_model\": prep_model})"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#sec-selection-of-the-algorithm-14",
    "href": "14_spot_ray_hpt_torch_cifar10.html#sec-selection-of-the-algorithm-14",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.5 Step 5: Select Model (algorithm) and core_model_hyper_dict",
    "text": "14.5 Step 5: Select Model (algorithm) and core_model_hyper_dict\nThe same neural network model as implemented in the section “Configurable neural network” of the PyTorch tutorial (PyTorch 2023a) is used here. We will show the implementation from PyTorch (2023a) in Section 14.5.0.1 first, before the extended implementation with spotPython is shown in Section 14.5.0.2.\n\n14.5.0.1 Implementing a Configurable Neural Network With Ray Tune\nWe used the same hyperparameters that are implemented as configurable in the PyTorch tutorial. We specify the layer sizes, namely l1 and l2, of the fully connected layers:\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nThe learning rate, i.e., lr, of the optimizer is made configurable, too:\noptimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n\n\n14.5.0.2 Implementing a Configurable Neural Network With spotPython\nspotPython implements a class which is similar to the class described in the PyTorch tutorial. The class is called Net_CIFAR10 and is implemented in the file netcifar10.py.\nfrom torch import nn\nimport torch.nn.functional as F\nimport spotPython.torch.netcore as netcore\n\n\nclass Net_CIFAR10(netcore.Net_Core):\n    def __init__(self, l1, l2, lr_mult, batch_size, epochs, k_folds, patience,\n    optimizer, sgd_momentum):\n        super(Net_CIFAR10, self).__init__(\n            lr_mult=lr_mult,\n            batch_size=batch_size,\n            epochs=epochs,\n            k_folds=k_folds,\n            patience=patience,\n            optimizer=optimizer,\n            sgd_momentum=sgd_momentum,\n        )\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\n14.5.1 The Net_Core class\nNet_CIFAR10 inherits from the class Net_Core which is implemented in the file netcore.py. It implements the additional attributes that are common to all neural network models. The Net_Core class is implemented in the file netcore.py. It implements hyperparameters as attributes, that are not used by the core_model, e.g.:\n\noptimizer (optimizer),\nlearning rate (lr),\nbatch size (batch_size),\nepochs (epochs),\nk_folds (k_folds), and\nearly stopping criterion “patience” (patience).\n\nUsers can add further attributes to the class. The class Net_Core is shown below.\nfrom torch import nn\n\n\nclass Net_Core(nn.Module):\n    def __init__(self, lr_mult, batch_size, epochs, k_folds, patience,\n        optimizer, sgd_momentum):\n        super(Net_Core, self).__init__()\n        self.lr_mult = lr_mult\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.k_folds = k_folds\n        self.patience = patience\n        self.optimizer = optimizer\n        self.sgd_momentum = sgd_momentum\n\n\n14.5.2 Comparison of the Approach Described in the PyTorch Tutorial With spotPython\nComparing the class Net from the PyTorch tutorial and the class Net_CIFAR10 from spotPython, we see that the class Net_CIFAR10 has additional attributes and does not inherit from nn directly. It adds an additional class, Net_core, that takes care of additional attributes that are common to all neural network models, e.g., the learning rate multiplier lr_mult or the batch size batch_size.\nspotPython’s core_model implements an instance of the Net_CIFAR10 class. In addition to the basic neural network model, the core_model can use these additional attributes. spotPython provides methods for handling these additional attributes to guarantee 100% compatibility with the PyTorch classes. The method add_core_model_to_fun_control adds the hyperparameters and additional attributes to the fun_control dictionary. The method is shown below.\n\nfrom spotPython.torch.netcifar10 import Net_CIFAR10\nfrom spotPython.data.torch_hyper_dict import TorchHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\ncore_model = Net_CIFAR10\nfun_control = add_core_model_to_fun_control(core_model=core_model,\n                              fun_control=fun_control,\n                              hyper_dict=TorchHyperDict,\n                              filename=None)\n\n\n\n14.5.3 The Search Space: Hyperparameters\nIn Section 14.5.4, we first describe how to configure the search space with ray[tune] (as shown in PyTorch (2023a)) and then how to configure the search space with spotPython in -14.\n\n\n14.5.4 Configuring the Search Space With Ray Tune\nRay Tune’s search space can be configured as follows (PyTorch 2023a):\nconfig = {\n    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"batch_size\": tune.choice([2, 4, 8, 16])\n}\nThe tune.sample_from() function enables the user to define sample methods to obtain hyperparameters. In this example, the l1 and l2 parameters should be powers of 2 between 4 and 256, so either 4, 8, 16, 32, 64, 128, or 256. The lr (learning rate) should be uniformly sampled between 0.0001 and 0.1. Lastly, the batch size is a choice between 2, 4, 8, and 16.\nAt each trial, ray[tune] will randomly sample a combination of parameters from these search spaces. It will then train a number of models in parallel and find the best performing one among these. ray[tune] uses the ASHAScheduler which will terminate bad performing trials early.\n\n\n14.5.5 Configuring the Search Space With spotPython\n\n14.5.5.1 The hyper_dict Hyperparameters for the Selected Algorithm\nspotPython uses JSON files for the specification of the hyperparameters. Users can specify their individual JSON files, or they can use the JSON files provided by spotPython. The JSON file for the core_model is called torch_hyper_dict.json.\nIn contrast to ray[tune], spotPython can handle numerical, boolean, and categorical hyperparameters. They can be specified in the JSON file in a similar way as the numerical hyperparameters as shown below. Each entry in the JSON file represents one hyperparameter with the following structure: type, default, transform, lower, and upper.\n\"factor_hyperparameter\": {\n    \"levels\": [\"A\", \"B\", \"C\"],\n    \"type\": \"factor\",\n    \"default\": \"B\",\n    \"transform\": \"None\",\n    \"core_model_parameter_type\": \"str\",\n    \"lower\": 0,\n    \"upper\": 2},\nThe corresponding entries for the core_model` class are shown below.\n\nfun_control['core_model_hyper_dict']\n\n{'l1': {'type': 'int',\n  'default': 5,\n  'transform': 'transform_power_2_int',\n  'lower': 2,\n  'upper': 9},\n 'l2': {'type': 'int',\n  'default': 5,\n  'transform': 'transform_power_2_int',\n  'lower': 2,\n  'upper': 9},\n 'lr_mult': {'type': 'float',\n  'default': 1.0,\n  'transform': 'None',\n  'lower': 0.1,\n  'upper': 10.0},\n 'batch_size': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 1,\n  'upper': 4},\n 'epochs': {'type': 'int',\n  'default': 3,\n  'transform': 'transform_power_2_int',\n  'lower': 3,\n  'upper': 4},\n 'k_folds': {'type': 'int',\n  'default': 1,\n  'transform': 'None',\n  'lower': 1,\n  'upper': 1},\n 'patience': {'type': 'int',\n  'default': 5,\n  'transform': 'None',\n  'lower': 2,\n  'upper': 10},\n 'optimizer': {'levels': ['Adadelta',\n   'Adagrad',\n   'Adam',\n   'AdamW',\n   'SparseAdam',\n   'Adamax',\n   'ASGD',\n   'NAdam',\n   'RAdam',\n   'RMSprop',\n   'Rprop',\n   'SGD'],\n  'type': 'factor',\n  'default': 'SGD',\n  'transform': 'None',\n  'class_name': 'torch.optim',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 12},\n 'sgd_momentum': {'type': 'float',\n  'default': 0.0,\n  'transform': 'None',\n  'lower': 0.0,\n  'upper': 1.0}}"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#sec-modification-of-hyperparameters-14",
    "href": "14_spot_ray_hpt_torch_cifar10.html#sec-modification-of-hyperparameters-14",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "14.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nRay tune (PyTorch 2023a) does not provide a way to change the specified hyperparameters without re-compilation. However, spotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions are described in the following.\n\n14.6.0.1 Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nAfter specifying the model, the corresponding hyperparameters, their types and bounds are loaded from the JSON file torch_hyper_dict.json. After loading, the user can modify the hyperparameters, e.g., the bounds. spotPython provides a simple rule for de-activating hyperparameters: If the lower and the upper bound are set to identical values, the hyperparameter is de-activated. This is useful for the hyperparameter tuning, because it allows to specify a hyperparameter in the JSON file, but to de-activate it in the fun_control dictionary. This is done in the next step.\n\n\n14.6.0.2 Modify Hyperparameters of Type numeric and integer (boolean)\nSince the hyperparameter k_folds is not used in the PyTorch tutorial, it is de-activated here by setting the lower and upper bound to the same value. Note, k_folds is of type “integer”.\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\nfun_control = modify_hyper_parameter_bounds(fun_control, \n    \"batch_size\", bounds=[1, 5])\nfun_control = modify_hyper_parameter_bounds(fun_control, \n    \"k_folds\", bounds=[0, 0])\nfun_control = modify_hyper_parameter_bounds(fun_control, \n    \"patience\", bounds=[3, 3])\n\n\n\n14.6.0.3 Modify Hyperparameter of Type factor\nIn a similar manner as for the numerical hyperparameters, the categorical hyperparameters can be modified. New configurations can be chosen by adding or deleting levels. For example, the hyperparameter optimizer can be re-configured as follows:\nIn the following setting, two optimizers (\"SGD\" and \"Adam\") will be compared during the spotPython hyperparameter tuning. The hyperparameter optimizer is active.\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_levels\nfun_control = modify_hyper_parameter_levels(fun_control,\n     \"optimizer\", [\"SGD\", \"Adam\"])\n\nThe hyperparameter optimizer can be de-activated by choosing only one value (level), here: \"SGD\".\n\nfun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\", [\"SGD\"])\n\nAs discussed in Section 14.6.1, there are some issues with the LBFGS optimizer. Therefore, the usage of the LBFGS optimizer is not deactivated in spotPython by default. However, the LBFGS optimizer can be activated by adding it to the list of optimizers. Rprop was removed, because it does perform very poorly (as some pre-tests have shown). However, it can also be activated by adding it to the list of optimizers. Since SparseAdam does not support dense gradients, Adam was used instead. Therefore, there are 10 default optimizers:\n\nfun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\",\n    [\"Adadelta\", \"Adagrad\", \"Adam\", \"AdamW\", \"Adamax\", \"ASGD\", \n    \"NAdam\", \"RAdam\", \"RMSprop\", \"SGD\"])\n\n\n\n14.6.1 Optimizers\nTable 14.1 shows some of the optimizers available in PyTorch:\n\\(a\\) denotes (0.9,0.999), \\(b\\) (0.5,1.2), and \\(c\\) (1e-6, 50), respectively. \\(R\\) denotes required, but unspecified. “m” denotes momentum, “w_d” weight_decay, “d” dampening, “n” nesterov, “r” rho, “l_s” learning rate for scaling delta, “l_d” lr_decay, “b” betas, “l” lambd, “a” alpha, “m_d” for momentum_decay, “e” etas, and “s_s” for step_sizes.\n\n\nTable 14.1: Optimizers available in PyTorch (selection). The default values are shown in the table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizer\nlr\nm\nw_d\nd\nn\nr\nl_s\nl_d\nb\nl\na\nm_d\ne\ns_s\n\n\n\n\nAdadelta\n-\n-\n0.\n-\n-\n0.9\n1.\n-\n-\n-\n-\n-\n-\n-\n\n\nAdagrad\n1e-2\n-\n0.\n-\n-\n-\n-\n0.\n-\n-\n-\n-\n-\n-\n\n\nAdam\n1e-3\n-\n0.\n-\n-\n-\n-\n-\n\\(a\\)\n-\n-\n-\n-\n-\n\n\nAdamW\n1e-3\n-\n1e-2\n-\n-\n-\n-\n-\n\\(a\\)\n-\n-\n-\n-\n-\n\n\nSparseAdam\n1e-3\n-\n-\n-\n-\n-\n-\n-\n\\(a\\)\n-\n-\n-\n-\n-\n\n\nAdamax\n2e-3\n-\n0.\n-\n-\n-\n-\n-\n\\(a\\)\n-\n-\n-\n-\n-\n\n\nASGD\n1e-2\n.9\n0.\n-\nF\n-\n-\n-\n-\n1e-4\n.75\n-\n-\n-\n\n\nLBFGS\n1.\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nNAdam\n2e-3\n-\n0.\n-\n-\n-\n-\n-\n\\(a\\)\n-\n-\n0\n-\n-\n\n\nRAdam\n1e-3\n-\n0.\n-\n-\n-\n-\n-\n\\(a\\)\n-\n-\n-\n-\n-\n\n\nRMSprop\n1e-2\n0.\n0.\n-\n-\n-\n-\n-\n\\(a\\)\n-\n-\n-\n-\n-\n\n\nRprop\n1e-2\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\\(b\\)\n\\(c\\)\n-\n-\n\n\nSGD\n\\(R\\)\n0.\n0.\n0.\nF\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n\n\nspotPython implements an optimization handler that maps the optimizer names to the corresponding PyTorch optimizers.\n\n\n\n\n\n\nA note on LBFGS\n\n\n\nWe recommend deactivating PyTorch’s LBFGS optimizer, because it does not perform very well. The PyTorch documentation, see https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS, states:\n\nThis is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes). If it doesn’t fit in memory try reducing the history size, or use a different algorithm.\n\nFurthermore, the LBFGS optimizer is not compatible with the PyTorch tutorial. The reason is that the LBFGS optimizer requires the closure function, which is not implemented in the PyTorch tutorial. Therefore, the LBFGS optimizer is recommended here. Since there are ten optimizers in the portfolio, it is not recommended tuning the hyperparameters that effect one single optimizer only.\n\n\n\n\n\n\n\n\nA note on the learning rate\n\n\n\nspotPython provides a multiplier for the default learning rates, lr_mult, because optimizers use different learning rates. Using a multiplier for the learning rates might enable a simultaneous tuning of the learning rates for all optimizers. However, this is not recommended, because the learning rates are not comparable across optimizers. Therefore, we recommend fixing the learning rate for all optimizers if multiple optimizers are used. This can be done by setting the lower and upper bounds of the learning rate multiplier to the same value as shown below.\nThus, the learning rate, which affects the SGD optimizer, will be set to a fixed value. We choose the default value of 1e-3 for the learning rate, because it is used in other PyTorch examples (it is also the default value used by spotPython as defined in the optimizer_handler() method). We recommend tuning the learning rate later, when a reduced set of optimizers is fixed. Here, we will demonstrate how to select in a screening phase the optimizers that should be used for the hyperparameter tuning.\n\n\nFor the same reason, we will fix the sgd_momentum to 0.9.\n\nfun_control = modify_hyper_parameter_bounds(fun_control,\n    \"lr_mult\", bounds=[1.0, 1.0])\nfun_control = modify_hyper_parameter_bounds(fun_control,\n    \"sgd_momentum\", bounds=[0.9, 0.9])"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#step-7-selection-of-the-objective-loss-function",
    "href": "14_spot_ray_hpt_torch_cifar10.html#step-7-selection-of-the-objective-loss-function",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.7 Step 7: Selection of the Objective (Loss) Function",
    "text": "14.7 Step 7: Selection of the Objective (Loss) Function\n\n14.7.1 Evaluation: Data Splitting\nThe evaluation procedure requires the specification of the way how the data is split into a train and a test set and the loss function (and a metric). As a default, spotPython provides a standard hold-out data split and cross validation.\n\n\n14.7.2 Hold-out Data Split\nIf a hold-out data split is used, the data will be partitioned into a training, a validation, and a test data set. The split depends on the setting of the eval parameter. If eval is set to train_hold_out, one data set, usually the original training data set, is split into a new training and a validation data set. The training data set is used for training the model. The validation data set is used for the evaluation of the hyperparameter configuration and early stopping to prevent overfitting. In this case, the original test data set is not used.\n\n\n\n\n\n\nNote\n\n\n\nspotPython returns the hyperparameters of the machine learning and deep learning models, e.g., number of layers, learning rate, or optimizer, but not the model weights. Therefore, after the SPOT run is finished, the corresponding model with the optimized architecture has to be trained again with the best hyperparameter configuration. The training is performed on the training data set. The test data set is used for the final evaluation of the model.\nSummarizing, the following splits are performed in the hold-out setting:\n\nRun spotPython with eval set to train_hold_out to determine the best hyperparameter configuration.\nTrain the model with the best hyperparameter configuration (“architecture”) on the training data set: train_tuned(model_spot, train, \"model_spot.pt\").\nTest the model on the test data: test_tuned(model_spot, test, \"model_spot.pt\")\n\nThese steps will be exemplified in the following sections.\n\n\nIn addition to this hold-out setting, spotPython provides another hold-out setting, where an explicit test data is specified by the user that will be used as the validation set. To choose this option, the eval parameter is set to test_hold_out. In this case, the training data set is used for the model training. Then, the explicitly defined test data set is used for the evaluation of the hyperparameter configuration (the validation).\n\n\n14.7.3 Cross-Validation\nThe cross validation setting is used by setting the eval parameter to train_cv or test_cv. In both cases, the data set is split into \\(k\\) folds. The model is trained on \\(k-1\\) folds and evaluated on the remaining fold. This is repeated \\(k\\) times, so that each fold is used exactly once for evaluation. The final evaluation is performed on the test data set. The cross validation setting is useful for small data sets, because it allows to use all data for training and evaluation. However, it is computationally expensive, because the model has to be trained \\(k\\) times.\n\n\n\n\n\n\nNote\n\n\n\nCombinations of the above settings are possible, e.g., cross validation can be used for training and hold-out for evaluation or vice versa. Also, cross validation can be used for training and testing. Because cross validation is not used in the PyTorch tutorial (PyTorch 2023a), it is not considered further here.\n\n\n\n\n14.7.4 Overview of the Evaluation Settings\n\n14.7.4.1 Settings for the Hyperparameter Tuning\nAn overview of the training evaluations is shown in Table 14.2. \"train_cv\" and \"test_cv\" use sklearn.model_selection.KFold() internally. More details on the data splitting are provided in Section 22.14 (in the Appendix).\n\n\nTable 14.2: Overview of the evaluation settings.\n\n\n\n\n\n\n\n\n\neval\ntrain\ntest\nfunction\ncomment\n\n\n\n\n\"train_hold_out\"\n\\(\\checkmark\\)\n\ntrain_one_epoch(), validate_one_epoch() for early stopping\nsplits the train data set internally\n\n\n\"test_hold_out\"\n\\(\\checkmark\\)\n\\(\\checkmark\\)\ntrain_one_epoch(), validate_one_epoch() for early stopping\nuse the test data set for validate_one_epoch()\n\n\n\"train_cv\"\n\\(\\checkmark\\)\n\nevaluate_cv(net, train)\nCV using the train data set\n\n\n\"test_cv\"\n\n\\(\\checkmark\\)\nevaluate_cv(net, test)\nCV using the test data set . Identical to \"train_cv\", uses only test data.\n\n\n\n\n\n\n14.7.4.2 Settings for the Final Evaluation of the Tuned Architecture\n\n14.7.4.2.1 Training of the Tuned Architecture\ntrain_tuned(model, train): train the model with the best hyperparameter configuration (or simply the default) on the training data set. It splits the traindata into new train and validation sets using create_train_val_data_loaders(), which calls torch.utils.data.random_split() internally. Currently, 60% of the data is used for training and 40% for validation. The train data is used for training the model with train_hold_out(). The validation data is used for early stopping using validate_fold_or_hold_out() on the validation data set.\n\n\n14.7.4.2.2 Testing of the Tuned Architecture\ntest_tuned(model, test): test the model on the test data set. No data splitting is performed. The (trained) model is evaluated using the validate_fold_or_hold_out() function. Note: During training, \"shuffle\" is set to True, whereas during testing, \"shuffle\" is set to False.\nSection 22.14.1.4 describes the final evaluation of the tuned architecture.\n\nfun_control.update({\n    \"eval\": \"train_hold_out\",\n    \"path\": \"torch_model.pt\",\n    \"shuffle\": True})\n\n\n\n\n\n14.7.5 Evaluation: Loss Functions and Metrics\nThe key \"loss_function\" specifies the loss function which is used during the optimization. There are several different loss functions under PyTorch’s nn package. For example, a simple loss is MSELoss, which computes the mean-squared error between the output and the target. In this tutorial we will use CrossEntropyLoss, because it is also used in the PyTorch tutorial.\n\nfrom torch.nn import CrossEntropyLoss\nloss_function = CrossEntropyLoss()\nfun_control.update({\"loss_function\": loss_function})\n\nIn addition to the loss functions, spotPython provides access to a large number of metrics.\n\nThe key \"metric_sklearn\" is used for metrics that follow the scikit-learn conventions.\nThe key \"river_metric\" is used for the river based evaluation (Montiel et al. 2021) via eval_oml_iter_progressive, and\nthe key \"metric_torch\" is used for the metrics from TorchMetrics.\n\nTorchMetrics is a collection of more than 90 PyTorch metrics, see https://torchmetrics.readthedocs.io/en/latest/. Because the PyTorch tutorial uses the accuracy as metric, we use the same metric here. Currently, accuracy is computed in the tutorial’s example code. We will use TorchMetrics instead, because it offers more flexibilty, e.g., it can be used for regression and classification. Furthermore, TorchMetrics offers the following advantages:\n* A standardized interface to increase reproducibility\n* Reduces Boilerplate\n* Distributed-training compatible\n* Rigorously tested\n* Automatic accumulation over batches\n* Automatic synchronization between multiple devices\nTherefore, we set\n\nimport torchmetrics\nmetric_torch = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(fun_control[\"device\"])\nfun_control.update({\"metric_torch\": metric_torch})"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#step-8-calling-the-spot-function",
    "href": "14_spot_ray_hpt_torch_cifar10.html#step-8-calling-the-spot-function",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.8 Step 8: Calling the SPOT Function",
    "text": "14.8 Step 8: Calling the SPOT Function\n\n14.8.1 Preparing the SPOT Call\nThe following code passes the information about the parameter ranges and bounds to spot.\n\nfrom spotPython.hyperparameters.values import (\n    get_var_type,\n    get_var_name,\n    get_bound_values\n    )\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\n\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table generates a design table as follows:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name         | type   | default   |   lower |   upper | transform             |\n|--------------|--------|-----------|---------|---------|-----------------------|\n| l1           | int    | 5         |     2   |     9   | transform_power_2_int |\n| l2           | int    | 5         |     2   |     9   | transform_power_2_int |\n| lr_mult      | float  | 1.0       |     1   |     1   | None                  |\n| batch_size   | int    | 4         |     1   |     5   | transform_power_2_int |\n| epochs       | int    | 3         |     3   |     4   | transform_power_2_int |\n| k_folds      | int    | 1         |     0   |     0   | None                  |\n| patience     | int    | 5         |     3   |     3   | None                  |\n| optimizer    | factor | SGD       |     0   |     9   | None                  |\n| sgd_momentum | float  | 0.0       |     0.9 |     0.9 | None                  |\n\n\nThis allows to check if all information is available and if the information is correct. ?tbl-design shows the experimental design for the hyperparameter tuning. The table shows the hyperparameters, their types, default values, lower and upper bounds, and the transformation function. The transformation function is used to transform the hyperparameter values from the unit hypercube to the original domain. The transformation function is applied to the hyperparameter values before the evaluation of the objective function. Hyperparameter transformations are shown in the column “transform”, e.g., the l1 default is 5, which results in the value \\(2^5 = 32\\) for the network, because the transformation transform_power_2_int was selected in the JSON file. The default value of the batch_size is set to 4, which results in a batch size of \\(2^4 = 16\\).\n\n\n14.8.2 The Objective Function fun_torch\nThe objective function fun_torch is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hypertorch import HyperTorch\nfun = HyperTorch().fun_torch\n\n\n\n14.8.3 Using Default Hyperparameters or Results from Previous Runs\nWe add the default setting to the initial design:\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nhyper_dict=TorchHyperDict().load()\nX_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\n\n\n\n14.8.4 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function. Here, we will run the tuner for approximately 30 minutes (max_time). Note: the initial design is always evaluated in the spotPython run. As a consequence, the run may take longer than specified by max_time, because the evaluation time of initial design (here: init_size, 10 points) is performed independently of max_time. During the run, results from the training is shown. These results can be visualized with Tensorboard as will be shown in Section 14.9.\n\nfrom spotPython.spot import spot\nfrom math import inf\nimport numpy as np\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\n\nconfig: {'l1': 128, 'l2': 8, 'lr_mult': 1.0, 'batch_size': 32, 'epochs': 16, 'k_folds': 0, 'patience': 3, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.3204999864101410 | Loss: 1.7195783685684205 | Acc: 0.3205000000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.4092499911785126 | Loss: 1.5543318355560303 | Acc: 0.4092500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.4625999927520752 | Loss: 1.4666586666107178 | Acc: 0.4626000000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.5035499930381775 | Loss: 1.3826328471183778 | Acc: 0.5035500000000001.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.5020499825477600 | Loss: 1.3756417757034303 | Acc: 0.5020500000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.5350000262260437 | Loss: 1.3087750485420226 | Acc: 0.5350000000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.5401499867439270 | Loss: 1.2923618167877198 | Acc: 0.5401500000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.5566999912261963 | Loss: 1.2593901928901672 | Acc: 0.5567000000000000.\nEpoch: 9 | \n\n\nMulticlassAccuracy: 0.5650500059127808 | Loss: 1.2375512265205384 | Acc: 0.5650500000000001.\nEpoch: 10 | \n\n\nMulticlassAccuracy: 0.5623499751091003 | Loss: 1.2673073800086976 | Acc: 0.5623500000000000.\nEpoch: 11 | \n\n\nMulticlassAccuracy: 0.5717499852180481 | Loss: 1.2292248003005981 | Acc: 0.5717500000000000.\nEpoch: 12 | \n\n\nMulticlassAccuracy: 0.5768499970436096 | Loss: 1.2091888725280762 | Acc: 0.5768500000000000.\nEpoch: 13 | \n\n\nMulticlassAccuracy: 0.5756999850273132 | Loss: 1.2520719365119934 | Acc: 0.5757000000000000.\nEpoch: 14 | \n\n\nMulticlassAccuracy: 0.5813000202178955 | Loss: 1.2286252538681031 | Acc: 0.5813000000000000.\nEpoch: 15 | \n\n\nMulticlassAccuracy: 0.5792000293731689 | Loss: 1.2443762385368347 | Acc: 0.5792000000000000.\nEarly stopping at epoch 14\nReturned to Spot: Validation loss: 1.2443762385368347\n\nconfig: {'l1': 16, 'l2': 16, 'lr_mult': 1.0, 'batch_size': 8, 'epochs': 8, 'k_folds': 0, 'patience': 3, 'optimizer': 'NAdam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.4406000077724457 | Loss: 1.5459977696418763 | Acc: 0.4406000000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.4713500142097473 | Loss: 1.4841171562194824 | Acc: 0.4713500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.4990999996662140 | Loss: 1.3932811554312705 | Acc: 0.4991000000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.5317999720573425 | Loss: 1.3225536759972571 | Acc: 0.5318000000000001.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.5376499891281128 | Loss: 1.2992083684086799 | Acc: 0.5376500000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.5412499904632568 | Loss: 1.3010351922631265 | Acc: 0.5412500000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.5234000086784363 | Loss: 1.4053673480093478 | Acc: 0.5234000000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.5582000017166138 | Loss: 1.3003024428412318 | Acc: 0.5582000000000000.\nEarly stopping at epoch 7\nReturned to Spot: Validation loss: 1.3003024428412318\n\nconfig: {'l1': 256, 'l2': 128, 'lr_mult': 1.0, 'batch_size': 2, 'epochs': 16, 'k_folds': 0, 'patience': 3, 'optimizer': 'RMSprop', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.1004500016570091 | Loss: 2.3084620434284209 | Acc: 0.1004500000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1004500016570091 | Loss: 2.3120818099498748 | Acc: 0.1004500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.0971999987959862 | Loss: 2.3063612022161486 | Acc: 0.0972000000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.0996999964118004 | Loss: 2.3065068053483961 | Acc: 0.0997000000000000.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.1004500016570091 | Loss: 2.3137721159219744 | Acc: 0.1004500000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.1039500012993813 | Loss: 2.3062243396520614 | Acc: 0.1039500000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.0982000008225441 | Loss: 2.3067432120084761 | Acc: 0.0982000000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.1004500016570091 | Loss: 2.3102738386869430 | Acc: 0.1004500000000000.\nEpoch: 9 | \n\n\nMulticlassAccuracy: 0.0982500016689301 | Loss: 2.3100540962219238 | Acc: 0.0982500000000000.\nEarly stopping at epoch 8\nReturned to Spot: Validation loss: 2.310054096221924\n\nconfig: {'l1': 8, 'l2': 32, 'lr_mult': 1.0, 'batch_size': 4, 'epochs': 8, 'k_folds': 0, 'patience': 3, 'optimizer': 'Adamax', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.3978999853134155 | Loss: 1.6570056634426118 | Acc: 0.3979000000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.4670499861240387 | Loss: 1.4822062881916762 | Acc: 0.4670500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.5036500096321106 | Loss: 1.3799343741714953 | Acc: 0.5036500000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.5186499953269958 | Loss: 1.3671279835909604 | Acc: 0.5186500000000001.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.5310500264167786 | Loss: 1.3451697098836304 | Acc: 0.5310500000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.5314499735832214 | Loss: 1.3393679239921272 | Acc: 0.5314500000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.5577499866485596 | Loss: 1.2752499621257185 | Acc: 0.5577500000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.5464500188827515 | Loss: 1.3134097831308842 | Acc: 0.5464500000000000.\nReturned to Spot: Validation loss: 1.3134097831308842\n\nconfig: {'l1': 64, 'l2': 512, 'lr_mult': 1.0, 'batch_size': 16, 'epochs': 16, 'k_folds': 0, 'patience': 3, 'optimizer': 'Adagrad', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.4566000103950500 | Loss: 1.4754037136554718 | Acc: 0.4566000000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.4854499995708466 | Loss: 1.4077516172885896 | Acc: 0.4854500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.5045499801635742 | Loss: 1.3551356828689576 | Acc: 0.5045500000000001.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.5176500082015991 | Loss: 1.3222326791286469 | Acc: 0.5176500000000001.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.5242499709129333 | Loss: 1.3060462886333466 | Acc: 0.5242500000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.5395500063896179 | Loss: 1.2658936917066574 | Acc: 0.5395500000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.5442000031471252 | Loss: 1.2592196984291077 | Acc: 0.5442000000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.5471500158309937 | Loss: 1.2472834128379822 | Acc: 0.5471500000000000.\nEpoch: 9 | \n\n\nMulticlassAccuracy: 0.5439000129699707 | Loss: 1.2604139344215393 | Acc: 0.5439000000000001.\nEpoch: 10 | \n\n\nMulticlassAccuracy: 0.5572000145912170 | Loss: 1.2293968648910523 | Acc: 0.5572000000000000.\nEpoch: 11 | \n\n\nMulticlassAccuracy: 0.5606999993324280 | Loss: 1.2252858976364136 | Acc: 0.5607000000000000.\nEpoch: 12 | \n\n\nMulticlassAccuracy: 0.5637500286102295 | Loss: 1.2164854894876480 | Acc: 0.5637500000000000.\nEpoch: 13 | \n\n\nMulticlassAccuracy: 0.5646499991416931 | Loss: 1.2217648525476457 | Acc: 0.5646500000000000.\nEpoch: 14 | \n\n\nMulticlassAccuracy: 0.5709000229835510 | Loss: 1.2099438600301742 | Acc: 0.5709000000000000.\nEpoch: 15 | \n\n\nMulticlassAccuracy: 0.5697000026702881 | Loss: 1.2078286586761475 | Acc: 0.5697000000000000.\nEpoch: 16 | \n\n\nMulticlassAccuracy: 0.5752999782562256 | Loss: 1.2011896395444870 | Acc: 0.5753000000000000.\nReturned to Spot: Validation loss: 1.201189639544487\n\n\n\nconfig: {'l1': 64, 'l2': 256, 'lr_mult': 1.0, 'batch_size': 16, 'epochs': 16, 'k_folds': 0, 'patience': 3, 'optimizer': 'Adagrad', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.4532000124454498 | Loss: 1.5017570102691651 | Acc: 0.4532000000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.4838500022888184 | Loss: 1.4474858260631562 | Acc: 0.4838500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.5088999867439270 | Loss: 1.3698600878238678 | Acc: 0.5089000000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.5187500119209290 | Loss: 1.3327592407703399 | Acc: 0.5187500000000000.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.5259000062942505 | Loss: 1.3174309848785399 | Acc: 0.5259000000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.5360000133514404 | Loss: 1.2933563767433167 | Acc: 0.5360000000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.5437499880790710 | Loss: 1.2758848504066467 | Acc: 0.5437500000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.5458999872207642 | Loss: 1.2721772467613219 | Acc: 0.5459000000000001.\nEpoch: 9 | \n\n\nMulticlassAccuracy: 0.5503500103950500 | Loss: 1.2617935348987579 | Acc: 0.5503500000000000.\nEpoch: 10 | \n\n\nMulticlassAccuracy: 0.5540000200271606 | Loss: 1.2496289710521697 | Acc: 0.5540000000000000.\nEpoch: 11 | \n\n\nMulticlassAccuracy: 0.5611000061035156 | Loss: 1.2301662773847579 | Acc: 0.5611000000000000.\nEpoch: 12 | \n\n\nMulticlassAccuracy: 0.5616499781608582 | Loss: 1.2375125718116760 | Acc: 0.5616500000000000.\nEpoch: 13 | \n\n\nMulticlassAccuracy: 0.5703499913215637 | Loss: 1.2185802567005157 | Acc: 0.5703500000000000.\nEpoch: 14 | \n\n\nMulticlassAccuracy: 0.5703499913215637 | Loss: 1.2083106976985931 | Acc: 0.5703500000000000.\nEpoch: 15 | \n\n\nMulticlassAccuracy: 0.5744000077247620 | Loss: 1.2058240345478057 | Acc: 0.5744000000000000.\nEpoch: 16 | \n\n\nMulticlassAccuracy: 0.5743499994277954 | Loss: 1.2105182710647584 | Acc: 0.5743500000000000.\nReturned to Spot: Validation loss: 1.2105182710647584\n\n\nspotPython tuning: 1.201189639544487 [####------] 37.20% \n\n\n\nconfig: {'l1': 64, 'l2': 128, 'lr_mult': 1.0, 'batch_size': 2, 'epochs': 8, 'k_folds': 0, 'patience': 3, 'optimizer': 'Adam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.4550999999046326 | Loss: 1.5234027903952170 | Acc: 0.4551000000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.4971500039100647 | Loss: 1.4979069290055376 | Acc: 0.4971500000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.5133500099182129 | Loss: 1.6043748682242724 | Acc: 0.5133500000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.5389500260353088 | Loss: 1.5171124046737854 | Acc: 0.5389500000000000.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.5473999977111816 | Loss: 1.4550704943379107 | Acc: 0.5474000000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.5594499707221985 | Loss: 1.5017284408286844 | Acc: 0.5594500000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.5680000185966492 | Loss: 1.4544702492281314 | Acc: 0.5679999999999999.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.5475000143051147 | Loss: 1.6271040855536409 | Acc: 0.5475000000000000.\nReturned to Spot: Validation loss: 1.627104085553641\n\n\nspotPython tuning: 1.201189639544487 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x186e0beb0&gt;"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#sec-tensorboard-14",
    "href": "14_spot_ray_hpt_torch_cifar10.html#sec-tensorboard-14",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.9 Step 9: Tensorboard",
    "text": "14.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard.\n\n14.9.1 Tensorboard: Start Tensorboard\nStart TensorBoard through the command line to visualize data you logged. Specify the root log directory as used in fun_control = fun_control_init(task=\"regression\", tensorboard_path=\"runs/24_spot_torch_regression\") as the tensorboard_path. The argument logdir points to directory where TensorBoard will look to find event files that it can display. TensorBoard will recursively walk the directory structure rooted at logdir, looking for .tfevents. files.\ntensorboard --logdir=runs\nGo to the URL it provides or to http://localhost:6006/. The following figures show some screenshots of Tensorboard.\n\n\n\nFigure 14.1: Tensorboard\n\n\n\n\n\nFigure 14.2: Tensorboard\n\n\n\n\n14.9.2 Saving the State of the Notebook\nThe state of the notebook can be saved and reloaded as follows:\n\nimport pickle\nSAVE = False\nLOAD = False\n\nif SAVE:\n    result_file_name = \"res_\" + experiment_name + \".pkl\"\n    with open(result_file_name, 'wb') as f:\n        pickle.dump(spot_tuner, f)\n\nif LOAD:\n    result_file_name = \"add_the_name_of_the_result_file_here.pkl\"\n    with open(result_file_name, 'rb') as f:\n        spot_tuner =  pickle.load(f)"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#sec-results-14",
    "href": "14_spot_ray_hpt_torch_cifar10.html#sec-results-14",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.10 Step 10: Results",
    "text": "14.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The following code generates the progress plot from ?fig-progress.\n\nspot_tuner.plot_progress(log_y=False, \n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n?fig-progress shows a typical behaviour that can be observed in many hyperparameter studies (Bartz et al. 2022): the largest improvement is obtained during the evaluation of the initial design. The surrogate model based optimization-optimization with the surrogate refines the results. ?fig-progress also illustrates one major difference between ray[tune] as used in PyTorch (2023a) and spotPython: the ray[tune] uses a random search and will generate results similar to the black dots, whereas spotPython uses a surrogate model based optimization and presents results represented by red dots in ?fig-progress. The surrogate model based optimization is considered to be more efficient than a random search, because the surrogate model guides the search towards promising regions in the hyperparameter space.\nIn addition to the improved (“optimized”) hyperparameter values, spotPython allows a statistical analysis, e.g., a sensitivity analysis, of the results. We can print the results of the hyperparameter tuning, see ?tbl-results. The table shows the hyperparameters, their types, default values, lower and upper bounds, and the transformation function. The column “tuned” shows the tuned values. The column “importance” shows the importance of the hyperparameters. The column “stars” shows the importance of the hyperparameters in stars. The importance is computed by the SPOT software.\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name         | type   | default   |   lower |   upper |   tuned | transform             |   importance | stars   |\n|--------------|--------|-----------|---------|---------|---------|-----------------------|--------------|---------|\n| l1           | int    | 5         |     2.0 |     9.0 |     6.0 | transform_power_2_int |        66.53 | **      |\n| l2           | int    | 5         |     2.0 |     9.0 |     9.0 | transform_power_2_int |         0.02 |         |\n| lr_mult      | float  | 1.0       |     1.0 |     1.0 |     1.0 | None                  |         0.00 |         |\n| batch_size   | int    | 4         |     1.0 |     5.0 |     4.0 | transform_power_2_int |       100.00 | ***     |\n| epochs       | int    | 3         |     3.0 |     4.0 |     4.0 | transform_power_2_int |         0.00 |         |\n| k_folds      | int    | 1         |     0.0 |     0.0 |     0.0 | None                  |         0.00 |         |\n| patience     | int    | 5         |     3.0 |     3.0 |     3.0 | None                  |         0.00 |         |\n| optimizer    | factor | SGD       |     0.0 |     9.0 |     1.0 | None                  |         2.65 | *       |\n| sgd_momentum | float  | 0.0       |     0.9 |     0.9 |     0.9 | None                  |         0.00 |         |\n\n\nTo visualize the most important hyperparameters, spotPython provides the function plot_importance. The following code generates the importance plot from ?fig-importance.\n\nspot_tuner.plot_importance(threshold=0.025,\n    filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n14.10.1 Get the Tuned Architecture (SPOT Results)\nThe architecture of the spotPython model can be obtained as follows. First, the numerical representation of the hyperparameters are obtained, i.e., the numpy array X is generated. This array is then used to generate the model model_spot by the function get_one_core_model_from_X. The model model_spot has the following architecture:\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\nmodel_spot\n\nNet_CIFAR10(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=512, bias=True)\n  (fc3): Linear(in_features=512, out_features=10, bias=True)\n)\n\n\n\n\n14.10.2 Get Default Hyperparameters\nIn a similar manner as in Section 14.10.1, the default hyperparameters can be obtained.\n\n# fun_control was modified, we generate a new one with the original \n# default hyperparameters\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nfc = fun_control\nfc.update({\"core_model_hyper_dict\":\n    hyper_dict[fun_control[\"core_model\"].__name__]})\nmodel_default = get_one_core_model_from_X(X_start, fun_control=fc)\nmodel_default\n\nNet_CIFAR10(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=32, bias=True)\n  (fc2): Linear(in_features=32, out_features=32, bias=True)\n  (fc3): Linear(in_features=32, out_features=10, bias=True)\n)\n\n\n\n\n14.10.3 Evaluation of the Default Architecture\nThe method train_tuned takes a model architecture without trained weights and trains this model with the train data. The train data is split into train and validation data. The validation data is used for early stopping. The trained model weights are saved as a dictionary.\nThis evaluation is similar to the final evaluation in PyTorch (2023a).\n\nfrom spotPython.torch.traintest import (\n    train_tuned,\n    test_tuned,\n    )\ntrain_tuned(net=model_default, train_dataset=train, shuffle=True,\n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        device = fun_control[\"device\"], show_batch_interval=1_000_000,\n        path=None,\n        task=fun_control[\"task\"],)\n\ntest_tuned(net=model_default, test_dataset=test, \n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        shuffle=False, \n        device = fun_control[\"device\"],\n        task=fun_control[\"task\"],)        \n\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.0995000004768372 | Loss: 2.3044450298309327 | Acc: 0.0995000000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.1039000004529953 | Loss: 2.3028123834609984 | Acc: 0.1039000000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.1176000013947487 | Loss: 2.3013742469787597 | Acc: 0.1176000000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.1338499933481216 | Loss: 2.2995696340560912 | Acc: 0.1338500000000000.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.1527500003576279 | Loss: 2.2966046888351439 | Acc: 0.1527500000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.1625999957323074 | Loss: 2.2908815006256105 | Acc: 0.1626000000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.1673000007867813 | Loss: 2.2764577651977538 | Acc: 0.1673000000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.2105499953031540 | Loss: 2.2344493661880493 | Acc: 0.2105500000000000.\nReturned to Spot: Validation loss: 2.2344493661880493\n\n\nMulticlassAccuracy: 0.2188999950885773 | Loss: 2.2321709022521974 | Acc: 0.2189000000000000.\nFinal evaluation: Validation loss: 2.2321709022521974\nFinal evaluation: Validation metric: 0.21889999508857727\n----------------------------------------------\n\n\n(2.2321709022521974, nan, tensor(0.2189))\n\n\n\n\n14.10.4 Evaluation of the Tuned Architecture\nThe following code trains the model model_spot.\nIf path is set to a filename, e.g., path = \"model_spot_trained.pt\", the weights of the trained model will be saved to this file.\nIf path is set to a filename, e.g., path = \"model_spot_trained.pt\", the weights of the trained model will be loaded from this file.\n\ntrain_tuned(net=model_spot, train_dataset=train,\n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        shuffle=True,\n        device = fun_control[\"device\"],\n        path=None,\n        task=fun_control[\"task\"],)\ntest_tuned(net=model_spot, test_dataset=test,\n            shuffle=False,\n            loss_function=fun_control[\"loss_function\"],\n            metric=fun_control[\"metric_torch\"],\n            device = fun_control[\"device\"],\n            task=fun_control[\"task\"],)\n\nEpoch: 1 | \n\n\nMulticlassAccuracy: 0.4342499971389771 | Loss: 1.5516764521121980 | Acc: 0.4342500000000000.\nEpoch: 2 | \n\n\nMulticlassAccuracy: 0.4722000062465668 | Loss: 1.4448473362922669 | Acc: 0.4722000000000000.\nEpoch: 3 | \n\n\nMulticlassAccuracy: 0.4871000051498413 | Loss: 1.4158093062877655 | Acc: 0.4871000000000000.\nEpoch: 4 | \n\n\nMulticlassAccuracy: 0.5069000124931335 | Loss: 1.3698412827014923 | Acc: 0.5069000000000000.\nEpoch: 5 | \n\n\nMulticlassAccuracy: 0.5153999924659729 | Loss: 1.3454468785762788 | Acc: 0.5154000000000000.\nEpoch: 6 | \n\n\nMulticlassAccuracy: 0.5220999717712402 | Loss: 1.3310146442413331 | Acc: 0.5221000000000000.\nEpoch: 7 | \n\n\nMulticlassAccuracy: 0.5242999792098999 | Loss: 1.3384939200401307 | Acc: 0.5243000000000000.\nEpoch: 8 | \n\n\nMulticlassAccuracy: 0.5397999882698059 | Loss: 1.3001257112979889 | Acc: 0.5397999999999999.\nEpoch: 9 | \n\n\nMulticlassAccuracy: 0.5405499935150146 | Loss: 1.2851560954332353 | Acc: 0.5405500000000000.\nEpoch: 10 | \n\n\nMulticlassAccuracy: 0.5453500151634216 | Loss: 1.2827382750034333 | Acc: 0.5453500000000000.\nEpoch: 11 | \n\n\nMulticlassAccuracy: 0.5504999756813049 | Loss: 1.2653467347383500 | Acc: 0.5505000000000000.\nEpoch: 12 | \n\n\nMulticlassAccuracy: 0.5558500289916992 | Loss: 1.2519751631498337 | Acc: 0.5558500000000000.\nEpoch: 13 | \n\n\nMulticlassAccuracy: 0.5612499713897705 | Loss: 1.2497284063339233 | Acc: 0.5612500000000000.\nEpoch: 14 | \n\n\nMulticlassAccuracy: 0.5599499940872192 | Loss: 1.2388118960618972 | Acc: 0.5599499999999999.\nEpoch: 15 | \n\n\nMulticlassAccuracy: 0.5644000172615051 | Loss: 1.2320098454952240 | Acc: 0.5644000000000000.\nEpoch: 16 | \n\n\nMulticlassAccuracy: 0.5692499876022339 | Loss: 1.2254201380014420 | Acc: 0.5692500000000000.\nReturned to Spot: Validation loss: 1.225420138001442\n\n\nMulticlassAccuracy: 0.5644000172615051 | Loss: 1.2276754027366639 | Acc: 0.5644000000000000.\nFinal evaluation: Validation loss: 1.2276754027366639\nFinal evaluation: Validation metric: 0.5644000172615051\n----------------------------------------------\n\n\n(1.2276754027366639, nan, tensor(0.5644))\n\n\n\n\n14.10.5 Detailed Hyperparameter Plots\nThe contour plots in this section visualize the interactions of the three most important hyperparameters. Since some of these hyperparameters take fatorial or integer values, sometimes step-like fitness landcapes (or response surfaces) are generated. SPOT draws the interactions of the main hyperparameters by default. It is also possible to visualize all interactions.\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nl1:  66.53234102024125\nbatch_size:  100.0\noptimizer:  2.6510977061704066\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n\n\n\n\nThe figures (?fig-contour) show the contour plots of the loss as a function of the hyperparameters. These plots are very helpful for benchmark studies and for understanding neural networks. spotPython provides additional tools for a visual inspection of the results and give valuable insights into the hyperparameter tuning process. This is especially useful for model explainability, transparency, and trustworthiness. In addition to the contour plots, ?fig-parallel shows the parallel plot of the hyperparameters.\n\nspot_tuner.parallel_plot()\n\n\n                                                \nParallel coordinates plots"
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#sec-summary",
    "href": "14_spot_ray_hpt_torch_cifar10.html#sec-summary",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.11 Summary and Outlook",
    "text": "14.11 Summary and Outlook\nThis tutorial presents the hyperparameter tuning open source software spotPython for PyTorch. To show its basic features, a comparison with the “official” PyTorch hyperparameter tuning tutorial (PyTorch 2023a) is presented. Some of the advantages of spotPython are:\n\nNumerical and categorical hyperparameters.\nPowerful surrogate models.\nFlexible approach and easy to use.\nSimple JSON files for the specification of the hyperparameters.\nExtension of default and user specified network classes.\nNoise handling techniques.\nInteraction with tensorboard.\n\nCurrently, only rudimentary parallel and distributed neural network training is possible, but these capabilities will be extended in the future. The next version of spotPython will also include a more detailed documentation and more examples.\n\n\n\n\n\n\nImportant\n\n\n\nImportant: This tutorial does not present a complete benchmarking study (Bartz-Beielstein et al. 2020). The results are only preliminary and highly dependent on the local configuration (hard- and software). Our goal is to provide a first impression of the performance of the hyperparameter tuning package spotPython. To demonstrate its capabilities, a quick comparison with ray[tune] was performed. ray[tune] was chosen, because it is presented as “an industry standard tool for distributed hyperparameter tuning.” The results should be interpreted with care."
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#sec-appendix",
    "href": "14_spot_ray_hpt_torch_cifar10.html#sec-appendix",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "14.12 Appendix",
    "text": "14.12 Appendix\n\n14.12.1 Sample Output From Ray Tune’s Run\nThe output from ray[tune] could look like this (PyTorch 2023b):\nNumber of trials: 10 (10 TERMINATED)\n------+------+-------------+--------------+---------+------------+--------------------+\n|   l1 |   l2 |          lr |   batch_size |    loss |   accuracy | training_iteration |\n+------+------+-------------+--------------+---------+------------+--------------------|\n|   64 |    4 | 0.00011629  |            2 | 1.87273 |     0.244  |                  2 |\n|   32 |   64 | 0.000339763 |            8 | 1.23603 |     0.567  |                  8 |\n|    8 |   16 | 0.00276249  |           16 | 1.1815  |     0.5836 |                 10 |\n|    4 |   64 | 0.000648721 |            4 | 1.31131 |     0.5224 |                  8 |\n|   32 |   16 | 0.000340753 |            8 | 1.26454 |     0.5444 |                  8 |\n|    8 |    4 | 0.000699775 |            8 | 1.99594 |     0.1983 |                  2 |\n|  256 |    8 | 0.0839654   |           16 | 2.3119  |     0.0993 |                  1 |\n|   16 |  128 | 0.0758154   |           16 | 2.33575 |     0.1327 |                  1 |\n|   16 |    8 | 0.0763312   |           16 | 2.31129 |     0.1042 |                  4 |\n|  128 |   16 | 0.000124903 |            4 | 2.26917 |     0.1945 |                  1 |\n+-----+------+------+-------------+--------------+---------+------------+--------------------+\nBest trial config: {'l1': 8, 'l2': 16, 'lr': 0.00276249, 'batch_size': 16, 'data_dir': '...'}\nBest trial final validation loss: 1.181501\nBest trial final validation accuracy: 0.5836\nBest trial test set accuracy: 0.5806\n\n\n\n\nBartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf Mersmann, eds. 2022. Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide. Springer.\n\n\nBartz-Beielstein, Thomas, Carola Doerr, Jakob Bossek, Sowmya Chandrasekaran, Tome Eftimov, Andreas Fischbach, Pascal Kerschke, et al. 2020. “Benchmarking in Optimization: Best Practice and Open Issues.” arXiv. https://arxiv.org/abs/2007.03488.\n\n\nMontiel, Jacob, Max Halford, Saulo Martiello Mastelini, Geoffrey Bolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, et al. 2021. “River: Machine Learning for Streaming Data in Python.”\n\n\nPyTorch. 2023a. “Hyperparameter Tuning with Ray Tune.” https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html.\n\n\n———. 2023b. “Training a Classifier.” https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html."
  },
  {
    "objectID": "14_spot_ray_hpt_torch_cifar10.html#footnotes",
    "href": "14_spot_ray_hpt_torch_cifar10.html#footnotes",
    "title": "14  HPT: PyTorch With spotPython and Ray Tune on CIFAR10",
    "section": "",
    "text": "Alternatively, the source code can be downloaded from gitHub: https://github.com/sequential-parameter-optimization/spotPython.↩︎\nWe were not able to install Ray Tune on our system. Therefore, we used the results from the PyTorch tutorial.↩︎"
  },
  {
    "objectID": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#sec-setup-16",
    "href": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#sec-setup-16",
    "title": "15  HPT: sklearn RandomForestClassifier VBDP Data",
    "section": "15.1 Step 1: Setup",
    "text": "15.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time and the initial design size.\n\nMAX_TIME = 1\nINIT_SIZE = 5\nORIGINAL = False\n\n\nimport os\nimport copy\nimport socket\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '16-rf-sklearn' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\n\n16-rf-sklearn_maans03_1min_5init_2023-06-28_04-06-19\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "href": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "title": "15  HPT: sklearn RandomForestClassifier VBDP Data",
    "section": "15.2 Step 2: Initialization of the Empty fun_control Dictionary",
    "text": "15.2 Step 2: Initialization of the Empty fun_control Dictionary\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"classification\",\n    tensorboard_path=\"runs/16_spot_hpt_sklearn_classification\")"
  },
  {
    "objectID": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-3-pytorch-data-loading",
    "href": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-3-pytorch-data-loading",
    "title": "15  HPT: sklearn RandomForestClassifier VBDP Data",
    "section": "15.3 Step 3: PyTorch Data Loading",
    "text": "15.3 Step 3: PyTorch Data Loading\n\n15.3.1 Load Data: Classification VBDP\n\nimport pandas as pd\nif ORIGINAL == True:\n    train_df = pd.read_csv('./data/VBDP/trainn.csv')\n    test_df = pd.read_csv('./data/VBDP/testt.csv')\nelse:\n    train_df = pd.read_csv('./data/VBDP/train.csv')\n    # remove the id column\n    train_df = train_df.drop(columns=['id'])\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\nn_samples = train_df.shape[0]\nn_features = train_df.shape[1] - 1\ntarget_column = \"prognosis\"\n# Encoder our prognosis labels as integers for easier decoding later\nenc = OrdinalEncoder()\ntrain_df[target_column] = enc.fit_transform(train_df[[target_column]])\ntrain_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nprint(train_df.shape)\ntrain_df.head()\n\n(707, 65)\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\n...\nx56\nx57\nx58\nx59\nx60\nx61\nx62\nx63\nx64\nprognosis\n\n\n\n\n0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n7.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n3.0\n\n\n3\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n6.0\n\n\n\n\n5 rows × 65 columns\n\n\n\nThe full data set train_df 64 features. The target column is labeled as prognosis.\n\n\n15.3.2 Holdout Train and Test Data\nWe split out a hold-out test set (25% of the data) so we can calculate an example MAP@K\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n                                                    random_state=42,\n                                                    test_size=0.25,\n                                                    stratify=train_df[target_column])\ntrain = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\ntest = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\ntrain.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\ntest.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nprint(train.shape)\nprint(test.shape)\ntrain.head()\n\n(530, 65)\n(177, 65)\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\n...\nx56\nx57\nx58\nx59\nx60\nx61\nx62\nx63\nx64\nprognosis\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n\n\n1\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4.0\n\n\n2\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n\n\n3\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n\n\n5 rows × 65 columns\n\n\n\n\n# add the dataset to the fun_control\nfun_control.update({\"data\": train_df, # full dataset,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": target_column})"
  },
  {
    "objectID": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#sec-specification-of-preprocessing-model-16",
    "href": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#sec-specification-of-preprocessing-model-16",
    "title": "15  HPT: sklearn RandomForestClassifier VBDP Data",
    "section": "15.4 Step 4: Specification of the Preprocessing Model",
    "text": "15.4 Step 4: Specification of the Preprocessing Model\nData preprocesssing can be very simple, e.g., you can ignore it. Then you would choose the prep_model “None”:\n\nprep_model = None\nfun_control.update({\"prep_model\": prep_model})\n\nA default approach for numerical data is the StandardScaler (mean 0, variance 1). This can be selected as follows:\n\n# prep_model = StandardScaler()\n# fun_control.update({\"prep_model\": prep_model})\n\nEven more complicated pre-processing steps are possible, e.g., the follwing pipeline:\n\n# categorical_columns = []\n# one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n# prep_model = ColumnTransformer(\n#         transformers=[\n#             (\"categorical\", one_hot_encoder, categorical_columns),\n#         ],\n#         remainder=StandardScaler(),\n#     )"
  },
  {
    "objectID": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-5-select-model-algorithm-and-core_model_hyper_dict",
    "href": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-5-select-model-algorithm-and-core_model_hyper_dict",
    "title": "15  HPT: sklearn RandomForestClassifier VBDP Data",
    "section": "15.5 Step 5: Select Model (algorithm) and core_model_hyper_dict",
    "text": "15.5 Step 5: Select Model (algorithm) and core_model_hyper_dict\nThe selection of the algorithm (ML model) that should be tuned is done by specifying the its name from the sklearn implementation. For example, the SVC support vector machine classifier is selected as follows:\nfun_control = add_core_model_to_fun_control(SVC, fun_control, SklearnHyperDict)\nOther core_models are, e.g.,:\n\nRidgeCV\nGradientBoostingRegressor\nElasticNet\nRandomForestClassifier\nLogisticRegression\nKNeighborsClassifier\nRandomForestClassifier\nGradientBoostingClassifier\nHistGradientBoostingClassifier\n\nWe will use the RandomForestClassifier classifier in this example.\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNet\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfrom spotPython.data.sklearn_hyper_dict import SklearnHyperDict\nfrom spotPython.fun.hypersklearn import HyperSklearn\n\n\n# core_model  = RidgeCV\n# core_model = GradientBoostingRegressor\n# core_model = ElasticNet\ncore_model = RandomForestClassifier\n# core_model = SVC\n# core_model = LogisticRegression\n# core_model = KNeighborsClassifier\n# core_model = GradientBoostingClassifier\nfun_control = add_core_model_to_fun_control(core_model=core_model,\n                              fun_control=fun_control,\n                              hyper_dict=SklearnHyperDict,\n                              filename=None)\n\nNow fun_control has the information from the JSON file. The available hyperparameters are:\n\nprint(*fun_control[\"core_model_hyper_dict\"].keys(), sep=\"\\n\")\n\nn_estimators\ncriterion\nmax_depth\nmin_samples_split\nmin_samples_leaf\nmin_weight_fraction_leaf\nmax_features\nmax_leaf_nodes\nmin_impurity_decrease\nbootstrap\noob_score"
  },
  {
    "objectID": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "href": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "title": "15  HPT: sklearn RandomForestClassifier VBDP Data",
    "section": "15.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "15.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\n\n15.6.1 Modify hyperparameter of type numeric and integer (boolean)\nNumeric and boolean values can be modified using the modify_hyper_parameter_bounds method. For example, to change the tol hyperparameter of the SVC model to the interval [1e-3, 1e-2], the following code can be used:\nfun_control = modify_hyper_parameter_bounds(fun_control, \"tol\", bounds=[1e-3, 1e-2])\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n# fun_control = modify_hyper_parameter_bounds(fun_control, \"tol\", bounds=[1e-3, 1e-2])\n\n\n\n15.6.2 Modify hyperparameter of type factor\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in Section 14.6.\nFactors can be modified with the modify_hyper_parameter_levels function. For example, to exclude the sigmoid kernel from the tuning, the kernel hyperparameter of the SVC model can be modified as follows:\nfun_control = modify_hyper_parameter_levels(fun_control, \"kernel\", [\"linear\", \"rbf\"])\nThe new setting can be controlled via:\nfun_control[\"core_model_hyper_dict\"][\"kernel\"]\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_levels\n# XGBoost:\n# fun_control = modify_hyper_parameter_levels(fun_control, \"loss\", [\"log_loss\"])\n\n\n\n\n\n\n\nNote: RandomForestClassifier and Out-of-bag Estimation\n\n\n\nSince oob_score requires the bootstrap hyperparameter to True, we set the oob_score parameter to False. The oob_score is later discussed in Section 15.7.3.\n\n\n\nfun_control = modify_hyper_parameter_bounds(fun_control, \"bootstrap\", bounds=[0, 1])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"oob_score\", bounds=[0, 0])\n\n\n\n15.6.3 Optimizers\nOptimizers are described in Section 14.6.1.\n\n\n15.6.4 Selection of the Objective: Metric and Loss Functions\n\nMachine learning models are optimized with respect to a metric, for example, the accuracy function.\nDeep learning, e.g., neural networks are optimized with respect to a loss function, for example, the cross_entropy function and evaluated with respect to a metric, for example, the accuracy function."
  },
  {
    "objectID": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-7-selection-of-the-objective-loss-function",
    "href": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-7-selection-of-the-objective-loss-function",
    "title": "15  HPT: sklearn RandomForestClassifier VBDP Data",
    "section": "15.7 Step 7: Selection of the Objective (Loss) Function",
    "text": "15.7 Step 7: Selection of the Objective (Loss) Function\nThe loss function, that is usually used in deep learning for optimizing the weights of the net, is stored in the fun_control dictionary as \"loss_function\".\n\n15.7.1 Metric Function\nThere are two different types of metrics in spotPython:\n\n\"metric_river\" is used for the river based evaluation via eval_oml_iter_progressive.\n\"metric_sklearn\" is used for the sklearn based evaluation.\n\nWe will consider multi-class classification metrics, e.g., mapk_score and top_k_accuracy_score.\n\n\n\n\n\n\nPredict Probabilities\n\n\n\nIn this multi-class classification example the machine learning algorithm should return the probabilities of the specific classes (\"predict_proba\") instead of the predicted values.\n\n\nWe set \"predict_proba\" to True in the fun_control dictionary.\n\n15.7.1.1 The MAPK Metric\nTo select the MAPK metric, the following two entries can be added to the fun_control dictionary:\n\"metric_sklearn\": mapk_score\"\n\"metric_params\": {\"k\": 3}.\n\n\n15.7.1.2 Other Metrics\nAlternatively, other metrics for multi-class classification can be used, e.g.,: * top_k_accuracy_score or * roc_auc_score\nThe metric roc_auc_score requires the parameter \"multi_class\", e.g.,\n\"multi_class\": \"ovr\".\nThis is set in the fun_control dictionary.\n\n\n\n\n\n\nWeights\n\n\n\nspotPython performs a minimization, therefore, metrics that should be maximized have to be multiplied by -1. This is done by setting \"weights\" to -1.\n\n\n\nThe complete setup for the metric in our example is:\n\n\nfrom spotPython.utils.metrics import mapk_score\nfun_control.update({\n               \"weights\": -1,\n               \"metric_sklearn\": mapk_score,\n               \"predict_proba\": True,\n               \"metric_params\": {\"k\": 3},\n               })\n\n\n\n\n15.7.2 Evaluation on Hold-out Data\n\nThe default method for computing the performance is \"eval_holdout\".\nAlternatively, cross-validation can be used for every machine learning model.\nSpecifically for RandomForests, the OOB-score can be used.\n\n\nfun_control.update({\n    \"eval\": \"train_hold_out\",\n})\n\n\n\n15.7.3 OOB Score\nUsing the OOB-Score is a very efficient way to estimate the performance of a random forest classifier. The OOB-Score is calculated on the training data and does not require a hold-out test set. If the OOB-Score is used, the key “eval” in the fun_control dictionary should be set to \"oob_score\" as shown below.\n\n\n\n\n\n\nOOB-Score\n\n\n\nIn addition to setting the key \"eval\" in the fun_control dictionary to \"oob_score\", the keys \"oob_score\" and \"bootstrap\" have to be set to True, because the OOB-Score requires the bootstrap method.\n\n\n\nUncomment the following lines to use the OOB-Score:\n\n\nfun_control.update({\n    \"eval\": \"eval_oob_score\",\n})\nfun_control = modify_hyper_parameter_bounds(fun_control, \"bootstrap\", bounds=[1, 1])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"oob_score\", bounds=[1, 1])\n\n\n15.7.3.1 Cross Validation\nInstead of using the OOB-score, the classical cross validation can be used. The number of folds is set by the key \"k_folds\". For example, to use 5-fold cross validation, the key \"k_folds\" is set to 5. Uncomment the following line to use cross validation:\n\n# fun_control.update({\n#      \"eval\": \"train_cv\",\n#      \"k_folds\": 10,\n# })"
  },
  {
    "objectID": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-8-calling-the-spot-function",
    "href": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#step-8-calling-the-spot-function",
    "title": "15  HPT: sklearn RandomForestClassifier VBDP Data",
    "section": "15.8 Step 8: Calling the SPOT Function",
    "text": "15.8 Step 8: Calling the SPOT Function\n\n15.8.1 Preparing the SPOT Call\n\nGet types and variable names as well as lower and upper bounds for the hyperparameters.\n\n\n# extract the variable types, names, and bounds\nfrom spotPython.hyperparameters.values import (get_bound_values,\n    get_var_name,\n    get_var_type,)\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name                     | type   | default   |   lower |   upper | transform              |\n|--------------------------|--------|-----------|---------|---------|------------------------|\n| n_estimators             | int    | 7         |       5 |   10    | transform_power_2_int  |\n| criterion                | factor | gini      |       0 |    2    | None                   |\n| max_depth                | int    | 10        |       1 |   20    | transform_power_2_int  |\n| min_samples_split        | int    | 2         |       2 |  100    | None                   |\n| min_samples_leaf         | int    | 1         |       1 |   25    | None                   |\n| min_weight_fraction_leaf | float  | 0.0       |       0 |    0.01 | None                   |\n| max_features             | factor | sqrt      |       0 |    1    | transform_none_to_None |\n| max_leaf_nodes           | int    | 10        |       7 |   12    | transform_power_2_int  |\n| min_impurity_decrease    | float  | 0.0       |       0 |    0.01 | None                   |\n| bootstrap                | factor | 1         |       1 |    1    | None                   |\n| oob_score                | factor | 0         |       1 |    1    | None                   |\n\n\n\n\n15.8.2 The Objective Function\nThe objective function is selected next. It implements an interface from sklearn’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hypersklearn import HyperSklearn\nfun = HyperSklearn().fun_sklearn\n\n\n\n15.8.3 Run the Spot Optimizer\n\nRun SPOT for approx. x mins (max_time).\nNote: the run takes longer, because the evaluation time of initial design (here: initi_size, 20 points) is not considered.\n\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nhyper_dict=SklearnHyperDict().load()\nX_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\nX_start\n\narray([[ 7.,  0., 10.,  2.,  1.,  0.,  0., 10.,  0.,  1.,  0.]])\n\n\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom math import inf\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\nspotPython tuning: -0.34371069182389935 [----------] 2.10% \n\n\nspotPython tuning: -0.34371069182389935 [----------] 3.46% \n\n\nspotPython tuning: -0.34371069182389935 [#---------] 5.48% \n\n\nspotPython tuning: -0.34371069182389935 [#---------] 7.50% \n\n\nspotPython tuning: -0.34371069182389935 [#---------] 9.66% \n\n\nspotPython tuning: -0.34402515723270444 [#---------] 12.22% \n\n\nspotPython tuning: -0.3474842767295597 [##--------] 15.18% \n\n\nspotPython tuning: -0.3474842767295597 [##--------] 17.96% \n\n\nspotPython tuning: -0.3474842767295597 [##--------] 21.07% \n\n\nspotPython tuning: -0.3474842767295597 [##--------] 23.65% \n\n\nspotPython tuning: -0.3474842767295597 [###-------] 26.36% \n\n\nspotPython tuning: -0.3474842767295597 [###-------] 29.62% \n\n\nspotPython tuning: -0.3474842767295597 [###-------] 32.52% \n\n\nspotPython tuning: -0.35188679245283017 [####------] 35.92% \n\n\nspotPython tuning: -0.35188679245283017 [####------] 40.02% \n\n\nspotPython tuning: -0.35188679245283017 [####------] 43.97% \n\n\nspotPython tuning: -0.35188679245283017 [#####-----] 48.24% \n\n\nspotPython tuning: -0.35188679245283017 [#####-----] 52.08% \n\n\nspotPython tuning: -0.35188679245283017 [######----] 55.61% \n\n\nspotPython tuning: -0.35188679245283017 [######----] 59.81% \n\n\nspotPython tuning: -0.35188679245283017 [######----] 63.90% \n\n\nspotPython tuning: -0.35188679245283017 [#######---] 70.86% \n\n\nspotPython tuning: -0.35188679245283017 [########--] 76.32% \n\n\nspotPython tuning: -0.35251572327044023 [########--] 79.98% \n\n\nspotPython tuning: -0.35251572327044023 [########--] 84.90% \n\n\nspotPython tuning: -0.3550314465408805 [#########-] 89.74% \n\n\nspotPython tuning: -0.3550314465408805 [#########-] 94.64% \n\n\nspotPython tuning: -0.3550314465408805 [##########] 99.32% \n\n\nspotPython tuning: -0.3550314465408805 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x185f36950&gt;"
  },
  {
    "objectID": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#sec-tensorboard-16",
    "href": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#sec-tensorboard-16",
    "title": "15  HPT: sklearn RandomForestClassifier VBDP Data",
    "section": "15.9 Step 9: Tensorboard",
    "text": "15.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard as described in Section 14.9, see also the description in the documentation: Tensorboard."
  },
  {
    "objectID": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#sec-results-tuning-16",
    "href": "16_spot_hpt_sklearn_multiclass_classification_randomforest.html#sec-results-tuning-16",
    "title": "15  HPT: sklearn RandomForestClassifier VBDP Data",
    "section": "15.10 Step 10: Results",
    "text": "15.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The following code generates the progress plot from ?fig-progress.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nPrint the results\n\n\nprint(gen_design_table(fun_control=fun_control,\n    spot=spot_tuner))\n\n| name                     | type   | default   |   lower |   upper |   tuned | transform              |   importance | stars   |\n|--------------------------|--------|-----------|---------|---------|---------|------------------------|--------------|---------|\n| n_estimators             | int    | 7         |     5.0 |    10.0 |     8.0 | transform_power_2_int  |        22.31 | *       |\n| criterion                | factor | gini      |     0.0 |     2.0 |     1.0 | None                   |         1.35 | *       |\n| max_depth                | int    | 10        |     1.0 |    20.0 |     8.0 | transform_power_2_int  |        11.98 | *       |\n| min_samples_split        | int    | 2         |     2.0 |   100.0 |    14.0 | None                   |       100.00 | ***     |\n| min_samples_leaf         | int    | 1         |     1.0 |    25.0 |     1.0 | None                   |         1.50 | *       |\n| min_weight_fraction_leaf | float  | 0.0       |     0.0 |    0.01 |    0.01 | None                   |         0.00 |         |\n| max_features             | factor | sqrt      |     0.0 |     1.0 |     0.0 | transform_none_to_None |         0.02 |         |\n| max_leaf_nodes           | int    | 10        |     7.0 |    12.0 |     9.0 | transform_power_2_int  |         0.00 |         |\n| min_impurity_decrease    | float  | 0.0       |     0.0 |    0.01 |    0.01 | None                   |         0.00 |         |\n| bootstrap                | factor | 1         |     1.0 |     1.0 |     1.0 | None                   |         0.00 |         |\n| oob_score                | factor | 0         |     1.0 |     1.0 |     1.0 | None                   |         0.00 |         |\n\n\n\n15.10.1 Show variable importance\n\nspot_tuner.plot_importance(threshold=0.025, filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n\n15.10.2 Get Default Hyperparameters\n\nfrom spotPython.hyperparameters.values import get_default_values, transform_hyper_parameter_values\nvalues_default = get_default_values(fun_control)\nvalues_default = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values_default)\nvalues_default\n\n{'n_estimators': 128,\n 'criterion': 'gini',\n 'max_depth': 1024,\n 'min_samples_split': 2,\n 'min_samples_leaf': 1,\n 'min_weight_fraction_leaf': 0.0,\n 'max_features': 'sqrt',\n 'max_leaf_nodes': 1024,\n 'min_impurity_decrease': 0.0,\n 'bootstrap': 1,\n 'oob_score': 0}\n\n\n\nfrom sklearn.pipeline import make_pipeline\nmodel_default = make_pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"](**values_default))\nmodel_default\n\nPipeline(steps=[('nonetype', None),\n                ('randomforestclassifier',\n                 RandomForestClassifier(bootstrap=1, max_depth=1024,\n                                        max_leaf_nodes=1024, n_estimators=128,\n                                        oob_score=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('nonetype', None),\n                ('randomforestclassifier',\n                 RandomForestClassifier(bootstrap=1, max_depth=1024,\n                                        max_leaf_nodes=1024, n_estimators=128,\n                                        oob_score=0))])NoneNoneRandomForestClassifierRandomForestClassifier(bootstrap=1, max_depth=1024, max_leaf_nodes=1024,\n                       n_estimators=128, oob_score=0)\n\n\n\n\n15.10.3 Get SPOT Results\n\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nprint(X)\n\n[[8.0e+00 1.0e+00 8.0e+00 1.4e+01 1.0e+00 1.0e-02 0.0e+00 9.0e+00 1.0e-02\n  1.0e+00 1.0e+00]]\n\n\n\nfrom spotPython.hyperparameters.values import assign_values, return_conf_list_from_var_dict\nv_dict = assign_values(X, fun_control[\"var_name\"])\nreturn_conf_list_from_var_dict(var_dict=v_dict, fun_control=fun_control)\n\n[{'n_estimators': 256,\n  'criterion': 'entropy',\n  'max_depth': 256,\n  'min_samples_split': 14,\n  'min_samples_leaf': 1,\n  'min_weight_fraction_leaf': 0.01,\n  'max_features': 'sqrt',\n  'max_leaf_nodes': 512,\n  'min_impurity_decrease': 0.01,\n  'bootstrap': 1,\n  'oob_score': 1}]\n\n\n\nfrom spotPython.hyperparameters.values import get_one_sklearn_model_from_X\nmodel_spot = get_one_sklearn_model_from_X(X, fun_control)\nmodel_spot\n\nRandomForestClassifier(bootstrap=1, criterion='entropy', max_depth=256,\n                       max_leaf_nodes=512, min_impurity_decrease=0.01,\n                       min_samples_split=14, min_weight_fraction_leaf=0.01,\n                       n_estimators=256, oob_score=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(bootstrap=1, criterion='entropy', max_depth=256,\n                       max_leaf_nodes=512, min_impurity_decrease=0.01,\n                       min_samples_split=14, min_weight_fraction_leaf=0.01,\n                       n_estimators=256, oob_score=1)\n\n\n\n\n15.10.4 Evaluate SPOT Results\n\nFetch the data.\n\n\nfrom spotPython.utils.convert import get_Xy_from_df\nX_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\nX_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\nX_test.shape, y_test.shape\n\n((177, 64), (177,))\n\n\n\nFit the model with the tuned hyperparameters. This gives one result:\n\n\nmodel_spot.fit(X_train, y_train)\ny_pred = model_spot.predict_proba(X_test)\nres = mapk_score(y_true=y_test, y_pred=y_pred, k=3)\nres\n\n0.36252354048964214\n\n\n\ndef repeated_eval(n, model):\n    res_values = []\n    for i in range(n):\n        model.fit(X_train, y_train)\n        y_pred = model.predict_proba(X_test)\n        res = mapk_score(y_true=y_test, y_pred=y_pred, k=3)\n        res_values.append(res)\n    mean_res = np.mean(res_values)\n    print(f\"mean_res: {mean_res}\")\n    std_res = np.std(res_values)\n    print(f\"std_res: {std_res}\")\n    min_res = np.min(res_values)\n    print(f\"min_res: {min_res}\")\n    max_res = np.max(res_values)\n    print(f\"max_res: {max_res}\")\n    median_res = np.median(res_values)\n    print(f\"median_res: {median_res}\")\n    return mean_res, std_res, min_res, max_res, median_res\n\n\n\n15.10.5 Handling Non-deterministic Results\n\nBecause the model is non-determinstic, we perform \\(n=30\\) runs and calculate the mean and standard deviation of the performance metric.\n\n\n_ = repeated_eval(30, model_spot)\n\nmean_res: 0.35916509730069046\nstd_res: 0.00721915172325078\nmin_res: 0.33992467043314495\nmax_res: 0.37099811676082856\nmedian_res: 0.3592278719397363\n\n\n\n\n15.10.6 Evalution of the Default Hyperparameters\n\nmodel_default.fit(X_train, y_train)[\"randomforestclassifier\"]\n\nRandomForestClassifier(bootstrap=1, max_depth=1024, max_leaf_nodes=1024,\n                       n_estimators=128, oob_score=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(bootstrap=1, max_depth=1024, max_leaf_nodes=1024,\n                       n_estimators=128, oob_score=0)\n\n\n\nOne evaluation of the default hyperparameters is performed on the hold-out test set.\n\n\ny_pred = model_default.predict_proba(X_test)\nmapk_score(y_true=y_test, y_pred=y_pred, k=3)\n\n0.34086629001883245\n\n\nSince one single evaluation is not meaningful, we perform, similar to the evaluation of the SPOT results, \\(n=30\\) runs of the default setting and and calculate the mean and standard deviation of the performance metric.\n\n_ = repeated_eval(30, model_default)\n\nmean_res: 0.3399874450721908\nstd_res: 0.015374533097141626\nmin_res: 0.3022598870056497\nmax_res: 0.3700564971751412\nmedian_res: 0.3403954802259887\n\n\n\n\n15.10.7 Plot: Compare Predictions\n\nfrom spotPython.plot.validation import plot_confusion_matrix\nplot_confusion_matrix(model_default, fun_control, title = \"Default\")\n\n\n\n\n\nplot_confusion_matrix(model_spot, fun_control, title=\"SPOT\")\n\n\n\n\n\nmin(spot_tuner.y), max(spot_tuner.y)\n\n(-0.3550314465408805, -0.2855345911949685)\n\n\n\n\n15.10.8 Cross-validated Evaluations\n\nfrom spotPython.sklearn.traintest import evaluate_cv\nfun_control.update({\n     \"eval\": \"train_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.36163522012578614, None)\n\n\n\nfun_control.update({\n     \"eval\": \"test_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.32434640522875813, None)\n\n\n\nThis is the evaluation that will be used in the comparison:\n\n\nfun_control.update({\n     \"eval\": \"data_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.35939973172367534, None)\n\n\n\n\n15.10.9 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nn_estimators:  22.312455385058723\ncriterion:  1.3543965377805414\nmax_depth:  11.975451171350256\nmin_samples_split:  100.0\nmin_samples_leaf:  1.502338460779938\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.10.10 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n                                                \n\n\n\n\n15.10.11 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
  },
  {
    "objectID": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#sec-setup-17",
    "href": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#sec-setup-17",
    "title": "16  HPT: sklearn XGB Classifier VBDP Data",
    "section": "16.1 Step 1: Setup",
    "text": "16.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time and the initial design size.\n\nMAX_TIME = 1\nINIT_SIZE = 5\nORIGINAL = False\n\n\nimport os\nimport copy\nimport socket\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '17-xgb-sklearn' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\n\n17-xgb-sklearn_maans03_1min_5init_2023-06-28_04-11-47\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "href": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "title": "16  HPT: sklearn XGB Classifier VBDP Data",
    "section": "16.2 Step 2: Initialization of the Empty fun_control Dictionary",
    "text": "16.2 Step 2: Initialization of the Empty fun_control Dictionary\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"classification\",\n    tensorboard_path=\"runs/16_spot_hpt_sklearn_classification\")"
  },
  {
    "objectID": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#sec-data-loading-17",
    "href": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#sec-data-loading-17",
    "title": "16  HPT: sklearn XGB Classifier VBDP Data",
    "section": "16.3 Step 3: PyTorch Data Loading",
    "text": "16.3 Step 3: PyTorch Data Loading\n\n16.3.1 1. Load Data: Classification VBDP\n\nimport pandas as pd\nif ORIGINAL == True:\n    train_df = pd.read_csv('./data/VBDP/trainn.csv')\n    test_df = pd.read_csv('./data/VBDP/testt.csv')\nelse:\n    train_df = pd.read_csv('./data/VBDP/train.csv')\n    # remove the id column\n    train_df = train_df.drop(columns=['id'])\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\nn_samples = train_df.shape[0]\nn_features = train_df.shape[1] - 1\ntarget_column = \"prognosis\"\n# Encoder our prognosis labels as integers for easier decoding later\nenc = OrdinalEncoder()\ntrain_df[target_column] = enc.fit_transform(train_df[[target_column]])\ntrain_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nprint(train_df.shape)\ntrain_df.head()\n\n(707, 65)\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\n...\nx56\nx57\nx58\nx59\nx60\nx61\nx62\nx63\nx64\nprognosis\n\n\n\n\n0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n7.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n3.0\n\n\n3\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n6.0\n\n\n\n\n5 rows × 65 columns\n\n\n\nThe full data set train_df 64 features. The target column is labeled as prognosis.\n\n\n16.3.2 Holdout Train and Test Data\nWe split out a hold-out test set (25% of the data) so we can calculate an example MAP@K\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n                                                    random_state=42,\n                                                    test_size=0.25,\n                                                    stratify=train_df[target_column])\ntrain = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\ntest = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\ntrain.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\ntest.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nprint(train.shape)\nprint(test.shape)\ntrain.head()\n\n(530, 65)\n(177, 65)\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\n...\nx56\nx57\nx58\nx59\nx60\nx61\nx62\nx63\nx64\nprognosis\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n\n\n1\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4.0\n\n\n2\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n\n\n3\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n\n\n5 rows × 65 columns\n\n\n\n\n# add the dataset to the fun_control\nfun_control.update({\"data\": train_df, # full dataset,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": target_column})"
  },
  {
    "objectID": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#sec-specification-of-preprocessing-model-17",
    "href": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#sec-specification-of-preprocessing-model-17",
    "title": "16  HPT: sklearn XGB Classifier VBDP Data",
    "section": "16.4 Step 4: Specification of the Preprocessing Model",
    "text": "16.4 Step 4: Specification of the Preprocessing Model\nData preprocesssing can be very simple, e.g., you can ignore it. Then you would choose the prep_model “None”:\n\nprep_model = None\nfun_control.update({\"prep_model\": prep_model})\n\nA default approach for numerical data is the StandardScaler (mean 0, variance 1). This can be selected as follows:\n\n# prep_model = StandardScaler()\n# fun_control.update({\"prep_model\": prep_model})\n\nEven more complicated pre-processing steps are possible, e.g., the follwing pipeline:\n\n# categorical_columns = []\n# one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n# prep_model = ColumnTransformer(\n#         transformers=[\n#             (\"categorical\", one_hot_encoder, categorical_columns),\n#         ],\n#         remainder=StandardScaler(),\n#     )"
  },
  {
    "objectID": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#step-5-select-model-algorithm-and-core_model_hyper_dict",
    "href": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#step-5-select-model-algorithm-and-core_model_hyper_dict",
    "title": "16  HPT: sklearn XGB Classifier VBDP Data",
    "section": "16.5 Step 5: Select Model (algorithm) and core_model_hyper_dict",
    "text": "16.5 Step 5: Select Model (algorithm) and core_model_hyper_dict\nThe selection of the algorithm (ML model) that should be tuned is done by specifying the its name from the sklearn implementation. For example, the SVC support vector machine classifier is selected as follows:\nfun_control = add_core_model_to_fun_control(SVC, fun_control, SklearnHyperDict)\nOther core_models are, e.g.,:\n\nRidgeCV\nGradientBoostingRegressor\nElasticNet\nRandomForestClassifier\nLogisticRegression\nKNeighborsClassifier\nRandomForestClassifier\nGradientBoostingClassifier\nHistGradientBoostingClassifier\n\nWe will use the RandomForestClassifier classifier in this example.\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.linear_model import ElasticNet\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfrom spotPython.data.sklearn_hyper_dict import SklearnHyperDict\nfrom spotPython.fun.hypersklearn import HyperSklearn\n\n\n# core_model  = RidgeCV\n# core_model = GradientBoostingRegressor\n# core_model = ElasticNet\ncore_model = RandomForestClassifier\n# core_model = SVC\n# core_model = LogisticRegression\n# core_model = KNeighborsClassifier\n# core_model = GradientBoostingClassifier\ncore_model = HistGradientBoostingClassifier\nfun_control = add_core_model_to_fun_control(core_model=core_model,\n                              fun_control=fun_control,\n                              hyper_dict=SklearnHyperDict,\n                              filename=None)\n\nNow fun_control has the information from the JSON file. The available hyperparameters are:\n\nprint(*fun_control[\"core_model_hyper_dict\"].keys(), sep=\"\\n\")\n\nloss\nlearning_rate\nmax_iter\nmax_leaf_nodes\nmax_depth\nmin_samples_leaf\nl2_regularization\nmax_bins\nearly_stopping\nn_iter_no_change\ntol"
  },
  {
    "objectID": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "href": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "title": "16  HPT: sklearn XGB Classifier VBDP Data",
    "section": "16.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "16.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\n\n16.6.1 Modify hyperparameter of type numeric and integer (boolean)\nNumeric and boolean values can be modified using the modify_hyper_parameter_bounds method. For example, to change the tol hyperparameter of the SVC model to the interval [1e-3, 1e-2], the following code can be used:\nfun_control = modify_hyper_parameter_bounds(fun_control, \"tol\", bounds=[1e-3, 1e-2])\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n# fun_control = modify_hyper_parameter_bounds(fun_control, \"tol\", bounds=[1e-3, 1e-2])\n# fun_control = modify_hyper_parameter_bounds(fun_control, \"min_samples_split\", bounds=[3, 20])\n# fun_control = modify_hyper_parameter_bounds(fun_control, \"dual\", bounds=[0, 0])\n# fun_control = modify_hyper_parameter_bounds(fun_control, \"probability\", bounds=[1, 1])\n# fun_control[\"core_model_hyper_dict\"][\"tol\"]\n# fun_control = modify_hyper_parameter_bounds(fun_control, \"min_samples_leaf\", bounds=[1, 25])\n# fun_control = modify_hyper_parameter_bounds(fun_control, \"n_estimators\", bounds=[5, 10])\n\n\n\n16.6.2 Modify hyperparameter of type factor\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in Section 14.6.\nFactors can be modified with the modify_hyper_parameter_levels function. For example, to exclude the sigmoid kernel from the tuning, the kernel hyperparameter of the SVC model can be modified as follows:\nfun_control = modify_hyper_parameter_levels(fun_control, \"kernel\", [\"linear\", \"rbf\"])\nThe new setting can be controlled via:\nfun_control[\"core_model_hyper_dict\"][\"kernel\"]\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_levels\n# XGBoost:\nfun_control = modify_hyper_parameter_levels(fun_control, \"loss\", [\"log_loss\"])\n\n\n\n16.6.3 Optimizers\nOptimizers are described in Section 14.6.1."
  },
  {
    "objectID": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#step-7-selection-of-the-objective-loss-function",
    "href": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#step-7-selection-of-the-objective-loss-function",
    "title": "16  HPT: sklearn XGB Classifier VBDP Data",
    "section": "16.7 Step 7: Selection of the Objective (Loss) Function",
    "text": "16.7 Step 7: Selection of the Objective (Loss) Function\n\n16.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set and\nthe loss function (and a metric).\n\n\n\n16.7.2 Selection of the Objective: Metric and Loss Functions\n\nMachine learning models are optimized with respect to a metric, for example, the accuracy function.\nDeep learning, e.g., neural networks are optimized with respect to a loss function, for example, the cross_entropy function and evaluated with respect to a metric, for example, the accuracy function.\n\n\n\n16.7.3 Loss Function\nThe loss function, that is usually used in deep learning for optimizing the weights of the net, is stored in the fun_control dictionary as \"loss_function\".\n\n\n16.7.4 Metric Function\nThere are two different types of metrics in spotPython:\n\n\"metric_river\" is used for the river based evaluation via eval_oml_iter_progressive.\n\"metric_sklearn\" is used for the sklearn based evaluation.\n\nWe will consider multi-class classification metrics, e.g., mapk_score and top_k_accuracy_score.\n\n\n\n\n\n\nPredict Probabilities\n\n\n\nIn this multi-class classification example the machine learning algorithm should return the probabilities of the specific classes (\"predict_proba\") instead of the predicted values.\n\n\nWe set \"predict_proba\" to True in the fun_control dictionary.\n\n16.7.4.1 The MAPK Metric\nTo select the MAPK metric, the following two entries can be added to the fun_control dictionary:\n\"metric_sklearn\": mapk_score\"\n\"metric_params\": {\"k\": 3}.\n\n\n16.7.4.2 Other Metrics\nAlternatively, other metrics for multi-class classification can be used, e.g.,: * top_k_accuracy_score or * roc_auc_score\nThe metric roc_auc_score requires the parameter \"multi_class\", e.g.,\n\"multi_class\": \"ovr\".\nThis is set in the fun_control dictionary.\n\n\n\n\n\n\nWeights\n\n\n\nspotPython performs a minimization, therefore, metrics that should be maximized have to be multiplied by -1. This is done by setting \"weights\" to -1.\n\n\n\nThe complete setup for the metric in our example is:\n\n\nfrom spotPython.utils.metrics import mapk_score\nfun_control.update({\n               \"weights\": -1,\n               \"metric_sklearn\": mapk_score,\n               \"predict_proba\": True,\n               \"metric_params\": {\"k\": 3},\n               })\n\n\n\n\n16.7.5 Evaluation on Hold-out Data\n\nThe default method for computing the performance is \"eval_holdout\".\nAlternatively, cross-validation can be used for every machine learning model.\nSpecifically for RandomForests, the OOB-score can be used.\n\n\nfun_control.update({\n    \"eval\": \"train_hold_out\",\n})\n\n\n16.7.5.1 Cross Validation\nInstead of using the OOB-score, the classical cross validation can be used. The number of folds is set by the key \"k_folds\". For example, to use 5-fold cross validation, the key \"k_folds\" is set to 5. Uncomment the following line to use cross validation:\n\n# fun_control.update({\n#      \"eval\": \"train_cv\",\n#      \"k_folds\": 10,\n# })"
  },
  {
    "objectID": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#step-8-calling-the-spot-function",
    "href": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#step-8-calling-the-spot-function",
    "title": "16  HPT: sklearn XGB Classifier VBDP Data",
    "section": "16.8 Step 8: Calling the SPOT Function",
    "text": "16.8 Step 8: Calling the SPOT Function\n\n16.8.1 Preparing the SPOT Call\n\nGet types and variable names as well as lower and upper bounds for the hyperparameters.\n\n\n# extract the variable types, names, and bounds\nfrom spotPython.hyperparameters.values import (get_bound_values,\n    get_var_name,\n    get_var_type,)\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name              | type   | default   |   lower |   upper | transform             |\n|-------------------|--------|-----------|---------|---------|-----------------------|\n| loss              | factor | log_loss  |   0     |   0     | None                  |\n| learning_rate     | float  | -1.0      |  -5     |   0     | transform_power_10    |\n| max_iter          | int    | 7         |   3     |  10     | transform_power_2_int |\n| max_leaf_nodes    | int    | 5         |   1     |  12     | transform_power_2_int |\n| max_depth         | int    | 2         |   1     |  20     | transform_power_2_int |\n| min_samples_leaf  | int    | 4         |   2     |  10     | transform_power_2_int |\n| l2_regularization | float  | 0.0       |   0     |  10     | None                  |\n| max_bins          | int    | 255       | 127     | 255     | None                  |\n| early_stopping    | factor | 1         |   0     |   1     | None                  |\n| n_iter_no_change  | int    | 10        |   5     |  20     | None                  |\n| tol               | float  | 0.0001    |   1e-05 |   0.001 | None                  |\n\n\n\n\n16.8.2 The Objective Function\nThe objective function is selected next. It implements an interface from sklearn’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hypersklearn import HyperSklearn\nfun = HyperSklearn().fun_sklearn\n\n\n\n16.8.3 Run the Spot Optimizer\n\nRun SPOT for approx. x mins (max_time).\nNote: the run takes longer, because the evaluation time of initial design (here: initi_size, 20 points) is not considered.\n\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nhyper_dict=SklearnHyperDict().load()\nX_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\nX_start\n\narray([[ 0.00e+00, -1.00e+00,  7.00e+00,  5.00e+00,  2.00e+00,  4.00e+00,\n         0.00e+00,  2.55e+02,  1.00e+00,  1.00e+01,  1.00e-04]])\n\n\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom math import inf\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\nspotPython tuning: -0.3408521303258145 [----------] 4.79% \n\n\nspotPython tuning: -0.3433583959899749 [#---------] 13.95% \n\n\nspotPython tuning: -0.40601503759398494 [##--------] 19.92% \n\n\nspotPython tuning: -0.40601503759398494 [##--------] 24.49% \n\n\nspotPython tuning: -0.40601503759398494 [###-------] 28.23% \n\n\nspotPython tuning: -0.40601503759398494 [###-------] 33.98% \n\n\nspotPython tuning: -0.40601503759398494 [####------] 37.63% \n\n\nspotPython tuning: -0.40601503759398494 [####------] 41.15% \n\n\nspotPython tuning: -0.40601503759398494 [####------] 44.64% \n\n\nspotPython tuning: -0.40601503759398494 [#####-----] 49.42% \n\n\nspotPython tuning: -0.40601503759398494 [#####-----] 53.32% \n\n\nspotPython tuning: -0.40601503759398494 [######----] 59.08% \n\n\nspotPython tuning: -0.40601503759398494 [######----] 62.90% \n\n\nspotPython tuning: -0.40601503759398494 [#######---] 67.89% \n\n\nspotPython tuning: -0.40601503759398494 [########--] 79.30% \n\n\nspotPython tuning: -0.40601503759398494 [########--] 84.59% \n\n\nspotPython tuning: -0.40601503759398494 [#########-] 89.30% \n\n\nspotPython tuning: -0.40601503759398494 [#########-] 94.86% \n\n\nspotPython tuning: -0.40601503759398494 [##########] 99.64% \n\n\nspotPython tuning: -0.40601503759398494 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x1812c6950&gt;"
  },
  {
    "objectID": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#sec-tensorboard-17",
    "href": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#sec-tensorboard-17",
    "title": "16  HPT: sklearn XGB Classifier VBDP Data",
    "section": "16.9 Step 9: Tensorboard",
    "text": "16.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard as described in Section 14.9, see also the description in the documentation: Tensorboard."
  },
  {
    "objectID": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#sec-results-tuning-17",
    "href": "17_spot_hpt_sklearn_multiclass_classification_xgb.html#sec-results-tuning-17",
    "title": "16  HPT: sklearn XGB Classifier VBDP Data",
    "section": "16.10 Step 10: Results",
    "text": "16.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The following code generates the progress plot from ?fig-progress.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nPrint the results\n\n\nprint(gen_design_table(fun_control=fun_control,\n    spot=spot_tuner))\n\n| name              | type   | default   |   lower |   upper |                 tuned | transform             |   importance | stars   |\n|-------------------|--------|-----------|---------|---------|-----------------------|-----------------------|--------------|---------|\n| loss              | factor | log_loss  |     0.0 |     0.0 |                   0.0 | None                  |         0.00 |         |\n| learning_rate     | float  | -1.0      |    -5.0 |     0.0 |   -0.2974321731802472 | transform_power_10    |        34.14 | *       |\n| max_iter          | int    | 7         |     3.0 |    10.0 |                   8.0 | transform_power_2_int |         0.00 |         |\n| max_leaf_nodes    | int    | 5         |     1.0 |    12.0 |                   2.0 | transform_power_2_int |         0.00 |         |\n| max_depth         | int    | 2         |     1.0 |    20.0 |                  18.0 | transform_power_2_int |         1.67 | *       |\n| min_samples_leaf  | int    | 4         |     2.0 |    10.0 |                   2.0 | transform_power_2_int |       100.00 | ***     |\n| l2_regularization | float  | 0.0       |     0.0 |    10.0 |     6.779635143767343 | None                  |         0.00 |         |\n| max_bins          | int    | 255       |   127.0 |   255.0 |                 166.0 | None                  |         0.00 |         |\n| early_stopping    | factor | 1         |     0.0 |     1.0 |                   1.0 | None                  |         3.89 | *       |\n| n_iter_no_change  | int    | 10        |     5.0 |    20.0 |                   9.0 | None                  |        21.55 | *       |\n| tol               | float  | 0.0001    |   1e-05 |   0.001 | 0.0009997980355769824 | None                  |         0.00 |         |\n\n\n\n16.10.1 Show variable importance\n\nspot_tuner.plot_importance(threshold=0.025, filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n\n16.10.2 Get Default Hyperparameters\n\nfrom spotPython.hyperparameters.values import get_default_values, transform_hyper_parameter_values\nvalues_default = get_default_values(fun_control)\nvalues_default = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values_default)\nvalues_default\n\n{'loss': 'log_loss',\n 'learning_rate': 0.1,\n 'max_iter': 128,\n 'max_leaf_nodes': 32,\n 'max_depth': 4,\n 'min_samples_leaf': 16,\n 'l2_regularization': 0.0,\n 'max_bins': 255,\n 'early_stopping': 1,\n 'n_iter_no_change': 10,\n 'tol': 0.0001}\n\n\n\nfrom sklearn.pipeline import make_pipeline\nmodel_default = make_pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"](**values_default))\nmodel_default\n\nPipeline(steps=[('nonetype', None),\n                ('histgradientboostingclassifier',\n                 HistGradientBoostingClassifier(early_stopping=1, max_depth=4,\n                                                max_iter=128, max_leaf_nodes=32,\n                                                min_samples_leaf=16,\n                                                tol=0.0001))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('nonetype', None),\n                ('histgradientboostingclassifier',\n                 HistGradientBoostingClassifier(early_stopping=1, max_depth=4,\n                                                max_iter=128, max_leaf_nodes=32,\n                                                min_samples_leaf=16,\n                                                tol=0.0001))])NoneNoneHistGradientBoostingClassifierHistGradientBoostingClassifier(early_stopping=1, max_depth=4, max_iter=128,\n                               max_leaf_nodes=32, min_samples_leaf=16,\n                               tol=0.0001)\n\n\n\n\n16.10.3 Get SPOT Results\n\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nprint(X)\n\n[[ 0.00000000e+00 -2.97432173e-01  8.00000000e+00  2.00000000e+00\n   1.80000000e+01  2.00000000e+00  6.77963514e+00  1.66000000e+02\n   1.00000000e+00  9.00000000e+00  9.99798036e-04]]\n\n\n\nfrom spotPython.hyperparameters.values import assign_values, return_conf_list_from_var_dict\nv_dict = assign_values(X, fun_control[\"var_name\"])\nreturn_conf_list_from_var_dict(var_dict=v_dict, fun_control=fun_control)\n\n[{'loss': 'log_loss',\n  'learning_rate': 0.5041593510615527,\n  'max_iter': 256,\n  'max_leaf_nodes': 4,\n  'max_depth': 262144,\n  'min_samples_leaf': 4,\n  'l2_regularization': 6.779635143767343,\n  'max_bins': 166,\n  'early_stopping': 1,\n  'n_iter_no_change': 9,\n  'tol': 0.0009997980355769824}]\n\n\n\nfrom spotPython.hyperparameters.values import get_one_sklearn_model_from_X\nmodel_spot = get_one_sklearn_model_from_X(X, fun_control)\nmodel_spot\n\nHistGradientBoostingClassifier(early_stopping=1,\n                               l2_regularization=6.779635143767343,\n                               learning_rate=0.5041593510615527, max_bins=166,\n                               max_depth=262144, max_iter=256, max_leaf_nodes=4,\n                               min_samples_leaf=4, n_iter_no_change=9,\n                               tol=0.0009997980355769824)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingClassifierHistGradientBoostingClassifier(early_stopping=1,\n                               l2_regularization=6.779635143767343,\n                               learning_rate=0.5041593510615527, max_bins=166,\n                               max_depth=262144, max_iter=256, max_leaf_nodes=4,\n                               min_samples_leaf=4, n_iter_no_change=9,\n                               tol=0.0009997980355769824)\n\n\n\n\n16.10.4 Evaluate SPOT Results\n\nFetch the data.\n\n\nfrom spotPython.utils.convert import get_Xy_from_df\nX_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\nX_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\nX_test.shape, y_test.shape\n\n((177, 64), (177,))\n\n\n\nFit the model with the tuned hyperparameters. This gives one result:\n\n\nmodel_spot.fit(X_train, y_train)\ny_pred = model_spot.predict_proba(X_test)\nres = mapk_score(y_true=y_test, y_pred=y_pred, k=3)\nres\n\n0.3305084745762712\n\n\n\ndef repeated_eval(n, model):\n    res_values = []\n    for i in range(n):\n        model.fit(X_train, y_train)\n        y_pred = model.predict_proba(X_test)\n        res = mapk_score(y_true=y_test, y_pred=y_pred, k=3)\n        res_values.append(res)\n    mean_res = np.mean(res_values)\n    print(f\"mean_res: {mean_res}\")\n    std_res = np.std(res_values)\n    print(f\"std_res: {std_res}\")\n    min_res = np.min(res_values)\n    print(f\"min_res: {min_res}\")\n    max_res = np.max(res_values)\n    print(f\"max_res: {max_res}\")\n    median_res = np.median(res_values)\n    print(f\"median_res: {median_res}\")\n    return mean_res, std_res, min_res, max_res, median_res\n\n\n\n16.10.5 Handling Non-deterministic Results\n\nBecause the model is non-determinstic, we perform \\(n=30\\) runs and calculate the mean and standard deviation of the performance metric.\n\n\n_ = repeated_eval(30, model_spot)\n\nmean_res: 0.3345888261142498\nstd_res: 0.015634540246323457\nmin_res: 0.2994350282485876\nmax_res: 0.3672316384180791\nmedian_res: 0.3333333333333333\n\n\n\n\n16.10.6 Evalution of the Default Hyperparameters\n\nmodel_default.fit(X_train, y_train)[\"histgradientboostingclassifier\"]\n\nHistGradientBoostingClassifier(early_stopping=1, max_depth=4, max_iter=128,\n                               max_leaf_nodes=32, min_samples_leaf=16,\n                               tol=0.0001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingClassifierHistGradientBoostingClassifier(early_stopping=1, max_depth=4, max_iter=128,\n                               max_leaf_nodes=32, min_samples_leaf=16,\n                               tol=0.0001)\n\n\n\nOne evaluation of the default hyperparameters is performed on the hold-out test set.\n\n\ny_pred = model_default.predict_proba(X_test)\nmapk_score(y_true=y_test, y_pred=y_pred, k=3)\n\n0.3305084745762712\n\n\nSince one single evaluation is not meaningful, we perform, similar to the evaluation of the SPOT results, \\(n=30\\) runs of the default setting and and calculate the mean and standard deviation of the performance metric.\n\n_ = repeated_eval(30, model_default)\n\nmean_res: 0.3377275580665412\nstd_res: 0.013922673398731414\nmin_res: 0.3126177024482109\nmax_res: 0.3700564971751412\nmedian_res: 0.3347457627118644\n\n\n\n\n16.10.7 Plot: Compare Predictions\n\nfrom spotPython.plot.validation import plot_confusion_matrix\nplot_confusion_matrix(model_default, fun_control, title = \"Default\")\n\n\n\n\n\nplot_confusion_matrix(model_spot, fun_control, title=\"SPOT\")\n\n\n\n\n\nmin(spot_tuner.y), max(spot_tuner.y)\n\n(-0.40601503759398494, -0.20927318295739344)\n\n\n\n\n16.10.8 Cross-validated Evaluations\n\nfrom spotPython.sklearn.traintest import evaluate_cv\nfun_control.update({\n     \"eval\": \"train_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.3584905660377359, None)\n\n\n\nfun_control.update({\n     \"eval\": \"test_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.26786492374727666, None)\n\n\n\nThis is the evaluation that will be used in the comparison:\n\n\nfun_control.update({\n     \"eval\": \"data_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.3455667337357478, None)\n\n\n\n\n16.10.9 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nlearning_rate:  34.1428808190174\nmax_depth:  1.6669142574885158\nmin_samples_leaf:  100.0\nearly_stopping:  3.889403682750624\nn_iter_no_change:  21.54942182917185\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.10.10 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n                                                \n\n\n\n\n16.10.11 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
  },
  {
    "objectID": "18_spot_hpt_sklearn_multiclass_classification_svc.html#sec-setup-18",
    "href": "18_spot_hpt_sklearn_multiclass_classification_svc.html#sec-setup-18",
    "title": "17  HPT: sklearn SVC VBDP Data",
    "section": "17.1 Step 1: Setup",
    "text": "17.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time and the initial design size.\n\nMAX_TIME = 1\nINIT_SIZE = 5\nORIGINAL = False\n\n\nimport os\nimport copy\nimport socket\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '18-svc-sklearn' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\n\n18-svc-sklearn_maans03_1min_5init_2023-06-28_04-15-57\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "href": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "title": "17  HPT: sklearn SVC VBDP Data",
    "section": "17.2 Step 2: Initialization of the Empty fun_control Dictionary",
    "text": "17.2 Step 2: Initialization of the Empty fun_control Dictionary\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"classification\",\n    tensorboard_path=\"runs/16_spot_hpt_sklearn_classification\")"
  },
  {
    "objectID": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-3-pytorch-data-loading",
    "href": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-3-pytorch-data-loading",
    "title": "17  HPT: sklearn SVC VBDP Data",
    "section": "17.3 Step 3: PyTorch Data Loading",
    "text": "17.3 Step 3: PyTorch Data Loading\n\n17.3.1 1. Load Data: Classification VBDP\n\nimport pandas as pd\nif ORIGINAL == True:\n    train_df = pd.read_csv('./data/VBDP/trainn.csv')\n    test_df = pd.read_csv('./data/VBDP/testt.csv')\nelse:\n    train_df = pd.read_csv('./data/VBDP/train.csv')\n    # remove the id column\n    train_df = train_df.drop(columns=['id'])\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\nn_samples = train_df.shape[0]\nn_features = train_df.shape[1] - 1\ntarget_column = \"prognosis\"\n# Encoder our prognosis labels as integers for easier decoding later\nenc = OrdinalEncoder()\ntrain_df[target_column] = enc.fit_transform(train_df[[target_column]])\ntrain_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nprint(train_df.shape)\ntrain_df.head()\n\n(707, 65)\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\n...\nx56\nx57\nx58\nx59\nx60\nx61\nx62\nx63\nx64\nprognosis\n\n\n\n\n0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n7.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n3.0\n\n\n3\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n6.0\n\n\n\n\n5 rows × 65 columns\n\n\n\nThe full data set train_df 64 features. The target column is labeled as prognosis.\n\n\n17.3.2 Holdout Train and Test Data\nWe split out a hold-out test set (25% of the data) so we can calculate an example MAP@K\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n                                                    random_state=42,\n                                                    test_size=0.25,\n                                                    stratify=train_df[target_column])\ntrain = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\ntest = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\ntrain.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\ntest.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nprint(train.shape)\nprint(test.shape)\ntrain.head()\n\n(530, 65)\n(177, 65)\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\n...\nx56\nx57\nx58\nx59\nx60\nx61\nx62\nx63\nx64\nprognosis\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n\n\n1\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4.0\n\n\n2\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n\n\n3\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n\n\n5 rows × 65 columns\n\n\n\n\n# add the dataset to the fun_control\nfun_control.update({\"data\": train_df, # full dataset,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": target_column})"
  },
  {
    "objectID": "18_spot_hpt_sklearn_multiclass_classification_svc.html#sec-specification-of-preprocessing-model-18",
    "href": "18_spot_hpt_sklearn_multiclass_classification_svc.html#sec-specification-of-preprocessing-model-18",
    "title": "17  HPT: sklearn SVC VBDP Data",
    "section": "17.4 Step 4: Specification of the Preprocessing Model",
    "text": "17.4 Step 4: Specification of the Preprocessing Model\nData preprocesssing can be very simple, e.g., you can ignore it. Then you would choose the prep_model “None”:\n\nprep_model = None\nfun_control.update({\"prep_model\": prep_model})\n\nA default approach for numerical data is the StandardScaler (mean 0, variance 1). This can be selected as follows:\n\n# prep_model = StandardScaler()\n# fun_control.update({\"prep_model\": prep_model})\n\nEven more complicated pre-processing steps are possible, e.g., the follwing pipeline:\n\n# categorical_columns = []\n# one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n# prep_model = ColumnTransformer(\n#         transformers=[\n#             (\"categorical\", one_hot_encoder, categorical_columns),\n#         ],\n#         remainder=StandardScaler(),\n#     )"
  },
  {
    "objectID": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-5-select-model-algorithm-and-core_model_hyper_dict",
    "href": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-5-select-model-algorithm-and-core_model_hyper_dict",
    "title": "17  HPT: sklearn SVC VBDP Data",
    "section": "17.5 Step 5: Select Model (algorithm) and core_model_hyper_dict",
    "text": "17.5 Step 5: Select Model (algorithm) and core_model_hyper_dict\nThe selection of the algorithm (ML model) that should be tuned is done by specifying the its name from the sklearn implementation. For example, the SVC support vector machine classifier is selected as follows:\nfun_control = add_core_model_to_fun_control(SVC, fun_control, SklearnHyperDict)\nOther core_models are, e.g.,:\n\nRidgeCV\nGradientBoostingRegressor\nElasticNet\nRandomForestClassifier\nLogisticRegression\nKNeighborsClassifier\nRandomForestClassifier\nGradientBoostingClassifier\nHistGradientBoostingClassifier\n\nWe will use the RandomForestClassifier classifier in this example.\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.linear_model import ElasticNet\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfrom spotPython.data.sklearn_hyper_dict import SklearnHyperDict\nfrom spotPython.fun.hypersklearn import HyperSklearn\n\n\n# core_model  = RidgeCV\n# core_model = GradientBoostingRegressor\n# core_model = ElasticNet\n# core_model = RandomForestClassifier\ncore_model = SVC\n# core_model = LogisticRegression\n# core_model = KNeighborsClassifier\n# core_model = GradientBoostingClassifier\n# core_model = HistGradientBoostingClassifier\nfun_control = add_core_model_to_fun_control(core_model=core_model,\n                              fun_control=fun_control,\n                              hyper_dict=SklearnHyperDict,\n                              filename=None)\n\nNow fun_control has the information from the JSON file. The available hyperparameters are:\n\nprint(*fun_control[\"core_model_hyper_dict\"].keys(), sep=\"\\n\")\n\nC\nkernel\ndegree\ngamma\ncoef0\nshrinking\nprobability\ntol\ncache_size\nbreak_ties"
  },
  {
    "objectID": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "href": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "title": "17  HPT: sklearn SVC VBDP Data",
    "section": "17.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "17.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\n\n17.6.1 Modify hyperparameter of type numeric and integer (boolean)\nNumeric and boolean values can be modified using the modify_hyper_parameter_bounds method. For example, to change the tol hyperparameter of the SVC model to the interval [1e-3, 1e-2], the following code can be used:\nfun_control = modify_hyper_parameter_bounds(fun_control, \"tol\", bounds=[1e-3, 1e-2])\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\nfun_control = modify_hyper_parameter_bounds(fun_control, \"probability\", bounds=[1, 1])\n\n\n\n17.6.2 Modify hyperparameter of type factor\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in Section 14.6.\nFactors can be modified with the modify_hyper_parameter_levels function. For example, to exclude the sigmoid kernel from the tuning, the kernel hyperparameter of the SVC model can be modified as follows:\nfun_control = modify_hyper_parameter_levels(fun_control, \"kernel\", [\"linear\", \"rbf\"])\nThe new setting can be controlled via:\nfun_control[\"core_model_hyper_dict\"][\"kernel\"]\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_levels\nfun_control = modify_hyper_parameter_levels(fun_control, \"kernel\", [\"rbf\"])\n\n\n\n17.6.3 Optimizers\nOptimizers are described in Section 14.6.1.\n\n\n17.6.4 Selection of the Objective: Metric and Loss Functions\n\nMachine learning models are optimized with respect to a metric, for example, the accuracy function.\nDeep learning, e.g., neural networks are optimized with respect to a loss function, for example, the cross_entropy function and evaluated with respect to a metric, for example, the accuracy function."
  },
  {
    "objectID": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-7-selection-of-the-objective-loss-function",
    "href": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-7-selection-of-the-objective-loss-function",
    "title": "17  HPT: sklearn SVC VBDP Data",
    "section": "17.7 Step 7: Selection of the Objective (Loss) Function",
    "text": "17.7 Step 7: Selection of the Objective (Loss) Function\nThe loss function, that is usually used in deep learning for optimizing the weights of the net, is stored in the fun_control dictionary as \"loss_function\".\n\n17.7.1 Metric Function\nThere are two different types of metrics in spotPython:\n\n\"metric_river\" is used for the river based evaluation via eval_oml_iter_progressive.\n\"metric_sklearn\" is used for the sklearn based evaluation.\n\nWe will consider multi-class classification metrics, e.g., mapk_score and top_k_accuracy_score.\n\n\n\n\n\n\nPredict Probabilities\n\n\n\nIn this multi-class classification example the machine learning algorithm should return the probabilities of the specific classes (\"predict_proba\") instead of the predicted values.\n\n\nWe set \"predict_proba\" to True in the fun_control dictionary.\n\n17.7.1.1 The MAPK Metric\nTo select the MAPK metric, the following two entries can be added to the fun_control dictionary:\n\"metric_sklearn\": mapk_score\"\n\"metric_params\": {\"k\": 3}.\n\n\n17.7.1.2 Other Metrics\nAlternatively, other metrics for multi-class classification can be used, e.g.,: * top_k_accuracy_score or * roc_auc_score\nThe metric roc_auc_score requires the parameter \"multi_class\", e.g.,\n\"multi_class\": \"ovr\".\nThis is set in the fun_control dictionary.\n\n\n\n\n\n\nWeights\n\n\n\nspotPython performs a minimization, therefore, metrics that should be maximized have to be multiplied by -1. This is done by setting \"weights\" to -1.\n\n\n\nThe complete setup for the metric in our example is:\n\n\nfrom spotPython.utils.metrics import mapk_score\nfun_control.update({\n               \"weights\": -1,\n               \"metric_sklearn\": mapk_score,\n               \"predict_proba\": True,\n               \"metric_params\": {\"k\": 3},\n               })\n\n\n\n\n17.7.2 Evaluation on Hold-out Data\n\nThe default method for computing the performance is \"eval_holdout\".\nAlternatively, cross-validation can be used for every machine learning model.\nSpecifically for RandomForests, the OOB-score can be used.\n\n\nfun_control.update({\n    \"eval\": \"train_hold_out\",\n})\n\n\n17.7.2.1 Cross Validation\nInstead of using the OOB-score, the classical cross validation can be used. The number of folds is set by the key \"k_folds\". For example, to use 5-fold cross validation, the key \"k_folds\" is set to 5. Uncomment the following line to use cross validation:\n\n# fun_control.update({\n#      \"eval\": \"train_cv\",\n#      \"k_folds\": 10,\n# })"
  },
  {
    "objectID": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-8-calling-the-spot-function",
    "href": "18_spot_hpt_sklearn_multiclass_classification_svc.html#step-8-calling-the-spot-function",
    "title": "17  HPT: sklearn SVC VBDP Data",
    "section": "17.8 Step 8: Calling the SPOT Function",
    "text": "17.8 Step 8: Calling the SPOT Function\n\n17.8.1 Preparing the SPOT Call\n\nGet types and variable names as well as lower and upper bounds for the hyperparameters.\n\n\n# extract the variable types, names, and bounds\nfrom spotPython.hyperparameters.values import (get_bound_values,\n    get_var_name,\n    get_var_type,)\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name        | type   | default   |    lower |   upper | transform   |\n|-------------|--------|-----------|----------|---------|-------------|\n| C           | float  | 1.0       |   0.1    |   10    | None        |\n| kernel      | factor | rbf       |   0      |    0    | None        |\n| degree      | int    | 3         |   3      |    3    | None        |\n| gamma       | factor | scale     |   0      |    1    | None        |\n| coef0       | float  | 0.0       |   0      |    0    | None        |\n| shrinking   | factor | 0         |   0      |    1    | None        |\n| probability | factor | 0         |   1      |    1    | None        |\n| tol         | float  | 0.001     |   0.0001 |    0.01 | None        |\n| cache_size  | float  | 200.0     | 100      |  400    | None        |\n| break_ties  | factor | 0         |   0      |    1    | None        |\n\n\n\n\n17.8.2 The Objective Function\nThe objective function is selected next. It implements an interface from sklearn’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hypersklearn import HyperSklearn\nfun = HyperSklearn().fun_sklearn\n\n\n\n17.8.3 Run the Spot Optimizer\n\nRun SPOT for approx. x mins (max_time).\nNote: the run takes longer, because the evaluation time of initial design (here: initi_size, 20 points) is not considered.\n\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nhyper_dict=SklearnHyperDict().load()\nX_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\nX_start\n\narray([[1.e+00, 2.e+00, 3.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-03,\n        2.e+02, 0.e+00]])\n\n\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom math import inf\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\nspotPython tuning: -0.3809523809523809 [----------] 0.72% \n\n\nspotPython tuning: -0.3809523809523809 [----------] 1.49% \n\n\nspotPython tuning: -0.3809523809523809 [----------] 2.20% \n\n\nspotPython tuning: -0.3809523809523809 [----------] 4.03% \n\n\nspotPython tuning: -0.3809523809523809 [#---------] 5.37% \n\n\nspotPython tuning: -0.3809523809523809 [#---------] 6.50% \n\n\nspotPython tuning: -0.3809523809523809 [#---------] 7.58% \n\n\nspotPython tuning: -0.3809523809523809 [#---------] 9.49% \n\n\nspotPython tuning: -0.3822055137844611 [#---------] 12.24% \n\n\nspotPython tuning: -0.3822055137844611 [#---------] 13.57% \n\n\nspotPython tuning: -0.3822055137844611 [#---------] 14.87% \n\n\nspotPython tuning: -0.3822055137844611 [##--------] 16.13% \n\n\nspotPython tuning: -0.3822055137844611 [##--------] 17.76% \n\n\nspotPython tuning: -0.3822055137844611 [##--------] 19.54% \n\n\nspotPython tuning: -0.3822055137844611 [##--------] 21.20% \n\n\nspotPython tuning: -0.3822055137844611 [##--------] 22.71% \n\n\nspotPython tuning: -0.3822055137844611 [##--------] 24.26% \n\n\nspotPython tuning: -0.3822055137844611 [###-------] 25.91% \n\n\nspotPython tuning: -0.38596491228070173 [###-------] 27.64% \n\n\nspotPython tuning: -0.38596491228070173 [###-------] 29.34% \n\n\nspotPython tuning: -0.38596491228070173 [###-------] 31.01% \n\n\nspotPython tuning: -0.38596491228070173 [###-------] 32.69% \n\n\nspotPython tuning: -0.38596491228070173 [###-------] 34.33% \n\n\nspotPython tuning: -0.38596491228070173 [####------] 35.86% \n\n\nspotPython tuning: -0.38596491228070173 [####------] 37.55% \n\n\nspotPython tuning: -0.38596491228070173 [####------] 39.36% \n\n\nspotPython tuning: -0.38596491228070173 [####------] 41.15% \n\n\nspotPython tuning: -0.38596491228070173 [####------] 42.99% \n\n\nspotPython tuning: -0.38596491228070173 [####------] 44.72% \n\n\nspotPython tuning: -0.38596491228070173 [#####-----] 46.46% \n\n\nspotPython tuning: -0.38596491228070173 [#####-----] 48.54% \n\n\nspotPython tuning: -0.38596491228070173 [#####-----] 50.19% \n\n\nspotPython tuning: -0.38596491228070173 [#####-----] 52.00% \n\n\nspotPython tuning: -0.38596491228070173 [#####-----] 53.66% \n\n\nspotPython tuning: -0.38596491228070173 [######----] 55.67% \n\n\nspotPython tuning: -0.38596491228070173 [######----] 57.75% \n\n\nspotPython tuning: -0.38596491228070173 [######----] 59.49% \n\n\nspotPython tuning: -0.38596491228070173 [######----] 61.38% \n\n\nspotPython tuning: -0.38596491228070173 [######----] 63.40% \n\n\nspotPython tuning: -0.38596491228070173 [#######---] 65.25% \n\n\nspotPython tuning: -0.38596491228070173 [#######---] 67.07% \n\n\nspotPython tuning: -0.38596491228070173 [#######---] 68.98% \n\n\nspotPython tuning: -0.38596491228070173 [#######---] 71.06% \n\n\nspotPython tuning: -0.38596491228070173 [#######---] 73.16% \n\n\nspotPython tuning: -0.38596491228070173 [########--] 75.12% \n\n\nspotPython tuning: -0.38596491228070173 [########--] 77.43% \n\n\nspotPython tuning: -0.39223057644110276 [########--] 79.67% \n\n\nspotPython tuning: -0.39223057644110276 [########--] 81.61% \n\n\nspotPython tuning: -0.39223057644110276 [########--] 83.71% \n\n\nspotPython tuning: -0.39223057644110276 [#########-] 85.78% \n\n\nspotPython tuning: -0.39223057644110276 [#########-] 87.92% \n\n\nspotPython tuning: -0.39223057644110276 [#########-] 90.05% \n\n\nspotPython tuning: -0.39223057644110276 [#########-] 92.10% \n\n\nspotPython tuning: -0.39223057644110276 [#########-] 94.59% \n\n\nspotPython tuning: -0.39223057644110276 [##########] 97.03% \n\n\nspotPython tuning: -0.39223057644110276 [##########] 99.66% \n\n\nspotPython tuning: -0.39223057644110276 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x17c4ce950&gt;"
  },
  {
    "objectID": "18_spot_hpt_sklearn_multiclass_classification_svc.html#sec-tensorboard-18",
    "href": "18_spot_hpt_sklearn_multiclass_classification_svc.html#sec-tensorboard-18",
    "title": "17  HPT: sklearn SVC VBDP Data",
    "section": "17.9 Step 9: Tensorboard",
    "text": "17.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard as described in Section 14.9, see also the description in the documentation: Tensorboard."
  },
  {
    "objectID": "18_spot_hpt_sklearn_multiclass_classification_svc.html#sec-results-tuning-18",
    "href": "18_spot_hpt_sklearn_multiclass_classification_svc.html#sec-results-tuning-18",
    "title": "17  HPT: sklearn SVC VBDP Data",
    "section": "17.10 Step 10: Results",
    "text": "17.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The following code generates the progress plot from ?fig-progress.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nPrint the results\n\n\nprint(gen_design_table(fun_control=fun_control,\n    spot=spot_tuner))\n\n| name        | type   | default   |   lower |   upper |              tuned | transform   |   importance | stars   |\n|-------------|--------|-----------|---------|---------|--------------------|-------------|--------------|---------|\n| C           | float  | 1.0       |     0.1 |    10.0 |  6.177805785268444 | None        |        23.15 | *       |\n| kernel      | factor | rbf       |     0.0 |     0.0 |                0.0 | None        |         0.00 |         |\n| degree      | int    | 3         |     3.0 |     3.0 |                3.0 | None        |         0.00 |         |\n| gamma       | factor | scale     |     0.0 |     1.0 |                1.0 | None        |       100.00 | ***     |\n| coef0       | float  | 0.0       |     0.0 |     0.0 |                0.0 | None        |         0.00 |         |\n| shrinking   | factor | 0         |     0.0 |     1.0 |                0.0 | None        |         0.85 | .       |\n| probability | factor | 0         |     1.0 |     1.0 |                1.0 | None        |         0.00 |         |\n| tol         | float  | 0.001     |  0.0001 |    0.01 |             0.0001 | None        |         0.00 |         |\n| cache_size  | float  | 200.0     |   100.0 |   400.0 | 110.11589070084857 | None        |         0.32 | .       |\n| break_ties  | factor | 0         |     0.0 |     1.0 |                0.0 | None        |         1.50 | *       |\n\n\n\n17.10.1 Show variable importance\n\nspot_tuner.plot_importance(threshold=0.025, filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n\n17.10.2 Get Default Hyperparameters\n\nfrom spotPython.hyperparameters.values import get_default_values, transform_hyper_parameter_values\nvalues_default = get_default_values(fun_control)\nvalues_default = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values_default)\nvalues_default\n\n{'C': 1.0,\n 'kernel': 'rbf',\n 'degree': 3,\n 'gamma': 'scale',\n 'coef0': 0.0,\n 'shrinking': 0,\n 'probability': 0,\n 'tol': 0.001,\n 'cache_size': 200.0,\n 'break_ties': 0}\n\n\n\nfrom sklearn.pipeline import make_pipeline\nmodel_default = make_pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"](**values_default))\nmodel_default\n\nPipeline(steps=[('nonetype', None),\n                ('svc',\n                 SVC(break_ties=0, cache_size=200.0, probability=0,\n                     shrinking=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('nonetype', None),\n                ('svc',\n                 SVC(break_ties=0, cache_size=200.0, probability=0,\n                     shrinking=0))])NoneNoneSVCSVC(break_ties=0, cache_size=200.0, probability=0, shrinking=0)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nDefault value for “probability” is False, but we need it to be True for the metric “mapk_score”.\n\n\nvalues_default.update({\"probability\": 1})\n\n\n\n\n\n17.10.3 Get SPOT Results\n\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nprint(X)\n\n[[6.17780579e+00 0.00000000e+00 3.00000000e+00 1.00000000e+00\n  0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e-04\n  1.10115891e+02 0.00000000e+00]]\n\n\n\nfrom spotPython.hyperparameters.values import assign_values, return_conf_list_from_var_dict\nv_dict = assign_values(X, fun_control[\"var_name\"])\nreturn_conf_list_from_var_dict(var_dict=v_dict, fun_control=fun_control)\n\n[{'C': 6.177805785268444,\n  'kernel': 'rbf',\n  'degree': 3,\n  'gamma': 'auto',\n  'coef0': 0.0,\n  'shrinking': 0,\n  'probability': 1,\n  'tol': 0.0001,\n  'cache_size': 110.11589070084857,\n  'break_ties': 0}]\n\n\n\nfrom spotPython.hyperparameters.values import get_one_sklearn_model_from_X\nmodel_spot = get_one_sklearn_model_from_X(X, fun_control)\nmodel_spot\n\nSVC(C=6.177805785268444, break_ties=0, cache_size=110.11589070084857,\n    gamma='auto', probability=1, shrinking=0, tol=0.0001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(C=6.177805785268444, break_ties=0, cache_size=110.11589070084857,\n    gamma='auto', probability=1, shrinking=0, tol=0.0001)\n\n\n\n\n17.10.4 Evaluate SPOT Results\n\nFetch the data.\n\n\nfrom spotPython.utils.convert import get_Xy_from_df\nX_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\nX_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\nX_test.shape, y_test.shape\n\n((177, 64), (177,))\n\n\n\nFit the model with the tuned hyperparameters. This gives one result:\n\n\nmodel_spot.fit(X_train, y_train)\ny_pred = model_spot.predict_proba(X_test)\nres = mapk_score(y_true=y_test, y_pred=y_pred, k=3)\nres\n\n0.3672316384180791\n\n\n\ndef repeated_eval(n, model):\n    res_values = []\n    for i in range(n):\n        model.fit(X_train, y_train)\n        y_pred = model.predict_proba(X_test)\n        res = mapk_score(y_true=y_test, y_pred=y_pred, k=3)\n        res_values.append(res)\n    mean_res = np.mean(res_values)\n    print(f\"mean_res: {mean_res}\")\n    std_res = np.std(res_values)\n    print(f\"std_res: {std_res}\")\n    min_res = np.min(res_values)\n    print(f\"min_res: {min_res}\")\n    max_res = np.max(res_values)\n    print(f\"max_res: {max_res}\")\n    median_res = np.median(res_values)\n    print(f\"median_res: {median_res}\")\n    return mean_res, std_res, min_res, max_res, median_res\n\n\n\n17.10.5 Handling Non-deterministic Results\n\nBecause the model is non-determinstic, we perform \\(n=30\\) runs and calculate the mean and standard deviation of the performance metric.\n\n\n_ = repeated_eval(30, model_spot)\n\nmean_res: 0.3655681104833647\nstd_res: 0.004447816858487075\nmin_res: 0.35781544256120523\nmax_res: 0.3775894538606403\nmedian_res: 0.365819209039548\n\n\n\n\n17.10.6 Evalution of the Default Hyperparameters\n\nmodel_default[\"svc\"].probability = True\nmodel_default.fit(X_train, y_train)[\"svc\"]\n\nSVC(break_ties=0, cache_size=200.0, probability=True, shrinking=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(break_ties=0, cache_size=200.0, probability=True, shrinking=0)\n\n\n\nOne evaluation of the default hyperparameters is performed on the hold-out test set.\n\n\ny_pred = model_default.predict_proba(X_test)\nmapk_score(y_true=y_test, y_pred=y_pred, k=3)\n\n0.3907721280602636\n\n\nSince one single evaluation is not meaningful, we perform, similar to the evaluation of the SPOT results, \\(n=30\\) runs of the default setting and and calculate the mean and standard deviation of the performance metric.\n\n_ = repeated_eval(30, model_default)\n\nmean_res: 0.38468298807281853\nstd_res: 0.0053528034467578926\nmin_res: 0.3757062146892655\nmax_res: 0.39736346516007537\nmedian_res: 0.38465160075329563\n\n\n\n\n17.10.7 Plot: Compare Predictions\n\nfrom spotPython.plot.validation import plot_confusion_matrix\nplot_confusion_matrix(model_default, fun_control, title = \"Default\")\n\n\n\n\n\nplot_confusion_matrix(model_spot, fun_control, title=\"SPOT\")\n\n\n\n\n\nmin(spot_tuner.y), max(spot_tuner.y)\n\n(-0.39223057644110276, -0.3132832080200501)\n\n\n\n\n17.10.8 Cross-validated Evaluations\n\nfrom spotPython.sklearn.traintest import evaluate_cv\nfun_control.update({\n     \"eval\": \"train_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.3481132075471698, None)\n\n\n\nfun_control.update({\n     \"eval\": \"test_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.3595860566448802, None)\n\n\n\nThis is the evaluation that will be used in the comparison:\n\n\nfun_control.update({\n     \"eval\": \"data_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.3669014084507042, None)\n\n\n\n\n17.10.9 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nC:  23.149856362237255\ngamma:  100.0\nshrinking:  0.8468232243334053\ncache_size:  0.3174417572455601\nbreak_ties:  1.4962135469016804\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17.10.10 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n                                                \n\n\n\n\n17.10.11 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
  },
  {
    "objectID": "19_spot_hpt_sklearn_multiclass_classification_knn.html#sec-setup-19",
    "href": "19_spot_hpt_sklearn_multiclass_classification_knn.html#sec-setup-19",
    "title": "18  HPT: sklearn KNN Classifier VBDP Data",
    "section": "18.1 Step 1: Setup",
    "text": "18.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time and the initial design size.\n\nMAX_TIME = 1\nINIT_SIZE = 5\nORIGINAL = False\n\n\nimport os\nimport copy\nimport socket\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '19-knn-sklearn' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\n\n19-knn-sklearn_maans03_1min_5init_2023-06-28_04-19-19\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "19_spot_hpt_sklearn_multiclass_classification_knn.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "href": "19_spot_hpt_sklearn_multiclass_classification_knn.html#step-2-initialization-of-the-empty-fun_control-dictionary",
    "title": "18  HPT: sklearn KNN Classifier VBDP Data",
    "section": "18.2 Step 2: Initialization of the Empty fun_control Dictionary",
    "text": "18.2 Step 2: Initialization of the Empty fun_control Dictionary\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"classification\",\n    tensorboard_path=\"runs/16_spot_hpt_sklearn_classification\")\n\n\n18.2.1 Load Data: Classification VBDP\n\nimport pandas as pd\nif ORIGINAL == True:\n    train_df = pd.read_csv('./data/VBDP/trainn.csv')\n    test_df = pd.read_csv('./data/VBDP/testt.csv')\nelse:\n    train_df = pd.read_csv('./data/VBDP/train.csv')\n    # remove the id column\n    train_df = train_df.drop(columns=['id'])\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\nn_samples = train_df.shape[0]\nn_features = train_df.shape[1] - 1\ntarget_column = \"prognosis\"\n# Encoder our prognosis labels as integers for easier decoding later\nenc = OrdinalEncoder()\ntrain_df[target_column] = enc.fit_transform(train_df[[target_column]])\ntrain_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nprint(train_df.shape)\ntrain_df.head()\n\n(707, 65)\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\n...\nx56\nx57\nx58\nx59\nx60\nx61\nx62\nx63\nx64\nprognosis\n\n\n\n\n0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n7.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n3.0\n\n\n3\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n6.0\n\n\n\n\n5 rows × 65 columns\n\n\n\nThe full data set train_df 64 features. The target column is labeled as prognosis.\n\n\n18.2.2 Holdout Train and Test Data\nWe split out a hold-out test set (25% of the data) so we can calculate an example MAP@K\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n                                                    random_state=42,\n                                                    test_size=0.25,\n                                                    stratify=train_df[target_column])\ntrain = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\ntest = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\ntrain.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\ntest.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nprint(train.shape)\nprint(test.shape)\ntrain.head()\n\n(530, 65)\n(177, 65)\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\n...\nx56\nx57\nx58\nx59\nx60\nx61\nx62\nx63\nx64\nprognosis\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n\n\n1\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4.0\n\n\n2\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n\n\n3\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n\n\n\n\n5 rows × 65 columns\n\n\n\n\n# add the dataset to the fun_control\nfun_control.update({\"data\": train_df, # full dataset,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": target_column})"
  },
  {
    "objectID": "19_spot_hpt_sklearn_multiclass_classification_knn.html#sec-specification-of-preprocessing-model-19",
    "href": "19_spot_hpt_sklearn_multiclass_classification_knn.html#sec-specification-of-preprocessing-model-19",
    "title": "18  HPT: sklearn KNN Classifier VBDP Data",
    "section": "18.3 Step 4: Specification of the Preprocessing Model",
    "text": "18.3 Step 4: Specification of the Preprocessing Model\nData preprocesssing can be very simple, e.g., you can ignore it. Then you would choose the prep_model “None”:\n\nprep_model = None\nfun_control.update({\"prep_model\": prep_model})\n\nA default approach for numerical data is the StandardScaler (mean 0, variance 1). This can be selected as follows:\n\n# prep_model = StandardScaler()\n# fun_control.update({\"prep_model\": prep_model})\n\nEven more complicated pre-processing steps are possible, e.g., the follwing pipeline:\n\n# categorical_columns = []\n# one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n# prep_model = ColumnTransformer(\n#         transformers=[\n#             (\"categorical\", one_hot_encoder, categorical_columns),\n#         ],\n#         remainder=StandardScaler(),\n#     )"
  },
  {
    "objectID": "19_spot_hpt_sklearn_multiclass_classification_knn.html#step-5-select-model-algorithm-and-core_model_hyper_dict",
    "href": "19_spot_hpt_sklearn_multiclass_classification_knn.html#step-5-select-model-algorithm-and-core_model_hyper_dict",
    "title": "18  HPT: sklearn KNN Classifier VBDP Data",
    "section": "18.4 Step 5: Select Model (algorithm) and core_model_hyper_dict",
    "text": "18.4 Step 5: Select Model (algorithm) and core_model_hyper_dict\nThe selection of the algorithm (ML model) that should be tuned is done by specifying the its name from the sklearn implementation. For example, the SVC support vector machine classifier is selected as follows:\nfun_control = add_core_model_to_fun_control(SVC, fun_control, SklearnHyperDict)\nOther core_models are, e.g.,:\n\nRidgeCV\nGradientBoostingRegressor\nElasticNet\nRandomForestClassifier\nLogisticRegression\nKNeighborsClassifier\nRandomForestClassifier\nGradientBoostingClassifier\nHistGradientBoostingClassifier\n\nWe will use the RandomForestClassifier classifier in this example.\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.linear_model import ElasticNet\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfrom spotPython.data.sklearn_hyper_dict import SklearnHyperDict\nfrom spotPython.fun.hypersklearn import HyperSklearn\n\n\n# core_model  = RidgeCV\n# core_model = GradientBoostingRegressor\n# core_model = ElasticNet\n# core_model = RandomForestClassifier\ncore_model = KNeighborsClassifier\n# core_model = LogisticRegression\n# core_model = KNeighborsClassifier\n# core_model = GradientBoostingClassifier\n# core_model = HistGradientBoostingClassifier\nfun_control = add_core_model_to_fun_control(core_model=core_model,\n                              fun_control=fun_control,\n                              hyper_dict=SklearnHyperDict,\n                              filename=None)\n\nNow fun_control has the information from the JSON file. The available hyperparameters are:\n\nprint(*fun_control[\"core_model_hyper_dict\"].keys(), sep=\"\\n\")\n\nn_neighbors\nweights\nalgorithm\nleaf_size\np"
  },
  {
    "objectID": "19_spot_hpt_sklearn_multiclass_classification_knn.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "href": "19_spot_hpt_sklearn_multiclass_classification_knn.html#step-6-modify-hyper_dict-hyperparameters-for-the-selected-algorithm-aka-core_model",
    "title": "18  HPT: sklearn KNN Classifier VBDP Data",
    "section": "18.5 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "18.5 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\n\n18.5.1 Modify hyperparameter of type numeric and integer (boolean)\nNumeric and boolean values can be modified using the modify_hyper_parameter_bounds method. For example, to change the tol hyperparameter of the SVC model to the interval [1e-3, 1e-2], the following code can be used:\nfun_control = modify_hyper_parameter_bounds(fun_control, \"tol\", bounds=[1e-3, 1e-2])\n\n# from spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n# fun_control = modify_hyper_parameter_bounds(fun_control, \"probability\", bounds=[1, 1])\n\n\n\n18.5.2 Modify hyperparameter of type factor\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in Section 14.6.\nFactors can be modified with the modify_hyper_parameter_levels function. For example, to exclude the sigmoid kernel from the tuning, the kernel hyperparameter of the SVC model can be modified as follows:\nfun_control = modify_hyper_parameter_levels(fun_control, \"kernel\", [\"linear\", \"rbf\"])\nThe new setting can be controlled via:\nfun_control[\"core_model_hyper_dict\"][\"kernel\"]\n\n# from spotPython.hyperparameters.values import modify_hyper_parameter_levels\n# fun_control = modify_hyper_parameter_levels(fun_control, \"kernel\", [\"rbf\"])\n\n\n\n18.5.3 Optimizers\nOptimizers are described in Section 14.6.1.\n\n\n18.5.4 Selection of the Objective: Metric and Loss Functions\n\nMachine learning models are optimized with respect to a metric, for example, the accuracy function.\nDeep learning, e.g., neural networks are optimized with respect to a loss function, for example, the cross_entropy function and evaluated with respect to a metric, for example, the accuracy function."
  },
  {
    "objectID": "19_spot_hpt_sklearn_multiclass_classification_knn.html#step-7-selection-of-the-objective-loss-function",
    "href": "19_spot_hpt_sklearn_multiclass_classification_knn.html#step-7-selection-of-the-objective-loss-function",
    "title": "18  HPT: sklearn KNN Classifier VBDP Data",
    "section": "18.6 Step 7: Selection of the Objective (Loss) Function",
    "text": "18.6 Step 7: Selection of the Objective (Loss) Function\nThe loss function, that is usually used in deep learning for optimizing the weights of the net, is stored in the fun_control dictionary as \"loss_function\".\n\n18.6.1 Metric Function\nThere are two different types of metrics in spotPython:\n\n\"metric_river\" is used for the river based evaluation via eval_oml_iter_progressive.\n\"metric_sklearn\" is used for the sklearn based evaluation.\n\nWe will consider multi-class classification metrics, e.g., mapk_score and top_k_accuracy_score.\n\n\n\n\n\n\nPredict Probabilities\n\n\n\nIn this multi-class classification example the machine learning algorithm should return the probabilities of the specific classes (\"predict_proba\") instead of the predicted values.\n\n\nWe set \"predict_proba\" to True in the fun_control dictionary.\n\n18.6.1.1 The MAPK Metric\nTo select the MAPK metric, the following two entries can be added to the fun_control dictionary:\n\"metric_sklearn\": mapk_score\"\n\"metric_params\": {\"k\": 3}.\n\n\n18.6.1.2 Other Metrics\nAlternatively, other metrics for multi-class classification can be used, e.g.,: * top_k_accuracy_score or * roc_auc_score\nThe metric roc_auc_score requires the parameter \"multi_class\", e.g.,\n\"multi_class\": \"ovr\".\nThis is set in the fun_control dictionary.\n\n\n\n\n\n\nWeights\n\n\n\nspotPython performs a minimization, therefore, metrics that should be maximized have to be multiplied by -1. This is done by setting \"weights\" to -1.\n\n\n\nThe complete setup for the metric in our example is:\n\n\nfrom spotPython.utils.metrics import mapk_score\nfun_control.update({\n               \"weights\": -1,\n               \"metric_sklearn\": mapk_score,\n               \"predict_proba\": True,\n               \"metric_params\": {\"k\": 3},\n               })\n\n\n\n\n18.6.2 Evaluation on Hold-out Data\n\nThe default method for computing the performance is \"eval_holdout\".\nAlternatively, cross-validation can be used for every machine learning model.\nSpecifically for RandomForests, the OOB-score can be used.\n\n\nfun_control.update({\n    \"eval\": \"train_hold_out\",\n})\n\n\n18.6.2.1 Cross Validation\nInstead of using the OOB-score, the classical cross validation can be used. The number of folds is set by the key \"k_folds\". For example, to use 5-fold cross validation, the key \"k_folds\" is set to 5. Uncomment the following line to use cross validation:\n\n# fun_control.update({\n#      \"eval\": \"train_cv\",\n#      \"k_folds\": 10,\n# })"
  },
  {
    "objectID": "19_spot_hpt_sklearn_multiclass_classification_knn.html#step-8-calling-the-spot-function",
    "href": "19_spot_hpt_sklearn_multiclass_classification_knn.html#step-8-calling-the-spot-function",
    "title": "18  HPT: sklearn KNN Classifier VBDP Data",
    "section": "18.7 Step 8: Calling the SPOT Function",
    "text": "18.7 Step 8: Calling the SPOT Function\n\n18.7.1 Preparing the SPOT Call\n\nGet types and variable names as well as lower and upper bounds for the hyperparameters.\n\n\n# extract the variable types, names, and bounds\nfrom spotPython.hyperparameters.values import (get_bound_values,\n    get_var_name,\n    get_var_type,)\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name        | type   | default   |   lower |   upper | transform             |\n|-------------|--------|-----------|---------|---------|-----------------------|\n| n_neighbors | int    | 2         |       1 |       7 | transform_power_2_int |\n| weights     | factor | uniform   |       0 |       1 | None                  |\n| algorithm   | factor | auto      |       0 |       3 | None                  |\n| leaf_size   | int    | 5         |       2 |       7 | transform_power_2_int |\n| p           | int    | 2         |       1 |       2 | None                  |\n\n\n\n\n18.7.2 The Objective Function\nThe objective function is selected next. It implements an interface from sklearn’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hypersklearn import HyperSklearn\nfun = HyperSklearn().fun_sklearn\n\n\n\n18.7.3 Run the Spot Optimizer\n\nRun SPOT for approx. x mins (max_time).\nNote: the run takes longer, because the evaluation time of initial design (here: initi_size, 20 points) is not considered.\n\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nhyper_dict=SklearnHyperDict().load()\nX_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\nX_start\n\narray([[2, 0, 0, 5, 2]])\n\n\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom math import inf\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\nspotPython tuning: -0.3107769423558897 [----------] 0.72% \n\n\nspotPython tuning: -0.3107769423558897 [----------] 1.50% \n\n\nspotPython tuning: -0.3107769423558897 [----------] 2.28% \n\n\nspotPython tuning: -0.3107769423558897 [----------] 3.01% \n\n\nspotPython tuning: -0.3107769423558897 [----------] 3.77% \n\n\nspotPython tuning: -0.3107769423558897 [----------] 4.61% \n\n\nspotPython tuning: -0.3107769423558897 [#---------] 5.81% \n\n\nspotPython tuning: -0.3107769423558897 [#---------] 6.77% \n\n\nspotPython tuning: -0.3107769423558897 [#---------] 7.70% \n\n\nspotPython tuning: -0.3107769423558897 [#---------] 8.57% \n\n\nspotPython tuning: -0.3107769423558897 [#---------] 9.52% \n\n\nspotPython tuning: -0.3107769423558897 [#---------] 10.84% \n\n\nspotPython tuning: -0.3107769423558897 [#---------] 12.20% \n\n\nspotPython tuning: -0.3107769423558897 [#---------] 13.66% \n\n\nspotPython tuning: -0.3107769423558897 [##--------] 15.14% \n\n\nspotPython tuning: -0.3107769423558897 [##--------] 16.68% \n\n\nspotPython tuning: -0.3107769423558897 [##--------] 18.63% \n\n\nspotPython tuning: -0.3107769423558897 [##--------] 20.06% \n\n\nspotPython tuning: -0.3107769423558897 [##--------] 21.73% \n\n\nspotPython tuning: -0.3107769423558897 [##--------] 23.17% \n\n\nspotPython tuning: -0.3107769423558897 [##--------] 24.52% \n\n\nspotPython tuning: -0.3107769423558897 [###-------] 25.73% \n\n\nspotPython tuning: -0.3107769423558897 [###-------] 27.24% \n\n\nspotPython tuning: -0.3107769423558897 [###-------] 29.01% \n\n\nspotPython tuning: -0.3107769423558897 [###-------] 30.44% \n\n\nspotPython tuning: -0.3107769423558897 [###-------] 32.30% \n\n\nspotPython tuning: -0.3107769423558897 [###-------] 33.71% \n\n\nspotPython tuning: -0.3107769423558897 [####------] 35.30% \n\n\nspotPython tuning: -0.3107769423558897 [####------] 37.07% \n\n\nspotPython tuning: -0.3107769423558897 [####------] 38.75% \n\n\nspotPython tuning: -0.3107769423558897 [####------] 40.34% \n\n\nspotPython tuning: -0.3107769423558897 [####------] 42.17% \n\n\nspotPython tuning: -0.3107769423558897 [####------] 43.69% \n\n\nspotPython tuning: -0.3107769423558897 [#####-----] 45.57% \n\n\nspotPython tuning: -0.3107769423558897 [#####-----] 47.94% \n\n\nspotPython tuning: -0.3107769423558897 [#####-----] 50.22% \n\n\nspotPython tuning: -0.3107769423558897 [#####-----] 52.31% \n\n\nspotPython tuning: -0.3107769423558897 [#####-----] 54.83% \n\n\nspotPython tuning: -0.3107769423558897 [######----] 57.02% \n\n\nspotPython tuning: -0.3107769423558897 [######----] 59.25% \n\n\nspotPython tuning: -0.3107769423558897 [######----] 61.96% \n\n\nspotPython tuning: -0.3107769423558897 [######----] 64.64% \n\n\nspotPython tuning: -0.3107769423558897 [#######---] 66.86% \n\n\nspotPython tuning: -0.3107769423558897 [#######---] 69.42% \n\n\nspotPython tuning: -0.3107769423558897 [#######---] 71.86% \n\n\nspotPython tuning: -0.3107769423558897 [#######---] 74.46% \n\n\nspotPython tuning: -0.3107769423558897 [########--] 77.65% \n\n\nspotPython tuning: -0.3107769423558897 [########--] 80.48% \n\n\nspotPython tuning: -0.3107769423558897 [########--] 83.48% \n\n\nspotPython tuning: -0.3107769423558897 [#########-] 86.88% \n\n\nspotPython tuning: -0.3107769423558897 [#########-] 90.10% \n\n\nspotPython tuning: -0.3107769423558897 [#########-] 92.94% \n\n\nspotPython tuning: -0.3107769423558897 [##########] 95.24% \n\n\nspotPython tuning: -0.3107769423558897 [##########] 98.19% \n\n\nspotPython tuning: -0.3107769423558897 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x17f742590&gt;"
  },
  {
    "objectID": "19_spot_hpt_sklearn_multiclass_classification_knn.html#sec-tensorboard-19",
    "href": "19_spot_hpt_sklearn_multiclass_classification_knn.html#sec-tensorboard-19",
    "title": "18  HPT: sklearn KNN Classifier VBDP Data",
    "section": "18.8 Step 9: Tensorboard",
    "text": "18.8 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard as described in Section 14.9, see also the description in the documentation: Tensorboard."
  },
  {
    "objectID": "19_spot_hpt_sklearn_multiclass_classification_knn.html#sec-results-tuning-19",
    "href": "19_spot_hpt_sklearn_multiclass_classification_knn.html#sec-results-tuning-19",
    "title": "18  HPT: sklearn KNN Classifier VBDP Data",
    "section": "18.9 Step 10: Results",
    "text": "18.9 Step 10: Results\nAfter the hyperparameter tuning run is finished, the progress of the hyperparameter tuning can be visualized. The following code generates the progress plot from ?fig-progress.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nPrint the results\n\n\nprint(gen_design_table(fun_control=fun_control,\n    spot=spot_tuner))\n\n| name        | type   | default   |   lower |   upper |   tuned | transform             |   importance | stars   |\n|-------------|--------|-----------|---------|---------|---------|-----------------------|--------------|---------|\n| n_neighbors | int    | 2         |       1 |       7 |     4.0 | transform_power_2_int |        14.06 | *       |\n| weights     | factor | uniform   |       0 |       1 |     1.0 | None                  |       100.00 | ***     |\n| algorithm   | factor | auto      |       0 |       3 |     2.0 | None                  |         0.00 |         |\n| leaf_size   | int    | 5         |       2 |       7 |     6.0 | transform_power_2_int |         0.03 |         |\n| p           | int    | 2         |       1 |       2 |     1.0 | None                  |         0.03 |         |\n\n\n\n18.9.1 Show variable importance\n\nspot_tuner.plot_importance(threshold=0.025, filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n\n18.9.2 Get Default Hyperparameters\n\nfrom spotPython.hyperparameters.values import get_default_values, transform_hyper_parameter_values\nvalues_default = get_default_values(fun_control)\nvalues_default = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values_default)\nvalues_default\n\n{'n_neighbors': 4,\n 'weights': 'uniform',\n 'algorithm': 'auto',\n 'leaf_size': 32,\n 'p': 2}\n\n\n\nfrom sklearn.pipeline import make_pipeline\nmodel_default = make_pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"](**values_default))\nmodel_default\n\nPipeline(steps=[('nonetype', None),\n                ('kneighborsclassifier',\n                 KNeighborsClassifier(leaf_size=32, n_neighbors=4))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('nonetype', None),\n                ('kneighborsclassifier',\n                 KNeighborsClassifier(leaf_size=32, n_neighbors=4))])NoneNoneKNeighborsClassifierKNeighborsClassifier(leaf_size=32, n_neighbors=4)\n\n\n\n\n18.9.3 Get SPOT Results\n\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nprint(X)\n\n[[4. 1. 2. 6. 1.]]\n\n\n\nfrom spotPython.hyperparameters.values import assign_values, return_conf_list_from_var_dict\nv_dict = assign_values(X, fun_control[\"var_name\"])\nreturn_conf_list_from_var_dict(var_dict=v_dict, fun_control=fun_control)\n\n[{'n_neighbors': 16,\n  'weights': 'distance',\n  'algorithm': 'kd_tree',\n  'leaf_size': 64,\n  'p': 1}]\n\n\n\nfrom spotPython.hyperparameters.values import get_one_sklearn_model_from_X\nmodel_spot = get_one_sklearn_model_from_X(X, fun_control)\nmodel_spot\n\nKNeighborsClassifier(algorithm='kd_tree', leaf_size=64, n_neighbors=16, p=1,\n                     weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(algorithm='kd_tree', leaf_size=64, n_neighbors=16, p=1,\n                     weights='distance')\n\n\n\n\n18.9.4 Evaluate SPOT Results\n\nFetch the data.\n\n\nfrom spotPython.utils.convert import get_Xy_from_df\nX_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\nX_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\nX_test.shape, y_test.shape\n\n((177, 64), (177,))\n\n\n\nFit the model with the tuned hyperparameters. This gives one result:\n\n\nmodel_spot.fit(X_train, y_train)\ny_pred = model_spot.predict_proba(X_test)\nres = mapk_score(y_true=y_test, y_pred=y_pred, k=3)\nres\n\n0.3267419962335216\n\n\n\ndef repeated_eval(n, model):\n    res_values = []\n    for i in range(n):\n        model.fit(X_train, y_train)\n        y_pred = model.predict_proba(X_test)\n        res = mapk_score(y_true=y_test, y_pred=y_pred, k=3)\n        res_values.append(res)\n    mean_res = np.mean(res_values)\n    print(f\"mean_res: {mean_res}\")\n    std_res = np.std(res_values)\n    print(f\"std_res: {std_res}\")\n    min_res = np.min(res_values)\n    print(f\"min_res: {min_res}\")\n    max_res = np.max(res_values)\n    print(f\"max_res: {max_res}\")\n    median_res = np.median(res_values)\n    print(f\"median_res: {median_res}\")\n    return mean_res, std_res, min_res, max_res, median_res\n\n\n\n18.9.5 Handling Non-deterministic Results\n\nBecause the model is non-determinstic, we perform \\(n=30\\) runs and calculate the mean and standard deviation of the performance metric.\n\n\n_ = repeated_eval(30, model_spot)\n\nmean_res: 0.3267419962335218\nstd_res: 1.6653345369377348e-16\nmin_res: 0.3267419962335216\nmax_res: 0.3267419962335216\nmedian_res: 0.3267419962335216\n\n\n\n\n18.9.6 Evalution of the Default Hyperparameters\n\nmodel_default.fit(X_train, y_train)[\"kneighborsclassifier\"]\n\nKNeighborsClassifier(leaf_size=32, n_neighbors=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(leaf_size=32, n_neighbors=4)\n\n\n\nOne evaluation of the default hyperparameters is performed on the hold-out test set.\n\n\ny_pred = model_default.predict_proba(X_test)\nmapk_score(y_true=y_test, y_pred=y_pred, k=3)\n\n0.2768361581920904\n\n\nSince one single evaluation is not meaningful, we perform, similar to the evaluation of the SPOT results, \\(n=30\\) runs of the default setting and and calculate the mean and standard deviation of the performance metric.\n\n_ = repeated_eval(30, model_default)\n\nmean_res: 0.2768361581920903\nstd_res: 1.1102230246251565e-16\nmin_res: 0.2768361581920904\nmax_res: 0.2768361581920904\nmedian_res: 0.2768361581920904\n\n\n\n\n18.9.7 Plot: Compare Predictions\n\nfrom spotPython.plot.validation import plot_confusion_matrix\nplot_confusion_matrix(model_default, fun_control, title = \"Default\")\n\n\n\n\n\nplot_confusion_matrix(model_spot, fun_control, title=\"SPOT\")\n\n\n\n\n\nmin(spot_tuner.y), max(spot_tuner.y)\n\n(-0.3107769423558897, -0.23558897243107768)\n\n\n\n\n18.9.8 Cross-validated Evaluations\n\nfrom spotPython.sklearn.traintest import evaluate_cv\nfun_control.update({\n     \"eval\": \"train_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.3157232704402516, None)\n\n\n\nfun_control.update({\n     \"eval\": \"test_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.2832788671023965, None)\n\n\n\nThis is the evaluation that will be used in the comparison:\n\n\nfun_control.update({\n     \"eval\": \"data_cv\",\n     \"k_folds\": 10,\n})\nevaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)\n\n(0.3061904761904762, None)\n\n\n\n\n18.9.9 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nn_neighbors:  14.06205916493912\nweights:  100.0\nleaf_size:  0.03194233933978853\np:  0.026575994881503626\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18.9.10 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n                                                \n\n\n\n\n18.9.11 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
  },
  {
    "objectID": "24_spot_torch_regression.html#sec-setup-24",
    "href": "24_spot_torch_regression.html#sec-setup-24",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.1 Step 1: Setup",
    "text": "19.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size and the device that is used.\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nThe device can be selected by setting the variable DEVICE.\nSince we are using a simple neural net, the setting \"cpu\" is preferred (on Mac).\nIf you have a GPU, you can use \"cuda:0\" instead.\nIf DEVICE is set to None, spotPython will automatically select the device.\n\nThis might result in \"mps\" on Macs, which is not the best choice for simple neural nets.\n\n\n\n\n\nMAX_TIME = 1\nINIT_SIZE = 5\nDEVICE = \"cpu\" # \"cuda:0\"\n\n\nfrom spotPython.utils.device import getDevice\nDEVICE = getDevice(DEVICE)\nprint(DEVICE)\n\ncpu\n\n\n\nimport os\nimport copy\nimport socket\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '24-torch' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\n\n24-torch_maans03_1min_5init_2023-06-28_04-22-10"
  },
  {
    "objectID": "24_spot_torch_regression.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "24_spot_torch_regression.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "19.2 Step 2: Initialization of the fun_control Dictionary\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process, which was described in Section 14.2.\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"regression\",\n tensorboard_path=\"runs/24_spot_torch_regression\",\n device=DEVICE)"
  },
  {
    "objectID": "24_spot_torch_regression.html#sec-data-loading-24",
    "href": "24_spot_torch_regression.html#sec-data-loading-24",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.3 Step 3: PyTorch Data Loading",
    "text": "19.3 Step 3: PyTorch Data Loading\n\n# Create dataset\nimport pandas as pd\nimport numpy as np\nfrom sklearn import datasets as sklearn_datasets\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nX, y = sklearn_datasets.make_regression(\n    n_samples=1000, n_features=10, noise=1, random_state=123)\ny = y.reshape(-1, 1)\n\n# Normalize the data\nX_scaler = MinMaxScaler()\nX_scaled = X_scaler.fit_transform(X)\ny_scaler = MinMaxScaler()\ny_scaled = y_scaler.fit_transform(y)\n\n# combine the features and target into a single dataframe named train_df\ntrain_df = pd.DataFrame(np.hstack((X_scaled, y_scaled)))\n\ntarget_column = \"y\"\nn_samples = train_df.shape[0]\nn_features = train_df.shape[1] - 1\ntrain_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nX_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column,\n    axis=1), \n    train_df[target_column],\n    random_state=42,\n    test_size=0.25)\ntrainset = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\ntestset = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\ntrainset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\ntestset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nprint(train_df.shape)\nprint(trainset.shape)\nprint(testset.shape)\n\n(1000, 11)\n(750, 11)\n(250, 11)\n\n\n\nimport torch\nfrom spotPython.torch.dataframedataset import DataFrameDataset\ndtype_x = torch.float32\ndtype_y = torch.float32\ntrain_df = DataFrameDataset(train_df, target_column=target_column,\n    dtype_x=dtype_x, dtype_y=dtype_y)\ntrain = DataFrameDataset(trainset, target_column=target_column,\n    dtype_x=dtype_x, dtype_y=dtype_y)\ntest = DataFrameDataset(testset, target_column=target_column,\n    dtype_x=dtype_x, dtype_y=dtype_y)\nn_samples = len(train)\n\n\nNow we can test the data loading:\n\n\nfrom spotPython.torch.traintest import create_train_val_data_loaders\ntrainloader, testloader = create_train_val_data_loaders(train, 2, True, 0)\nfor i, data in enumerate(trainloader, 0):\n    inputs, labels = data\n    print(inputs.shape)\n    print(labels.shape)\n    print(inputs)\n    print(labels)\n    break\n\ntorch.Size([2, 10])\ntorch.Size([2])\ntensor([[0.3155, 0.6065, 0.3707, 0.4169, 0.3334, 0.7090, 0.4231, 0.7365, 0.5904,\n         0.5225],\n        [0.4784, 0.3794, 0.7014, 0.5763, 0.3840, 0.6080, 0.4512, 0.5228, 0.8908,\n         0.5452]])\ntensor([0.5717, 0.5226])\n\n\n\nSince this works fine, we can add the data loading to the fun_control dictionary:\n\n\n# add the dataset to the fun_control\nfun_control.update({\"data\": train_df, # full dataset,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": target_column,})"
  },
  {
    "objectID": "24_spot_torch_regression.html#sec-specification-of-preprocessing-model-24",
    "href": "24_spot_torch_regression.html#sec-specification-of-preprocessing-model-24",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.4 Step 4: Specification of the Preprocessing Model",
    "text": "19.4 Step 4: Specification of the Preprocessing Model\nAfter the training and test data are specified and added to the fun_control dictionary, spotPython allows the specification of a data preprocessing pipeline, e.g., for the scaling of the data or for the one-hot encoding of categorical variables, see Section 14.4. This feature is not used here, so we do not change the default value (which is None)."
  },
  {
    "objectID": "24_spot_torch_regression.html#sec-selection-of-the-algorithm-24",
    "href": "24_spot_torch_regression.html#sec-selection-of-the-algorithm-24",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.5 Step 5: Select Model (algorithm) and core_model_hyper_dict",
    "text": "19.5 Step 5: Select Model (algorithm) and core_model_hyper_dict\n\n19.5.1 Implementing a Configurable Neural Network With spotPython\nspotPython includes the Net_lin_reg class which is implemented in the file netregression.py.\nThis class inherits from the class Net_Core which is implemented in the file netcore.py, see Section 14.5.1.\nfrom torch import nn\nimport spotPython.torch.netcore as netcore\n\n\nclass Net_lin_reg(netcore.Net_Core):\n    def __init__(\n        self, _L_in, _L_out, l1, dropout_prob, lr_mult,\n        batch_size, epochs, k_folds, patience, optimizer,\n        sgd_momentum\n    ):\n        super(Net_lin_reg, self).__init__(\n            lr_mult=lr_mult,\n            batch_size=batch_size,\n            epochs=epochs,\n            k_folds=k_folds,\n            patience=patience,\n            optimizer=optimizer,\n            sgd_momentum=sgd_momentum,\n        )\n        l2 = max(l1 // 2, 4)\n        self.fc1 = nn.Linear(_L_in, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, _L_out)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        self.dropout1 = nn.Dropout(p=dropout_prob)\n        self.dropout2 = nn.Dropout(p=dropout_prob / 2)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.dropout2(x)\n        x = self.fc3(x)\n        return x\n\n\n19.5.1.1 The Net_Core class\nNet_lin_reg inherits from the class Net_Core which is implemented in the file netcore.py. This class was described in Section 14.5.1.\n\nfrom spotPython.torch.netregression import Net_lin_reg\nfrom spotPython.data.torch_hyper_dict import TorchHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfun_control = add_core_model_to_fun_control(core_model=Net_lin_reg,\n                              fun_control=fun_control,\n                              hyper_dict=TorchHyperDict,\n                              filename=None)\n\n\n\n\n19.5.2 The Search Space\n\n\n19.5.3 Configuring the Search Space With spotPython\n\n19.5.3.1 The hyper_dict Hyperparameters for the Selected Algorithm\nspotPython uses JSON files for the specification of the hyperparameters, which were described in Section 14.5.5.\nThe corresponding entries for the core_model class are shown below.\n\nfun_control['core_model_hyper_dict']\n\n{'_L_in': {'type': 'int',\n  'default': 10,\n  'transform': 'None',\n  'lower': 10,\n  'upper': 10},\n '_L_out': {'type': 'int',\n  'default': 1,\n  'transform': 'None',\n  'lower': 1,\n  'upper': 1},\n 'l1': {'type': 'int',\n  'default': 3,\n  'transform': 'transform_power_2_int',\n  'lower': 3,\n  'upper': 8},\n 'dropout_prob': {'type': 'float',\n  'default': 0.01,\n  'transform': 'None',\n  'lower': 0.0,\n  'upper': 0.9},\n 'lr_mult': {'type': 'float',\n  'default': 1.0,\n  'transform': 'None',\n  'lower': 0.1,\n  'upper': 10.0},\n 'batch_size': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 1,\n  'upper': 4},\n 'epochs': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 4,\n  'upper': 9},\n 'k_folds': {'type': 'int',\n  'default': 1,\n  'transform': 'None',\n  'lower': 1,\n  'upper': 1},\n 'patience': {'type': 'int',\n  'default': 2,\n  'transform': 'transform_power_2_int',\n  'lower': 1,\n  'upper': 5},\n 'optimizer': {'levels': ['Adadelta',\n   'Adagrad',\n   'Adam',\n   'AdamW',\n   'SparseAdam',\n   'Adamax',\n   'ASGD',\n   'NAdam',\n   'RAdam',\n   'RMSprop',\n   'Rprop',\n   'SGD'],\n  'type': 'factor',\n  'default': 'SGD',\n  'transform': 'None',\n  'class_name': 'torch.optim',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 12},\n 'sgd_momentum': {'type': 'float',\n  'default': 0.0,\n  'transform': 'None',\n  'lower': 0.0,\n  'upper': 1.0}}"
  },
  {
    "objectID": "24_spot_torch_regression.html#sec-modification-of-hyperparameters-24",
    "href": "24_spot_torch_regression.html#sec-modification-of-hyperparameters-24",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "19.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in Section 14.6.\n\n19.6.1 Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\n\n19.6.1.1 Modify Hyperparameters of Type numeric and integer (boolean)\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n\nfun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[2, 16])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[3, 7])\n\n\n\n19.6.1.2 Modify Hyperparameter of Type factor\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_levels\nfun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\",\n    [\"Adadelta\", \"Adagrad\", \"Adam\", \"AdamW\", \"Adamax\", \"ASGD\", \"NAdam\"])\n\n\nfun_control.update({\n               \"_L_in\": n_features,\n               \"_L_out\": 1,})\n\n\n\n\n19.6.2 Optimizers\nOptimizers are described in Section 14.6.1."
  },
  {
    "objectID": "24_spot_torch_regression.html#step-7-selection-of-the-objective-loss-function",
    "href": "24_spot_torch_regression.html#step-7-selection-of-the-objective-loss-function",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.7 Step 7: Selection of the Objective (Loss) Function",
    "text": "19.7 Step 7: Selection of the Objective (Loss) Function\n\n19.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set (see Section 14.7.1)\nthe loss function (and a metric).\n\n\n\n19.7.2 Loss Functions and Metrics\nThe key \"loss_function\" specifies the loss function which is used during the optimization, see Section 14.7.5.\nWe will use MSE loss for the regression task.\n\nfrom torch.nn import MSELoss\nloss_torch = MSELoss()\nfun_control.update({\"loss_function\": loss_torch})\n\n\n\n19.7.3 Metric\n\nfrom torchmetrics import MeanAbsoluteError\nmetric_torch = MeanAbsoluteError().to(fun_control[\"device\"])\nfun_control.update({\"metric_torch\": metric_torch})"
  },
  {
    "objectID": "24_spot_torch_regression.html#step-8-calling-the-spot-function",
    "href": "24_spot_torch_regression.html#step-8-calling-the-spot-function",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.8 Step 8: Calling the SPOT Function",
    "text": "19.8 Step 8: Calling the SPOT Function\n\n19.8.1 Preparing the SPOT Call\nThe following code passes the information about the parameter ranges and bounds to spot.\n\n# extract the variable types, names, and bounds\nfrom spotPython.hyperparameters.values import (get_bound_values,\n    get_var_name,\n    get_var_type,)\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name         | type   | default   |   lower |   upper | transform             |\n|--------------|--------|-----------|---------|---------|-----------------------|\n| _L_in        | int    | 10        |    10   |    10   | None                  |\n| _L_out       | int    | 1         |     1   |     1   | None                  |\n| l1           | int    | 3         |     3   |     8   | transform_power_2_int |\n| dropout_prob | float  | 0.01      |     0   |     0.9 | None                  |\n| lr_mult      | float  | 1.0       |     0.1 |    10   | None                  |\n| batch_size   | int    | 4         |     1   |     4   | transform_power_2_int |\n| epochs       | int    | 4         |     2   |    16   | transform_power_2_int |\n| k_folds      | int    | 1         |     1   |     1   | None                  |\n| patience     | int    | 2         |     3   |     7   | transform_power_2_int |\n| optimizer    | factor | SGD       |     0   |     6   | None                  |\n| sgd_momentum | float  | 0.0       |     0   |     1   | None                  |\n\n\n\n\n19.8.2 The Objective Function fun_torch\nThe objective function fun_torch is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hypertorch import HyperTorch\nfun = HyperTorch().fun_torch\n\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nhyper_dict=TorchHyperDict().load()\nX_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\n\n\n\n19.8.3 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function as described in Section 14.8.4.\n\nfrom spotPython.spot import spot\nfrom math import inf\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\n\nconfig: {'_L_in': 10, '_L_out': 1, 'l1': 64, 'dropout_prob': 0.7103122166156, 'lr_mult': 3.62368745191023, 'batch_size': 8, 'epochs': 128, 'k_folds': 1, 'patience': 16, 'optimizer': 'Adam', 'sgd_momentum': 0.5446851204405787}\nEpoch: 1 | MeanAbsoluteError: 0.2039379924535751 | Loss: 0.0651115601588237 | Epoch: 2 | MeanAbsoluteError: 0.1767044663429260 | Loss: 0.0470499819340675 | Epoch: 3 | \n\n\nMeanAbsoluteError: 0.1620440036058426 | Loss: 0.0406880484844901 | Epoch: 4 | MeanAbsoluteError: 0.1465133577585220 | Loss: 0.0327078353086683 | Epoch: 5 | MeanAbsoluteError: 0.1473990678787231 | Loss: 0.0325913693461763 | Epoch: 6 | MeanAbsoluteError: 0.1477124840021133 | Loss: 0.0348307829913928 | Epoch: 7 | \n\n\nMeanAbsoluteError: 0.1388593465089798 | Loss: 0.0304127819591055 | Epoch: 8 | MeanAbsoluteError: 0.1380051076412201 | Loss: 0.0292197934008743 | Epoch: 9 | MeanAbsoluteError: 0.1329804211854935 | Loss: 0.0277498348763115 | Epoch: 10 | MeanAbsoluteError: 0.1365214139223099 | Loss: 0.0305306835002021 | Epoch: 11 | \n\n\nMeanAbsoluteError: 0.1215601116418839 | Loss: 0.0234468719480853 | Epoch: 12 | MeanAbsoluteError: 0.1284227222204208 | Loss: 0.0260808660379170 | Epoch: 13 | MeanAbsoluteError: 0.1351587027311325 | Loss: 0.0282939883733266 | Epoch: 14 | MeanAbsoluteError: 0.1291095167398453 | Loss: 0.0260214255019826 | Epoch: 15 | \n\n\nMeanAbsoluteError: 0.1134317144751549 | Loss: 0.0205320634352239 | Epoch: 16 | MeanAbsoluteError: 0.1155797541141510 | Loss: 0.0214307874833283 | Epoch: 17 | MeanAbsoluteError: 0.1163521558046341 | Loss: 0.0210927399549339 | Epoch: 18 | MeanAbsoluteError: 0.1179060488939285 | Loss: 0.0220890734344721 | Epoch: 19 | \n\n\nMeanAbsoluteError: 0.1200031787157059 | Loss: 0.0227700594788123 | Epoch: 20 | MeanAbsoluteError: 0.1202970743179321 | Loss: 0.0218182273307129 | Epoch: 21 | MeanAbsoluteError: 0.1175114065408707 | Loss: 0.0213017875855593 | Epoch: 22 | MeanAbsoluteError: 0.1137538775801659 | Loss: 0.0199538474461358 | Epoch: 23 | \n\n\nMeanAbsoluteError: 0.1056108549237251 | Loss: 0.0179119481717383 | Epoch: 24 | MeanAbsoluteError: 0.1154386773705482 | Loss: 0.0199114227373349 | Epoch: 25 | MeanAbsoluteError: 0.1026468575000763 | Loss: 0.0171343116947499 | Epoch: 26 | MeanAbsoluteError: 0.1279029846191406 | Loss: 0.0246763175819069 | Epoch: 27 | \n\n\nMeanAbsoluteError: 0.1016558706760406 | Loss: 0.0170920102146307 | Epoch: 28 | MeanAbsoluteError: 0.0984223857522011 | Loss: 0.0150153057061528 | Epoch: 29 | MeanAbsoluteError: 0.1023813262581825 | Loss: 0.0161468921425311 | Epoch: 30 | MeanAbsoluteError: 0.0907443910837173 | Loss: 0.0141862089819226 | Epoch: 31 | \n\n\nMeanAbsoluteError: 0.0957550108432770 | Loss: 0.0164538792236463 | Epoch: 32 | MeanAbsoluteError: 0.0939725339412689 | Loss: 0.0147865572209029 | Epoch: 33 | MeanAbsoluteError: 0.0940778106451035 | Loss: 0.0148354842891230 | Epoch: 34 | MeanAbsoluteError: 0.0889420285820961 | Loss: 0.0130552607588470 | Epoch: 35 | \n\n\nMeanAbsoluteError: 0.0900059342384338 | Loss: 0.0137241720589564 | Epoch: 36 | MeanAbsoluteError: 0.0897719487547874 | Loss: 0.0131742794765159 | Epoch: 37 | MeanAbsoluteError: 0.0882063359022141 | Loss: 0.0127719902390565 | Epoch: 38 | MeanAbsoluteError: 0.0860841348767281 | Loss: 0.0126436544569993 | Epoch: 39 | \n\n\nMeanAbsoluteError: 0.0922655314207077 | Loss: 0.0145925906437792 | Epoch: 40 | MeanAbsoluteError: 0.0949865207076073 | Loss: 0.0146264424179926 | Epoch: 41 | MeanAbsoluteError: 0.0838512480258942 | Loss: 0.0123768227609904 | Epoch: 42 | MeanAbsoluteError: 0.0826744362711906 | Loss: 0.0111953310153790 | Epoch: 43 | \n\n\nMeanAbsoluteError: 0.0818504244089127 | Loss: 0.0123963381751980 | Epoch: 44 | MeanAbsoluteError: 0.0961625277996063 | Loss: 0.0141663695195396 | Epoch: 45 | MeanAbsoluteError: 0.0908778756856918 | Loss: 0.0149765169358273 | Epoch: 46 | MeanAbsoluteError: 0.0847114771604538 | Loss: 0.0126292228784510 | Epoch: 47 | \n\n\nMeanAbsoluteError: 0.0860272347927094 | Loss: 0.0134428481963512 | Epoch: 48 | MeanAbsoluteError: 0.0732816532254219 | Loss: 0.0094434167289077 | Epoch: 49 | MeanAbsoluteError: 0.0842339470982552 | Loss: 0.0124017173595923 | Epoch: 50 | MeanAbsoluteError: 0.0814987272024155 | Loss: 0.0108845747982789 | Epoch: 51 | \n\n\nMeanAbsoluteError: 0.0938208252191544 | Loss: 0.0156861975678782 | Epoch: 52 | MeanAbsoluteError: 0.0880927965044975 | Loss: 0.0136299664154649 | Epoch: 53 | MeanAbsoluteError: 0.0891034230589867 | Loss: 0.0141755179747155 | Epoch: 54 | MeanAbsoluteError: 0.0860426053404808 | Loss: 0.0131161971091244 | Epoch: 55 | \n\n\nMeanAbsoluteError: 0.0816535577178001 | Loss: 0.0119751108975738 | Epoch: 56 | MeanAbsoluteError: 0.0874154344201088 | Loss: 0.0142062472510397 | Epoch: 57 | MeanAbsoluteError: 0.0944045856595039 | Loss: 0.0145814335954032 | Epoch: 58 | MeanAbsoluteError: 0.0845059603452682 | Loss: 0.0125228414912463 | Epoch: 59 | \n\n\nMeanAbsoluteError: 0.0823366641998291 | Loss: 0.0122528826009090 | Epoch: 60 | MeanAbsoluteError: 0.0890166684985161 | Loss: 0.0145609944470619 | Epoch: 61 | MeanAbsoluteError: 0.0796255469322205 | Loss: 0.0108279051246906 | Epoch: 62 | MeanAbsoluteError: 0.0810501426458359 | Loss: 0.0122793627608764 | Epoch: 63 | \n\n\nMeanAbsoluteError: 0.0848743170499802 | Loss: 0.0125216238605055 | Epoch: 64 | MeanAbsoluteError: 0.0870588719844818 | Loss: 0.0134136206715515 | Early stopping at epoch 63\nReturned to Spot: Validation loss: 0.013413620671551479\n\nconfig: {'_L_in': 10, '_L_out': 1, 'l1': 32, 'dropout_prob': 0.19981931523998656, 'lr_mult': 7.004318498645526, 'batch_size': 16, 'epochs': 2048, 'k_folds': 1, 'patience': 32, 'optimizer': 'Adadelta', 'sgd_momentum': 0.07401195908206384}\nEpoch: 1 | MeanAbsoluteError: 0.2279668152332306 | Loss: 0.0730608895813164 | Epoch: 2 | MeanAbsoluteError: 0.1943579614162445 | Loss: 0.0530797280371189 | Epoch: 3 | MeanAbsoluteError: 0.1783196032047272 | Loss: 0.0457427435015377 | Epoch: 4 | MeanAbsoluteError: 0.1455443054437637 | Loss: 0.0317246500206621 | Epoch: 5 | \n\n\nMeanAbsoluteError: 0.1782627105712891 | Loss: 0.0448358628506723 | Epoch: 6 | MeanAbsoluteError: 0.1870051324367523 | Loss: 0.0472179617928831 | Epoch: 7 | MeanAbsoluteError: 0.2745476663112640 | Loss: 0.0904999112612323 | Epoch: 8 | MeanAbsoluteError: 0.1959953904151917 | Loss: 0.0490562072709987 | Epoch: 9 | MeanAbsoluteError: 0.1801746636629105 | Loss: 0.0422604796721747 | Epoch: 10 | MeanAbsoluteError: 0.2088561207056046 | Loss: 0.0551059087248225 | Epoch: 11 | MeanAbsoluteError: 0.1442996859550476 | Loss: 0.0282283556206446 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.1498247087001801 | Loss: 0.0299407016780031 | Epoch: 13 | MeanAbsoluteError: 0.1607764214277267 | Loss: 0.0326588000906141 | Epoch: 14 | MeanAbsoluteError: 0.1875439584255219 | Loss: 0.0419628513290694 | Epoch: 15 | MeanAbsoluteError: 0.1274394094944000 | Loss: 0.0216788800531312 | Epoch: 16 | MeanAbsoluteError: 0.1229052320122719 | Loss: 0.0199363566444893 | Epoch: 17 | MeanAbsoluteError: 0.1134651973843575 | Loss: 0.0173077334306742 | Epoch: 18 | MeanAbsoluteError: 0.0810273140668869 | Loss: 0.0094888016434484 | Epoch: 19 | \n\n\nMeanAbsoluteError: 0.1389012336730957 | Loss: 0.0239202830156213 | Epoch: 20 | MeanAbsoluteError: 0.1607699543237686 | Loss: 0.0311916392100485 | Epoch: 21 | MeanAbsoluteError: 0.0588547848165035 | Loss: 0.0051995561164069 | Epoch: 22 | MeanAbsoluteError: 0.0997065752744675 | Loss: 0.0128893694399219 | Epoch: 23 | MeanAbsoluteError: 0.1351020783185959 | Loss: 0.0222132669173573 | Epoch: 24 | MeanAbsoluteError: 0.1663973629474640 | Loss: 0.0332004446536303 | Epoch: 25 | MeanAbsoluteError: 0.1216233298182487 | Loss: 0.0189505720903215 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.0909788161516190 | Loss: 0.0114744643827802 | Epoch: 27 | MeanAbsoluteError: 0.1365180164575577 | Loss: 0.0227311748619142 | Epoch: 28 | MeanAbsoluteError: 0.0865994989871979 | Loss: 0.0108381827655984 | Epoch: 29 | MeanAbsoluteError: 0.2057358771562576 | Loss: 0.0483426990869798 | Epoch: 30 | MeanAbsoluteError: 0.0469194501638412 | Loss: 0.0037602314321128 | Epoch: 31 | MeanAbsoluteError: 0.0448267795145512 | Loss: 0.0035583891317640 | Epoch: 32 | MeanAbsoluteError: 0.0766490548849106 | Loss: 0.0090261244607207 | Epoch: 33 | \n\n\nMeanAbsoluteError: 0.1303312182426453 | Loss: 0.0213924754214914 | Epoch: 34 | MeanAbsoluteError: 0.0697895959019661 | Loss: 0.0072220992787104 | Epoch: 35 | MeanAbsoluteError: 0.1846313625574112 | Loss: 0.0396531166410760 | Epoch: 36 | MeanAbsoluteError: 0.1583141088485718 | Loss: 0.0284327727399374 | Epoch: 37 | MeanAbsoluteError: 0.0793339759111404 | Loss: 0.0081537633791174 | Epoch: 38 | MeanAbsoluteError: 0.0739497914910316 | Loss: 0.0094841570274806 | Epoch: 39 | MeanAbsoluteError: 0.1662362515926361 | Loss: 0.0332997327572421 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.1420662552118301 | Loss: 0.0239153548486923 | Epoch: 41 | MeanAbsoluteError: 0.0867024436593056 | Loss: 0.0096690679263127 | Epoch: 42 | MeanAbsoluteError: 0.0763177126646042 | Loss: 0.0082932123914361 | Epoch: 43 | MeanAbsoluteError: 0.1864439398050308 | Loss: 0.0395096333599404 | Epoch: 44 | MeanAbsoluteError: 0.0798052623867989 | Loss: 0.0090271937415788 | Epoch: 45 | MeanAbsoluteError: 0.1052517592906952 | Loss: 0.0137152883567308 | Epoch: 46 | MeanAbsoluteError: 0.1182453557848930 | Loss: 0.0181570319752944 | Epoch: 47 | \n\n\nMeanAbsoluteError: 0.1668316274881363 | Loss: 0.0308517109798758 | Epoch: 48 | MeanAbsoluteError: 0.1110147461295128 | Loss: 0.0158752806386665 | Epoch: 49 | MeanAbsoluteError: 0.1179506778717041 | Loss: 0.0160490939099538 | Epoch: 50 | MeanAbsoluteError: 0.0818099975585938 | Loss: 0.0104144844716709 | Epoch: 51 | MeanAbsoluteError: 0.0841964557766914 | Loss: 0.0109950211389284 | Epoch: 52 | MeanAbsoluteError: 0.0779909044504166 | Loss: 0.0078892512859679 | Epoch: 53 | MeanAbsoluteError: 0.0487340353429317 | Loss: 0.0034641669940596 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0700075179338455 | Loss: 0.0074443858803103 | Epoch: 55 | MeanAbsoluteError: 0.0599659904837608 | Loss: 0.0056789780378734 | Epoch: 56 | MeanAbsoluteError: 0.0719801783561707 | Loss: 0.0065798873670007 | Epoch: 57 | MeanAbsoluteError: 0.1040535196661949 | Loss: 0.0130038033974798 | Epoch: 58 | MeanAbsoluteError: 0.0962022021412849 | Loss: 0.0114611196576765 | Epoch: 59 | MeanAbsoluteError: 0.1152371466159821 | Loss: 0.0167004823390591 | Epoch: 60 | MeanAbsoluteError: 0.0862926244735718 | Loss: 0.0112873034698791 | Epoch: 61 | \n\n\nMeanAbsoluteError: 0.1474125534296036 | Loss: 0.0244496893137693 | Epoch: 62 | MeanAbsoluteError: 0.0500518269836903 | Loss: 0.0041627376156516 | Epoch: 63 | MeanAbsoluteError: 0.1175862625241280 | Loss: 0.0177539302232234 | Epoch: 64 | MeanAbsoluteError: 0.1492669135332108 | Loss: 0.0255475984396119 | Epoch: 65 | MeanAbsoluteError: 0.0395804867148399 | Loss: 0.0030609877890368 | Epoch: 66 | MeanAbsoluteError: 0.0660640299320221 | Loss: 0.0069263227112395 | Epoch: 67 | MeanAbsoluteError: 0.0900965705513954 | Loss: 0.0112227676436305 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.1059087216854095 | Loss: 0.0161116328335514 | Epoch: 69 | MeanAbsoluteError: 0.0994285941123962 | Loss: 0.0117991115946911 | Epoch: 70 | MeanAbsoluteError: 0.0859782472252846 | Loss: 0.0101065068799806 | Epoch: 71 | MeanAbsoluteError: 0.0632252916693687 | Loss: 0.0055160937144568 | Epoch: 72 | MeanAbsoluteError: 0.0404573939740658 | Loss: 0.0030459093211807 | Epoch: 73 | MeanAbsoluteError: 0.0857437178492546 | Loss: 0.0099626786663736 | Epoch: 74 | MeanAbsoluteError: 0.1511537283658981 | Loss: 0.0265587598673607 | Epoch: 75 | \n\n\nMeanAbsoluteError: 0.0646555349230766 | Loss: 0.0066608924198111 | Epoch: 76 | MeanAbsoluteError: 0.1031965389847755 | Loss: 0.0146031939473591 | Epoch: 77 | MeanAbsoluteError: 0.0475217364728451 | Loss: 0.0033787749830241 | Epoch: 78 | MeanAbsoluteError: 0.0855163037776947 | Loss: 0.0097266553999170 | Epoch: 79 | MeanAbsoluteError: 0.1032926067709923 | Loss: 0.0125563403003310 | Epoch: 80 | MeanAbsoluteError: 0.1013218984007835 | Loss: 0.0132121227093433 | Epoch: 81 | MeanAbsoluteError: 0.0446976013481617 | Loss: 0.0032387520738044 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0564983114600182 | Loss: 0.0055826119465851 | Epoch: 83 | MeanAbsoluteError: 0.1198457106947899 | Loss: 0.0160797359421849 | Epoch: 84 | MeanAbsoluteError: 0.0967130735516548 | Loss: 0.0117464226327444 | Epoch: 85 | MeanAbsoluteError: 0.1013648286461830 | Loss: 0.0125555145113092 | Epoch: 86 | MeanAbsoluteError: 0.1382317543029785 | Loss: 0.0221787381329035 | Epoch: 87 | MeanAbsoluteError: 0.1271218061447144 | Loss: 0.0183710536772483 | Epoch: 88 | MeanAbsoluteError: 0.0904059708118439 | Loss: 0.0104597118732176 | Epoch: 89 | \n\n\nMeanAbsoluteError: 0.0826918259263039 | Loss: 0.0111324938112184 | Epoch: 90 | MeanAbsoluteError: 0.0489262230694294 | Loss: 0.0039457296094808 | Epoch: 91 | MeanAbsoluteError: 0.0390503890812397 | Loss: 0.0024470696378940 | Epoch: 92 | MeanAbsoluteError: 0.1222936362028122 | Loss: 0.0171547859024845 | Epoch: 93 | MeanAbsoluteError: 0.0776587203145027 | Loss: 0.0076592049904560 | Epoch: 94 | MeanAbsoluteError: 0.1007902622222900 | Loss: 0.0127964182511756 | Epoch: 95 | MeanAbsoluteError: 0.0730546638369560 | Loss: 0.0092279901728034 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0294175110757351 | Loss: 0.0016112055960356 | Epoch: 97 | MeanAbsoluteError: 0.0720206424593925 | Loss: 0.0087846253116272 | Epoch: 98 | MeanAbsoluteError: 0.1097121462225914 | Loss: 0.0157666019605179 | Epoch: 99 | MeanAbsoluteError: 0.0631164982914925 | Loss: 0.0053555719113271 | Epoch: 100 | MeanAbsoluteError: 0.0683856680989265 | Loss: 0.0066683712709499 | Epoch: 101 | MeanAbsoluteError: 0.0803041085600853 | Loss: 0.0081551621520990 | Epoch: 102 | MeanAbsoluteError: 0.0603091940283775 | Loss: 0.0060057822541383 | Epoch: 103 | \n\n\nMeanAbsoluteError: 0.0536838918924332 | Loss: 0.0043626468212001 | Epoch: 104 | MeanAbsoluteError: 0.0719090998172760 | Loss: 0.0076389560034793 | Epoch: 105 | MeanAbsoluteError: 0.0449344329535961 | Loss: 0.0032421149815874 | Epoch: 106 | MeanAbsoluteError: 0.0900320261716843 | Loss: 0.0092780679023187 | Epoch: 107 | MeanAbsoluteError: 0.0667212009429932 | Loss: 0.0079713496134469 | Epoch: 108 | MeanAbsoluteError: 0.0421115420758724 | Loss: 0.0026515793870203 | Epoch: 109 | MeanAbsoluteError: 0.1047971248626709 | Loss: 0.0139890252367446 | Epoch: 110 | \n\n\nMeanAbsoluteError: 0.0642061382532120 | Loss: 0.0062053107141860 | Epoch: 111 | MeanAbsoluteError: 0.1082605123519897 | Loss: 0.0138008100911975 | Epoch: 112 | MeanAbsoluteError: 0.0604376085102558 | Loss: 0.0056236247581087 | Epoch: 113 | MeanAbsoluteError: 0.0996381416916847 | Loss: 0.0121009073122159 | Epoch: 114 | MeanAbsoluteError: 0.1398922502994537 | Loss: 0.0227468806367956 | Epoch: 115 | MeanAbsoluteError: 0.0455017834901810 | Loss: 0.0037813053809498 | Epoch: 116 | MeanAbsoluteError: 0.0638563632965088 | Loss: 0.0059528130743849 | Epoch: 117 | \n\n\nMeanAbsoluteError: 0.0629519298672676 | Loss: 0.0064933414963123 | Epoch: 118 | MeanAbsoluteError: 0.1223500072956085 | Loss: 0.0174680155162749 | Epoch: 119 | MeanAbsoluteError: 0.0400099121034145 | Loss: 0.0029936367768402 | Epoch: 120 | MeanAbsoluteError: 0.0626464933156967 | Loss: 0.0064293794750579 | Epoch: 121 | MeanAbsoluteError: 0.0615461654961109 | Loss: 0.0062645051358758 | Epoch: 122 | MeanAbsoluteError: 0.0877861827611923 | Loss: 0.0093834406549209 | Epoch: 123 | MeanAbsoluteError: 0.0406103394925594 | Loss: 0.0030191649566405 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0676956549286842 | Loss: 0.0068483953731821 | Epoch: 125 | MeanAbsoluteError: 0.0466613061726093 | Loss: 0.0036530480720103 | Epoch: 126 | MeanAbsoluteError: 0.1071237549185753 | Loss: 0.0153765492141247 | Epoch: 127 | MeanAbsoluteError: 0.0755853354930878 | Loss: 0.0074220634261636 | Epoch: 128 | MeanAbsoluteError: 0.0744201093912125 | Loss: 0.0084128797691511 | Early stopping at epoch 127\nReturned to Spot: Validation loss: 0.008412879769151149\n\nconfig: {'_L_in': 10, '_L_out': 1, 'l1': 128, 'dropout_prob': 0.8582565260508446, 'lr_mult': 0.4977322453322358, 'batch_size': 2, 'epochs': 32768, 'k_folds': 1, 'patience': 128, 'optimizer': 'ASGD', 'sgd_momentum': 0.6834550718769361}\nEpoch: 1 | \n\n\nMeanAbsoluteError: 0.2120186090469360 | Loss: 0.0703260603727540 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1810018420219421 | Loss: 0.0505256056096793 | Epoch: 3 | \n\n\nMeanAbsoluteError: 0.1865642964839935 | Loss: 0.0566911546637615 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1646195650100708 | Loss: 0.0437425101858874 | Epoch: 5 | \n\n\nMeanAbsoluteError: 0.1568263173103333 | Loss: 0.0371684035678481 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.1412837058305740 | Loss: 0.0330436270256178 | Epoch: 7 | \n\n\nMeanAbsoluteError: 0.1431057304143906 | Loss: 0.0315908586202810 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.1355354636907578 | Loss: 0.0296658951790111 | Epoch: 9 | \n\n\nMeanAbsoluteError: 0.1423906832933426 | Loss: 0.0309638479724526 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.1394383162260056 | Loss: 0.0300875054730568 | Epoch: 11 | \n\n\nMeanAbsoluteError: 0.1428710371255875 | Loss: 0.0317877288514865 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.1465101093053818 | Loss: 0.0327670423194165 | Epoch: 13 | \n\n\nMeanAbsoluteError: 0.1430340707302094 | Loss: 0.0325605637036885 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.1328897029161453 | Loss: 0.0293118926799616 | Epoch: 15 | \n\n\nMeanAbsoluteError: 0.1347741186618805 | Loss: 0.0305029440240469 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.1387603431940079 | Loss: 0.0306267528469713 | Epoch: 17 | \n\n\nMeanAbsoluteError: 0.1334645003080368 | Loss: 0.0291014335666356 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.1407389938831329 | Loss: 0.0321883206760200 | Epoch: 19 | \n\n\nMeanAbsoluteError: 0.1371100544929504 | Loss: 0.0299572567752330 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.1350206434726715 | Loss: 0.0295970215832494 | Epoch: 21 | \n\n\nMeanAbsoluteError: 0.1375719010829926 | Loss: 0.0307873552478850 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.1362883448600769 | Loss: 0.0298302128487315 | Epoch: 23 | \n\n\nMeanAbsoluteError: 0.1387530416250229 | Loss: 0.0304712078013108 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.1391585767269135 | Loss: 0.0302951655749105 | Epoch: 25 | \n\n\nMeanAbsoluteError: 0.1324227154254913 | Loss: 0.0296312491426458 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.1331820189952850 | Loss: 0.0289244223200145 | Epoch: 27 | \n\n\nMeanAbsoluteError: 0.1381265223026276 | Loss: 0.0297591915292044 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.1361352503299713 | Loss: 0.0295886449449851 | Epoch: 29 | \n\n\nMeanAbsoluteError: 0.1326842010021210 | Loss: 0.0294289984286297 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.1377019435167313 | Loss: 0.0302947151190892 | Epoch: 31 | \n\n\nMeanAbsoluteError: 0.1371371448040009 | Loss: 0.0293225908155243 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.1376724243164062 | Loss: 0.0298109506095110 | Epoch: 33 | \n\n\nMeanAbsoluteError: 0.1331874728202820 | Loss: 0.0278915133918539 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.1353287249803543 | Loss: 0.0287761341780424 | Epoch: 35 | \n\n\nMeanAbsoluteError: 0.1371500343084335 | Loss: 0.0290998212846656 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.1316709071397781 | Loss: 0.0279534811475545 | Epoch: 37 | \n\n\nMeanAbsoluteError: 0.1321931481361389 | Loss: 0.0278238536942930 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.1326586902141571 | Loss: 0.0281608481489820 | Epoch: 39 | \n\n\nMeanAbsoluteError: 0.1386570036411285 | Loss: 0.0302074358894606 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.1343215554952621 | Loss: 0.0285096010980002 | Epoch: 41 | \n\n\nMeanAbsoluteError: 0.1350935548543930 | Loss: 0.0289603875417773 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.1319969147443771 | Loss: 0.0279253492220111 | Epoch: 43 | \n\n\nMeanAbsoluteError: 0.1388250291347504 | Loss: 0.0303426023551341 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.1345654129981995 | Loss: 0.0292127694224473 | Epoch: 45 | \n\n\nMeanAbsoluteError: 0.1327299177646637 | Loss: 0.0284869099524803 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.1330085396766663 | Loss: 0.0285624090783919 | Epoch: 47 | \n\n\nMeanAbsoluteError: 0.1385326534509659 | Loss: 0.0294748234395714 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.1337530314922333 | Loss: 0.0289091440066113 | Epoch: 49 | \n\n\nMeanAbsoluteError: 0.1339507251977921 | Loss: 0.0294155366136692 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.1339053213596344 | Loss: 0.0283644676118274 | Epoch: 51 | \n\n\nMeanAbsoluteError: 0.1360580772161484 | Loss: 0.0290031787902020 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.1303653568029404 | Loss: 0.0279984095306039 | Epoch: 53 | \n\n\nMeanAbsoluteError: 0.1319592893123627 | Loss: 0.0274316242581699 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.1355709880590439 | Loss: 0.0292173637603992 | Epoch: 55 | \n\n\nMeanAbsoluteError: 0.1334413141012192 | Loss: 0.0280288543089409 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.1323261409997940 | Loss: 0.0280817442221451 | Epoch: 57 | \n\n\nMeanAbsoluteError: 0.1378538459539413 | Loss: 0.0301353955361507 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.1334509849548340 | Loss: 0.0286659593512377 | Epoch: 59 | \n\n\nMeanAbsoluteError: 0.1366765201091766 | Loss: 0.0291514151597706 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.1326360553503036 | Loss: 0.0280965372391014 | Epoch: 61 | \n\n\nMeanAbsoluteError: 0.1350936442613602 | Loss: 0.0302423853119399 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.1308288872241974 | Loss: 0.0281096940835899 | Epoch: 63 | \n\n\nMeanAbsoluteError: 0.1355766206979752 | Loss: 0.0288882262321810 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.1337034255266190 | Loss: 0.0282789689570200 | Epoch: 65 | \n\n\nMeanAbsoluteError: 0.1345267444849014 | Loss: 0.0279725155958537 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.1359749287366867 | Loss: 0.0284281274370733 | Epoch: 67 | \n\n\nMeanAbsoluteError: 0.1328504532575607 | Loss: 0.0282725923762579 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.1336105167865753 | Loss: 0.0287253562085486 | Epoch: 69 | \n\n\nMeanAbsoluteError: 0.1342981904745102 | Loss: 0.0283503484051713 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.1325342655181885 | Loss: 0.0288481734237090 | Epoch: 71 | \n\n\nMeanAbsoluteError: 0.1366259008646011 | Loss: 0.0295639015012057 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.1374526470899582 | Loss: 0.0297523628144215 | Epoch: 73 | \n\n\nMeanAbsoluteError: 0.1305706202983856 | Loss: 0.0279974152372840 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.1386067122220993 | Loss: 0.0301010068183920 | Epoch: 75 | \n\n\nMeanAbsoluteError: 0.1312133669853210 | Loss: 0.0281737590644237 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.1333376616239548 | Loss: 0.0288979109056527 | Epoch: 77 | \n\n\nMeanAbsoluteError: 0.1323501765727997 | Loss: 0.0279138443401462 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.1342074722051620 | Loss: 0.0290591010072967 | Epoch: 79 | \n\n\nMeanAbsoluteError: 0.1321343928575516 | Loss: 0.0285792711131702 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.1327207237482071 | Loss: 0.0282896691568142 | Epoch: 81 | \n\n\nMeanAbsoluteError: 0.1333977282047272 | Loss: 0.0281625716512159 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.1292987763881683 | Loss: 0.0273094001925589 | Epoch: 83 | \n\n\nMeanAbsoluteError: 0.1364092230796814 | Loss: 0.0294512013099544 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.1293014734983444 | Loss: 0.0275924904380615 | Epoch: 85 | \n\n\nMeanAbsoluteError: 0.1311487406492233 | Loss: 0.0273892649616270 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.1317169517278671 | Loss: 0.0287904202892241 | Epoch: 87 | \n\n\nMeanAbsoluteError: 0.1361114233732224 | Loss: 0.0289758128756269 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.1340002566576004 | Loss: 0.0282278227577141 | Epoch: 89 | \n\n\nMeanAbsoluteError: 0.1289712488651276 | Loss: 0.0267172804403208 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.1305756270885468 | Loss: 0.0272678010169572 | Epoch: 91 | \n\n\nMeanAbsoluteError: 0.1316790729761124 | Loss: 0.0278686433073502 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.1332269757986069 | Loss: 0.0286615654393487 | Epoch: 93 | \n\n\nMeanAbsoluteError: 0.1310379952192307 | Loss: 0.0275241333114536 | Epoch: 94 | \n\n\nMeanAbsoluteError: 0.1304696649312973 | Loss: 0.0270054221163931 | Epoch: 95 | \n\n\nMeanAbsoluteError: 0.1310379356145859 | Loss: 0.0284023737051029 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.1312282383441925 | Loss: 0.0273880553885829 | Epoch: 97 | \n\n\nMeanAbsoluteError: 0.1353152394294739 | Loss: 0.0286431961657945 | Epoch: 98 | \n\n\nMeanAbsoluteError: 0.1354440152645111 | Loss: 0.0287843259796985 | Epoch: 99 | \n\n\nMeanAbsoluteError: 0.1329240053892136 | Loss: 0.0286299581407608 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.1318104863166809 | Loss: 0.0273586199727530 | Epoch: 101 | \n\n\nMeanAbsoluteError: 0.1330233067274094 | Loss: 0.0282443891942967 | Epoch: 102 | \n\n\nMeanAbsoluteError: 0.1353953480720520 | Loss: 0.0289314922907700 | Epoch: 103 | \n\n\nMeanAbsoluteError: 0.1366895735263824 | Loss: 0.0293128538174399 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.1299520134925842 | Loss: 0.0271956734422565 | Epoch: 105 | \n\n\nMeanAbsoluteError: 0.1286842077970505 | Loss: 0.0264736638077981 | Epoch: 106 | \n\n\nMeanAbsoluteError: 0.1300923526287079 | Loss: 0.0280097470215939 | Epoch: 107 | \n\n\nMeanAbsoluteError: 0.1317453086376190 | Loss: 0.0272147196092313 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.1310875415802002 | Loss: 0.0275518631739639 | Epoch: 109 | \n\n\nMeanAbsoluteError: 0.1288941055536270 | Loss: 0.0273225454625208 | Epoch: 110 | \n\n\nMeanAbsoluteError: 0.1327576935291290 | Loss: 0.0269843656129281 | Epoch: 111 | \n\n\nMeanAbsoluteError: 0.1322483569383621 | Loss: 0.0275993045151699 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.1297423839569092 | Loss: 0.0269908279016515 | Epoch: 113 | \n\n\nMeanAbsoluteError: 0.1320201307535172 | Loss: 0.0287010687141931 | Epoch: 114 | \n\n\nMeanAbsoluteError: 0.1348825097084045 | Loss: 0.0275787125873224 | Epoch: 115 | \n\n\nMeanAbsoluteError: 0.1302690058946609 | Loss: 0.0282409454350219 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.1284077763557434 | Loss: 0.0269025370860375 | Epoch: 117 | \n\n\nMeanAbsoluteError: 0.1287221461534500 | Loss: 0.0269270132705424 | Epoch: 118 | \n\n\nMeanAbsoluteError: 0.1350702792406082 | Loss: 0.0287112471359433 | Epoch: 119 | \n\n\nMeanAbsoluteError: 0.1365754753351212 | Loss: 0.0297595323049851 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.1336092650890350 | Loss: 0.0276606052325224 | Epoch: 121 | \n\n\nMeanAbsoluteError: 0.1307865381240845 | Loss: 0.0278533773931849 | Epoch: 122 | \n\n\nMeanAbsoluteError: 0.1312122344970703 | Loss: 0.0273731775764342 | Epoch: 123 | \n\n\nMeanAbsoluteError: 0.1259114891290665 | Loss: 0.0248430239704127 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.1351978033781052 | Loss: 0.0293973915860988 | Epoch: 125 | \n\n\nMeanAbsoluteError: 0.1337301582098007 | Loss: 0.0287696035550713 | Epoch: 126 | \n\n\nMeanAbsoluteError: 0.1292222142219543 | Loss: 0.0269459883719780 | Epoch: 127 | \n\n\nMeanAbsoluteError: 0.1285059005022049 | Loss: 0.0268508184685682 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.1300139129161835 | Loss: 0.0273952740839930 | Epoch: 129 | \n\n\nMeanAbsoluteError: 0.1335322707891464 | Loss: 0.0282488289336713 | Epoch: 130 | \n\n\nMeanAbsoluteError: 0.1252573430538177 | Loss: 0.0255147537779218 | Epoch: 131 | \n\n\nMeanAbsoluteError: 0.1305999308824539 | Loss: 0.0278443359248195 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.1342406719923019 | Loss: 0.0293141385663572 | Epoch: 133 | \n\n\nMeanAbsoluteError: 0.1304786354303360 | Loss: 0.0272754495363915 | Epoch: 134 | \n\n\nMeanAbsoluteError: 0.1279505044221878 | Loss: 0.0268895215221952 | Epoch: 135 | \n\n\nMeanAbsoluteError: 0.1309549957513809 | Loss: 0.0271134553019268 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.1349014639854431 | Loss: 0.0279526457898707 | Epoch: 137 | \n\n\nMeanAbsoluteError: 0.1312617957592010 | Loss: 0.0271330525574740 | Epoch: 138 | \n\n\nMeanAbsoluteError: 0.1275500655174255 | Loss: 0.0270875825789820 | Epoch: 139 | \n\n\nMeanAbsoluteError: 0.1325341612100601 | Loss: 0.0288123536831699 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.1316670626401901 | Loss: 0.0272573304520726 | Epoch: 141 | \n\n\nMeanAbsoluteError: 0.1277662217617035 | Loss: 0.0256321397927994 | Epoch: 142 | \n\n\nMeanAbsoluteError: 0.1335927993059158 | Loss: 0.0277764047860789 | Epoch: 143 | \n\n\nMeanAbsoluteError: 0.1306809931993484 | Loss: 0.0271367594196151 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.1311477869749069 | Loss: 0.0268596000155473 | Epoch: 145 | \n\n\nMeanAbsoluteError: 0.1324852108955383 | Loss: 0.0276451517064318 | Epoch: 146 | \n\n\nMeanAbsoluteError: 0.1284862756729126 | Loss: 0.0273937621685279 | Epoch: 147 | \n\n\nMeanAbsoluteError: 0.1308698207139969 | Loss: 0.0266675920836527 | Epoch: 148 | \n\n\nMeanAbsoluteError: 0.1298611909151077 | Loss: 0.0260687085362345 | Epoch: 149 | \n\n\nMeanAbsoluteError: 0.1300632357597351 | Loss: 0.0275133539268184 | Epoch: 150 | \n\n\nMeanAbsoluteError: 0.1315701305866241 | Loss: 0.0277302167108185 | Epoch: 151 | \n\n\nMeanAbsoluteError: 0.1310491412878036 | Loss: 0.0287894432103591 | Epoch: 152 | \n\n\nMeanAbsoluteError: 0.1290020048618317 | Loss: 0.0272625670838170 | Epoch: 153 | \n\n\nMeanAbsoluteError: 0.1248474493622780 | Loss: 0.0250066204535930 | Epoch: 154 | \n\n\nMeanAbsoluteError: 0.1285637617111206 | Loss: 0.0259650660865009 | Epoch: 155 | \n\n\nMeanAbsoluteError: 0.1254619061946869 | Loss: 0.0256261140022737 | Epoch: 156 | \n\n\nMeanAbsoluteError: 0.1265094131231308 | Loss: 0.0251080634034588 | Epoch: 157 | \n\n\nMeanAbsoluteError: 0.1304135769605637 | Loss: 0.0281591438706285 | Epoch: 158 | \n\n\nMeanAbsoluteError: 0.1299573034048080 | Loss: 0.0265635735887160 | Epoch: 159 | \n\n\nMeanAbsoluteError: 0.1280676722526550 | Loss: 0.0259627108234175 | Epoch: 160 | \n\n\nMeanAbsoluteError: 0.1290737390518188 | Loss: 0.0258418709537242 | Epoch: 161 | \n\n\nMeanAbsoluteError: 0.1282917857170105 | Loss: 0.0263422335257443 | Epoch: 162 | \n\n\nMeanAbsoluteError: 0.1255858689546585 | Loss: 0.0258821290247821 | Epoch: 163 | \n\n\nMeanAbsoluteError: 0.1247152835130692 | Loss: 0.0252802162902587 | Epoch: 164 | \n\n\nMeanAbsoluteError: 0.1281282305717468 | Loss: 0.0273769241780004 | Epoch: 165 | \n\n\nMeanAbsoluteError: 0.1326295137405396 | Loss: 0.0276103059679735 | Epoch: 166 | \n\n\nMeanAbsoluteError: 0.1292429864406586 | Loss: 0.0258200121601112 | Epoch: 167 | \n\n\nMeanAbsoluteError: 0.1271379590034485 | Loss: 0.0263768902743080 | Epoch: 168 | \n\n\nMeanAbsoluteError: 0.1260447204113007 | Loss: 0.0259915925845174 | Epoch: 169 | \n\n\nMeanAbsoluteError: 0.1272253245115280 | Loss: 0.0257439455026179 | Epoch: 170 | \n\n\nMeanAbsoluteError: 0.1278935819864273 | Loss: 0.0265197621749636 | Epoch: 171 | \n\n\nMeanAbsoluteError: 0.1273311674594879 | Loss: 0.0263499089929004 | Epoch: 172 | \n\n\nMeanAbsoluteError: 0.1287077069282532 | Loss: 0.0269597334081967 | Epoch: 173 | \n\n\nMeanAbsoluteError: 0.1280221641063690 | Loss: 0.0269945946401761 | Epoch: 174 | \n\n\nMeanAbsoluteError: 0.1294401437044144 | Loss: 0.0257164478666770 | Epoch: 175 | \n\n\nMeanAbsoluteError: 0.1294787526130676 | Loss: 0.0261703853804890 | Epoch: 176 | \n\n\nMeanAbsoluteError: 0.1294941604137421 | Loss: 0.0258839001452725 | Epoch: 177 | \n\n\nMeanAbsoluteError: 0.1255796104669571 | Loss: 0.0255944881494361 | Epoch: 178 | \n\n\nMeanAbsoluteError: 0.1276107877492905 | Loss: 0.0264388611582884 | Epoch: 179 | \n\n\nMeanAbsoluteError: 0.1290149390697479 | Loss: 0.0274764347590584 | Epoch: 180 | \n\n\nMeanAbsoluteError: 0.1264387369155884 | Loss: 0.0256394068574688 | Epoch: 181 | \n\n\nMeanAbsoluteError: 0.1296527534723282 | Loss: 0.0274288167705042 | Epoch: 182 | \n\n\nMeanAbsoluteError: 0.1267398595809937 | Loss: 0.0253798474264355 | Epoch: 183 | \n\n\nMeanAbsoluteError: 0.1243575140833855 | Loss: 0.0257659080597417 | Epoch: 184 | \n\n\nMeanAbsoluteError: 0.1295987665653229 | Loss: 0.0261230329452398 | Epoch: 185 | \n\n\nMeanAbsoluteError: 0.1255244165658951 | Loss: 0.0248979027364840 | Epoch: 186 | \n\n\nMeanAbsoluteError: 0.1232759505510330 | Loss: 0.0252266960297857 | Epoch: 187 | \n\n\nMeanAbsoluteError: 0.1304834038019180 | Loss: 0.0264449265131649 | Epoch: 188 | \n\n\nMeanAbsoluteError: 0.1306728422641754 | Loss: 0.0269722481963496 | Epoch: 189 | \n\n\nMeanAbsoluteError: 0.1308171749114990 | Loss: 0.0276412888390284 | Epoch: 190 | \n\n\nMeanAbsoluteError: 0.1268077343702316 | Loss: 0.0257614707598479 | Epoch: 191 | \n\n\nMeanAbsoluteError: 0.1293821632862091 | Loss: 0.0258688859509615 | Epoch: 192 | \n\n\nMeanAbsoluteError: 0.1285744905471802 | Loss: 0.0259593739083114 | Epoch: 193 | \n\n\nMeanAbsoluteError: 0.1311777979135513 | Loss: 0.0279130068793893 | Epoch: 194 | \n\n\nMeanAbsoluteError: 0.1265490055084229 | Loss: 0.0251082413139617 | Epoch: 195 | \n\n\nMeanAbsoluteError: 0.1294135153293610 | Loss: 0.0255390535813058 | Epoch: 196 | \n\n\nMeanAbsoluteError: 0.1285629719495773 | Loss: 0.0262572451553812 | Epoch: 197 | \n\n\nMeanAbsoluteError: 0.1269385963678360 | Loss: 0.0266547033157743 | Epoch: 198 | \n\n\nMeanAbsoluteError: 0.1232421398162842 | Loss: 0.0242614319078469 | Epoch: 199 | \n\n\nMeanAbsoluteError: 0.1254636943340302 | Loss: 0.0252718027384253 | Epoch: 200 | \n\n\nMeanAbsoluteError: 0.1319849342107773 | Loss: 0.0268504156509395 | Epoch: 201 | \n\n\nMeanAbsoluteError: 0.1240827292203903 | Loss: 0.0252360228451046 | Epoch: 202 | \n\n\nMeanAbsoluteError: 0.1279827505350113 | Loss: 0.0262571594592979 | Epoch: 203 | \n\n\nMeanAbsoluteError: 0.1287140101194382 | Loss: 0.0263287174220507 | Epoch: 204 | \n\n\nMeanAbsoluteError: 0.1302036345005035 | Loss: 0.0274176565465556 | Epoch: 205 | \n\n\nMeanAbsoluteError: 0.1273660659790039 | Loss: 0.0251756095251282 | Epoch: 206 | \n\n\nMeanAbsoluteError: 0.1235448420047760 | Loss: 0.0250906329894982 | Epoch: 207 | \n\n\nMeanAbsoluteError: 0.1312414705753326 | Loss: 0.0280390263841703 | Epoch: 208 | \n\n\nMeanAbsoluteError: 0.1310118138790131 | Loss: 0.0271201221932036 | Epoch: 209 | \n\n\nMeanAbsoluteError: 0.1235399022698402 | Loss: 0.0243623909369732 | Epoch: 210 | \n\n\nMeanAbsoluteError: 0.1265280544757843 | Loss: 0.0250182282552123 | Epoch: 211 | \n\n\nMeanAbsoluteError: 0.1269433498382568 | Loss: 0.0249399786528860 | Epoch: 212 | \n\n\nMeanAbsoluteError: 0.1302993446588516 | Loss: 0.0256780259546698 | Epoch: 213 | \n\n\nMeanAbsoluteError: 0.1256161183118820 | Loss: 0.0263076916363692 | Epoch: 214 | \n\n\nMeanAbsoluteError: 0.1283417195081711 | Loss: 0.0262967085385753 | Epoch: 215 | \n\n\nMeanAbsoluteError: 0.1261629462242126 | Loss: 0.0249634917238533 | Epoch: 216 | \n\n\nMeanAbsoluteError: 0.1240288764238358 | Loss: 0.0261096895515220 | Epoch: 217 | \n\n\nMeanAbsoluteError: 0.1248076111078262 | Loss: 0.0249110484545721 | Epoch: 218 | \n\n\nMeanAbsoluteError: 0.1244403421878815 | Loss: 0.0252735649663373 | Epoch: 219 | \n\n\nMeanAbsoluteError: 0.1301815360784531 | Loss: 0.0262761523796750 | Epoch: 220 | \n\n\nMeanAbsoluteError: 0.1255878955125809 | Loss: 0.0255247489494892 | Epoch: 221 | \n\n\nMeanAbsoluteError: 0.1212717816233635 | Loss: 0.0236844168064029 | Epoch: 222 | \n\n\nMeanAbsoluteError: 0.1210223510861397 | Loss: 0.0240907421886853 | Epoch: 223 | \n\n\nMeanAbsoluteError: 0.1245773732662201 | Loss: 0.0254115196868467 | Epoch: 224 | \n\n\nMeanAbsoluteError: 0.1296063810586929 | Loss: 0.0267020640968622 | Epoch: 225 | \n\n\nMeanAbsoluteError: 0.1290546804666519 | Loss: 0.0264953143390206 | Epoch: 226 | \n\n\nMeanAbsoluteError: 0.1263755857944489 | Loss: 0.0249678306888867 | Epoch: 227 | \n\n\nMeanAbsoluteError: 0.1294817328453064 | Loss: 0.0261235514160944 | Epoch: 228 | \n\n\nMeanAbsoluteError: 0.1252022236585617 | Loss: 0.0245552085716433 | Epoch: 229 | \n\n\nMeanAbsoluteError: 0.1228638365864754 | Loss: 0.0237399011155746 | Epoch: 230 | \n\n\nMeanAbsoluteError: 0.1273647844791412 | Loss: 0.0247154118215743 | Epoch: 231 | \n\n\nMeanAbsoluteError: 0.1268457472324371 | Loss: 0.0253372148740649 | Epoch: 232 | \n\n\nMeanAbsoluteError: 0.1280005425214767 | Loss: 0.0258165353963947 | Epoch: 233 | \n\n\nMeanAbsoluteError: 0.1241355240345001 | Loss: 0.0248814803803301 | Epoch: 234 | \n\n\nMeanAbsoluteError: 0.1284396499395370 | Loss: 0.0257010499637302 | Epoch: 235 | \n\n\nMeanAbsoluteError: 0.1193779781460762 | Loss: 0.0230889252180350 | Epoch: 236 | \n\n\nMeanAbsoluteError: 0.1274115890264511 | Loss: 0.0255122450779891 | Epoch: 237 | \n\n\nMeanAbsoluteError: 0.1271371394395828 | Loss: 0.0257170588116666 | Epoch: 238 | \n\n\nMeanAbsoluteError: 0.1254123747348785 | Loss: 0.0247624477742162 | Epoch: 239 | \n\n\nMeanAbsoluteError: 0.1213201880455017 | Loss: 0.0237755814501240 | Epoch: 240 | \n\n\nMeanAbsoluteError: 0.1258153021335602 | Loss: 0.0251912250626871 | Epoch: 241 | \n\n\nMeanAbsoluteError: 0.1223629415035248 | Loss: 0.0236775729261475 | Epoch: 242 | \n\n\nMeanAbsoluteError: 0.1251785308122635 | Loss: 0.0239661729044747 | Epoch: 243 | \n\n\nMeanAbsoluteError: 0.1228665784001350 | Loss: 0.0237026224320289 | Epoch: 244 | \n\n\nMeanAbsoluteError: 0.1250875294208527 | Loss: 0.0242480014949494 | Epoch: 245 | \n\n\nMeanAbsoluteError: 0.1230146288871765 | Loss: 0.0239936253873520 | Epoch: 246 | \n\n\nMeanAbsoluteError: 0.1234828531742096 | Loss: 0.0245903088720903 | Epoch: 247 | \n\n\nMeanAbsoluteError: 0.1220218017697334 | Loss: 0.0237435963563621 | Epoch: 248 | \n\n\nMeanAbsoluteError: 0.1262366771697998 | Loss: 0.0254853705598786 | Epoch: 249 | \n\n\nMeanAbsoluteError: 0.1230556517839432 | Loss: 0.0233316236736331 | Epoch: 250 | \n\n\nMeanAbsoluteError: 0.1228279471397400 | Loss: 0.0243914106880159 | Epoch: 251 | \n\n\nMeanAbsoluteError: 0.1252284497022629 | Loss: 0.0243648657014516 | Epoch: 252 | \n\n\nMeanAbsoluteError: 0.1202717870473862 | Loss: 0.0240059932134075 | Epoch: 253 | \n\n\nMeanAbsoluteError: 0.1248144507408142 | Loss: 0.0248603751148645 | Epoch: 254 | \n\n\nMeanAbsoluteError: 0.1245057657361031 | Loss: 0.0244372017423787 | Epoch: 255 | \n\n\nMeanAbsoluteError: 0.1244364157319069 | Loss: 0.0241599635980553 | Epoch: 256 | \n\n\nMeanAbsoluteError: 0.1202607825398445 | Loss: 0.0237279424523391 | Epoch: 257 | \n\n\nMeanAbsoluteError: 0.1238339766860008 | Loss: 0.0249960520690850 | Epoch: 258 | \n\n\nMeanAbsoluteError: 0.1276592314243317 | Loss: 0.0255534794282721 | Epoch: 259 | \n\n\nMeanAbsoluteError: 0.1237990185618401 | Loss: 0.0241695611561105 | Epoch: 260 | \n\n\nMeanAbsoluteError: 0.1236243546009064 | Loss: 0.0241929944899554 | Epoch: 261 | \n\n\nMeanAbsoluteError: 0.1241858452558517 | Loss: 0.0239720756068709 | Epoch: 262 | \n\n\nMeanAbsoluteError: 0.1264587640762329 | Loss: 0.0261759109650302 | Epoch: 263 | \n\n\nMeanAbsoluteError: 0.1240594759583473 | Loss: 0.0240301931565045 | Epoch: 264 | \n\n\nMeanAbsoluteError: 0.1183805316686630 | Loss: 0.0225189908091367 | Epoch: 265 | \n\n\nMeanAbsoluteError: 0.1230043023824692 | Loss: 0.0242889010253809 | Epoch: 266 | \n\n\nMeanAbsoluteError: 0.1248593404889107 | Loss: 0.0242704000816836 | Epoch: 267 | \n\n\nMeanAbsoluteError: 0.1270556449890137 | Loss: 0.0251376440583651 | Epoch: 268 | \n\n\nMeanAbsoluteError: 0.1215813159942627 | Loss: 0.0244915152084044 | Epoch: 269 | \n\n\nMeanAbsoluteError: 0.1237580478191376 | Loss: 0.0244735323522279 | Epoch: 270 | \n\n\nMeanAbsoluteError: 0.1221730932593346 | Loss: 0.0240955389445298 | Epoch: 271 | \n\n\nMeanAbsoluteError: 0.1270957142114639 | Loss: 0.0252545164004550 | Epoch: 272 | \n\n\nMeanAbsoluteError: 0.1148176714777946 | Loss: 0.0216579226520844 | Epoch: 273 | \n\n\nMeanAbsoluteError: 0.1180030703544617 | Loss: 0.0223365843643963 | Epoch: 274 | \n\n\nMeanAbsoluteError: 0.1258095800876617 | Loss: 0.0245579604363108 | Epoch: 275 | \n\n\nMeanAbsoluteError: 0.1188945248723030 | Loss: 0.0228291239771837 | Epoch: 276 | \n\n\nMeanAbsoluteError: 0.1236833706498146 | Loss: 0.0246532167559296 | Epoch: 277 | \n\n\nMeanAbsoluteError: 0.1246208697557449 | Loss: 0.0244073370661742 | Epoch: 278 | \n\n\nMeanAbsoluteError: 0.1254842132329941 | Loss: 0.0252427536984518 | Epoch: 279 | \n\n\nMeanAbsoluteError: 0.1263048946857452 | Loss: 0.0248078077769605 | Epoch: 280 | \n\n\nMeanAbsoluteError: 0.1262692958116531 | Loss: 0.0248185696126711 | Epoch: 281 | \n\n\nMeanAbsoluteError: 0.1216441094875336 | Loss: 0.0232797028021499 | Epoch: 282 | \n\n\nMeanAbsoluteError: 0.1130554601550102 | Loss: 0.0203451398388521 | Epoch: 283 | \n\n\nMeanAbsoluteError: 0.1219326630234718 | Loss: 0.0233581321200472 | Epoch: 284 | \n\n\nMeanAbsoluteError: 0.1215624138712883 | Loss: 0.0238994007571212 | Epoch: 285 | \n\n\nMeanAbsoluteError: 0.1205800026655197 | Loss: 0.0243435294715164 | Epoch: 286 | \n\n\nMeanAbsoluteError: 0.1192249655723572 | Loss: 0.0230997134694674 | Epoch: 287 | \n\n\nMeanAbsoluteError: 0.1234614178538322 | Loss: 0.0242771851475603 | Epoch: 288 | \n\n\nMeanAbsoluteError: 0.1182799115777016 | Loss: 0.0223148198079434 | Epoch: 289 | \n\n\nMeanAbsoluteError: 0.1169704571366310 | Loss: 0.0223457057627578 | Epoch: 290 | \n\n\nMeanAbsoluteError: 0.1209284067153931 | Loss: 0.0224033042090984 | Epoch: 291 | \n\n\nMeanAbsoluteError: 0.1235294118523598 | Loss: 0.0244694840216350 | Epoch: 292 | \n\n\nMeanAbsoluteError: 0.1184247583150864 | Loss: 0.0221251936704599 | Epoch: 293 | \n\n\nMeanAbsoluteError: 0.1233468726277351 | Loss: 0.0243563544736147 | Epoch: 294 | \n\n\nMeanAbsoluteError: 0.1227299645543098 | Loss: 0.0240183893190988 | Epoch: 295 | \n\n\nMeanAbsoluteError: 0.1160365417599678 | Loss: 0.0222095203233766 | Epoch: 296 | \n\n\nMeanAbsoluteError: 0.1197649538516998 | Loss: 0.0235147712959830 | Epoch: 297 | \n\n\nMeanAbsoluteError: 0.1229562386870384 | Loss: 0.0239945658267728 | Epoch: 298 | \n\n\nMeanAbsoluteError: 0.1172887906432152 | Loss: 0.0219848311548897 | Epoch: 299 | \n\n\nMeanAbsoluteError: 0.1177181079983711 | Loss: 0.0223715692064798 | Epoch: 300 | \n\n\nMeanAbsoluteError: 0.1196416467428207 | Loss: 0.0229865148598522 | Epoch: 301 | \n\n\nMeanAbsoluteError: 0.1237161234021187 | Loss: 0.0239540329347559 | Epoch: 302 | \n\n\nMeanAbsoluteError: 0.1178010702133179 | Loss: 0.0225494718429885 | Epoch: 303 | \n\n\nMeanAbsoluteError: 0.1154479607939720 | Loss: 0.0223864391911290 | Epoch: 304 | \n\n\nMeanAbsoluteError: 0.1218003183603287 | Loss: 0.0235368169397164 | Epoch: 305 | \n\n\nMeanAbsoluteError: 0.1182143017649651 | Loss: 0.0224322849211664 | Epoch: 306 | \n\n\nMeanAbsoluteError: 0.1199145764112473 | Loss: 0.0228012109755946 | Epoch: 307 | \n\n\nMeanAbsoluteError: 0.1186259686946869 | Loss: 0.0228908841177084 | Epoch: 308 | \n\n\nMeanAbsoluteError: 0.1161173507571220 | Loss: 0.0223159791681246 | Epoch: 309 | \n\n\nMeanAbsoluteError: 0.1220061853528023 | Loss: 0.0241791760429624 | Epoch: 310 | \n\n\nMeanAbsoluteError: 0.1177025213837624 | Loss: 0.0228037213983043 | Epoch: 311 | \n\n\nMeanAbsoluteError: 0.1149889752268791 | Loss: 0.0205935589219492 | Epoch: 312 | \n\n\nMeanAbsoluteError: 0.1240251287817955 | Loss: 0.0248186623354559 | Epoch: 313 | \n\n\nMeanAbsoluteError: 0.1141514182090759 | Loss: 0.0222491351027080 | Epoch: 314 | \n\n\nMeanAbsoluteError: 0.1195131838321686 | Loss: 0.0233344910492694 | Epoch: 315 | \n\n\nMeanAbsoluteError: 0.1196048706769943 | Loss: 0.0226705304540034 | Epoch: 316 | \n\n\nMeanAbsoluteError: 0.1168054416775703 | Loss: 0.0221898538475701 | Epoch: 317 | \n\n\nMeanAbsoluteError: 0.1133572757244110 | Loss: 0.0215118650939742 | Epoch: 318 | \n\n\nMeanAbsoluteError: 0.1191605255007744 | Loss: 0.0230204561053930 | Epoch: 319 | \n\n\nMeanAbsoluteError: 0.1234762445092201 | Loss: 0.0244066480956584 | Epoch: 320 | \n\n\nMeanAbsoluteError: 0.1194803491234779 | Loss: 0.0225032700943848 | Epoch: 321 | \n\n\nMeanAbsoluteError: 0.1213790252804756 | Loss: 0.0229107769188704 | Epoch: 322 | \n\n\nMeanAbsoluteError: 0.1200166717171669 | Loss: 0.0236483666318721 | Epoch: 323 | \n\n\nMeanAbsoluteError: 0.1156007498502731 | Loss: 0.0214190742625700 | Epoch: 324 | \n\n\nMeanAbsoluteError: 0.1178014650940895 | Loss: 0.0224270146629230 | Epoch: 325 | \n\n\nMeanAbsoluteError: 0.1224776580929756 | Loss: 0.0233188403211534 | Epoch: 326 | \n\n\nMeanAbsoluteError: 0.1174355819821358 | Loss: 0.0224939409431439 | Epoch: 327 | \n\n\nMeanAbsoluteError: 0.1206283941864967 | Loss: 0.0231061742840878 | Epoch: 328 | \n\n\nMeanAbsoluteError: 0.1176040545105934 | Loss: 0.0215762830985477 | Epoch: 329 | \n\n\nMeanAbsoluteError: 0.1216259747743607 | Loss: 0.0237466979571339 | Epoch: 330 | \n\n\nMeanAbsoluteError: 0.1130979284644127 | Loss: 0.0211817415445694 | Epoch: 331 | \n\n\nMeanAbsoluteError: 0.1163686886429787 | Loss: 0.0221614260180892 | Epoch: 332 | \n\n\nMeanAbsoluteError: 0.1212222278118134 | Loss: 0.0232812536013080 | Epoch: 333 | \n\n\nMeanAbsoluteError: 0.1218428164720535 | Loss: 0.0232874786917819 | Epoch: 334 | \n\n\nMeanAbsoluteError: 0.1223931238055229 | Loss: 0.0240712700355895 | Epoch: 335 | \n\n\nMeanAbsoluteError: 0.1182417646050453 | Loss: 0.0225046846823534 | Epoch: 336 | \n\n\nMeanAbsoluteError: 0.1179233714938164 | Loss: 0.0222809315001359 | Epoch: 337 | \n\n\nMeanAbsoluteError: 0.1159746050834656 | Loss: 0.0209766264357071 | Epoch: 338 | \n\n\nMeanAbsoluteError: 0.1240573376417160 | Loss: 0.0243991470833619 | Epoch: 339 | \n\n\nMeanAbsoluteError: 0.1158688962459564 | Loss: 0.0212150746114457 | Epoch: 340 | \n\n\nMeanAbsoluteError: 0.1194109097123146 | Loss: 0.0232431417821984 | Epoch: 341 | \n\n\nMeanAbsoluteError: 0.1165453940629959 | Loss: 0.0220187043458767 | Epoch: 342 | \n\n\nMeanAbsoluteError: 0.1239329054951668 | Loss: 0.0240606422839725 | Epoch: 343 | \n\n\nMeanAbsoluteError: 0.1172060742974281 | Loss: 0.0227854207880106 | Epoch: 344 | \n\n\nMeanAbsoluteError: 0.1118096560239792 | Loss: 0.0214330766673023 | Epoch: 345 | \n\n\nMeanAbsoluteError: 0.1185575202107430 | Loss: 0.0219240760512184 | Epoch: 346 | \n\n\nMeanAbsoluteError: 0.1200724318623543 | Loss: 0.0234210979263298 | Epoch: 347 | \n\n\nMeanAbsoluteError: 0.1211094781756401 | Loss: 0.0233070489597352 | Epoch: 348 | \n\n\nMeanAbsoluteError: 0.1116342321038246 | Loss: 0.0215540307319801 | Epoch: 349 | \n\n\nMeanAbsoluteError: 0.1199874356389046 | Loss: 0.0233954876606974 | Epoch: 350 | \n\n\nMeanAbsoluteError: 0.1184523776173592 | Loss: 0.0217940312173838 | Epoch: 351 | \n\n\nMeanAbsoluteError: 0.1195407882332802 | Loss: 0.0223121488569086 | Epoch: 352 | \n\n\nMeanAbsoluteError: 0.1165901795029640 | Loss: 0.0222961723754997 | Epoch: 353 | \n\n\nMeanAbsoluteError: 0.1175039261579514 | Loss: 0.0225267448831194 | Epoch: 354 | \n\n\nMeanAbsoluteError: 0.1195927783846855 | Loss: 0.0226402265532912 | Epoch: 355 | \n\n\nMeanAbsoluteError: 0.1143198683857918 | Loss: 0.0220579164640124 | Epoch: 356 | \n\n\nMeanAbsoluteError: 0.1136191412806511 | Loss: 0.0213257168446823 | Epoch: 357 | \n\n\nMeanAbsoluteError: 0.1149886325001717 | Loss: 0.0211458008988484 | Epoch: 358 | \n\n\nMeanAbsoluteError: 0.1107658520340919 | Loss: 0.0192682874692643 | Epoch: 359 | \n\n\nMeanAbsoluteError: 0.1147879064083099 | Loss: 0.0217471077268904 | Epoch: 360 | \n\n\nMeanAbsoluteError: 0.1130817160010338 | Loss: 0.0220801224560516 | Epoch: 361 | \n\n\nMeanAbsoluteError: 0.1132632195949554 | Loss: 0.0207273442575145 | Epoch: 362 | \n\n\nMeanAbsoluteError: 0.1167882308363914 | Loss: 0.0214462839899352 | Epoch: 363 | \n\n\nMeanAbsoluteError: 0.1171880364418030 | Loss: 0.0221092081700529 | Epoch: 364 | \n\n\nMeanAbsoluteError: 0.1186132580041885 | Loss: 0.0228502222472647 | Epoch: 365 | \n\n\nMeanAbsoluteError: 0.1150320321321487 | Loss: 0.0220632059804241 | Epoch: 366 | \n\n\nMeanAbsoluteError: 0.1182830780744553 | Loss: 0.0222643580336201 | Epoch: 367 | \n\n\nMeanAbsoluteError: 0.1114400625228882 | Loss: 0.0204887805360098 | Epoch: 368 | \n\n\nMeanAbsoluteError: 0.1140805333852768 | Loss: 0.0208576685126172 | Epoch: 369 | \n\n\nMeanAbsoluteError: 0.1168960407376289 | Loss: 0.0221471475024979 | Epoch: 370 | \n\n\nMeanAbsoluteError: 0.1135093793272972 | Loss: 0.0208930895405986 | Epoch: 371 | \n\n\nMeanAbsoluteError: 0.1154692098498344 | Loss: 0.0214158519555349 | Epoch: 372 | \n\n\nMeanAbsoluteError: 0.1163660585880280 | Loss: 0.0217955080953349 | Epoch: 373 | \n\n\nMeanAbsoluteError: 0.1174861788749695 | Loss: 0.0218460062206335 | Epoch: 374 | \n\n\nMeanAbsoluteError: 0.1104818060994148 | Loss: 0.0202772429944404 | Epoch: 375 | \n\n\nMeanAbsoluteError: 0.1119258999824524 | Loss: 0.0202332984918030 | Epoch: 376 | \n\n\nMeanAbsoluteError: 0.1124326959252357 | Loss: 0.0204381199866960 | Epoch: 377 | \n\n\nMeanAbsoluteError: 0.1144667789340019 | Loss: 0.0209275649079791 | Epoch: 378 | \n\n\nMeanAbsoluteError: 0.1174579784274101 | Loss: 0.0212124693510123 | Epoch: 379 | \n\n\nMeanAbsoluteError: 0.1129389703273773 | Loss: 0.0209273037657598 | Epoch: 380 | \n\n\nMeanAbsoluteError: 0.1165678799152374 | Loss: 0.0225179783445492 | Epoch: 381 | \n\n\nMeanAbsoluteError: 0.1173844635486603 | Loss: 0.0222274894804771 | Epoch: 382 | \n\n\nMeanAbsoluteError: 0.1146249547600746 | Loss: 0.0218489120373852 | Epoch: 383 | \n\n\nMeanAbsoluteError: 0.1142560318112373 | Loss: 0.0212487880100283 | Epoch: 384 | \n\n\nMeanAbsoluteError: 0.1170064434409142 | Loss: 0.0214430761767532 | Epoch: 385 | \n\n\nMeanAbsoluteError: 0.1178095862269402 | Loss: 0.0227114900810799 | Epoch: 386 | \n\n\nMeanAbsoluteError: 0.1152981594204903 | Loss: 0.0203805722447578 | Epoch: 387 | \n\n\nMeanAbsoluteError: 0.1099806576967239 | Loss: 0.0209198426429733 | Epoch: 388 | \n\n\nMeanAbsoluteError: 0.1163994818925858 | Loss: 0.0212299005427481 | Epoch: 389 | \n\n\nMeanAbsoluteError: 0.1105142757296562 | Loss: 0.0194530630986962 | Epoch: 390 | \n\n\nMeanAbsoluteError: 0.1095910891890526 | Loss: 0.0195296341612993 | Epoch: 391 | \n\n\nMeanAbsoluteError: 0.1189906075596809 | Loss: 0.0217397838178052 | Epoch: 392 | \n\n\nMeanAbsoluteError: 0.1124819442629814 | Loss: 0.0200611247494089 | Epoch: 393 | \n\n\nMeanAbsoluteError: 0.1092707812786102 | Loss: 0.0188723875061260 | Epoch: 394 | \n\n\nMeanAbsoluteError: 0.1224186196923256 | Loss: 0.0239548805183343 | Epoch: 395 | \n\n\nMeanAbsoluteError: 0.1111733391880989 | Loss: 0.0198476465742957 | Epoch: 396 | \n\n\nMeanAbsoluteError: 0.1122889444231987 | Loss: 0.0196844923822209 | Epoch: 397 | \n\n\nMeanAbsoluteError: 0.1127983331680298 | Loss: 0.0210506061947914 | Epoch: 398 | \n\n\nMeanAbsoluteError: 0.1201107800006866 | Loss: 0.0217311210777552 | Epoch: 399 | \n\n\nMeanAbsoluteError: 0.1139668896794319 | Loss: 0.0220585581381056 | Epoch: 400 | \n\n\nMeanAbsoluteError: 0.1116853207349777 | Loss: 0.0196129331177713 | Epoch: 401 | \n\n\nMeanAbsoluteError: 0.1118237301707268 | Loss: 0.0202204244317060 | Epoch: 402 | \n\n\nMeanAbsoluteError: 0.1143093258142471 | Loss: 0.0204367951146560 | Epoch: 403 | \n\n\nMeanAbsoluteError: 0.1152245178818703 | Loss: 0.0214275294529701 | Epoch: 404 | \n\n\nMeanAbsoluteError: 0.1135919988155365 | Loss: 0.0200008137225329 | Epoch: 405 | \n\n\nMeanAbsoluteError: 0.1076651290059090 | Loss: 0.0194742295491354 | Epoch: 406 | \n\n\nMeanAbsoluteError: 0.1136413812637329 | Loss: 0.0208493048435776 | Epoch: 407 | \n\n\nMeanAbsoluteError: 0.1132154613733292 | Loss: 0.0207499179282847 | Epoch: 408 | \n\n\nMeanAbsoluteError: 0.1149136200547218 | Loss: 0.0209862414887175 | Epoch: 409 | \n\n\nMeanAbsoluteError: 0.1167603433132172 | Loss: 0.0212696080754904 | Epoch: 410 | \n\n\nMeanAbsoluteError: 0.1070325225591660 | Loss: 0.0187220245665715 | Epoch: 411 | \n\n\nMeanAbsoluteError: 0.1128829792141914 | Loss: 0.0203247473289957 | Epoch: 412 | \n\n\nMeanAbsoluteError: 0.1117471307516098 | Loss: 0.0202869968284964 | Epoch: 413 | \n\n\nMeanAbsoluteError: 0.1157646551728249 | Loss: 0.0228727674825738 | Epoch: 414 | \n\n\nMeanAbsoluteError: 0.1118759289383888 | Loss: 0.0203418312695188 | Epoch: 415 | \n\n\nMeanAbsoluteError: 0.1099071726202965 | Loss: 0.0195838913729919 | Epoch: 416 | \n\n\nMeanAbsoluteError: 0.1062930598855019 | Loss: 0.0181335148458372 | Epoch: 417 | \n\n\nMeanAbsoluteError: 0.1133919879794121 | Loss: 0.0218653961762902 | Epoch: 418 | \n\n\nMeanAbsoluteError: 0.1095461174845695 | Loss: 0.0193049807691326 | Epoch: 419 | \n\n\nMeanAbsoluteError: 0.1115411221981049 | Loss: 0.0198163908892699 | Epoch: 420 | \n\n\nMeanAbsoluteError: 0.1105751767754555 | Loss: 0.0198441829991255 | Epoch: 421 | \n\n\nMeanAbsoluteError: 0.1121315285563469 | Loss: 0.0207345993018437 | Epoch: 422 | \n\n\nMeanAbsoluteError: 0.1124980449676514 | Loss: 0.0206662259569081 | Epoch: 423 | \n\n\nMeanAbsoluteError: 0.1063612550497055 | Loss: 0.0187105724980938 | Epoch: 424 | \n\n\nMeanAbsoluteError: 0.1118531897664070 | Loss: 0.0200530546596807 | Epoch: 425 | \n\n\nMeanAbsoluteError: 0.1182781383395195 | Loss: 0.0212830083446291 | Epoch: 426 | \n\n\nMeanAbsoluteError: 0.1134472936391830 | Loss: 0.0215693161903376 | Epoch: 427 | \n\n\nMeanAbsoluteError: 0.1090097054839134 | Loss: 0.0193272986187852 | Epoch: 428 | \n\n\nMeanAbsoluteError: 0.1189713403582573 | Loss: 0.0218191263085464 | Epoch: 429 | \n\n\nMeanAbsoluteError: 0.1069624871015549 | Loss: 0.0187613645412057 | Epoch: 430 | \n\n\nMeanAbsoluteError: 0.1119088754057884 | Loss: 0.0203336658654249 | Epoch: 431 | \n\n\nMeanAbsoluteError: 0.1126610338687897 | Loss: 0.0198100412018660 | Epoch: 432 | \n\n\nMeanAbsoluteError: 0.1110223531723022 | Loss: 0.0186322512332117 | Epoch: 433 | \n\n\nMeanAbsoluteError: 0.1140923276543617 | Loss: 0.0204350578333833 | Epoch: 434 | \n\n\nMeanAbsoluteError: 0.1150250881910324 | Loss: 0.0206340097723781 | Epoch: 435 | \n\n\nMeanAbsoluteError: 0.1165786087512970 | Loss: 0.0209026506920539 | Epoch: 436 | \n\n\nMeanAbsoluteError: 0.1100169718265533 | Loss: 0.0206727742153938 | Epoch: 437 | \n\n\nMeanAbsoluteError: 0.1091668456792831 | Loss: 0.0188276749003368 | Epoch: 438 | \n\n\nMeanAbsoluteError: 0.1130163073539734 | Loss: 0.0201033811205222 | Epoch: 439 | \n\n\nMeanAbsoluteError: 0.1066261306405067 | Loss: 0.0188351355722989 | Epoch: 440 | \n\n\nMeanAbsoluteError: 0.1106951385736465 | Loss: 0.0188639791893850 | Epoch: 441 | \n\n\nMeanAbsoluteError: 0.1061897948384285 | Loss: 0.0177954066362387 | Epoch: 442 | \n\n\nMeanAbsoluteError: 0.1125590279698372 | Loss: 0.0206496567290863 | Epoch: 443 | \n\n\nMeanAbsoluteError: 0.1091147363185883 | Loss: 0.0195956855426933 | Epoch: 444 | \n\n\nMeanAbsoluteError: 0.1073110699653625 | Loss: 0.0189308695066817 | Epoch: 445 | \n\n\nMeanAbsoluteError: 0.1100836917757988 | Loss: 0.0199722595442048 | Epoch: 446 | \n\n\nMeanAbsoluteError: 0.1107389181852341 | Loss: 0.0195951706223423 | Epoch: 447 | \n\n\nMeanAbsoluteError: 0.1052037775516510 | Loss: 0.0189546610579904 | Epoch: 448 | \n\n\nMeanAbsoluteError: 0.1115877255797386 | Loss: 0.0200833873615852 | Epoch: 449 | \n\n\nMeanAbsoluteError: 0.1146247610449791 | Loss: 0.0205188814418216 | Epoch: 450 | \n\n\nMeanAbsoluteError: 0.1059228628873825 | Loss: 0.0186571678551748 | Epoch: 451 | \n\n\nMeanAbsoluteError: 0.1099486127495766 | Loss: 0.0193073006440560 | Epoch: 452 | \n\n\nMeanAbsoluteError: 0.1110368743538857 | Loss: 0.0202684994322044 | Epoch: 453 | \n\n\nMeanAbsoluteError: 0.1076720282435417 | Loss: 0.0194474371172206 | Epoch: 454 | \n\n\nMeanAbsoluteError: 0.1085707247257233 | Loss: 0.0193632638013029 | Epoch: 455 | \n\n\nMeanAbsoluteError: 0.1119993180036545 | Loss: 0.0197665508571663 | Epoch: 456 | \n\n\nMeanAbsoluteError: 0.1078148558735847 | Loss: 0.0198078485786876 | Epoch: 457 | \n\n\nMeanAbsoluteError: 0.1077051907777786 | Loss: 0.0188418281148491 | Epoch: 458 | \n\n\nMeanAbsoluteError: 0.1088232323527336 | Loss: 0.0187828998936069 | Epoch: 459 | \n\n\nMeanAbsoluteError: 0.1088168844580650 | Loss: 0.0196887009260020 | Epoch: 460 | \n\n\nMeanAbsoluteError: 0.1073557287454605 | Loss: 0.0190714586085960 | Epoch: 461 | \n\n\nMeanAbsoluteError: 0.1092256829142570 | Loss: 0.0198141352272887 | Epoch: 462 | \n\n\nMeanAbsoluteError: 0.1099240854382515 | Loss: 0.0186665884500932 | Epoch: 463 | \n\n\nMeanAbsoluteError: 0.1136161014437675 | Loss: 0.0214421719939007 | Epoch: 464 | \n\n\nMeanAbsoluteError: 0.1088020801544189 | Loss: 0.0193513300618603 | Epoch: 465 | \n\n\nMeanAbsoluteError: 0.1101175919175148 | Loss: 0.0188832387547397 | Epoch: 466 | \n\n\nMeanAbsoluteError: 0.1157854050397873 | Loss: 0.0209773484472438 | Epoch: 467 | \n\n\nMeanAbsoluteError: 0.1083779036998749 | Loss: 0.0192595252413109 | Epoch: 468 | \n\n\nMeanAbsoluteError: 0.1117968261241913 | Loss: 0.0196791176327194 | Epoch: 469 | \n\n\nMeanAbsoluteError: 0.1069977283477783 | Loss: 0.0178869550989475 | Epoch: 470 | \n\n\nMeanAbsoluteError: 0.1114884838461876 | Loss: 0.0197425666337707 | Epoch: 471 | \n\n\nMeanAbsoluteError: 0.1074325665831566 | Loss: 0.0184646821472537 | Epoch: 472 | \n\n\nMeanAbsoluteError: 0.1089370101690292 | Loss: 0.0190090847151684 | Epoch: 473 | \n\n\nMeanAbsoluteError: 0.1038711071014404 | Loss: 0.0183318219510450 | Epoch: 474 | \n\n\nMeanAbsoluteError: 0.1095535531640053 | Loss: 0.0198067576886145 | Epoch: 475 | \n\n\nMeanAbsoluteError: 0.1044411137700081 | Loss: 0.0174304261065360 | Epoch: 476 | \n\n\nMeanAbsoluteError: 0.1055609583854675 | Loss: 0.0175141988883115 | Epoch: 477 | \n\n\nMeanAbsoluteError: 0.1080038473010063 | Loss: 0.0188586116144870 | Epoch: 478 | \n\n\nMeanAbsoluteError: 0.1137972772121429 | Loss: 0.0210723636394202 | Epoch: 479 | \n\n\nMeanAbsoluteError: 0.1094622313976288 | Loss: 0.0190880379493422 | Epoch: 480 | \n\n\nMeanAbsoluteError: 0.1076379939913750 | Loss: 0.0186179803912334 | Epoch: 481 | \n\n\nMeanAbsoluteError: 0.1077275201678276 | Loss: 0.0187832832897523 | Epoch: 482 | \n\n\nMeanAbsoluteError: 0.1037254631519318 | Loss: 0.0180000595395298 | Epoch: 483 | \n\n\nMeanAbsoluteError: 0.1060369014739990 | Loss: 0.0175557094983439 | Epoch: 484 | \n\n\nMeanAbsoluteError: 0.1062298491597176 | Loss: 0.0189767235146671 | Epoch: 485 | \n\n\nMeanAbsoluteError: 0.0947444885969162 | Loss: 0.0154709707254369 | Epoch: 486 | \n\n\nMeanAbsoluteError: 0.1057794094085693 | Loss: 0.0177173632144695 | Epoch: 487 | \n\n\nMeanAbsoluteError: 0.1046545878052711 | Loss: 0.0178889535735167 | Epoch: 488 | \n\n\nMeanAbsoluteError: 0.1099312752485275 | Loss: 0.0194071084388997 | Epoch: 489 | \n\n\nMeanAbsoluteError: 0.1025017425417900 | Loss: 0.0178505614860236 | Epoch: 490 | \n\n\nMeanAbsoluteError: 0.1092862933874130 | Loss: 0.0193082285848989 | Epoch: 491 | \n\n\nMeanAbsoluteError: 0.1080968379974365 | Loss: 0.0183372163528111 | Epoch: 492 | \n\n\nMeanAbsoluteError: 0.1017381101846695 | Loss: 0.0167504107874023 | Epoch: 493 | \n\n\nMeanAbsoluteError: 0.1042402684688568 | Loss: 0.0174840147737996 | Epoch: 494 | \n\n\nMeanAbsoluteError: 0.1043858751654625 | Loss: 0.0184208144734293 | Epoch: 495 | \n\n\nMeanAbsoluteError: 0.1062314882874489 | Loss: 0.0179238509391629 | Epoch: 496 | \n\n\nMeanAbsoluteError: 0.1091011911630630 | Loss: 0.0194128277201222 | Epoch: 497 | \n\n\nMeanAbsoluteError: 0.1104706823825836 | Loss: 0.0201215961499838 | Epoch: 498 | \n\n\nMeanAbsoluteError: 0.1001475229859352 | Loss: 0.0165334040873374 | Epoch: 499 | \n\n\nMeanAbsoluteError: 0.1071850359439850 | Loss: 0.0184573122509755 | Epoch: 500 | \n\n\nMeanAbsoluteError: 0.1079270318150520 | Loss: 0.0191663156198289 | Epoch: 501 | \n\n\nMeanAbsoluteError: 0.1032684668898582 | Loss: 0.0187757803884718 | Epoch: 502 | \n\n\nMeanAbsoluteError: 0.1106522604823112 | Loss: 0.0186514720858152 | Epoch: 503 | \n\n\nMeanAbsoluteError: 0.1056912466883659 | Loss: 0.0184784351492999 | Epoch: 504 | \n\n\nMeanAbsoluteError: 0.1123366579413414 | Loss: 0.0194886658246105 | Epoch: 505 | \n\n\nMeanAbsoluteError: 0.1020871549844742 | Loss: 0.0171283518666557 | Epoch: 506 | \n\n\nMeanAbsoluteError: 0.1099625900387764 | Loss: 0.0193053018746771 | Epoch: 507 | \n\n\nMeanAbsoluteError: 0.1057959049940109 | Loss: 0.0184205576314707 | Epoch: 508 | \n\n\nMeanAbsoluteError: 0.1038676947355270 | Loss: 0.0175468089199179 | Epoch: 509 | \n\n\nMeanAbsoluteError: 0.1036882251501083 | Loss: 0.0173189685720718 | Epoch: 510 | \n\n\nMeanAbsoluteError: 0.1058391407132149 | Loss: 0.0179719612869667 | Epoch: 511 | \n\n\nMeanAbsoluteError: 0.1026219874620438 | Loss: 0.0170506559462228 | Epoch: 512 | \n\n\nMeanAbsoluteError: 0.0984667241573334 | Loss: 0.0157294110538593 | Epoch: 513 | \n\n\nMeanAbsoluteError: 0.0963794961571693 | Loss: 0.0151451077467451 | Epoch: 514 | \n\n\nMeanAbsoluteError: 0.1058434844017029 | Loss: 0.0187967700057197 | Epoch: 515 | \n\n\nMeanAbsoluteError: 0.1053993999958038 | Loss: 0.0184244545627250 | Epoch: 516 | \n\n\nMeanAbsoluteError: 0.1047592908143997 | Loss: 0.0183238756390832 | Epoch: 517 | \n\n\nMeanAbsoluteError: 0.1045590490102768 | Loss: 0.0179726985700351 | Epoch: 518 | \n\n\nMeanAbsoluteError: 0.1043996736407280 | Loss: 0.0173806734641160 | Epoch: 519 | \n\n\nMeanAbsoluteError: 0.1018174365162849 | Loss: 0.0168100096089620 | Epoch: 520 | \n\n\nMeanAbsoluteError: 0.1036037281155586 | Loss: 0.0177736185428027 | Epoch: 521 | \n\n\nMeanAbsoluteError: 0.1098827347159386 | Loss: 0.0189843560596152 | Epoch: 522 | \n\n\nMeanAbsoluteError: 0.1076234579086304 | Loss: 0.0191162837922942 | Epoch: 523 | \n\n\nMeanAbsoluteError: 0.1121683791279793 | Loss: 0.0199142908409218 | Epoch: 524 | \n\n\nMeanAbsoluteError: 0.0990705788135529 | Loss: 0.0159909857563131 | Epoch: 525 | \n\n\nMeanAbsoluteError: 0.1011214256286621 | Loss: 0.0170957497146931 | Epoch: 526 | \n\n\nMeanAbsoluteError: 0.1057583689689636 | Loss: 0.0182843252634242 | Epoch: 527 | \n\n\nMeanAbsoluteError: 0.1026385352015495 | Loss: 0.0177298954515451 | Epoch: 528 | \n\n\nMeanAbsoluteError: 0.1036797836422920 | Loss: 0.0176685270298033 | Epoch: 529 | \n\n\nMeanAbsoluteError: 0.1021144613623619 | Loss: 0.0169967648339419 | Epoch: 530 | \n\n\nMeanAbsoluteError: 0.1005478128790855 | Loss: 0.0162389042888147 | Epoch: 531 | \n\n\nMeanAbsoluteError: 0.1071672216057777 | Loss: 0.0184063646741561 | Epoch: 532 | \n\n\nMeanAbsoluteError: 0.0992245450615883 | Loss: 0.0166699003398359 | Epoch: 533 | \n\n\nMeanAbsoluteError: 0.1025934293866158 | Loss: 0.0174387250799919 | Epoch: 534 | \n\n\nMeanAbsoluteError: 0.1019488051533699 | Loss: 0.0169387980240693 | Epoch: 535 | \n\n\nMeanAbsoluteError: 0.1093226745724678 | Loss: 0.0191453485909733 | Epoch: 536 | \n\n\nMeanAbsoluteError: 0.0991153717041016 | Loss: 0.0170649848805806 | Epoch: 537 | \n\n\nMeanAbsoluteError: 0.1036320626735687 | Loss: 0.0174394600429999 | Epoch: 538 | \n\n\nMeanAbsoluteError: 0.1045218408107758 | Loss: 0.0170834861121451 | Epoch: 539 | \n\n\nMeanAbsoluteError: 0.1059826016426086 | Loss: 0.0183736312145144 | Epoch: 540 | \n\n\nMeanAbsoluteError: 0.1053560972213745 | Loss: 0.0182977810488471 | Epoch: 541 | \n\n\nMeanAbsoluteError: 0.1024782434105873 | Loss: 0.0167820183684429 | Epoch: 542 | \n\n\nMeanAbsoluteError: 0.1007051765918732 | Loss: 0.0171532136343497 | Epoch: 543 | \n\n\nMeanAbsoluteError: 0.0989533588290215 | Loss: 0.0157962201863605 | Epoch: 544 | \n\n\nMeanAbsoluteError: 0.0977284088730812 | Loss: 0.0159198719449826 | Epoch: 545 | \n\n\nMeanAbsoluteError: 0.1046197041869164 | Loss: 0.0174507487334874 | Epoch: 546 | \n\n\nMeanAbsoluteError: 0.1063097417354584 | Loss: 0.0189787255899622 | Epoch: 547 | \n\n\nMeanAbsoluteError: 0.1037757769227028 | Loss: 0.0174586184900545 | Epoch: 548 | \n\n\nMeanAbsoluteError: 0.1009741052985191 | Loss: 0.0169355445164062 | Epoch: 549 | \n\n\nMeanAbsoluteError: 0.1004598587751389 | Loss: 0.0171607647017906 | Epoch: 550 | \n\n\nMeanAbsoluteError: 0.1049593761563301 | Loss: 0.0178271236491855 | Epoch: 551 | \n\n\nMeanAbsoluteError: 0.0968673676252365 | Loss: 0.0153622336272504 | Epoch: 552 | \n\n\nMeanAbsoluteError: 0.1053495854139328 | Loss: 0.0177080371673219 | Epoch: 553 | \n\n\nMeanAbsoluteError: 0.0917631685733795 | Loss: 0.0139624662077586 | Epoch: 554 | \n\n\nMeanAbsoluteError: 0.1059290990233421 | Loss: 0.0185206676373006 | Epoch: 555 | \n\n\nMeanAbsoluteError: 0.1090244650840759 | Loss: 0.0186611500026023 | Epoch: 556 | \n\n\nMeanAbsoluteError: 0.0989286005496979 | Loss: 0.0155607260502196 | Epoch: 557 | \n\n\nMeanAbsoluteError: 0.1016364172101021 | Loss: 0.0166696473531192 | Epoch: 558 | \n\n\nMeanAbsoluteError: 0.0991171523928642 | Loss: 0.0155860804372302 | Epoch: 559 | \n\n\nMeanAbsoluteError: 0.1085531637072563 | Loss: 0.0193293628847459 | Epoch: 560 | \n\n\nMeanAbsoluteError: 0.1050070002675056 | Loss: 0.0173055122577352 | Epoch: 561 | \n\n\nMeanAbsoluteError: 0.1049596816301346 | Loss: 0.0174974759949449 | Epoch: 562 | \n\n\nMeanAbsoluteError: 0.0976474806666374 | Loss: 0.0154829299928891 | Epoch: 563 | \n\n\nMeanAbsoluteError: 0.1009484529495239 | Loss: 0.0168563839448325 | Epoch: 564 | \n\n\nMeanAbsoluteError: 0.1100180149078369 | Loss: 0.0191995749878697 | Epoch: 565 | \n\n\nMeanAbsoluteError: 0.1036848500370979 | Loss: 0.0169461284847421 | Epoch: 566 | \n\n\nMeanAbsoluteError: 0.1063543483614922 | Loss: 0.0181447704850537 | Epoch: 567 | \n\n\nMeanAbsoluteError: 0.0980910137295723 | Loss: 0.0156319987219952 | Epoch: 568 | \n\n\nMeanAbsoluteError: 0.1005250439047813 | Loss: 0.0168173396761510 | Epoch: 569 | \n\n\nMeanAbsoluteError: 0.0973320603370667 | Loss: 0.0152427606395455 | Epoch: 570 | \n\n\nMeanAbsoluteError: 0.1013324186205864 | Loss: 0.0166899567916334 | Epoch: 571 | \n\n\nMeanAbsoluteError: 0.1006063967943192 | Loss: 0.0154312570751063 | Epoch: 572 | \n\n\nMeanAbsoluteError: 0.0968297943472862 | Loss: 0.0156542769152050 | Epoch: 573 | \n\n\nMeanAbsoluteError: 0.1000195145606995 | Loss: 0.0164880910557501 | Epoch: 574 | \n\n\nMeanAbsoluteError: 0.0987437367439270 | Loss: 0.0155341785521402 | Epoch: 575 | \n\n\nMeanAbsoluteError: 0.0950973257422447 | Loss: 0.0156331943875800 | Epoch: 576 | \n\n\nMeanAbsoluteError: 0.0990119948983192 | Loss: 0.0163470193679192 | Epoch: 577 | \n\n\nMeanAbsoluteError: 0.0999817624688148 | Loss: 0.0165230337281294 | Epoch: 578 | \n\n\nMeanAbsoluteError: 0.0985621809959412 | Loss: 0.0154159512619420 | Epoch: 579 | \n\n\nMeanAbsoluteError: 0.1059064865112305 | Loss: 0.0179229909184505 | Epoch: 580 | \n\n\nMeanAbsoluteError: 0.1018125638365746 | Loss: 0.0177243646977756 | Epoch: 581 | \n\n\nMeanAbsoluteError: 0.1045570895075798 | Loss: 0.0184059294830270 | Epoch: 582 | \n\n\nMeanAbsoluteError: 0.0917261242866516 | Loss: 0.0148760795785347 | Epoch: 583 | \n\n\nMeanAbsoluteError: 0.1014644131064415 | Loss: 0.0167620740744496 | Epoch: 584 | \n\n\nMeanAbsoluteError: 0.1041641831398010 | Loss: 0.0171324321910894 | Epoch: 585 | \n\n\nMeanAbsoluteError: 0.1032962575554848 | Loss: 0.0167932148542241 | Epoch: 586 | \n\n\nMeanAbsoluteError: 0.1003072485327721 | Loss: 0.0170567643045797 | Epoch: 587 | \n\n\nMeanAbsoluteError: 0.0960631743073463 | Loss: 0.0155754161516961 | Epoch: 588 | \n\n\nMeanAbsoluteError: 0.1030910909175873 | Loss: 0.0174660051330769 | Epoch: 589 | \n\n\nMeanAbsoluteError: 0.0999056920409203 | Loss: 0.0162576378193141 | Epoch: 590 | \n\n\nMeanAbsoluteError: 0.1046823188662529 | Loss: 0.0182745897204344 | Epoch: 591 | \n\n\nMeanAbsoluteError: 0.0961284339427948 | Loss: 0.0153532146379591 | Epoch: 592 | \n\n\nMeanAbsoluteError: 0.1002001464366913 | Loss: 0.0162265629448423 | Epoch: 593 | \n\n\nMeanAbsoluteError: 0.1027173176407814 | Loss: 0.0160838911562072 | Epoch: 594 | \n\n\nMeanAbsoluteError: 0.0971160084009171 | Loss: 0.0159319897391723 | Epoch: 595 | \n\n\nMeanAbsoluteError: 0.0993569046258926 | Loss: 0.0165263822780859 | Epoch: 596 | \n\n\nMeanAbsoluteError: 0.0993890762329102 | Loss: 0.0167159886836453 | Epoch: 597 | \n\n\nMeanAbsoluteError: 0.1018065735697746 | Loss: 0.0174810209735006 | Epoch: 598 | \n\n\nMeanAbsoluteError: 0.0989343672990799 | Loss: 0.0154981983806162 | Epoch: 599 | \n\n\nMeanAbsoluteError: 0.0970872566103935 | Loss: 0.0156598225844209 | Epoch: 600 | \n\n\nMeanAbsoluteError: 0.0985487774014473 | Loss: 0.0164378613822919 | Epoch: 601 | \n\n\nMeanAbsoluteError: 0.1021191328763962 | Loss: 0.0166418106949520 | Epoch: 602 | \n\n\nMeanAbsoluteError: 0.0942861735820770 | Loss: 0.0146297039077278 | Epoch: 603 | \n\n\nMeanAbsoluteError: 0.0999025627970695 | Loss: 0.0162536532582211 | Epoch: 604 | \n\n\nMeanAbsoluteError: 0.1018275991082191 | Loss: 0.0162397063848524 | Epoch: 605 | \n\n\nMeanAbsoluteError: 0.0986867547035217 | Loss: 0.0161505515468889 | Epoch: 606 | \n\n\nMeanAbsoluteError: 0.1018792837858200 | Loss: 0.0172084139857907 | Epoch: 607 | \n\n\nMeanAbsoluteError: 0.0978070944547653 | Loss: 0.0159062439377400 | Epoch: 608 | \n\n\nMeanAbsoluteError: 0.1040997356176376 | Loss: 0.0172362576164596 | Epoch: 609 | \n\n\nMeanAbsoluteError: 0.0991204604506493 | Loss: 0.0165179419725124 | Epoch: 610 | \n\n\nMeanAbsoluteError: 0.0972090438008308 | Loss: 0.0152519755832085 | Epoch: 611 | \n\n\nMeanAbsoluteError: 0.0932131633162498 | Loss: 0.0144609173606053 | Epoch: 612 | \n\n\nMeanAbsoluteError: 0.0971724614500999 | Loss: 0.0156310006904217 | Epoch: 613 | \n\n\nMeanAbsoluteError: 0.0951151177287102 | Loss: 0.0151300622258471 | Epoch: 614 | \n\n\nMeanAbsoluteError: 0.0976530909538269 | Loss: 0.0155173187237718 | Epoch: 615 | \n\n\nMeanAbsoluteError: 0.0860147103667259 | Loss: 0.0129059647457325 | Epoch: 616 | \n\n\nMeanAbsoluteError: 0.0983313545584679 | Loss: 0.0156831854104288 | Epoch: 617 | \n\n\nMeanAbsoluteError: 0.0948770418763161 | Loss: 0.0150985292023203 | Epoch: 618 | \n\n\nMeanAbsoluteError: 0.0983140245079994 | Loss: 0.0160234434593319 | Epoch: 619 | \n\n\nMeanAbsoluteError: 0.0963659957051277 | Loss: 0.0146887228733976 | Epoch: 620 | \n\n\nMeanAbsoluteError: 0.0926956161856651 | Loss: 0.0143941404977280 | Epoch: 621 | \n\n\nMeanAbsoluteError: 0.0968189239501953 | Loss: 0.0152737065647913 | Epoch: 622 | \n\n\nMeanAbsoluteError: 0.0959322005510330 | Loss: 0.0154287242990298 | Epoch: 623 | \n\n\nMeanAbsoluteError: 0.0945542380213737 | Loss: 0.0153316612476677 | Epoch: 624 | \n\n\nMeanAbsoluteError: 0.0974727869033813 | Loss: 0.0156631404762932 | Epoch: 625 | \n\n\nMeanAbsoluteError: 0.1013954430818558 | Loss: 0.0167194210136465 | Epoch: 626 | \n\n\nMeanAbsoluteError: 0.0966712534427643 | Loss: 0.0156041120393396 | Epoch: 627 | \n\n\nMeanAbsoluteError: 0.0993990674614906 | Loss: 0.0158690734113528 | Epoch: 628 | \n\n\nMeanAbsoluteError: 0.0950199142098427 | Loss: 0.0147170811857601 | Epoch: 629 | \n\n\nMeanAbsoluteError: 0.0955079495906830 | Loss: 0.0151732299436238 | Epoch: 630 | \n\n\nMeanAbsoluteError: 0.0933324471116066 | Loss: 0.0150091121890970 | Epoch: 631 | \n\n\nMeanAbsoluteError: 0.0980171635746956 | Loss: 0.0158048873169658 | Epoch: 632 | \n\n\nMeanAbsoluteError: 0.0986608713865280 | Loss: 0.0158562715338000 | Epoch: 633 | \n\n\nMeanAbsoluteError: 0.1023561507463455 | Loss: 0.0169264494172239 | Epoch: 634 | \n\n\nMeanAbsoluteError: 0.1003357768058777 | Loss: 0.0165384271242632 | Epoch: 635 | \n\n\nMeanAbsoluteError: 0.0929949209094048 | Loss: 0.0146257627385300 | Epoch: 636 | \n\n\nMeanAbsoluteError: 0.0951469838619232 | Loss: 0.0153302086798067 | Epoch: 637 | \n\n\nMeanAbsoluteError: 0.0890128165483475 | Loss: 0.0130082002596464 | Epoch: 638 | \n\n\nMeanAbsoluteError: 0.0988552942872047 | Loss: 0.0167403974087695 | Epoch: 639 | \n\n\nMeanAbsoluteError: 0.0951830148696899 | Loss: 0.0146781039407991 | Epoch: 640 | \n\n\nMeanAbsoluteError: 0.1001801714301109 | Loss: 0.0162062385391134 | Epoch: 641 | \n\n\nMeanAbsoluteError: 0.0958799123764038 | Loss: 0.0150759715742121 | Epoch: 642 | \n\n\nMeanAbsoluteError: 0.0975896045565605 | Loss: 0.0159086109469839 | Epoch: 643 | \n\n\nMeanAbsoluteError: 0.0920776799321175 | Loss: 0.0142660328320926 | Epoch: 644 | \n\n\nMeanAbsoluteError: 0.0981658995151520 | Loss: 0.0169585607743587 | Epoch: 645 | \n\n\nMeanAbsoluteError: 0.1010406389832497 | Loss: 0.0174651583224962 | Epoch: 646 | \n\n\nMeanAbsoluteError: 0.0962655618786812 | Loss: 0.0153625727149241 | Epoch: 647 | \n\n\nMeanAbsoluteError: 0.0975456908345222 | Loss: 0.0149634760402356 | Epoch: 648 | \n\n\nMeanAbsoluteError: 0.0904602855443954 | Loss: 0.0134210630924402 | Epoch: 649 | \n\n\nMeanAbsoluteError: 0.0973210483789444 | Loss: 0.0160565740411160 | Epoch: 650 | \n\n\nMeanAbsoluteError: 0.0899243727326393 | Loss: 0.0142011024843669 | Epoch: 651 | \n\n\nMeanAbsoluteError: 0.0978483334183693 | Loss: 0.0159782370398170 | Epoch: 652 | \n\n\nMeanAbsoluteError: 0.0943374484777451 | Loss: 0.0153088302161389 | Epoch: 653 | \n\n\nMeanAbsoluteError: 0.0935397446155548 | Loss: 0.0150333106934219 | Epoch: 654 | \n\n\nMeanAbsoluteError: 0.0926215872168541 | Loss: 0.0138501231903016 | Epoch: 655 | \n\n\nMeanAbsoluteError: 0.0978369936347008 | Loss: 0.0156206018945280 | Epoch: 656 | \n\n\nMeanAbsoluteError: 0.0951854735612869 | Loss: 0.0147040625739222 | Epoch: 657 | \n\n\nMeanAbsoluteError: 0.0949792340397835 | Loss: 0.0146754516385166 | Epoch: 658 | \n\n\nMeanAbsoluteError: 0.0941840782761574 | Loss: 0.0145256604592820 | Epoch: 659 | \n\n\nMeanAbsoluteError: 0.1007379963994026 | Loss: 0.0168417397700129 | Epoch: 660 | \n\n\nMeanAbsoluteError: 0.0952702686190605 | Loss: 0.0146381405797244 | Epoch: 661 | \n\n\nMeanAbsoluteError: 0.1013915464282036 | Loss: 0.0182209717477599 | Epoch: 662 | \n\n\nMeanAbsoluteError: 0.0916105583310127 | Loss: 0.0142263032533447 | Epoch: 663 | \n\n\nMeanAbsoluteError: 0.0958929732441902 | Loss: 0.0154816908481977 | Epoch: 664 | \n\n\nMeanAbsoluteError: 0.0891810655593872 | Loss: 0.0136653652940731 | Epoch: 665 | \n\n\nMeanAbsoluteError: 0.0975197330117226 | Loss: 0.0155981396961943 | Epoch: 666 | \n\n\nMeanAbsoluteError: 0.0988456085324287 | Loss: 0.0164214955700057 | Epoch: 667 | \n\n\nMeanAbsoluteError: 0.0921951085329056 | Loss: 0.0144256019248375 | Epoch: 668 | \n\n\nMeanAbsoluteError: 0.0928115248680115 | Loss: 0.0136755297589358 | Epoch: 669 | \n\n\nMeanAbsoluteError: 0.0983774363994598 | Loss: 0.0157521517603891 | Epoch: 670 | \n\n\nMeanAbsoluteError: 0.0951461642980576 | Loss: 0.0141835129390044 | Epoch: 671 | \n\n\nMeanAbsoluteError: 0.0910786762833595 | Loss: 0.0141854207198776 | Epoch: 672 | \n\n\nMeanAbsoluteError: 0.0877511054277420 | Loss: 0.0125908807836095 | Epoch: 673 | \n\n\nMeanAbsoluteError: 0.0877727493643761 | Loss: 0.0130835724467033 | Epoch: 674 | \n\n\nMeanAbsoluteError: 0.0924363359808922 | Loss: 0.0139344813472902 | Epoch: 675 | \n\n\nMeanAbsoluteError: 0.0993929803371429 | Loss: 0.0154380698225911 | Epoch: 676 | \n\n\nMeanAbsoluteError: 0.0967289581894875 | Loss: 0.0154198124254860 | Epoch: 677 | \n\n\nMeanAbsoluteError: 0.0950058251619339 | Loss: 0.0147981545101114 | Epoch: 678 | \n\n\nMeanAbsoluteError: 0.0881096050143242 | Loss: 0.0135371439086642 | Epoch: 679 | \n\n\nMeanAbsoluteError: 0.0907551720738411 | Loss: 0.0133918949023549 | Epoch: 680 | \n\n\nMeanAbsoluteError: 0.0928677394986153 | Loss: 0.0147567740010224 | Epoch: 681 | \n\n\nMeanAbsoluteError: 0.0898245498538017 | Loss: 0.0133310988837547 | Epoch: 682 | \n\n\nMeanAbsoluteError: 0.0911816731095314 | Loss: 0.0139795007380568 | Epoch: 683 | \n\n\nMeanAbsoluteError: 0.0908115506172180 | Loss: 0.0133981075510383 | Epoch: 684 | \n\n\nMeanAbsoluteError: 0.0872737467288971 | Loss: 0.0127124052596628 | Epoch: 685 | \n\n\nMeanAbsoluteError: 0.0887402072548866 | Loss: 0.0134497750012088 | Epoch: 686 | \n\n\nMeanAbsoluteError: 0.0900749713182449 | Loss: 0.0139834810303243 | Epoch: 687 | \n\n\nMeanAbsoluteError: 0.0928239598870277 | Loss: 0.0142104500128335 | Epoch: 688 | \n\n\nMeanAbsoluteError: 0.0918295085430145 | Loss: 0.0138829600310176 | Epoch: 689 | \n\n\nMeanAbsoluteError: 0.0915411338210106 | Loss: 0.0141499493661953 | Epoch: 690 | \n\n\nMeanAbsoluteError: 0.0922488495707512 | Loss: 0.0152968317256940 | Epoch: 691 | \n\n\nMeanAbsoluteError: 0.0902643501758575 | Loss: 0.0135808024069653 | Epoch: 692 | \n\n\nMeanAbsoluteError: 0.0948316305875778 | Loss: 0.0145704091138517 | Epoch: 693 | \n\n\nMeanAbsoluteError: 0.1016058921813965 | Loss: 0.0176177753208261 | Epoch: 694 | \n\n\nMeanAbsoluteError: 0.0910881534218788 | Loss: 0.0144480604108685 | Epoch: 695 | \n\n\nMeanAbsoluteError: 0.0925195217132568 | Loss: 0.0142469276231714 | Epoch: 696 | \n\n\nMeanAbsoluteError: 0.0957957878708839 | Loss: 0.0154571371636121 | Epoch: 697 | \n\n\nMeanAbsoluteError: 0.0876320600509644 | Loss: 0.0131819519627606 | Epoch: 698 | \n\n\nMeanAbsoluteError: 0.0937943384051323 | Loss: 0.0143552000865020 | Epoch: 699 | \n\n\nMeanAbsoluteError: 0.0980210974812508 | Loss: 0.0154769351973664 | Epoch: 700 | \n\n\nMeanAbsoluteError: 0.0907896235585213 | Loss: 0.0136432989284125 | Epoch: 701 | \n\n\nMeanAbsoluteError: 0.0876436978578568 | Loss: 0.0135800638429888 | Epoch: 702 | \n\n\nMeanAbsoluteError: 0.0892059728503227 | Loss: 0.0133131819405389 | Epoch: 703 | \n\n\nMeanAbsoluteError: 0.0915817990899086 | Loss: 0.0138908508270348 | Epoch: 704 | \n\n\nMeanAbsoluteError: 0.0852026641368866 | Loss: 0.0123656088467254 | Epoch: 705 | \n\n\nMeanAbsoluteError: 0.0962381735444069 | Loss: 0.0156611849483549 | Epoch: 706 | \n\n\nMeanAbsoluteError: 0.0963429883122444 | Loss: 0.0167211482290198 | Epoch: 707 | \n\n\nMeanAbsoluteError: 0.0923006534576416 | Loss: 0.0144891551710801 | Epoch: 708 | \n\n\nMeanAbsoluteError: 0.0932165309786797 | Loss: 0.0147450670267184 | Epoch: 709 | \n\n\nMeanAbsoluteError: 0.0896697416901588 | Loss: 0.0134632237087559 | Epoch: 710 | \n\n\nMeanAbsoluteError: 0.0873664841055870 | Loss: 0.0124458964622196 | Epoch: 711 | \n\n\nMeanAbsoluteError: 0.0873264223337173 | Loss: 0.0130664107805205 | Epoch: 712 | \n\n\nMeanAbsoluteError: 0.1008769571781158 | Loss: 0.0167601397414304 | Epoch: 713 | \n\n\nMeanAbsoluteError: 0.0870294645428658 | Loss: 0.0132183647956299 | Epoch: 714 | \n\n\nMeanAbsoluteError: 0.0901519581675529 | Loss: 0.0136273481553750 | Epoch: 715 | \n\n\nMeanAbsoluteError: 0.0903048887848854 | Loss: 0.0138714655912675 | Epoch: 716 | \n\n\nMeanAbsoluteError: 0.0869337916374207 | Loss: 0.0126860929916438 | Epoch: 717 | \n\n\nMeanAbsoluteError: 0.0958177074790001 | Loss: 0.0150282188468797 | Epoch: 718 | \n\n\nMeanAbsoluteError: 0.0917349755764008 | Loss: 0.0151962997783994 | Epoch: 719 | \n\n\nMeanAbsoluteError: 0.0913892462849617 | Loss: 0.0135784395210915 | Epoch: 720 | \n\n\nMeanAbsoluteError: 0.0873709693551064 | Loss: 0.0128777966832664 | Epoch: 721 | \n\n\nMeanAbsoluteError: 0.0851389467716217 | Loss: 0.0132436603446937 | Epoch: 722 | \n\n\nMeanAbsoluteError: 0.0859052240848541 | Loss: 0.0125464308589532 | Epoch: 723 | \n\n\nMeanAbsoluteError: 0.0911987274885178 | Loss: 0.0136043035128387 | Epoch: 724 | \n\n\nMeanAbsoluteError: 0.0913837999105453 | Loss: 0.0140152149878365 | Epoch: 725 | \n\n\nMeanAbsoluteError: 0.0878583565354347 | Loss: 0.0136757735416177 | Epoch: 726 | \n\n\nMeanAbsoluteError: 0.0901663154363632 | Loss: 0.0131215452230147 | Epoch: 727 | \n\n\nMeanAbsoluteError: 0.0910472720861435 | Loss: 0.0141139147608919 | Epoch: 728 | \n\n\nMeanAbsoluteError: 0.0944709107279778 | Loss: 0.0150470846747097 | Epoch: 729 | \n\n\nMeanAbsoluteError: 0.0911858975887299 | Loss: 0.0145232518286017 | Epoch: 730 | \n\n\nMeanAbsoluteError: 0.0914022400975227 | Loss: 0.0137268464091206 | Epoch: 731 | \n\n\nMeanAbsoluteError: 0.0924823433160782 | Loss: 0.0142518726281742 | Epoch: 732 | \n\n\nMeanAbsoluteError: 0.0890439748764038 | Loss: 0.0135950199144168 | Epoch: 733 | \n\n\nMeanAbsoluteError: 0.0843726098537445 | Loss: 0.0123720152242004 | Epoch: 734 | \n\n\nMeanAbsoluteError: 0.0934913530945778 | Loss: 0.0148951505515777 | Epoch: 735 | \n\n\nMeanAbsoluteError: 0.0927467569708824 | Loss: 0.0141562378144590 | Epoch: 736 | \n\n\nMeanAbsoluteError: 0.0858572572469711 | Loss: 0.0132494518778170 | Epoch: 737 | \n\n\nMeanAbsoluteError: 0.0889513567090034 | Loss: 0.0137819672523377 | Epoch: 738 | \n\n\nMeanAbsoluteError: 0.0906553193926811 | Loss: 0.0137185524417631 | Epoch: 739 | \n\n\nMeanAbsoluteError: 0.0904521718621254 | Loss: 0.0143335427093795 | Epoch: 740 | \n\n\nMeanAbsoluteError: 0.0916708409786224 | Loss: 0.0135537809667100 | Epoch: 741 | \n\n\nMeanAbsoluteError: 0.0889285877346992 | Loss: 0.0138559782788919 | Epoch: 742 | \n\n\nMeanAbsoluteError: 0.0884241983294487 | Loss: 0.0129560982324377 | Epoch: 743 | \n\n\nMeanAbsoluteError: 0.0908810943365097 | Loss: 0.0145461533048001 | Epoch: 744 | \n\n\nMeanAbsoluteError: 0.0899408459663391 | Loss: 0.0139002339481764 | Epoch: 745 | \n\n\nMeanAbsoluteError: 0.0934693142771721 | Loss: 0.0148635784396417 | Epoch: 746 | \n\n\nMeanAbsoluteError: 0.0849223956465721 | Loss: 0.0120262817300196 | Epoch: 747 | \n\n\nMeanAbsoluteError: 0.0879457965493202 | Loss: 0.0132208535500589 | Epoch: 748 | \n\n\nMeanAbsoluteError: 0.0933552831411362 | Loss: 0.0143108899091991 | Epoch: 749 | \n\n\nMeanAbsoluteError: 0.0854841023683548 | Loss: 0.0124943667383195 | Epoch: 750 | \n\n\nMeanAbsoluteError: 0.0909569561481476 | Loss: 0.0136782560273423 | Epoch: 751 | \n\n\nMeanAbsoluteError: 0.0900074616074562 | Loss: 0.0137236923938811 | Epoch: 752 | \n\n\nMeanAbsoluteError: 0.0872161015868187 | Loss: 0.0124604850373483 | Epoch: 753 | \n\n\nMeanAbsoluteError: 0.0917188152670860 | Loss: 0.0139009584828045 | Epoch: 754 | \n\n\nMeanAbsoluteError: 0.0782198831439018 | Loss: 0.0104874390899992 | Epoch: 755 | \n\n\nMeanAbsoluteError: 0.0909943431615829 | Loss: 0.0141973154893155 | Epoch: 756 | \n\n\nMeanAbsoluteError: 0.0908592864871025 | Loss: 0.0139784193825714 | Epoch: 757 | \n\n\nMeanAbsoluteError: 0.0839707031846046 | Loss: 0.0117290160934984 | Epoch: 758 | \n\n\nMeanAbsoluteError: 0.0893910601735115 | Loss: 0.0131232935178559 | Epoch: 759 | \n\n\nMeanAbsoluteError: 0.0886907204985619 | Loss: 0.0134898246209680 | Epoch: 760 | \n\n\nMeanAbsoluteError: 0.0925139263272285 | Loss: 0.0145252272133075 | Epoch: 761 | \n\n\nMeanAbsoluteError: 0.0867981985211372 | Loss: 0.0128469172129129 | Epoch: 762 | \n\n\nMeanAbsoluteError: 0.0833914279937744 | Loss: 0.0115191780018601 | Epoch: 763 | \n\n\nMeanAbsoluteError: 0.0842401385307312 | Loss: 0.0121186521552833 | Epoch: 764 | \n\n\nMeanAbsoluteError: 0.0900855734944344 | Loss: 0.0140558015684170 | Epoch: 765 | \n\n\nMeanAbsoluteError: 0.0875684022903442 | Loss: 0.0127358317760809 | Epoch: 766 | \n\n\nMeanAbsoluteError: 0.0829081982374191 | Loss: 0.0122422089551098 | Epoch: 767 | \n\n\nMeanAbsoluteError: 0.0867778956890106 | Loss: 0.0126872996595618 | Epoch: 768 | \n\n\nMeanAbsoluteError: 0.0871879607439041 | Loss: 0.0128659972355429 | Epoch: 769 | \n\n\nMeanAbsoluteError: 0.0903099626302719 | Loss: 0.0141418682656270 | Epoch: 770 | \n\n\nMeanAbsoluteError: 0.0843642950057983 | Loss: 0.0125597887736634 | Epoch: 771 | \n\n\nMeanAbsoluteError: 0.0872574225068092 | Loss: 0.0128508980553791 | Epoch: 772 | \n\n\nMeanAbsoluteError: 0.0896963179111481 | Loss: 0.0140545649644021 | Epoch: 773 | \n\n\nMeanAbsoluteError: 0.0850620046257973 | Loss: 0.0122833400909197 | Epoch: 774 | \n\n\nMeanAbsoluteError: 0.0843407213687897 | Loss: 0.0128838659569374 | Epoch: 775 | \n\n\nMeanAbsoluteError: 0.0886899754405022 | Loss: 0.0129380962790068 | Epoch: 776 | \n\n\nMeanAbsoluteError: 0.0931899547576904 | Loss: 0.0147731562312523 | Epoch: 777 | \n\n\nMeanAbsoluteError: 0.0888117477297783 | Loss: 0.0131064142861093 | Epoch: 778 | \n\n\nMeanAbsoluteError: 0.0836424231529236 | Loss: 0.0121774772709371 | Epoch: 779 | \n\n\nMeanAbsoluteError: 0.0962003543972969 | Loss: 0.0149663350119711 | Epoch: 780 | \n\n\nMeanAbsoluteError: 0.0887271165847778 | Loss: 0.0139300864930010 | Epoch: 781 | \n\n\nMeanAbsoluteError: 0.0874563381075859 | Loss: 0.0129827766710393 | Epoch: 782 | \n\n\nMeanAbsoluteError: 0.0832744762301445 | Loss: 0.0118449339082993 | Epoch: 783 | \n\n\nMeanAbsoluteError: 0.0810452923178673 | Loss: 0.0116053797968925 | Epoch: 784 | \n\n\nMeanAbsoluteError: 0.0825993865728378 | Loss: 0.0118839594703725 | Epoch: 785 | \n\n\nMeanAbsoluteError: 0.0899229720234871 | Loss: 0.0129689053292774 | Epoch: 786 | \n\n\nMeanAbsoluteError: 0.0892427414655685 | Loss: 0.0134500695246364 | Epoch: 787 | \n\n\nMeanAbsoluteError: 0.0881848037242889 | Loss: 0.0128977680909884 | Epoch: 788 | \n\n\nMeanAbsoluteError: 0.0862503275275230 | Loss: 0.0128217596280350 | Epoch: 789 | \n\n\nMeanAbsoluteError: 0.0890542566776276 | Loss: 0.0137797965389655 | Epoch: 790 | \n\n\nMeanAbsoluteError: 0.0869007706642151 | Loss: 0.0131951511169488 | Epoch: 791 | \n\n\nMeanAbsoluteError: 0.0834550336003304 | Loss: 0.0121086389157669 | Epoch: 792 | \n\n\nMeanAbsoluteError: 0.0835298001766205 | Loss: 0.0123835518950364 | Epoch: 793 | \n\n\nMeanAbsoluteError: 0.0905648320913315 | Loss: 0.0149987138411476 | Epoch: 794 | \n\n\nMeanAbsoluteError: 0.0809204727411270 | Loss: 0.0117803016642332 | Epoch: 795 | \n\n\nMeanAbsoluteError: 0.0860572084784508 | Loss: 0.0122575138193740 | Epoch: 796 | \n\n\nMeanAbsoluteError: 0.0862538814544678 | Loss: 0.0123319207069774 | Epoch: 797 | \n\n\nMeanAbsoluteError: 0.0893970429897308 | Loss: 0.0132230902504913 | Epoch: 798 | \n\n\nMeanAbsoluteError: 0.0860835835337639 | Loss: 0.0125098691043483 | Epoch: 799 | \n\n\nMeanAbsoluteError: 0.0879913046956062 | Loss: 0.0129163359248196 | Epoch: 800 | \n\n\nMeanAbsoluteError: 0.0834209173917770 | Loss: 0.0117174673514758 | Epoch: 801 | \n\n\nMeanAbsoluteError: 0.0836713388562202 | Loss: 0.0117928251372602 | Epoch: 802 | \n\n\nMeanAbsoluteError: 0.0938349589705467 | Loss: 0.0149585416320285 | Epoch: 803 | \n\n\nMeanAbsoluteError: 0.0871034264564514 | Loss: 0.0125086384870034 | Epoch: 804 | \n\n\nMeanAbsoluteError: 0.0837868750095367 | Loss: 0.0117289127124059 | Epoch: 805 | \n\n\nMeanAbsoluteError: 0.0830954536795616 | Loss: 0.0116166988515761 | Epoch: 806 | \n\n\nMeanAbsoluteError: 0.0851906612515450 | Loss: 0.0124521796038607 | Epoch: 807 | \n\n\nMeanAbsoluteError: 0.0820395946502686 | Loss: 0.0112233843640994 | Epoch: 808 | \n\n\nMeanAbsoluteError: 0.0879077911376953 | Loss: 0.0131799748158179 | Epoch: 809 | \n\n\nMeanAbsoluteError: 0.0793781653046608 | Loss: 0.0103792258531515 | Epoch: 810 | \n\n\nMeanAbsoluteError: 0.0859363749623299 | Loss: 0.0124132350815732 | Epoch: 811 | \n\n\nMeanAbsoluteError: 0.0831531584262848 | Loss: 0.0113784314496055 | Epoch: 812 | \n\n\nMeanAbsoluteError: 0.0877959206700325 | Loss: 0.0134623827005104 | Epoch: 813 | \n\n\nMeanAbsoluteError: 0.0865776091814041 | Loss: 0.0122857846679229 | Epoch: 814 | \n\n\nMeanAbsoluteError: 0.0876188650727272 | Loss: 0.0132171660490227 | Epoch: 815 | \n\n\nMeanAbsoluteError: 0.0804865732789040 | Loss: 0.0113157687906945 | Epoch: 816 | \n\n\nMeanAbsoluteError: 0.0898168981075287 | Loss: 0.0132756126399424 | Epoch: 817 | \n\n\nMeanAbsoluteError: 0.0866966918110847 | Loss: 0.0133688613671014 | Epoch: 818 | \n\n\nMeanAbsoluteError: 0.0801234245300293 | Loss: 0.0105980817029679 | Epoch: 819 | \n\n\nMeanAbsoluteError: 0.0835135951638222 | Loss: 0.0120581849807058 | Epoch: 820 | \n\n\nMeanAbsoluteError: 0.0847514718770981 | Loss: 0.0119619197195910 | Epoch: 821 | \n\n\nMeanAbsoluteError: 0.0840147808194160 | Loss: 0.0115575103019364 | Epoch: 822 | \n\n\nMeanAbsoluteError: 0.0849616602063179 | Loss: 0.0126005454071371 | Epoch: 823 | \n\n\nMeanAbsoluteError: 0.0867443159222603 | Loss: 0.0127533768659972 | Epoch: 824 | \n\n\nMeanAbsoluteError: 0.0830225944519043 | Loss: 0.0115867110807267 | Epoch: 825 | \n\n\nMeanAbsoluteError: 0.0854062288999557 | Loss: 0.0119782135994562 | Epoch: 826 | \n\n\nMeanAbsoluteError: 0.0844532549381256 | Loss: 0.0128305454189103 | Epoch: 827 | \n\n\nMeanAbsoluteError: 0.0858768746256828 | Loss: 0.0122528880932199 | Epoch: 828 | \n\n\nMeanAbsoluteError: 0.0850727781653404 | Loss: 0.0125784363553066 | Epoch: 829 | \n\n\nMeanAbsoluteError: 0.0815022140741348 | Loss: 0.0117670193669619 | Epoch: 830 | \n\n\nMeanAbsoluteError: 0.0809046849608421 | Loss: 0.0118626279118325 | Epoch: 831 | \n\n\nMeanAbsoluteError: 0.0818876549601555 | Loss: 0.0118265629597590 | Epoch: 832 | \n\n\nMeanAbsoluteError: 0.0864217728376389 | Loss: 0.0126107688905662 | Epoch: 833 | \n\n\nMeanAbsoluteError: 0.0907039120793343 | Loss: 0.0144206715093363 | Epoch: 834 | \n\n\nMeanAbsoluteError: 0.0817326605319977 | Loss: 0.0117729200561007 | Epoch: 835 | \n\n\nMeanAbsoluteError: 0.0789137110114098 | Loss: 0.0105061601126605 | Epoch: 836 | \n\n\nMeanAbsoluteError: 0.0833445712924004 | Loss: 0.0122779125305897 | Epoch: 837 | \n\n\nMeanAbsoluteError: 0.0827598348259926 | Loss: 0.0123987089356524 | Epoch: 838 | \n\n\nMeanAbsoluteError: 0.0751952156424522 | Loss: 0.0102017939670971 | Epoch: 839 | \n\n\nMeanAbsoluteError: 0.0817220360040665 | Loss: 0.0118188605205311 | Epoch: 840 | \n\n\nMeanAbsoluteError: 0.0807937830686569 | Loss: 0.0111049522263420 | Epoch: 841 | \n\n\nMeanAbsoluteError: 0.0837605893611908 | Loss: 0.0114332473099057 | Epoch: 842 | \n\n\nMeanAbsoluteError: 0.0814781486988068 | Loss: 0.0121046569601458 | Epoch: 843 | \n\n\nMeanAbsoluteError: 0.0803124010562897 | Loss: 0.0108815456184190 | Epoch: 844 | \n\n\nMeanAbsoluteError: 0.0868163183331490 | Loss: 0.0134968111558798 | Epoch: 845 | \n\n\nMeanAbsoluteError: 0.0840883404016495 | Loss: 0.0121671526759746 | Epoch: 846 | \n\n\nMeanAbsoluteError: 0.0820056498050690 | Loss: 0.0115329726519121 | Epoch: 847 | \n\n\nMeanAbsoluteError: 0.0833782702684402 | Loss: 0.0129510531894145 | Epoch: 848 | \n\n\nMeanAbsoluteError: 0.0854192972183228 | Loss: 0.0124608524953517 | Epoch: 849 | \n\n\nMeanAbsoluteError: 0.0819972455501556 | Loss: 0.0117955128551542 | Epoch: 850 | \n\n\nMeanAbsoluteError: 0.0830818340182304 | Loss: 0.0125219980868375 | Epoch: 851 | \n\n\nMeanAbsoluteError: 0.0844301655888557 | Loss: 0.0129935751801531 | Epoch: 852 | \n\n\nMeanAbsoluteError: 0.0802783891558647 | Loss: 0.0115100423464052 | Epoch: 853 | \n\n\nMeanAbsoluteError: 0.0833505839109421 | Loss: 0.0118169031086533 | Epoch: 854 | \n\n\nMeanAbsoluteError: 0.0838959962129593 | Loss: 0.0126430459102577 | Epoch: 855 | \n\n\nMeanAbsoluteError: 0.0823426917195320 | Loss: 0.0110425230892724 | Epoch: 856 | \n\n\nMeanAbsoluteError: 0.0828281193971634 | Loss: 0.0121104329407293 | Epoch: 857 | \n\n\nMeanAbsoluteError: 0.0824762508273125 | Loss: 0.0113179754928084 | Epoch: 858 | \n\n\nMeanAbsoluteError: 0.0863152518868446 | Loss: 0.0125535581639269 | Epoch: 859 | \n\n\nMeanAbsoluteError: 0.0859963223338127 | Loss: 0.0122490289879109 | Epoch: 860 | \n\n\nMeanAbsoluteError: 0.0781435817480087 | Loss: 0.0118727505957456 | Epoch: 861 | \n\n\nMeanAbsoluteError: 0.0768708512187004 | Loss: 0.0106294993653864 | Epoch: 862 | \n\n\nMeanAbsoluteError: 0.0821595042943954 | Loss: 0.0113427623395304 | Epoch: 863 | \n\n\nMeanAbsoluteError: 0.0808522328734398 | Loss: 0.0116975125599401 | Epoch: 864 | \n\n\nMeanAbsoluteError: 0.0833480656147003 | Loss: 0.0119540756899611 | Epoch: 865 | \n\n\nMeanAbsoluteError: 0.0787297412753105 | Loss: 0.0109924657471735 | Epoch: 866 | \n\n\nMeanAbsoluteError: 0.0797687247395515 | Loss: 0.0108470366983965 | Epoch: 867 | \n\n\nMeanAbsoluteError: 0.0811495110392570 | Loss: 0.0111339508518480 | Epoch: 868 | \n\n\nMeanAbsoluteError: 0.0801423490047455 | Loss: 0.0109958180395188 | Epoch: 869 | \n\n\nMeanAbsoluteError: 0.0786699578166008 | Loss: 0.0113782659853132 | Epoch: 870 | \n\n\nMeanAbsoluteError: 0.0847815796732903 | Loss: 0.0121816602428832 | Epoch: 871 | \n\n\nMeanAbsoluteError: 0.0813475921750069 | Loss: 0.0116541267938737 | Epoch: 872 | \n\n\nMeanAbsoluteError: 0.0843992903828621 | Loss: 0.0125543567305795 | Epoch: 873 | \n\n\nMeanAbsoluteError: 0.0813746899366379 | Loss: 0.0113958239653099 | Epoch: 874 | \n\n\nMeanAbsoluteError: 0.0786771327257156 | Loss: 0.0110712991015680 | Epoch: 875 | \n\n\nMeanAbsoluteError: 0.0791269540786743 | Loss: 0.0109537382283452 | Epoch: 876 | \n\n\nMeanAbsoluteError: 0.0818461105227470 | Loss: 0.0110708165898056 | Epoch: 877 | \n\n\nMeanAbsoluteError: 0.0819649025797844 | Loss: 0.0110287559686306 | Epoch: 878 | \n\n\nMeanAbsoluteError: 0.0799859389662743 | Loss: 0.0109235139052483 | Epoch: 879 | \n\n\nMeanAbsoluteError: 0.0788929089903831 | Loss: 0.0109401716073626 | Epoch: 880 | \n\n\nMeanAbsoluteError: 0.0805591195821762 | Loss: 0.0116782069822390 | Epoch: 881 | \n\n\nMeanAbsoluteError: 0.0815792307257652 | Loss: 0.0116627109412608 | Epoch: 882 | \n\n\nMeanAbsoluteError: 0.0759766995906830 | Loss: 0.0100251412577927 | Epoch: 883 | \n\n\nMeanAbsoluteError: 0.0858740955591202 | Loss: 0.0124087188768317 | Epoch: 884 | \n\n\nMeanAbsoluteError: 0.0846439525485039 | Loss: 0.0126131183540565 | Epoch: 885 | \n\n\nMeanAbsoluteError: 0.0756167024374008 | Loss: 0.0104804770056459 | Epoch: 886 | \n\n\nMeanAbsoluteError: 0.0862602069973946 | Loss: 0.0125295193013153 | Epoch: 887 | \n\n\nMeanAbsoluteError: 0.0842633470892906 | Loss: 0.0120939403042333 | Epoch: 888 | \n\n\nMeanAbsoluteError: 0.0782253518700600 | Loss: 0.0102482067305633 | Epoch: 889 | \n\n\nMeanAbsoluteError: 0.0844241604208946 | Loss: 0.0131225794031222 | Epoch: 890 | \n\n\nMeanAbsoluteError: 0.0898871570825577 | Loss: 0.0136839044233299 | Epoch: 891 | \n\n\nMeanAbsoluteError: 0.0757718831300735 | Loss: 0.0099002271682669 | Epoch: 892 | \n\n\nMeanAbsoluteError: 0.0748435184359550 | Loss: 0.0098378815049606 | Epoch: 893 | \n\n\nMeanAbsoluteError: 0.0757410526275635 | Loss: 0.0101484150824539 | Epoch: 894 | \n\n\nMeanAbsoluteError: 0.0812772586941719 | Loss: 0.0113163437708863 | Epoch: 895 | \n\n\nMeanAbsoluteError: 0.0780863910913467 | Loss: 0.0106433528299143 | Epoch: 896 | \n\n\nMeanAbsoluteError: 0.0773562714457512 | Loss: 0.0106513015190285 | Epoch: 897 | \n\n\nMeanAbsoluteError: 0.0749993249773979 | Loss: 0.0096264199471307 | Epoch: 898 | \n\n\nMeanAbsoluteError: 0.0789592117071152 | Loss: 0.0113375335422703 | Epoch: 899 | \n\n\nMeanAbsoluteError: 0.0816815271973610 | Loss: 0.0110189523385391 | Epoch: 900 | \n\n\nMeanAbsoluteError: 0.0838501453399658 | Loss: 0.0112654177425429 | Epoch: 901 | \n\n\nMeanAbsoluteError: 0.0778255090117455 | Loss: 0.0102900812072555 | Epoch: 902 | \n\n\nMeanAbsoluteError: 0.0807135105133057 | Loss: 0.0116795355185483 | Epoch: 903 | \n\n\nMeanAbsoluteError: 0.0787260606884956 | Loss: 0.0114334946652586 | Epoch: 904 | \n\n\nMeanAbsoluteError: 0.0844293981790543 | Loss: 0.0129608468680575 | Epoch: 905 | \n\n\nMeanAbsoluteError: 0.0813209414482117 | Loss: 0.0120041672233250 | Epoch: 906 | \n\n\nMeanAbsoluteError: 0.0814835280179977 | Loss: 0.0110535776576823 | Epoch: 907 | \n\n\nMeanAbsoluteError: 0.0768981575965881 | Loss: 0.0103869849042288 | Epoch: 908 | \n\n\nMeanAbsoluteError: 0.0809586048126221 | Loss: 0.0112749618426703 | Epoch: 909 | \n\n\nMeanAbsoluteError: 0.0754221379756927 | Loss: 0.0092557174715269 | Epoch: 910 | \n\n\nMeanAbsoluteError: 0.0788604915142059 | Loss: 0.0103900427998209 | Epoch: 911 | \n\n\nMeanAbsoluteError: 0.0857787132263184 | Loss: 0.0124388265590339 | Epoch: 912 | \n\n\nMeanAbsoluteError: 0.0798002108931541 | Loss: 0.0115736024549794 | Epoch: 913 | \n\n\nMeanAbsoluteError: 0.0741291120648384 | Loss: 0.0101267338575811 | Epoch: 914 | \n\n\nMeanAbsoluteError: 0.0726627632975578 | Loss: 0.0089439469636333 | Epoch: 915 | \n\n\nMeanAbsoluteError: 0.0820481851696968 | Loss: 0.0115670912910233 | Epoch: 916 | \n\n\nMeanAbsoluteError: 0.0853462070226669 | Loss: 0.0127835107024293 | Epoch: 917 | \n\n\nMeanAbsoluteError: 0.0784476771950722 | Loss: 0.0106395146898042 | Epoch: 918 | \n\n\nMeanAbsoluteError: 0.0830525532364845 | Loss: 0.0125364735704958 | Epoch: 919 | \n\n\nMeanAbsoluteError: 0.0793120712041855 | Loss: 0.0109584602366158 | Epoch: 920 | \n\n\nMeanAbsoluteError: 0.0786806195974350 | Loss: 0.0102068519811580 | Epoch: 921 | \n\n\nMeanAbsoluteError: 0.0768987834453583 | Loss: 0.0104221758601003 | Epoch: 922 | \n\n\nMeanAbsoluteError: 0.0767249390482903 | Loss: 0.0103767137270188 | Epoch: 923 | \n\n\nMeanAbsoluteError: 0.0786586701869965 | Loss: 0.0113181284102514 | Epoch: 924 | \n\n\nMeanAbsoluteError: 0.0739174559712410 | Loss: 0.0096574041480865 | Epoch: 925 | \n\n\nMeanAbsoluteError: 0.0838286355137825 | Loss: 0.0122198558820916 | Epoch: 926 | \n\n\nMeanAbsoluteError: 0.0730580687522888 | Loss: 0.0092778074751914 | Epoch: 927 | \n\n\nMeanAbsoluteError: 0.0756944939494133 | Loss: 0.0103191590101612 | Epoch: 928 | \n\n\nMeanAbsoluteError: 0.0759568586945534 | Loss: 0.0102724424427045 | Epoch: 929 | \n\n\nMeanAbsoluteError: 0.0775511935353279 | Loss: 0.0102831589439908 | Epoch: 930 | \n\n\nMeanAbsoluteError: 0.0754893273115158 | Loss: 0.0104268004150072 | Epoch: 931 | \n\n\nMeanAbsoluteError: 0.0726296156644821 | Loss: 0.0095147245946282 | Epoch: 932 | \n\n\nMeanAbsoluteError: 0.0820389315485954 | Loss: 0.0116214555169669 | Epoch: 933 | \n\n\nMeanAbsoluteError: 0.0827655792236328 | Loss: 0.0117853652490157 | Epoch: 934 | \n\n\nMeanAbsoluteError: 0.0804846957325935 | Loss: 0.0119696573951842 | Epoch: 935 | \n\n\nMeanAbsoluteError: 0.0767912194132805 | Loss: 0.0105601694553964 | Epoch: 936 | \n\n\nMeanAbsoluteError: 0.0790959522128105 | Loss: 0.0107952303733327 | Epoch: 937 | \n\n\nMeanAbsoluteError: 0.0731723159551620 | Loss: 0.0097686151477319 | Epoch: 938 | \n\n\nMeanAbsoluteError: 0.0808879435062408 | Loss: 0.0111010913275337 | Epoch: 939 | \n\n\nMeanAbsoluteError: 0.0794004648923874 | Loss: 0.0102978994961692 | Epoch: 940 | \n\n\nMeanAbsoluteError: 0.0738274678587914 | Loss: 0.0094864033326303 | Epoch: 941 | \n\n\nMeanAbsoluteError: 0.0749234929680824 | Loss: 0.0099489252865654 | Epoch: 942 | \n\n\nMeanAbsoluteError: 0.0811494290828705 | Loss: 0.0118170250146098 | Epoch: 943 | \n\n\nMeanAbsoluteError: 0.0822333320975304 | Loss: 0.0115008416788017 | Epoch: 944 | \n\n\nMeanAbsoluteError: 0.0790784955024719 | Loss: 0.0112112806899192 | Epoch: 945 | \n\n\nMeanAbsoluteError: 0.0768588036298752 | Loss: 0.0103507605288723 | Epoch: 946 | \n\n\nMeanAbsoluteError: 0.0775722116231918 | Loss: 0.0110586457593793 | Epoch: 947 | \n\n\nMeanAbsoluteError: 0.0771796852350235 | Loss: 0.0097859355963495 | Epoch: 948 | \n\n\nMeanAbsoluteError: 0.0768987685441971 | Loss: 0.0105974008322422 | Epoch: 949 | \n\n\nMeanAbsoluteError: 0.0761746391654015 | Loss: 0.0103491205203075 | Epoch: 950 | \n\n\nMeanAbsoluteError: 0.0793229416012764 | Loss: 0.0104981532203965 | Epoch: 951 | \n\n\nMeanAbsoluteError: 0.0742906555533409 | Loss: 0.0100338138694012 | Epoch: 952 | \n\n\nMeanAbsoluteError: 0.0717527940869331 | Loss: 0.0092270109616705 | Epoch: 953 | \n\n\nMeanAbsoluteError: 0.0752084553241730 | Loss: 0.0102649586030020 | Epoch: 954 | \n\n\nMeanAbsoluteError: 0.0754370316863060 | Loss: 0.0108004060934036 | Epoch: 955 | \n\n\nMeanAbsoluteError: 0.0758293643593788 | Loss: 0.0107496383518082 | Epoch: 956 | \n\n\nMeanAbsoluteError: 0.0796445384621620 | Loss: 0.0109894070438168 | Epoch: 957 | \n\n\nMeanAbsoluteError: 0.0740624442696571 | Loss: 0.0100846323144894 | Epoch: 958 | \n\n\nMeanAbsoluteError: 0.0720881372690201 | Loss: 0.0096127927380924 | Epoch: 959 | \n\n\nMeanAbsoluteError: 0.0735991895198822 | Loss: 0.0098538780075008 | Epoch: 960 | \n\n\nMeanAbsoluteError: 0.0824889168143272 | Loss: 0.0114498444737546 | Epoch: 961 | \n\n\nMeanAbsoluteError: 0.0755169019103050 | Loss: 0.0096750844635729 | Epoch: 962 | \n\n\nMeanAbsoluteError: 0.0747082680463791 | Loss: 0.0097680197055585 | Epoch: 963 | \n\n\nMeanAbsoluteError: 0.0780020579695702 | Loss: 0.0109107464712967 | Epoch: 964 | \n\n\nMeanAbsoluteError: 0.0750912502408028 | Loss: 0.0101700004924108 | Epoch: 965 | \n\n\nMeanAbsoluteError: 0.0766906961798668 | Loss: 0.0110235938556192 | Epoch: 966 | \n\n\nMeanAbsoluteError: 0.0774621292948723 | Loss: 0.0110000495247853 | Epoch: 967 | \n\n\nMeanAbsoluteError: 0.0772939324378967 | Loss: 0.0103758150965587 | Epoch: 968 | \n\n\nMeanAbsoluteError: 0.0762898027896881 | Loss: 0.0111336868050557 | Epoch: 969 | \n\n\nMeanAbsoluteError: 0.0776784867048264 | Loss: 0.0104963949816617 | Epoch: 970 | \n\n\nMeanAbsoluteError: 0.0774522796273232 | Loss: 0.0102377303567482 | Epoch: 971 | \n\n\nMeanAbsoluteError: 0.0779293850064278 | Loss: 0.0108898497324829 | Epoch: 972 | \n\n\nMeanAbsoluteError: 0.0773804709315300 | Loss: 0.0111483196996899 | Epoch: 973 | \n\n\nMeanAbsoluteError: 0.0734580084681511 | Loss: 0.0097250065119442 | Epoch: 974 | \n\n\nMeanAbsoluteError: 0.0698473155498505 | Loss: 0.0090547812792768 | Epoch: 975 | \n\n\nMeanAbsoluteError: 0.0758954510092735 | Loss: 0.0105161764061692 | Epoch: 976 | \n\n\nMeanAbsoluteError: 0.0771692842245102 | Loss: 0.0105114353021539 | Epoch: 977 | \n\n\nMeanAbsoluteError: 0.0768925771117210 | Loss: 0.0101258732359080 | Epoch: 978 | \n\n\nMeanAbsoluteError: 0.0759636461734772 | Loss: 0.0096715043471825 | Epoch: 979 | \n\n\nMeanAbsoluteError: 0.0731292963027954 | Loss: 0.0092510197652155 | Epoch: 980 | \n\n\nMeanAbsoluteError: 0.0754137560725212 | Loss: 0.0102910660080670 | Epoch: 981 | \n\n\nMeanAbsoluteError: 0.0756300389766693 | Loss: 0.0105604793619845 | Epoch: 982 | \n\n\nMeanAbsoluteError: 0.0752184689044952 | Loss: 0.0107742288427471 | Epoch: 983 | \n\n\nMeanAbsoluteError: 0.0746436938643456 | Loss: 0.0099925042744629 | Epoch: 984 | \n\n\nMeanAbsoluteError: 0.0735620483756065 | Loss: 0.0092939956776778 | Epoch: 985 | \n\n\nMeanAbsoluteError: 0.0767181888222694 | Loss: 0.0107770497055996 | Epoch: 986 | \n\n\nMeanAbsoluteError: 0.0701046511530876 | Loss: 0.0085874649930368 | Epoch: 987 | \n\n\nMeanAbsoluteError: 0.0704341754317284 | Loss: 0.0090705875546943 | Epoch: 988 | \n\n\nMeanAbsoluteError: 0.0702798888087273 | Loss: 0.0093222978862832 | Epoch: 989 | \n\n\nMeanAbsoluteError: 0.0817665308713913 | Loss: 0.0122767028746481 | Epoch: 990 | \n\n\nMeanAbsoluteError: 0.0707507953047752 | Loss: 0.0090121791203637 | Epoch: 991 | \n\n\nMeanAbsoluteError: 0.0752551034092903 | Loss: 0.0101698146625184 | Epoch: 992 | \n\n\nMeanAbsoluteError: 0.0746623873710632 | Loss: 0.0102317832089708 | Epoch: 993 | \n\n\nMeanAbsoluteError: 0.0730829313397408 | Loss: 0.0090266401559105 | Epoch: 994 | \n\n\nMeanAbsoluteError: 0.0806959867477417 | Loss: 0.0112649497182671 | Epoch: 995 | \n\n\nMeanAbsoluteError: 0.0719759985804558 | Loss: 0.0092319580728326 | Epoch: 996 | \n\n\nMeanAbsoluteError: 0.0743474289774895 | Loss: 0.0090339256021374 | Epoch: 997 | \n\n\nMeanAbsoluteError: 0.0722305104136467 | Loss: 0.0093443173475680 | Epoch: 998 | \n\n\nMeanAbsoluteError: 0.0760636329650879 | Loss: 0.0105550973083397 | Epoch: 999 | \n\n\nMeanAbsoluteError: 0.0778869092464447 | Loss: 0.0107093226293606 | Epoch: 1000 | \n\n\nMeanAbsoluteError: 0.0720398202538490 | Loss: 0.0096198635313097 | Epoch: 1001 | \n\n\nMeanAbsoluteError: 0.0779487788677216 | Loss: 0.0103948014295020 | Epoch: 1002 | \n\n\nMeanAbsoluteError: 0.0743956565856934 | Loss: 0.0103463096082366 | Epoch: 1003 | \n\n\nMeanAbsoluteError: 0.0764098092913628 | Loss: 0.0113546338843783 | Epoch: 1004 | \n\n\nMeanAbsoluteError: 0.0718340724706650 | Loss: 0.0092673373440630 | Epoch: 1005 | \n\n\nMeanAbsoluteError: 0.0753624066710472 | Loss: 0.0097416962429998 | Epoch: 1006 | \n\n\nMeanAbsoluteError: 0.0742793157696724 | Loss: 0.0102859737910527 | Epoch: 1007 | \n\n\nMeanAbsoluteError: 0.0740352272987366 | Loss: 0.0098483210969183 | Epoch: 1008 | \n\n\nMeanAbsoluteError: 0.0770121291279793 | Loss: 0.0101375273075428 | Epoch: 1009 | \n\n\nMeanAbsoluteError: 0.0735422819852829 | Loss: 0.0091241588189102 | Epoch: 1010 | \n\n\nMeanAbsoluteError: 0.0769165828824043 | Loss: 0.0102839080139529 | Epoch: 1011 | \n\n\nMeanAbsoluteError: 0.0737181827425957 | Loss: 0.0101681386808317 | Epoch: 1012 | \n\n\nMeanAbsoluteError: 0.0727175921201706 | Loss: 0.0091971428943816 | Epoch: 1013 | \n\n\nMeanAbsoluteError: 0.0700171738862991 | Loss: 0.0086749405318672 | Epoch: 1014 | \n\n\nMeanAbsoluteError: 0.0782439559698105 | Loss: 0.0109434373721403 | Epoch: 1015 | \n\n\nMeanAbsoluteError: 0.0742497593164444 | Loss: 0.0098111885105512 | Epoch: 1016 | \n\n\nMeanAbsoluteError: 0.0749841183423996 | Loss: 0.0100662982639915 | Epoch: 1017 | \n\n\nMeanAbsoluteError: 0.0696621015667915 | Loss: 0.0084821208809869 | Epoch: 1018 | \n\n\nMeanAbsoluteError: 0.0756380110979080 | Loss: 0.0096034490731351 | Epoch: 1019 | \n\n\nMeanAbsoluteError: 0.0719589069485664 | Loss: 0.0091640125274049 | Epoch: 1020 | \n\n\nMeanAbsoluteError: 0.0728491842746735 | Loss: 0.0099292557383160 | Epoch: 1021 | \n\n\nMeanAbsoluteError: 0.0775198116898537 | Loss: 0.0120478441376933 | Epoch: 1022 | \n\n\nMeanAbsoluteError: 0.0726411342620850 | Loss: 0.0099778433927349 | Epoch: 1023 | \n\n\nMeanAbsoluteError: 0.0746380984783173 | Loss: 0.0099609534257767 | Epoch: 1024 | \n\n\nMeanAbsoluteError: 0.0696916058659554 | Loss: 0.0086019314503801 | Epoch: 1025 | \n\n\nMeanAbsoluteError: 0.0736414119601250 | Loss: 0.0094543453772591 | Epoch: 1026 | \n\n\nMeanAbsoluteError: 0.0665561333298683 | Loss: 0.0080108324666677 | Epoch: 1027 | \n\n\nMeanAbsoluteError: 0.0769356787204742 | Loss: 0.0102971428724413 | Epoch: 1028 | \n\n\nMeanAbsoluteError: 0.0729255229234695 | Loss: 0.0102974782467193 | Epoch: 1029 | \n\n\nMeanAbsoluteError: 0.0732668042182922 | Loss: 0.0093654376582223 | Epoch: 1030 | \n\n\nMeanAbsoluteError: 0.0670821070671082 | Loss: 0.0080835466588420 | Epoch: 1031 | \n\n\nMeanAbsoluteError: 0.0789594054222107 | Loss: 0.0111720860324567 | Epoch: 1032 | \n\n\nMeanAbsoluteError: 0.0722104236483574 | Loss: 0.0095345339063473 | Epoch: 1033 | \n\n\nMeanAbsoluteError: 0.0762107148766518 | Loss: 0.0101591128386402 | Epoch: 1034 | \n\n\nMeanAbsoluteError: 0.0774746760725975 | Loss: 0.0105936730924683 | Epoch: 1035 | \n\n\nMeanAbsoluteError: 0.0738628506660461 | Loss: 0.0094281948700761 | Epoch: 1036 | \n\n\nMeanAbsoluteError: 0.0712879821658134 | Loss: 0.0089683109217731 | Epoch: 1037 | \n\n\nMeanAbsoluteError: 0.0688697397708893 | Loss: 0.0089978134306026 | Epoch: 1038 | \n\n\nMeanAbsoluteError: 0.0714824721217155 | Loss: 0.0086015057790064 | Epoch: 1039 | \n\n\nMeanAbsoluteError: 0.0679570510983467 | Loss: 0.0084872436084576 | Epoch: 1040 | \n\n\nMeanAbsoluteError: 0.0716466382145882 | Loss: 0.0094488786419485 | Epoch: 1041 | \n\n\nMeanAbsoluteError: 0.0701106488704681 | Loss: 0.0092831242653483 | Epoch: 1042 | \n\n\nMeanAbsoluteError: 0.0772826224565506 | Loss: 0.0105078839341877 | Epoch: 1043 | \n\n\nMeanAbsoluteError: 0.0738253444433212 | Loss: 0.0101541155498611 | Epoch: 1044 | \n\n\nMeanAbsoluteError: 0.0699052959680557 | Loss: 0.0104089312412058 | Epoch: 1045 | \n\n\nMeanAbsoluteError: 0.0740487873554230 | Loss: 0.0096738500107313 | Epoch: 1046 | \n\n\nMeanAbsoluteError: 0.0705165192484856 | Loss: 0.0087628259141184 | Epoch: 1047 | \n\n\nMeanAbsoluteError: 0.0789414942264557 | Loss: 0.0111807936271847 | Epoch: 1048 | \n\n\nMeanAbsoluteError: 0.0721265003085136 | Loss: 0.0095928019699204 | Epoch: 1049 | \n\n\nMeanAbsoluteError: 0.0691344514489174 | Loss: 0.0086269412225496 | Epoch: 1050 | \n\n\nMeanAbsoluteError: 0.0687873288989067 | Loss: 0.0081101695930617 | Epoch: 1051 | \n\n\nMeanAbsoluteError: 0.0731238722801208 | Loss: 0.0092851184171983 | Epoch: 1052 | \n\n\nMeanAbsoluteError: 0.0734232366085052 | Loss: 0.0098786560941274 | Epoch: 1053 | \n\n\nMeanAbsoluteError: 0.0688166394829750 | Loss: 0.0090314692878989 | Epoch: 1054 | \n\n\nMeanAbsoluteError: 0.0699285715818405 | Loss: 0.0091359428715077 | Epoch: 1055 | \n\n\nMeanAbsoluteError: 0.0730866193771362 | Loss: 0.0094299958583671 | Epoch: 1056 | \n\n\nMeanAbsoluteError: 0.0727207660675049 | Loss: 0.0095885968681735 | Epoch: 1057 | \n\n\nMeanAbsoluteError: 0.0760412365198135 | Loss: 0.0098416999675343 | Epoch: 1058 | \n\n\nMeanAbsoluteError: 0.0767604932188988 | Loss: 0.0101127151486738 | Epoch: 1059 | \n\n\nMeanAbsoluteError: 0.0676267594099045 | Loss: 0.0081645882454177 | Epoch: 1060 | \n\n\nMeanAbsoluteError: 0.0734525248408318 | Loss: 0.0092125491981157 | Epoch: 1061 | \n\n\nMeanAbsoluteError: 0.0657661780714989 | Loss: 0.0077343113956158 | Epoch: 1062 | \n\n\nMeanAbsoluteError: 0.0711654424667358 | Loss: 0.0092218807821094 | Epoch: 1063 | \n\n\nMeanAbsoluteError: 0.0720278173685074 | Loss: 0.0096218293286802 | Epoch: 1064 | \n\n\nMeanAbsoluteError: 0.0667654201388359 | Loss: 0.0087259133614134 | Epoch: 1065 | \n\n\nMeanAbsoluteError: 0.0757054761052132 | Loss: 0.0104550373913225 | Epoch: 1066 | \n\n\nMeanAbsoluteError: 0.0741326808929443 | Loss: 0.0104189237749476 | Epoch: 1067 | \n\n\nMeanAbsoluteError: 0.0693280696868896 | Loss: 0.0089221268550682 | Epoch: 1068 | \n\n\nMeanAbsoluteError: 0.0700264871120453 | Loss: 0.0084216324460431 | Epoch: 1069 | \n\n\nMeanAbsoluteError: 0.0708019286394119 | Loss: 0.0098267441815551 | Epoch: 1070 | \n\n\nMeanAbsoluteError: 0.0650755986571312 | Loss: 0.0071900325659468 | Epoch: 1071 | \n\n\nMeanAbsoluteError: 0.0692143663764000 | Loss: 0.0080474145010521 | Epoch: 1072 | \n\n\nMeanAbsoluteError: 0.0690489038825035 | Loss: 0.0088566975049859 | Epoch: 1073 | \n\n\nMeanAbsoluteError: 0.0692298188805580 | Loss: 0.0083669702976476 | Epoch: 1074 | \n\n\nMeanAbsoluteError: 0.0644997879862785 | Loss: 0.0083346251809765 | Epoch: 1075 | \n\n\nMeanAbsoluteError: 0.0699879005551338 | Loss: 0.0087075877806637 | Epoch: 1076 | \n\n\nMeanAbsoluteError: 0.0731918066740036 | Loss: 0.0089177198837918 | Epoch: 1077 | \n\n\nMeanAbsoluteError: 0.0673024952411652 | Loss: 0.0076451655269193 | Epoch: 1078 | \n\n\nMeanAbsoluteError: 0.0780040696263313 | Loss: 0.0110309877640369 | Epoch: 1079 | \n\n\nMeanAbsoluteError: 0.0726638063788414 | Loss: 0.0098563764763336 | Epoch: 1080 | \n\n\nMeanAbsoluteError: 0.0694653987884521 | Loss: 0.0083910764001090 | Epoch: 1081 | \n\n\nMeanAbsoluteError: 0.0753333866596222 | Loss: 0.0098505976941669 | Epoch: 1082 | \n\n\nMeanAbsoluteError: 0.0713788941502571 | Loss: 0.0090424508767440 | Epoch: 1083 | \n\n\nMeanAbsoluteError: 0.0676964446902275 | Loss: 0.0079056499903224 | Epoch: 1084 | \n\n\nMeanAbsoluteError: 0.0657676681876183 | Loss: 0.0082160364747688 | Epoch: 1085 | \n\n\nMeanAbsoluteError: 0.0705958977341652 | Loss: 0.0087706079297156 | Epoch: 1086 | \n\n\nMeanAbsoluteError: 0.0721160694956779 | Loss: 0.0093239356941987 | Epoch: 1087 | \n\n\nMeanAbsoluteError: 0.0723033174872398 | Loss: 0.0092838020112201 | Epoch: 1088 | \n\n\nMeanAbsoluteError: 0.0660838186740875 | Loss: 0.0077160189786567 | Epoch: 1089 | \n\n\nMeanAbsoluteError: 0.0676441192626953 | Loss: 0.0084353468952516 | Epoch: 1090 | \n\n\nMeanAbsoluteError: 0.0731453374028206 | Loss: 0.0103008913255637 | Epoch: 1091 | \n\n\nMeanAbsoluteError: 0.0673939511179924 | Loss: 0.0081727778207035 | Epoch: 1092 | \n\n\nMeanAbsoluteError: 0.0724844560027122 | Loss: 0.0099187351230406 | Epoch: 1093 | \n\n\nMeanAbsoluteError: 0.0678938701748848 | Loss: 0.0078804595554781 | Epoch: 1094 | \n\n\nMeanAbsoluteError: 0.0693686529994011 | Loss: 0.0091490653704326 | Epoch: 1095 | \n\n\nMeanAbsoluteError: 0.0680792555212975 | Loss: 0.0083232817657214 | Epoch: 1096 | \n\n\nMeanAbsoluteError: 0.0690410584211349 | Loss: 0.0089697228502094 | Epoch: 1097 | \n\n\nMeanAbsoluteError: 0.0718171298503876 | Loss: 0.0098677927207700 | Epoch: 1098 | \n\n\nMeanAbsoluteError: 0.0674389898777008 | Loss: 0.0083939791747252 | Epoch: 1099 | \n\n\nMeanAbsoluteError: 0.0717358291149139 | Loss: 0.0090133832444311 | Epoch: 1100 | \n\n\nMeanAbsoluteError: 0.0739688798785210 | Loss: 0.0094597810850125 | Epoch: 1101 | \n\n\nMeanAbsoluteError: 0.0694686099886894 | Loss: 0.0089566359939514 | Epoch: 1102 | \n\n\nMeanAbsoluteError: 0.0670281648635864 | Loss: 0.0079575374723451 | Epoch: 1103 | \n\n\nMeanAbsoluteError: 0.0687978491187096 | Loss: 0.0083505783579191 | Epoch: 1104 | \n\n\nMeanAbsoluteError: 0.0716039612889290 | Loss: 0.0092817377277788 | Epoch: 1105 | \n\n\nMeanAbsoluteError: 0.0673574283719063 | Loss: 0.0086778577233417 | Epoch: 1106 | \n\n\nMeanAbsoluteError: 0.0729501694440842 | Loss: 0.0097370582541043 | Epoch: 1107 | \n\n\nMeanAbsoluteError: 0.0691455230116844 | Loss: 0.0083085532730426 | Epoch: 1108 | \n\n\nMeanAbsoluteError: 0.0718903243541718 | Loss: 0.0090782378828711 | Epoch: 1109 | \n\n\nMeanAbsoluteError: 0.0677653029561043 | Loss: 0.0084625953488770 | Epoch: 1110 | \n\n\nMeanAbsoluteError: 0.0719968527555466 | Loss: 0.0100513323513405 | Epoch: 1111 | \n\n\nMeanAbsoluteError: 0.0732856839895248 | Loss: 0.0103035570613793 | Epoch: 1112 | \n\n\nMeanAbsoluteError: 0.0657692253589630 | Loss: 0.0081465854973552 | Epoch: 1113 | \n\n\nMeanAbsoluteError: 0.0688527524471283 | Loss: 0.0088976123409945 | Epoch: 1114 | \n\n\nMeanAbsoluteError: 0.0730108097195625 | Loss: 0.0095125951221659 | Epoch: 1115 | \n\n\nMeanAbsoluteError: 0.0706983208656311 | Loss: 0.0089552376624730 | Epoch: 1116 | \n\n\nMeanAbsoluteError: 0.0660114362835884 | Loss: 0.0074057093539644 | Epoch: 1117 | \n\n\nMeanAbsoluteError: 0.0674215331673622 | Loss: 0.0092474647725036 | Epoch: 1118 | \n\n\nMeanAbsoluteError: 0.0678834542632103 | Loss: 0.0083031287684995 | Epoch: 1119 | \n\n\nMeanAbsoluteError: 0.0646256729960442 | Loss: 0.0077420907072720 | Epoch: 1120 | \n\n\nMeanAbsoluteError: 0.0669479966163635 | Loss: 0.0086385928851693 | Epoch: 1121 | \n\n\nMeanAbsoluteError: 0.0682039782404900 | Loss: 0.0086245867845052 | Epoch: 1122 | \n\n\nMeanAbsoluteError: 0.0700607523322105 | Loss: 0.0089739582416466 | Epoch: 1123 | \n\n\nMeanAbsoluteError: 0.0729626044631004 | Loss: 0.0099916859427079 | Epoch: 1124 | \n\n\nMeanAbsoluteError: 0.0652863308787346 | Loss: 0.0085486604427570 | Epoch: 1125 | \n\n\nMeanAbsoluteError: 0.0727873519062996 | Loss: 0.0102140495024651 | Epoch: 1126 | \n\n\nMeanAbsoluteError: 0.0680016428232193 | Loss: 0.0084815464481896 | Epoch: 1127 | \n\n\nMeanAbsoluteError: 0.0711305215954781 | Loss: 0.0086464910846765 | Epoch: 1128 | \n\n\nMeanAbsoluteError: 0.0759663209319115 | Loss: 0.0113041704091059 | Epoch: 1129 | \n\n\nMeanAbsoluteError: 0.0642922669649124 | Loss: 0.0078219495041655 | Epoch: 1130 | \n\n\nMeanAbsoluteError: 0.0657243207097054 | Loss: 0.0080099981367918 | Epoch: 1131 | \n\n\nMeanAbsoluteError: 0.0734397545456886 | Loss: 0.0098357565967308 | Epoch: 1132 | \n\n\nMeanAbsoluteError: 0.0678710713982582 | Loss: 0.0085856813653292 | Epoch: 1133 | \n\n\nMeanAbsoluteError: 0.0732141211628914 | Loss: 0.0103830230649517 | Epoch: 1134 | \n\n\nMeanAbsoluteError: 0.0667561143636703 | Loss: 0.0082688259294082 | Epoch: 1135 | \n\n\nMeanAbsoluteError: 0.0694440305233002 | Loss: 0.0091339488648737 | Epoch: 1136 | \n\n\nMeanAbsoluteError: 0.0693826675415039 | Loss: 0.0089811658700819 | Epoch: 1137 | \n\n\nMeanAbsoluteError: 0.0663972422480583 | Loss: 0.0080842123863840 | Epoch: 1138 | \n\n\nMeanAbsoluteError: 0.0712732821702957 | Loss: 0.0091408676128291 | Epoch: 1139 | \n\n\nMeanAbsoluteError: 0.0679482370615005 | Loss: 0.0085543151824701 | Epoch: 1140 | \n\n\nMeanAbsoluteError: 0.0687408968806267 | Loss: 0.0084972527365850 | Epoch: 1141 | \n\n\nMeanAbsoluteError: 0.0617909952998161 | Loss: 0.0068506706202303 | Epoch: 1142 | \n\n\nMeanAbsoluteError: 0.0697473660111427 | Loss: 0.0086555504574305 | Epoch: 1143 | \n\n\nMeanAbsoluteError: 0.0669830217957497 | Loss: 0.0085359005730425 | Epoch: 1144 | \n\n\nMeanAbsoluteError: 0.0673908516764641 | Loss: 0.0089954569409504 | Epoch: 1145 | \n\n\nMeanAbsoluteError: 0.0693574696779251 | Loss: 0.0088607828178616 | Epoch: 1146 | \n\n\nMeanAbsoluteError: 0.0664647743105888 | Loss: 0.0080709363289679 | Epoch: 1147 | \n\n\nMeanAbsoluteError: 0.0636717528104782 | Loss: 0.0073265443558194 | Epoch: 1148 | \n\n\nMeanAbsoluteError: 0.0680214017629623 | Loss: 0.0088513015501060 | Epoch: 1149 | \n\n\nMeanAbsoluteError: 0.0616060905158520 | Loss: 0.0071822679799394 | Epoch: 1150 | \n\n\nMeanAbsoluteError: 0.0693037658929825 | Loss: 0.0090736829189700 | Epoch: 1151 | \n\n\nMeanAbsoluteError: 0.0709946453571320 | Loss: 0.0095651891752368 | Epoch: 1152 | \n\n\nMeanAbsoluteError: 0.0703042000532150 | Loss: 0.0094333537458442 | Epoch: 1153 | \n\n\nMeanAbsoluteError: 0.0679570287466049 | Loss: 0.0088563727543442 | Epoch: 1154 | \n\n\nMeanAbsoluteError: 0.0654124990105629 | Loss: 0.0077503197198045 | Epoch: 1155 | \n\n\nMeanAbsoluteError: 0.0656018555164337 | Loss: 0.0080045959256662 | Epoch: 1156 | \n\n\nMeanAbsoluteError: 0.0683291926980019 | Loss: 0.0086190673567277 | Epoch: 1157 | \n\n\nMeanAbsoluteError: 0.0642938986420631 | Loss: 0.0079159168376160 | Epoch: 1158 | \n\n\nMeanAbsoluteError: 0.0669683739542961 | Loss: 0.0084633944495727 | Epoch: 1159 | \n\n\nMeanAbsoluteError: 0.0678181275725365 | Loss: 0.0085617000729508 | Epoch: 1160 | \n\n\nMeanAbsoluteError: 0.0657332465052605 | Loss: 0.0084250527957314 | Epoch: 1161 | \n\n\nMeanAbsoluteError: 0.0688761472702026 | Loss: 0.0093720016202860 | Epoch: 1162 | \n\n\nMeanAbsoluteError: 0.0663707703351974 | Loss: 0.0079506558790551 | Epoch: 1163 | \n\n\nMeanAbsoluteError: 0.0656562373042107 | Loss: 0.0079118877113372 | Epoch: 1164 | \n\n\nMeanAbsoluteError: 0.0643649250268936 | Loss: 0.0080916666667326 | Epoch: 1165 | \n\n\nMeanAbsoluteError: 0.0680041983723640 | Loss: 0.0087951285620996 | Epoch: 1166 | \n\n\nMeanAbsoluteError: 0.0678870901465416 | Loss: 0.0085654191825840 | Epoch: 1167 | \n\n\nMeanAbsoluteError: 0.0679367333650589 | Loss: 0.0082978765782415 | Epoch: 1168 | \n\n\nMeanAbsoluteError: 0.0696634948253632 | Loss: 0.0097682165031923 | Epoch: 1169 | \n\n\nMeanAbsoluteError: 0.0652889907360077 | Loss: 0.0077443552194260 | Epoch: 1170 | \n\n\nMeanAbsoluteError: 0.0666768923401833 | Loss: 0.0082194163703631 | Epoch: 1171 | \n\n\nMeanAbsoluteError: 0.0657704174518585 | Loss: 0.0074883989692413 | Epoch: 1172 | \n\n\nMeanAbsoluteError: 0.0651457756757736 | Loss: 0.0078489330079598 | Epoch: 1173 | \n\n\nMeanAbsoluteError: 0.0693830326199532 | Loss: 0.0088708204636108 | Epoch: 1174 | \n\n\nMeanAbsoluteError: 0.0694972574710846 | Loss: 0.0093048847154811 | Epoch: 1175 | \n\n\nMeanAbsoluteError: 0.0662789642810822 | Loss: 0.0078874856250513 | Epoch: 1176 | \n\n\nMeanAbsoluteError: 0.0667212978005409 | Loss: 0.0082125821374090 | Epoch: 1177 | \n\n\nMeanAbsoluteError: 0.0734151899814606 | Loss: 0.0096741469310291 | Epoch: 1178 | \n\n\nMeanAbsoluteError: 0.0599578283727169 | Loss: 0.0064652643685561 | Epoch: 1179 | \n\n\nMeanAbsoluteError: 0.0580412670969963 | Loss: 0.0065380673889498 | Epoch: 1180 | \n\n\nMeanAbsoluteError: 0.0673172622919083 | Loss: 0.0083990239719909 | Epoch: 1181 | \n\n\nMeanAbsoluteError: 0.0635443702340126 | Loss: 0.0075008552627241 | Epoch: 1182 | \n\n\nMeanAbsoluteError: 0.0688332021236420 | Loss: 0.0084193368866522 | Epoch: 1183 | \n\n\nMeanAbsoluteError: 0.0614050664007664 | Loss: 0.0070019406336542 | Epoch: 1184 | \n\n\nMeanAbsoluteError: 0.0725873336195946 | Loss: 0.0095247836032164 | Epoch: 1185 | \n\n\nMeanAbsoluteError: 0.0631201192736626 | Loss: 0.0075504100346370 | Epoch: 1186 | \n\n\nMeanAbsoluteError: 0.0598200783133507 | Loss: 0.0067591396390344 | Epoch: 1187 | \n\n\nMeanAbsoluteError: 0.0688802897930145 | Loss: 0.0087647415907516 | Epoch: 1188 | \n\n\nMeanAbsoluteError: 0.0672845318913460 | Loss: 0.0080773334367647 | Epoch: 1189 | \n\n\nMeanAbsoluteError: 0.0718253180384636 | Loss: 0.0099886663118923 | Epoch: 1190 | \n\n\nMeanAbsoluteError: 0.0648327991366386 | Loss: 0.0083424238005803 | Epoch: 1191 | \n\n\nMeanAbsoluteError: 0.0656736269593239 | Loss: 0.0079512323070715 | Epoch: 1192 | \n\n\nMeanAbsoluteError: 0.0691467225551605 | Loss: 0.0085086468727968 | Epoch: 1193 | \n\n\nMeanAbsoluteError: 0.0644766539335251 | Loss: 0.0079917737466061 | Epoch: 1194 | \n\n\nMeanAbsoluteError: 0.0672025531530380 | Loss: 0.0084207049561034 | Epoch: 1195 | \n\n\nMeanAbsoluteError: 0.0672320649027824 | Loss: 0.0086288009390834 | Epoch: 1196 | \n\n\nMeanAbsoluteError: 0.0700475722551346 | Loss: 0.0092206977110861 | Epoch: 1197 | \n\n\nMeanAbsoluteError: 0.0657538250088692 | Loss: 0.0083736638657865 | Epoch: 1198 | \n\n\nMeanAbsoluteError: 0.0667580887675285 | Loss: 0.0079303164165079 | Epoch: 1199 | \n\n\nMeanAbsoluteError: 0.0673972070217133 | Loss: 0.0081275697577560 | Epoch: 1200 | \n\n\nMeanAbsoluteError: 0.0683912113308907 | Loss: 0.0083580242333361 | Epoch: 1201 | \n\n\nMeanAbsoluteError: 0.0673546493053436 | Loss: 0.0085633602183467 | Epoch: 1202 | \n\n\nMeanAbsoluteError: 0.0640498399734497 | Loss: 0.0073484861293400 | Epoch: 1203 | \n\n\nMeanAbsoluteError: 0.0649962797760963 | Loss: 0.0080463310493603 | Epoch: 1204 | \n\n\nMeanAbsoluteError: 0.0676889196038246 | Loss: 0.0087382554875319 | Epoch: 1205 | \n\n\nMeanAbsoluteError: 0.0634265691041946 | Loss: 0.0076963582890221 | Epoch: 1206 | \n\n\nMeanAbsoluteError: 0.0672416612505913 | Loss: 0.0086406343161798 | Epoch: 1207 | \n\n\nMeanAbsoluteError: 0.0607302486896515 | Loss: 0.0071208293993641 | Epoch: 1208 | \n\n\nMeanAbsoluteError: 0.0633427724242210 | Loss: 0.0077834844982135 | Epoch: 1209 | \n\n\nMeanAbsoluteError: 0.0707996264100075 | Loss: 0.0099585656186173 | Epoch: 1210 | \n\n\nMeanAbsoluteError: 0.0703249648213387 | Loss: 0.0096644216869269 | Epoch: 1211 | \n\n\nMeanAbsoluteError: 0.0605143606662750 | Loss: 0.0069407044255324 | Epoch: 1212 | \n\n\nMeanAbsoluteError: 0.0663889348506927 | Loss: 0.0081599827234944 | Epoch: 1213 | \n\n\nMeanAbsoluteError: 0.0650738924741745 | Loss: 0.0076173485303783 | Epoch: 1214 | \n\n\nMeanAbsoluteError: 0.0660064965486526 | Loss: 0.0085368441296790 | Epoch: 1215 | \n\n\nMeanAbsoluteError: 0.0666344687342644 | Loss: 0.0083101045862956 | Epoch: 1216 | \n\n\nMeanAbsoluteError: 0.0662740767002106 | Loss: 0.0081676420901204 | Epoch: 1217 | \n\n\nMeanAbsoluteError: 0.0697476789355278 | Loss: 0.0094093449627447 | Epoch: 1218 | \n\n\nMeanAbsoluteError: 0.0644536316394806 | Loss: 0.0075904457949825 | Epoch: 1219 | \n\n\nMeanAbsoluteError: 0.0639291703701019 | Loss: 0.0078523608061369 | Epoch: 1220 | \n\n\nMeanAbsoluteError: 0.0627166777849197 | Loss: 0.0075473807953070 | Epoch: 1221 | \n\n\nMeanAbsoluteError: 0.0646778643131256 | Loss: 0.0083342059622297 | Epoch: 1222 | \n\n\nMeanAbsoluteError: 0.0620104037225246 | Loss: 0.0068115254967415 | Epoch: 1223 | \n\n\nMeanAbsoluteError: 0.0638338401913643 | Loss: 0.0073833704114804 | Epoch: 1224 | \n\n\nMeanAbsoluteError: 0.0643633902072906 | Loss: 0.0075306710961498 | Epoch: 1225 | \n\n\nMeanAbsoluteError: 0.0590556412935257 | Loss: 0.0068484746932573 | Epoch: 1226 | \n\n\nMeanAbsoluteError: 0.0676670745015144 | Loss: 0.0090763521115605 | Epoch: 1227 | \n\n\nMeanAbsoluteError: 0.0626463443040848 | Loss: 0.0077942043763566 | Epoch: 1228 | \n\n\nMeanAbsoluteError: 0.0744474157691002 | Loss: 0.0094104053012416 | Epoch: 1229 | \n\n\nMeanAbsoluteError: 0.0631347224116325 | Loss: 0.0080022379014554 | Epoch: 1230 | \n\n\nMeanAbsoluteError: 0.0660008564591408 | Loss: 0.0084981075404357 | Epoch: 1231 | \n\n\nMeanAbsoluteError: 0.0657832995057106 | Loss: 0.0082284381745679 | Epoch: 1232 | \n\n\nMeanAbsoluteError: 0.0690364167094231 | Loss: 0.0093943224468785 | Epoch: 1233 | \n\n\nMeanAbsoluteError: 0.0633040592074394 | Loss: 0.0075687119804570 | Epoch: 1234 | \n\n\nMeanAbsoluteError: 0.0619405880570412 | Loss: 0.0074118443819680 | Epoch: 1235 | \n\n\nMeanAbsoluteError: 0.0649457275867462 | Loss: 0.0081274369042164 | Epoch: 1236 | \n\n\nMeanAbsoluteError: 0.0655936077237129 | Loss: 0.0073755241975596 | Epoch: 1237 | \n\n\nMeanAbsoluteError: 0.0625272542238235 | Loss: 0.0074496600142523 | Epoch: 1238 | \n\n\nMeanAbsoluteError: 0.0645509436726570 | Loss: 0.0079105462144101 | Epoch: 1239 | \n\n\nMeanAbsoluteError: 0.0648228675127029 | Loss: 0.0079734970387608 | Epoch: 1240 | \n\n\nMeanAbsoluteError: 0.0657517164945602 | Loss: 0.0081444165878141 | Epoch: 1241 | \n\n\nMeanAbsoluteError: 0.0696343556046486 | Loss: 0.0091211316512636 | Epoch: 1242 | \n\n\nMeanAbsoluteError: 0.0664084330201149 | Loss: 0.0085119293380558 | Epoch: 1243 | \n\n\nMeanAbsoluteError: 0.0617525540292263 | Loss: 0.0073335118497107 | Epoch: 1244 | \n\n\nMeanAbsoluteError: 0.0660496279597282 | Loss: 0.0084180838358589 | Epoch: 1245 | \n\n\nMeanAbsoluteError: 0.0644617006182671 | Loss: 0.0083456542651250 | Epoch: 1246 | \n\n\nMeanAbsoluteError: 0.0626951307058334 | Loss: 0.0079965106961269 | Epoch: 1247 | \n\n\nMeanAbsoluteError: 0.0656822994351387 | Loss: 0.0081314274602361 | Epoch: 1248 | \n\n\nMeanAbsoluteError: 0.0658625289797783 | Loss: 0.0083424416943550 | Epoch: 1249 | \n\n\nMeanAbsoluteError: 0.0631842166185379 | Loss: 0.0076478562877552 | Epoch: 1250 | \n\n\nMeanAbsoluteError: 0.0710783749818802 | Loss: 0.0096555245716687 | Epoch: 1251 | \n\n\nMeanAbsoluteError: 0.0592614412307739 | Loss: 0.0067425348636728 | Epoch: 1252 | \n\n\nMeanAbsoluteError: 0.0663901418447495 | Loss: 0.0078053863039743 | Epoch: 1253 | \n\n\nMeanAbsoluteError: 0.0691507086157799 | Loss: 0.0091492249032035 | Epoch: 1254 | \n\n\nMeanAbsoluteError: 0.0629217624664307 | Loss: 0.0073927624459066 | Epoch: 1255 | \n\n\nMeanAbsoluteError: 0.0661026611924171 | Loss: 0.0086853649568078 | Epoch: 1256 | \n\n\nMeanAbsoluteError: 0.0613975636661053 | Loss: 0.0073402260030556 | Epoch: 1257 | \n\n\nMeanAbsoluteError: 0.0622594952583313 | Loss: 0.0082377701445512 | Epoch: 1258 | \n\n\nMeanAbsoluteError: 0.0641540363430977 | Loss: 0.0079475528408754 | Epoch: 1259 | \n\n\nMeanAbsoluteError: 0.0628931969404221 | Loss: 0.0079986319399904 | Epoch: 1260 | \n\n\nMeanAbsoluteError: 0.0638000443577766 | Loss: 0.0078782466680180 | Epoch: 1261 | \n\n\nMeanAbsoluteError: 0.0619503036141396 | Loss: 0.0073178483744414 | Epoch: 1262 | \n\n\nMeanAbsoluteError: 0.0666416957974434 | Loss: 0.0078105177853286 | Epoch: 1263 | \n\n\nMeanAbsoluteError: 0.0633086860179901 | Loss: 0.0073046338178756 | Epoch: 1264 | \n\n\nMeanAbsoluteError: 0.0645361915230751 | Loss: 0.0079287140754362 | Epoch: 1265 | \n\n\nMeanAbsoluteError: 0.0655589029192924 | Loss: 0.0081492260969147 | Epoch: 1266 | \n\n\nMeanAbsoluteError: 0.0629814341664314 | Loss: 0.0070983510216805 | Epoch: 1267 | \n\n\nMeanAbsoluteError: 0.0615937300026417 | Loss: 0.0073031668441581 | Epoch: 1268 | \n\n\nMeanAbsoluteError: 0.0603879913687706 | Loss: 0.0073668406700745 | Epoch: 1269 | \n\n\nMeanAbsoluteError: 0.0627440959215164 | Loss: 0.0083399073574401 | Epoch: 1270 | \n\n\nMeanAbsoluteError: 0.0631140545010567 | Loss: 0.0078467480128696 | Epoch: 1271 | \n\n\nMeanAbsoluteError: 0.0631535425782204 | Loss: 0.0073726633348754 | Epoch: 1272 | \n\n\nMeanAbsoluteError: 0.0681693702936172 | Loss: 0.0085652533283186 | Epoch: 1273 | \n\n\nMeanAbsoluteError: 0.0679913759231567 | Loss: 0.0089663909710225 | Epoch: 1274 | \n\n\nMeanAbsoluteError: 0.0638831630349159 | Loss: 0.0077398593795321 | Epoch: 1275 | \n\n\nMeanAbsoluteError: 0.0597271397709846 | Loss: 0.0065460173830797 | Epoch: 1276 | \n\n\nMeanAbsoluteError: 0.0623087435960770 | Loss: 0.0071761787057646 | Epoch: 1277 | \n\n\nMeanAbsoluteError: 0.0658652782440186 | Loss: 0.0077676612543898 | Epoch: 1278 | \n\n\nMeanAbsoluteError: 0.0607490278780460 | Loss: 0.0063674711668621 | Epoch: 1279 | \n\n\nMeanAbsoluteError: 0.0653584450483322 | Loss: 0.0082736487706400 | Epoch: 1280 | \n\n\nMeanAbsoluteError: 0.0612515397369862 | Loss: 0.0073823748131948 | Epoch: 1281 | \n\n\nMeanAbsoluteError: 0.0577714294195175 | Loss: 0.0059906483489128 | Epoch: 1282 | \n\n\nMeanAbsoluteError: 0.0636217370629311 | Loss: 0.0075078894463877 | Epoch: 1283 | \n\n\nMeanAbsoluteError: 0.0669948905706406 | Loss: 0.0091785873640432 | Epoch: 1284 | \n\n\nMeanAbsoluteError: 0.0647152587771416 | Loss: 0.0077506060533172 | Epoch: 1285 | \n\n\nMeanAbsoluteError: 0.0637116655707359 | Loss: 0.0083345165411568 | Epoch: 1286 | \n\n\nMeanAbsoluteError: 0.0615287721157074 | Loss: 0.0075570796747585 | Epoch: 1287 | \n\n\nMeanAbsoluteError: 0.0627585723996162 | Loss: 0.0077325745858525 | Epoch: 1288 | \n\n\nMeanAbsoluteError: 0.0594178326427937 | Loss: 0.0071623881909424 | Epoch: 1289 | \n\n\nMeanAbsoluteError: 0.0631281286478043 | Loss: 0.0076947690849314 | Epoch: 1290 | \n\n\nMeanAbsoluteError: 0.0687495768070221 | Loss: 0.0090009811636749 | Epoch: 1291 | \n\n\nMeanAbsoluteError: 0.0611864775419235 | Loss: 0.0072575815082685 | Epoch: 1292 | \n\n\nMeanAbsoluteError: 0.0623224638402462 | Loss: 0.0076216359390188 | Epoch: 1293 | \n\n\nMeanAbsoluteError: 0.0596178136765957 | Loss: 0.0066092767629743 | Epoch: 1294 | \n\n\nMeanAbsoluteError: 0.0591997578740120 | Loss: 0.0067692844378598 | Epoch: 1295 | \n\n\nMeanAbsoluteError: 0.0618137232959270 | Loss: 0.0076758719666032 | Epoch: 1296 | \n\n\nMeanAbsoluteError: 0.0630993098020554 | Loss: 0.0076221840595463 | Epoch: 1297 | \n\n\nMeanAbsoluteError: 0.0569693110883236 | Loss: 0.0062320979560991 | Epoch: 1298 | \n\n\nMeanAbsoluteError: 0.0603442452847958 | Loss: 0.0067663997373529 | Epoch: 1299 | \n\n\nMeanAbsoluteError: 0.0664865300059319 | Loss: 0.0080050058282844 | Epoch: 1300 | \n\n\nMeanAbsoluteError: 0.0613676384091377 | Loss: 0.0073078804378626 | Epoch: 1301 | \n\n\nMeanAbsoluteError: 0.0591113604605198 | Loss: 0.0073416990917030 | Epoch: 1302 | \n\n\nMeanAbsoluteError: 0.0688521414995193 | Loss: 0.0092936008319642 | Epoch: 1303 | \n\n\nMeanAbsoluteError: 0.0603582523763180 | Loss: 0.0077363609932460 | Epoch: 1304 | \n\n\nMeanAbsoluteError: 0.0604171305894852 | Loss: 0.0069053361094484 | Epoch: 1305 | \n\n\nMeanAbsoluteError: 0.0656120851635933 | Loss: 0.0092996229118459 | Epoch: 1306 | \n\n\nMeanAbsoluteError: 0.0681336447596550 | Loss: 0.0089222637532172 | Epoch: 1307 | \n\n\nMeanAbsoluteError: 0.0631796792149544 | Loss: 0.0074132126236267 | Epoch: 1308 | \n\n\nMeanAbsoluteError: 0.0571129210293293 | Loss: 0.0064256568697056 | Epoch: 1309 | \n\n\nMeanAbsoluteError: 0.0581771470606327 | Loss: 0.0064751993235889 | Epoch: 1310 | \n\n\nMeanAbsoluteError: 0.0659632608294487 | Loss: 0.0085294104796291 | Epoch: 1311 | \n\n\nMeanAbsoluteError: 0.0654981955885887 | Loss: 0.0083613010432843 | Epoch: 1312 | \n\n\nMeanAbsoluteError: 0.0641982257366180 | Loss: 0.0078601786323937 | Epoch: 1313 | \n\n\nMeanAbsoluteError: 0.0655744969844818 | Loss: 0.0083486020751479 | Epoch: 1314 | \n\n\nMeanAbsoluteError: 0.0618980787694454 | Loss: 0.0068474308218113 | Epoch: 1315 | \n\n\nMeanAbsoluteError: 0.0606571510434151 | Loss: 0.0066551382639833 | Epoch: 1316 | \n\n\nMeanAbsoluteError: 0.0659828558564186 | Loss: 0.0076740462243106 | Epoch: 1317 | \n\n\nMeanAbsoluteError: 0.0636007934808731 | Loss: 0.0075362953451622 | Epoch: 1318 | \n\n\nMeanAbsoluteError: 0.0610351301729679 | Loss: 0.0072893645455285 | Epoch: 1319 | \n\n\nMeanAbsoluteError: 0.0648982003331184 | Loss: 0.0079490030502681 | Epoch: 1320 | \n\n\nMeanAbsoluteError: 0.0659230649471283 | Loss: 0.0082103288002933 | Epoch: 1321 | \n\n\nMeanAbsoluteError: 0.0638283789157867 | Loss: 0.0080537408736806 | Epoch: 1322 | \n\n\nMeanAbsoluteError: 0.0628258585929871 | Loss: 0.0076462148066154 | Epoch: 1323 | \n\n\nMeanAbsoluteError: 0.0612796843051910 | Loss: 0.0082226912758658 | Epoch: 1324 | \n\n\nMeanAbsoluteError: 0.0640576481819153 | Loss: 0.0073426463079401 | Epoch: 1325 | \n\n\nMeanAbsoluteError: 0.0649810507893562 | Loss: 0.0080242532556440 | Epoch: 1326 | \n\n\nMeanAbsoluteError: 0.0658443793654442 | Loss: 0.0081944584563886 | Epoch: 1327 | \n\n\nMeanAbsoluteError: 0.0594921037554741 | Loss: 0.0071367407279710 | Epoch: 1328 | \n\n\nMeanAbsoluteError: 0.0622470974922180 | Loss: 0.0073772592710156 | Epoch: 1329 | \n\n\nMeanAbsoluteError: 0.0638925135135651 | Loss: 0.0073507328958779 | Epoch: 1330 | \n\n\nMeanAbsoluteError: 0.0613439828157425 | Loss: 0.0075942828436412 | Epoch: 1331 | \n\n\nMeanAbsoluteError: 0.0627369210124016 | Loss: 0.0080936611416109 | Epoch: 1332 | \n\n\nMeanAbsoluteError: 0.0659620761871338 | Loss: 0.0082486138389019 | Epoch: 1333 | \n\n\nMeanAbsoluteError: 0.0669009387493134 | Loss: 0.0085588378130554 | Epoch: 1334 | \n\n\nMeanAbsoluteError: 0.0593864507973194 | Loss: 0.0073226425299557 | Epoch: 1335 | \n\n\nMeanAbsoluteError: 0.0668848007917404 | Loss: 0.0089274118261649 | Epoch: 1336 | \n\n\nMeanAbsoluteError: 0.0626833289861679 | Loss: 0.0080482954162774 | Epoch: 1337 | \n\n\nMeanAbsoluteError: 0.0681412294507027 | Loss: 0.0086113578570803 | Epoch: 1338 | \n\n\nMeanAbsoluteError: 0.0619549229741096 | Loss: 0.0071865626999594 | Epoch: 1339 | \n\n\nMeanAbsoluteError: 0.0577534548938274 | Loss: 0.0068647926874269 | Epoch: 1340 | \n\n\nMeanAbsoluteError: 0.0648892372846603 | Loss: 0.0079271563574972 | Epoch: 1341 | \n\n\nMeanAbsoluteError: 0.0599417947232723 | Loss: 0.0072856977568275 | Epoch: 1342 | \n\n\nMeanAbsoluteError: 0.0585133992135525 | Loss: 0.0066426347479683 | Epoch: 1343 | \n\n\nMeanAbsoluteError: 0.0592656470835209 | Loss: 0.0064163675654102 | Epoch: 1344 | \n\n\nMeanAbsoluteError: 0.0585694387555122 | Loss: 0.0066246400722108 | Epoch: 1345 | \n\n\nMeanAbsoluteError: 0.0655615478754044 | Loss: 0.0085577568532123 | Epoch: 1346 | \n\n\nMeanAbsoluteError: 0.0579981058835983 | Loss: 0.0065804204240006 | Epoch: 1347 | \n\n\nMeanAbsoluteError: 0.0594330169260502 | Loss: 0.0069182658041730 | Epoch: 1348 | \n\n\nMeanAbsoluteError: 0.0606903582811356 | Loss: 0.0071616659410938 | Epoch: 1349 | \n\n\nMeanAbsoluteError: 0.0644739121198654 | Loss: 0.0080271231005903 | Epoch: 1350 | \n\n\nMeanAbsoluteError: 0.0648517161607742 | Loss: 0.0079085881559877 | Epoch: 1351 | \n\n\nMeanAbsoluteError: 0.0578875914216042 | Loss: 0.0068001509958170 | Epoch: 1352 | \n\n\nMeanAbsoluteError: 0.0598816499114037 | Loss: 0.0072739881250285 | Epoch: 1353 | \n\n\nMeanAbsoluteError: 0.0576623529195786 | Loss: 0.0062618329499310 | Epoch: 1354 | \n\n\nMeanAbsoluteError: 0.0652350708842278 | Loss: 0.0072805486790821 | Epoch: 1355 | \n\n\nMeanAbsoluteError: 0.0589376017451286 | Loss: 0.0067567535201427 | Epoch: 1356 | \n\n\nMeanAbsoluteError: 0.0620323121547699 | Loss: 0.0075309611857180 | Epoch: 1357 | \n\n\nMeanAbsoluteError: 0.0596172846853733 | Loss: 0.0071653588284789 | Epoch: 1358 | \n\n\nMeanAbsoluteError: 0.0576868951320648 | Loss: 0.0061686315582127 | Epoch: 1359 | \n\n\nMeanAbsoluteError: 0.0645977556705475 | Loss: 0.0078676144802193 | Epoch: 1360 | \n\n\nMeanAbsoluteError: 0.0610854513943195 | Loss: 0.0074290275428454 | Epoch: 1361 | \n\n\nMeanAbsoluteError: 0.0626583695411682 | Loss: 0.0072783519872003 | Epoch: 1362 | \n\n\nMeanAbsoluteError: 0.0598940216004848 | Loss: 0.0073326017659080 | Epoch: 1363 | \n\n\nMeanAbsoluteError: 0.0628657042980194 | Loss: 0.0076343426211436 | Epoch: 1364 | \n\n\nMeanAbsoluteError: 0.0626758337020874 | Loss: 0.0074214211044584 | Epoch: 1365 | \n\n\nMeanAbsoluteError: 0.0630908235907555 | Loss: 0.0074013020695929 | Epoch: 1366 | \n\n\nMeanAbsoluteError: 0.0668975189328194 | Loss: 0.0084319539456919 | Epoch: 1367 | \n\n\nMeanAbsoluteError: 0.0603662431240082 | Loss: 0.0069776913037488 | Epoch: 1368 | \n\n\nMeanAbsoluteError: 0.0588293820619583 | Loss: 0.0064183403227071 | Epoch: 1369 | \n\n\nMeanAbsoluteError: 0.0637662038207054 | Loss: 0.0078723645236945 | Epoch: 1370 | \n\n\nMeanAbsoluteError: 0.0588091462850571 | Loss: 0.0069223752062074 | Epoch: 1371 | \n\n\nMeanAbsoluteError: 0.0628173574805260 | Loss: 0.0077129484620188 | Epoch: 1372 | \n\n\nMeanAbsoluteError: 0.0615661293268204 | Loss: 0.0066804262919080 | Epoch: 1373 | \n\n\nMeanAbsoluteError: 0.0601491741836071 | Loss: 0.0074429841534463 | Epoch: 1374 | \n\n\nMeanAbsoluteError: 0.0628163665533066 | Loss: 0.0080529964443122 | Epoch: 1375 | \n\n\nMeanAbsoluteError: 0.0610780976712704 | Loss: 0.0072439810824835 | Epoch: 1376 | \n\n\nMeanAbsoluteError: 0.0615586340427399 | Loss: 0.0075220664229573 | Epoch: 1377 | \n\n\nMeanAbsoluteError: 0.0655870512127876 | Loss: 0.0085177213107242 | Epoch: 1378 | \n\n\nMeanAbsoluteError: 0.0621405839920044 | Loss: 0.0070382044211450 | Epoch: 1379 | \n\n\nMeanAbsoluteError: 0.0629293844103813 | Loss: 0.0075561998593912 | Epoch: 1380 | \n\n\nMeanAbsoluteError: 0.0647263973951340 | Loss: 0.0075968776316343 | Epoch: 1381 | \n\n\nMeanAbsoluteError: 0.0633037239313126 | Loss: 0.0082035739783411 | Epoch: 1382 | \n\n\nMeanAbsoluteError: 0.0596193559467793 | Loss: 0.0067920155506969 | Epoch: 1383 | \n\n\nMeanAbsoluteError: 0.0610357224941254 | Loss: 0.0067351213099209 | Epoch: 1384 | \n\n\nMeanAbsoluteError: 0.0604575425386429 | Loss: 0.0069347037476715 | Epoch: 1385 | \n\n\nMeanAbsoluteError: 0.0630428940057755 | Loss: 0.0078613430275072 | Epoch: 1386 | \n\n\nMeanAbsoluteError: 0.0557468049228191 | Loss: 0.0055941310770019 | Epoch: 1387 | \n\n\nMeanAbsoluteError: 0.0653910934925079 | Loss: 0.0081823993795842 | Epoch: 1388 | \n\n\nMeanAbsoluteError: 0.0609768666327000 | Loss: 0.0072358684982949 | Epoch: 1389 | \n\n\nMeanAbsoluteError: 0.0630459934473038 | Loss: 0.0082121096309250 | Epoch: 1390 | \n\n\nMeanAbsoluteError: 0.0574517957866192 | Loss: 0.0064201581860107 | Epoch: 1391 | \n\n\nMeanAbsoluteError: 0.0608390606939793 | Loss: 0.0075138575353534 | Epoch: 1392 | \n\n\nMeanAbsoluteError: 0.0637797713279724 | Loss: 0.0081821147376953 | Epoch: 1393 | \n\n\nMeanAbsoluteError: 0.0619392842054367 | Loss: 0.0071006959322403 | Epoch: 1394 | \n\n\nMeanAbsoluteError: 0.0608973763883114 | Loss: 0.0073425817383031 | Epoch: 1395 | \n\n\nMeanAbsoluteError: 0.0590603202581406 | Loss: 0.0067009916191212 | Epoch: 1396 | \n\n\nMeanAbsoluteError: 0.0598850771784782 | Loss: 0.0064711393390220 | Epoch: 1397 | \n\n\nMeanAbsoluteError: 0.0579687133431435 | Loss: 0.0068264358496011 | Epoch: 1398 | \n\n\nMeanAbsoluteError: 0.0621901303529739 | Loss: 0.0077843456117989 | Epoch: 1399 | \n\n\nMeanAbsoluteError: 0.0554886050522327 | Loss: 0.0065323020050225 | Epoch: 1400 | \n\n\nMeanAbsoluteError: 0.0585726350545883 | Loss: 0.0063812456223332 | Epoch: 1401 | \n\n\nMeanAbsoluteError: 0.0595266409218311 | Loss: 0.0066495802176117 | Epoch: 1402 | \n\n\nMeanAbsoluteError: 0.0564505271613598 | Loss: 0.0059481488551470 | Epoch: 1403 | \n\n\nMeanAbsoluteError: 0.0609580725431442 | Loss: 0.0070444400612178 | Epoch: 1404 | \n\n\nMeanAbsoluteError: 0.0610485151410103 | Loss: 0.0070876434495100 | Epoch: 1405 | \n\n\nMeanAbsoluteError: 0.0646022707223892 | Loss: 0.0077245486735531 | Epoch: 1406 | \n\n\nMeanAbsoluteError: 0.0599624328315258 | Loss: 0.0075175275501048 | Epoch: 1407 | \n\n\nMeanAbsoluteError: 0.0629485696554184 | Loss: 0.0079945615638038 | Epoch: 1408 | \n\n\nMeanAbsoluteError: 0.0621159225702286 | Loss: 0.0074681204626116 | Epoch: 1409 | \n\n\nMeanAbsoluteError: 0.0612281411886215 | Loss: 0.0070486192709814 | Epoch: 1410 | \n\n\nMeanAbsoluteError: 0.0550104342401028 | Loss: 0.0057177691774632 | Epoch: 1411 | \n\n\nMeanAbsoluteError: 0.0595599114894867 | Loss: 0.0066950862192395 | Epoch: 1412 | \n\n\nMeanAbsoluteError: 0.0615208931267262 | Loss: 0.0076609754780778 | Epoch: 1413 | \n\n\nMeanAbsoluteError: 0.0627471581101418 | Loss: 0.0075245182066828 | Epoch: 1414 | \n\n\nMeanAbsoluteError: 0.0540531612932682 | Loss: 0.0054639841180445 | Epoch: 1415 | \n\n\nMeanAbsoluteError: 0.0580978579819202 | Loss: 0.0068022566330304 | Epoch: 1416 | \n\n\nMeanAbsoluteError: 0.0589563511312008 | Loss: 0.0068117699861614 | Epoch: 1417 | \n\n\nMeanAbsoluteError: 0.0654342621564865 | Loss: 0.0087138114844371 | Epoch: 1418 | \n\n\nMeanAbsoluteError: 0.0625852122902870 | Loss: 0.0079007905727303 | Epoch: 1419 | \n\n\nMeanAbsoluteError: 0.0595000833272934 | Loss: 0.0067177054821514 | Epoch: 1420 | \n\n\nMeanAbsoluteError: 0.0630208924412727 | Loss: 0.0081595044481946 | Epoch: 1421 | \n\n\nMeanAbsoluteError: 0.0596583634614944 | Loss: 0.0070155220117431 | Epoch: 1422 | \n\n\nMeanAbsoluteError: 0.0563492886722088 | Loss: 0.0061280055019597 | Epoch: 1423 | \n\n\nMeanAbsoluteError: 0.0568573176860809 | Loss: 0.0059560658186244 | Epoch: 1424 | \n\n\nMeanAbsoluteError: 0.0635042488574982 | Loss: 0.0079317366654868 | Epoch: 1425 | \n\n\nMeanAbsoluteError: 0.0614443644881248 | Loss: 0.0070845112582416 | Epoch: 1426 | \n\n\nMeanAbsoluteError: 0.0578879751265049 | Loss: 0.0070317449785944 | Epoch: 1427 | \n\n\nMeanAbsoluteError: 0.0617973655462265 | Loss: 0.0073400209678137 | Epoch: 1428 | \n\n\nMeanAbsoluteError: 0.0629037544131279 | Loss: 0.0078971544696833 | Epoch: 1429 | \n\n\nMeanAbsoluteError: 0.0568309277296066 | Loss: 0.0061271951137193 | Epoch: 1430 | \n\n\nMeanAbsoluteError: 0.0637255460023880 | Loss: 0.0081020164922787 | Epoch: 1431 | \n\n\nMeanAbsoluteError: 0.0606876872479916 | Loss: 0.0073934473152682 | Epoch: 1432 | \n\n\nMeanAbsoluteError: 0.0606285296380520 | Loss: 0.0068772154379743 | Epoch: 1433 | \n\n\nMeanAbsoluteError: 0.0598543658852577 | Loss: 0.0068534901927645 | Epoch: 1434 | \n\n\nMeanAbsoluteError: 0.0577264465391636 | Loss: 0.0062952594307175 | Epoch: 1435 | \n\n\nMeanAbsoluteError: 0.0566153451800346 | Loss: 0.0060020417197939 | Epoch: 1436 | \n\n\nMeanAbsoluteError: 0.0569514669477940 | Loss: 0.0057531625264528 | Epoch: 1437 | \n\n\nMeanAbsoluteError: 0.0588546134531498 | Loss: 0.0067939646513003 | Epoch: 1438 | \n\n\nMeanAbsoluteError: 0.0598287209868431 | Loss: 0.0071928950549651 | Epoch: 1439 | \n\n\nMeanAbsoluteError: 0.0595452375710011 | Loss: 0.0070618342731177 | Epoch: 1440 | \n\n\nMeanAbsoluteError: 0.0601314529776573 | Loss: 0.0066510267496051 | Epoch: 1441 | \n\n\nMeanAbsoluteError: 0.0608237199485302 | Loss: 0.0073465335534032 | Epoch: 1442 | \n\n\nMeanAbsoluteError: 0.0611941479146481 | Loss: 0.0074115830165586 | Epoch: 1443 | \n\n\nMeanAbsoluteError: 0.0641322433948517 | Loss: 0.0078006902541044 | Epoch: 1444 | \n\n\nMeanAbsoluteError: 0.0613203942775726 | Loss: 0.0073344964501484 | Epoch: 1445 | \n\n\nMeanAbsoluteError: 0.0655398294329643 | Loss: 0.0082086389334169 | Epoch: 1446 | \n\n\nMeanAbsoluteError: 0.0590847842395306 | Loss: 0.0064967799023483 | Epoch: 1447 | \n\n\nMeanAbsoluteError: 0.0593362823128700 | Loss: 0.0069061919079589 | Epoch: 1448 | \n\n\nMeanAbsoluteError: 0.0617946125566959 | Loss: 0.0070335763952122 | Epoch: 1449 | \n\n\nMeanAbsoluteError: 0.0559296682476997 | Loss: 0.0058933307213374 | Epoch: 1450 | \n\n\nMeanAbsoluteError: 0.0607487186789513 | Loss: 0.0072420141933011 | Epoch: 1451 | \n\n\nMeanAbsoluteError: 0.0568230040371418 | Loss: 0.0062379391867134 | Epoch: 1452 | \n\n\nMeanAbsoluteError: 0.0592043064534664 | Loss: 0.0068653943131418 | Epoch: 1453 | \n\n\nMeanAbsoluteError: 0.0585530735552311 | Loss: 0.0064114884527817 | Epoch: 1454 | \n\n\nMeanAbsoluteError: 0.0612958595156670 | Loss: 0.0072774865300304 | Epoch: 1455 | \n\n\nMeanAbsoluteError: 0.0544577911496162 | Loss: 0.0060185654557336 | Epoch: 1456 | \n\n\nMeanAbsoluteError: 0.0564851574599743 | Loss: 0.0063838139343534 | Epoch: 1457 | \n\n\nMeanAbsoluteError: 0.0590332671999931 | Loss: 0.0063706257057129 | Epoch: 1458 | \n\n\nMeanAbsoluteError: 0.0602172650396824 | Loss: 0.0069457742053298 | Epoch: 1459 | \n\n\nMeanAbsoluteError: 0.0570246130228043 | Loss: 0.0067005614480392 | Epoch: 1460 | \n\n\nMeanAbsoluteError: 0.0588791333138943 | Loss: 0.0072759615967758 | Epoch: 1461 | \n\n\nMeanAbsoluteError: 0.0581545270979404 | Loss: 0.0067146958703233 | Epoch: 1462 | \n\n\nMeanAbsoluteError: 0.0560465417802334 | Loss: 0.0062615142924309 | Epoch: 1463 | \n\n\nMeanAbsoluteError: 0.0567421838641167 | Loss: 0.0061797196029484 | Epoch: 1464 | \n\n\nMeanAbsoluteError: 0.0608792416751385 | Loss: 0.0081759539295005 | Epoch: 1465 | \n\n\nMeanAbsoluteError: 0.0540264844894409 | Loss: 0.0056628065039543 | Epoch: 1466 | \n\n\nMeanAbsoluteError: 0.0626941993832588 | Loss: 0.0077584880123080 | Epoch: 1467 | \n\n\nMeanAbsoluteError: 0.0609045401215553 | Loss: 0.0070329850789130 | Epoch: 1468 | \n\n\nMeanAbsoluteError: 0.0571609176695347 | Loss: 0.0061745644482289 | Epoch: 1469 | \n\n\nMeanAbsoluteError: 0.0572561584413052 | Loss: 0.0063625234858288 | Epoch: 1470 | \n\n\nMeanAbsoluteError: 0.0561894178390503 | Loss: 0.0058469102583937 | Epoch: 1471 | \n\n\nMeanAbsoluteError: 0.0555493533611298 | Loss: 0.0059518529471825 | Epoch: 1472 | \n\n\nMeanAbsoluteError: 0.0607462823390961 | Loss: 0.0071936794825706 | Epoch: 1473 | \n\n\nMeanAbsoluteError: 0.0591857545077801 | Loss: 0.0064904925536151 | Epoch: 1474 | \n\n\nMeanAbsoluteError: 0.0567146874964237 | Loss: 0.0064657233679281 | Epoch: 1475 | \n\n\nMeanAbsoluteError: 0.0583138130605221 | Loss: 0.0067540435390644 | Epoch: 1476 | \n\n\nMeanAbsoluteError: 0.0585293248295784 | Loss: 0.0068784701071369 | Epoch: 1477 | \n\n\nMeanAbsoluteError: 0.0550506263971329 | Loss: 0.0060790875371458 | Epoch: 1478 | \n\n\nMeanAbsoluteError: 0.0583598949015141 | Loss: 0.0067877414921289 | Epoch: 1479 | \n\n\nMeanAbsoluteError: 0.0597087480127811 | Loss: 0.0073796428007214 | Epoch: 1480 | \n\n\nMeanAbsoluteError: 0.0621837303042412 | Loss: 0.0079900910481892 | Epoch: 1481 | \n\n\nMeanAbsoluteError: 0.0561173073947430 | Loss: 0.0060663188176962 | Epoch: 1482 | \n\n\nMeanAbsoluteError: 0.0636659339070320 | Loss: 0.0088170848041288 | Epoch: 1483 | \n\n\nMeanAbsoluteError: 0.0564384013414383 | Loss: 0.0059670863603477 | Epoch: 1484 | \n\n\nMeanAbsoluteError: 0.0598085597157478 | Loss: 0.0070228538805926 | Epoch: 1485 | \n\n\nMeanAbsoluteError: 0.0620708391070366 | Loss: 0.0069964616883317 | Epoch: 1486 | \n\n\nMeanAbsoluteError: 0.0593630708754063 | Loss: 0.0067587251235940 | Epoch: 1487 | \n\n\nMeanAbsoluteError: 0.0571919828653336 | Loss: 0.0061245596024204 | Epoch: 1488 | \n\n\nMeanAbsoluteError: 0.0554173402488232 | Loss: 0.0058075080491593 | Epoch: 1489 | \n\n\nMeanAbsoluteError: 0.0617776736617088 | Loss: 0.0074278251774861 | Epoch: 1490 | \n\n\nMeanAbsoluteError: 0.0583424568176270 | Loss: 0.0071185579402209 | Epoch: 1491 | \n\n\nMeanAbsoluteError: 0.0590678416192532 | Loss: 0.0060316182867488 | Epoch: 1492 | \n\n\nMeanAbsoluteError: 0.0611250065267086 | Loss: 0.0079489372973330 | Epoch: 1493 | \n\n\nMeanAbsoluteError: 0.0575046427547932 | Loss: 0.0065379245016932 | Epoch: 1494 | \n\n\nMeanAbsoluteError: 0.0599452257156372 | Loss: 0.0066323907016795 | Epoch: 1495 | \n\n\nMeanAbsoluteError: 0.0564880259335041 | Loss: 0.0066275868277929 | Epoch: 1496 | \n\n\nMeanAbsoluteError: 0.0607132352888584 | Loss: 0.0073421649497444 | Epoch: 1497 | \n\n\nMeanAbsoluteError: 0.0550044700503349 | Loss: 0.0061235469804766 | Epoch: 1498 | \n\n\nMeanAbsoluteError: 0.0589673109352589 | Loss: 0.0064349750632149 | Epoch: 1499 | \n\n\nMeanAbsoluteError: 0.0623871684074402 | Loss: 0.0072864257859813 | Epoch: 1500 | \n\n\nMeanAbsoluteError: 0.0584965832531452 | Loss: 0.0062126975714151 | Epoch: 1501 | \n\n\nMeanAbsoluteError: 0.0581140071153641 | Loss: 0.0069907736621947 | Epoch: 1502 | \n\n\nMeanAbsoluteError: 0.0585397519171238 | Loss: 0.0071351458445019 | Epoch: 1503 | \n\n\nMeanAbsoluteError: 0.0566186830401421 | Loss: 0.0058102885575984 | Epoch: 1504 | \n\n\nMeanAbsoluteError: 0.0514515340328217 | Loss: 0.0053346174557494 | Epoch: 1505 | \n\n\nMeanAbsoluteError: 0.0588541924953461 | Loss: 0.0067322062558378 | Epoch: 1506 | \n\n\nMeanAbsoluteError: 0.0611049979925156 | Loss: 0.0073007947087838 | Epoch: 1507 | \n\n\nMeanAbsoluteError: 0.0590155012905598 | Loss: 0.0064723473369365 | Epoch: 1508 | \n\n\nMeanAbsoluteError: 0.0556226521730423 | Loss: 0.0057251397646299 | Epoch: 1509 | \n\n\nMeanAbsoluteError: 0.0592522881925106 | Loss: 0.0064128320958844 | Epoch: 1510 | \n\n\nMeanAbsoluteError: 0.0573557280004025 | Loss: 0.0065038149802482 | Epoch: 1511 | \n\n\nMeanAbsoluteError: 0.0570864304900169 | Loss: 0.0072322448862603 | Epoch: 1512 | \n\n\nMeanAbsoluteError: 0.0560695976018906 | Loss: 0.0063441371824107 | Epoch: 1513 | \n\n\nMeanAbsoluteError: 0.0552160069346428 | Loss: 0.0063130933165060 | Epoch: 1514 | \n\n\nMeanAbsoluteError: 0.0563834272325039 | Loss: 0.0058760714143621 | Epoch: 1515 | \n\n\nMeanAbsoluteError: 0.0590880326926708 | Loss: 0.0068554726526054 | Epoch: 1516 | \n\n\nMeanAbsoluteError: 0.0542897917330265 | Loss: 0.0058576164690142 | Epoch: 1517 | \n\n\nMeanAbsoluteError: 0.0572415851056576 | Loss: 0.0074241557133185 | Epoch: 1518 | \n\n\nMeanAbsoluteError: 0.0584487095475197 | Loss: 0.0063925747815908 | Epoch: 1519 | \n\n\nMeanAbsoluteError: 0.0612252615392208 | Loss: 0.0078249099850132 | Epoch: 1520 | \n\n\nMeanAbsoluteError: 0.0616814605891705 | Loss: 0.0083344935874144 | Epoch: 1521 | \n\n\nMeanAbsoluteError: 0.0601919032633305 | Loss: 0.0075178123542203 | Epoch: 1522 | \n\n\nMeanAbsoluteError: 0.0590008608996868 | Loss: 0.0062044191520060 | Epoch: 1523 | \n\n\nMeanAbsoluteError: 0.0584641247987747 | Loss: 0.0067185101495609 | Epoch: 1524 | \n\n\nMeanAbsoluteError: 0.0617752708494663 | Loss: 0.0081977045831703 | Epoch: 1525 | \n\n\nMeanAbsoluteError: 0.0592674463987350 | Loss: 0.0069768544348085 | Epoch: 1526 | \n\n\nMeanAbsoluteError: 0.0595626719295979 | Loss: 0.0064878778460358 | Epoch: 1527 | \n\n\nMeanAbsoluteError: 0.0582184717059135 | Loss: 0.0066545080789971 | Epoch: 1528 | \n\n\nMeanAbsoluteError: 0.0545827113091946 | Loss: 0.0056743257863976 | Epoch: 1529 | \n\n\nMeanAbsoluteError: 0.0565802901983261 | Loss: 0.0066049661677486 | Epoch: 1530 | \n\n\nMeanAbsoluteError: 0.0585284158587456 | Loss: 0.0073259312756242 | Epoch: 1531 | \n\n\nMeanAbsoluteError: 0.0606790930032730 | Loss: 0.0074857515947527 | Epoch: 1532 | \n\n\nMeanAbsoluteError: 0.0599543713033199 | Loss: 0.0073707027018827 | Epoch: 1533 | \n\n\nMeanAbsoluteError: 0.0589322149753571 | Loss: 0.0065160046357172 | Epoch: 1534 | \n\n\nMeanAbsoluteError: 0.0624856390058994 | Loss: 0.0072933966877342 | Epoch: 1535 | \n\n\nMeanAbsoluteError: 0.0581333935260773 | Loss: 0.0065054212175104 | Epoch: 1536 | \n\n\nMeanAbsoluteError: 0.0588551536202431 | Loss: 0.0072181104060989 | Epoch: 1537 | \n\n\nMeanAbsoluteError: 0.0593400374054909 | Loss: 0.0068486307815086 | Epoch: 1538 | \n\n\nMeanAbsoluteError: 0.0538115948438644 | Loss: 0.0055438067286741 | Epoch: 1539 | \n\n\nMeanAbsoluteError: 0.0617442838847637 | Loss: 0.0079487399504069 | Epoch: 1540 | \n\n\nMeanAbsoluteError: 0.0526672117412090 | Loss: 0.0062005978906988 | Epoch: 1541 | \n\n\nMeanAbsoluteError: 0.0551434978842735 | Loss: 0.0061382774560752 | Epoch: 1542 | \n\n\nMeanAbsoluteError: 0.0573317520320415 | Loss: 0.0065059656192170 | Epoch: 1543 | \n\n\nMeanAbsoluteError: 0.0577440522611141 | Loss: 0.0074270219938262 | Epoch: 1544 | \n\n\nMeanAbsoluteError: 0.0573340430855751 | Loss: 0.0063662101831081 | Epoch: 1545 | \n\n\nMeanAbsoluteError: 0.0588192604482174 | Loss: 0.0074278733579922 | Epoch: 1546 | \n\n\nMeanAbsoluteError: 0.0537186935544014 | Loss: 0.0056983283015385 | Epoch: 1547 | \n\n\nMeanAbsoluteError: 0.0525781363248825 | Loss: 0.0057567028981551 | Epoch: 1548 | \n\n\nMeanAbsoluteError: 0.0564888082444668 | Loss: 0.0060897070422470 | Epoch: 1549 | \n\n\nMeanAbsoluteError: 0.0597775205969810 | Loss: 0.0068713635826073 | Epoch: 1550 | \n\n\nMeanAbsoluteError: 0.0564454644918442 | Loss: 0.0061874540848900 | Epoch: 1551 | \n\n\nMeanAbsoluteError: 0.0543310679495335 | Loss: 0.0058398441626923 | Epoch: 1552 | \n\n\nMeanAbsoluteError: 0.0549813471734524 | Loss: 0.0059277948390036 | Epoch: 1553 | \n\n\nMeanAbsoluteError: 0.0559862144291401 | Loss: 0.0062820557449838 | Epoch: 1554 | \n\n\nMeanAbsoluteError: 0.0593153573572636 | Loss: 0.0076089804630465 | Epoch: 1555 | \n\n\nMeanAbsoluteError: 0.0556497462093830 | Loss: 0.0057446500171985 | Epoch: 1556 | \n\n\nMeanAbsoluteError: 0.0556917935609818 | Loss: 0.0063098347948471 | Epoch: 1557 | \n\n\nMeanAbsoluteError: 0.0591951869428158 | Loss: 0.0067199304193370 | Epoch: 1558 | \n\n\nMeanAbsoluteError: 0.0565958134829998 | Loss: 0.0069277433506553 | Epoch: 1559 | \n\n\nMeanAbsoluteError: 0.0585650578141212 | Loss: 0.0069038028959282 | Epoch: 1560 | \n\n\nMeanAbsoluteError: 0.0566771440207958 | Loss: 0.0063669216869948 | Epoch: 1561 | \n\n\nMeanAbsoluteError: 0.0584382638335228 | Loss: 0.0067343108001448 | Epoch: 1562 | \n\n\nMeanAbsoluteError: 0.0551296621561050 | Loss: 0.0064312993721918 | Epoch: 1563 | \n\n\nMeanAbsoluteError: 0.0582252107560635 | Loss: 0.0069283715624715 | Epoch: 1564 | \n\n\nMeanAbsoluteError: 0.0562768690288067 | Loss: 0.0064639076084541 | Epoch: 1565 | \n\n\nMeanAbsoluteError: 0.0568633824586868 | Loss: 0.0068642903705283 | Epoch: 1566 | \n\n\nMeanAbsoluteError: 0.0574978515505791 | Loss: 0.0067841521415418 | Epoch: 1567 | \n\n\nMeanAbsoluteError: 0.0511563457548618 | Loss: 0.0054461890127823 | Epoch: 1568 | \n\n\nMeanAbsoluteError: 0.0595544800162315 | Loss: 0.0075012604155442 | Epoch: 1569 | \n\n\nMeanAbsoluteError: 0.0598609335720539 | Loss: 0.0071616695215622 | Epoch: 1570 | \n\n\nMeanAbsoluteError: 0.0580797083675861 | Loss: 0.0063938814660651 | Epoch: 1571 | \n\n\nMeanAbsoluteError: 0.0576029270887375 | Loss: 0.0060638986350144 | Epoch: 1572 | \n\n\nMeanAbsoluteError: 0.0553678832948208 | Loss: 0.0061639622226691 | Epoch: 1573 | \n\n\nMeanAbsoluteError: 0.0508598722517490 | Loss: 0.0052943346326841 | Epoch: 1574 | \n\n\nMeanAbsoluteError: 0.0531462430953979 | Loss: 0.0063758181208444 | Epoch: 1575 | \n\n\nMeanAbsoluteError: 0.0591979920864105 | Loss: 0.0070068758429261 | Epoch: 1576 | \n\n\nMeanAbsoluteError: 0.0571466684341431 | Loss: 0.0063588593092057 | Epoch: 1577 | \n\n\nMeanAbsoluteError: 0.0594029948115349 | Loss: 0.0068344125316586 | Epoch: 1578 | \n\n\nMeanAbsoluteError: 0.0559696443378925 | Loss: 0.0061422976451043 | Epoch: 1579 | \n\n\nMeanAbsoluteError: 0.0583123601973057 | Loss: 0.0066640160674069 | Epoch: 1580 | \n\n\nMeanAbsoluteError: 0.0595979578793049 | Loss: 0.0069568857288292 | Epoch: 1581 | \n\n\nMeanAbsoluteError: 0.0582186132669449 | Loss: 0.0065920792843826 | Epoch: 1582 | \n\n\nMeanAbsoluteError: 0.0619518011808395 | Loss: 0.0077717895456256 | Epoch: 1583 | \n\n\nMeanAbsoluteError: 0.0660845786333084 | Loss: 0.0089880954277639 | Epoch: 1584 | \n\n\nMeanAbsoluteError: 0.0604231916368008 | Loss: 0.0070668555911235 | Epoch: 1585 | \n\n\nMeanAbsoluteError: 0.0590038560330868 | Loss: 0.0063789264224397 | Epoch: 1586 | \n\n\nMeanAbsoluteError: 0.0579728707671165 | Loss: 0.0063030432286905 | Epoch: 1587 | \n\n\nMeanAbsoluteError: 0.0586073435842991 | Loss: 0.0071331991134988 | Epoch: 1588 | \n\n\nMeanAbsoluteError: 0.0591963715851307 | Loss: 0.0073250042466013 | Epoch: 1589 | \n\n\nMeanAbsoluteError: 0.0555204264819622 | Loss: 0.0064479182322490 | Epoch: 1590 | \n\n\nMeanAbsoluteError: 0.0595271177589893 | Loss: 0.0069068160131064 | Epoch: 1591 | \n\n\nMeanAbsoluteError: 0.0539468191564083 | Loss: 0.0055526436754008 | Epoch: 1592 | \n\n\nMeanAbsoluteError: 0.0594338476657867 | Loss: 0.0067314726287683 | Epoch: 1593 | \n\n\nMeanAbsoluteError: 0.0612202435731888 | Loss: 0.0071729541219368 | Epoch: 1594 | \n\n\nMeanAbsoluteError: 0.0553525537252426 | Loss: 0.0061720286923264 | Epoch: 1595 | \n\n\nMeanAbsoluteError: 0.0587227232754230 | Loss: 0.0072161349876721 | Epoch: 1596 | \n\n\nMeanAbsoluteError: 0.0581123791635036 | Loss: 0.0071741146810624 | Epoch: 1597 | \n\n\nMeanAbsoluteError: 0.0549004487693310 | Loss: 0.0062787956997696 | Epoch: 1598 | \n\n\nMeanAbsoluteError: 0.0589465126395226 | Loss: 0.0070939499205027 | Epoch: 1599 | \n\n\nMeanAbsoluteError: 0.0565500892698765 | Loss: 0.0059914706796159 | Epoch: 1600 | \n\n\nMeanAbsoluteError: 0.0560108423233032 | Loss: 0.0065569024559287 | Epoch: 1601 | \n\n\nMeanAbsoluteError: 0.0560511723160744 | Loss: 0.0059933562072244 | Epoch: 1602 | \n\n\nMeanAbsoluteError: 0.0526280924677849 | Loss: 0.0054129806314631 | Epoch: 1603 | \n\n\nMeanAbsoluteError: 0.0571338534355164 | Loss: 0.0069253733551644 | Epoch: 1604 | \n\n\nMeanAbsoluteError: 0.0563411712646484 | Loss: 0.0065149287245974 | Epoch: 1605 | \n\n\nMeanAbsoluteError: 0.0538618862628937 | Loss: 0.0057722070405847 | Epoch: 1606 | \n\n\nMeanAbsoluteError: 0.0578503981232643 | Loss: 0.0064657366806690 | Epoch: 1607 | \n\n\nMeanAbsoluteError: 0.0602208375930786 | Loss: 0.0072994894675018 | Epoch: 1608 | \n\n\nMeanAbsoluteError: 0.0537015721201897 | Loss: 0.0058864769622217 | Epoch: 1609 | \n\n\nMeanAbsoluteError: 0.0548165813088417 | Loss: 0.0058949502846129 | Epoch: 1610 | \n\n\nMeanAbsoluteError: 0.0585968606173992 | Loss: 0.0062600534798851 | Epoch: 1611 | \n\n\nMeanAbsoluteError: 0.0596475228667259 | Loss: 0.0072151128257489 | Epoch: 1612 | \n\n\nMeanAbsoluteError: 0.0560741089284420 | Loss: 0.0057985083983355 | Epoch: 1613 | \n\n\nMeanAbsoluteError: 0.0570459291338921 | Loss: 0.0064450579137338 | Epoch: 1614 | \n\n\nMeanAbsoluteError: 0.0557756945490837 | Loss: 0.0056779925660764 | Epoch: 1615 | \n\n\nMeanAbsoluteError: 0.0609859973192215 | Loss: 0.0072346288894672 | Epoch: 1616 | \n\n\nMeanAbsoluteError: 0.0607016682624817 | Loss: 0.0075735672300956 | Epoch: 1617 | \n\n\nMeanAbsoluteError: 0.0619006417691708 | Loss: 0.0077904277738708 | Epoch: 1618 | \n\n\nMeanAbsoluteError: 0.0539200603961945 | Loss: 0.0057725985298991 | Epoch: 1619 | \n\n\nMeanAbsoluteError: 0.0544773153960705 | Loss: 0.0056612047332722 | Epoch: 1620 | \n\n\nMeanAbsoluteError: 0.0553835742175579 | Loss: 0.0054117151297093 | Epoch: 1621 | \n\n\nMeanAbsoluteError: 0.0552757121622562 | Loss: 0.0066715226248562 | Epoch: 1622 | \n\n\nMeanAbsoluteError: 0.0574897900223732 | Loss: 0.0066875571224955 | Epoch: 1623 | \n\n\nMeanAbsoluteError: 0.0556308291852474 | Loss: 0.0062807670517122 | Epoch: 1624 | \n\n\nMeanAbsoluteError: 0.0542036071419716 | Loss: 0.0058883306489709 | Epoch: 1625 | \n\n\nMeanAbsoluteError: 0.0554331317543983 | Loss: 0.0059762205064665 | Epoch: 1626 | \n\n\nMeanAbsoluteError: 0.0569927729666233 | Loss: 0.0068506259529780 | Epoch: 1627 | \n\n\nMeanAbsoluteError: 0.0544589124619961 | Loss: 0.0058211016483134 | Epoch: 1628 | \n\n\nMeanAbsoluteError: 0.0611028485000134 | Loss: 0.0073855958422549 | Epoch: 1629 | \n\n\nMeanAbsoluteError: 0.0570620521903038 | Loss: 0.0065265244283789 | Epoch: 1630 | \n\n\nMeanAbsoluteError: 0.0547689385712147 | Loss: 0.0061875236471860 | Epoch: 1631 | \n\n\nMeanAbsoluteError: 0.0542536862194538 | Loss: 0.0058183936086668 | Epoch: 1632 | \n\n\nMeanAbsoluteError: 0.0553718060255051 | Loss: 0.0066378544155790 | Epoch: 1633 | \n\n\nMeanAbsoluteError: 0.0537652894854546 | Loss: 0.0058181826675976 | Epoch: 1634 | \n\n\nMeanAbsoluteError: 0.0558824203908443 | Loss: 0.0062766616182883 | Epoch: 1635 | \n\n\nMeanAbsoluteError: 0.0631215050816536 | Loss: 0.0073801453908936 | Epoch: 1636 | \n\n\nMeanAbsoluteError: 0.0556124672293663 | Loss: 0.0058967587256120 | Epoch: 1637 | \n\n\nMeanAbsoluteError: 0.0591134913265705 | Loss: 0.0068196285513356 | Epoch: 1638 | \n\n\nMeanAbsoluteError: 0.0580380186438560 | Loss: 0.0063402910194903 | Epoch: 1639 | \n\n\nMeanAbsoluteError: 0.0559827946126461 | Loss: 0.0063289226956476 | Epoch: 1640 | \n\n\nMeanAbsoluteError: 0.0601848587393761 | Loss: 0.0074016132738082 | Epoch: 1641 | \n\n\nMeanAbsoluteError: 0.0495024882256985 | Loss: 0.0052670385120655 | Epoch: 1642 | \n\n\nMeanAbsoluteError: 0.0572557970881462 | Loss: 0.0069569462674796 | Epoch: 1643 | \n\n\nMeanAbsoluteError: 0.0566009469330311 | Loss: 0.0063424466902597 | Epoch: 1644 | \n\n\nMeanAbsoluteError: 0.0590084381401539 | Loss: 0.0077526277595462 | Epoch: 1645 | \n\n\nMeanAbsoluteError: 0.0552432574331760 | Loss: 0.0062941310089506 | Epoch: 1646 | \n\n\nMeanAbsoluteError: 0.0558334141969681 | Loss: 0.0058698034214225 | Epoch: 1647 | \n\n\nMeanAbsoluteError: 0.0562132000923157 | Loss: 0.0058166496353321 | Epoch: 1648 | \n\n\nMeanAbsoluteError: 0.0551449321210384 | Loss: 0.0057520433616204 | Epoch: 1649 | \n\n\nMeanAbsoluteError: 0.0487559922039509 | Loss: 0.0049756763955763 | Epoch: 1650 | \n\n\nMeanAbsoluteError: 0.0599856488406658 | Loss: 0.0071049067593170 | Epoch: 1651 | \n\n\nMeanAbsoluteError: 0.0557973235845566 | Loss: 0.0064810248465377 | Epoch: 1652 | \n\n\nMeanAbsoluteError: 0.0585634224116802 | Loss: 0.0066410742688216 | Epoch: 1653 | \n\n\nMeanAbsoluteError: 0.0515858493745327 | Loss: 0.0054450089862833 | Epoch: 1654 | \n\n\nMeanAbsoluteError: 0.0577606260776520 | Loss: 0.0063662424975882 | Epoch: 1655 | \n\n\nMeanAbsoluteError: 0.0586462095379829 | Loss: 0.0074929432775176 | Epoch: 1656 | \n\n\nMeanAbsoluteError: 0.0552153140306473 | Loss: 0.0058172878704014 | Epoch: 1657 | \n\n\nMeanAbsoluteError: 0.0602266564965248 | Loss: 0.0080534686857815 | Epoch: 1658 | \n\n\nMeanAbsoluteError: 0.0511646009981632 | Loss: 0.0053651618206974 | Epoch: 1659 | \n\n\nMeanAbsoluteError: 0.0577527917921543 | Loss: 0.0076464330198360 | Epoch: 1660 | \n\n\nMeanAbsoluteError: 0.0515875443816185 | Loss: 0.0054905883258592 | Epoch: 1661 | \n\n\nMeanAbsoluteError: 0.0524228550493717 | Loss: 0.0049453062814428 | Epoch: 1662 | \n\n\nMeanAbsoluteError: 0.0530005134642124 | Loss: 0.0052429326689526 | Epoch: 1663 | \n\n\nMeanAbsoluteError: 0.0549740679562092 | Loss: 0.0060118170157875 | Epoch: 1664 | \n\n\nMeanAbsoluteError: 0.0527226366102695 | Loss: 0.0057165209412506 | Epoch: 1665 | \n\n\nMeanAbsoluteError: 0.0555218197405338 | Loss: 0.0059679607162616 | Epoch: 1666 | \n\n\nMeanAbsoluteError: 0.0560717210173607 | Loss: 0.0057731759615490 | Epoch: 1667 | \n\n\nMeanAbsoluteError: 0.0527108684182167 | Loss: 0.0058487113205410 | Epoch: 1668 | \n\n\nMeanAbsoluteError: 0.0539147444069386 | Loss: 0.0058142789137491 | Epoch: 1669 | \n\n\nMeanAbsoluteError: 0.0548561848700047 | Loss: 0.0061971404062084 | Epoch: 1670 | \n\n\nMeanAbsoluteError: 0.0533187426626682 | Loss: 0.0055916303759841 | Epoch: 1671 | \n\n\nMeanAbsoluteError: 0.0580234415829182 | Loss: 0.0064365714733382 | Epoch: 1672 | \n\n\nMeanAbsoluteError: 0.0567257553339005 | Loss: 0.0059287970558459 | Epoch: 1673 | \n\n\nMeanAbsoluteError: 0.0502965673804283 | Loss: 0.0050029023537354 | Epoch: 1674 | \n\n\nMeanAbsoluteError: 0.0547111593186855 | Loss: 0.0061619703602870 | Epoch: 1675 | \n\n\nMeanAbsoluteError: 0.0536468066275120 | Loss: 0.0055991256012802 | Epoch: 1676 | \n\n\nMeanAbsoluteError: 0.0615705475211143 | Loss: 0.0079045337905215 | Epoch: 1677 | \n\n\nMeanAbsoluteError: 0.0566242784261703 | Loss: 0.0070669428548111 | Epoch: 1678 | \n\n\nMeanAbsoluteError: 0.0562850870192051 | Loss: 0.0069353545134557 | Epoch: 1679 | \n\n\nMeanAbsoluteError: 0.0541872642934322 | Loss: 0.0056858191206023 | Epoch: 1680 | \n\n\nMeanAbsoluteError: 0.0550294108688831 | Loss: 0.0060552540383833 | Epoch: 1681 | \n\n\nMeanAbsoluteError: 0.0512236021459103 | Loss: 0.0053640057839220 | Epoch: 1682 | \n\n\nMeanAbsoluteError: 0.0548242256045341 | Loss: 0.0062414543559650 | Epoch: 1683 | \n\n\nMeanAbsoluteError: 0.0526203140616417 | Loss: 0.0058686330787350 | Epoch: 1684 | \n\n\nMeanAbsoluteError: 0.0575759969651699 | Loss: 0.0065712987329456 | Epoch: 1685 | \n\n\nMeanAbsoluteError: 0.0544109866023064 | Loss: 0.0058211940445517 | Epoch: 1686 | \n\n\nMeanAbsoluteError: 0.0505823194980621 | Loss: 0.0050424731834816 | Epoch: 1687 | \n\n\nMeanAbsoluteError: 0.0530380681157112 | Loss: 0.0057298669071649 | Epoch: 1688 | \n\n\nMeanAbsoluteError: 0.0526227802038193 | Loss: 0.0050933099199756 | Epoch: 1689 | \n\n\nMeanAbsoluteError: 0.0553098358213902 | Loss: 0.0067567156313574 | Epoch: 1690 | \n\n\nMeanAbsoluteError: 0.0603062547743320 | Loss: 0.0076736942207814 | Epoch: 1691 | \n\n\nMeanAbsoluteError: 0.0554349310696125 | Loss: 0.0063141175558000 | Epoch: 1692 | \n\n\nMeanAbsoluteError: 0.0549280047416687 | Loss: 0.0060228403523494 | Epoch: 1693 | \n\n\nMeanAbsoluteError: 0.0599954687058926 | Loss: 0.0076434919223296 | Epoch: 1694 | \n\n\nMeanAbsoluteError: 0.0563484430313110 | Loss: 0.0063657163221554 | Epoch: 1695 | \n\n\nMeanAbsoluteError: 0.0555757060647011 | Loss: 0.0065377301944500 | Epoch: 1696 | \n\n\nMeanAbsoluteError: 0.0568361990153790 | Loss: 0.0065354607671967 | Epoch: 1697 | \n\n\nMeanAbsoluteError: 0.0557150952517986 | Loss: 0.0067962968102681 | Epoch: 1698 | \n\n\nMeanAbsoluteError: 0.0531820170581341 | Loss: 0.0057392618272327 | Epoch: 1699 | \n\n\nMeanAbsoluteError: 0.0573050230741501 | Loss: 0.0068300442088275 | Epoch: 1700 | \n\n\nMeanAbsoluteError: 0.0584871917963028 | Loss: 0.0068644791708842 | Epoch: 1701 | \n\n\nMeanAbsoluteError: 0.0565600320696831 | Loss: 0.0070278698328548 | Epoch: 1702 | \n\n\nMeanAbsoluteError: 0.0543044283986092 | Loss: 0.0058959733843676 | Epoch: 1703 | \n\n\nMeanAbsoluteError: 0.0527439042925835 | Loss: 0.0060185667824893 | Epoch: 1704 | \n\n\nMeanAbsoluteError: 0.0511177219450474 | Loss: 0.0050597149051290 | Epoch: 1705 | \n\n\nMeanAbsoluteError: 0.0530991517007351 | Loss: 0.0059717006178289 | Epoch: 1706 | \n\n\nMeanAbsoluteError: 0.0541766025125980 | Loss: 0.0058771391116534 | Epoch: 1707 | \n\n\nMeanAbsoluteError: 0.0562634579837322 | Loss: 0.0064909814035430 | Epoch: 1708 | \n\n\nMeanAbsoluteError: 0.0513111017644405 | Loss: 0.0052006221542524 | Epoch: 1709 | \n\n\nMeanAbsoluteError: 0.0571211427450180 | Loss: 0.0061892475660594 | Epoch: 1710 | \n\n\nMeanAbsoluteError: 0.0542303100228310 | Loss: 0.0056159967903416 | Epoch: 1711 | \n\n\nMeanAbsoluteError: 0.0523721091449261 | Loss: 0.0054739824898449 | Epoch: 1712 | \n\n\nMeanAbsoluteError: 0.0553244538605213 | Loss: 0.0059681399094673 | Epoch: 1713 | \n\n\nMeanAbsoluteError: 0.0541989468038082 | Loss: 0.0060589304598701 | Epoch: 1714 | \n\n\nMeanAbsoluteError: 0.0493219941854477 | Loss: 0.0046447164854483 | Epoch: 1715 | \n\n\nMeanAbsoluteError: 0.0567266382277012 | Loss: 0.0067200506668147 | Epoch: 1716 | \n\n\nMeanAbsoluteError: 0.0567823350429535 | Loss: 0.0071037505895159 | Epoch: 1717 | \n\n\nMeanAbsoluteError: 0.0542170666158199 | Loss: 0.0064804939832902 | Epoch: 1718 | \n\n\nMeanAbsoluteError: 0.0545531399548054 | Loss: 0.0065328875268582 | Epoch: 1719 | \n\n\nMeanAbsoluteError: 0.0577066689729691 | Loss: 0.0062045657288763 | Epoch: 1720 | \n\n\nMeanAbsoluteError: 0.0572446957230568 | Loss: 0.0071098073739752 | Epoch: 1721 | \n\n\nMeanAbsoluteError: 0.0503896921873093 | Loss: 0.0050893755783181 | Epoch: 1722 | \n\n\nMeanAbsoluteError: 0.0527745932340622 | Loss: 0.0051990514569146 | Epoch: 1723 | \n\n\nMeanAbsoluteError: 0.0540905632078648 | Loss: 0.0064400674802285 | Epoch: 1724 | \n\n\nMeanAbsoluteError: 0.0541576705873013 | Loss: 0.0056869500831211 | Epoch: 1725 | \n\n\nMeanAbsoluteError: 0.0528625734150410 | Loss: 0.0059094270233375 | Epoch: 1726 | \n\n\nMeanAbsoluteError: 0.0501728281378746 | Loss: 0.0047142025915127 | Epoch: 1727 | \n\n\nMeanAbsoluteError: 0.0530073679983616 | Loss: 0.0053925154534227 | Epoch: 1728 | \n\n\nMeanAbsoluteError: 0.0546256825327873 | Loss: 0.0062212071426954 | Epoch: 1729 | \n\n\nMeanAbsoluteError: 0.0547172464430332 | Loss: 0.0059216468853507 | Epoch: 1730 | \n\n\nMeanAbsoluteError: 0.0563973039388657 | Loss: 0.0062609088026511 | Epoch: 1731 | \n\n\nMeanAbsoluteError: 0.0592011436820030 | Loss: 0.0071375068870111 | Epoch: 1732 | \n\n\nMeanAbsoluteError: 0.0574880912899971 | Loss: 0.0067776581654728 | Epoch: 1733 | \n\n\nMeanAbsoluteError: 0.0542982369661331 | Loss: 0.0056808615339348 | Epoch: 1734 | \n\n\nMeanAbsoluteError: 0.0537175945937634 | Loss: 0.0059470562493758 | Epoch: 1735 | \n\n\nMeanAbsoluteError: 0.0543930865824223 | Loss: 0.0055614802878821 | Epoch: 1736 | \n\n\nMeanAbsoluteError: 0.0587329491972923 | Loss: 0.0069043482315556 | Epoch: 1737 | \n\n\nMeanAbsoluteError: 0.0514332354068756 | Loss: 0.0056074491603643 | Epoch: 1738 | \n\n\nMeanAbsoluteError: 0.0539975985884666 | Loss: 0.0061376004513385 | Epoch: 1739 | \n\n\nMeanAbsoluteError: 0.0546318553388119 | Loss: 0.0059011168649825 | Epoch: 1740 | \n\n\nMeanAbsoluteError: 0.0542313195765018 | Loss: 0.0057843312275145 | Epoch: 1741 | \n\n\nMeanAbsoluteError: 0.0549177862703800 | Loss: 0.0061514313343408 | Epoch: 1742 | \n\n\nMeanAbsoluteError: 0.0549786761403084 | Loss: 0.0060841877119674 | Epoch: 1743 | \n\n\nMeanAbsoluteError: 0.0545264370739460 | Loss: 0.0061353571395982 | Epoch: 1744 | \n\n\nMeanAbsoluteError: 0.0503638386726379 | Loss: 0.0053272868832573 | Epoch: 1745 | \n\n\nMeanAbsoluteError: 0.0537457838654518 | Loss: 0.0059170832104011 | Epoch: 1746 | \n\n\nMeanAbsoluteError: 0.0573706701397896 | Loss: 0.0062717274201229 | Epoch: 1747 | \n\n\nMeanAbsoluteError: 0.0562324598431587 | Loss: 0.0064643136919873 | Epoch: 1748 | \n\n\nMeanAbsoluteError: 0.0521775782108307 | Loss: 0.0059382748792693 | Epoch: 1749 | \n\n\nMeanAbsoluteError: 0.0557709112763405 | Loss: 0.0059327313440857 | Epoch: 1750 | \n\n\nMeanAbsoluteError: 0.0588436499238014 | Loss: 0.0068781786101317 | Epoch: 1751 | \n\n\nMeanAbsoluteError: 0.0554587543010712 | Loss: 0.0063508547167817 | Epoch: 1752 | \n\n\nMeanAbsoluteError: 0.0566331371665001 | Loss: 0.0064465753617111 | Epoch: 1753 | \n\n\nMeanAbsoluteError: 0.0559606440365314 | Loss: 0.0067267020143542 | Epoch: 1754 | \n\n\nMeanAbsoluteError: 0.0525003485381603 | Loss: 0.0054616926436574 | Epoch: 1755 | \n\n\nMeanAbsoluteError: 0.0565732717514038 | Loss: 0.0064213594585090 | Epoch: 1756 | \n\n\nMeanAbsoluteError: 0.0549422055482864 | Loss: 0.0059590806525618 | Epoch: 1757 | \n\n\nMeanAbsoluteError: 0.0505712926387787 | Loss: 0.0052414434471393 | Epoch: 1758 | \n\n\nMeanAbsoluteError: 0.0548144355416298 | Loss: 0.0059395047014793 | Epoch: 1759 | \n\n\nMeanAbsoluteError: 0.0516493953764439 | Loss: 0.0056368254161544 | Epoch: 1760 | \n\n\nMeanAbsoluteError: 0.0500401109457016 | Loss: 0.0049944148950453 | Epoch: 1761 | \n\n\nMeanAbsoluteError: 0.0525849573314190 | Loss: 0.0056447821101756 | Epoch: 1762 | \n\n\nMeanAbsoluteError: 0.0572411082684994 | Loss: 0.0069980894210312 | Epoch: 1763 | \n\n\nMeanAbsoluteError: 0.0534650050103664 | Loss: 0.0058687689486881 | Epoch: 1764 | \n\n\nMeanAbsoluteError: 0.0506563223898411 | Loss: 0.0050929434494295 | Epoch: 1765 | \n\n\nMeanAbsoluteError: 0.0514749735593796 | Loss: 0.0051215574166417 | Epoch: 1766 | \n\n\nMeanAbsoluteError: 0.0541000105440617 | Loss: 0.0060020443789896 | Epoch: 1767 | \n\n\nMeanAbsoluteError: 0.0603361651301384 | Loss: 0.0081324399379203 | Epoch: 1768 | \n\n\nMeanAbsoluteError: 0.0485439449548721 | Loss: 0.0046208261836243 | Epoch: 1769 | \n\n\nMeanAbsoluteError: 0.0556120872497559 | Loss: 0.0063639117045422 | Epoch: 1770 | \n\n\nMeanAbsoluteError: 0.0534418411552906 | Loss: 0.0057281750596546 | Epoch: 1771 | \n\n\nMeanAbsoluteError: 0.0537180341780186 | Loss: 0.0058176039548319 | Epoch: 1772 | \n\n\nMeanAbsoluteError: 0.0594886392354965 | Loss: 0.0072189256158648 | Epoch: 1773 | \n\n\nMeanAbsoluteError: 0.0495357848703861 | Loss: 0.0047440489286722 | Epoch: 1774 | \n\n\nMeanAbsoluteError: 0.0520695969462395 | Loss: 0.0049408390678309 | Epoch: 1775 | \n\n\nMeanAbsoluteError: 0.0526261888444424 | Loss: 0.0056645385464465 | Epoch: 1776 | \n\n\nMeanAbsoluteError: 0.0506388247013092 | Loss: 0.0052368093857998 | Epoch: 1777 | \n\n\nMeanAbsoluteError: 0.0529772676527500 | Loss: 0.0058790313867212 | Epoch: 1778 | \n\n\nMeanAbsoluteError: 0.0548219047486782 | Loss: 0.0059880448152884 | Epoch: 1779 | \n\n\nMeanAbsoluteError: 0.0494584292173386 | Loss: 0.0046724281268083 | Epoch: 1780 | \n\n\nMeanAbsoluteError: 0.0553830973803997 | Loss: 0.0064619359597062 | Epoch: 1781 | \n\n\nMeanAbsoluteError: 0.0571145676076412 | Loss: 0.0064782019242542 | Epoch: 1782 | \n\n\nMeanAbsoluteError: 0.0527243502438068 | Loss: 0.0056690358651516 | Epoch: 1783 | \n\n\nMeanAbsoluteError: 0.0605803057551384 | Loss: 0.0077477004756171 | Epoch: 1784 | \n\n\nMeanAbsoluteError: 0.0503293015062809 | Loss: 0.0051697845640319 | Epoch: 1785 | \n\n\nMeanAbsoluteError: 0.0533033385872841 | Loss: 0.0057868798722666 | Epoch: 1786 | \n\n\nMeanAbsoluteError: 0.0524382479488850 | Loss: 0.0053750565193332 | Epoch: 1787 | \n\n\nMeanAbsoluteError: 0.0537499040365219 | Loss: 0.0059100069111325 | Epoch: 1788 | \n\n\nMeanAbsoluteError: 0.0603418275713921 | Loss: 0.0070479806075825 | Epoch: 1789 | \n\n\nMeanAbsoluteError: 0.0553692504763603 | Loss: 0.0061079415130249 | Epoch: 1790 | \n\n\nMeanAbsoluteError: 0.0502296239137650 | Loss: 0.0052315150774727 | Epoch: 1791 | \n\n\nMeanAbsoluteError: 0.0503746345639229 | Loss: 0.0049548101581301 | Epoch: 1792 | \n\n\nMeanAbsoluteError: 0.0524749085307121 | Loss: 0.0054663046812129 | Epoch: 1793 | \n\n\nMeanAbsoluteError: 0.0547716021537781 | Loss: 0.0059567071165490 | Epoch: 1794 | \n\n\nMeanAbsoluteError: 0.0541539378464222 | Loss: 0.0059915939658943 | Epoch: 1795 | \n\n\nMeanAbsoluteError: 0.0537357404828072 | Loss: 0.0060551017538516 | Epoch: 1796 | \n\n\nMeanAbsoluteError: 0.0542064979672432 | Loss: 0.0057125104517218 | Epoch: 1797 | \n\n\nMeanAbsoluteError: 0.0557869784533978 | Loss: 0.0065833726932821 | Epoch: 1798 | \n\n\nMeanAbsoluteError: 0.0517376959323883 | Loss: 0.0057311829050923 | Epoch: 1799 | \n\n\nMeanAbsoluteError: 0.0537721887230873 | Loss: 0.0057913192443145 | Epoch: 1800 | \n\n\nMeanAbsoluteError: 0.0545071810483932 | Loss: 0.0061261364931912 | Epoch: 1801 | \n\n\nMeanAbsoluteError: 0.0541378892958164 | Loss: 0.0058355865727935 | Epoch: 1802 | \n\n\nMeanAbsoluteError: 0.0512549132108688 | Loss: 0.0052817949273776 | Epoch: 1803 | \n\n\nMeanAbsoluteError: 0.0582789555191994 | Loss: 0.0069519099888627 | Epoch: 1804 | \n\n\nMeanAbsoluteError: 0.0592664331197739 | Loss: 0.0066764379077116 | Epoch: 1805 | \n\n\nMeanAbsoluteError: 0.0541085228323936 | Loss: 0.0054435619765233 | Epoch: 1806 | \n\n\nMeanAbsoluteError: 0.0530443079769611 | Loss: 0.0058441754332064 | Epoch: 1807 | \n\n\nMeanAbsoluteError: 0.0541337765753269 | Loss: 0.0057632691668490 | Epoch: 1808 | \n\n\nMeanAbsoluteError: 0.0536297149956226 | Loss: 0.0056934918102221 | Epoch: 1809 | \n\n\nMeanAbsoluteError: 0.0547499768435955 | Loss: 0.0059784451551453 | Epoch: 1810 | \n\n\nMeanAbsoluteError: 0.0531882494688034 | Loss: 0.0059410639992469 | Epoch: 1811 | \n\n\nMeanAbsoluteError: 0.0521989241242409 | Loss: 0.0065113950471527 | Epoch: 1812 | \n\n\nMeanAbsoluteError: 0.0554530918598175 | Loss: 0.0064196280965439 | Epoch: 1813 | \n\n\nMeanAbsoluteError: 0.0545629113912582 | Loss: 0.0067650788255257 | Epoch: 1814 | \n\n\nMeanAbsoluteError: 0.0517791099846363 | Loss: 0.0054986516806290 | Epoch: 1815 | \n\n\nMeanAbsoluteError: 0.0502002686262131 | Loss: 0.0052573464261150 | Epoch: 1816 | \n\n\nMeanAbsoluteError: 0.0539012774825096 | Loss: 0.0059246949300526 | Epoch: 1817 | \n\n\nMeanAbsoluteError: 0.0526082813739777 | Loss: 0.0058490037519490 | Epoch: 1818 | \n\n\nMeanAbsoluteError: 0.0573352053761482 | Loss: 0.0064653025769803 | Epoch: 1819 | \n\n\nMeanAbsoluteError: 0.0575718507170677 | Loss: 0.0062220591710987 | Epoch: 1820 | \n\n\nMeanAbsoluteError: 0.0539355166256428 | Loss: 0.0063837048819672 | Epoch: 1821 | \n\n\nMeanAbsoluteError: 0.0574164390563965 | Loss: 0.0066031256316637 | Epoch: 1822 | \n\n\nMeanAbsoluteError: 0.0542204044759274 | Loss: 0.0066337065923290 | Epoch: 1823 | \n\n\nMeanAbsoluteError: 0.0531250163912773 | Loss: 0.0059558908859193 | Epoch: 1824 | \n\n\nMeanAbsoluteError: 0.0520262047648430 | Loss: 0.0056712002121518 | Epoch: 1825 | \n\n\nMeanAbsoluteError: 0.0503606684505939 | Loss: 0.0053422487081237 | Epoch: 1826 | \n\n\nMeanAbsoluteError: 0.0555749945342541 | Loss: 0.0063651243812577 | Epoch: 1827 | \n\n\nMeanAbsoluteError: 0.0521297566592693 | Loss: 0.0052324630650886 | Epoch: 1828 | \n\n\nMeanAbsoluteError: 0.0501011013984680 | Loss: 0.0050190656788072 | Epoch: 1829 | \n\n\nMeanAbsoluteError: 0.0513907633721828 | Loss: 0.0053857540449705 | Epoch: 1830 | \n\n\nMeanAbsoluteError: 0.0547336190938950 | Loss: 0.0062785684916024 | Epoch: 1831 | \n\n\nMeanAbsoluteError: 0.0562349073588848 | Loss: 0.0069576850374384 | Epoch: 1832 | \n\n\nMeanAbsoluteError: 0.0530915446579456 | Loss: 0.0058403601153259 | Epoch: 1833 | \n\n\nMeanAbsoluteError: 0.0565877594053745 | Loss: 0.0069570799452170 | Epoch: 1834 | \n\n\nMeanAbsoluteError: 0.0528177358210087 | Loss: 0.0062083040141442 | Epoch: 1835 | \n\n\nMeanAbsoluteError: 0.0522513948380947 | Loss: 0.0052859728648764 | Epoch: 1836 | \n\n\nMeanAbsoluteError: 0.0520836114883423 | Loss: 0.0055245701563642 | Epoch: 1837 | \n\n\nMeanAbsoluteError: 0.0523837208747864 | Loss: 0.0057222110172734 | Epoch: 1838 | \n\n\nMeanAbsoluteError: 0.0538205839693546 | Loss: 0.0061255472148817 | Epoch: 1839 | \n\n\nMeanAbsoluteError: 0.0550850406289101 | Loss: 0.0060298559178530 | Epoch: 1840 | \n\n\nMeanAbsoluteError: 0.0564753971993923 | Loss: 0.0064267087302384 | Epoch: 1841 | \n\n\nMeanAbsoluteError: 0.0543031580746174 | Loss: 0.0063252954023331 | Epoch: 1842 | \n\n\nMeanAbsoluteError: 0.0543348044157028 | Loss: 0.0061898249982429 | Epoch: 1843 | \n\n\nMeanAbsoluteError: 0.0554329231381416 | Loss: 0.0057300869396083 | Epoch: 1844 | \n\n\nMeanAbsoluteError: 0.0471373051404953 | Loss: 0.0045674472391867 | Epoch: 1845 | \n\n\nMeanAbsoluteError: 0.0549331903457642 | Loss: 0.0059834132928154 | Epoch: 1846 | \n\n\nMeanAbsoluteError: 0.0591903924942017 | Loss: 0.0072903746837619 | Epoch: 1847 | \n\n\nMeanAbsoluteError: 0.0518199279904366 | Loss: 0.0054999857357992 | Epoch: 1848 | \n\n\nMeanAbsoluteError: 0.0530238710343838 | Loss: 0.0054391766180106 | Epoch: 1849 | \n\n\nMeanAbsoluteError: 0.0508578345179558 | Loss: 0.0057202838912607 | Epoch: 1850 | \n\n\nMeanAbsoluteError: 0.0507235229015350 | Loss: 0.0054135428815698 | Epoch: 1851 | \n\n\nMeanAbsoluteError: 0.0515371859073639 | Loss: 0.0049893991007199 | Epoch: 1852 | \n\n\nMeanAbsoluteError: 0.0556802488863468 | Loss: 0.0062424524410380 | Epoch: 1853 | \n\n\nMeanAbsoluteError: 0.0537615343928337 | Loss: 0.0055336556226879 | Epoch: 1854 | \n\n\nMeanAbsoluteError: 0.0525154322385788 | Loss: 0.0052082025141620 | Epoch: 1855 | \n\n\nMeanAbsoluteError: 0.0530362278223038 | Loss: 0.0054335843020332 | Epoch: 1856 | \n\n\nMeanAbsoluteError: 0.0529441647231579 | Loss: 0.0058744536746114 | Epoch: 1857 | \n\n\nMeanAbsoluteError: 0.0559269785881042 | Loss: 0.0064880601151890 | Epoch: 1858 | \n\n\nMeanAbsoluteError: 0.0544346235692501 | Loss: 0.0059122101341188 | Epoch: 1859 | \n\n\nMeanAbsoluteError: 0.0568367689847946 | Loss: 0.0065442498928799 | Epoch: 1860 | \n\n\nMeanAbsoluteError: 0.0513018444180489 | Loss: 0.0050327327160380 | Epoch: 1861 | \n\n\nMeanAbsoluteError: 0.0521446987986565 | Loss: 0.0053710832865181 | Epoch: 1862 | \n\n\nMeanAbsoluteError: 0.0497689582407475 | Loss: 0.0045862212810486 | Epoch: 1863 | \n\n\nMeanAbsoluteError: 0.0516411513090134 | Loss: 0.0052191951124769 | Epoch: 1864 | \n\n\nMeanAbsoluteError: 0.0561211779713631 | Loss: 0.0062764773524759 | Epoch: 1865 | \n\n\nMeanAbsoluteError: 0.0564737319946289 | Loss: 0.0069082407238481 | Epoch: 1866 | \n\n\nMeanAbsoluteError: 0.0541575178503990 | Loss: 0.0056320949001868 | Epoch: 1867 | \n\n\nMeanAbsoluteError: 0.0532288178801537 | Loss: 0.0061020349718213 | Epoch: 1868 | \n\n\nMeanAbsoluteError: 0.0513834208250046 | Loss: 0.0051749415482118 | Epoch: 1869 | \n\n\nMeanAbsoluteError: 0.0534313581883907 | Loss: 0.0061250821310508 | Epoch: 1870 | \n\n\nMeanAbsoluteError: 0.0515962764620781 | Loss: 0.0056060672349122 | Epoch: 1871 | \n\n\nMeanAbsoluteError: 0.0525446683168411 | Loss: 0.0055727876708473 | Epoch: 1872 | \n\n\nMeanAbsoluteError: 0.0535247288644314 | Loss: 0.0061441371789139 | Epoch: 1873 | \n\n\nMeanAbsoluteError: 0.0529469810426235 | Loss: 0.0054363000352411 | Epoch: 1874 | \n\n\nMeanAbsoluteError: 0.0518481098115444 | Loss: 0.0054529537681204 | Epoch: 1875 | \n\n\nMeanAbsoluteError: 0.0510464608669281 | Loss: 0.0057128720828769 | Epoch: 1876 | \n\n\nMeanAbsoluteError: 0.0498599037528038 | Loss: 0.0052804996416186 | Epoch: 1877 | \n\n\nMeanAbsoluteError: 0.0490738339722157 | Loss: 0.0049536435248441 | Epoch: 1878 | \n\n\nMeanAbsoluteError: 0.0527827776968479 | Loss: 0.0057644256282917 | Epoch: 1879 | \n\n\nMeanAbsoluteError: 0.0538446940481663 | Loss: 0.0052919105778468 | Epoch: 1880 | \n\n\nMeanAbsoluteError: 0.0517316721379757 | Loss: 0.0053640982490469 | Epoch: 1881 | \n\n\nMeanAbsoluteError: 0.0581483468413353 | Loss: 0.0071078038306465 | Epoch: 1882 | \n\n\nMeanAbsoluteError: 0.0519385486841202 | Loss: 0.0054316737881406 | Epoch: 1883 | \n\n\nMeanAbsoluteError: 0.0539497509598732 | Loss: 0.0057275392644563 | Epoch: 1884 | \n\n\nMeanAbsoluteError: 0.0507662370800972 | Loss: 0.0054251843150087 | Epoch: 1885 | \n\n\nMeanAbsoluteError: 0.0594491250813007 | Loss: 0.0070897351357689 | Epoch: 1886 | \n\n\nMeanAbsoluteError: 0.0510000176727772 | Loss: 0.0050094671346127 | Epoch: 1887 | \n\n\nMeanAbsoluteError: 0.0582648962736130 | Loss: 0.0079761301059868 | Epoch: 1888 | \n\n\nMeanAbsoluteError: 0.0578624233603477 | Loss: 0.0063868838330920 | Epoch: 1889 | \n\n\nMeanAbsoluteError: 0.0622499324381351 | Loss: 0.0072929452470999 | Epoch: 1890 | \n\n\nMeanAbsoluteError: 0.0535569190979004 | Loss: 0.0065637214242694 | Epoch: 1891 | \n\n\nMeanAbsoluteError: 0.0499526150524616 | Loss: 0.0050800881215885 | Epoch: 1892 | \n\n\nMeanAbsoluteError: 0.0528640709817410 | Loss: 0.0052865316640721 | Epoch: 1893 | \n\n\nMeanAbsoluteError: 0.0493415184319019 | Loss: 0.0053568917597780 | Epoch: 1894 | \n\n\nMeanAbsoluteError: 0.0548605360090733 | Loss: 0.0064600867632786 | Epoch: 1895 | \n\n\nMeanAbsoluteError: 0.0558091662824154 | Loss: 0.0059210510173095 | Epoch: 1896 | \n\n\nMeanAbsoluteError: 0.0539606027305126 | Loss: 0.0061218105142446 | Epoch: 1897 | \n\n\nMeanAbsoluteError: 0.0465786568820477 | Loss: 0.0045029136724770 | Epoch: 1898 | \n\n\nMeanAbsoluteError: 0.0536297485232353 | Loss: 0.0057186065324671 | Epoch: 1899 | \n\n\nMeanAbsoluteError: 0.0545922592282295 | Loss: 0.0063300321613754 | Epoch: 1900 | \n\n\nMeanAbsoluteError: 0.0524322353303432 | Loss: 0.0055022514983042 | Epoch: 1901 | \n\n\nMeanAbsoluteError: 0.0518556423485279 | Loss: 0.0054492853991542 | Epoch: 1902 | \n\n\nMeanAbsoluteError: 0.0460865795612335 | Loss: 0.0044683223122653 | Epoch: 1903 | \n\n\nMeanAbsoluteError: 0.0519580021500587 | Loss: 0.0055364320434091 | Epoch: 1904 | \n\n\nMeanAbsoluteError: 0.0537593029439449 | Loss: 0.0065458786177017 | Epoch: 1905 | \n\n\nMeanAbsoluteError: 0.0537641979753971 | Loss: 0.0059945746904547 | Epoch: 1906 | \n\n\nMeanAbsoluteError: 0.0545641519129276 | Loss: 0.0066227706610516 | Epoch: 1907 | \n\n\nMeanAbsoluteError: 0.0508891977369785 | Loss: 0.0057091281888885 | Epoch: 1908 | \n\n\nMeanAbsoluteError: 0.0509221740067005 | Loss: 0.0057188268239315 | Epoch: 1909 | \n\n\nMeanAbsoluteError: 0.0497599877417088 | Loss: 0.0052864782051741 | Epoch: 1910 | \n\n\nMeanAbsoluteError: 0.0527534671127796 | Loss: 0.0055509042962149 | Epoch: 1911 | \n\n\nMeanAbsoluteError: 0.0541079342365265 | Loss: 0.0060154323532955 | Epoch: 1912 | \n\n\nMeanAbsoluteError: 0.0521735325455666 | Loss: 0.0067607128837396 | Epoch: 1913 | \n\n\nMeanAbsoluteError: 0.0519286915659904 | Loss: 0.0052070139339033 | Epoch: 1914 | \n\n\nMeanAbsoluteError: 0.0514238104224205 | Loss: 0.0050420815338536 | Epoch: 1915 | \n\n\nMeanAbsoluteError: 0.0595226921141148 | Loss: 0.0072247991548329 | Epoch: 1916 | \n\n\nMeanAbsoluteError: 0.0499023348093033 | Loss: 0.0053781505263032 | Epoch: 1917 | \n\n\nMeanAbsoluteError: 0.0536227747797966 | Loss: 0.0064158391062665 | Epoch: 1918 | \n\n\nMeanAbsoluteError: 0.0463785342872143 | Loss: 0.0044535442422542 | Epoch: 1919 | \n\n\nMeanAbsoluteError: 0.0557653047144413 | Loss: 0.0060700687345040 | Epoch: 1920 | \n\n\nMeanAbsoluteError: 0.0528316348791122 | Loss: 0.0059517248364038 | Epoch: 1921 | \n\n\nMeanAbsoluteError: 0.0523693226277828 | Loss: 0.0058455584728942 | Epoch: 1922 | \n\n\nMeanAbsoluteError: 0.0519629940390587 | Loss: 0.0054899213700264 | Epoch: 1923 | \n\n\nMeanAbsoluteError: 0.0511366538703442 | Loss: 0.0052646988915876 | Epoch: 1924 | \n\n\nMeanAbsoluteError: 0.0576793476939201 | Loss: 0.0062738305390424 | Epoch: 1925 | \n\n\nMeanAbsoluteError: 0.0545312799513340 | Loss: 0.0064757262945507 | Epoch: 1926 | \n\n\nMeanAbsoluteError: 0.0482823736965656 | Loss: 0.0044259476983219 | Epoch: 1927 | \n\n\nMeanAbsoluteError: 0.0505558699369431 | Loss: 0.0052388447992053 | Epoch: 1928 | \n\n\nMeanAbsoluteError: 0.0502457246184349 | Loss: 0.0052083005423992 | Epoch: 1929 | \n\n\nMeanAbsoluteError: 0.0499854274094105 | Loss: 0.0050472482850758 | Epoch: 1930 | \n\n\nMeanAbsoluteError: 0.0515144802629948 | Loss: 0.0053744658395832 | Epoch: 1931 | \n\n\nMeanAbsoluteError: 0.0531905144453049 | Loss: 0.0055409813517448 | Epoch: 1932 | \n\n\nMeanAbsoluteError: 0.0527931600809097 | Loss: 0.0061007736717033 | Epoch: 1933 | \n\n\nMeanAbsoluteError: 0.0504168272018433 | Loss: 0.0047543264884492 | Epoch: 1934 | \n\n\nMeanAbsoluteError: 0.0523778051137924 | Loss: 0.0065524441458183 | Epoch: 1935 | \n\n\nMeanAbsoluteError: 0.0517104417085648 | Loss: 0.0064413694373798 | Epoch: 1936 | \n\n\nMeanAbsoluteError: 0.0545302443206310 | Loss: 0.0062657477656338 | Epoch: 1937 | \n\n\nMeanAbsoluteError: 0.0518252439796925 | Loss: 0.0056798428789140 | Epoch: 1938 | \n\n\nMeanAbsoluteError: 0.0509566552937031 | Loss: 0.0054914870523650 | Epoch: 1939 | \n\n\nMeanAbsoluteError: 0.0517476424574852 | Loss: 0.0057466710892519 | Epoch: 1940 | \n\n\nMeanAbsoluteError: 0.0568683035671711 | Loss: 0.0058977935196162 | Epoch: 1941 | \n\n\nMeanAbsoluteError: 0.0543791055679321 | Loss: 0.0062220578521313 | Epoch: 1942 | \n\n\nMeanAbsoluteError: 0.0475735068321228 | Loss: 0.0043291371798599 | Epoch: 1943 | \n\n\nMeanAbsoluteError: 0.0555602274835110 | Loss: 0.0061206078115477 | Epoch: 1944 | \n\n\nMeanAbsoluteError: 0.0558293536305428 | Loss: 0.0065410597893909 | Epoch: 1945 | \n\n\nMeanAbsoluteError: 0.0502003282308578 | Loss: 0.0050376824795724 | Epoch: 1946 | \n\n\nMeanAbsoluteError: 0.0523294471204281 | Loss: 0.0054950224288723 | Epoch: 1947 | \n\n\nMeanAbsoluteError: 0.0556845664978027 | Loss: 0.0061711000780391 | Epoch: 1948 | \n\n\nMeanAbsoluteError: 0.0496158041059971 | Loss: 0.0047182382809357 | Epoch: 1949 | \n\n\nMeanAbsoluteError: 0.0493911355733871 | Loss: 0.0048738983728617 | Epoch: 1950 | \n\n\nMeanAbsoluteError: 0.0521661676466465 | Loss: 0.0052264236305200 | Epoch: 1951 | \n\n\nMeanAbsoluteError: 0.0510238893330097 | Loss: 0.0052746698403401 | Epoch: 1952 | \n\n\nMeanAbsoluteError: 0.0519300363957882 | Loss: 0.0054733926016343 | Epoch: 1953 | \n\n\nMeanAbsoluteError: 0.0507193803787231 | Loss: 0.0051012385212137 | Epoch: 1954 | \n\n\nMeanAbsoluteError: 0.0511028915643692 | Loss: 0.0061207763418255 | Epoch: 1955 | \n\n\nMeanAbsoluteError: 0.0525621287524700 | Loss: 0.0051209472644223 | Epoch: 1956 | \n\n\nMeanAbsoluteError: 0.0556977353990078 | Loss: 0.0057821692129316 | Epoch: 1957 | \n\n\nMeanAbsoluteError: 0.0520627275109291 | Loss: 0.0057241559392035 | Epoch: 1958 | \n\n\nMeanAbsoluteError: 0.0513293333351612 | Loss: 0.0053976025261606 | Epoch: 1959 | \n\n\nMeanAbsoluteError: 0.0516766645014286 | Loss: 0.0053554129007171 | Epoch: 1960 | \n\n\nMeanAbsoluteError: 0.0465607866644859 | Loss: 0.0043287458044930 | Epoch: 1961 | \n\n\nMeanAbsoluteError: 0.0488069951534271 | Loss: 0.0051583761923871 | Epoch: 1962 | \n\n\nMeanAbsoluteError: 0.0512076169252396 | Loss: 0.0051233274443166 | Epoch: 1963 | \n\n\nMeanAbsoluteError: 0.0505892448127270 | Loss: 0.0051322595794045 | Epoch: 1964 | \n\n\nMeanAbsoluteError: 0.0503406971693039 | Loss: 0.0051983977904964 | Epoch: 1965 | \n\n\nMeanAbsoluteError: 0.0505575165152550 | Loss: 0.0051773238760385 | Epoch: 1966 | \n\n\nMeanAbsoluteError: 0.0486222431063652 | Loss: 0.0053346830229566 | Epoch: 1967 | \n\n\nMeanAbsoluteError: 0.0511637330055237 | Loss: 0.0053092271830246 | Epoch: 1968 | \n\n\nMeanAbsoluteError: 0.0547820851206779 | Loss: 0.0062971281913311 | Epoch: 1969 | \n\n\nMeanAbsoluteError: 0.0566340945661068 | Loss: 0.0066842389467259 | Epoch: 1970 | \n\n\nMeanAbsoluteError: 0.0573518499732018 | Loss: 0.0065943920934903 | Epoch: 1971 | \n\n\nMeanAbsoluteError: 0.0501427836716175 | Loss: 0.0051892218494838 | Epoch: 1972 | \n\n\nMeanAbsoluteError: 0.0487446077167988 | Loss: 0.0047076305232137 | Epoch: 1973 | \n\n\nMeanAbsoluteError: 0.0503676906228065 | Loss: 0.0049584849183824 | Epoch: 1974 | \n\n\nMeanAbsoluteError: 0.0532350651919842 | Loss: 0.0054826444249799 | Epoch: 1975 | \n\n\nMeanAbsoluteError: 0.0509713068604469 | Loss: 0.0052428236465736 | Epoch: 1976 | \n\n\nMeanAbsoluteError: 0.0506746843457222 | Loss: 0.0050513034447416 | Epoch: 1977 | \n\n\nMeanAbsoluteError: 0.0505817048251629 | Loss: 0.0051665769809491 | Epoch: 1978 | \n\n\nMeanAbsoluteError: 0.0553724877536297 | Loss: 0.0065721242407017 | Epoch: 1979 | \n\n\nMeanAbsoluteError: 0.0506308414041996 | Loss: 0.0051380624868519 | Epoch: 1980 | \n\n\nMeanAbsoluteError: 0.0506252907216549 | Loss: 0.0056433085003058 | Epoch: 1981 | \n\n\nMeanAbsoluteError: 0.0550810620188713 | Loss: 0.0065067624881825 | Epoch: 1982 | \n\n\nMeanAbsoluteError: 0.0512991063296795 | Loss: 0.0054416606501703 | Epoch: 1983 | \n\n\nMeanAbsoluteError: 0.0509408265352249 | Loss: 0.0055333391686539 | Epoch: 1984 | \n\n\nMeanAbsoluteError: 0.0505401529371738 | Loss: 0.0051888331203069 | Epoch: 1985 | \n\n\nMeanAbsoluteError: 0.0518969371914864 | Loss: 0.0051683830933386 | Epoch: 1986 | \n\n\nMeanAbsoluteError: 0.0485664121806622 | Loss: 0.0051874331858608 | Epoch: 1987 | \n\n\nMeanAbsoluteError: 0.0541962236166000 | Loss: 0.0061776286277563 | Epoch: 1988 | \n\n\nMeanAbsoluteError: 0.0475922711193562 | Loss: 0.0049668614289294 | Epoch: 1989 | \n\n\nMeanAbsoluteError: 0.0521477237343788 | Loss: 0.0052548892481324 | Epoch: 1990 | \n\n\nMeanAbsoluteError: 0.0495453067123890 | Loss: 0.0052677779108967 | Epoch: 1991 | \n\n\nMeanAbsoluteError: 0.0535094141960144 | Loss: 0.0056645291081804 | Epoch: 1992 | \n\n\nMeanAbsoluteError: 0.0529265701770782 | Loss: 0.0053592371295478 | Epoch: 1993 | \n\n\nMeanAbsoluteError: 0.0492882020771503 | Loss: 0.0051059785340836 | Epoch: 1994 | \n\n\nMeanAbsoluteError: 0.0520598813891411 | Loss: 0.0055736418291356 | Epoch: 1995 | \n\n\nMeanAbsoluteError: 0.0521604195237160 | Loss: 0.0053161430437103 | Epoch: 1996 | \n\n\nMeanAbsoluteError: 0.0471750423312187 | Loss: 0.0046975576195230 | Epoch: 1997 | \n\n\nMeanAbsoluteError: 0.0547370538115501 | Loss: 0.0059495966487035 | Epoch: 1998 | \n\n\nMeanAbsoluteError: 0.0561044514179230 | Loss: 0.0063704330073233 | Epoch: 1999 | \n\n\nMeanAbsoluteError: 0.0508149825036526 | Loss: 0.0054598792628046 | Epoch: 2000 | \n\n\nMeanAbsoluteError: 0.0564102046191692 | Loss: 0.0069641909043154 | Epoch: 2001 | \n\n\nMeanAbsoluteError: 0.0514730624854565 | Loss: 0.0055662430194858 | Epoch: 2002 | \n\n\nMeanAbsoluteError: 0.0531044565141201 | Loss: 0.0058131970101628 | Epoch: 2003 | \n\n\nMeanAbsoluteError: 0.0484006404876709 | Loss: 0.0049598873742555 | Epoch: 2004 | \n\n\nMeanAbsoluteError: 0.0494129359722137 | Loss: 0.0046089630755584 | Epoch: 2005 | \n\n\nMeanAbsoluteError: 0.0510955676436424 | Loss: 0.0052582988870548 | Epoch: 2006 | \n\n\nMeanAbsoluteError: 0.0512111224234104 | Loss: 0.0052758243930051 | Epoch: 2007 | \n\n\nMeanAbsoluteError: 0.0518993064761162 | Loss: 0.0058457583921427 | Epoch: 2008 | \n\n\nMeanAbsoluteError: 0.0543324276804924 | Loss: 0.0064301767631271 | Epoch: 2009 | \n\n\nMeanAbsoluteError: 0.0530960336327553 | Loss: 0.0059026273352598 | Epoch: 2010 | \n\n\nMeanAbsoluteError: 0.0552155561745167 | Loss: 0.0061834326045209 | Epoch: 2011 | \n\n\nMeanAbsoluteError: 0.0487168766558170 | Loss: 0.0049005347998142 | Epoch: 2012 | \n\n\nMeanAbsoluteError: 0.0540129654109478 | Loss: 0.0067244078485768 | Epoch: 2013 | \n\n\nMeanAbsoluteError: 0.0506168566644192 | Loss: 0.0053606274849153 | Epoch: 2014 | \n\n\nMeanAbsoluteError: 0.0533688664436340 | Loss: 0.0054071011574433 | Epoch: 2015 | \n\n\nMeanAbsoluteError: 0.0509411953389645 | Loss: 0.0051958686469091 | Epoch: 2016 | \n\n\nMeanAbsoluteError: 0.0542351789772511 | Loss: 0.0061460497850597 | Epoch: 2017 | \n\n\nMeanAbsoluteError: 0.0521504655480385 | Loss: 0.0056615560351444 | Epoch: 2018 | \n\n\nMeanAbsoluteError: 0.0509775876998901 | Loss: 0.0052512839301198 | Epoch: 2019 | \n\n\nMeanAbsoluteError: 0.0541717708110809 | Loss: 0.0069534398616429 | Epoch: 2020 | \n\n\nMeanAbsoluteError: 0.0534142926335335 | Loss: 0.0060335993611140 | Epoch: 2021 | \n\n\nMeanAbsoluteError: 0.0503667816519737 | Loss: 0.0052642841021414 | Epoch: 2022 | \n\n\nMeanAbsoluteError: 0.0492455661296844 | Loss: 0.0048588586818490 | Epoch: 2023 | \n\n\nMeanAbsoluteError: 0.0497297272086143 | Loss: 0.0049008107425258 | Epoch: 2024 | \n\n\nMeanAbsoluteError: 0.0496141761541367 | Loss: 0.0054037134486862 | Epoch: 2025 | \n\n\nMeanAbsoluteError: 0.0469581037759781 | Loss: 0.0046334788053840 | Epoch: 2026 | \n\n\nMeanAbsoluteError: 0.0483034551143646 | Loss: 0.0045094049023949 | Epoch: 2027 | \n\n\nMeanAbsoluteError: 0.0526286885142326 | Loss: 0.0053712516465112 | Epoch: 2028 | \n\n\nMeanAbsoluteError: 0.0543219372630119 | Loss: 0.0058732009872074 | Epoch: 2029 | \n\n\nMeanAbsoluteError: 0.0514067336916924 | Loss: 0.0052766032688002 | Epoch: 2030 | \n\n\nMeanAbsoluteError: 0.0504963696002960 | Loss: 0.0052886846910981 | Epoch: 2031 | \n\n\nMeanAbsoluteError: 0.0505036488175392 | Loss: 0.0053473419952396 | Epoch: 2032 | \n\n\nMeanAbsoluteError: 0.0488204546272755 | Loss: 0.0050345697322139 | Epoch: 2033 | \n\n\nMeanAbsoluteError: 0.0489469170570374 | Loss: 0.0049729787775505 | Epoch: 2034 | \n\n\nMeanAbsoluteError: 0.0510589107871056 | Loss: 0.0054691712521283 | Epoch: 2035 | \n\n\nMeanAbsoluteError: 0.0478337593376637 | Loss: 0.0046376422166304 | Epoch: 2036 | \n\n\nMeanAbsoluteError: 0.0525794588029385 | Loss: 0.0056743151720245 | Epoch: 2037 | \n\n\nMeanAbsoluteError: 0.0472183339297771 | Loss: 0.0043460518347213 | Epoch: 2038 | \n\n\nMeanAbsoluteError: 0.0515209063887596 | Loss: 0.0056997243884689 | Epoch: 2039 | \n\n\nMeanAbsoluteError: 0.0516517907381058 | Loss: 0.0055810822816420 | Epoch: 2040 | \n\n\nMeanAbsoluteError: 0.0497473888099194 | Loss: 0.0050487724701300 | Epoch: 2041 | \n\n\nMeanAbsoluteError: 0.0527808330953121 | Loss: 0.0053147202354982 | Epoch: 2042 | \n\n\nMeanAbsoluteError: 0.0499077551066875 | Loss: 0.0048314340068100 | Epoch: 2043 | \n\n\nMeanAbsoluteError: 0.0528342165052891 | Loss: 0.0059463435237800 | Epoch: 2044 | \n\n\nMeanAbsoluteError: 0.0522359944880009 | Loss: 0.0060030324963282 | Epoch: 2045 | \n\n\nMeanAbsoluteError: 0.0530208200216293 | Loss: 0.0059056973547589 | Epoch: 2046 | \n\n\nMeanAbsoluteError: 0.0490049980580807 | Loss: 0.0046626681515651 | Epoch: 2047 | \n\n\nMeanAbsoluteError: 0.0553079023957253 | Loss: 0.0060460019625498 | Epoch: 2048 | \n\n\nMeanAbsoluteError: 0.0491122193634510 | Loss: 0.0054547886104562 | Epoch: 2049 | \n\n\nMeanAbsoluteError: 0.0470683723688126 | Loss: 0.0048534921421591 | Epoch: 2050 | \n\n\nMeanAbsoluteError: 0.0505293831229210 | Loss: 0.0053166388002955 | Epoch: 2051 | \n\n\nMeanAbsoluteError: 0.0481923818588257 | Loss: 0.0044546060537292 | Epoch: 2052 | \n\n\nMeanAbsoluteError: 0.0549190193414688 | Loss: 0.0059659256985253 | Epoch: 2053 | \n\n\nMeanAbsoluteError: 0.0550779104232788 | Loss: 0.0062507092849728 | Epoch: 2054 | \n\n\nMeanAbsoluteError: 0.0493971817195415 | Loss: 0.0045987191626288 | Epoch: 2055 | \n\n\nMeanAbsoluteError: 0.0494136437773705 | Loss: 0.0050778071487851 | Epoch: 2056 | \n\n\nMeanAbsoluteError: 0.0539003945887089 | Loss: 0.0059544701939700 | Epoch: 2057 | \n\n\nMeanAbsoluteError: 0.0484215579926968 | Loss: 0.0051545273587241 | Epoch: 2058 | \n\n\nMeanAbsoluteError: 0.0486796908080578 | Loss: 0.0049403830999123 | Epoch: 2059 | \n\n\nMeanAbsoluteError: 0.0523946918547153 | Loss: 0.0057292071580984 | Epoch: 2060 | \n\n\nMeanAbsoluteError: 0.0472117699682713 | Loss: 0.0050400102910377 | Epoch: 2061 | \n\n\nMeanAbsoluteError: 0.0495496243238449 | Loss: 0.0052952359876281 | Epoch: 2062 | \n\n\nMeanAbsoluteError: 0.0497122853994370 | Loss: 0.0053860244070506 | Epoch: 2063 | \n\n\nMeanAbsoluteError: 0.0493595935404301 | Loss: 0.0050638134153390 | Epoch: 2064 | \n\n\nMeanAbsoluteError: 0.0493420287966728 | Loss: 0.0050437419972635 | Epoch: 2065 | \n\n\nMeanAbsoluteError: 0.0484863780438900 | Loss: 0.0050904778051624 | Epoch: 2066 | \n\n\nMeanAbsoluteError: 0.0541963763535023 | Loss: 0.0061983644076524 | Epoch: 2067 | \n\n\nMeanAbsoluteError: 0.0494011603295803 | Loss: 0.0049673597692648 | Epoch: 2068 | \n\n\nMeanAbsoluteError: 0.0527238212525845 | Loss: 0.0055360726871246 | Epoch: 2069 | \n\n\nMeanAbsoluteError: 0.0534642972052097 | Loss: 0.0056667058788177 | Epoch: 2070 | \n\n\nMeanAbsoluteError: 0.0548698306083679 | Loss: 0.0056012413902257 | Epoch: 2071 | \n\n\nMeanAbsoluteError: 0.0575713366270065 | Loss: 0.0073693624153081 | Epoch: 2072 | \n\n\nMeanAbsoluteError: 0.0535859949886799 | Loss: 0.0063882647461287 | Epoch: 2073 | \n\n\nMeanAbsoluteError: 0.0495268255472183 | Loss: 0.0054353178566104 | Epoch: 2074 | \n\n\nMeanAbsoluteError: 0.0502927191555500 | Loss: 0.0047319109140183 | Epoch: 2075 | \n\n\nMeanAbsoluteError: 0.0459045432507992 | Loss: 0.0043129685534041 | Epoch: 2076 | \n\n\nMeanAbsoluteError: 0.0543551705777645 | Loss: 0.0068420049652195 | Epoch: 2077 | \n\n\nMeanAbsoluteError: 0.0519080795347691 | Loss: 0.0059481725796953 | Epoch: 2078 | \n\n\nMeanAbsoluteError: 0.0577586740255356 | Loss: 0.0064191583344291 | Epoch: 2079 | \n\n\nMeanAbsoluteError: 0.0490789711475372 | Loss: 0.0048786708045858 | Epoch: 2080 | \n\n\nMeanAbsoluteError: 0.0517160743474960 | Loss: 0.0064439519985065 | Epoch: 2081 | \n\n\nMeanAbsoluteError: 0.0512666590511799 | Loss: 0.0049506952782152 | Epoch: 2082 | \n\n\nMeanAbsoluteError: 0.0549897626042366 | Loss: 0.0055606676929104 | Epoch: 2083 | \n\n\nMeanAbsoluteError: 0.0528894029557705 | Loss: 0.0052974656719986 | Epoch: 2084 | \n\n\nMeanAbsoluteError: 0.0500475056469440 | Loss: 0.0051829523698446 | Epoch: 2085 | \n\n\nMeanAbsoluteError: 0.0508480630815029 | Loss: 0.0057295006642365 | Epoch: 2086 | \n\n\nMeanAbsoluteError: 0.0531814880669117 | Loss: 0.0059393086332905 | Epoch: 2087 | \n\n\nMeanAbsoluteError: 0.0543371662497520 | Loss: 0.0059171247957905 | Epoch: 2088 | \n\n\nMeanAbsoluteError: 0.0485528670251369 | Loss: 0.0045304446061103 | Epoch: 2089 | \n\n\nMeanAbsoluteError: 0.0554311946034431 | Loss: 0.0060421934142020 | Epoch: 2090 | \n\n\nMeanAbsoluteError: 0.0477735586464405 | Loss: 0.0047259818546687 | Epoch: 2091 | \n\n\nMeanAbsoluteError: 0.0540810078382492 | Loss: 0.0060125850820198 | Epoch: 2092 | \n\n\nMeanAbsoluteError: 0.0483526550233364 | Loss: 0.0049243338534628 | Epoch: 2093 | \n\n\nMeanAbsoluteError: 0.0510395132005215 | Loss: 0.0052195704301266 | Epoch: 2094 | \n\n\nMeanAbsoluteError: 0.0473085679113865 | Loss: 0.0045546375474563 | Epoch: 2095 | \n\n\nMeanAbsoluteError: 0.0515376739203930 | Loss: 0.0051899845040195 | Epoch: 2096 | \n\n\nMeanAbsoluteError: 0.0530627854168415 | Loss: 0.0065874394677485 | Epoch: 2097 | \n\n\nMeanAbsoluteError: 0.0550767332315445 | Loss: 0.0060018470629469 | Epoch: 2098 | \n\n\nMeanAbsoluteError: 0.0496176145970821 | Loss: 0.0053481603846270 | Epoch: 2099 | \n\n\nMeanAbsoluteError: 0.0514996647834778 | Loss: 0.0050701476855011 | Epoch: 2100 | \n\n\nMeanAbsoluteError: 0.0482524856925011 | Loss: 0.0048475791496639 | Epoch: 2101 | \n\n\nMeanAbsoluteError: 0.0503344200551510 | Loss: 0.0053506654256368 | Epoch: 2102 | \n\n\nMeanAbsoluteError: 0.0570065379142761 | Loss: 0.0063897902967384 | Epoch: 2103 | \n\n\nMeanAbsoluteError: 0.0526905804872513 | Loss: 0.0057623600227210 | Epoch: 2104 | \n\n\nMeanAbsoluteError: 0.0492489151656628 | Loss: 0.0048617314475511 | Epoch: 2105 | \n\n\nMeanAbsoluteError: 0.0507092662155628 | Loss: 0.0050399959395872 | Epoch: 2106 | \n\n\nMeanAbsoluteError: 0.0474940873682499 | Loss: 0.0048675703263962 | Epoch: 2107 | \n\n\nMeanAbsoluteError: 0.0495715998113155 | Loss: 0.0048166533059642 | Epoch: 2108 | \n\n\nMeanAbsoluteError: 0.0497810989618301 | Loss: 0.0052648041979478 | Epoch: 2109 | \n\n\nMeanAbsoluteError: 0.0498389787971973 | Loss: 0.0053515755218086 | Epoch: 2110 | \n\n\nMeanAbsoluteError: 0.0516628175973892 | Loss: 0.0053542422072254 | Epoch: 2111 | \n\n\nMeanAbsoluteError: 0.0558166690170765 | Loss: 0.0061930867651730 | Epoch: 2112 | \n\n\nMeanAbsoluteError: 0.0483270660042763 | Loss: 0.0047731439413944 | Epoch: 2113 | \n\n\nMeanAbsoluteError: 0.0495909005403519 | Loss: 0.0050724230647999 | Epoch: 2114 | \n\n\nMeanAbsoluteError: 0.0515753775835037 | Loss: 0.0056974111167680 | Epoch: 2115 | \n\n\nMeanAbsoluteError: 0.0507196821272373 | Loss: 0.0053619196554852 | Epoch: 2116 | \n\n\nMeanAbsoluteError: 0.0548356436192989 | Loss: 0.0066025503327182 | Epoch: 2117 | \n\n\nMeanAbsoluteError: 0.0467670634388924 | Loss: 0.0050000552785423 | Epoch: 2118 | \n\n\nMeanAbsoluteError: 0.0505778491497040 | Loss: 0.0047682491882248 | Epoch: 2119 | \n\n\nMeanAbsoluteError: 0.0526523739099503 | Loss: 0.0058418979971490 | Epoch: 2120 | \n\n\nMeanAbsoluteError: 0.0504660941660404 | Loss: 0.0049959892111625 | Epoch: 2121 | \n\n\nMeanAbsoluteError: 0.0510740652680397 | Loss: 0.0054546157465332 | Epoch: 2122 | \n\n\nMeanAbsoluteError: 0.0481637790799141 | Loss: 0.0050109088189750 | Epoch: 2123 | \n\n\nMeanAbsoluteError: 0.0512762889266014 | Loss: 0.0053630670252460 | Epoch: 2124 | \n\n\nMeanAbsoluteError: 0.0498408302664757 | Loss: 0.0044657714119239 | Epoch: 2125 | \n\n\nMeanAbsoluteError: 0.0503101460635662 | Loss: 0.0050298506072189 | Epoch: 2126 | \n\n\nMeanAbsoluteError: 0.0516901984810829 | Loss: 0.0058471993458807 | Epoch: 2127 | \n\n\nMeanAbsoluteError: 0.0484206825494766 | Loss: 0.0052050515689888 | Epoch: 2128 | \n\n\nMeanAbsoluteError: 0.0479530543088913 | Loss: 0.0043644465435500 | Epoch: 2129 | \n\n\nMeanAbsoluteError: 0.0509919784963131 | Loss: 0.0050926111767270 | Epoch: 2130 | \n\n\nMeanAbsoluteError: 0.0484330691397190 | Loss: 0.0044589800504400 | Epoch: 2131 | \n\n\nMeanAbsoluteError: 0.0489155687391758 | Loss: 0.0047744793480282 | Epoch: 2132 | \n\n\nMeanAbsoluteError: 0.0520199835300446 | Loss: 0.0057917233471987 | Epoch: 2133 | \n\n\nMeanAbsoluteError: 0.0523665361106396 | Loss: 0.0055813116317950 | Epoch: 2134 | \n\n\nMeanAbsoluteError: 0.0511784553527832 | Loss: 0.0055653616484900 | Epoch: 2135 | \n\n\nMeanAbsoluteError: 0.0483585260808468 | Loss: 0.0049303865717417 | Epoch: 2136 | \n\n\nMeanAbsoluteError: 0.0501587279140949 | Loss: 0.0054435429615053 | Epoch: 2137 | \n\n\nMeanAbsoluteError: 0.0516221411526203 | Loss: 0.0053299282209673 | Epoch: 2138 | \n\n\nMeanAbsoluteError: 0.0513198636472225 | Loss: 0.0056784475382907 | Epoch: 2139 | \n\n\nMeanAbsoluteError: 0.0483607053756714 | Loss: 0.0047259186492738 | Epoch: 2140 | \n\n\nMeanAbsoluteError: 0.0517377480864525 | Loss: 0.0053079439432592 | Epoch: 2141 | \n\n\nMeanAbsoluteError: 0.0498862713575363 | Loss: 0.0052571056654051 | Epoch: 2142 | \n\n\nMeanAbsoluteError: 0.0478393360972404 | Loss: 0.0048484391607720 | Epoch: 2143 | \n\n\nMeanAbsoluteError: 0.0530061163008213 | Loss: 0.0059455390679250 | Epoch: 2144 | \n\n\nMeanAbsoluteError: 0.0478056035935879 | Loss: 0.0045823284984726 | Epoch: 2145 | \n\n\nMeanAbsoluteError: 0.0531346723437309 | Loss: 0.0055918285353133 | Epoch: 2146 | \n\n\nMeanAbsoluteError: 0.0489911921322346 | Loss: 0.0051003052788474 | Epoch: 2147 | \n\n\nMeanAbsoluteError: 0.0514356568455696 | Loss: 0.0059554447349789 | Epoch: 2148 | \n\n\nMeanAbsoluteError: 0.0480888076126575 | Loss: 0.0044858254986927 | Epoch: 2149 | \n\n\nMeanAbsoluteError: 0.0486486032605171 | Loss: 0.0049942986365750 | Epoch: 2150 | \n\n\nMeanAbsoluteError: 0.0496467538177967 | Loss: 0.0051988564657540 | Epoch: 2151 | \n\n\nMeanAbsoluteError: 0.0491588637232780 | Loss: 0.0045036379858986 | Epoch: 2152 | \n\n\nMeanAbsoluteError: 0.0489342771470547 | Loss: 0.0050889326337589 | Epoch: 2153 | \n\n\nMeanAbsoluteError: 0.0551741980016232 | Loss: 0.0063616373174530 | Epoch: 2154 | \n\n\nMeanAbsoluteError: 0.0483066067099571 | Loss: 0.0048633379888361 | Epoch: 2155 | \n\n\nMeanAbsoluteError: 0.0490882843732834 | Loss: 0.0051346345279914 | Epoch: 2156 | \n\n\nMeanAbsoluteError: 0.0475569963455200 | Loss: 0.0043124396289932 | Epoch: 2157 | \n\n\nMeanAbsoluteError: 0.0465142056345940 | Loss: 0.0044100084935356 | Epoch: 2158 | \n\n\nMeanAbsoluteError: 0.0475621446967125 | Loss: 0.0043971991295742 | Epoch: 2159 | \n\n\nMeanAbsoluteError: 0.0544955618679523 | Loss: 0.0056966767197446 | Epoch: 2160 | \n\n\nMeanAbsoluteError: 0.0496545061469078 | Loss: 0.0055897702717569 | Epoch: 2161 | \n\n\nMeanAbsoluteError: 0.0474576391279697 | Loss: 0.0049948059118227 | Epoch: 2162 | \n\n\nMeanAbsoluteError: 0.0540831424295902 | Loss: 0.0059187331937574 | Epoch: 2163 | \n\n\nMeanAbsoluteError: 0.0509313531219959 | Loss: 0.0048754442545639 | Epoch: 2164 | \n\n\nMeanAbsoluteError: 0.0530764497816563 | Loss: 0.0061077126008604 | Epoch: 2165 | \n\n\nMeanAbsoluteError: 0.0496105290949345 | Loss: 0.0050523038985678 | Epoch: 2166 | \n\n\nMeanAbsoluteError: 0.0504399351775646 | Loss: 0.0054526484883597 | Epoch: 2167 | \n\n\nMeanAbsoluteError: 0.0491991154849529 | Loss: 0.0047912167107036 | Epoch: 2168 | \n\n\nMeanAbsoluteError: 0.0509487465023994 | Loss: 0.0055721190258964 | Epoch: 2169 | \n\n\nMeanAbsoluteError: 0.0514652468264103 | Loss: 0.0057611112436462 | Epoch: 2170 | \n\n\nMeanAbsoluteError: 0.0516527406871319 | Loss: 0.0056624958893129 | Epoch: 2171 | \n\n\nMeanAbsoluteError: 0.0487035773694515 | Loss: 0.0051564002699646 | Epoch: 2172 | \n\n\nMeanAbsoluteError: 0.0456098765134811 | Loss: 0.0044481949393336 | Epoch: 2173 | \n\n\nMeanAbsoluteError: 0.0564084611833096 | Loss: 0.0067115298829807 | Epoch: 2174 | \n\n\nMeanAbsoluteError: 0.0493844226002693 | Loss: 0.0048039186139264 | Epoch: 2175 | \n\n\nMeanAbsoluteError: 0.0545862838625908 | Loss: 0.0060761878532260 | Epoch: 2176 | \n\n\nMeanAbsoluteError: 0.0519213862717152 | Loss: 0.0056626268945062 | Epoch: 2177 | \n\n\nMeanAbsoluteError: 0.0471390224993229 | Loss: 0.0044638506575575 | Epoch: 2178 | \n\n\nMeanAbsoluteError: 0.0502665154635906 | Loss: 0.0051346843900077 | Epoch: 2179 | \n\n\nMeanAbsoluteError: 0.0489214956760406 | Loss: 0.0054801460511802 | Epoch: 2180 | \n\n\nMeanAbsoluteError: 0.0508318357169628 | Loss: 0.0053856979361202 | Epoch: 2181 | \n\n\nMeanAbsoluteError: 0.0487864799797535 | Loss: 0.0046945482644514 | Epoch: 2182 | \n\n\nMeanAbsoluteError: 0.0563699454069138 | Loss: 0.0065882375498040 | Epoch: 2183 | \n\n\nMeanAbsoluteError: 0.0532992109656334 | Loss: 0.0060286436864408 | Epoch: 2184 | \n\n\nMeanAbsoluteError: 0.0468473322689533 | Loss: 0.0048563585143832 | Epoch: 2185 | \n\n\nMeanAbsoluteError: 0.0473783500492573 | Loss: 0.0044970547341897 | Epoch: 2186 | \n\n\nMeanAbsoluteError: 0.0472449846565723 | Loss: 0.0041966277310773 | Epoch: 2187 | \n\n\nMeanAbsoluteError: 0.0553256534039974 | Loss: 0.0057638499742219 | Epoch: 2188 | \n\n\nMeanAbsoluteError: 0.0461128577589989 | Loss: 0.0040800096919581 | Epoch: 2189 | \n\n\nMeanAbsoluteError: 0.0550459362566471 | Loss: 0.0063721059699916 | Epoch: 2190 | \n\n\nMeanAbsoluteError: 0.0529698356986046 | Loss: 0.0056451289134217 | Epoch: 2191 | \n\n\nMeanAbsoluteError: 0.0489141158759594 | Loss: 0.0052642084489162 | Epoch: 2192 | \n\n\nMeanAbsoluteError: 0.0478025972843170 | Loss: 0.0044342259935062 | Epoch: 2193 | \n\n\nMeanAbsoluteError: 0.0497823767364025 | Loss: 0.0052513136467314 | Epoch: 2194 | \n\n\nMeanAbsoluteError: 0.0528331100940704 | Loss: 0.0060086856038712 | Epoch: 2195 | \n\n\nMeanAbsoluteError: 0.0488273464143276 | Loss: 0.0049583695211671 | Epoch: 2196 | \n\n\nMeanAbsoluteError: 0.0500863194465637 | Loss: 0.0058769497933251 | Epoch: 2197 | \n\n\nMeanAbsoluteError: 0.0461849831044674 | Loss: 0.0040819339354857 | Epoch: 2198 | \n\n\nMeanAbsoluteError: 0.0482791773974895 | Loss: 0.0046929718184401 | Epoch: 2199 | \n\n\nMeanAbsoluteError: 0.0451430007815361 | Loss: 0.0040965852323764 | Epoch: 2200 | \n\n\nMeanAbsoluteError: 0.0498573854565620 | Loss: 0.0050521100638434 | Epoch: 2201 | \n\n\nMeanAbsoluteError: 0.0492491126060486 | Loss: 0.0043940624659202 | Epoch: 2202 | \n\n\nMeanAbsoluteError: 0.0464411973953247 | Loss: 0.0045747716481856 | Epoch: 2203 | \n\n\nMeanAbsoluteError: 0.0483649186789989 | Loss: 0.0049904841618824 | Epoch: 2204 | \n\n\nMeanAbsoluteError: 0.0480507612228394 | Loss: 0.0046293963333922 | Epoch: 2205 | \n\n\nMeanAbsoluteError: 0.0513865351676941 | Loss: 0.0053597369507042 | Epoch: 2206 | \n\n\nMeanAbsoluteError: 0.0534763522446156 | Loss: 0.0057172434720815 | Epoch: 2207 | \n\n\nMeanAbsoluteError: 0.0502175241708755 | Loss: 0.0052436932544091 | Epoch: 2208 | \n\n\nMeanAbsoluteError: 0.0521408170461655 | Loss: 0.0053869157880126 | Epoch: 2209 | \n\n\nMeanAbsoluteError: 0.0499975010752678 | Loss: 0.0050419097584317 | Epoch: 2210 | \n\n\nMeanAbsoluteError: 0.0479558072984219 | Loss: 0.0043788267177903 | Epoch: 2211 | \n\n\nMeanAbsoluteError: 0.0512605719268322 | Loss: 0.0053585718635319 | Epoch: 2212 | \n\n\nMeanAbsoluteError: 0.0511519946157932 | Loss: 0.0052282036187307 | Epoch: 2213 | \n\n\nMeanAbsoluteError: 0.0484275780618191 | Loss: 0.0044381750685231 | Epoch: 2214 | \n\n\nMeanAbsoluteError: 0.0508180707693100 | Loss: 0.0052387075245012 | Epoch: 2215 | \n\n\nMeanAbsoluteError: 0.0519539192318916 | Loss: 0.0056810486069783 | Epoch: 2216 | \n\n\nMeanAbsoluteError: 0.0489872992038727 | Loss: 0.0047070006837597 | Epoch: 2217 | \n\n\nMeanAbsoluteError: 0.0532800555229187 | Loss: 0.0062605110557585 | Epoch: 2218 | \n\n\nMeanAbsoluteError: 0.0474954061210155 | Loss: 0.0051215866571255 | Epoch: 2219 | \n\n\nMeanAbsoluteError: 0.0510926432907581 | Loss: 0.0052731372179793 | Epoch: 2220 | \n\n\nMeanAbsoluteError: 0.0465900786221027 | Loss: 0.0047097458060853 | Epoch: 2221 | \n\n\nMeanAbsoluteError: 0.0504422523081303 | Loss: 0.0047525554753035 | Epoch: 2222 | \n\n\nMeanAbsoluteError: 0.0554601363837719 | Loss: 0.0060849905761400 | Epoch: 2223 | \n\n\nMeanAbsoluteError: 0.0470483489334583 | Loss: 0.0047078329755470 | Epoch: 2224 | \n\n\nMeanAbsoluteError: 0.0472545586526394 | Loss: 0.0045944787813793 | Epoch: 2225 | \n\n\nMeanAbsoluteError: 0.0488640926778316 | Loss: 0.0048926890947526 | Epoch: 2226 | \n\n\nMeanAbsoluteError: 0.0495597012341022 | Loss: 0.0048231834895099 | Epoch: 2227 | \n\n\nMeanAbsoluteError: 0.0414857342839241 | Loss: 0.0037624065890490 | Epoch: 2228 | \n\n\nMeanAbsoluteError: 0.0540198311209679 | Loss: 0.0057647867657215 | Epoch: 2229 | \n\n\nMeanAbsoluteError: 0.0522354207932949 | Loss: 0.0059277333238576 | Epoch: 2230 | \n\n\nMeanAbsoluteError: 0.0492044910788536 | Loss: 0.0051781908543588 | Epoch: 2231 | \n\n\nMeanAbsoluteError: 0.0483266152441502 | Loss: 0.0049066665314876 | Epoch: 2232 | \n\n\nMeanAbsoluteError: 0.0517158955335617 | Loss: 0.0053449909897821 | Epoch: 2233 | \n\n\nMeanAbsoluteError: 0.0486241653561592 | Loss: 0.0047486923054627 | Epoch: 2234 | \n\n\nMeanAbsoluteError: 0.0500047206878662 | Loss: 0.0053059027621991 | Epoch: 2235 | \n\n\nMeanAbsoluteError: 0.0497084371745586 | Loss: 0.0051742158001828 | Epoch: 2236 | \n\n\nMeanAbsoluteError: 0.0479113832116127 | Loss: 0.0045052035794167 | Epoch: 2237 | \n\n\nMeanAbsoluteError: 0.0505400262773037 | Loss: 0.0054502244773175 | Epoch: 2238 | \n\n\nMeanAbsoluteError: 0.0539358258247375 | Loss: 0.0061278783083617 | Epoch: 2239 | \n\n\nMeanAbsoluteError: 0.0493227951228619 | Loss: 0.0049400958680868 | Epoch: 2240 | \n\n\nMeanAbsoluteError: 0.0491435714066029 | Loss: 0.0054472279354256 | Epoch: 2241 | \n\n\nMeanAbsoluteError: 0.0520682632923126 | Loss: 0.0054385923487598 | Epoch: 2242 | \n\n\nMeanAbsoluteError: 0.0490238554775715 | Loss: 0.0054221187988272 | Epoch: 2243 | \n\n\nMeanAbsoluteError: 0.0455042421817780 | Loss: 0.0041923761209724 | Epoch: 2244 | \n\n\nMeanAbsoluteError: 0.0513693951070309 | Loss: 0.0051375423223605 | Epoch: 2245 | \n\n\nMeanAbsoluteError: 0.0490623861551285 | Loss: 0.0048600240998167 | Epoch: 2246 | \n\n\nMeanAbsoluteError: 0.0508887693285942 | Loss: 0.0053671975297387 | Epoch: 2247 | \n\n\nMeanAbsoluteError: 0.0506015270948410 | Loss: 0.0054212005334072 | Epoch: 2248 | \n\n\nMeanAbsoluteError: 0.0482258610427380 | Loss: 0.0045926817739746 | Epoch: 2249 | \n\n\nMeanAbsoluteError: 0.0507761985063553 | Loss: 0.0051576629179181 | Epoch: 2250 | \n\n\nMeanAbsoluteError: 0.0484218485653400 | Loss: 0.0047928856964973 | Epoch: 2251 | \n\n\nMeanAbsoluteError: 0.0507043600082397 | Loss: 0.0054231097956532 | Epoch: 2252 | \n\n\nMeanAbsoluteError: 0.0467334911227226 | Loss: 0.0040878407009128 | Epoch: 2253 | \n\n\nMeanAbsoluteError: 0.0503480695188046 | Loss: 0.0056571266291697 | Epoch: 2254 | \n\n\nMeanAbsoluteError: 0.0495885238051414 | Loss: 0.0058282299283625 | Epoch: 2255 | \n\n\nMeanAbsoluteError: 0.0514898449182510 | Loss: 0.0060286415237230 | Epoch: 2256 | \n\n\nMeanAbsoluteError: 0.0538332164287567 | Loss: 0.0059044320553039 | Epoch: 2257 | \n\n\nMeanAbsoluteError: 0.0486703626811504 | Loss: 0.0049549929402987 | Epoch: 2258 | \n\n\nMeanAbsoluteError: 0.0503663085401058 | Loss: 0.0049126900852085 | Epoch: 2259 | \n\n\nMeanAbsoluteError: 0.0504832528531551 | Loss: 0.0055621585431315 | Epoch: 2260 | \n\n\nMeanAbsoluteError: 0.0526245050132275 | Loss: 0.0061052321925498 | Epoch: 2261 | \n\n\nMeanAbsoluteError: 0.0506215207278728 | Loss: 0.0049533872256870 | Epoch: 2262 | \n\n\nMeanAbsoluteError: 0.0445700213313103 | Loss: 0.0038082624810947 | Epoch: 2263 | \n\n\nMeanAbsoluteError: 0.0474841594696045 | Loss: 0.0048610006966131 | Epoch: 2264 | \n\n\nMeanAbsoluteError: 0.0496164485812187 | Loss: 0.0048112074314849 | Epoch: 2265 | \n\n\nMeanAbsoluteError: 0.0497408546507359 | Loss: 0.0051362968205649 | Epoch: 2266 | \n\n\nMeanAbsoluteError: 0.0455256849527359 | Loss: 0.0045438862572458 | Epoch: 2267 | \n\n\nMeanAbsoluteError: 0.0512654893100262 | Loss: 0.0052706643799714 | Epoch: 2268 | \n\n\nMeanAbsoluteError: 0.0500065945088863 | Loss: 0.0050134347513570 | Epoch: 2269 | \n\n\nMeanAbsoluteError: 0.0467154234647751 | Loss: 0.0042814013797518 | Epoch: 2270 | \n\n\nMeanAbsoluteError: 0.0502683669328690 | Loss: 0.0051202753297093 | Epoch: 2271 | \n\n\nMeanAbsoluteError: 0.0502090789377689 | Loss: 0.0055269490482321 | Epoch: 2272 | \n\n\nMeanAbsoluteError: 0.0533046126365662 | Loss: 0.0055378690696671 | Epoch: 2273 | \n\n\nMeanAbsoluteError: 0.0488349311053753 | Loss: 0.0047704421402887 | Epoch: 2274 | \n\n\nMeanAbsoluteError: 0.0487081035971642 | Loss: 0.0053597781049515 | Epoch: 2275 | \n\n\nMeanAbsoluteError: 0.0530542731285095 | Loss: 0.0056586667869215 | Epoch: 2276 | \n\n\nMeanAbsoluteError: 0.0439674668014050 | Loss: 0.0036371626339678 | Epoch: 2277 | \n\n\nMeanAbsoluteError: 0.0490726232528687 | Loss: 0.0054232720535856 | Epoch: 2278 | \n\n\nMeanAbsoluteError: 0.0490035451948643 | Loss: 0.0053801196104390 | Epoch: 2279 | \n\n\nMeanAbsoluteError: 0.0498022697865963 | Loss: 0.0050110443188828 | Epoch: 2280 | \n\n\nMeanAbsoluteError: 0.0521225184202194 | Loss: 0.0061911110881283 | Epoch: 2281 | \n\n\nMeanAbsoluteError: 0.0484686829149723 | Loss: 0.0047396032646702 | Epoch: 2282 | \n\n\nMeanAbsoluteError: 0.0498228780925274 | Loss: 0.0050187484832471 | Epoch: 2283 | \n\n\nMeanAbsoluteError: 0.0485101938247681 | Loss: 0.0046413964039723 | Epoch: 2284 | \n\n\nMeanAbsoluteError: 0.0476440377533436 | Loss: 0.0048045678383255 | Epoch: 2285 | \n\n\nMeanAbsoluteError: 0.0472132600843906 | Loss: 0.0044680899839356 | Epoch: 2286 | \n\n\nMeanAbsoluteError: 0.0504258796572685 | Loss: 0.0054320692016214 | Epoch: 2287 | \n\n\nMeanAbsoluteError: 0.0466857850551605 | Loss: 0.0045341811776355 | Epoch: 2288 | \n\n\nMeanAbsoluteError: 0.0477001555263996 | Loss: 0.0046274507321323 | Epoch: 2289 | \n\n\nMeanAbsoluteError: 0.0526601262390614 | Loss: 0.0056980111241865 | Epoch: 2290 | \n\n\nMeanAbsoluteError: 0.0482580997049809 | Loss: 0.0048445910567534 | Epoch: 2291 | \n\n\nMeanAbsoluteError: 0.0528410412371159 | Loss: 0.0060515370762005 | Epoch: 2292 | \n\n\nMeanAbsoluteError: 0.0487855188548565 | Loss: 0.0053546765628759 | Epoch: 2293 | \n\n\nMeanAbsoluteError: 0.0521530285477638 | Loss: 0.0058098563332533 | Epoch: 2294 | \n\n\nMeanAbsoluteError: 0.0482901334762573 | Loss: 0.0041417360362296 | Epoch: 2295 | \n\n\nMeanAbsoluteError: 0.0492099560797215 | Loss: 0.0055247538009924 | Epoch: 2296 | \n\n\nMeanAbsoluteError: 0.0529043748974800 | Loss: 0.0058052576794398 | Epoch: 2297 | \n\n\nMeanAbsoluteError: 0.0519558638334274 | Loss: 0.0054321944470818 | Epoch: 2298 | \n\n\nMeanAbsoluteError: 0.0540995076298714 | Loss: 0.0059255983841740 | Epoch: 2299 | \n\n\nMeanAbsoluteError: 0.0527193211019039 | Loss: 0.0059852935168010 | Epoch: 2300 | \n\n\nMeanAbsoluteError: 0.0535541027784348 | Loss: 0.0056488551969104 | Epoch: 2301 | \n\n\nMeanAbsoluteError: 0.0497251562774181 | Loss: 0.0050836452287634 | Epoch: 2302 | \n\n\nMeanAbsoluteError: 0.0485454127192497 | Loss: 0.0046317733164081 | Epoch: 2303 | \n\n\nMeanAbsoluteError: 0.0516242794692516 | Loss: 0.0063695762854089 | Epoch: 2304 | \n\n\nMeanAbsoluteError: 0.0502903759479523 | Loss: 0.0053403251633669 | Epoch: 2305 | \n\n\nMeanAbsoluteError: 0.0489827618002892 | Loss: 0.0048326114876500 | Epoch: 2306 | \n\n\nMeanAbsoluteError: 0.0470514893531799 | Loss: 0.0041615004059956 | Epoch: 2307 | \n\n\nMeanAbsoluteError: 0.0551695451140404 | Loss: 0.0071591969797555 | Epoch: 2308 | \n\n\nMeanAbsoluteError: 0.0527448467910290 | Loss: 0.0055241383618340 | Epoch: 2309 | \n\n\nMeanAbsoluteError: 0.0509823225438595 | Loss: 0.0053462781220090 | Epoch: 2310 | \n\n\nMeanAbsoluteError: 0.0502913221716881 | Loss: 0.0057100909795531 | Epoch: 2311 | \n\n\nMeanAbsoluteError: 0.0447010658681393 | Loss: 0.0041680522656073 | Epoch: 2312 | \n\n\nMeanAbsoluteError: 0.0482162684202194 | Loss: 0.0050033698110822 | Epoch: 2313 | \n\n\nMeanAbsoluteError: 0.0520333275198936 | Loss: 0.0054106281374152 | Epoch: 2314 | \n\n\nMeanAbsoluteError: 0.0500354245305061 | Loss: 0.0051489369439139 | Epoch: 2315 | \n\n\nMeanAbsoluteError: 0.0496130064129829 | Loss: 0.0048618702535956 | Epoch: 2316 | \n\n\nMeanAbsoluteError: 0.0460628271102905 | Loss: 0.0046536912927634 | Epoch: 2317 | \n\n\nMeanAbsoluteError: 0.0486624352633953 | Loss: 0.0050470440027751 | Epoch: 2318 | \n\n\nMeanAbsoluteError: 0.0474831573665142 | Loss: 0.0047080888984116 | Epoch: 2319 | \n\n\nMeanAbsoluteError: 0.0529102161526680 | Loss: 0.0054170389866219 | Epoch: 2320 | \n\n\nMeanAbsoluteError: 0.0538688674569130 | Loss: 0.0060643640976014 | Epoch: 2321 | \n\n\nMeanAbsoluteError: 0.0474075824022293 | Loss: 0.0049661216413854 | Epoch: 2322 | \n\n\nMeanAbsoluteError: 0.0461876168847084 | Loss: 0.0047925004124522 | Epoch: 2323 | \n\n\nMeanAbsoluteError: 0.0523887835443020 | Loss: 0.0065217024742257 | Epoch: 2324 | \n\n\nMeanAbsoluteError: 0.0465410016477108 | Loss: 0.0045699556426962 | Epoch: 2325 | \n\n\nMeanAbsoluteError: 0.0488825738430023 | Loss: 0.0048495502043443 | Epoch: 2326 | \n\n\nMeanAbsoluteError: 0.0508254542946815 | Loss: 0.0050943947774552 | Epoch: 2327 | \n\n\nMeanAbsoluteError: 0.0479211620986462 | Loss: 0.0043689563689501 | Epoch: 2328 | \n\n\nMeanAbsoluteError: 0.0494069308042526 | Loss: 0.0051829116225417 | Epoch: 2329 | \n\n\nMeanAbsoluteError: 0.0455342344939709 | Loss: 0.0041858166285844 | Epoch: 2330 | \n\n\nMeanAbsoluteError: 0.0533076487481594 | Loss: 0.0056952177333490 | Epoch: 2331 | \n\n\nMeanAbsoluteError: 0.0475188121199608 | Loss: 0.0049667418336018 | Epoch: 2332 | \n\n\nMeanAbsoluteError: 0.0504839383065701 | Loss: 0.0054843469161127 | Epoch: 2333 | \n\n\nMeanAbsoluteError: 0.0508178547024727 | Loss: 0.0052779219162464 | Epoch: 2334 | \n\n\nMeanAbsoluteError: 0.0471548996865749 | Loss: 0.0047698562689038 | Epoch: 2335 | \n\n\nMeanAbsoluteError: 0.0475326776504517 | Loss: 0.0050609935685429 | Epoch: 2336 | \n\n\nMeanAbsoluteError: 0.0523922480642796 | Loss: 0.0055751334967984 | Epoch: 2337 | \n\n\nMeanAbsoluteError: 0.0502215139567852 | Loss: 0.0051684699527353 | Epoch: 2338 | \n\n\nMeanAbsoluteError: 0.0493761263787746 | Loss: 0.0052379263912735 | Epoch: 2339 | \n\n\nMeanAbsoluteError: 0.0518511906266212 | Loss: 0.0057510674667901 | Epoch: 2340 | \n\n\nMeanAbsoluteError: 0.0518568046391010 | Loss: 0.0061046414055151 | Epoch: 2341 | \n\n\nMeanAbsoluteError: 0.0506386719644070 | Loss: 0.0056426884539057 | Epoch: 2342 | \n\n\nMeanAbsoluteError: 0.0472983717918396 | Loss: 0.0049563837655417 | Epoch: 2343 | \n\n\nMeanAbsoluteError: 0.0510114207863808 | Loss: 0.0057507701717259 | Epoch: 2344 | \n\n\nMeanAbsoluteError: 0.0531255006790161 | Loss: 0.0058352129714088 | Epoch: 2345 | \n\n\nMeanAbsoluteError: 0.0533580034971237 | Loss: 0.0063216027745511 | Epoch: 2346 | \n\n\nMeanAbsoluteError: 0.0516010634601116 | Loss: 0.0053796832961719 | Epoch: 2347 | \n\n\nMeanAbsoluteError: 0.0526483133435249 | Loss: 0.0056106403514059 | Epoch: 2348 | \n\n\nMeanAbsoluteError: 0.0487502589821815 | Loss: 0.0052889514248333 | Epoch: 2349 | \n\n\nMeanAbsoluteError: 0.0477842018008232 | Loss: 0.0046582632494453 | Epoch: 2350 | \n\n\nMeanAbsoluteError: 0.0498476140201092 | Loss: 0.0048704350759484 | Epoch: 2351 | \n\n\nMeanAbsoluteError: 0.0512418523430824 | Loss: 0.0058317251462662 | Epoch: 2352 | \n\n\nMeanAbsoluteError: 0.0494362488389015 | Loss: 0.0054178266596231 | Epoch: 2353 | \n\n\nMeanAbsoluteError: 0.0504942275583744 | Loss: 0.0052678020266952 | Epoch: 2354 | \n\n\nMeanAbsoluteError: 0.0512288324534893 | Loss: 0.0054143552811123 | Epoch: 2355 | \n\n\nMeanAbsoluteError: 0.0468337386846542 | Loss: 0.0043812656533889 | Epoch: 2356 | \n\n\nMeanAbsoluteError: 0.0476289540529251 | Loss: 0.0044555496390725 | Epoch: 2357 | \n\n\nMeanAbsoluteError: 0.0510654933750629 | Loss: 0.0051125890253024 | Epoch: 2358 | \n\n\nMeanAbsoluteError: 0.0504216253757477 | Loss: 0.0054114399360818 | Epoch: 2359 | \n\n\nMeanAbsoluteError: 0.0467402823269367 | Loss: 0.0042538922054943 | Epoch: 2360 | \n\n\nMeanAbsoluteError: 0.0500267483294010 | Loss: 0.0054323471848390 | Epoch: 2361 | \n\n\nMeanAbsoluteError: 0.0496944785118103 | Loss: 0.0048162275107370 | Epoch: 2362 | \n\n\nMeanAbsoluteError: 0.0519372224807739 | Loss: 0.0055401354407998 | Epoch: 2363 | \n\n\nMeanAbsoluteError: 0.0476076863706112 | Loss: 0.0045836419801829 | Epoch: 2364 | \n\n\nMeanAbsoluteError: 0.0509594380855560 | Loss: 0.0056476120240283 | Epoch: 2365 | \n\n\nMeanAbsoluteError: 0.0502804517745972 | Loss: 0.0056061178709084 | Epoch: 2366 | \n\n\nMeanAbsoluteError: 0.0489975586533546 | Loss: 0.0055004535251707 | Epoch: 2367 | \n\n\nMeanAbsoluteError: 0.0504837855696678 | Loss: 0.0050979674380384 | Epoch: 2368 | \n\n\nMeanAbsoluteError: 0.0522496998310089 | Loss: 0.0058835616935054 | Epoch: 2369 | \n\n\nMeanAbsoluteError: 0.0469272099435329 | Loss: 0.0043236032209218 | Epoch: 2370 | \n\n\nMeanAbsoluteError: 0.0454217381775379 | Loss: 0.0047672157694979 | Epoch: 2371 | \n\n\nMeanAbsoluteError: 0.0470436997711658 | Loss: 0.0046681523211009 | Epoch: 2372 | \n\n\nMeanAbsoluteError: 0.0466814897954464 | Loss: 0.0044595789594829 | Epoch: 2373 | \n\n\nMeanAbsoluteError: 0.0461893826723099 | Loss: 0.0040551047010856 | Epoch: 2374 | \n\n\nMeanAbsoluteError: 0.0520933195948601 | Loss: 0.0062161720622620 | Epoch: 2375 | \n\n\nMeanAbsoluteError: 0.0473858825862408 | Loss: 0.0045212199324548 | Epoch: 2376 | \n\n\nMeanAbsoluteError: 0.0509522706270218 | Loss: 0.0055190507548468 | Epoch: 2377 | \n\n\nMeanAbsoluteError: 0.0487980321049690 | Loss: 0.0047926905119039 | Epoch: 2378 | \n\n\nMeanAbsoluteError: 0.0479232370853424 | Loss: 0.0045709664731597 | Epoch: 2379 | \n\n\nMeanAbsoluteError: 0.0505140833556652 | Loss: 0.0052897260474432 | Epoch: 2380 | \n\n\nMeanAbsoluteError: 0.0484383217990398 | Loss: 0.0047015279821214 | Epoch: 2381 | \n\n\nMeanAbsoluteError: 0.0450119599699974 | Loss: 0.0041824530359797 | Epoch: 2382 | \n\n\nMeanAbsoluteError: 0.0496746860444546 | Loss: 0.0053789594022419 | Epoch: 2383 | \n\n\nMeanAbsoluteError: 0.0501800328493118 | Loss: 0.0050403776730794 | Epoch: 2384 | \n\n\nMeanAbsoluteError: 0.0519096516072750 | Loss: 0.0060225809808405 | Epoch: 2385 | \n\n\nMeanAbsoluteError: 0.0511189848184586 | Loss: 0.0055538627667556 | Epoch: 2386 | \n\n\nMeanAbsoluteError: 0.0499657802283764 | Loss: 0.0056442129387870 | Epoch: 2387 | \n\n\nMeanAbsoluteError: 0.0509490780532360 | Loss: 0.0055052837876550 | Epoch: 2388 | \n\n\nMeanAbsoluteError: 0.0476448461413383 | Loss: 0.0044280091456312 | Epoch: 2389 | \n\n\nMeanAbsoluteError: 0.0478855594992638 | Loss: 0.0047412690749600 | Epoch: 2390 | \n\n\nMeanAbsoluteError: 0.0502227991819382 | Loss: 0.0051300095433665 | Epoch: 2391 | \n\n\nMeanAbsoluteError: 0.0493359751999378 | Loss: 0.0056666372516838 | Epoch: 2392 | \n\n\nMeanAbsoluteError: 0.0468969978392124 | Loss: 0.0043018097168533 | Epoch: 2393 | \n\n\nMeanAbsoluteError: 0.0477692112326622 | Loss: 0.0047293790035231 | Epoch: 2394 | \n\n\nMeanAbsoluteError: 0.0446246936917305 | Loss: 0.0038551850914155 | Epoch: 2395 | \n\n\nMeanAbsoluteError: 0.0467439368367195 | Loss: 0.0039889198289166 | Epoch: 2396 | \n\n\nMeanAbsoluteError: 0.0495277531445026 | Loss: 0.0056959540271782 | Epoch: 2397 | \n\n\nMeanAbsoluteError: 0.0489016436040401 | Loss: 0.0053074449128326 | Epoch: 2398 | \n\n\nMeanAbsoluteError: 0.0513200461864471 | Loss: 0.0052757823315581 | Epoch: 2399 | \n\n\nMeanAbsoluteError: 0.0499289110302925 | Loss: 0.0050067462657929 | Epoch: 2400 | \n\n\nMeanAbsoluteError: 0.0455825775861740 | Loss: 0.0044781183817850 | Epoch: 2401 | \n\n\nMeanAbsoluteError: 0.0500051230192184 | Loss: 0.0049798804013820 | Epoch: 2402 | \n\n\nMeanAbsoluteError: 0.0496815890073776 | Loss: 0.0050003188256233 | Epoch: 2403 | \n\n\nMeanAbsoluteError: 0.0487396605312824 | Loss: 0.0051065985867793 | Epoch: 2404 | \n\n\nMeanAbsoluteError: 0.0485574156045914 | Loss: 0.0047230738250281 | Early stopping at epoch 2403\nReturned to Spot: Validation loss: 0.004723073825028147\n\nconfig: {'_L_in': 10, '_L_out': 1, 'l1': 16, 'dropout_prob': 0.1773189149831582, 'lr_mult': 9.06715620679689, 'batch_size': 4, 'epochs': 4, 'k_folds': 1, 'patience': 8, 'optimizer': 'Adamax', 'sgd_momentum': 0.22706194978124503}\nEpoch: 1 | MeanAbsoluteError: 0.1341543793678284 | Loss: 0.0282058145067034 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1317252665758133 | Loss: 0.0266506999917328 | Epoch: 3 | MeanAbsoluteError: 0.1187010332942009 | Loss: 0.0226723635724435 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1316164582967758 | Loss: 0.0274209188111126 | Returned to Spot: Validation loss: 0.02742091881111264\n\nconfig: {'_L_in': 10, '_L_out': 1, 'l1': 32, 'dropout_prob': 0.3840970624671163, 'lr_mult': 4.593111165984917, 'batch_size': 8, 'epochs': 256, 'k_folds': 1, 'patience': 64, 'optimizer': 'Adam', 'sgd_momentum': 0.8945915831939406}\nEpoch: 1 | MeanAbsoluteError: 0.1577828675508499 | Loss: 0.0396680165768454 | Epoch: 2 | MeanAbsoluteError: 0.1531970798969269 | Loss: 0.0357943068650600 | Epoch: 3 | MeanAbsoluteError: 0.1354434639215469 | Loss: 0.0283069256325497 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1248374655842781 | Loss: 0.0237815922959463 | Epoch: 5 | MeanAbsoluteError: 0.1190699487924576 | Loss: 0.0224893650309624 | Epoch: 6 | MeanAbsoluteError: 0.1193105205893517 | Loss: 0.0219147950901013 | Epoch: 7 | MeanAbsoluteError: 0.1136403381824493 | Loss: 0.0205738441038289 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.1123237833380699 | Loss: 0.0190981777850538 | Epoch: 9 | MeanAbsoluteError: 0.0990597754716873 | Loss: 0.0156040972307030 | Epoch: 10 | MeanAbsoluteError: 0.0923467800021172 | Loss: 0.0134347289506542 | Epoch: 11 | MeanAbsoluteError: 0.0901258438825607 | Loss: 0.0129575862328669 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0849676281213760 | Loss: 0.0121943660288755 | Epoch: 13 | MeanAbsoluteError: 0.0920457541942596 | Loss: 0.0126718412104406 | Epoch: 14 | MeanAbsoluteError: 0.0768595561385155 | Loss: 0.0090687401457935 | Epoch: 15 | MeanAbsoluteError: 0.0761612728238106 | Loss: 0.0097372857363600 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0678072348237038 | Loss: 0.0070772565821627 | Epoch: 17 | MeanAbsoluteError: 0.0683974772691727 | Loss: 0.0079315247222487 | Epoch: 18 | MeanAbsoluteError: 0.0660292208194733 | Loss: 0.0068538732164981 | Epoch: 19 | MeanAbsoluteError: 0.0671575143933296 | Loss: 0.0075403975359605 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0683768168091774 | Loss: 0.0075039070522364 | Epoch: 21 | MeanAbsoluteError: 0.0683320388197899 | Loss: 0.0078506712912043 | Epoch: 22 | MeanAbsoluteError: 0.0624173097312450 | Loss: 0.0064052636122429 | Epoch: 23 | MeanAbsoluteError: 0.0859738439321518 | Loss: 0.0102079651551321 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0578384958207607 | Loss: 0.0058746729505641 | Epoch: 25 | MeanAbsoluteError: 0.0708958134055138 | Loss: 0.0078511588079365 | Epoch: 26 | MeanAbsoluteError: 0.0510204248130322 | Loss: 0.0045366644534576 | Epoch: 27 | MeanAbsoluteError: 0.0516149476170540 | Loss: 0.0050526465684494 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0530957579612732 | Loss: 0.0050107582371167 | Epoch: 29 | MeanAbsoluteError: 0.0633170530200005 | Loss: 0.0065521177511025 | Epoch: 30 | MeanAbsoluteError: 0.0525331236422062 | Loss: 0.0046198939181570 | Epoch: 31 | MeanAbsoluteError: 0.0601808726787567 | Loss: 0.0055815867227992 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0610184855759144 | Loss: 0.0055535954490647 | Epoch: 33 | MeanAbsoluteError: 0.0474381707608700 | Loss: 0.0044069871042953 | Epoch: 34 | MeanAbsoluteError: 0.0577852129936218 | Loss: 0.0054373348799632 | Epoch: 35 | MeanAbsoluteError: 0.0467461347579956 | Loss: 0.0040284384311618 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0574632771313190 | Loss: 0.0054446664871648 | Epoch: 37 | MeanAbsoluteError: 0.0445038117468357 | Loss: 0.0035465935805788 | Epoch: 38 | MeanAbsoluteError: 0.0426731482148170 | Loss: 0.0034253862885587 | Epoch: 39 | MeanAbsoluteError: 0.0500576347112656 | Loss: 0.0043978291612707 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0540582016110420 | Loss: 0.0045276106071756 | Epoch: 41 | MeanAbsoluteError: 0.0471341758966446 | Loss: 0.0039674502056043 | Epoch: 42 | MeanAbsoluteError: 0.0558150932192802 | Loss: 0.0048523617136341 | Epoch: 43 | MeanAbsoluteError: 0.0488086082041264 | Loss: 0.0048573563184810 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0480412729084492 | Loss: 0.0040256025937484 | Epoch: 45 | MeanAbsoluteError: 0.0465971268713474 | Loss: 0.0039051140943469 | Epoch: 46 | MeanAbsoluteError: 0.0583623871207237 | Loss: 0.0052861798689456 | Epoch: 47 | MeanAbsoluteError: 0.0520922951400280 | Loss: 0.0050386486722058 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0484520159661770 | Loss: 0.0043241257172715 | Epoch: 49 | MeanAbsoluteError: 0.0768648311495781 | Loss: 0.0081738001899794 | Epoch: 50 | MeanAbsoluteError: 0.0489559285342693 | Loss: 0.0041480900621728 | Epoch: 51 | MeanAbsoluteError: 0.0464768894016743 | Loss: 0.0041071634316866 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0449984073638916 | Loss: 0.0036148314309110 | Epoch: 53 | MeanAbsoluteError: 0.0549848042428493 | Loss: 0.0052397623596909 | Epoch: 54 | MeanAbsoluteError: 0.0499219074845314 | Loss: 0.0044127783774839 | Epoch: 55 | MeanAbsoluteError: 0.0544359646737576 | Loss: 0.0048383563652782 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0481666140258312 | Loss: 0.0040837096166797 | Epoch: 57 | MeanAbsoluteError: 0.0466728135943413 | Loss: 0.0036733666255666 | Epoch: 58 | MeanAbsoluteError: 0.0441151708364487 | Loss: 0.0034196568855183 | Epoch: 59 | MeanAbsoluteError: 0.0561262443661690 | Loss: 0.0049322245758958 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0455266870558262 | Loss: 0.0037700661035304 | Epoch: 61 | MeanAbsoluteError: 0.0515410862863064 | Loss: 0.0046962711249331 | Epoch: 62 | MeanAbsoluteError: 0.0430333055555820 | Loss: 0.0033540611850760 | Epoch: 63 | MeanAbsoluteError: 0.0476235635578632 | Loss: 0.0037815429662403 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0531653314828873 | Loss: 0.0045884917337006 | Epoch: 65 | MeanAbsoluteError: 0.0493933707475662 | Loss: 0.0041540723667774 | Epoch: 66 | MeanAbsoluteError: 0.0550049319863319 | Loss: 0.0046156273517562 | Epoch: 67 | MeanAbsoluteError: 0.0481015481054783 | Loss: 0.0047363162157126 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0431655421853065 | Loss: 0.0035115994702976 | Epoch: 69 | MeanAbsoluteError: 0.0498947389423847 | Loss: 0.0041451541320911 | Epoch: 70 | MeanAbsoluteError: 0.0403305329382420 | Loss: 0.0028214565444566 | Epoch: 71 | MeanAbsoluteError: 0.0441643036901951 | Loss: 0.0034259779862814 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0498654730618000 | Loss: 0.0040531480013957 | Epoch: 73 | MeanAbsoluteError: 0.0456992127001286 | Loss: 0.0033258167187389 | Epoch: 74 | MeanAbsoluteError: 0.0461698211729527 | Loss: 0.0038516020576935 | Epoch: 75 | MeanAbsoluteError: 0.0430048890411854 | Loss: 0.0031840731703836 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0483233518898487 | Loss: 0.0038842495981187 | Epoch: 77 | MeanAbsoluteError: 0.0471607334911823 | Loss: 0.0040479815277075 | Epoch: 78 | MeanAbsoluteError: 0.0388897657394409 | Loss: 0.0029293337927237 | Epoch: 79 | MeanAbsoluteError: 0.0408277139067650 | Loss: 0.0029378990324116 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0491923391819000 | Loss: 0.0038897904941175 | Epoch: 81 | MeanAbsoluteError: 0.0566509701311588 | Loss: 0.0048656474314875 | Epoch: 82 | MeanAbsoluteError: 0.0464061163365841 | Loss: 0.0040187672191103 | Epoch: 83 | MeanAbsoluteError: 0.0477442629635334 | Loss: 0.0038858131156303 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0631739944219589 | Loss: 0.0059290469507687 | Epoch: 85 | MeanAbsoluteError: 0.0524744726717472 | Loss: 0.0044997251028881 | Epoch: 86 | MeanAbsoluteError: 0.0515260957181454 | Loss: 0.0041863398892977 | Epoch: 87 | MeanAbsoluteError: 0.0468790680170059 | Loss: 0.0042512363166009 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0395649895071983 | Loss: 0.0028763440051606 | Epoch: 89 | MeanAbsoluteError: 0.0438371226191521 | Loss: 0.0038168192933019 | Epoch: 90 | MeanAbsoluteError: 0.0524297617375851 | Loss: 0.0041870071344372 | Epoch: 91 | MeanAbsoluteError: 0.0436246097087860 | Loss: 0.0037573828579441 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0535974875092506 | Loss: 0.0044554270908089 | Epoch: 93 | MeanAbsoluteError: 0.0470317304134369 | Loss: 0.0035817120086377 | Epoch: 94 | MeanAbsoluteError: 0.0487175993621349 | Loss: 0.0038834443809097 | Epoch: 95 | MeanAbsoluteError: 0.0457327142357826 | Loss: 0.0033400075703149 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0429349690675735 | Loss: 0.0033139792565060 | Epoch: 97 | MeanAbsoluteError: 0.0406634807586670 | Loss: 0.0027312449858169 | Epoch: 98 | MeanAbsoluteError: 0.0440043844282627 | Loss: 0.0036116520779232 | Epoch: 99 | MeanAbsoluteError: 0.0392075814306736 | Loss: 0.0027153647717664 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0510272309184074 | Loss: 0.0041394945215679 | Epoch: 101 | MeanAbsoluteError: 0.0447255596518517 | Loss: 0.0032087066777565 | Epoch: 102 | MeanAbsoluteError: 0.0459904931485653 | Loss: 0.0032000882324371 | Epoch: 103 | MeanAbsoluteError: 0.0445540919899940 | Loss: 0.0033564085116316 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0397237278521061 | Loss: 0.0028704397585611 | Epoch: 105 | MeanAbsoluteError: 0.0468754060566425 | Loss: 0.0043085485169860 | Epoch: 106 | MeanAbsoluteError: 0.0538074038922787 | Loss: 0.0053672626167291 | Epoch: 107 | MeanAbsoluteError: 0.0408504605293274 | Loss: 0.0034740751396289 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0429585427045822 | Loss: 0.0031729807256182 | Epoch: 109 | MeanAbsoluteError: 0.0440834797918797 | Loss: 0.0037573623558274 | Epoch: 110 | MeanAbsoluteError: 0.0414589606225491 | Loss: 0.0031895326147175 | Epoch: 111 | MeanAbsoluteError: 0.0416589118540287 | Loss: 0.0033368245617327 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0412781015038490 | Loss: 0.0033249287141524 | Epoch: 113 | MeanAbsoluteError: 0.0451746582984924 | Loss: 0.0034644880139048 | Epoch: 114 | MeanAbsoluteError: 0.0439899526536465 | Loss: 0.0034226202804252 | Epoch: 115 | MeanAbsoluteError: 0.0362359471619129 | Loss: 0.0025124995062422 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0364292450249195 | Loss: 0.0026003501476034 | Epoch: 117 | MeanAbsoluteError: 0.0468883179128170 | Loss: 0.0035091986340520 | Epoch: 118 | MeanAbsoluteError: 0.0504346564412117 | Loss: 0.0043637162674905 | Epoch: 119 | MeanAbsoluteError: 0.0435557439923286 | Loss: 0.0033148778081676 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0395594425499439 | Loss: 0.0032740356630376 | Epoch: 121 | MeanAbsoluteError: 0.0545294135808945 | Loss: 0.0048890726205795 | Epoch: 122 | MeanAbsoluteError: 0.0403546430170536 | Loss: 0.0032938346802870 | Epoch: 123 | \n\n\nMeanAbsoluteError: 0.0384179353713989 | Loss: 0.0025744389054843 | Epoch: 124 | MeanAbsoluteError: 0.0459996797144413 | Loss: 0.0038354636451818 | Epoch: 125 | MeanAbsoluteError: 0.0480953268706799 | Loss: 0.0040913942257727 | Epoch: 126 | MeanAbsoluteError: 0.0524273067712784 | Loss: 0.0049993038961762 | Epoch: 127 | \n\n\nMeanAbsoluteError: 0.0484561175107956 | Loss: 0.0037221552246544 | Epoch: 128 | MeanAbsoluteError: 0.0404690913856030 | Loss: 0.0030930281691805 | Epoch: 129 | MeanAbsoluteError: 0.0453326404094696 | Loss: 0.0038700337356300 | Epoch: 130 | MeanAbsoluteError: 0.0399830415844917 | Loss: 0.0032237035476627 | Epoch: 131 | \n\n\nMeanAbsoluteError: 0.0446662493050098 | Loss: 0.0036410658817898 | Epoch: 132 | MeanAbsoluteError: 0.0452163033187389 | Loss: 0.0039038896180787 | Epoch: 133 | MeanAbsoluteError: 0.0463493801653385 | Loss: 0.0038114855767497 | Epoch: 134 | MeanAbsoluteError: 0.0394623503088951 | Loss: 0.0025962453402922 | Epoch: 135 | \n\n\nMeanAbsoluteError: 0.0399547703564167 | Loss: 0.0030911046931961 | Epoch: 136 | MeanAbsoluteError: 0.0390558354556561 | Loss: 0.0033667942211890 | Epoch: 137 | MeanAbsoluteError: 0.0470521412789822 | Loss: 0.0039217000563727 | Epoch: 138 | MeanAbsoluteError: 0.0388789698481560 | Loss: 0.0030215465384045 | Epoch: 139 | \n\n\nMeanAbsoluteError: 0.0347169265151024 | Loss: 0.0023798049530198 | Epoch: 140 | MeanAbsoluteError: 0.0397176407277584 | Loss: 0.0027184262121170 | Epoch: 141 | MeanAbsoluteError: 0.0402794256806374 | Loss: 0.0030184878146668 | Epoch: 142 | MeanAbsoluteError: 0.0359130837023258 | Loss: 0.0025564546579661 | Epoch: 143 | \n\n\nMeanAbsoluteError: 0.0460473001003265 | Loss: 0.0034396862994120 | Epoch: 144 | MeanAbsoluteError: 0.0444099381566048 | Loss: 0.0034307878522668 | Epoch: 145 | MeanAbsoluteError: 0.0426484681665897 | Loss: 0.0033252822207655 | Epoch: 146 | MeanAbsoluteError: 0.0467832721769810 | Loss: 0.0042925656215582 | Epoch: 147 | \n\n\nMeanAbsoluteError: 0.0366268157958984 | Loss: 0.0026666902497374 | Epoch: 148 | MeanAbsoluteError: 0.0370317250490189 | Loss: 0.0027060983319595 | Epoch: 149 | MeanAbsoluteError: 0.0367013402283192 | Loss: 0.0026113480975925 | Epoch: 150 | MeanAbsoluteError: 0.0375338755548000 | Loss: 0.0029956502525908 | Epoch: 151 | \n\n\nMeanAbsoluteError: 0.0457814298570156 | Loss: 0.0036441219348681 | Epoch: 152 | MeanAbsoluteError: 0.0415855683386326 | Loss: 0.0031611157878376 | Epoch: 153 | MeanAbsoluteError: 0.0346399657428265 | Loss: 0.0023344256026719 | Epoch: 154 | MeanAbsoluteError: 0.0446813404560089 | Loss: 0.0039791403848396 | Epoch: 155 | \n\n\nMeanAbsoluteError: 0.0384975597262383 | Loss: 0.0027789743647813 | Epoch: 156 | MeanAbsoluteError: 0.0358452163636684 | Loss: 0.0023966104153691 | Epoch: 157 | MeanAbsoluteError: 0.0393751002848148 | Loss: 0.0029143915821087 | Epoch: 158 | MeanAbsoluteError: 0.0459643974900246 | Loss: 0.0035728946309782 | Epoch: 159 | \n\n\nMeanAbsoluteError: 0.0423121899366379 | Loss: 0.0033044185065113 | Epoch: 160 | MeanAbsoluteError: 0.0383162014186382 | Loss: 0.0026058180810651 | Epoch: 161 | MeanAbsoluteError: 0.0457266978919506 | Loss: 0.0035361871261174 | Epoch: 162 | MeanAbsoluteError: 0.0424456894397736 | Loss: 0.0031678579475020 | Epoch: 163 | \n\n\nMeanAbsoluteError: 0.0364864207804203 | Loss: 0.0025498644582082 | Epoch: 164 | MeanAbsoluteError: 0.0421980731189251 | Loss: 0.0028879528315309 | Epoch: 165 | MeanAbsoluteError: 0.0332209467887878 | Loss: 0.0021640758618320 | Epoch: 166 | MeanAbsoluteError: 0.0463715121150017 | Loss: 0.0035810783523821 | Epoch: 167 | \n\n\nMeanAbsoluteError: 0.0347659476101398 | Loss: 0.0022464603994434 | Epoch: 168 | MeanAbsoluteError: 0.0356247760355473 | Loss: 0.0023913703855789 | Epoch: 169 | MeanAbsoluteError: 0.0391595959663391 | Loss: 0.0028932687801946 | Epoch: 170 | MeanAbsoluteError: 0.0477165505290031 | Loss: 0.0041139374003059 | Epoch: 171 | \n\n\nMeanAbsoluteError: 0.0391102470457554 | Loss: 0.0027263947474574 | Epoch: 172 | MeanAbsoluteError: 0.0358642525970936 | Loss: 0.0029200312840801 | Epoch: 173 | MeanAbsoluteError: 0.0393927916884422 | Loss: 0.0029349388756887 | Epoch: 174 | MeanAbsoluteError: 0.0504653714597225 | Loss: 0.0041890841155117 | Epoch: 175 | \n\n\nMeanAbsoluteError: 0.0373914986848831 | Loss: 0.0029681553967652 | Epoch: 176 | MeanAbsoluteError: 0.0381548069417477 | Loss: 0.0029401363618059 | Epoch: 177 | MeanAbsoluteError: 0.0390880256891251 | Loss: 0.0027991831011605 | Epoch: 178 | MeanAbsoluteError: 0.0446569584310055 | Loss: 0.0034006005421122 | Epoch: 179 | \n\n\nMeanAbsoluteError: 0.0398855097591877 | Loss: 0.0028594718284072 | Epoch: 180 | MeanAbsoluteError: 0.0429512858390808 | Loss: 0.0035040811589600 | Epoch: 181 | MeanAbsoluteError: 0.0359716750681400 | Loss: 0.0024312784915214 | Epoch: 182 | MeanAbsoluteError: 0.0420269593596458 | Loss: 0.0029213254578951 | Epoch: 183 | \n\n\nMeanAbsoluteError: 0.0384062007069588 | Loss: 0.0026745505961835 | Epoch: 184 | MeanAbsoluteError: 0.0317529402673244 | Loss: 0.0022928251549418 | Epoch: 185 | MeanAbsoluteError: 0.0362841039896011 | Loss: 0.0025474179986448 | Epoch: 186 | MeanAbsoluteError: 0.0430733449757099 | Loss: 0.0033151673269458 | Epoch: 187 | \n\n\nMeanAbsoluteError: 0.0582897439599037 | Loss: 0.0047330782981589 | Epoch: 188 | MeanAbsoluteError: 0.0422103665769100 | Loss: 0.0030801664259726 | Epoch: 189 | MeanAbsoluteError: 0.0436674579977989 | Loss: 0.0033850157434292 | Epoch: 190 | MeanAbsoluteError: 0.0400523580610752 | Loss: 0.0029023930408967 | Epoch: 191 | \n\n\nMeanAbsoluteError: 0.0367833152413368 | Loss: 0.0027128866529797 | Epoch: 192 | MeanAbsoluteError: 0.0451920367777348 | Loss: 0.0033889277868806 | Epoch: 193 | MeanAbsoluteError: 0.0543144084513187 | Loss: 0.0051252240541783 | Epoch: 194 | MeanAbsoluteError: 0.0379489362239838 | Loss: 0.0028645209952772 | Epoch: 195 | \n\n\nMeanAbsoluteError: 0.0362335890531540 | Loss: 0.0023362010037281 | Epoch: 196 | MeanAbsoluteError: 0.0384349711239338 | Loss: 0.0030058410009492 | Epoch: 197 | MeanAbsoluteError: 0.0618425421416759 | Loss: 0.0066602358128875 | Epoch: 198 | MeanAbsoluteError: 0.0486262589693069 | Loss: 0.0045092841778808 | Epoch: 199 | \n\n\nMeanAbsoluteError: 0.0348447784781456 | Loss: 0.0024664705109133 | Epoch: 200 | MeanAbsoluteError: 0.0655846223235130 | Loss: 0.0065645788563415 | Epoch: 201 | MeanAbsoluteError: 0.0372694246470928 | Loss: 0.0028751832938532 | Epoch: 202 | MeanAbsoluteError: 0.0373912118375301 | Loss: 0.0023648683732302 | Epoch: 203 | \n\n\nMeanAbsoluteError: 0.0396722815930843 | Loss: 0.0031263532904344 | Epoch: 204 | MeanAbsoluteError: 0.0435742028057575 | Loss: 0.0036308791849909 | Epoch: 205 | MeanAbsoluteError: 0.0439013876020908 | Loss: 0.0035939320591033 | Epoch: 206 | MeanAbsoluteError: 0.0404686480760574 | Loss: 0.0030979362196185 | Epoch: 207 | \n\n\nMeanAbsoluteError: 0.0372690893709660 | Loss: 0.0025242125863626 | Epoch: 208 | MeanAbsoluteError: 0.0417134054005146 | Loss: 0.0032360961286988 | Epoch: 209 | MeanAbsoluteError: 0.0502009578049183 | Loss: 0.0045471348658841 | Epoch: 210 | MeanAbsoluteError: 0.0447604805231094 | Loss: 0.0034736719680950 | Epoch: 211 | \n\n\nMeanAbsoluteError: 0.0439983978867531 | Loss: 0.0034031998289528 | Epoch: 212 | MeanAbsoluteError: 0.0437414310872555 | Loss: 0.0033649879968786 | Epoch: 213 | MeanAbsoluteError: 0.0348468832671642 | Loss: 0.0024739301343767 | Epoch: 214 | MeanAbsoluteError: 0.0356610529124737 | Loss: 0.0025508366908118 | Epoch: 215 | \n\n\nMeanAbsoluteError: 0.0378197506070137 | Loss: 0.0024265021768913 | Epoch: 216 | MeanAbsoluteError: 0.0395811796188354 | Loss: 0.0030195840495014 | Epoch: 217 | MeanAbsoluteError: 0.0389065593481064 | Loss: 0.0026350427161608 | Epoch: 218 | MeanAbsoluteError: 0.0572971850633621 | Loss: 0.0049289595580807 | Epoch: 219 | \n\n\nMeanAbsoluteError: 0.0360941477119923 | Loss: 0.0025017682652871 | Epoch: 220 | MeanAbsoluteError: 0.0353940092027187 | Loss: 0.0020963414727272 | Epoch: 221 | MeanAbsoluteError: 0.0435257628560066 | Loss: 0.0034657275896997 | Epoch: 222 | MeanAbsoluteError: 0.0406438261270523 | Loss: 0.0027032917362368 | Epoch: 223 | \n\n\nMeanAbsoluteError: 0.0388551205396652 | Loss: 0.0026814431812933 | Epoch: 224 | MeanAbsoluteError: 0.0491702631115913 | Loss: 0.0045205788031269 | Epoch: 225 | MeanAbsoluteError: 0.0491867139935493 | Loss: 0.0041514269649174 | Epoch: 226 | MeanAbsoluteError: 0.0400542542338371 | Loss: 0.0030104166975147 | Epoch: 227 | \n\n\nMeanAbsoluteError: 0.0430674515664577 | Loss: 0.0032972475225841 | Epoch: 228 | MeanAbsoluteError: 0.0368786826729774 | Loss: 0.0026278859295417 | Epoch: 229 | MeanAbsoluteError: 0.0384484678506851 | Loss: 0.0029523037971414 | Epoch: 230 | MeanAbsoluteError: 0.0359239839017391 | Loss: 0.0024948913656102 | Epoch: 231 | \n\n\nMeanAbsoluteError: 0.0378324426710606 | Loss: 0.0028527656778399 | Epoch: 232 | MeanAbsoluteError: 0.0491792522370815 | Loss: 0.0040237548344425 | Epoch: 233 | MeanAbsoluteError: 0.0402098707854748 | Loss: 0.0027015749089371 | Epoch: 234 | MeanAbsoluteError: 0.0489675030112267 | Loss: 0.0039640494230154 | Epoch: 235 | \n\n\nMeanAbsoluteError: 0.0586858987808228 | Loss: 0.0053062336932941 | Epoch: 236 | MeanAbsoluteError: 0.0364255569875240 | Loss: 0.0024225319273108 | Epoch: 237 | MeanAbsoluteError: 0.0442326031625271 | Loss: 0.0036862836384803 | Epoch: 238 | MeanAbsoluteError: 0.0436502397060394 | Loss: 0.0033014413370677 | Epoch: 239 | \n\n\nMeanAbsoluteError: 0.0417223460972309 | Loss: 0.0030934942953632 | Epoch: 240 | MeanAbsoluteError: 0.0435906648635864 | Loss: 0.0035639303601592 | Epoch: 241 | MeanAbsoluteError: 0.0464707426726818 | Loss: 0.0038511495145749 | Epoch: 242 | MeanAbsoluteError: 0.0366733670234680 | Loss: 0.0026709146638352 | Epoch: 243 | \n\n\nMeanAbsoluteError: 0.0404129922389984 | Loss: 0.0026035554596762 | Epoch: 244 | MeanAbsoluteError: 0.0448841303586960 | Loss: 0.0037554898574997 | Epoch: 245 | MeanAbsoluteError: 0.0421559996902943 | Loss: 0.0033251109148535 | Epoch: 246 | MeanAbsoluteError: 0.0383120775222778 | Loss: 0.0029913841411288 | Epoch: 247 | \n\n\nMeanAbsoluteError: 0.0392761081457138 | Loss: 0.0030351035315298 | Epoch: 248 | MeanAbsoluteError: 0.0463172756135464 | Loss: 0.0040964109541260 | Epoch: 249 | MeanAbsoluteError: 0.0426288396120071 | Loss: 0.0030045447944614 | Epoch: 250 | MeanAbsoluteError: 0.0556920692324638 | Loss: 0.0046244519303161 | Epoch: 251 | \n\n\nMeanAbsoluteError: 0.0362177230417728 | Loss: 0.0024899556424077 | Epoch: 252 | MeanAbsoluteError: 0.0415796115994453 | Loss: 0.0030783311322969 | Epoch: 253 | MeanAbsoluteError: 0.0466689281165600 | Loss: 0.0039868756491495 | Epoch: 254 | MeanAbsoluteError: 0.0621741674840450 | Loss: 0.0061284547279540 | Epoch: 255 | \n\n\nMeanAbsoluteError: 0.0397331938147545 | Loss: 0.0030845723407478 | Epoch: 256 | MeanAbsoluteError: 0.0362316817045212 | Loss: 0.0026928081229256 | Returned to Spot: Validation loss: 0.0026928081229255583\n\n\n\nconfig: {'_L_in': 10, '_L_out': 1, 'l1': 32, 'dropout_prob': 0.37288241671787775, 'lr_mult': 5.251167900251978, 'batch_size': 8, 'epochs': 256, 'k_folds': 1, 'patience': 64, 'optimizer': 'Adam', 'sgd_momentum': 0.925546678557008}\nEpoch: 1 | MeanAbsoluteError: 0.1788003742694855 | Loss: 0.0495152713142728 | Epoch: 2 | MeanAbsoluteError: 0.1484744995832443 | Loss: 0.0352569957950005 | Epoch: 3 | MeanAbsoluteError: 0.1437907963991165 | Loss: 0.0338486939374554 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1298568546772003 | Loss: 0.0271337932392367 | Epoch: 5 | MeanAbsoluteError: 0.1220112591981888 | Loss: 0.0238924099034385 | Epoch: 6 | MeanAbsoluteError: 0.1256590485572815 | Loss: 0.0246970759165522 | Epoch: 7 | MeanAbsoluteError: 0.1271134018898010 | Loss: 0.0258046697433058 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.1174443811178207 | Loss: 0.0213969883165861 | Epoch: 9 | MeanAbsoluteError: 0.1049987301230431 | Loss: 0.0187767874143135 | Epoch: 10 | MeanAbsoluteError: 0.0970513373613358 | Loss: 0.0155399951373080 | Epoch: 11 | MeanAbsoluteError: 0.0996378734707832 | Loss: 0.0165102071753752 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0925909727811813 | Loss: 0.0149451987450256 | Epoch: 13 | MeanAbsoluteError: 0.0900110527873039 | Loss: 0.0134543035007817 | Epoch: 14 | MeanAbsoluteError: 0.0918070524930954 | Loss: 0.0133824798793189 | Epoch: 15 | MeanAbsoluteError: 0.0853181108832359 | Loss: 0.0111766204936430 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0869071856141090 | Loss: 0.0112271452273585 | Epoch: 17 | MeanAbsoluteError: 0.0818444117903709 | Loss: 0.0104420962927275 | Epoch: 18 | MeanAbsoluteError: 0.0807873457670212 | Loss: 0.0109589518611564 | Epoch: 19 | MeanAbsoluteError: 0.0886074677109718 | Loss: 0.0124582523774160 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0691571533679962 | Loss: 0.0086991865874121 | Epoch: 21 | MeanAbsoluteError: 0.0740849003195763 | Loss: 0.0100895385441713 | Epoch: 22 | MeanAbsoluteError: 0.0702500715851784 | Loss: 0.0090145933515343 | Epoch: 23 | MeanAbsoluteError: 0.0726152807474136 | Loss: 0.0088475783429060 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0651042386889458 | Loss: 0.0075447020415021 | Epoch: 25 | MeanAbsoluteError: 0.0639942884445190 | Loss: 0.0075745777193285 | Epoch: 26 | MeanAbsoluteError: 0.0751336291432381 | Loss: 0.0091181947010275 | Epoch: 27 | MeanAbsoluteError: 0.0695135220885277 | Loss: 0.0079060675588583 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0702764242887497 | Loss: 0.0082846923400403 | Epoch: 29 | MeanAbsoluteError: 0.0665449649095535 | Loss: 0.0075531119149865 | Epoch: 30 | MeanAbsoluteError: 0.0719570443034172 | Loss: 0.0091183335416166 | Epoch: 31 | MeanAbsoluteError: 0.0654135420918465 | Loss: 0.0075809256308467 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0676623135805130 | Loss: 0.0075514499667885 | Epoch: 33 | MeanAbsoluteError: 0.0580015294253826 | Loss: 0.0069734499250588 | Epoch: 34 | MeanAbsoluteError: 0.0545695498585701 | Loss: 0.0056960095535032 | Epoch: 35 | MeanAbsoluteError: 0.0634781941771507 | Loss: 0.0073325184394458 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0611042641103268 | Loss: 0.0063603241548040 | Epoch: 37 | MeanAbsoluteError: 0.0587673299014568 | Loss: 0.0067382957234881 | Epoch: 38 | MeanAbsoluteError: 0.0607418045401573 | Loss: 0.0067753561122931 | Epoch: 39 | MeanAbsoluteError: 0.0586076676845551 | Loss: 0.0067486781933296 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0556235313415527 | Loss: 0.0056541893833415 | Epoch: 41 | MeanAbsoluteError: 0.0528657436370850 | Loss: 0.0060976605102616 | Epoch: 42 | MeanAbsoluteError: 0.0623977743089199 | Loss: 0.0059548661014751 | Epoch: 43 | MeanAbsoluteError: 0.0576801672577858 | Loss: 0.0058120296759482 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0617263615131378 | Loss: 0.0063483300955819 | Epoch: 45 | MeanAbsoluteError: 0.0555348284542561 | Loss: 0.0054748593400674 | Epoch: 46 | MeanAbsoluteError: 0.0608731508255005 | Loss: 0.0070327689080793 | Epoch: 47 | MeanAbsoluteError: 0.0570939891040325 | Loss: 0.0054438970247774 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0587859973311424 | Loss: 0.0059289421739155 | Epoch: 49 | MeanAbsoluteError: 0.0591227412223816 | Loss: 0.0063783441743764 | Epoch: 50 | MeanAbsoluteError: 0.0494535341858864 | Loss: 0.0045940379143423 | Epoch: 51 | MeanAbsoluteError: 0.0518842563033104 | Loss: 0.0053058772004749 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0701239928603172 | Loss: 0.0076898326913483 | Epoch: 53 | MeanAbsoluteError: 0.0488914698362350 | Loss: 0.0047377705295252 | Epoch: 54 | MeanAbsoluteError: 0.0593264587223530 | Loss: 0.0058872234535796 | Epoch: 55 | MeanAbsoluteError: 0.0516217499971390 | Loss: 0.0053266753062704 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0498078912496567 | Loss: 0.0047875959625296 | Epoch: 57 | MeanAbsoluteError: 0.0520175099372864 | Loss: 0.0056076840685962 | Epoch: 58 | MeanAbsoluteError: 0.0510582365095615 | Loss: 0.0051449965593699 | Epoch: 59 | MeanAbsoluteError: 0.0585960447788239 | Loss: 0.0061260902163524 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0459827110171318 | Loss: 0.0049303178246574 | Epoch: 61 | MeanAbsoluteError: 0.0439704582095146 | Loss: 0.0043098582058232 | Epoch: 62 | MeanAbsoluteError: 0.0548916608095169 | Loss: 0.0056283151099801 | Epoch: 63 | MeanAbsoluteError: 0.0473905280232430 | Loss: 0.0043927040548480 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0602939277887344 | Loss: 0.0059366704448812 | Epoch: 65 | MeanAbsoluteError: 0.0555891245603561 | Loss: 0.0052891807894124 | Epoch: 66 | MeanAbsoluteError: 0.0440952852368355 | Loss: 0.0043026893476245 | Epoch: 67 | MeanAbsoluteError: 0.0462158843874931 | Loss: 0.0049354237608465 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0489712730050087 | Loss: 0.0058278386652666 | Epoch: 69 | MeanAbsoluteError: 0.0533244609832764 | Loss: 0.0050766499786589 | Epoch: 70 | MeanAbsoluteError: 0.0457734689116478 | Loss: 0.0048989663758364 | Epoch: 71 | MeanAbsoluteError: 0.0566029436886311 | Loss: 0.0058291288135622 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0550025887787342 | Loss: 0.0054458027221508 | Epoch: 73 | MeanAbsoluteError: 0.0439804270863533 | Loss: 0.0040467973740306 | Epoch: 74 | MeanAbsoluteError: 0.0517812818288803 | Loss: 0.0055495534416925 | Epoch: 75 | MeanAbsoluteError: 0.0508720688521862 | Loss: 0.0055459603195471 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0500964149832726 | Loss: 0.0047584191718335 | Epoch: 77 | MeanAbsoluteError: 0.0627663582563400 | Loss: 0.0080490499109912 | Epoch: 78 | MeanAbsoluteError: 0.0503186136484146 | Loss: 0.0055362482173797 | Epoch: 79 | MeanAbsoluteError: 0.0533462576568127 | Loss: 0.0060468623261458 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0496645607054234 | Loss: 0.0055797339576346 | Epoch: 81 | MeanAbsoluteError: 0.0587679669260979 | Loss: 0.0076378562049573 | Epoch: 82 | MeanAbsoluteError: 0.0644006058573723 | Loss: 0.0070087496944899 | Epoch: 83 | MeanAbsoluteError: 0.0528826713562012 | Loss: 0.0057493814107913 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0499895811080933 | Loss: 0.0047790685815639 | Epoch: 85 | MeanAbsoluteError: 0.0477929934859276 | Loss: 0.0049858797268060 | Epoch: 86 | MeanAbsoluteError: 0.0580549426376820 | Loss: 0.0057543204855296 | Epoch: 87 | MeanAbsoluteError: 0.0584454424679279 | Loss: 0.0063368780232084 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0523155555129051 | Loss: 0.0059390475599732 | Epoch: 89 | MeanAbsoluteError: 0.0520558059215546 | Loss: 0.0058749795578861 | Epoch: 90 | MeanAbsoluteError: 0.0463036373257637 | Loss: 0.0038127907786187 | Epoch: 91 | MeanAbsoluteError: 0.0493869706988335 | Loss: 0.0053666591457410 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0442037358880043 | Loss: 0.0046364528365972 | Epoch: 93 | MeanAbsoluteError: 0.0530945323407650 | Loss: 0.0058990439099848 | Epoch: 94 | MeanAbsoluteError: 0.0468620881438255 | Loss: 0.0048464517902558 | Epoch: 95 | MeanAbsoluteError: 0.0498133674263954 | Loss: 0.0054202044595609 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0595665499567986 | Loss: 0.0063402700164404 | Epoch: 97 | MeanAbsoluteError: 0.0507498271763325 | Loss: 0.0047715200236876 | Epoch: 98 | MeanAbsoluteError: 0.0501660071313381 | Loss: 0.0053612040003492 | Epoch: 99 | MeanAbsoluteError: 0.0480739511549473 | Loss: 0.0047944725631264 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0485140010714531 | Loss: 0.0042178012418414 | Epoch: 101 | MeanAbsoluteError: 0.0611505769193172 | Loss: 0.0068289857360804 | Epoch: 102 | MeanAbsoluteError: 0.0505498312413692 | Loss: 0.0050762250719249 | Epoch: 103 | MeanAbsoluteError: 0.0484622120857239 | Loss: 0.0042084197316159 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0553912557661533 | Loss: 0.0054599824269596 | Epoch: 105 | MeanAbsoluteError: 0.0409590452909470 | Loss: 0.0047228629214499 | Epoch: 106 | MeanAbsoluteError: 0.0535028092563152 | Loss: 0.0059362589220798 | Epoch: 107 | MeanAbsoluteError: 0.0463519841432571 | Loss: 0.0053539813793338 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0462686643004417 | Loss: 0.0045380837599575 | Epoch: 109 | MeanAbsoluteError: 0.0508206114172935 | Loss: 0.0047010108958746 | Epoch: 110 | MeanAbsoluteError: 0.0479105897247791 | Loss: 0.0051968404609636 | Epoch: 111 | MeanAbsoluteError: 0.0427098721265793 | Loss: 0.0044201499258634 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0460737310349941 | Loss: 0.0051655112375125 | Epoch: 113 | MeanAbsoluteError: 0.0650523602962494 | Loss: 0.0067283518912614 | Epoch: 114 | MeanAbsoluteError: 0.0423309914767742 | Loss: 0.0040139557593073 | Epoch: 115 | MeanAbsoluteError: 0.0486193150281906 | Loss: 0.0047400111578004 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0451689288020134 | Loss: 0.0036978110530120 | Epoch: 117 | MeanAbsoluteError: 0.0428737625479698 | Loss: 0.0037590979060808 | Epoch: 118 | MeanAbsoluteError: 0.0440466590225697 | Loss: 0.0036843878119008 | Epoch: 119 | MeanAbsoluteError: 0.0397822521626949 | Loss: 0.0035790438877915 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0549111887812614 | Loss: 0.0055069764731391 | Epoch: 121 | MeanAbsoluteError: 0.0479137152433395 | Loss: 0.0050247456284093 | Epoch: 122 | MeanAbsoluteError: 0.0442284457385540 | Loss: 0.0042157993402246 | Epoch: 123 | MeanAbsoluteError: 0.0512056723237038 | Loss: 0.0056185786302912 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0442824810743332 | Loss: 0.0044877662171769 | Epoch: 125 | MeanAbsoluteError: 0.0442082472145557 | Loss: 0.0042548820201773 | Epoch: 126 | MeanAbsoluteError: 0.0514594279229641 | Loss: 0.0049511027510131 | Epoch: 127 | MeanAbsoluteError: 0.0499363020062447 | Loss: 0.0044798523426595 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0461187921464443 | Loss: 0.0047382786418127 | Epoch: 129 | MeanAbsoluteError: 0.0533568896353245 | Loss: 0.0051233590991040 | Epoch: 130 | MeanAbsoluteError: 0.0505236089229584 | Loss: 0.0064623706842356 | Epoch: 131 | MeanAbsoluteError: 0.0488856509327888 | Loss: 0.0051083230125522 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.0482578836381435 | Loss: 0.0051249735368305 | Epoch: 133 | MeanAbsoluteError: 0.0418444238603115 | Loss: 0.0031777717338532 | Epoch: 134 | MeanAbsoluteError: 0.0546250417828560 | Loss: 0.0058466261834837 | Epoch: 135 | MeanAbsoluteError: 0.0926701277494431 | Loss: 0.0120652922809026 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.0541116334497929 | Loss: 0.0059903155626288 | Epoch: 137 | MeanAbsoluteError: 0.0416741259396076 | Loss: 0.0041229926129362 | Epoch: 138 | MeanAbsoluteError: 0.0435509495437145 | Loss: 0.0048651089700319 | Epoch: 139 | MeanAbsoluteError: 0.0443377010524273 | Loss: 0.0037267694592868 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.0500821098685265 | Loss: 0.0048757645632385 | Epoch: 141 | MeanAbsoluteError: 0.0618361793458462 | Loss: 0.0071789838900594 | Epoch: 142 | MeanAbsoluteError: 0.0593284852802753 | Loss: 0.0074732272815278 | Epoch: 143 | MeanAbsoluteError: 0.0492683686316013 | Loss: 0.0051416600242527 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.0541899614036083 | Loss: 0.0056043512733212 | Epoch: 145 | MeanAbsoluteError: 0.0544170364737511 | Loss: 0.0065898107315757 | Epoch: 146 | MeanAbsoluteError: 0.0621706955134869 | Loss: 0.0067471640313191 | Epoch: 147 | MeanAbsoluteError: 0.0710065364837646 | Loss: 0.0091246172975700 | Epoch: 148 | \n\n\nMeanAbsoluteError: 0.0524544604122639 | Loss: 0.0064890929283329 | Epoch: 149 | MeanAbsoluteError: 0.0580127090215683 | Loss: 0.0078501763958852 | Epoch: 150 | MeanAbsoluteError: 0.0474566966295242 | Loss: 0.0056332595122512 | Epoch: 151 | MeanAbsoluteError: 0.0481439419090748 | Loss: 0.0058382279040463 | Epoch: 152 | \n\n\nMeanAbsoluteError: 0.0486965738236904 | Loss: 0.0055117991808067 | Epoch: 153 | MeanAbsoluteError: 0.0507362857460976 | Loss: 0.0055018875711156 | Epoch: 154 | MeanAbsoluteError: 0.0505257211625576 | Loss: 0.0058545108553708 | Epoch: 155 | MeanAbsoluteError: 0.0600244849920273 | Loss: 0.0068716657666588 | Epoch: 156 | \n\n\nMeanAbsoluteError: 0.0481324978172779 | Loss: 0.0048244536109881 | Epoch: 157 | MeanAbsoluteError: 0.0473736263811588 | Loss: 0.0054851596000126 | Epoch: 158 | MeanAbsoluteError: 0.0456356033682823 | Loss: 0.0050214195722028 | Epoch: 159 | MeanAbsoluteError: 0.0435530953109264 | Loss: 0.0043418077356795 | Epoch: 160 | \n\n\nMeanAbsoluteError: 0.0477509796619415 | Loss: 0.0060095056867699 | Epoch: 161 | MeanAbsoluteError: 0.0524843074381351 | Loss: 0.0058893273633562 | Epoch: 162 | MeanAbsoluteError: 0.0499815084040165 | Loss: 0.0056886541546862 | Epoch: 163 | MeanAbsoluteError: 0.0437492430210114 | Loss: 0.0051457384696206 | Epoch: 164 | \n\n\nMeanAbsoluteError: 0.0494947917759418 | Loss: 0.0051071424754091 | Epoch: 165 | MeanAbsoluteError: 0.0533304400742054 | Loss: 0.0066956794976968 | Epoch: 166 | MeanAbsoluteError: 0.0644447281956673 | Loss: 0.0076125916154859 | Epoch: 167 | MeanAbsoluteError: 0.0473401956260204 | Loss: 0.0066497268754143 | Epoch: 168 | \n\n\nMeanAbsoluteError: 0.0472682490944862 | Loss: 0.0063506998520001 | Epoch: 169 | MeanAbsoluteError: 0.0492614880204201 | Loss: 0.0059984694863057 | Epoch: 170 | MeanAbsoluteError: 0.0503905005753040 | Loss: 0.0056042647644828 | Epoch: 171 | MeanAbsoluteError: 0.0442466735839844 | Loss: 0.0053956771673831 | Epoch: 172 | \n\n\nMeanAbsoluteError: 0.0496078655123711 | Loss: 0.0051302352328907 | Epoch: 173 | MeanAbsoluteError: 0.0479540266096592 | Loss: 0.0049760512499090 | Epoch: 174 | MeanAbsoluteError: 0.0422492139041424 | Loss: 0.0045315861207200 | Epoch: 175 | MeanAbsoluteError: 0.0482688732445240 | Loss: 0.0055035228801180 | Epoch: 176 | \n\n\nMeanAbsoluteError: 0.0436020791530609 | Loss: 0.0052246247880823 | Epoch: 177 | MeanAbsoluteError: 0.0564588420093060 | Loss: 0.0061533711467698 | Epoch: 178 | MeanAbsoluteError: 0.0508911497890949 | Loss: 0.0050301815897815 | Epoch: 179 | MeanAbsoluteError: 0.0672817230224609 | Loss: 0.0077458531381317 | Epoch: 180 | \n\n\nMeanAbsoluteError: 0.0460721738636494 | Loss: 0.0047806577824399 | Epoch: 181 | MeanAbsoluteError: 0.0510121844708920 | Loss: 0.0060680349905804 | Epoch: 182 | MeanAbsoluteError: 0.0448247455060482 | Loss: 0.0046610297653579 | Epoch: 183 | MeanAbsoluteError: 0.0466562472283840 | Loss: 0.0050779630469869 | Epoch: 184 | \n\n\nMeanAbsoluteError: 0.0489644929766655 | Loss: 0.0050565443923819 | Epoch: 185 | MeanAbsoluteError: 0.0575930327177048 | Loss: 0.0073179692814225 | Epoch: 186 | MeanAbsoluteError: 0.0580960139632225 | Loss: 0.0059634323674560 | Epoch: 187 | MeanAbsoluteError: 0.0423379130661488 | Loss: 0.0045391563915856 | Epoch: 188 | \n\n\nMeanAbsoluteError: 0.0574076846241951 | Loss: 0.0059305921558438 | Epoch: 189 | MeanAbsoluteError: 0.0594052299857140 | Loss: 0.0082433239440434 | Epoch: 190 | MeanAbsoluteError: 0.0489445067942142 | Loss: 0.0046851342746155 | Epoch: 191 | MeanAbsoluteError: 0.0451365932822227 | Loss: 0.0043536202329530 | Epoch: 192 | \n\n\nMeanAbsoluteError: 0.0540440678596497 | Loss: 0.0057669042366079 | Epoch: 193 | MeanAbsoluteError: 0.0509759001433849 | Loss: 0.0049522462772745 | Epoch: 194 | MeanAbsoluteError: 0.0445080734789371 | Loss: 0.0050656439852901 | Epoch: 195 | \n\n\nMeanAbsoluteError: 0.0463104806840420 | Loss: 0.0058541475905498 | Epoch: 196 | MeanAbsoluteError: 0.0428176969289780 | Loss: 0.0048344730258625 | Epoch: 197 | MeanAbsoluteError: 0.0552312396466732 | Loss: 0.0064801622448661 | Early stopping at epoch 196\nReturned to Spot: Validation loss: 0.006480162244866063\n\n\nspotPython tuning: 0.0026928081229255583 [##--------] 21.75% \n\n\n\nconfig: {'_L_in': 10, '_L_out': 1, 'l1': 32, 'dropout_prob': 0.4963545188846238, 'lr_mult': 2.30619801403408, 'batch_size': 8, 'epochs': 2048, 'k_folds': 1, 'patience': 64, 'optimizer': 'Adam', 'sgd_momentum': 0.7648944431996759}\nEpoch: 1 | MeanAbsoluteError: 0.1445112526416779 | Loss: 0.0332820704206824 | Epoch: 2 | MeanAbsoluteError: 0.1355931162834167 | Loss: 0.0298400232078213 | Epoch: 3 | MeanAbsoluteError: 0.1335650980472565 | Loss: 0.0290617803601842 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1348835378885269 | Loss: 0.0290810958678393 | Epoch: 5 | MeanAbsoluteError: 0.1257081031799316 | Loss: 0.0240295848282250 | Epoch: 6 | MeanAbsoluteError: 0.1227420791983604 | Loss: 0.0239549448076440 | Epoch: 7 | MeanAbsoluteError: 0.1186374649405479 | Loss: 0.0210924580154058 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.1107496097683907 | Loss: 0.0197842462282432 | Epoch: 9 | MeanAbsoluteError: 0.1091675162315369 | Loss: 0.0197294875734339 | Epoch: 10 | MeanAbsoluteError: 0.1129201129078865 | Loss: 0.0202298521211273 | Epoch: 11 | MeanAbsoluteError: 0.1076966747641563 | Loss: 0.0189989986952002 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.1013517454266548 | Loss: 0.0161336000739156 | Epoch: 13 | MeanAbsoluteError: 0.1053119674324989 | Loss: 0.0171227594347377 | Epoch: 14 | MeanAbsoluteError: 0.0970462709665298 | Loss: 0.0150121678399094 | Epoch: 15 | MeanAbsoluteError: 0.0930029451847076 | Loss: 0.0146758258587828 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0950367376208305 | Loss: 0.0143309225708148 | Epoch: 17 | MeanAbsoluteError: 0.0951478257775307 | Loss: 0.0149966479665110 | Epoch: 18 | MeanAbsoluteError: 0.0922853350639343 | Loss: 0.0130760302019649 | Epoch: 19 | MeanAbsoluteError: 0.0895099788904190 | Loss: 0.0123937821603919 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0832956433296204 | Loss: 0.0108596876226856 | Epoch: 21 | MeanAbsoluteError: 0.0810806900262833 | Loss: 0.0111148868423985 | Epoch: 22 | MeanAbsoluteError: 0.0844098553061485 | Loss: 0.0109319090475573 | Epoch: 23 | MeanAbsoluteError: 0.0813331678509712 | Loss: 0.0112429540543082 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0803490355610847 | Loss: 0.0108466404612715 | Epoch: 25 | MeanAbsoluteError: 0.0786239355802536 | Loss: 0.0099694566543851 | Epoch: 26 | MeanAbsoluteError: 0.0880240052938461 | Loss: 0.0119001437897647 | Epoch: 27 | MeanAbsoluteError: 0.0737877562642097 | Loss: 0.0084494651711889 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0732731595635414 | Loss: 0.0096352956813797 | Epoch: 29 | MeanAbsoluteError: 0.0705563351511955 | Loss: 0.0083055865436204 | Epoch: 30 | MeanAbsoluteError: 0.0725688785314560 | Loss: 0.0090622492012029 | Epoch: 31 | MeanAbsoluteError: 0.0736944600939751 | Loss: 0.0084369094080390 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0681341886520386 | Loss: 0.0073999187504677 | Epoch: 33 | MeanAbsoluteError: 0.0570420511066914 | Loss: 0.0060372415755410 | Epoch: 34 | MeanAbsoluteError: 0.0666058510541916 | Loss: 0.0075596849170611 | Epoch: 35 | MeanAbsoluteError: 0.0653619691729546 | Loss: 0.0069728245113143 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0606943070888519 | Loss: 0.0063503252734479 | Epoch: 37 | MeanAbsoluteError: 0.0557897835969925 | Loss: 0.0057970621764023 | Epoch: 38 | MeanAbsoluteError: 0.0596126168966293 | Loss: 0.0061665684977350 | Epoch: 39 | MeanAbsoluteError: 0.0620498545467854 | Loss: 0.0065618791635834 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0598894879221916 | Loss: 0.0063304104811610 | Epoch: 41 | MeanAbsoluteError: 0.0549854226410389 | Loss: 0.0052400716905188 | Epoch: 42 | MeanAbsoluteError: 0.0663814768195152 | Loss: 0.0076143486382391 | Epoch: 43 | MeanAbsoluteError: 0.0653294995427132 | Loss: 0.0076031340408678 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0588328987360001 | Loss: 0.0060247682451614 | Epoch: 45 | MeanAbsoluteError: 0.0567927733063698 | Loss: 0.0059476567272979 | Epoch: 46 | MeanAbsoluteError: 0.0634067729115486 | Loss: 0.0067552808966292 | Epoch: 47 | MeanAbsoluteError: 0.0607420019805431 | Loss: 0.0061292744756333 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0538666918873787 | Loss: 0.0056135209528484 | Epoch: 49 | MeanAbsoluteError: 0.0489527210593224 | Loss: 0.0041298468453246 | Epoch: 50 | MeanAbsoluteError: 0.0594216585159302 | Loss: 0.0059090108184855 | Epoch: 51 | MeanAbsoluteError: 0.0513624809682369 | Loss: 0.0046073082760957 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0509896874427795 | Loss: 0.0051080561978252 | Epoch: 53 | MeanAbsoluteError: 0.0559132136404514 | Loss: 0.0061146089222013 | Epoch: 54 | MeanAbsoluteError: 0.0574104301631451 | Loss: 0.0057476794310404 | Epoch: 55 | MeanAbsoluteError: 0.0575681701302528 | Loss: 0.0055780699752320 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0504116043448448 | Loss: 0.0047406763371423 | Epoch: 57 | MeanAbsoluteError: 0.0579606182873249 | Loss: 0.0058420026800473 | Epoch: 58 | MeanAbsoluteError: 0.0540390089154243 | Loss: 0.0053513148450293 | Epoch: 59 | MeanAbsoluteError: 0.0514649674296379 | Loss: 0.0048829831196743 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0459657460451126 | Loss: 0.0041332440282292 | Epoch: 61 | MeanAbsoluteError: 0.0529360659420490 | Loss: 0.0048568615058780 | Epoch: 62 | MeanAbsoluteError: 0.0479884259402752 | Loss: 0.0044233066896834 | Epoch: 63 | MeanAbsoluteError: 0.0544039532542229 | Loss: 0.0049663110096988 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0488170608878136 | Loss: 0.0044584785967967 | Epoch: 65 | MeanAbsoluteError: 0.0559838227927685 | Loss: 0.0054894037066812 | Epoch: 66 | MeanAbsoluteError: 0.0636463463306427 | Loss: 0.0063847481352767 | Epoch: 67 | MeanAbsoluteError: 0.0493822768330574 | Loss: 0.0044754746547704 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0546382963657379 | Loss: 0.0053813544769869 | Epoch: 69 | MeanAbsoluteError: 0.0488552115857601 | Loss: 0.0052400177810341 | Epoch: 70 | MeanAbsoluteError: 0.0682816356420517 | Loss: 0.0073295428178665 | Epoch: 71 | MeanAbsoluteError: 0.0604398362338543 | Loss: 0.0060765127479834 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0490662530064583 | Loss: 0.0045370911922012 | Epoch: 73 | MeanAbsoluteError: 0.0505478680133820 | Loss: 0.0046048059501979 | Epoch: 74 | MeanAbsoluteError: 0.0624685399234295 | Loss: 0.0063393230206872 | Epoch: 75 | MeanAbsoluteError: 0.0530062913894653 | Loss: 0.0050595090280311 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0521521121263504 | Loss: 0.0050122040020621 | Epoch: 77 | MeanAbsoluteError: 0.0588915236294270 | Loss: 0.0061121175807018 | Epoch: 78 | MeanAbsoluteError: 0.0540672242641449 | Loss: 0.0052168395777699 | Epoch: 79 | MeanAbsoluteError: 0.0555727072060108 | Loss: 0.0054487874792693 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0526589863002300 | Loss: 0.0043027348338479 | Epoch: 81 | MeanAbsoluteError: 0.0533140599727631 | Loss: 0.0055384132183941 | Epoch: 82 | MeanAbsoluteError: 0.0494821965694427 | Loss: 0.0041473884423459 | Epoch: 83 | MeanAbsoluteError: 0.0526374280452728 | Loss: 0.0047222335614558 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0445261336863041 | Loss: 0.0035365770025620 | Epoch: 85 | MeanAbsoluteError: 0.0493604503571987 | Loss: 0.0044125484357784 | Epoch: 86 | MeanAbsoluteError: 0.0475554540753365 | Loss: 0.0038555737073215 | Epoch: 87 | MeanAbsoluteError: 0.0553306825459003 | Loss: 0.0053452758936481 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0584815666079521 | Loss: 0.0057560990650305 | Epoch: 89 | MeanAbsoluteError: 0.0549016334116459 | Loss: 0.0048397066451511 | Epoch: 90 | MeanAbsoluteError: 0.0474520139396191 | Loss: 0.0043389529087826 | Epoch: 91 | MeanAbsoluteError: 0.0482886135578156 | Loss: 0.0042966168058276 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0461248569190502 | Loss: 0.0037750517015076 | Epoch: 93 | MeanAbsoluteError: 0.0502140559256077 | Loss: 0.0041587947834112 | Epoch: 94 | MeanAbsoluteError: 0.0573170967400074 | Loss: 0.0056460617128515 | Epoch: 95 | MeanAbsoluteError: 0.0550860464572906 | Loss: 0.0051562234293669 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0481077879667282 | Loss: 0.0039671596176432 | Epoch: 97 | MeanAbsoluteError: 0.0487364381551743 | Loss: 0.0043772286260623 | Epoch: 98 | MeanAbsoluteError: 0.0519358031451702 | Loss: 0.0048246871110199 | Epoch: 99 | MeanAbsoluteError: 0.0575206503272057 | Loss: 0.0055777405688882 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0516895167529583 | Loss: 0.0045968089252710 | Epoch: 101 | MeanAbsoluteError: 0.0459341704845428 | Loss: 0.0040134824628599 | Epoch: 102 | MeanAbsoluteError: 0.0508887395262718 | Loss: 0.0048104918708927 | Epoch: 103 | MeanAbsoluteError: 0.0446634627878666 | Loss: 0.0033371507844621 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0480608381330967 | Loss: 0.0042143047581990 | Epoch: 105 | MeanAbsoluteError: 0.0450639724731445 | Loss: 0.0037420395942478 | Epoch: 106 | MeanAbsoluteError: 0.0520322769880295 | Loss: 0.0047408946174025 | Epoch: 107 | MeanAbsoluteError: 0.0524176880717278 | Loss: 0.0052128787608940 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0483536869287491 | Loss: 0.0039200340030029 | Epoch: 109 | MeanAbsoluteError: 0.0436943136155605 | Loss: 0.0038326450066004 | Epoch: 110 | MeanAbsoluteError: 0.0455863885581493 | Loss: 0.0040165322333403 | Epoch: 111 | MeanAbsoluteError: 0.0540837235748768 | Loss: 0.0052114292966986 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0481110811233521 | Loss: 0.0050095674064084 | Epoch: 113 | MeanAbsoluteError: 0.0529083311557770 | Loss: 0.0049460196781805 | Epoch: 114 | MeanAbsoluteError: 0.0477158054709435 | Loss: 0.0050256374379387 | Epoch: 115 | MeanAbsoluteError: 0.0449558086693287 | Loss: 0.0041369791357092 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0480180345475674 | Loss: 0.0041182890984744 | Epoch: 117 | MeanAbsoluteError: 0.0551816932857037 | Loss: 0.0051125202796692 | Epoch: 118 | MeanAbsoluteError: 0.0564595535397530 | Loss: 0.0057688546244447 | Epoch: 119 | MeanAbsoluteError: 0.0487187020480633 | Loss: 0.0046125254609107 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0471959076821804 | Loss: 0.0043777440533679 | Epoch: 121 | MeanAbsoluteError: 0.0462834388017654 | Loss: 0.0035297710610872 | Epoch: 122 | MeanAbsoluteError: 0.0604779049754143 | Loss: 0.0057392957724484 | Epoch: 123 | MeanAbsoluteError: 0.0494473390281200 | Loss: 0.0037965436073902 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0518596209585667 | Loss: 0.0048599343373146 | Epoch: 125 | MeanAbsoluteError: 0.0511408522725105 | Loss: 0.0043527099156850 | Epoch: 126 | MeanAbsoluteError: 0.0590190552175045 | Loss: 0.0060206717829906 | Epoch: 127 | MeanAbsoluteError: 0.0619754344224930 | Loss: 0.0064173482854753 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0501530468463898 | Loss: 0.0047785232921964 | Epoch: 129 | MeanAbsoluteError: 0.0478816777467728 | Loss: 0.0042783977742625 | Epoch: 130 | MeanAbsoluteError: 0.0539482235908508 | Loss: 0.0050610459048154 | Epoch: 131 | MeanAbsoluteError: 0.0477845109999180 | Loss: 0.0042853091359384 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.0510971061885357 | Loss: 0.0044859063591024 | Epoch: 133 | MeanAbsoluteError: 0.0480275042355061 | Loss: 0.0041628436837958 | Epoch: 134 | MeanAbsoluteError: 0.0514216311275959 | Loss: 0.0049327115917358 | Epoch: 135 | MeanAbsoluteError: 0.0582421496510506 | Loss: 0.0051135444327405 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.0516047365963459 | Loss: 0.0043707964482325 | Epoch: 137 | MeanAbsoluteError: 0.0425158105790615 | Loss: 0.0035277401483128 | Epoch: 138 | MeanAbsoluteError: 0.0493979640305042 | Loss: 0.0045251211972515 | Epoch: 139 | MeanAbsoluteError: 0.0441518947482109 | Loss: 0.0037184906893679 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.0582973547279835 | Loss: 0.0054465552889987 | Epoch: 141 | MeanAbsoluteError: 0.0519024729728699 | Loss: 0.0051040068869234 | Epoch: 142 | MeanAbsoluteError: 0.0528804101049900 | Loss: 0.0046749842402182 | Epoch: 143 | MeanAbsoluteError: 0.0503691360354424 | Loss: 0.0044208094293840 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.0594334863126278 | Loss: 0.0060934619212180 | Epoch: 145 | MeanAbsoluteError: 0.0448886603116989 | Loss: 0.0036753435269929 | Epoch: 146 | MeanAbsoluteError: 0.0498350858688354 | Loss: 0.0045456553190505 | Epoch: 147 | MeanAbsoluteError: 0.0487548597157001 | Loss: 0.0042444908115278 | Epoch: 148 | \n\n\nMeanAbsoluteError: 0.0649130120873451 | Loss: 0.0061742004916979 | Epoch: 149 | MeanAbsoluteError: 0.0454035140573978 | Loss: 0.0034167704702428 | Epoch: 150 | MeanAbsoluteError: 0.0504807010293007 | Loss: 0.0043602325328577 | Epoch: 151 | MeanAbsoluteError: 0.0508321113884449 | Loss: 0.0050566821921261 | Epoch: 152 | \n\n\nMeanAbsoluteError: 0.0505737513303757 | Loss: 0.0043919861230901 | Epoch: 153 | MeanAbsoluteError: 0.0462934784591198 | Loss: 0.0038505013891202 | Epoch: 154 | MeanAbsoluteError: 0.0523270443081856 | Loss: 0.0043963562899367 | Epoch: 155 | MeanAbsoluteError: 0.0489914789795876 | Loss: 0.0048644252467631 | Epoch: 156 | \n\n\nMeanAbsoluteError: 0.0453770533204079 | Loss: 0.0041492053852907 | Epoch: 157 | MeanAbsoluteError: 0.0483201555907726 | Loss: 0.0050053852150756 | Epoch: 158 | MeanAbsoluteError: 0.0492590479552746 | Loss: 0.0041773544538668 | Epoch: 159 | MeanAbsoluteError: 0.0495357997715473 | Loss: 0.0045022874500750 | Epoch: 160 | \n\n\nMeanAbsoluteError: 0.0449782982468605 | Loss: 0.0038336778800984 | Epoch: 161 | MeanAbsoluteError: 0.0449270159006119 | Loss: 0.0037534266333463 | Epoch: 162 | MeanAbsoluteError: 0.0478990301489830 | Loss: 0.0039296186204362 | Epoch: 163 | MeanAbsoluteError: 0.0404757000505924 | Loss: 0.0033738767930405 | Epoch: 164 | \n\n\nMeanAbsoluteError: 0.0441668108105659 | Loss: 0.0034728734853592 | Epoch: 165 | MeanAbsoluteError: 0.0559108853340149 | Loss: 0.0057448442109346 | Epoch: 166 | MeanAbsoluteError: 0.0594354197382927 | Loss: 0.0053861783296605 | Epoch: 167 | MeanAbsoluteError: 0.0476232133805752 | Loss: 0.0046751723221908 | Early stopping at epoch 166\nReturned to Spot: Validation loss: 0.0046751723221907585\n\n\nspotPython tuning: 0.0026928081229255583 [####------] 40.98% \n\n\n\nconfig: {'_L_in': 10, '_L_out': 1, 'l1': 32, 'dropout_prob': 0.5193951384270946, 'lr_mult': 0.5759385081082424, 'batch_size': 8, 'epochs': 2048, 'k_folds': 1, 'patience': 64, 'optimizer': 'Adam', 'sgd_momentum': 0.6612711616550582}\nEpoch: 1 | MeanAbsoluteError: 0.2045582979917526 | Loss: 0.0664565464187609 | Epoch: 2 | MeanAbsoluteError: 0.2171531468629837 | Loss: 0.0751113750432667 | Epoch: 3 | MeanAbsoluteError: 0.1902566552162170 | Loss: 0.0559820683035803 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1910177916288376 | Loss: 0.0562048046359498 | Epoch: 5 | MeanAbsoluteError: 0.1716200113296509 | Loss: 0.0459898313458421 | Epoch: 6 | MeanAbsoluteError: 0.1796826571226120 | Loss: 0.0512229447302065 | Epoch: 7 | MeanAbsoluteError: 0.1711835414171219 | Loss: 0.0434792472147628 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.1753378510475159 | Loss: 0.0489750233429827 | Epoch: 9 | MeanAbsoluteError: 0.1685604751110077 | Loss: 0.0464607608798695 | Epoch: 10 | MeanAbsoluteError: 0.1590208411216736 | Loss: 0.0404295806135786 | Epoch: 11 | MeanAbsoluteError: 0.1504978537559509 | Loss: 0.0358028350839097 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.1595379263162613 | Loss: 0.0407867857843245 | Epoch: 13 | MeanAbsoluteError: 0.1536683291196823 | Loss: 0.0386660173780432 | Epoch: 14 | MeanAbsoluteError: 0.1561008244752884 | Loss: 0.0378163780919031 | Epoch: 15 | MeanAbsoluteError: 0.1555273383855820 | Loss: 0.0392864714376628 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.1513137668371201 | Loss: 0.0371768501538195 | Epoch: 17 | MeanAbsoluteError: 0.1419540047645569 | Loss: 0.0307963931638944 | Epoch: 18 | MeanAbsoluteError: 0.1480831205844879 | Loss: 0.0343313964917079 | Epoch: 19 | MeanAbsoluteError: 0.1465393900871277 | Loss: 0.0341880853827062 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.1473562270402908 | Loss: 0.0321381021609628 | Epoch: 21 | MeanAbsoluteError: 0.1476230174303055 | Loss: 0.0352126380771791 | Epoch: 22 | MeanAbsoluteError: 0.1320648342370987 | Loss: 0.0281723092898334 | Epoch: 23 | MeanAbsoluteError: 0.1353592425584793 | Loss: 0.0292559434953881 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.1300944834947586 | Loss: 0.0280407096479872 | Epoch: 25 | MeanAbsoluteError: 0.1317217946052551 | Loss: 0.0286467326314826 | Epoch: 26 | MeanAbsoluteError: 0.1259917318820953 | Loss: 0.0256623011695123 | Epoch: 27 | MeanAbsoluteError: 0.1372796446084976 | Loss: 0.0300934367543577 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.1406313776969910 | Loss: 0.0303486459153263 | Epoch: 29 | MeanAbsoluteError: 0.1236243918538094 | Loss: 0.0265465958999764 | Epoch: 30 | MeanAbsoluteError: 0.1307795941829681 | Loss: 0.0265651941789608 | Epoch: 31 | MeanAbsoluteError: 0.1281430125236511 | Loss: 0.0260540697306983 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.1270732581615448 | Loss: 0.0253277545378200 | Epoch: 33 | MeanAbsoluteError: 0.1305359154939651 | Loss: 0.0269775797220829 | Epoch: 34 | MeanAbsoluteError: 0.1263422518968582 | Loss: 0.0269512461841499 | Epoch: 35 | MeanAbsoluteError: 0.1268246024847031 | Loss: 0.0256807595540426 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.1209264621138573 | Loss: 0.0228760515609266 | Epoch: 37 | MeanAbsoluteError: 0.1128260642290115 | Loss: 0.0198833252989540 | Epoch: 38 | MeanAbsoluteError: 0.1216493472456932 | Loss: 0.0235985981692609 | Epoch: 39 | MeanAbsoluteError: 0.1263959556818008 | Loss: 0.0249965360731279 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.1141894161701202 | Loss: 0.0209585432771985 | Epoch: 41 | MeanAbsoluteError: 0.1172092407941818 | Loss: 0.0218800161475022 | Epoch: 42 | MeanAbsoluteError: 0.1124115884304047 | Loss: 0.0207972639534426 | Epoch: 43 | MeanAbsoluteError: 0.1182216480374336 | Loss: 0.0233161252874293 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.1178560405969620 | Loss: 0.0212417750276233 | Epoch: 45 | MeanAbsoluteError: 0.1149980276823044 | Loss: 0.0203813470434397 | Epoch: 46 | MeanAbsoluteError: 0.1118010729551315 | Loss: 0.0208571324198458 | Epoch: 47 | MeanAbsoluteError: 0.1101400405168533 | Loss: 0.0188273972966463 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.1098902374505997 | Loss: 0.0199643646636488 | Epoch: 49 | MeanAbsoluteError: 0.1115574762225151 | Loss: 0.0211675311806367 | Epoch: 50 | MeanAbsoluteError: 0.1102382019162178 | Loss: 0.0193559989811068 | Epoch: 51 | MeanAbsoluteError: 0.1122766733169556 | Loss: 0.0196819863355670 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.1039528325200081 | Loss: 0.0177447431794319 | Epoch: 53 | MeanAbsoluteError: 0.1173311099410057 | Loss: 0.0224969784582132 | Epoch: 54 | MeanAbsoluteError: 0.1045069471001625 | Loss: 0.0184317956568281 | Epoch: 55 | MeanAbsoluteError: 0.1062532514333725 | Loss: 0.0177454736671950 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.1023848801851273 | Loss: 0.0174858941892652 | Epoch: 57 | MeanAbsoluteError: 0.1003846302628517 | Loss: 0.0160905395781523 | Epoch: 58 | MeanAbsoluteError: 0.1054382771253586 | Loss: 0.0179287127317174 | Epoch: 59 | MeanAbsoluteError: 0.1092285513877869 | Loss: 0.0183372822541155 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.1069116741418839 | Loss: 0.0173995994448074 | Epoch: 61 | MeanAbsoluteError: 0.1035945490002632 | Loss: 0.0163810187828188 | Epoch: 62 | MeanAbsoluteError: 0.0968707576394081 | Loss: 0.0146630028887701 | Epoch: 63 | MeanAbsoluteError: 0.0978786721825600 | Loss: 0.0163969460654220 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0986272022128105 | Loss: 0.0167760063624500 | Epoch: 65 | MeanAbsoluteError: 0.1000781133770943 | Loss: 0.0159236612901288 | Epoch: 66 | MeanAbsoluteError: 0.0936114788055420 | Loss: 0.0137328956717331 | Epoch: 67 | MeanAbsoluteError: 0.0979457050561905 | Loss: 0.0158065555151552 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0913032442331314 | Loss: 0.0140504537542400 | Epoch: 69 | MeanAbsoluteError: 0.0996517315506935 | Loss: 0.0158510952942858 | Epoch: 70 | MeanAbsoluteError: 0.0929025486111641 | Loss: 0.0141260376518690 | Epoch: 71 | MeanAbsoluteError: 0.0939294695854187 | Loss: 0.0144792887210650 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0944542437791824 | Loss: 0.0150603035550663 | Epoch: 73 | MeanAbsoluteError: 0.0973372682929039 | Loss: 0.0149351826219476 | Epoch: 74 | MeanAbsoluteError: 0.0914822816848755 | Loss: 0.0131802312220986 | Epoch: 75 | MeanAbsoluteError: 0.0858394280076027 | Loss: 0.0117626171589731 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0853340625762939 | Loss: 0.0124636394412894 | Epoch: 77 | MeanAbsoluteError: 0.0921071693301201 | Loss: 0.0137760115566810 | Epoch: 78 | MeanAbsoluteError: 0.0874209553003311 | Loss: 0.0134109858251912 | Epoch: 79 | MeanAbsoluteError: 0.0890191048383713 | Loss: 0.0132469563365081 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0882180333137512 | Loss: 0.0127548224746103 | Epoch: 81 | MeanAbsoluteError: 0.0833838954567909 | Loss: 0.0114171365275979 | Epoch: 82 | MeanAbsoluteError: 0.0874118357896805 | Loss: 0.0132195222234402 | Epoch: 83 | MeanAbsoluteError: 0.0915939211845398 | Loss: 0.0134646123961398 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0917182862758636 | Loss: 0.0138449092659070 | Epoch: 85 | MeanAbsoluteError: 0.0859638229012489 | Loss: 0.0122196747229042 | Epoch: 86 | MeanAbsoluteError: 0.0817820429801941 | Loss: 0.0108543711310056 | Epoch: 87 | MeanAbsoluteError: 0.0819408372044563 | Loss: 0.0117488127495897 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0864305123686790 | Loss: 0.0123263411285160 | Epoch: 89 | MeanAbsoluteError: 0.0836956948041916 | Loss: 0.0128242443744583 | Epoch: 90 | MeanAbsoluteError: 0.0871646255254745 | Loss: 0.0125961908457899 | Epoch: 91 | MeanAbsoluteError: 0.0871865525841713 | Loss: 0.0126996123121659 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0825461000204086 | Loss: 0.0119049466631718 | Epoch: 93 | MeanAbsoluteError: 0.0846775919198990 | Loss: 0.0119895527543696 | Epoch: 94 | MeanAbsoluteError: 0.0818405821919441 | Loss: 0.0112402216641625 | Epoch: 95 | MeanAbsoluteError: 0.0781301334500313 | Loss: 0.0103700080224754 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0813701599836349 | Loss: 0.0110942369302441 | Epoch: 97 | MeanAbsoluteError: 0.0796420574188232 | Loss: 0.0098232554124766 | Epoch: 98 | MeanAbsoluteError: 0.0799898505210876 | Loss: 0.0104277771866978 | Epoch: 99 | MeanAbsoluteError: 0.0783088952302933 | Loss: 0.0095933907256327 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0801916867494583 | Loss: 0.0106696032330786 | Epoch: 101 | MeanAbsoluteError: 0.0788465812802315 | Loss: 0.0098391233998547 | Epoch: 102 | MeanAbsoluteError: 0.0788943767547607 | Loss: 0.0103647874263850 | Epoch: 103 | MeanAbsoluteError: 0.0762783214449883 | Loss: 0.0102354946583019 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0757830590009689 | Loss: 0.0099454872359179 | Epoch: 105 | MeanAbsoluteError: 0.0769040510058403 | Loss: 0.0098378891860576 | Epoch: 106 | MeanAbsoluteError: 0.0773427188396454 | Loss: 0.0099096841938598 | Epoch: 107 | MeanAbsoluteError: 0.0775219723582268 | Loss: 0.0101655593270583 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0778258442878723 | Loss: 0.0106045169002180 | Epoch: 109 | MeanAbsoluteError: 0.0725799053907394 | Loss: 0.0095307056632775 | Epoch: 110 | MeanAbsoluteError: 0.0798923075199127 | Loss: 0.0105437937891111 | Epoch: 111 | MeanAbsoluteError: 0.0790480151772499 | Loss: 0.0105331532540731 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0732189044356346 | Loss: 0.0090473812551385 | Epoch: 113 | MeanAbsoluteError: 0.0749660730361938 | Loss: 0.0091842609384146 | Epoch: 114 | MeanAbsoluteError: 0.0694784522056580 | Loss: 0.0084311788224640 | Epoch: 115 | MeanAbsoluteError: 0.0742352604866028 | Loss: 0.0094033768829448 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0734540075063705 | Loss: 0.0094991333133198 | Epoch: 117 | MeanAbsoluteError: 0.0763663053512573 | Loss: 0.0100979120225499 | Epoch: 118 | MeanAbsoluteError: 0.0708634033799171 | Loss: 0.0089271486565275 | Epoch: 119 | MeanAbsoluteError: 0.0679935887455940 | Loss: 0.0081492581092262 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0694913640618324 | Loss: 0.0082280009825665 | Epoch: 121 | MeanAbsoluteError: 0.0704921036958694 | Loss: 0.0084463975661875 | Epoch: 122 | MeanAbsoluteError: 0.0717147439718246 | Loss: 0.0100720710439706 | Epoch: 123 | MeanAbsoluteError: 0.0692954659461975 | Loss: 0.0089033561587138 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0718271508812904 | Loss: 0.0085537336095800 | Epoch: 125 | MeanAbsoluteError: 0.0704684704542160 | Loss: 0.0085429696114979 | Epoch: 126 | MeanAbsoluteError: 0.0693241879343987 | Loss: 0.0082193480461444 | Epoch: 127 | MeanAbsoluteError: 0.0720877349376678 | Loss: 0.0093890272393382 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0683174282312393 | Loss: 0.0085435692760113 | Epoch: 129 | MeanAbsoluteError: 0.0722929090261459 | Loss: 0.0094837174370983 | Epoch: 130 | MeanAbsoluteError: 0.0720413327217102 | Loss: 0.0087242809210071 | Epoch: 131 | MeanAbsoluteError: 0.0680722072720528 | Loss: 0.0085204892859206 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.0710822790861130 | Loss: 0.0081166674683214 | Epoch: 133 | MeanAbsoluteError: 0.0692225992679596 | Loss: 0.0081180772486780 | Epoch: 134 | MeanAbsoluteError: 0.0677574574947357 | Loss: 0.0091031353743012 | Epoch: 135 | MeanAbsoluteError: 0.0706790313124657 | Loss: 0.0085258830926920 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.0641743913292885 | Loss: 0.0071823637240174 | Epoch: 137 | MeanAbsoluteError: 0.0691457614302635 | Loss: 0.0084141201621510 | Epoch: 138 | MeanAbsoluteError: 0.0683356598019600 | Loss: 0.0082393928406466 | Epoch: 139 | MeanAbsoluteError: 0.0633533075451851 | Loss: 0.0070562994806096 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.0660708844661713 | Loss: 0.0082464851561542 | Epoch: 141 | MeanAbsoluteError: 0.0660287514328957 | Loss: 0.0079252850754481 | Epoch: 142 | MeanAbsoluteError: 0.0615896545350552 | Loss: 0.0069893508273373 | Epoch: 143 | MeanAbsoluteError: 0.0656875222921371 | Loss: 0.0069864085097307 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.0685102567076683 | Loss: 0.0080035296421939 | Epoch: 145 | MeanAbsoluteError: 0.0642851665616035 | Loss: 0.0074193401439851 | Epoch: 146 | MeanAbsoluteError: 0.0691228508949280 | Loss: 0.0085727646518008 | Epoch: 147 | MeanAbsoluteError: 0.0676110163331032 | Loss: 0.0077628214034791 | Epoch: 148 | \n\n\nMeanAbsoluteError: 0.0660957396030426 | Loss: 0.0073654426071842 | Epoch: 149 | MeanAbsoluteError: 0.0627973899245262 | Loss: 0.0068502997470971 | Epoch: 150 | MeanAbsoluteError: 0.0714625641703606 | Loss: 0.0085073647619299 | Epoch: 151 | MeanAbsoluteError: 0.0655696690082550 | Loss: 0.0085433452883368 | Epoch: 152 | \n\n\nMeanAbsoluteError: 0.0646360814571381 | Loss: 0.0070287899059047 | Epoch: 153 | MeanAbsoluteError: 0.0667133033275604 | Loss: 0.0074427158412475 | Epoch: 154 | MeanAbsoluteError: 0.0628070160746574 | Loss: 0.0076512539281363 | Epoch: 155 | MeanAbsoluteError: 0.0635056197643280 | Loss: 0.0070587545970308 | Epoch: 156 | \n\n\nMeanAbsoluteError: 0.0651356354355812 | Loss: 0.0078756200505028 | Epoch: 157 | MeanAbsoluteError: 0.0660578161478043 | Loss: 0.0071462861225499 | Epoch: 158 | MeanAbsoluteError: 0.0642611533403397 | Loss: 0.0070337423607462 | Epoch: 159 | MeanAbsoluteError: 0.0646979585289955 | Loss: 0.0070060398889166 | Epoch: 160 | \n\n\nMeanAbsoluteError: 0.0653013885021210 | Loss: 0.0081268845150541 | Epoch: 161 | MeanAbsoluteError: 0.0609768666327000 | Loss: 0.0068528527412318 | Epoch: 162 | MeanAbsoluteError: 0.0614423304796219 | Loss: 0.0066815378622299 | Epoch: 163 | MeanAbsoluteError: 0.0605393424630165 | Loss: 0.0064666358153581 | Epoch: 164 | \n\n\nMeanAbsoluteError: 0.0630810558795929 | Loss: 0.0071575589980440 | Epoch: 165 | MeanAbsoluteError: 0.0644065588712692 | Loss: 0.0077560602037824 | Epoch: 166 | MeanAbsoluteError: 0.0623339079320431 | Loss: 0.0067092306041894 | Epoch: 167 | MeanAbsoluteError: 0.0650273188948631 | Loss: 0.0071217980186798 | Epoch: 168 | \n\n\nMeanAbsoluteError: 0.0599445216357708 | Loss: 0.0061546276280607 | Epoch: 169 | MeanAbsoluteError: 0.0613158531486988 | Loss: 0.0065580537101548 | Epoch: 170 | MeanAbsoluteError: 0.0597055815160275 | Loss: 0.0059483514602394 | Epoch: 171 | MeanAbsoluteError: 0.0574085414409637 | Loss: 0.0065052565616114 | Epoch: 172 | \n\n\nMeanAbsoluteError: 0.0617124997079372 | Loss: 0.0067949389738619 | Epoch: 173 | MeanAbsoluteError: 0.0608066171407700 | Loss: 0.0061361210261423 | Epoch: 174 | MeanAbsoluteError: 0.0600310899317265 | Loss: 0.0063320093077460 | Epoch: 175 | MeanAbsoluteError: 0.0639642924070358 | Loss: 0.0070078416304712 | Epoch: 176 | \n\n\nMeanAbsoluteError: 0.0633501335978508 | Loss: 0.0070241107636033 | Epoch: 177 | MeanAbsoluteError: 0.0608059428632259 | Loss: 0.0066518901363260 | Epoch: 178 | MeanAbsoluteError: 0.0622788667678833 | Loss: 0.0068146677130196 | Epoch: 179 | MeanAbsoluteError: 0.0612077713012695 | Loss: 0.0077442545627587 | Epoch: 180 | \n\n\nMeanAbsoluteError: 0.0621337257325649 | Loss: 0.0071094247426740 | Epoch: 181 | MeanAbsoluteError: 0.0590338595211506 | Loss: 0.0065332399665893 | Epoch: 182 | MeanAbsoluteError: 0.0646195784211159 | Loss: 0.0071703218180980 | Epoch: 183 | MeanAbsoluteError: 0.0631960257887840 | Loss: 0.0074875548184776 | Epoch: 184 | \n\n\nMeanAbsoluteError: 0.0610930584371090 | Loss: 0.0069260394574437 | Epoch: 185 | MeanAbsoluteError: 0.0599422007799149 | Loss: 0.0072692063281705 | Epoch: 186 | MeanAbsoluteError: 0.0576072894036770 | Loss: 0.0067207298446814 | Epoch: 187 | MeanAbsoluteError: 0.0623949877917767 | Loss: 0.0072001506785838 | Epoch: 188 | \n\n\nMeanAbsoluteError: 0.0617798678576946 | Loss: 0.0061570051337568 | Epoch: 189 | MeanAbsoluteError: 0.0619607344269753 | Loss: 0.0066297907461902 | Epoch: 190 | MeanAbsoluteError: 0.0564539097249508 | Loss: 0.0058271232134614 | Epoch: 191 | MeanAbsoluteError: 0.0616723299026489 | Loss: 0.0075304710021316 | Epoch: 192 | \n\n\nMeanAbsoluteError: 0.0589235313236713 | Loss: 0.0064918373031297 | Epoch: 193 | MeanAbsoluteError: 0.0622260347008705 | Loss: 0.0071949902094754 | Epoch: 194 | MeanAbsoluteError: 0.0590490326285362 | Loss: 0.0061384921734172 | Epoch: 195 | MeanAbsoluteError: 0.0625202804803848 | Loss: 0.0077373363463649 | Epoch: 196 | \n\n\nMeanAbsoluteError: 0.0576339289546013 | Loss: 0.0061987500306905 | Epoch: 197 | MeanAbsoluteError: 0.0571131147444248 | Loss: 0.0062265040408085 | Epoch: 198 | MeanAbsoluteError: 0.0599471963942051 | Loss: 0.0069374309509600 | Epoch: 199 | MeanAbsoluteError: 0.0603327676653862 | Loss: 0.0066292068314444 | Epoch: 200 | \n\n\nMeanAbsoluteError: 0.0579970218241215 | Loss: 0.0061044869607462 | Epoch: 201 | MeanAbsoluteError: 0.0599905475974083 | Loss: 0.0067050733456486 | Epoch: 202 | MeanAbsoluteError: 0.0614159852266312 | Loss: 0.0072042858228087 | Epoch: 203 | MeanAbsoluteError: 0.0578341595828533 | Loss: 0.0065829421785709 | Epoch: 204 | \n\n\nMeanAbsoluteError: 0.0577018037438393 | Loss: 0.0058304888110510 | Epoch: 205 | MeanAbsoluteError: 0.0563860014081001 | Loss: 0.0060047637609961 | Epoch: 206 | MeanAbsoluteError: 0.0605419799685478 | Loss: 0.0064006775746269 | Epoch: 207 | MeanAbsoluteError: 0.0658068954944611 | Loss: 0.0077923036021131 | Epoch: 208 | \n\n\nMeanAbsoluteError: 0.0580926835536957 | Loss: 0.0059708368253747 | Epoch: 209 | MeanAbsoluteError: 0.0581137388944626 | Loss: 0.0066072738631383 | Epoch: 210 | MeanAbsoluteError: 0.0562309138476849 | Loss: 0.0061350039307543 | Epoch: 211 | MeanAbsoluteError: 0.0559477992355824 | Loss: 0.0064367847455862 | Epoch: 212 | \n\n\nMeanAbsoluteError: 0.0563021078705788 | Loss: 0.0060838513676763 | Epoch: 213 | MeanAbsoluteError: 0.0592040680348873 | Loss: 0.0065276330539123 | Epoch: 214 | MeanAbsoluteError: 0.0570081248879433 | Loss: 0.0066300057974570 | Epoch: 215 | MeanAbsoluteError: 0.0599465370178223 | Loss: 0.0060183272274260 | Epoch: 216 | \n\n\nMeanAbsoluteError: 0.0576203204691410 | Loss: 0.0058511221983568 | Epoch: 217 | MeanAbsoluteError: 0.0591285824775696 | Loss: 0.0063477795001266 | Epoch: 218 | MeanAbsoluteError: 0.0598197877407074 | Loss: 0.0070409843541902 | Epoch: 219 | MeanAbsoluteError: 0.0581089034676552 | Loss: 0.0062176288552827 | Epoch: 220 | \n\n\nMeanAbsoluteError: 0.0573862344026566 | Loss: 0.0061964313423040 | Epoch: 221 | MeanAbsoluteError: 0.0545315220952034 | Loss: 0.0055756553713428 | Epoch: 222 | MeanAbsoluteError: 0.0535555630922318 | Loss: 0.0056067164589063 | Epoch: 223 | MeanAbsoluteError: 0.0574216917157173 | Loss: 0.0065291311107821 | Epoch: 224 | \n\n\nMeanAbsoluteError: 0.0522734969854355 | Loss: 0.0047528204250787 | Epoch: 225 | MeanAbsoluteError: 0.0613566003739834 | Loss: 0.0063401581168077 | Epoch: 226 | MeanAbsoluteError: 0.0581550672650337 | Loss: 0.0063043694152791 | Epoch: 227 | MeanAbsoluteError: 0.0602180920541286 | Loss: 0.0064416037915324 | Epoch: 228 | \n\n\nMeanAbsoluteError: 0.0627460703253746 | Loss: 0.0073557150347992 | Epoch: 229 | MeanAbsoluteError: 0.0606556124985218 | Loss: 0.0062335778738519 | Epoch: 230 | MeanAbsoluteError: 0.0520058237016201 | Loss: 0.0053953740111626 | Epoch: 231 | MeanAbsoluteError: 0.0612286888062954 | Loss: 0.0075627676591179 | Epoch: 232 | \n\n\nMeanAbsoluteError: 0.0569234415888786 | Loss: 0.0060552851738114 | Epoch: 233 | MeanAbsoluteError: 0.0549749620258808 | Loss: 0.0053737692865788 | Epoch: 234 | MeanAbsoluteError: 0.0547900311648846 | Loss: 0.0053033482552947 | Epoch: 235 | MeanAbsoluteError: 0.0544002354145050 | Loss: 0.0052286531151223 | Epoch: 236 | \n\n\nMeanAbsoluteError: 0.0562681891024113 | Loss: 0.0056024893174732 | Epoch: 237 | MeanAbsoluteError: 0.0612680315971375 | Loss: 0.0065844518708457 | Epoch: 238 | MeanAbsoluteError: 0.0571114346385002 | Loss: 0.0062357514673893 | Epoch: 239 | MeanAbsoluteError: 0.0575451515614986 | Loss: 0.0058805281081923 | Epoch: 240 | \n\n\nMeanAbsoluteError: 0.0559863969683647 | Loss: 0.0058107872119811 | Epoch: 241 | MeanAbsoluteError: 0.0597157366573811 | Loss: 0.0062020936380386 | Epoch: 242 | MeanAbsoluteError: 0.0554095655679703 | Loss: 0.0060990020559218 | Epoch: 243 | MeanAbsoluteError: 0.0508533716201782 | Loss: 0.0047646714634826 | Epoch: 244 | \n\n\nMeanAbsoluteError: 0.0568192787468433 | Loss: 0.0060923489938049 | Epoch: 245 | MeanAbsoluteError: 0.0577268972992897 | Loss: 0.0064545189563528 | Epoch: 246 | MeanAbsoluteError: 0.0547268874943256 | Loss: 0.0057728575714129 | Epoch: 247 | MeanAbsoluteError: 0.0572540163993835 | Loss: 0.0056374658628269 | Epoch: 248 | \n\n\nMeanAbsoluteError: 0.0521886795759201 | Loss: 0.0053364766614610 | Epoch: 249 | MeanAbsoluteError: 0.0560475923120975 | Loss: 0.0065822289080212 | Epoch: 250 | MeanAbsoluteError: 0.0573076941072941 | Loss: 0.0062042174548352 | Epoch: 251 | MeanAbsoluteError: 0.0563167780637741 | Loss: 0.0069065159431806 | Epoch: 252 | \n\n\nMeanAbsoluteError: 0.0577764399349689 | Loss: 0.0060992410749589 | Epoch: 253 | MeanAbsoluteError: 0.0545193180441856 | Loss: 0.0065235805676294 | Epoch: 254 | MeanAbsoluteError: 0.0587926246225834 | Loss: 0.0068132577243408 | Epoch: 255 | MeanAbsoluteError: 0.0542503744363785 | Loss: 0.0055862281273891 | Epoch: 256 | \n\n\nMeanAbsoluteError: 0.0572753697633743 | Loss: 0.0064863086690342 | Epoch: 257 | MeanAbsoluteError: 0.0533645711839199 | Loss: 0.0056026324631315 | Epoch: 258 | MeanAbsoluteError: 0.0523100756108761 | Loss: 0.0050446732325087 | Epoch: 259 | MeanAbsoluteError: 0.0577649176120758 | Loss: 0.0061774578460149 | Epoch: 260 | \n\n\nMeanAbsoluteError: 0.0547831989824772 | Loss: 0.0054930773300217 | Epoch: 261 | MeanAbsoluteError: 0.0550632588565350 | Loss: 0.0052494418893107 | Epoch: 262 | MeanAbsoluteError: 0.0622536353766918 | Loss: 0.0069729796500484 | Epoch: 263 | MeanAbsoluteError: 0.0557974353432655 | Loss: 0.0054601462953111 | Epoch: 264 | \n\n\nMeanAbsoluteError: 0.0535519868135452 | Loss: 0.0055785962061858 | Epoch: 265 | MeanAbsoluteError: 0.0518876761198044 | Loss: 0.0051451524042549 | Epoch: 266 | MeanAbsoluteError: 0.0563549734652042 | Loss: 0.0055939802075859 | Epoch: 267 | MeanAbsoluteError: 0.0552962608635426 | Loss: 0.0058623214794543 | Epoch: 268 | \n\n\nMeanAbsoluteError: 0.0549556538462639 | Loss: 0.0053635812456425 | Epoch: 269 | MeanAbsoluteError: 0.0546085797250271 | Loss: 0.0054289510099855 | Epoch: 270 | MeanAbsoluteError: 0.0529124028980732 | Loss: 0.0048587385957178 | Epoch: 271 | MeanAbsoluteError: 0.0576753430068493 | Loss: 0.0065279289265163 | Epoch: 272 | \n\n\nMeanAbsoluteError: 0.0547669529914856 | Loss: 0.0053883662116469 | Epoch: 273 | MeanAbsoluteError: 0.0537675470113754 | Loss: 0.0055226873588062 | Epoch: 274 | MeanAbsoluteError: 0.0525390356779099 | Loss: 0.0050190083841842 | Epoch: 275 | MeanAbsoluteError: 0.0591459795832634 | Loss: 0.0063191837796598 | Epoch: 276 | \n\n\nMeanAbsoluteError: 0.0563288368284702 | Loss: 0.0061593920507125 | Epoch: 277 | MeanAbsoluteError: 0.0524735115468502 | Loss: 0.0053021509374958 | Epoch: 278 | MeanAbsoluteError: 0.0513983443379402 | Loss: 0.0047332289426489 | Epoch: 279 | MeanAbsoluteError: 0.0547447316348553 | Loss: 0.0052317635609621 | Epoch: 280 | \n\n\nMeanAbsoluteError: 0.0541355013847351 | Loss: 0.0058167553996961 | Epoch: 281 | MeanAbsoluteError: 0.0492964498698711 | Loss: 0.0045730204315317 | Epoch: 282 | MeanAbsoluteError: 0.0562447309494019 | Loss: 0.0055712333230890 | Epoch: 283 | MeanAbsoluteError: 0.0544481649994850 | Loss: 0.0053533476709976 | Epoch: 284 | \n\n\nMeanAbsoluteError: 0.0518017709255219 | Loss: 0.0051799384726971 | Epoch: 285 | MeanAbsoluteError: 0.0615201368927956 | Loss: 0.0067805805899154 | Epoch: 286 | MeanAbsoluteError: 0.0551925860345364 | Loss: 0.0057699397270641 | Epoch: 287 | MeanAbsoluteError: 0.0527108572423458 | Loss: 0.0050078445178530 | Epoch: 288 | \n\n\nMeanAbsoluteError: 0.0570776611566544 | Loss: 0.0066706597204565 | Epoch: 289 | MeanAbsoluteError: 0.0501575842499733 | Loss: 0.0050747628219573 | Epoch: 290 | MeanAbsoluteError: 0.0539694912731647 | Loss: 0.0056220701632188 | Epoch: 291 | MeanAbsoluteError: 0.0590302385389805 | Loss: 0.0064502601560793 | Epoch: 292 | \n\n\nMeanAbsoluteError: 0.0528624914586544 | Loss: 0.0056434546560539 | Epoch: 293 | MeanAbsoluteError: 0.0588996745646000 | Loss: 0.0065863973271151 | Epoch: 294 | MeanAbsoluteError: 0.0558897331357002 | Loss: 0.0055175624530468 | Epoch: 295 | MeanAbsoluteError: 0.0562937930226326 | Loss: 0.0061784320308729 | Epoch: 296 | \n\n\nMeanAbsoluteError: 0.0522351600229740 | Loss: 0.0052405085665860 | Epoch: 297 | MeanAbsoluteError: 0.0528365448117256 | Loss: 0.0048317715531754 | Epoch: 298 | MeanAbsoluteError: 0.0573235377669334 | Loss: 0.0060417504098855 | Epoch: 299 | MeanAbsoluteError: 0.0587856210768223 | Loss: 0.0071754715815922 | Epoch: 300 | \n\n\nMeanAbsoluteError: 0.0527897663414478 | Loss: 0.0056169129491441 | Epoch: 301 | MeanAbsoluteError: 0.0491397567093372 | Loss: 0.0047406215070575 | Epoch: 302 | MeanAbsoluteError: 0.0499684736132622 | Loss: 0.0047815263571561 | Epoch: 303 | MeanAbsoluteError: 0.0503799393773079 | Loss: 0.0050195222444783 | Epoch: 304 | \n\n\nMeanAbsoluteError: 0.0519566684961319 | Loss: 0.0053529620866022 | Epoch: 305 | MeanAbsoluteError: 0.0525524951517582 | Loss: 0.0047718349577687 | Epoch: 306 | MeanAbsoluteError: 0.0530807189643383 | Loss: 0.0053890769823307 | Epoch: 307 | MeanAbsoluteError: 0.0558630563318729 | Loss: 0.0063481902838440 | Epoch: 308 | \n\n\nMeanAbsoluteError: 0.0506465509533882 | Loss: 0.0047495148135472 | Epoch: 309 | MeanAbsoluteError: 0.0546940751373768 | Loss: 0.0064417010657244 | Epoch: 310 | MeanAbsoluteError: 0.0494024343788624 | Loss: 0.0046881875421509 | Epoch: 311 | MeanAbsoluteError: 0.0590305067598820 | Loss: 0.0063808917582623 | Epoch: 312 | \n\n\nMeanAbsoluteError: 0.0526258610188961 | Loss: 0.0050752241881319 | Epoch: 313 | MeanAbsoluteError: 0.0567499846220016 | Loss: 0.0059040920092038 | Epoch: 314 | MeanAbsoluteError: 0.0541877821087837 | Loss: 0.0058170706845550 | Epoch: 315 | MeanAbsoluteError: 0.0544961281120777 | Loss: 0.0052060468036583 | Epoch: 316 | \n\n\nMeanAbsoluteError: 0.0499292612075806 | Loss: 0.0043354135643887 | Epoch: 317 | MeanAbsoluteError: 0.0476932190358639 | Loss: 0.0041562702298458 | Epoch: 318 | MeanAbsoluteError: 0.0518461391329765 | Loss: 0.0051208348871258 | Epoch: 319 | MeanAbsoluteError: 0.0509585514664650 | Loss: 0.0047022724174894 | Epoch: 320 | \n\n\nMeanAbsoluteError: 0.0537071973085403 | Loss: 0.0050478463802536 | Epoch: 321 | MeanAbsoluteError: 0.0512152686715126 | Loss: 0.0053651251724468 | Epoch: 322 | MeanAbsoluteError: 0.0547722578048706 | Loss: 0.0059596157409128 | Epoch: 323 | MeanAbsoluteError: 0.0539283379912376 | Loss: 0.0055716073838994 | Epoch: 324 | \n\n\nMeanAbsoluteError: 0.0527550429105759 | Loss: 0.0050924825880333 | Epoch: 325 | MeanAbsoluteError: 0.0526205077767372 | Loss: 0.0051451644532407 | Epoch: 326 | MeanAbsoluteError: 0.0534522384405136 | Loss: 0.0056310885640989 | Epoch: 327 | MeanAbsoluteError: 0.0548657663166523 | Loss: 0.0056174567588115 | Epoch: 328 | \n\n\nMeanAbsoluteError: 0.0536378473043442 | Loss: 0.0050605442941703 | Epoch: 329 | MeanAbsoluteError: 0.0571259483695030 | Loss: 0.0066923788123715 | Epoch: 330 | MeanAbsoluteError: 0.0501839704811573 | Loss: 0.0045425233425033 | Epoch: 331 | MeanAbsoluteError: 0.0501563735306263 | Loss: 0.0048730432187624 | Epoch: 332 | \n\n\nMeanAbsoluteError: 0.0568613037467003 | Loss: 0.0060709275207237 | Epoch: 333 | MeanAbsoluteError: 0.0547423809766769 | Loss: 0.0055807454612621 | Epoch: 334 | MeanAbsoluteError: 0.0540656410157681 | Loss: 0.0057398862861048 | Epoch: 335 | MeanAbsoluteError: 0.0521371290087700 | Loss: 0.0050412605633028 | Epoch: 336 | \n\n\nMeanAbsoluteError: 0.0518840476870537 | Loss: 0.0052269413779294 | Epoch: 337 | MeanAbsoluteError: 0.0538337901234627 | Loss: 0.0052655906673815 | Epoch: 338 | MeanAbsoluteError: 0.0534175373613834 | Loss: 0.0053393930483205 | Epoch: 339 | MeanAbsoluteError: 0.0573552586138248 | Loss: 0.0062775157520695 | Epoch: 340 | \n\n\nMeanAbsoluteError: 0.0556331388652325 | Loss: 0.0054621933622433 | Epoch: 341 | MeanAbsoluteError: 0.0562108680605888 | Loss: 0.0060976350827045 | Epoch: 342 | MeanAbsoluteError: 0.0568107739090919 | Loss: 0.0059261476241150 | Epoch: 343 | MeanAbsoluteError: 0.0531774908304214 | Loss: 0.0052056677078853 | Epoch: 344 | \n\n\nMeanAbsoluteError: 0.0528540052473545 | Loss: 0.0050444026097753 | Epoch: 345 | MeanAbsoluteError: 0.0536469332873821 | Loss: 0.0055552375024969 | Epoch: 346 | MeanAbsoluteError: 0.0535390339791775 | Loss: 0.0061010998110042 | Epoch: 347 | MeanAbsoluteError: 0.0522125959396362 | Loss: 0.0049145947582713 | Epoch: 348 | \n\n\nMeanAbsoluteError: 0.0518797188997269 | Loss: 0.0055376938054044 | Epoch: 349 | MeanAbsoluteError: 0.0499233342707157 | Loss: 0.0047811234130916 | Epoch: 350 | MeanAbsoluteError: 0.0522428937256336 | Loss: 0.0053516427149972 | Epoch: 351 | MeanAbsoluteError: 0.0514635331928730 | Loss: 0.0051675244727736 | Epoch: 352 | \n\n\nMeanAbsoluteError: 0.0552113391458988 | Loss: 0.0056482903248827 | Epoch: 353 | MeanAbsoluteError: 0.0523838885128498 | Loss: 0.0051678822890512 | Epoch: 354 | MeanAbsoluteError: 0.0511284098029137 | Loss: 0.0056246231757349 | Epoch: 355 | MeanAbsoluteError: 0.0534237101674080 | Loss: 0.0053336631129863 | Epoch: 356 | \n\n\nMeanAbsoluteError: 0.0508409067988396 | Loss: 0.0047501328824039 | Epoch: 357 | MeanAbsoluteError: 0.0489588007330894 | Loss: 0.0053053130947151 | Epoch: 358 | MeanAbsoluteError: 0.0491736531257629 | Loss: 0.0044814416446331 | Epoch: 359 | MeanAbsoluteError: 0.0576365143060684 | Loss: 0.0060041639279868 | Epoch: 360 | \n\n\nMeanAbsoluteError: 0.0503662452101707 | Loss: 0.0048760721294552 | Epoch: 361 | MeanAbsoluteError: 0.0512927584350109 | Loss: 0.0050762227681269 | Epoch: 362 | MeanAbsoluteError: 0.0522095076739788 | Loss: 0.0054179642619075 | Epoch: 363 | MeanAbsoluteError: 0.0509493835270405 | Loss: 0.0051159076002047 | Epoch: 364 | \n\n\nMeanAbsoluteError: 0.0502438060939312 | Loss: 0.0049997625602899 | Epoch: 365 | MeanAbsoluteError: 0.0542702674865723 | Loss: 0.0057927029158332 | Epoch: 366 | MeanAbsoluteError: 0.0508087649941444 | Loss: 0.0055337281909918 | Epoch: 367 | MeanAbsoluteError: 0.0543509162962437 | Loss: 0.0055460604214060 | Epoch: 368 | \n\n\nMeanAbsoluteError: 0.0552474781870842 | Loss: 0.0058107145351211 | Epoch: 369 | MeanAbsoluteError: 0.0527579970657825 | Loss: 0.0055010391454408 | Epoch: 370 | MeanAbsoluteError: 0.0543742738664150 | Loss: 0.0059649134460702 | Epoch: 371 | MeanAbsoluteError: 0.0506374314427376 | Loss: 0.0047586733432438 | Epoch: 372 | \n\n\nMeanAbsoluteError: 0.0520481728017330 | Loss: 0.0052226252186014 | Epoch: 373 | MeanAbsoluteError: 0.0509713441133499 | Loss: 0.0047187133168336 | Epoch: 374 | MeanAbsoluteError: 0.0471923351287842 | Loss: 0.0044958811256373 | Epoch: 375 | MeanAbsoluteError: 0.0529358200728893 | Loss: 0.0056234333406840 | Epoch: 376 | \n\n\nMeanAbsoluteError: 0.0523931570351124 | Loss: 0.0054367666046968 | Epoch: 377 | MeanAbsoluteError: 0.0519788637757301 | Loss: 0.0051051132797607 | Epoch: 378 | MeanAbsoluteError: 0.0490975193679333 | Loss: 0.0044626431833757 | Epoch: 379 | MeanAbsoluteError: 0.0541114285588264 | Loss: 0.0053841579509409 | Epoch: 380 | \n\n\nMeanAbsoluteError: 0.0466215945780277 | Loss: 0.0042652324298519 | Epoch: 381 | MeanAbsoluteError: 0.0604426264762878 | Loss: 0.0070051196640921 | Early stopping at epoch 380\nReturned to Spot: Validation loss: 0.0070051196640921074\n\n\nspotPython tuning: 0.0026928081229255583 [########--] 81.45% \n\n\n\nconfig: {'_L_in': 10, '_L_out': 1, 'l1': 64, 'dropout_prob': 0.6030489890562855, 'lr_mult': 4.651911257572381, 'batch_size': 4, 'epochs': 2048, 'k_folds': 1, 'patience': 128, 'optimizer': 'ASGD', 'sgd_momentum': 0.8635684269940229}\nEpoch: 1 | MeanAbsoluteError: 0.1410160213708878 | Loss: 0.0330551920489719 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1446376740932465 | Loss: 0.0322559025580995 | Epoch: 3 | MeanAbsoluteError: 0.1381869912147522 | Loss: 0.0303576125049343 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1394120156764984 | Loss: 0.0305460554050903 | Epoch: 5 | MeanAbsoluteError: 0.1371094435453415 | Loss: 0.0297550112816195 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.1374791413545609 | Loss: 0.0299286144793344 | Epoch: 7 | MeanAbsoluteError: 0.1374308913946152 | Loss: 0.0295500665747871 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.1328539401292801 | Loss: 0.0274108486684660 | Epoch: 9 | MeanAbsoluteError: 0.1362082958221436 | Loss: 0.0286461578356102 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.1333158612251282 | Loss: 0.0286706717032939 | Epoch: 11 | MeanAbsoluteError: 0.1404359191656113 | Loss: 0.0301872765024503 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.1367169171571732 | Loss: 0.0287884282444914 | Epoch: 13 | MeanAbsoluteError: 0.1315941512584686 | Loss: 0.0276997201548268 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.1293466240167618 | Loss: 0.0276108074855680 | Epoch: 15 | MeanAbsoluteError: 0.1348992139101028 | Loss: 0.0280702397599816 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.1334165632724762 | Loss: 0.0280637075348447 | Epoch: 17 | MeanAbsoluteError: 0.1291176676750183 | Loss: 0.0266487744978319 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.1305958628654480 | Loss: 0.0265419797226787 | Epoch: 19 | MeanAbsoluteError: 0.1319153010845184 | Loss: 0.0268487681758900 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.1343556791543961 | Loss: 0.0278694057588776 | Epoch: 21 | MeanAbsoluteError: 0.1245909854769707 | Loss: 0.0254258904031788 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.1276183426380157 | Loss: 0.0252496625855565 | Epoch: 23 | MeanAbsoluteError: 0.1265672296285629 | Loss: 0.0245431569746385 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.1222467944025993 | Loss: 0.0234255712262044 | Epoch: 25 | MeanAbsoluteError: 0.1214449554681778 | Loss: 0.0226678171133002 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.1276982128620148 | Loss: 0.0249534015978376 | Epoch: 27 | MeanAbsoluteError: 0.1223513036966324 | Loss: 0.0233493931715687 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.1221544370055199 | Loss: 0.0233093811819951 | Epoch: 29 | MeanAbsoluteError: 0.1207732260227203 | Loss: 0.0230202294482539 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.1194441094994545 | Loss: 0.0234580190976461 | Epoch: 31 | MeanAbsoluteError: 0.1189523041248322 | Loss: 0.0218484262827163 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.1179379746317863 | Loss: 0.0220450638141483 | Epoch: 33 | MeanAbsoluteError: 0.1155039593577385 | Loss: 0.0214971126150340 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.1258676052093506 | Loss: 0.0239298723544925 | Epoch: 35 | MeanAbsoluteError: 0.1117690280079842 | Loss: 0.0198814276357492 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.1074220761656761 | Loss: 0.0188382521116485 | Epoch: 37 | MeanAbsoluteError: 0.1139092221856117 | Loss: 0.0207339871147027 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.1090901046991348 | Loss: 0.0191278000206997 | Epoch: 39 | MeanAbsoluteError: 0.1106493920087814 | Loss: 0.0192253648365537 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.1076140329241753 | Loss: 0.0181964013725519 | Epoch: 41 | MeanAbsoluteError: 0.1074601709842682 | Loss: 0.0192916117298106 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.1051395237445831 | Loss: 0.0180914792992795 | Epoch: 43 | MeanAbsoluteError: 0.1084942594170570 | Loss: 0.0180816055772205 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.1036405488848686 | Loss: 0.0175723686286559 | Epoch: 45 | MeanAbsoluteError: 0.0999806895852089 | Loss: 0.0159449931117706 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.1074839159846306 | Loss: 0.0184073678861993 | Epoch: 47 | MeanAbsoluteError: 0.1017277091741562 | Loss: 0.0165815575777863 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.1054007038474083 | Loss: 0.0185688375309110 | Epoch: 49 | MeanAbsoluteError: 0.1085071787238121 | Loss: 0.0185274014808238 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.1069516092538834 | Loss: 0.0186463504936546 | Epoch: 51 | MeanAbsoluteError: 0.1007072478532791 | Loss: 0.0159975101208935 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0990009531378746 | Loss: 0.0161084246930356 | Epoch: 53 | MeanAbsoluteError: 0.0951313599944115 | Loss: 0.0154977448327312 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0955102145671844 | Loss: 0.0149541421188042 | Epoch: 55 | MeanAbsoluteError: 0.0953147411346436 | Loss: 0.0155110443787028 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.1003293618559837 | Loss: 0.0164266153114537 | Epoch: 57 | MeanAbsoluteError: 0.0921228826045990 | Loss: 0.0136421464461212 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.0981105789542198 | Loss: 0.0151147677097470 | Epoch: 59 | MeanAbsoluteError: 0.0931100025773048 | Loss: 0.0150895972837073 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0969353541731834 | Loss: 0.0147952970505382 | Epoch: 61 | MeanAbsoluteError: 0.0903662964701653 | Loss: 0.0133743331224347 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.0864632725715637 | Loss: 0.0123785943499145 | Epoch: 63 | MeanAbsoluteError: 0.0812229216098785 | Loss: 0.0110277971920247 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0900237262248993 | Loss: 0.0123591344244778 | Epoch: 65 | MeanAbsoluteError: 0.0933052822947502 | Loss: 0.0142766266820642 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.0867117121815681 | Loss: 0.0127620412983621 | Epoch: 67 | MeanAbsoluteError: 0.0838956162333488 | Loss: 0.0113692480597335 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0898124873638153 | Loss: 0.0134467101943058 | Epoch: 69 | MeanAbsoluteError: 0.0894106328487396 | Loss: 0.0133524385405083 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.0832638517022133 | Loss: 0.0121192859609922 | Epoch: 71 | MeanAbsoluteError: 0.0829946622252464 | Loss: 0.0120119461525852 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0790543779730797 | Loss: 0.0124477161827963 | Epoch: 73 | MeanAbsoluteError: 0.0842929556965828 | Loss: 0.0113627588896391 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.0824427828192711 | Loss: 0.0114932664604081 | Epoch: 75 | MeanAbsoluteError: 0.0832720771431923 | Loss: 0.0119103753970315 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0779025554656982 | Loss: 0.0099766555971776 | Epoch: 77 | MeanAbsoluteError: 0.0821850672364235 | Loss: 0.0114500344071227 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.0840114504098892 | Loss: 0.0114252025199433 | Epoch: 79 | MeanAbsoluteError: 0.0832782313227654 | Loss: 0.0114247074971596 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0867705941200256 | Loss: 0.0121141583596667 | Epoch: 81 | MeanAbsoluteError: 0.0767876207828522 | Loss: 0.0101710942123706 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0801891759037971 | Loss: 0.0108393493909777 | Epoch: 83 | MeanAbsoluteError: 0.0844199433922768 | Loss: 0.0117150233158221 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0797322988510132 | Loss: 0.0108908044523560 | Epoch: 85 | MeanAbsoluteError: 0.0805594772100449 | Loss: 0.0115168238683448 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.0831634998321533 | Loss: 0.0117945878456036 | Epoch: 87 | MeanAbsoluteError: 0.0741984322667122 | Loss: 0.0099744871510969 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0766062512993813 | Loss: 0.0103398590271051 | Epoch: 89 | MeanAbsoluteError: 0.0772451311349869 | Loss: 0.0107940186797835 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.0783471390604973 | Loss: 0.0102785604998159 | Epoch: 91 | MeanAbsoluteError: 0.0669420808553696 | Loss: 0.0080226845332072 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0784224346280098 | Loss: 0.0100423730692516 | Epoch: 93 | MeanAbsoluteError: 0.0706930905580521 | Loss: 0.0086860310526875 | Epoch: 94 | \n\n\nMeanAbsoluteError: 0.0723172649741173 | Loss: 0.0090804347052472 | Epoch: 95 | MeanAbsoluteError: 0.0711510479450226 | Loss: 0.0092901954050952 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0711720436811447 | Loss: 0.0084171387642467 | Epoch: 97 | MeanAbsoluteError: 0.0696380659937859 | Loss: 0.0086293809954077 | Epoch: 98 | \n\n\nMeanAbsoluteError: 0.0702240616083145 | Loss: 0.0086311061874342 | Epoch: 99 | MeanAbsoluteError: 0.0660340040922165 | Loss: 0.0077143581904238 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0735689327120781 | Loss: 0.0098953290687253 | Epoch: 101 | MeanAbsoluteError: 0.0668647736310959 | Loss: 0.0082956242441044 | Epoch: 102 | \n\n\nMeanAbsoluteError: 0.0693250373005867 | Loss: 0.0087153536698315 | Epoch: 103 | MeanAbsoluteError: 0.0666899606585503 | Loss: 0.0082717897521798 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0640919283032417 | Loss: 0.0072250319489588 | Epoch: 105 | MeanAbsoluteError: 0.0738608315587044 | Loss: 0.0092603344769062 | Epoch: 106 | \n\n\nMeanAbsoluteError: 0.0681944787502289 | Loss: 0.0080748784850584 | Epoch: 107 | MeanAbsoluteError: 0.0671121776103973 | Loss: 0.0085353267379105 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0709246844053268 | Loss: 0.0087993005181973 | Epoch: 109 | MeanAbsoluteError: 0.0671767070889473 | Loss: 0.0084891854432256 | Epoch: 110 | \n\n\nMeanAbsoluteError: 0.0777461975812912 | Loss: 0.0100330737543603 | Epoch: 111 | MeanAbsoluteError: 0.0691517591476440 | Loss: 0.0085423603708235 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0621987134218216 | Loss: 0.0069460979934471 | Epoch: 113 | MeanAbsoluteError: 0.0641082599759102 | Loss: 0.0076105836580973 | Epoch: 114 | \n\n\nMeanAbsoluteError: 0.0659625381231308 | Loss: 0.0080358590730854 | Epoch: 115 | MeanAbsoluteError: 0.0679249241948128 | Loss: 0.0084950262874675 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0661268681287766 | Loss: 0.0083229707389061 | Epoch: 117 | MeanAbsoluteError: 0.0769124105572701 | Loss: 0.0094567107568340 | Epoch: 118 | \n\n\nMeanAbsoluteError: 0.0685820803046227 | Loss: 0.0080329130395936 | Epoch: 119 | MeanAbsoluteError: 0.0662343576550484 | Loss: 0.0077684027810271 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0604363493621349 | Loss: 0.0071817925277477 | Epoch: 121 | MeanAbsoluteError: 0.0689634680747986 | Loss: 0.0081053920028110 | Epoch: 122 | \n\n\nMeanAbsoluteError: 0.0613805204629898 | Loss: 0.0067815271951258 | Epoch: 123 | MeanAbsoluteError: 0.0632236450910568 | Loss: 0.0066045680191989 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0683804675936699 | Loss: 0.0087462504305101 | Epoch: 125 | MeanAbsoluteError: 0.0645621046423912 | Loss: 0.0076541975594591 | Epoch: 126 | \n\n\nMeanAbsoluteError: 0.0616827383637428 | Loss: 0.0069750167324673 | Epoch: 127 | MeanAbsoluteError: 0.0621259100735188 | Loss: 0.0073729134560563 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0629602149128914 | Loss: 0.0073648360926503 | Epoch: 129 | MeanAbsoluteError: 0.0597081445157528 | Loss: 0.0068316200920769 | Epoch: 130 | \n\n\nMeanAbsoluteError: 0.0625531822443008 | Loss: 0.0072450110636419 | Epoch: 131 | MeanAbsoluteError: 0.0641449764370918 | Loss: 0.0076110737747513 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.0626915171742439 | Loss: 0.0072346513858065 | Epoch: 133 | MeanAbsoluteError: 0.0598332211375237 | Loss: 0.0066691886338716 | Epoch: 134 | \n\n\nMeanAbsoluteError: 0.0649966076016426 | Loss: 0.0078978117415681 | Epoch: 135 | MeanAbsoluteError: 0.0681470260024071 | Loss: 0.0081956277745000 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.0582640469074249 | Loss: 0.0067703153119267 | Epoch: 137 | MeanAbsoluteError: 0.0616923533380032 | Loss: 0.0069055944564752 | Epoch: 138 | \n\n\nMeanAbsoluteError: 0.0591978207230568 | Loss: 0.0063925692371170 | Epoch: 139 | MeanAbsoluteError: 0.0609814263880253 | Loss: 0.0070889281460162 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.0596991032361984 | Loss: 0.0064010652053791 | Epoch: 141 | MeanAbsoluteError: 0.0628975331783295 | Loss: 0.0071506034051223 | Epoch: 142 | \n\n\nMeanAbsoluteError: 0.0654566287994385 | Loss: 0.0081557888568689 | Epoch: 143 | MeanAbsoluteError: 0.0677564293146133 | Loss: 0.0079451117499654 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.0584082789719105 | Loss: 0.0068364995124284 | Epoch: 145 | MeanAbsoluteError: 0.0631662905216217 | Loss: 0.0072677090779568 | Epoch: 146 | \n\n\nMeanAbsoluteError: 0.0634668841958046 | Loss: 0.0068689097553336 | Epoch: 147 | MeanAbsoluteError: 0.0553543679416180 | Loss: 0.0056546189884345 | Epoch: 148 | \n\n\nMeanAbsoluteError: 0.0601067803800106 | Loss: 0.0069820455104734 | Epoch: 149 | MeanAbsoluteError: 0.0559836961328983 | Loss: 0.0057291803245122 | Epoch: 150 | \n\n\nMeanAbsoluteError: 0.0628093704581261 | Loss: 0.0077898954107271 | Epoch: 151 | MeanAbsoluteError: 0.0621419586241245 | Loss: 0.0073191877097512 | Epoch: 152 | \n\n\nMeanAbsoluteError: 0.0625055506825447 | Loss: 0.0076417384461577 | Epoch: 153 | MeanAbsoluteError: 0.0573233552277088 | Loss: 0.0060014584020246 | Epoch: 154 | \n\n\nMeanAbsoluteError: 0.0562003888189793 | Loss: 0.0059239783813246 | Epoch: 155 | MeanAbsoluteError: 0.0573719665408134 | Loss: 0.0059159844993458 | Epoch: 156 | \n\n\nMeanAbsoluteError: 0.0592846497893333 | Loss: 0.0069319048714048 | Epoch: 157 | MeanAbsoluteError: 0.0603599622845650 | Loss: 0.0073523338426215 | Epoch: 158 | \n\n\nMeanAbsoluteError: 0.0581473596394062 | Loss: 0.0058612320129760 | Epoch: 159 | MeanAbsoluteError: 0.0626941546797752 | Loss: 0.0073043736625308 | Epoch: 160 | \n\n\nMeanAbsoluteError: 0.0583581738173962 | Loss: 0.0067376023496035 | Epoch: 161 | MeanAbsoluteError: 0.0561523512005806 | Loss: 0.0058106175053399 | Epoch: 162 | \n\n\nMeanAbsoluteError: 0.0554212182760239 | Loss: 0.0059945510523782 | Epoch: 163 | MeanAbsoluteError: 0.0519727095961571 | Loss: 0.0051320548235284 | Epoch: 164 | \n\n\nMeanAbsoluteError: 0.0578213818371296 | Loss: 0.0060910463996697 | Epoch: 165 | MeanAbsoluteError: 0.0568415783345699 | Loss: 0.0058914846965733 | Epoch: 166 | \n\n\nMeanAbsoluteError: 0.0594708882272243 | Loss: 0.0065439561502232 | Epoch: 167 | MeanAbsoluteError: 0.0575905032455921 | Loss: 0.0059906013292493 | Epoch: 168 | \n\n\nMeanAbsoluteError: 0.0598178356885910 | Loss: 0.0063319427069897 | Epoch: 169 | MeanAbsoluteError: 0.0597157143056393 | Loss: 0.0068088854267262 | Epoch: 170 | \n\n\nMeanAbsoluteError: 0.0567392557859421 | Loss: 0.0062208756804466 | Epoch: 171 | MeanAbsoluteError: 0.0590122528374195 | Loss: 0.0070847374573350 | Epoch: 172 | \n\n\nMeanAbsoluteError: 0.0614562779664993 | Loss: 0.0071366745342190 | Epoch: 173 | MeanAbsoluteError: 0.0558472834527493 | Loss: 0.0054916674769872 | Epoch: 174 | \n\n\nMeanAbsoluteError: 0.0586356483399868 | Loss: 0.0061270485833908 | Epoch: 175 | MeanAbsoluteError: 0.0597294308245182 | Loss: 0.0065632935357280 | Epoch: 176 | \n\n\nMeanAbsoluteError: 0.0554641224443913 | Loss: 0.0063310784972661 | Epoch: 177 | MeanAbsoluteError: 0.0604863502085209 | Loss: 0.0066942496033153 | Epoch: 178 | \n\n\nMeanAbsoluteError: 0.0500584915280342 | Loss: 0.0046956882818874 | Epoch: 179 | MeanAbsoluteError: 0.0540496632456779 | Loss: 0.0060235365657718 | Epoch: 180 | \n\n\nMeanAbsoluteError: 0.0555200465023518 | Loss: 0.0058309380503003 | Epoch: 181 | MeanAbsoluteError: 0.0568605214357376 | Loss: 0.0065100298218507 | Epoch: 182 | \n\n\nMeanAbsoluteError: 0.0552150420844555 | Loss: 0.0056039027159568 | Epoch: 183 | MeanAbsoluteError: 0.0663308277726173 | Loss: 0.0078350099610786 | Epoch: 184 | \n\n\nMeanAbsoluteError: 0.0551104880869389 | Loss: 0.0052505514813432 | Epoch: 185 | MeanAbsoluteError: 0.0570148341357708 | Loss: 0.0058449960484480 | Epoch: 186 | \n\n\nMeanAbsoluteError: 0.0509347654879093 | Loss: 0.0050294269380781 | Epoch: 187 | MeanAbsoluteError: 0.0538689419627190 | Loss: 0.0052956285983479 | Epoch: 188 | \n\n\nMeanAbsoluteError: 0.0658945664763451 | Loss: 0.0077197101642378 | Epoch: 189 | MeanAbsoluteError: 0.0651315376162529 | Loss: 0.0083250643887247 | Epoch: 190 | \n\n\nMeanAbsoluteError: 0.0554090440273285 | Loss: 0.0052991025929805 | Epoch: 191 | MeanAbsoluteError: 0.0512379184365273 | Loss: 0.0052386599072876 | Epoch: 192 | \n\n\nMeanAbsoluteError: 0.0514104440808296 | Loss: 0.0051368224195903 | Epoch: 193 | MeanAbsoluteError: 0.0559420250356197 | Loss: 0.0065539117760879 | Epoch: 194 | \n\n\nMeanAbsoluteError: 0.0546976029872894 | Loss: 0.0056653316156007 | Epoch: 195 | MeanAbsoluteError: 0.0592932887375355 | Loss: 0.0065645143036575 | Epoch: 196 | \n\n\nMeanAbsoluteError: 0.0573203265666962 | Loss: 0.0059573408860403 | Epoch: 197 | MeanAbsoluteError: 0.0523943901062012 | Loss: 0.0059326466374720 | Epoch: 198 | \n\n\nMeanAbsoluteError: 0.0593158416450024 | Loss: 0.0064220852707513 | Epoch: 199 | MeanAbsoluteError: 0.0567591078579426 | Loss: 0.0063893381263673 | Epoch: 200 | \n\n\nMeanAbsoluteError: 0.0564936958253384 | Loss: 0.0062371160931070 | Epoch: 201 | MeanAbsoluteError: 0.0533522590994835 | Loss: 0.0053232810025414 | Epoch: 202 | \n\n\nMeanAbsoluteError: 0.0547106824815273 | Loss: 0.0056456499940638 | Epoch: 203 | MeanAbsoluteError: 0.0587269663810730 | Loss: 0.0060610565213331 | Epoch: 204 | \n\n\nMeanAbsoluteError: 0.0541753880679607 | Loss: 0.0055495734612001 | Epoch: 205 | MeanAbsoluteError: 0.0608521401882172 | Loss: 0.0071811889202218 | Epoch: 206 | \n\n\nMeanAbsoluteError: 0.0570027232170105 | Loss: 0.0067091834118279 | Epoch: 207 | MeanAbsoluteError: 0.0559541583061218 | Loss: 0.0062644317606464 | Epoch: 208 | \n\n\nMeanAbsoluteError: 0.0512436628341675 | Loss: 0.0054647744503260 | Epoch: 209 | MeanAbsoluteError: 0.0540779493749142 | Loss: 0.0054601843228253 | Epoch: 210 | \n\n\nMeanAbsoluteError: 0.0533178187906742 | Loss: 0.0052568084727197 | Epoch: 211 | MeanAbsoluteError: 0.0574380941689014 | Loss: 0.0056607826925271 | Epoch: 212 | \n\n\nMeanAbsoluteError: 0.0534116812050343 | Loss: 0.0053714882749288 | Epoch: 213 | MeanAbsoluteError: 0.0535012558102608 | Loss: 0.0054875520492593 | Epoch: 214 | \n\n\nMeanAbsoluteError: 0.0509912483394146 | Loss: 0.0049476247843510 | Epoch: 215 | MeanAbsoluteError: 0.0542970709502697 | Loss: 0.0061796416221963 | Epoch: 216 | \n\n\nMeanAbsoluteError: 0.0541329048573971 | Loss: 0.0056496668055964 | Epoch: 217 | MeanAbsoluteError: 0.0606639720499516 | Loss: 0.0077252841488613 | Epoch: 218 | \n\n\nMeanAbsoluteError: 0.0564655698835850 | Loss: 0.0063644003410203 | Epoch: 219 | MeanAbsoluteError: 0.0526059083640575 | Loss: 0.0056764187749165 | Epoch: 220 | \n\n\nMeanAbsoluteError: 0.0533168874680996 | Loss: 0.0057329587622856 | Epoch: 221 | MeanAbsoluteError: 0.0517273470759392 | Loss: 0.0050314334714009 | Epoch: 222 | \n\n\nMeanAbsoluteError: 0.0520244389772415 | Loss: 0.0051691916449151 | Epoch: 223 | MeanAbsoluteError: 0.0572088509798050 | Loss: 0.0063398754482235 | Epoch: 224 | \n\n\nMeanAbsoluteError: 0.0546128898859024 | Loss: 0.0055671279683399 | Epoch: 225 | MeanAbsoluteError: 0.0540505163371563 | Loss: 0.0057060423281897 | Epoch: 226 | \n\n\nMeanAbsoluteError: 0.0539673604071140 | Loss: 0.0063016994588543 | Epoch: 227 | MeanAbsoluteError: 0.0549822226166725 | Loss: 0.0067315674693479 | Epoch: 228 | \n\n\nMeanAbsoluteError: 0.0554960444569588 | Loss: 0.0063870610157028 | Epoch: 229 | MeanAbsoluteError: 0.0525849424302578 | Loss: 0.0055673836051331 | Epoch: 230 | \n\n\nMeanAbsoluteError: 0.0548039749264717 | Loss: 0.0054075817530975 | Epoch: 231 | MeanAbsoluteError: 0.0528058782219887 | Loss: 0.0055019666832717 | Epoch: 232 | \n\n\nMeanAbsoluteError: 0.0525537729263306 | Loss: 0.0054834752525979 | Epoch: 233 | MeanAbsoluteError: 0.0494619831442833 | Loss: 0.0045758256620320 | Epoch: 234 | \n\n\nMeanAbsoluteError: 0.0526458509266376 | Loss: 0.0051919102692045 | Epoch: 235 | MeanAbsoluteError: 0.0542999021708965 | Loss: 0.0052902087619683 | Epoch: 236 | \n\n\nMeanAbsoluteError: 0.0502070076763630 | Loss: 0.0048473691721059 | Epoch: 237 | MeanAbsoluteError: 0.0535231828689575 | Loss: 0.0053444961578740 | Epoch: 238 | \n\n\nMeanAbsoluteError: 0.0555566735565662 | Loss: 0.0063523560785688 | Epoch: 239 | MeanAbsoluteError: 0.0501616150140762 | Loss: 0.0047861327859573 | Epoch: 240 | \n\n\nMeanAbsoluteError: 0.0661387667059898 | Loss: 0.0076035359870487 | Epoch: 241 | MeanAbsoluteError: 0.0565300323069096 | Loss: 0.0056566705977699 | Epoch: 242 | \n\n\nMeanAbsoluteError: 0.0513373091816902 | Loss: 0.0050727387560376 | Epoch: 243 | MeanAbsoluteError: 0.0481881015002728 | Loss: 0.0048076275237448 | Epoch: 244 | \n\n\nMeanAbsoluteError: 0.0489313937723637 | Loss: 0.0047842409793520 | Epoch: 245 | MeanAbsoluteError: 0.0507025457918644 | Loss: 0.0048770856500293 | Epoch: 246 | \n\n\nMeanAbsoluteError: 0.0544087141752243 | Loss: 0.0060240865463857 | Epoch: 247 | MeanAbsoluteError: 0.0585506074130535 | Loss: 0.0062068030102334 | Epoch: 248 | \n\n\nMeanAbsoluteError: 0.0554454550147057 | Loss: 0.0062171239149757 | Epoch: 249 | MeanAbsoluteError: 0.0532264932990074 | Loss: 0.0053454048175869 | Epoch: 250 | \n\n\nMeanAbsoluteError: 0.0553541705012321 | Loss: 0.0058537125668954 | Epoch: 251 | MeanAbsoluteError: 0.0578911453485489 | Loss: 0.0061944401718210 | Epoch: 252 | \n\n\nMeanAbsoluteError: 0.0474231727421284 | Loss: 0.0045783905514205 | Epoch: 253 | MeanAbsoluteError: 0.0470747165381908 | Loss: 0.0043909413615863 | Epoch: 254 | \n\n\nMeanAbsoluteError: 0.0498881898820400 | Loss: 0.0049880384997232 | Epoch: 255 | MeanAbsoluteError: 0.0504282154142857 | Loss: 0.0052881621509247 | Epoch: 256 | \n\n\nMeanAbsoluteError: 0.0482624024152756 | Loss: 0.0047011681304624 | Epoch: 257 | MeanAbsoluteError: 0.0471826754510403 | Loss: 0.0045137240992820 | Epoch: 258 | \n\n\nMeanAbsoluteError: 0.0499424003064632 | Loss: 0.0041160462367892 | Epoch: 259 | MeanAbsoluteError: 0.0523801185190678 | Loss: 0.0053304641850021 | Epoch: 260 | \n\n\nMeanAbsoluteError: 0.0560660362243652 | Loss: 0.0059335717768408 | Epoch: 261 | MeanAbsoluteError: 0.0522783100605011 | Loss: 0.0058047064937030 | Epoch: 262 | \n\n\nMeanAbsoluteError: 0.0546850785613060 | Loss: 0.0062121323123574 | Epoch: 263 | MeanAbsoluteError: 0.0531295686960220 | Loss: 0.0050469706308407 | Epoch: 264 | \n\n\nMeanAbsoluteError: 0.0519841536879539 | Loss: 0.0050009493161148 | Epoch: 265 | MeanAbsoluteError: 0.0546168461441994 | Loss: 0.0057877273735357 | Epoch: 266 | \n\n\nMeanAbsoluteError: 0.0522160828113556 | Loss: 0.0051239888639733 | Epoch: 267 | MeanAbsoluteError: 0.0536728911101818 | Loss: 0.0059429830776450 | Epoch: 268 | \n\n\nMeanAbsoluteError: 0.0547285079956055 | Loss: 0.0058940995928909 | Epoch: 269 | MeanAbsoluteError: 0.0519401170313358 | Loss: 0.0051954035084539 | Epoch: 270 | \n\n\nMeanAbsoluteError: 0.0489148236811161 | Loss: 0.0043887910180880 | Epoch: 271 | MeanAbsoluteError: 0.0548114217817783 | Loss: 0.0066461836150847 | Epoch: 272 | \n\n\nMeanAbsoluteError: 0.0489772558212280 | Loss: 0.0048500692687230 | Epoch: 273 | MeanAbsoluteError: 0.0530206002295017 | Loss: 0.0056353312179757 | Epoch: 274 | \n\n\nMeanAbsoluteError: 0.0535652860999107 | Loss: 0.0057608545858723 | Epoch: 275 | MeanAbsoluteError: 0.0482841245830059 | Loss: 0.0044578260004831 | Epoch: 276 | \n\n\nMeanAbsoluteError: 0.0510544404387474 | Loss: 0.0047662847549266 | Epoch: 277 | MeanAbsoluteError: 0.0474836751818657 | Loss: 0.0045183502916674 | Epoch: 278 | \n\n\nMeanAbsoluteError: 0.0434792786836624 | Loss: 0.0035835887754608 | Epoch: 279 | MeanAbsoluteError: 0.0501062944531441 | Loss: 0.0051768907260460 | Epoch: 280 | \n\n\nMeanAbsoluteError: 0.0508466549217701 | Loss: 0.0043249889420501 | Epoch: 281 | MeanAbsoluteError: 0.0522779300808907 | Loss: 0.0054122773800433 | Epoch: 282 | \n\n\nMeanAbsoluteError: 0.0518611595034599 | Loss: 0.0055231723509496 | Epoch: 283 | MeanAbsoluteError: 0.0536690950393677 | Loss: 0.0052607965252052 | Epoch: 284 | \n\n\nMeanAbsoluteError: 0.0520121604204178 | Loss: 0.0054107366758399 | Epoch: 285 | MeanAbsoluteError: 0.0555229894816875 | Loss: 0.0060194876650348 | Epoch: 286 | \n\n\nMeanAbsoluteError: 0.0504622571170330 | Loss: 0.0052237306738001 | Epoch: 287 | MeanAbsoluteError: 0.0512639842927456 | Loss: 0.0052361385574720 | Epoch: 288 | \n\n\nMeanAbsoluteError: 0.0466973111033440 | Loss: 0.0045604683295824 | Epoch: 289 | MeanAbsoluteError: 0.0504240207374096 | Loss: 0.0050435072971353 | Epoch: 290 | \n\n\nMeanAbsoluteError: 0.0503786168992519 | Loss: 0.0051980702383056 | Epoch: 291 | MeanAbsoluteError: 0.0560869351029396 | Loss: 0.0061791219566173 | Epoch: 292 | \n\n\nMeanAbsoluteError: 0.0482281073927879 | Loss: 0.0049068322139404 | Epoch: 293 | MeanAbsoluteError: 0.0487491749227047 | Loss: 0.0049878844941365 | Epoch: 294 | \n\n\nMeanAbsoluteError: 0.0470203235745430 | Loss: 0.0046778473533535 | Epoch: 295 | MeanAbsoluteError: 0.0510626584291458 | Loss: 0.0051851619151421 | Epoch: 296 | \n\n\nMeanAbsoluteError: 0.0512569881975651 | Loss: 0.0049067890484973 | Epoch: 297 | MeanAbsoluteError: 0.0543300956487656 | Loss: 0.0056286358926445 | Epoch: 298 | \n\n\nMeanAbsoluteError: 0.0525091923773289 | Loss: 0.0053495194161466 | Epoch: 299 | MeanAbsoluteError: 0.0521620512008667 | Loss: 0.0054494927222064 | Epoch: 300 | \n\n\nMeanAbsoluteError: 0.0462273508310318 | Loss: 0.0040922604446920 | Epoch: 301 | MeanAbsoluteError: 0.0514522790908813 | Loss: 0.0053290189659068 | Epoch: 302 | \n\n\nMeanAbsoluteError: 0.0481660962104797 | Loss: 0.0048193102392058 | Epoch: 303 | MeanAbsoluteError: 0.0515345744788647 | Loss: 0.0049298125877976 | Epoch: 304 | \n\n\nMeanAbsoluteError: 0.0485807508230209 | Loss: 0.0042760002398669 | Epoch: 305 | MeanAbsoluteError: 0.0499870255589485 | Loss: 0.0046892475606486 | Epoch: 306 | \n\n\nMeanAbsoluteError: 0.0471970476210117 | Loss: 0.0045133029708328 | Epoch: 307 | MeanAbsoluteError: 0.0551366433501244 | Loss: 0.0061214333708631 | Epoch: 308 | \n\n\nMeanAbsoluteError: 0.0511462613940239 | Loss: 0.0057534884079359 | Epoch: 309 | MeanAbsoluteError: 0.0494348779320717 | Loss: 0.0044227213656995 | Epoch: 310 | \n\n\nMeanAbsoluteError: 0.0491472035646439 | Loss: 0.0047654159363204 | Epoch: 311 | MeanAbsoluteError: 0.0510734245181084 | Loss: 0.0049499110905648 | Epoch: 312 | \n\n\nMeanAbsoluteError: 0.0495627298951149 | Loss: 0.0045561165820497 | Epoch: 313 | MeanAbsoluteError: 0.0490550063550472 | Loss: 0.0050240923254751 | Epoch: 314 | \n\n\nMeanAbsoluteError: 0.0499010421335697 | Loss: 0.0048105727458217 | Epoch: 315 | MeanAbsoluteError: 0.0477031841874123 | Loss: 0.0041096608410589 | Epoch: 316 | \n\n\nMeanAbsoluteError: 0.0565362870693207 | Loss: 0.0063931302450753 | Epoch: 317 | MeanAbsoluteError: 0.0473321899771690 | Loss: 0.0045718702007434 | Epoch: 318 | \n\n\nMeanAbsoluteError: 0.0493125244975090 | Loss: 0.0043138844360753 | Epoch: 319 | MeanAbsoluteError: 0.0491621829569340 | Loss: 0.0044072401927163 | Epoch: 320 | \n\n\nMeanAbsoluteError: 0.0503745488822460 | Loss: 0.0051451813635261 | Epoch: 321 | MeanAbsoluteError: 0.0477905906736851 | Loss: 0.0042762893816689 | Epoch: 322 | \n\n\nMeanAbsoluteError: 0.0509378835558891 | Loss: 0.0045102435173855 | Epoch: 323 | MeanAbsoluteError: 0.0464884489774704 | Loss: 0.0045274469074017 | Epoch: 324 | \n\n\nMeanAbsoluteError: 0.0476617552340031 | Loss: 0.0048707052659787 | Epoch: 325 | MeanAbsoluteError: 0.0540266744792461 | Loss: 0.0057508789575271 | Epoch: 326 | \n\n\nMeanAbsoluteError: 0.0492130592465401 | Loss: 0.0045939386310056 | Epoch: 327 | MeanAbsoluteError: 0.0480968877673149 | Loss: 0.0051061439587890 | Epoch: 328 | \n\n\nMeanAbsoluteError: 0.0494499206542969 | Loss: 0.0047029087470340 | Epoch: 329 | MeanAbsoluteError: 0.0501534678041935 | Loss: 0.0047564627105991 | Epoch: 330 | \n\n\nMeanAbsoluteError: 0.0470126941800117 | Loss: 0.0042647732270416 | Epoch: 331 | MeanAbsoluteError: 0.0472428798675537 | Loss: 0.0043776567177459 | Epoch: 332 | \n\n\nMeanAbsoluteError: 0.0506497360765934 | Loss: 0.0049570010629638 | Epoch: 333 | MeanAbsoluteError: 0.0495028272271156 | Loss: 0.0044332401058152 | Epoch: 334 | \n\n\nMeanAbsoluteError: 0.0522122122347355 | Loss: 0.0055821759998798 | Epoch: 335 | MeanAbsoluteError: 0.0485757887363434 | Loss: 0.0045175147615373 | Epoch: 336 | \n\n\nMeanAbsoluteError: 0.0540160499513149 | Loss: 0.0064055296087948 | Epoch: 337 | MeanAbsoluteError: 0.0484851300716400 | Loss: 0.0046458123864916 | Epoch: 338 | \n\n\nMeanAbsoluteError: 0.0507192686200142 | Loss: 0.0053518580975166 | Epoch: 339 | MeanAbsoluteError: 0.0499616228044033 | Loss: 0.0051836203294806 | Epoch: 340 | \n\n\nMeanAbsoluteError: 0.0454424954950809 | Loss: 0.0040550476374726 | Epoch: 341 | MeanAbsoluteError: 0.0534632429480553 | Loss: 0.0054036211625983 | Epoch: 342 | \n\n\nMeanAbsoluteError: 0.0505959764122963 | Loss: 0.0046164030284854 | Epoch: 343 | MeanAbsoluteError: 0.0464157909154892 | Loss: 0.0043301124172285 | Epoch: 344 | \n\n\nMeanAbsoluteError: 0.0509919188916683 | Loss: 0.0055108200534596 | Epoch: 345 | MeanAbsoluteError: 0.0584966465830803 | Loss: 0.0069208111012510 | Epoch: 346 | \n\n\nMeanAbsoluteError: 0.0546887218952179 | Loss: 0.0058479381890114 | Epoch: 347 | MeanAbsoluteError: 0.0507684051990509 | Loss: 0.0055689109714391 | Epoch: 348 | \n\n\nMeanAbsoluteError: 0.0514648705720901 | Loss: 0.0048424210042382 | Epoch: 349 | MeanAbsoluteError: 0.0543108694255352 | Loss: 0.0065020669236158 | Epoch: 350 | \n\n\nMeanAbsoluteError: 0.0524846091866493 | Loss: 0.0051321337614597 | Epoch: 351 | MeanAbsoluteError: 0.0519338175654411 | Loss: 0.0055725759578248 | Epoch: 352 | \n\n\nMeanAbsoluteError: 0.0480285584926605 | Loss: 0.0041363964796377 | Epoch: 353 | MeanAbsoluteError: 0.0465803332626820 | Loss: 0.0038319406637068 | Epoch: 354 | \n\n\nMeanAbsoluteError: 0.0526652261614799 | Loss: 0.0055225072413062 | Epoch: 355 | MeanAbsoluteError: 0.0513392053544521 | Loss: 0.0050126676548583 | Epoch: 356 | \n\n\nMeanAbsoluteError: 0.0526792965829372 | Loss: 0.0057110572292004 | Epoch: 357 | MeanAbsoluteError: 0.0524572245776653 | Loss: 0.0053451906969228 | Epoch: 358 | \n\n\nMeanAbsoluteError: 0.0499896109104156 | Loss: 0.0051202726527117 | Epoch: 359 | MeanAbsoluteError: 0.0485992208123207 | Loss: 0.0050338091173520 | Epoch: 360 | \n\n\nMeanAbsoluteError: 0.0546162724494934 | Loss: 0.0054377635762406 | Epoch: 361 | MeanAbsoluteError: 0.0452959761023521 | Loss: 0.0047190541519861 | Epoch: 362 | \n\n\nMeanAbsoluteError: 0.0475983135402203 | Loss: 0.0044291503541172 | Epoch: 363 | MeanAbsoluteError: 0.0473993383347988 | Loss: 0.0046582022974811 | Epoch: 364 | \n\n\nMeanAbsoluteError: 0.0485245585441589 | Loss: 0.0044211982997755 | Epoch: 365 | MeanAbsoluteError: 0.0487501770257950 | Loss: 0.0046258013095940 | Epoch: 366 | \n\n\nMeanAbsoluteError: 0.0517052263021469 | Loss: 0.0053059174364898 | Epoch: 367 | MeanAbsoluteError: 0.0465506985783577 | Loss: 0.0044066703886104 | Epoch: 368 | \n\n\nMeanAbsoluteError: 0.0516819022595882 | Loss: 0.0050189139503830 | Epoch: 369 | MeanAbsoluteError: 0.0483882986009121 | Loss: 0.0044910277174010 | Epoch: 370 | \n\n\nMeanAbsoluteError: 0.0492440164089203 | Loss: 0.0048186376822802 | Epoch: 371 | MeanAbsoluteError: 0.0470928288996220 | Loss: 0.0042723344175223 | Epoch: 372 | \n\n\nMeanAbsoluteError: 0.0485440567135811 | Loss: 0.0043676902718532 | Epoch: 373 | MeanAbsoluteError: 0.0489655211567879 | Loss: 0.0046465256953767 | Epoch: 374 | \n\n\nMeanAbsoluteError: 0.0491125471889973 | Loss: 0.0045182283265361 | Epoch: 375 | MeanAbsoluteError: 0.0540316961705685 | Loss: 0.0054616326939625 | Epoch: 376 | \n\n\nMeanAbsoluteError: 0.0487948134541512 | Loss: 0.0049011161201634 | Epoch: 377 | MeanAbsoluteError: 0.0513366684317589 | Loss: 0.0046838170443273 | Epoch: 378 | \n\n\nMeanAbsoluteError: 0.0479011982679367 | Loss: 0.0046351648638180 | Epoch: 379 | MeanAbsoluteError: 0.0446074455976486 | Loss: 0.0040359319913356 | Epoch: 380 | \n\n\nMeanAbsoluteError: 0.0509321019053459 | Loss: 0.0048928437525562 | Epoch: 381 | MeanAbsoluteError: 0.0479034669697285 | Loss: 0.0046369360821942 | Epoch: 382 | \n\n\nMeanAbsoluteError: 0.0495193675160408 | Loss: 0.0051309234503424 | Epoch: 383 | MeanAbsoluteError: 0.0510992035269737 | Loss: 0.0050872974644881 | Epoch: 384 | \n\n\nMeanAbsoluteError: 0.0489227548241615 | Loss: 0.0047685851802817 | Epoch: 385 | MeanAbsoluteError: 0.0501937642693520 | Loss: 0.0049769177361547 | Epoch: 386 | \n\n\nMeanAbsoluteError: 0.0472001023590565 | Loss: 0.0042071338091046 | Epoch: 387 | MeanAbsoluteError: 0.0425518825650215 | Loss: 0.0037004404689651 | Epoch: 388 | \n\n\nMeanAbsoluteError: 0.0506885126233101 | Loss: 0.0049997513822746 | Epoch: 389 | MeanAbsoluteError: 0.0496575273573399 | Loss: 0.0043246536453565 | Epoch: 390 | \n\n\nMeanAbsoluteError: 0.0460200645029545 | Loss: 0.0044527608257098 | Epoch: 391 | MeanAbsoluteError: 0.0493900142610073 | Loss: 0.0046502126929893 | Epoch: 392 | \n\n\nMeanAbsoluteError: 0.0483104176819324 | Loss: 0.0044841232209001 | Epoch: 393 | MeanAbsoluteError: 0.0511729232966900 | Loss: 0.0048651516208580 | Epoch: 394 | \n\n\nMeanAbsoluteError: 0.0442194417119026 | Loss: 0.0039612202792584 | Epoch: 395 | MeanAbsoluteError: 0.0477092862129211 | Loss: 0.0047022056063482 | Epoch: 396 | \n\n\nMeanAbsoluteError: 0.0467097237706184 | Loss: 0.0041385471717998 | Epoch: 397 | MeanAbsoluteError: 0.0468577444553375 | Loss: 0.0041306895013743 | Epoch: 398 | \n\n\nMeanAbsoluteError: 0.0492485240101814 | Loss: 0.0055696927255485 | Epoch: 399 | MeanAbsoluteError: 0.0541904717683792 | Loss: 0.0051452718657674 | Epoch: 400 | \n\n\nMeanAbsoluteError: 0.0451606437563896 | Loss: 0.0037410268278715 | Epoch: 401 | MeanAbsoluteError: 0.0440681204199791 | Loss: 0.0036797620496266 | Epoch: 402 | \n\n\nMeanAbsoluteError: 0.0491785630583763 | Loss: 0.0047642871784046 | Epoch: 403 | MeanAbsoluteError: 0.0506835579872131 | Loss: 0.0054447877155326 | Epoch: 404 | \n\n\nMeanAbsoluteError: 0.0515647679567337 | Loss: 0.0056245957978535 | Epoch: 405 | MeanAbsoluteError: 0.0443008393049240 | Loss: 0.0041712260482988 | Epoch: 406 | \n\n\nMeanAbsoluteError: 0.0484815239906311 | Loss: 0.0047341522816957 | Early stopping at epoch 405\nReturned to Spot: Validation loss: 0.004734152281695666\n\n\nspotPython tuning: 0.0026928081229255583 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x186582fb0&gt;"
  },
  {
    "objectID": "24_spot_torch_regression.html#sec-tensorboard-24",
    "href": "24_spot_torch_regression.html#sec-tensorboard-24",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.9 Step 9: Tensorboard",
    "text": "19.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard as described in Section 14.9, see also the description in the documentation: Tensorboard."
  },
  {
    "objectID": "24_spot_torch_regression.html#sec-results-tuning-24",
    "href": "24_spot_torch_regression.html#sec-results-tuning-24",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.10 Step 10: Results",
    "text": "19.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed as described in Section 14.10.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name         | type   | default   |   lower |   upper |              tuned | transform             |   importance | stars   |\n|--------------|--------|-----------|---------|---------|--------------------|-----------------------|--------------|---------|\n| _L_in        | int    | 10        |    10.0 |    10.0 |               10.0 | None                  |         0.00 |         |\n| _L_out       | int    | 1         |     1.0 |     1.0 |                1.0 | None                  |         0.00 |         |\n| l1           | int    | 3         |     3.0 |     8.0 |                5.0 | transform_power_2_int |         0.00 |         |\n| dropout_prob | float  | 0.01      |     0.0 |     0.9 | 0.3840970624671163 | None                  |         0.00 |         |\n| lr_mult      | float  | 1.0       |     0.1 |    10.0 |  4.593111165984917 | None                  |         0.00 |         |\n| batch_size   | int    | 4         |     1.0 |     4.0 |                3.0 | transform_power_2_int |         0.00 |         |\n| epochs       | int    | 4         |     2.0 |    16.0 |                8.0 | transform_power_2_int |         0.00 |         |\n| k_folds      | int    | 1         |     1.0 |     1.0 |                1.0 | None                  |         0.00 |         |\n| patience     | int    | 2         |     3.0 |     7.0 |                6.0 | transform_power_2_int |       100.00 | ***     |\n| optimizer    | factor | SGD       |     0.0 |     6.0 |                2.0 | None                  |         3.06 | *       |\n| sgd_momentum | float  | 0.0       |     0.0 |     1.0 | 0.8945915831939406 | None                  |         0.00 |         |\n\n\n\nspot_tuner.plot_importance(threshold=0.025,\n    filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n19.10.1 Get the Tuned Architecture (SPOT Results)\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\nmodel_spot\n\nNet_lin_reg(\n  (fc1): Linear(in_features=10, out_features=32, bias=True)\n  (fc2): Linear(in_features=32, out_features=16, bias=True)\n  (fc3): Linear(in_features=16, out_features=1, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n  (dropout1): Dropout(p=0.3840970624671163, inplace=False)\n  (dropout2): Dropout(p=0.19204853123355814, inplace=False)\n)\n\n\n\n\n19.10.2 Evaluation of the Tuned Architecture\n\nfrom spotPython.torch.traintest import (\n    train_tuned,\n    test_tuned,\n    )\n\n\ntrain_tuned(net=model_spot, train_dataset=train,\n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        shuffle=True,\n        device = fun_control[\"device\"],\n        path=None,\n        task=fun_control[\"task\"],)\n\nEpoch: 1 | MeanAbsoluteError: 0.1452026963233948 | Loss: 0.0335182444388537 | Epoch: 2 | MeanAbsoluteError: 0.1487469822168350 | Loss: 0.0341262839440452 | Epoch: 3 | MeanAbsoluteError: 0.1354807168245316 | Loss: 0.0289670378459912 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1382693797349930 | Loss: 0.0304818746854404 | Epoch: 5 | MeanAbsoluteError: 0.1338861733675003 | Loss: 0.0285172380972654 | Epoch: 6 | MeanAbsoluteError: 0.1252007633447647 | Loss: 0.0243285162325360 | Epoch: 7 | MeanAbsoluteError: 0.1191776692867279 | Loss: 0.0227438320626358 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.1256391704082489 | Loss: 0.0241356412267410 | Epoch: 9 | MeanAbsoluteError: 0.1135099828243256 | Loss: 0.0203280732354247 | Epoch: 10 | MeanAbsoluteError: 0.1194888055324554 | Loss: 0.0229325490807624 | Epoch: 11 | MeanAbsoluteError: 0.1016323491930962 | Loss: 0.0161328654687263 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.1057643294334412 | Loss: 0.0171578947185098 | Epoch: 13 | MeanAbsoluteError: 0.0896292850375175 | Loss: 0.0120296734507735 | Epoch: 14 | MeanAbsoluteError: 0.0897451937198639 | Loss: 0.0134868050717111 | Epoch: 15 | MeanAbsoluteError: 0.0916105881333351 | Loss: 0.0137139958586838 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0833887010812759 | Loss: 0.0111732909051505 | Epoch: 17 | MeanAbsoluteError: 0.0831177160143852 | Loss: 0.0112475744313806 | Epoch: 18 | MeanAbsoluteError: 0.0788021609187126 | Loss: 0.0098636918456147 | Epoch: 19 | MeanAbsoluteError: 0.0685281082987785 | Loss: 0.0080855214527171 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0758379921317101 | Loss: 0.0098980836122983 | Epoch: 21 | MeanAbsoluteError: 0.0679179280996323 | Loss: 0.0083931007753371 | Epoch: 22 | MeanAbsoluteError: 0.0719663128256798 | Loss: 0.0084830545299490 | Epoch: 23 | MeanAbsoluteError: 0.0706743150949478 | Loss: 0.0080732407422099 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0633461475372314 | Loss: 0.0067800931696854 | Epoch: 25 | MeanAbsoluteError: 0.0587578006088734 | Loss: 0.0053307320371172 | Epoch: 26 | MeanAbsoluteError: 0.0561871416866779 | Loss: 0.0058208949110275 | Epoch: 27 | MeanAbsoluteError: 0.0539141707122326 | Loss: 0.0052158068620453 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0558262690901756 | Loss: 0.0055686693828504 | Epoch: 29 | MeanAbsoluteError: 0.0545781701803207 | Loss: 0.0059934325384491 | Epoch: 30 | MeanAbsoluteError: 0.0566979646682739 | Loss: 0.0056958928992237 | Epoch: 31 | MeanAbsoluteError: 0.0712664499878883 | Loss: 0.0080342411161645 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0689297392964363 | Loss: 0.0070113622534432 | Epoch: 33 | MeanAbsoluteError: 0.0550902001559734 | Loss: 0.0056472656862369 | Epoch: 34 | MeanAbsoluteError: 0.0627219602465630 | Loss: 0.0071714967581149 | Epoch: 35 | MeanAbsoluteError: 0.0551956743001938 | Loss: 0.0058335360740066 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0536488480865955 | Loss: 0.0056213534890527 | Epoch: 37 | MeanAbsoluteError: 0.0481662303209305 | Loss: 0.0045641687236065 | Epoch: 38 | MeanAbsoluteError: 0.0485285855829716 | Loss: 0.0041965541772937 | Epoch: 39 | MeanAbsoluteError: 0.0599705316126347 | Loss: 0.0062119007159613 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0544856525957584 | Loss: 0.0053551847737062 | Epoch: 41 | MeanAbsoluteError: 0.0551142767071724 | Loss: 0.0052154710888556 | Epoch: 42 | MeanAbsoluteError: 0.0635040849447250 | Loss: 0.0071087777446098 | Epoch: 43 | MeanAbsoluteError: 0.0564471110701561 | Loss: 0.0065697279600212 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0460282079875469 | Loss: 0.0043486493507915 | Epoch: 45 | MeanAbsoluteError: 0.0491646267473698 | Loss: 0.0044483513078089 | Epoch: 46 | MeanAbsoluteError: 0.0551451630890369 | Loss: 0.0053309493033387 | Epoch: 47 | MeanAbsoluteError: 0.0499053969979286 | Loss: 0.0049254095815980 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0540398545563221 | Loss: 0.0060243633848394 | Epoch: 49 | MeanAbsoluteError: 0.0559504330158234 | Loss: 0.0057218678779655 | Epoch: 50 | MeanAbsoluteError: 0.0549939908087254 | Loss: 0.0056132243158859 | Epoch: 51 | MeanAbsoluteError: 0.0549095422029495 | Loss: 0.0047687915564319 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0580382980406284 | Loss: 0.0057796993856563 | Epoch: 53 | MeanAbsoluteError: 0.0573721677064896 | Loss: 0.0057160441494106 | Epoch: 54 | MeanAbsoluteError: 0.0511518716812134 | Loss: 0.0057901769882607 | Epoch: 55 | MeanAbsoluteError: 0.0551319010555744 | Loss: 0.0054667607197955 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0485331341624260 | Loss: 0.0047709644702416 | Epoch: 57 | MeanAbsoluteError: 0.0687390789389610 | Loss: 0.0076224829597203 | Epoch: 58 | MeanAbsoluteError: 0.0703577697277069 | Loss: 0.0078628746896835 | Epoch: 59 | MeanAbsoluteError: 0.0503661073744297 | Loss: 0.0049471122788657 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0594974309206009 | Loss: 0.0066817105643598 | Epoch: 61 | MeanAbsoluteError: 0.0622154437005520 | Loss: 0.0062373629127825 | Epoch: 62 | MeanAbsoluteError: 0.0572934336960316 | Loss: 0.0060310982915230 | Epoch: 63 | MeanAbsoluteError: 0.0561948269605637 | Loss: 0.0060380990555706 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0667899623513222 | Loss: 0.0069170497897032 | Epoch: 65 | MeanAbsoluteError: 0.0524131059646606 | Loss: 0.0057148226386696 | Epoch: 66 | MeanAbsoluteError: 0.0485458374023438 | Loss: 0.0048595825708042 | Epoch: 67 | MeanAbsoluteError: 0.0524177104234695 | Loss: 0.0054084433083373 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0604204162955284 | Loss: 0.0063080012547097 | Epoch: 69 | MeanAbsoluteError: 0.0531986616551876 | Loss: 0.0057227053039242 | Epoch: 70 | MeanAbsoluteError: 0.0575113855302334 | Loss: 0.0055489024502764 | Epoch: 71 | MeanAbsoluteError: 0.0546585731208324 | Loss: 0.0055625870054906 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0580201670527458 | Loss: 0.0066199835641065 | Epoch: 73 | MeanAbsoluteError: 0.0523259602487087 | Loss: 0.0051274239414948 | Epoch: 74 | MeanAbsoluteError: 0.0562457405030727 | Loss: 0.0063354896081268 | Epoch: 75 | MeanAbsoluteError: 0.0518712438642979 | Loss: 0.0049856757150816 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0535834059119225 | Loss: 0.0052631487058917 | Epoch: 77 | MeanAbsoluteError: 0.0443778224289417 | Loss: 0.0038585444272030 | Epoch: 78 | MeanAbsoluteError: 0.0585601031780243 | Loss: 0.0064650922983052 | Epoch: 79 | MeanAbsoluteError: 0.0481479316949844 | Loss: 0.0044849265173788 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0501040033996105 | Loss: 0.0054745332477308 | Epoch: 81 | MeanAbsoluteError: 0.0484105125069618 | Loss: 0.0049299117618878 | Epoch: 82 | MeanAbsoluteError: 0.0465478412806988 | Loss: 0.0039917105137934 | Epoch: 83 | MeanAbsoluteError: 0.0489464886486530 | Loss: 0.0042108425312970 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0470662228763103 | Loss: 0.0047444540870952 | Epoch: 85 | MeanAbsoluteError: 0.0468078926205635 | Loss: 0.0045774908825191 | Epoch: 86 | MeanAbsoluteError: 0.0538612231612206 | Loss: 0.0048591699757564 | Epoch: 87 | MeanAbsoluteError: 0.0644884705543518 | Loss: 0.0069115093363890 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0541604235768318 | Loss: 0.0056323671815335 | Epoch: 89 | MeanAbsoluteError: 0.0488942824304104 | Loss: 0.0046288594726081 | Epoch: 90 | MeanAbsoluteError: 0.0428629145026207 | Loss: 0.0038760601386648 | Epoch: 91 | MeanAbsoluteError: 0.0462682470679283 | Loss: 0.0043188090249283 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0533552430570126 | Loss: 0.0050286553573403 | Epoch: 93 | MeanAbsoluteError: 0.0470659472048283 | Loss: 0.0047704164646443 | Epoch: 94 | MeanAbsoluteError: 0.0483082570135593 | Loss: 0.0037135705903271 | Epoch: 95 | MeanAbsoluteError: 0.0498273931443691 | Loss: 0.0050782654725481 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0453369282186031 | Loss: 0.0044591944070386 | Epoch: 97 | MeanAbsoluteError: 0.0567113496363163 | Loss: 0.0058389523619553 | Epoch: 98 | MeanAbsoluteError: 0.0690312013030052 | Loss: 0.0077860488274478 | Epoch: 99 | MeanAbsoluteError: 0.0498708710074425 | Loss: 0.0047168485930582 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0563189685344696 | Loss: 0.0053789089862747 | Epoch: 101 | MeanAbsoluteError: 0.0525321289896965 | Loss: 0.0046791873755865 | Epoch: 102 | MeanAbsoluteError: 0.0488443039357662 | Loss: 0.0048144515465920 | Epoch: 103 | MeanAbsoluteError: 0.0584140792489052 | Loss: 0.0060584342159257 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0480701699852943 | Loss: 0.0049129211095676 | Epoch: 105 | MeanAbsoluteError: 0.0442164652049541 | Loss: 0.0037912613624940 | Epoch: 106 | MeanAbsoluteError: 0.0528691262006760 | Loss: 0.0051575787656475 | Epoch: 107 | MeanAbsoluteError: 0.0471432060003281 | Loss: 0.0039041719681240 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0468285195529461 | Loss: 0.0045120795808831 | Epoch: 109 | MeanAbsoluteError: 0.0427706614136696 | Loss: 0.0041039643983822 | Epoch: 110 | MeanAbsoluteError: 0.0622927099466324 | Loss: 0.0071573330113958 | Epoch: 111 | MeanAbsoluteError: 0.0546166598796844 | Loss: 0.0054101783944612 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0573270022869110 | Loss: 0.0056644472550895 | Epoch: 113 | MeanAbsoluteError: 0.0425492711365223 | Loss: 0.0038134879042015 | Epoch: 114 | MeanAbsoluteError: 0.0638893097639084 | Loss: 0.0056761280131085 | Epoch: 115 | MeanAbsoluteError: 0.0543937161564827 | Loss: 0.0056046907266136 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0496372207999229 | Loss: 0.0046350359778827 | Epoch: 117 | MeanAbsoluteError: 0.0484010428190231 | Loss: 0.0045255945419501 | Epoch: 118 | MeanAbsoluteError: 0.0425686649978161 | Loss: 0.0039990221871688 | Epoch: 119 | MeanAbsoluteError: 0.0527838915586472 | Loss: 0.0060718047944216 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0460994578897953 | Loss: 0.0041397361951807 | Epoch: 121 | MeanAbsoluteError: 0.0550101213157177 | Loss: 0.0062662790756133 | Epoch: 122 | MeanAbsoluteError: 0.0533699803054333 | Loss: 0.0048624770639244 | Epoch: 123 | MeanAbsoluteError: 0.0565025843679905 | Loss: 0.0061208362182553 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0469417832791805 | Loss: 0.0042170958568979 | Epoch: 125 | MeanAbsoluteError: 0.0526153668761253 | Loss: 0.0053058824361008 | Epoch: 126 | MeanAbsoluteError: 0.0482078082859516 | Loss: 0.0040698700743776 | Epoch: 127 | MeanAbsoluteError: 0.0506132207810879 | Loss: 0.0045438143271512 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0633010417222977 | Loss: 0.0069029444160773 | Epoch: 129 | MeanAbsoluteError: 0.0630537942051888 | Loss: 0.0056558443818511 | Epoch: 130 | MeanAbsoluteError: 0.0507780648767948 | Loss: 0.0055017440892315 | Epoch: 131 | MeanAbsoluteError: 0.0546514652669430 | Loss: 0.0054505378219275 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.0607702508568764 | Loss: 0.0060054426675809 | Epoch: 133 | MeanAbsoluteError: 0.0558654926717281 | Loss: 0.0059519913767489 | Epoch: 134 | MeanAbsoluteError: 0.0539874844253063 | Loss: 0.0052682884025240 | Epoch: 135 | MeanAbsoluteError: 0.0650022178888321 | Loss: 0.0069788512344913 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.0536550283432007 | Loss: 0.0048969971493965 | Epoch: 137 | MeanAbsoluteError: 0.0535493902862072 | Loss: 0.0056299232398344 | Epoch: 138 | MeanAbsoluteError: 0.0489221960306168 | Loss: 0.0049820207626142 | Epoch: 139 | MeanAbsoluteError: 0.0430713295936584 | Loss: 0.0046571926937567 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.0469382852315903 | Loss: 0.0041525583824535 | Epoch: 141 | MeanAbsoluteError: 0.0477350726723671 | Loss: 0.0042928338470550 | Epoch: 142 | MeanAbsoluteError: 0.0543576739728451 | Loss: 0.0057710894417189 | Epoch: 143 | MeanAbsoluteError: 0.0606821998953819 | Loss: 0.0063249394741816 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.0427135005593300 | Loss: 0.0039767120814711 | Epoch: 145 | MeanAbsoluteError: 0.0494462437927723 | Loss: 0.0044808400944978 | Epoch: 146 | MeanAbsoluteError: 0.0452370420098305 | Loss: 0.0036158441186049 | Epoch: 147 | MeanAbsoluteError: 0.0456318557262421 | Loss: 0.0043448218891985 | Epoch: 148 | \n\n\nMeanAbsoluteError: 0.0483167916536331 | Loss: 0.0052216279835097 | Epoch: 149 | MeanAbsoluteError: 0.0597462281584740 | Loss: 0.0069507358467059 | Epoch: 150 | MeanAbsoluteError: 0.0590606108307838 | Loss: 0.0066568704450650 | Epoch: 151 | MeanAbsoluteError: 0.0485299378633499 | Loss: 0.0055955018109863 | Epoch: 152 | \n\n\nMeanAbsoluteError: 0.0520428679883480 | Loss: 0.0054339627692427 | Epoch: 153 | MeanAbsoluteError: 0.0463419258594513 | Loss: 0.0045223792641130 | Epoch: 154 | MeanAbsoluteError: 0.0523883849382401 | Loss: 0.0053032617602424 | Epoch: 155 | MeanAbsoluteError: 0.0476644486188889 | Loss: 0.0052935515904162 | Epoch: 156 | \n\n\nMeanAbsoluteError: 0.0453763268887997 | Loss: 0.0044404032462473 | Epoch: 157 | MeanAbsoluteError: 0.0447034835815430 | Loss: 0.0037191047250465 | Epoch: 158 | MeanAbsoluteError: 0.0471230037510395 | Loss: 0.0047506307401866 | Epoch: 159 | MeanAbsoluteError: 0.0483901165425777 | Loss: 0.0053879846236669 | Epoch: 160 | \n\n\nMeanAbsoluteError: 0.0479925163090229 | Loss: 0.0056050146966682 | Epoch: 161 | MeanAbsoluteError: 0.0448642261326313 | Loss: 0.0044390837628223 | Epoch: 162 | MeanAbsoluteError: 0.0538970753550529 | Loss: 0.0048866522541319 | Epoch: 163 | MeanAbsoluteError: 0.0527297332882881 | Loss: 0.0061316138690975 | Epoch: 164 | \n\n\nMeanAbsoluteError: 0.0481771081686020 | Loss: 0.0047785721667184 | Epoch: 165 | MeanAbsoluteError: 0.0525175333023071 | Loss: 0.0052247868887590 | Epoch: 166 | MeanAbsoluteError: 0.0523976311087608 | Loss: 0.0055565093521421 | Epoch: 167 | MeanAbsoluteError: 0.0616324730217457 | Loss: 0.0069241013169583 | Epoch: 168 | \n\n\nMeanAbsoluteError: 0.0592200532555580 | Loss: 0.0065482112389107 | Epoch: 169 | MeanAbsoluteError: 0.0487206429243088 | Loss: 0.0046945939952581 | Epoch: 170 | MeanAbsoluteError: 0.0566787794232368 | Loss: 0.0062957583577372 | Epoch: 171 | MeanAbsoluteError: 0.0569334737956524 | Loss: 0.0062679202191679 | Epoch: 172 | \n\n\nMeanAbsoluteError: 0.0477848798036575 | Loss: 0.0057289662731101 | Epoch: 173 | MeanAbsoluteError: 0.0613473057746887 | Loss: 0.0075936166379650 | Epoch: 174 | MeanAbsoluteError: 0.0499169789254665 | Loss: 0.0048451106964709 | Epoch: 175 | MeanAbsoluteError: 0.0551849380135536 | Loss: 0.0055272122508062 | Epoch: 176 | \n\n\nMeanAbsoluteError: 0.0461988113820553 | Loss: 0.0045190446406578 | Epoch: 177 | MeanAbsoluteError: 0.0586024671792984 | Loss: 0.0060940641192964 | Epoch: 178 | MeanAbsoluteError: 0.0450408756732941 | Loss: 0.0044018112561102 | Epoch: 179 | MeanAbsoluteError: 0.0529470853507519 | Loss: 0.0054178257552466 | Epoch: 180 | \n\n\nMeanAbsoluteError: 0.0452706180512905 | Loss: 0.0041042745211407 | Epoch: 181 | MeanAbsoluteError: 0.0577796921133995 | Loss: 0.0059670083022578 | Epoch: 182 | MeanAbsoluteError: 0.0578277260065079 | Loss: 0.0060725318372103 | Epoch: 183 | MeanAbsoluteError: 0.0494318045675755 | Loss: 0.0050907516637572 | Epoch: 184 | \n\n\nMeanAbsoluteError: 0.0487061962485313 | Loss: 0.0049642747556055 | Epoch: 185 | MeanAbsoluteError: 0.0510985516011715 | Loss: 0.0049591213372830 | Epoch: 186 | MeanAbsoluteError: 0.0461977124214172 | Loss: 0.0051538410514599 | Epoch: 187 | MeanAbsoluteError: 0.0551898814737797 | Loss: 0.0054252109900852 | Epoch: 188 | \n\n\nMeanAbsoluteError: 0.0558465942740440 | Loss: 0.0060526994853526 | Epoch: 189 | MeanAbsoluteError: 0.0536580346524715 | Loss: 0.0057559458486809 | Epoch: 190 | MeanAbsoluteError: 0.0551383607089520 | Loss: 0.0057680543165924 | Epoch: 191 | MeanAbsoluteError: 0.0489641875028610 | Loss: 0.0052112467666647 | Epoch: 192 | \n\n\nMeanAbsoluteError: 0.0474665500223637 | Loss: 0.0056002891421820 | Epoch: 193 | MeanAbsoluteError: 0.0574605166912079 | Loss: 0.0066853589929738 | Epoch: 194 | MeanAbsoluteError: 0.0557099841535091 | Loss: 0.0061846214873520 | Epoch: 195 | MeanAbsoluteError: 0.0557685643434525 | Loss: 0.0060732990294989 | Epoch: 196 | \n\n\nMeanAbsoluteError: 0.0540490336716175 | Loss: 0.0063105675140641 | Epoch: 197 | MeanAbsoluteError: 0.0461926981806755 | Loss: 0.0046726744101187 | Epoch: 198 | MeanAbsoluteError: 0.0546614266932011 | Loss: 0.0054784145874069 | Epoch: 199 | MeanAbsoluteError: 0.0501543283462524 | Loss: 0.0051057703462184 | Epoch: 200 | \n\n\nMeanAbsoluteError: 0.0566999167203903 | Loss: 0.0062276882021443 | Epoch: 201 | MeanAbsoluteError: 0.0473076142370701 | Loss: 0.0050469956507808 | Epoch: 202 | MeanAbsoluteError: 0.0602193512022495 | Loss: 0.0068808807822949 | Epoch: 203 | MeanAbsoluteError: 0.0543025583028793 | Loss: 0.0058838093284162 | Epoch: 204 | \n\n\nMeanAbsoluteError: 0.0520493090152740 | Loss: 0.0054484932931557 | Epoch: 205 | MeanAbsoluteError: 0.0606290325522423 | Loss: 0.0080512465241267 | Epoch: 206 | MeanAbsoluteError: 0.0482079647481441 | Loss: 0.0048516261066604 | Epoch: 207 | MeanAbsoluteError: 0.0549888536334038 | Loss: 0.0054076505751398 | Epoch: 208 | \n\n\nMeanAbsoluteError: 0.0519401654601097 | Loss: 0.0055202552956823 | Epoch: 209 | MeanAbsoluteError: 0.0533474460244179 | Loss: 0.0052288246117410 | Epoch: 210 | MeanAbsoluteError: 0.0492738485336304 | Loss: 0.0047098303299495 | Early stopping at epoch 209\nReturned to Spot: Validation loss: 0.004709830329949527\n\n\nIf path is set to a filename, e.g., path = \"model_spot_trained.pt\", the weights of the trained model will be loaded from this file.\n\ntest_tuned(net=model_spot, test_dataset=test,\n            shuffle=False,\n            loss_function=fun_control[\"loss_function\"],\n            metric=fun_control[\"metric_torch\"],\n            device = fun_control[\"device\"],\n            task=fun_control[\"task\"],)\n\nMeanAbsoluteError: 0.0531265325844288 | Loss: 0.0060760189371649 | Final evaluation: Validation loss: 0.006076018937164918\nFinal evaluation: Validation metric: 0.05312653258442879\n----------------------------------------------\n\n\n(0.006076018937164918, nan, tensor(0.0531))\n\n\n\n\n19.10.3 Cross-validated Evaluations\n\nThis is the evaluation that will be used in the comparison (evaluatecv has to be updated before, to get metric vlaues!):\n\n\nfrom spotPython.torch.traintest import evaluate_cv\n# modify k-kolds:\nsetattr(model_spot, \"k_folds\",  10)\ndf_eval, df_preds, df_metrics = evaluate_cv(net=model_spot,\n            dataset=fun_control[\"data\"],\n            loss_function=fun_control[\"loss_function\"],\n            metric=fun_control[\"metric_torch\"],\n            task=fun_control[\"task\"],\n            writer=fun_control[\"writer\"],\n            writerId=\"model_spot_cv\",\n            device = fun_control[\"device\"])\n\nFold: 1\nEpoch: 1 | \n\n\nMeanAbsoluteError: 0.1414459794759750 | Loss: 0.0343159539445948 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1174980252981186 | Loss: 0.0213862570145955 | Epoch: 3 | \n\n\nMeanAbsoluteError: 0.1134593784809113 | Loss: 0.0185263883322477 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1088417917490005 | Loss: 0.0171441211221883 | Epoch: 5 | \n\n\nMeanAbsoluteError: 0.0819596946239471 | Loss: 0.0111374479467766 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.0827154070138931 | Loss: 0.0111573870795277 | Epoch: 7 | \n\n\nMeanAbsoluteError: 0.0803340375423431 | Loss: 0.0107760501022522 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.0606685541570187 | Loss: 0.0062929676272548 | Epoch: 9 | \n\n\nMeanAbsoluteError: 0.0738035365939140 | Loss: 0.0089774020105744 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.0579815208911896 | Loss: 0.0058487777311641 | Epoch: 11 | \n\n\nMeanAbsoluteError: 0.0639534965157509 | Loss: 0.0075883112417964 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0510539449751377 | Loss: 0.0043777781717766 | Epoch: 13 | \n\n\nMeanAbsoluteError: 0.0617730580270290 | Loss: 0.0072024756899247 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.0561680570244789 | Loss: 0.0056264951753502 | Epoch: 15 | \n\n\nMeanAbsoluteError: 0.0515886098146439 | Loss: 0.0045624360880958 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0607611648738384 | Loss: 0.0059379524766253 | Epoch: 17 | \n\n\nMeanAbsoluteError: 0.0603702627122402 | Loss: 0.0057825706093214 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.0526198185980320 | Loss: 0.0046221539736367 | Epoch: 19 | \n\n\nMeanAbsoluteError: 0.0480859912931919 | Loss: 0.0042050249534301 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0525608584284782 | Loss: 0.0044762180545009 | Epoch: 21 | \n\n\nMeanAbsoluteError: 0.0452177338302135 | Loss: 0.0040025304770097 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.0584472641348839 | Loss: 0.0050833130058331 | Epoch: 23 | \n\n\nMeanAbsoluteError: 0.0397060327231884 | Loss: 0.0029526501714897 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0506323426961899 | Loss: 0.0042771412543236 | Epoch: 25 | \n\n\nMeanAbsoluteError: 0.0520481877028942 | Loss: 0.0048374398790586 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.0629656463861465 | Loss: 0.0064647101904624 | Epoch: 27 | \n\n\nMeanAbsoluteError: 0.0506992787122726 | Loss: 0.0051372679411613 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0548158697783947 | Loss: 0.0049737666447002 | Epoch: 29 | \n\n\nMeanAbsoluteError: 0.0415629632771015 | Loss: 0.0036219620557788 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.0466059483587742 | Loss: 0.0041548862086179 | Epoch: 31 | \n\n\nMeanAbsoluteError: 0.0423435792326927 | Loss: 0.0032504037595712 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0444596484303474 | Loss: 0.0032252942596992 | Epoch: 33 | \n\n\nMeanAbsoluteError: 0.0427757725119591 | Loss: 0.0030582434712694 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.0495980009436607 | Loss: 0.0044539631792129 | Epoch: 35 | \n\n\nMeanAbsoluteError: 0.0495584122836590 | Loss: 0.0048267537226471 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0463276244699955 | Loss: 0.0041339638499686 | Epoch: 37 | \n\n\nMeanAbsoluteError: 0.0519955195486546 | Loss: 0.0042136315560828 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.0383013933897018 | Loss: 0.0028521726713874 | Epoch: 39 | \n\n\nMeanAbsoluteError: 0.0447151996195316 | Loss: 0.0032913151931447 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0558878928422928 | Loss: 0.0050429218950180 | Epoch: 41 | \n\n\nMeanAbsoluteError: 0.0459970459342003 | Loss: 0.0043259898218541 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.0380936823785305 | Loss: 0.0025572267611726 | Epoch: 43 | \n\n\nMeanAbsoluteError: 0.0630036517977715 | Loss: 0.0062541522336407 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0471007190644741 | Loss: 0.0044827803951473 | Epoch: 45 | \n\n\nMeanAbsoluteError: 0.0413703918457031 | Loss: 0.0037594548283288 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.0453184731304646 | Loss: 0.0041830385050092 | Epoch: 47 | \n\n\nMeanAbsoluteError: 0.0489127635955811 | Loss: 0.0044013114126686 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0490909926593304 | Loss: 0.0036841656636590 | Epoch: 49 | \n\n\nMeanAbsoluteError: 0.0532665960490704 | Loss: 0.0051290580617765 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.0466977134346962 | Loss: 0.0044637448691691 | Epoch: 51 | \n\n\nMeanAbsoluteError: 0.0435238033533096 | Loss: 0.0033079311788942 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0389570929110050 | Loss: 0.0030524581849862 | Epoch: 53 | \n\n\nMeanAbsoluteError: 0.0485423654317856 | Loss: 0.0039717235530798 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0466747507452965 | Loss: 0.0060992299650724 | Epoch: 55 | \n\n\nMeanAbsoluteError: 0.0424691289663315 | Loss: 0.0030187284753013 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0552474446594715 | Loss: 0.0048328063567169 | Epoch: 57 | \n\n\nMeanAbsoluteError: 0.0740467980504036 | Loss: 0.0084938488255900 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.0465559959411621 | Loss: 0.0037135265122813 | Epoch: 59 | \n\n\nMeanAbsoluteError: 0.0451292954385281 | Loss: 0.0039707075607461 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0456060990691185 | Loss: 0.0038158484340574 | Epoch: 61 | \n\n\nMeanAbsoluteError: 0.0407601930201054 | Loss: 0.0031428031283073 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.0524439588189125 | Loss: 0.0039543206355749 | Epoch: 63 | \n\n\nMeanAbsoluteError: 0.0736591443419456 | Loss: 0.0081044684450787 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0480665601789951 | Loss: 0.0044348191451998 | Epoch: 65 | \n\n\nMeanAbsoluteError: 0.0359581224620342 | Loss: 0.0022479488577049 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.0440688878297806 | Loss: 0.0031657908067036 | Epoch: 67 | \n\n\nMeanAbsoluteError: 0.0362556278705597 | Loss: 0.0025871890404512 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0370789915323257 | Loss: 0.0026082525728270 | Epoch: 69 | \n\n\nMeanAbsoluteError: 0.0576655119657516 | Loss: 0.0062952100191838 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.0589235872030258 | Loss: 0.0064265239410675 | Epoch: 71 | \n\n\nMeanAbsoluteError: 0.0547582432627678 | Loss: 0.0049889752945791 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0401506051421165 | Loss: 0.0045713396975771 | Epoch: 73 | \n\n\nMeanAbsoluteError: 0.0505148060619831 | Loss: 0.0046664822058609 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.0468519814312458 | Loss: 0.0043577287713281 | Epoch: 75 | \n\n\nMeanAbsoluteError: 0.0422547161579132 | Loss: 0.0032796861397891 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0394993573427200 | Loss: 0.0040135237280853 | Epoch: 77 | \n\n\nMeanAbsoluteError: 0.0367196314036846 | Loss: 0.0022696747955006 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.0372427441179752 | Loss: 0.0026127084230001 | Epoch: 79 | \n\n\nMeanAbsoluteError: 0.0445588342845440 | Loss: 0.0037776491774891 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0452931448817253 | Loss: 0.0041482351505413 | Epoch: 81 | \n\n\nMeanAbsoluteError: 0.0360108725726604 | Loss: 0.0022816292699785 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0371038950979710 | Loss: 0.0029401152457397 | Epoch: 83 | \n\n\nMeanAbsoluteError: 0.0435616672039032 | Loss: 0.0040739925417046 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0486861765384674 | Loss: 0.0037722617674332 | Epoch: 85 | \n\n\nMeanAbsoluteError: 0.0394847095012665 | Loss: 0.0025569898649477 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.0491144657135010 | Loss: 0.0047199115192947 | Epoch: 87 | \n\n\nMeanAbsoluteError: 0.0406341888010502 | Loss: 0.0031010487604922 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0517573319375515 | Loss: 0.0041876722288390 | Epoch: 89 | \n\n\nMeanAbsoluteError: 0.0380269400775433 | Loss: 0.0026425424752793 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.0512255392968655 | Loss: 0.0046716816186045 | Epoch: 91 | \n\n\nMeanAbsoluteError: 0.0454722531139851 | Loss: 0.0039347942533473 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0383610092103481 | Loss: 0.0024193648792929 | Epoch: 93 | \n\n\nMeanAbsoluteError: 0.0440395250916481 | Loss: 0.0072222095108233 | Epoch: 94 | \n\n\nMeanAbsoluteError: 0.0386998131871223 | Loss: 0.0029340055771172 | Epoch: 95 | \n\n\nMeanAbsoluteError: 0.0419969856739044 | Loss: 0.0029570712470629 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0520037449896336 | Loss: 0.0068653584726585 | Epoch: 97 | \n\n\nMeanAbsoluteError: 0.0456604324281216 | Loss: 0.0044470995198935 | Epoch: 98 | \n\n\nMeanAbsoluteError: 0.0499907433986664 | Loss: 0.0037328437454282 | Epoch: 99 | \n\n\nMeanAbsoluteError: 0.0383889228105545 | Loss: 0.0032272482819210 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0456809960305691 | Loss: 0.0033910385189721 | Epoch: 101 | \n\n\nMeanAbsoluteError: 0.0433116257190704 | Loss: 0.0034254762666443 | Epoch: 102 | \n\n\nMeanAbsoluteError: 0.0426959954202175 | Loss: 0.0040048450956909 | Epoch: 103 | \n\n\nMeanAbsoluteError: 0.0495121777057648 | Loss: 0.0052856160685993 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0498760491609573 | Loss: 0.0048163797563085 | Epoch: 105 | \n\n\nMeanAbsoluteError: 0.0462423115968704 | Loss: 0.0044333080743225 | Epoch: 106 | \n\n\nMeanAbsoluteError: 0.0488385334610939 | Loss: 0.0053751401585312 | Epoch: 107 | \n\n\nMeanAbsoluteError: 0.0793270170688629 | Loss: 0.0103572852945385 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0453376770019531 | Loss: 0.0035733615379566 | Epoch: 109 | \n\n\nMeanAbsoluteError: 0.0418521389365196 | Loss: 0.0038670879797652 | Epoch: 110 | \n\n\nMeanAbsoluteError: 0.0455928593873978 | Loss: 0.0043073426932096 | Epoch: 111 | \n\n\nMeanAbsoluteError: 0.0460077524185181 | Loss: 0.0038052351267722 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0499333590269089 | Loss: 0.0051794557635171 | Epoch: 113 | \n\n\nMeanAbsoluteError: 0.0400393307209015 | Loss: 0.0028843451649524 | Epoch: 114 | \n\n\nMeanAbsoluteError: 0.0389871411025524 | Loss: 0.0024107403408449 | Epoch: 115 | \n\n\nMeanAbsoluteError: 0.0412452109158039 | Loss: 0.0027365403590151 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0395373776555061 | Loss: 0.0028374254918442 | Epoch: 117 | \n\n\nMeanAbsoluteError: 0.0421992689371109 | Loss: 0.0031628722521978 | Epoch: 118 | \n\n\nMeanAbsoluteError: 0.0424080453813076 | Loss: 0.0037683719464649 | Epoch: 119 | \n\n\nMeanAbsoluteError: 0.0396137274801731 | Loss: 0.0029371986568619 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0410588271915913 | Loss: 0.0030616287974856 | Epoch: 121 | \n\n\nMeanAbsoluteError: 0.0498473010957241 | Loss: 0.0039837769716262 | Epoch: 122 | \n\n\nMeanAbsoluteError: 0.0441325455904007 | Loss: 0.0041976726699012 | Epoch: 123 | \n\n\nMeanAbsoluteError: 0.0391433499753475 | Loss: 0.0032168456281607 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0416422858834267 | Loss: 0.0030849234004003 | Epoch: 125 | \n\n\nMeanAbsoluteError: 0.0427857302129269 | Loss: 0.0050014355757202 | Epoch: 126 | \n\n\nMeanAbsoluteError: 0.0440392643213272 | Loss: 0.0035048165860084 | Epoch: 127 | \n\n\nMeanAbsoluteError: 0.0445310696959496 | Loss: 0.0039271182971648 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0425569005310535 | Loss: 0.0027420160163624 | Epoch: 129 | \n\n\nMeanAbsoluteError: 0.0477427020668983 | Loss: 0.0055573631901867 | Early stopping at epoch 128\nFold: 2\nEpoch: 1 | \n\n\nMeanAbsoluteError: 0.1332310289144516 | Loss: 0.0278673535929276 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1139545515179634 | Loss: 0.0199073676306468 | Epoch: 3 | \n\n\nMeanAbsoluteError: 0.0964583009481430 | Loss: 0.0171973413716142 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.0983900576829910 | Loss: 0.0189792334030454 | Epoch: 5 | \n\n\nMeanAbsoluteError: 0.0862642824649811 | Loss: 0.0134047811421064 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.0867632180452347 | Loss: 0.0125144871954735 | Epoch: 7 | \n\n\nMeanAbsoluteError: 0.0776880159974098 | Loss: 0.0118246583554607 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.0724654495716095 | Loss: 0.0095800613053143 | Epoch: 9 | \n\n\nMeanAbsoluteError: 0.0690836012363434 | Loss: 0.0087936605022360 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.0686206817626953 | Loss: 0.0090425435561114 | Epoch: 11 | \n\n\nMeanAbsoluteError: 0.0870825350284576 | Loss: 0.0127476962516084 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0723229572176933 | Loss: 0.0094726675261672 | Epoch: 13 | \n\n\nMeanAbsoluteError: 0.0713137611746788 | Loss: 0.0086960808660548 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.0846883133053780 | Loss: 0.0117944101982105 | Epoch: 15 | \n\n\nMeanAbsoluteError: 0.0669880062341690 | Loss: 0.0073239228670270 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0556209124624729 | Loss: 0.0067309947111286 | Epoch: 17 | \n\n\nMeanAbsoluteError: 0.0739338770508766 | Loss: 0.0084775480298469 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.0668265521526337 | Loss: 0.0092704670969397 | Epoch: 19 | \n\n\nMeanAbsoluteError: 0.0596642866730690 | Loss: 0.0071237383661075 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0698312297463417 | Loss: 0.0086232012388511 | Epoch: 21 | \n\n\nMeanAbsoluteError: 0.0638480931520462 | Loss: 0.0071179475635290 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.0512224733829498 | Loss: 0.0062378321487743 | Epoch: 23 | \n\n\nMeanAbsoluteError: 0.0596751943230629 | Loss: 0.0078682555781248 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0568793676793575 | Loss: 0.0090369559519996 | Epoch: 25 | \n\n\nMeanAbsoluteError: 0.0704581514000893 | Loss: 0.0096770946581203 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.0550059825181961 | Loss: 0.0074069899990438 | Epoch: 27 | \n\n\nMeanAbsoluteError: 0.0631615146994591 | Loss: 0.0072373607097409 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0536009445786476 | Loss: 0.0055860902469319 | Epoch: 29 | \n\n\nMeanAbsoluteError: 0.0578113272786140 | Loss: 0.0070646650408610 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.0662948191165924 | Loss: 0.0091564517486124 | Epoch: 31 | \n\n\nMeanAbsoluteError: 0.0538021773099899 | Loss: 0.0068048521464404 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0670781135559082 | Loss: 0.0121964773234840 | Epoch: 33 | \n\n\nMeanAbsoluteError: 0.0643002390861511 | Loss: 0.0078195624877341 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.0653563290834427 | Loss: 0.0081525595966153 | Epoch: 35 | \n\n\nMeanAbsoluteError: 0.0599883794784546 | Loss: 0.0075962405042866 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0616889670491219 | Loss: 0.0078826848095140 | Epoch: 37 | \n\n\nMeanAbsoluteError: 0.0591864213347435 | Loss: 0.0069125594320492 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.0698462724685669 | Loss: 0.0093496728133267 | Epoch: 39 | \n\n\nMeanAbsoluteError: 0.0633565932512283 | Loss: 0.0078419355626325 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0532219298183918 | Loss: 0.0064006889140448 | Epoch: 41 | \n\n\nMeanAbsoluteError: 0.0630075633525848 | Loss: 0.0077368332288013 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.0642099604010582 | Loss: 0.0087771067628637 | Epoch: 43 | \n\n\nMeanAbsoluteError: 0.0541371963918209 | Loss: 0.0061019367287652 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0627305805683136 | Loss: 0.0074738652552836 | Epoch: 45 | \n\n\nMeanAbsoluteError: 0.0507902614772320 | Loss: 0.0061526339447412 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.0622054561972618 | Loss: 0.0083533932651895 | Epoch: 47 | \n\n\nMeanAbsoluteError: 0.0621203891932964 | Loss: 0.0084077218242993 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0560700260102749 | Loss: 0.0064565229516190 | Epoch: 49 | \n\n\nMeanAbsoluteError: 0.0621315352618694 | Loss: 0.0089949927328584 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.0532745234668255 | Loss: 0.0068013738804998 | Epoch: 51 | \n\n\nMeanAbsoluteError: 0.0600913986563683 | Loss: 0.0064013727331677 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0578413754701614 | Loss: 0.0081545118905174 | Epoch: 53 | \n\n\nMeanAbsoluteError: 0.0513724088668823 | Loss: 0.0072334785318862 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0557351745665073 | Loss: 0.0054930627632600 | Epoch: 55 | \n\n\nMeanAbsoluteError: 0.0606428049504757 | Loss: 0.0066883452953054 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0622951127588749 | Loss: 0.0079536779532925 | Epoch: 57 | \n\n\nMeanAbsoluteError: 0.0677779614925385 | Loss: 0.0080241210973607 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.0543663464486599 | Loss: 0.0072242792588300 | Epoch: 59 | \n\n\nMeanAbsoluteError: 0.0521804951131344 | Loss: 0.0056622631292647 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0559543073177338 | Loss: 0.0068288203597499 | Epoch: 61 | \n\n\nMeanAbsoluteError: 0.0554745160043240 | Loss: 0.0070009645289527 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.0494281873106956 | Loss: 0.0047930295203024 | Epoch: 63 | \n\n\nMeanAbsoluteError: 0.0692287981510162 | Loss: 0.0094673152959261 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0642237067222595 | Loss: 0.0069282833182325 | Epoch: 65 | \n\n\nMeanAbsoluteError: 0.0590957365930080 | Loss: 0.0075612385452797 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.0605021007359028 | Loss: 0.0069154529796483 | Epoch: 67 | \n\n\nMeanAbsoluteError: 0.0643986240029335 | Loss: 0.0074046533107041 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0483250804245472 | Loss: 0.0049306459259242 | Epoch: 69 | \n\n\nMeanAbsoluteError: 0.0578743107616901 | Loss: 0.0069614783323442 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.0594417415559292 | Loss: 0.0066104625897984 | Epoch: 71 | \n\n\nMeanAbsoluteError: 0.0742496028542519 | Loss: 0.0098562382089977 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0515954196453094 | Loss: 0.0058658482023300 | Epoch: 73 | \n\n\nMeanAbsoluteError: 0.0602756626904011 | Loss: 0.0068906646651717 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.0519215352833271 | Loss: 0.0055764707727716 | Epoch: 75 | \n\n\nMeanAbsoluteError: 0.0620294250547886 | Loss: 0.0089137953204604 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0790545344352722 | Loss: 0.0098972147187361 | Epoch: 77 | \n\n\nMeanAbsoluteError: 0.0572292171418667 | Loss: 0.0057155339387604 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.0582163855433464 | Loss: 0.0065904146311088 | Epoch: 79 | \n\n\nMeanAbsoluteError: 0.0587846003472805 | Loss: 0.0074373682728037 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0650518909096718 | Loss: 0.0067503118457702 | Epoch: 81 | \n\n\nMeanAbsoluteError: 0.0612091161310673 | Loss: 0.0079734579283887 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0568082481622696 | Loss: 0.0068873257483714 | Epoch: 83 | \n\n\nMeanAbsoluteError: 0.0566450171172619 | Loss: 0.0064917965254818 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0605608336627483 | Loss: 0.0076881683377836 | Epoch: 85 | \n\n\nMeanAbsoluteError: 0.0487345084547997 | Loss: 0.0061025918447950 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.0493685379624367 | Loss: 0.0053053805022500 | Epoch: 87 | \n\n\nMeanAbsoluteError: 0.0526784546673298 | Loss: 0.0048954793914723 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0513558015227318 | Loss: 0.0063035636423872 | Epoch: 89 | \n\n\nMeanAbsoluteError: 0.0534873530268669 | Loss: 0.0058381399945714 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.0564029179513454 | Loss: 0.0084887304379103 | Epoch: 91 | \n\n\nMeanAbsoluteError: 0.0504155308008194 | Loss: 0.0054169748282920 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0605378672480583 | Loss: 0.0074832273575549 | Epoch: 93 | \n\n\nMeanAbsoluteError: 0.0633422434329987 | Loss: 0.0069398865497743 | Epoch: 94 | \n\n\nMeanAbsoluteError: 0.0521688945591450 | Loss: 0.0054575483081862 | Epoch: 95 | \n\n\nMeanAbsoluteError: 0.0600866712629795 | Loss: 0.0064447607594327 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0546328350901604 | Loss: 0.0049674416435524 | Epoch: 97 | \n\n\nMeanAbsoluteError: 0.0539725795388222 | Loss: 0.0057810210945228 | Epoch: 98 | \n\n\nMeanAbsoluteError: 0.0537769943475723 | Loss: 0.0059502974371963 | Epoch: 99 | \n\n\nMeanAbsoluteError: 0.0577385239303112 | Loss: 0.0071608074134789 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0551474839448929 | Loss: 0.0056130377745901 | Epoch: 101 | \n\n\nMeanAbsoluteError: 0.0509676784276962 | Loss: 0.0052143881061616 | Epoch: 102 | \n\n\nMeanAbsoluteError: 0.0496866852045059 | Loss: 0.0057555721857800 | Epoch: 103 | \n\n\nMeanAbsoluteError: 0.0561871007084846 | Loss: 0.0056136877795395 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0586884021759033 | Loss: 0.0062427973025478 | Epoch: 105 | \n\n\nMeanAbsoluteError: 0.0604123435914516 | Loss: 0.0093695795826184 | Epoch: 106 | \n\n\nMeanAbsoluteError: 0.0551675930619240 | Loss: 0.0080278294134097 | Epoch: 107 | \n\n\nMeanAbsoluteError: 0.0545764788985252 | Loss: 0.0054643327961318 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0478693544864655 | Loss: 0.0057878981001872 | Epoch: 109 | \n\n\nMeanAbsoluteError: 0.0613265559077263 | Loss: 0.0072709781637129 | Epoch: 110 | \n\n\nMeanAbsoluteError: 0.0452532656490803 | Loss: 0.0044721059250430 | Epoch: 111 | \n\n\nMeanAbsoluteError: 0.0591460131108761 | Loss: 0.0073826856409701 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0546461790800095 | Loss: 0.0056275592651218 | Epoch: 113 | \n\n\nMeanAbsoluteError: 0.0533169396221638 | Loss: 0.0058856490963640 | Epoch: 114 | \n\n\nMeanAbsoluteError: 0.0501347072422504 | Loss: 0.0049963545304938 | Epoch: 115 | \n\n\nMeanAbsoluteError: 0.0657620131969452 | Loss: 0.0084207726199216 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0657791122794151 | Loss: 0.0094020414488533 | Epoch: 117 | \n\n\nMeanAbsoluteError: 0.0522600561380386 | Loss: 0.0064158427457397 | Epoch: 118 | \n\n\nMeanAbsoluteError: 0.0531551837921143 | Loss: 0.0062114741706934 | Epoch: 119 | \n\n\nMeanAbsoluteError: 0.0587166659533978 | Loss: 0.0079517466589235 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0459537208080292 | Loss: 0.0062225439648431 | Epoch: 121 | \n\n\nMeanAbsoluteError: 0.0559995993971825 | Loss: 0.0062648138711945 | Epoch: 122 | \n\n\nMeanAbsoluteError: 0.0516127049922943 | Loss: 0.0055273410398513 | Epoch: 123 | \n\n\nMeanAbsoluteError: 0.0691503733396530 | Loss: 0.0094828760394683 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0443283170461655 | Loss: 0.0048112267988752 | Epoch: 125 | \n\n\nMeanAbsoluteError: 0.0491857342422009 | Loss: 0.0053619989462627 | Epoch: 126 | \n\n\nMeanAbsoluteError: 0.0493065938353539 | Loss: 0.0055411853424560 | Epoch: 127 | \n\n\nMeanAbsoluteError: 0.0490184873342514 | Loss: 0.0049483513710304 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0541563518345356 | Loss: 0.0058877065043467 | Epoch: 129 | \n\n\nMeanAbsoluteError: 0.0575437322258949 | Loss: 0.0059994983200270 | Epoch: 130 | \n\n\nMeanAbsoluteError: 0.0577137693762779 | Loss: 0.0064103277489686 | Epoch: 131 | \n\n\nMeanAbsoluteError: 0.0557388626039028 | Loss: 0.0059761048043863 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.0538627468049526 | Loss: 0.0060142394459735 | Epoch: 133 | \n\n\nMeanAbsoluteError: 0.0624377168715000 | Loss: 0.0064727325135699 | Epoch: 134 | \n\n\nMeanAbsoluteError: 0.0496986731886864 | Loss: 0.0047350695061211 | Epoch: 135 | \n\n\nMeanAbsoluteError: 0.0431477352976799 | Loss: 0.0037801152435490 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.0479090213775635 | Loss: 0.0048570081215495 | Epoch: 137 | \n\n\nMeanAbsoluteError: 0.0573737770318985 | Loss: 0.0066619076936219 | Epoch: 138 | \n\n\nMeanAbsoluteError: 0.0551331676542759 | Loss: 0.0075660067288062 | Epoch: 139 | \n\n\nMeanAbsoluteError: 0.0486815869808197 | Loss: 0.0049104317485426 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.0552750006318092 | Loss: 0.0059511438728525 | Epoch: 141 | \n\n\nMeanAbsoluteError: 0.0493481867015362 | Loss: 0.0053031411169706 | Epoch: 142 | \n\n\nMeanAbsoluteError: 0.0649333596229553 | Loss: 0.0082625127397478 | Epoch: 143 | \n\n\nMeanAbsoluteError: 0.0573728196322918 | Loss: 0.0058022429808401 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.0528466366231441 | Loss: 0.0076229240602026 | Epoch: 145 | \n\n\nMeanAbsoluteError: 0.0563116557896137 | Loss: 0.0060882373318936 | Epoch: 146 | \n\n\nMeanAbsoluteError: 0.0438472777605057 | Loss: 0.0053212960841707 | Epoch: 147 | \n\n\nMeanAbsoluteError: 0.0512560904026031 | Loss: 0.0060893848735409 | Epoch: 148 | \n\n\nMeanAbsoluteError: 0.0491110049188137 | Loss: 0.0050262031279719 | Epoch: 149 | \n\n\nMeanAbsoluteError: 0.0483608208596706 | Loss: 0.0047982152005156 | Epoch: 150 | \n\n\nMeanAbsoluteError: 0.0571874603629112 | Loss: 0.0069665574343302 | Epoch: 151 | \n\n\nMeanAbsoluteError: 0.0571475401520729 | Loss: 0.0070579408381421 | Epoch: 152 | \n\n\nMeanAbsoluteError: 0.0593664348125458 | Loss: 0.0079321221520121 | Epoch: 153 | \n\n\nMeanAbsoluteError: 0.0620533749461174 | Loss: 0.0076904481885811 | Epoch: 154 | \n\n\nMeanAbsoluteError: 0.0494084246456623 | Loss: 0.0050670842084890 | Epoch: 155 | \n\n\nMeanAbsoluteError: 0.0769236311316490 | Loss: 0.0090878022870479 | Epoch: 156 | \n\n\nMeanAbsoluteError: 0.0601508133113384 | Loss: 0.0063139814077518 | Epoch: 157 | \n\n\nMeanAbsoluteError: 0.0521630421280861 | Loss: 0.0054474690651449 | Epoch: 158 | \n\n\nMeanAbsoluteError: 0.0609308369457722 | Loss: 0.0062483497357999 | Epoch: 159 | \n\n\nMeanAbsoluteError: 0.0464910119771957 | Loss: 0.0052157959659011 | Epoch: 160 | \n\n\nMeanAbsoluteError: 0.0599785931408405 | Loss: 0.0076162024007107 | Epoch: 161 | \n\n\nMeanAbsoluteError: 0.0486163273453712 | Loss: 0.0051004710190822 | Epoch: 162 | \n\n\nMeanAbsoluteError: 0.0534530542790890 | Loss: 0.0059892022624039 | Epoch: 163 | \n\n\nMeanAbsoluteError: 0.0667490139603615 | Loss: 0.0078150029115092 | Epoch: 164 | \n\n\nMeanAbsoluteError: 0.0530772171914577 | Loss: 0.0057068816072737 | Epoch: 165 | \n\n\nMeanAbsoluteError: 0.0471036992967129 | Loss: 0.0047431577874634 | Epoch: 166 | \n\n\nMeanAbsoluteError: 0.0456185005605221 | Loss: 0.0054911572504072 | Epoch: 167 | \n\n\nMeanAbsoluteError: 0.0512059219181538 | Loss: 0.0051111432168489 | Epoch: 168 | \n\n\nMeanAbsoluteError: 0.0530637800693512 | Loss: 0.0071527658236356 | Epoch: 169 | \n\n\nMeanAbsoluteError: 0.0522609129548073 | Loss: 0.0060266656845880 | Epoch: 170 | \n\n\nMeanAbsoluteError: 0.0545457787811756 | Loss: 0.0063342518047788 | Epoch: 171 | \n\n\nMeanAbsoluteError: 0.0549884289503098 | Loss: 0.0053607562843424 | Epoch: 172 | \n\n\nMeanAbsoluteError: 0.0711482763290405 | Loss: 0.0117054490211348 | Epoch: 173 | \n\n\nMeanAbsoluteError: 0.0537208802998066 | Loss: 0.0062432967018909 | Epoch: 174 | \n\n\nMeanAbsoluteError: 0.0468603000044823 | Loss: 0.0055245803355669 | Epoch: 175 | \n\n\nMeanAbsoluteError: 0.0630777478218079 | Loss: 0.0070389825582074 | Epoch: 176 | \n\n\nMeanAbsoluteError: 0.0467945672571659 | Loss: 0.0049339733534278 | Epoch: 177 | \n\n\nMeanAbsoluteError: 0.0531113296747208 | Loss: 0.0060792795489900 | Epoch: 178 | \n\n\nMeanAbsoluteError: 0.0564550384879112 | Loss: 0.0069336915800634 | Epoch: 179 | \n\n\nMeanAbsoluteError: 0.0427604727447033 | Loss: 0.0039901644385491 | Epoch: 180 | \n\n\nMeanAbsoluteError: 0.0494440980255604 | Loss: 0.0059773804163202 | Epoch: 181 | \n\n\nMeanAbsoluteError: 0.0625592917203903 | Loss: 0.0075381045373013 | Epoch: 182 | \n\n\nMeanAbsoluteError: 0.0617341473698616 | Loss: 0.0087789397221059 | Epoch: 183 | \n\n\nMeanAbsoluteError: 0.0479502379894257 | Loss: 0.0047626945328935 | Epoch: 184 | \n\n\nMeanAbsoluteError: 0.0578187517821789 | Loss: 0.0059091058971647 | Epoch: 185 | \n\n\nMeanAbsoluteError: 0.0546604022383690 | Loss: 0.0062263323119483 | Epoch: 186 | \n\n\nMeanAbsoluteError: 0.0602780096232891 | Loss: 0.0082336872762356 | Epoch: 187 | \n\n\nMeanAbsoluteError: 0.0495447367429733 | Loss: 0.0054652120586700 | Epoch: 188 | \n\n\nMeanAbsoluteError: 0.0821062475442886 | Loss: 0.0111677493685140 | Epoch: 189 | \n\n\nMeanAbsoluteError: 0.0570247694849968 | Loss: 0.0062989638849663 | Epoch: 190 | \n\n\nMeanAbsoluteError: 0.0648201406002045 | Loss: 0.0091014491268792 | Epoch: 191 | \n\n\nMeanAbsoluteError: 0.0582264699041843 | Loss: 0.0065426738538708 | Epoch: 192 | \n\n\nMeanAbsoluteError: 0.0464373677968979 | Loss: 0.0042968954792461 | Epoch: 193 | \n\n\nMeanAbsoluteError: 0.0488625653088093 | Loss: 0.0051360325484823 | Epoch: 194 | \n\n\nMeanAbsoluteError: 0.0482565499842167 | Loss: 0.0055672827696141 | Epoch: 195 | \n\n\nMeanAbsoluteError: 0.0492969639599323 | Loss: 0.0067251466394877 | Epoch: 196 | \n\n\nMeanAbsoluteError: 0.0621665716171265 | Loss: 0.0095112709543453 | Epoch: 197 | \n\n\nMeanAbsoluteError: 0.0717512145638466 | Loss: 0.0088005644591668 | Epoch: 198 | \n\n\nMeanAbsoluteError: 0.0576187819242477 | Loss: 0.0075537315211617 | Epoch: 199 | \n\n\nMeanAbsoluteError: 0.0532177016139030 | Loss: 0.0058548893116844 | Early stopping at epoch 198\nFold: 3\nEpoch: 1 | \n\n\nMeanAbsoluteError: 0.1305504739284515 | Loss: 0.0255278668438013 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1227560937404633 | Loss: 0.0223353340362127 | Epoch: 3 | \n\n\nMeanAbsoluteError: 0.1094683259725571 | Loss: 0.0185870254555574 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1189861223101616 | Loss: 0.0211073203334728 | Epoch: 5 | \n\n\nMeanAbsoluteError: 0.0998469814658165 | Loss: 0.0154380003850047 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.0921492576599121 | Loss: 0.0128939767511418 | Epoch: 7 | \n\n\nMeanAbsoluteError: 0.0810423791408539 | Loss: 0.0107272092539531 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.0840980112552643 | Loss: 0.0111490273489975 | Epoch: 9 | \n\n\nMeanAbsoluteError: 0.0756841674447060 | Loss: 0.0101465506908985 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.0797782912850380 | Loss: 0.0109801957777773 | Epoch: 11 | \n\n\nMeanAbsoluteError: 0.0607731156051159 | Loss: 0.0067385339106505 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0650684535503387 | Loss: 0.0071976521912103 | Epoch: 13 | \n\n\nMeanAbsoluteError: 0.0647333860397339 | Loss: 0.0082996478483367 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.0605008937418461 | Loss: 0.0077389734215103 | Epoch: 15 | \n\n\nMeanAbsoluteError: 0.0741562247276306 | Loss: 0.0091133664648693 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0597483217716217 | Loss: 0.0062767153946110 | Epoch: 17 | \n\n\nMeanAbsoluteError: 0.0605355873703957 | Loss: 0.0058466915551645 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.0617148987948895 | Loss: 0.0067939327922291 | Epoch: 19 | \n\n\nMeanAbsoluteError: 0.0614890493452549 | Loss: 0.0076526814104559 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0644187256693840 | Loss: 0.0078561210180991 | Epoch: 21 | \n\n\nMeanAbsoluteError: 0.0580398365855217 | Loss: 0.0063902555893247 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.0498611927032471 | Loss: 0.0051345638393496 | Epoch: 23 | \n\n\nMeanAbsoluteError: 0.0505430921912193 | Loss: 0.0051699554225287 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0590252205729485 | Loss: 0.0063041188360120 | Epoch: 25 | \n\n\nMeanAbsoluteError: 0.0515257641673088 | Loss: 0.0048084556205700 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.0630534812808037 | Loss: 0.0066676731848230 | Epoch: 27 | \n\n\nMeanAbsoluteError: 0.0534884296357632 | Loss: 0.0061513389496563 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0627419874072075 | Loss: 0.0062690181836772 | Epoch: 29 | \n\n\nMeanAbsoluteError: 0.0506634674966335 | Loss: 0.0051125427081178 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.0531082339584827 | Loss: 0.0046146755608229 | Epoch: 31 | \n\n\nMeanAbsoluteError: 0.0449960045516491 | Loss: 0.0047255590772973 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0516395680606365 | Loss: 0.0044117506074074 | Epoch: 33 | \n\n\nMeanAbsoluteError: 0.0489494837820530 | Loss: 0.0049321009696891 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.0570706836879253 | Loss: 0.0061938325390936 | Epoch: 35 | \n\n\nMeanAbsoluteError: 0.0518912449479103 | Loss: 0.0058871061012794 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0603836476802826 | Loss: 0.0077531195890445 | Epoch: 37 | \n\n\nMeanAbsoluteError: 0.0451935492455959 | Loss: 0.0037257734578676 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.0443186797201633 | Loss: 0.0044210661614600 | Epoch: 39 | \n\n\nMeanAbsoluteError: 0.0436777099967003 | Loss: 0.0050586648621202 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0530651956796646 | Loss: 0.0051678386158668 | Epoch: 41 | \n\n\nMeanAbsoluteError: 0.0480441376566887 | Loss: 0.0039671665559021 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.0531826466321945 | Loss: 0.0057211823743553 | Epoch: 43 | \n\n\nMeanAbsoluteError: 0.0395344272255898 | Loss: 0.0025062521767373 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0420762635767460 | Loss: 0.0037450188179859 | Epoch: 45 | \n\n\nMeanAbsoluteError: 0.0502467527985573 | Loss: 0.0042748294072226 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.0438862852752209 | Loss: 0.0036425705318554 | Epoch: 47 | \n\n\nMeanAbsoluteError: 0.0567883029580116 | Loss: 0.0060093225553059 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0469487085938454 | Loss: 0.0042695726285904 | Epoch: 49 | \n\n\nMeanAbsoluteError: 0.0535376109182835 | Loss: 0.0054078174110215 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.0445385016500950 | Loss: 0.0044476781443406 | Epoch: 51 | \n\n\nMeanAbsoluteError: 0.0540870390832424 | Loss: 0.0051992401265754 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0405233278870583 | Loss: 0.0029749646581387 | Epoch: 53 | \n\n\nMeanAbsoluteError: 0.0507781319320202 | Loss: 0.0046699876995542 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0418309345841408 | Loss: 0.0034886091620697 | Epoch: 55 | \n\n\nMeanAbsoluteError: 0.0500016249716282 | Loss: 0.0041420180708743 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0400384180247784 | Loss: 0.0032580169186426 | Epoch: 57 | \n\n\nMeanAbsoluteError: 0.0623810440301895 | Loss: 0.0061047517049771 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.0454857721924782 | Loss: 0.0042457574521765 | Epoch: 59 | \n\n\nMeanAbsoluteError: 0.0486841052770615 | Loss: 0.0054403216792987 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0454535447061062 | Loss: 0.0038897694455675 | Epoch: 61 | \n\n\nMeanAbsoluteError: 0.0469865649938583 | Loss: 0.0041499322351928 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.0579420663416386 | Loss: 0.0073877452120471 | Epoch: 63 | \n\n\nMeanAbsoluteError: 0.0540305487811565 | Loss: 0.0056190938563444 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0430741794407368 | Loss: 0.0034590282481916 | Epoch: 65 | \n\n\nMeanAbsoluteError: 0.0506105571985245 | Loss: 0.0053758931014902 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.0542336590588093 | Loss: 0.0049403106319145 | Epoch: 67 | \n\n\nMeanAbsoluteError: 0.0423583462834358 | Loss: 0.0033160718616832 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0621731616556644 | Loss: 0.0067496184134283 | Epoch: 69 | \n\n\nMeanAbsoluteError: 0.0578075982630253 | Loss: 0.0051821055463873 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.0599722340703011 | Loss: 0.0062626335773250 | Epoch: 71 | \n\n\nMeanAbsoluteError: 0.0465344898402691 | Loss: 0.0036338526361550 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0553027540445328 | Loss: 0.0061472804339316 | Epoch: 73 | \n\n\nMeanAbsoluteError: 0.0497039854526520 | Loss: 0.0044631295694182 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.0576527863740921 | Loss: 0.0063696012724764 | Epoch: 75 | \n\n\nMeanAbsoluteError: 0.0656058341264725 | Loss: 0.0080692537594587 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0512407645583153 | Loss: 0.0061044981165861 | Epoch: 77 | \n\n\nMeanAbsoluteError: 0.0427016131579876 | Loss: 0.0037279523855362 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.0615924447774887 | Loss: 0.0069336196622596 | Epoch: 79 | \n\n\nMeanAbsoluteError: 0.0452021211385727 | Loss: 0.0042963005074013 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0464493595063686 | Loss: 0.0035693077504850 | Epoch: 81 | \n\n\nMeanAbsoluteError: 0.0592117793858051 | Loss: 0.0071984190278902 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0490319766104221 | Loss: 0.0039923657269145 | Epoch: 83 | \n\n\nMeanAbsoluteError: 0.0551796667277813 | Loss: 0.0055440807571778 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0500128567218781 | Loss: 0.0048358629797944 | Epoch: 85 | \n\n\nMeanAbsoluteError: 0.0473583266139030 | Loss: 0.0038197020588156 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.0510963089764118 | Loss: 0.0044465001242665 | Epoch: 87 | \n\n\nMeanAbsoluteError: 0.0508699193596840 | Loss: 0.0039716684349025 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0482854284346104 | Loss: 0.0035698862226966 | Epoch: 89 | \n\n\nMeanAbsoluteError: 0.0547107905149460 | Loss: 0.0060017702814478 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.0513130538165569 | Loss: 0.0040579588027098 | Epoch: 91 | \n\n\nMeanAbsoluteError: 0.0580847077071667 | Loss: 0.0051894251328821 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0446197353303432 | Loss: 0.0035456346992690 | Epoch: 93 | \n\n\nMeanAbsoluteError: 0.0507429018616676 | Loss: 0.0042342475006500 | Epoch: 94 | \n\n\nMeanAbsoluteError: 0.0441951192915440 | Loss: 0.0033734473444593 | Epoch: 95 | \n\n\nMeanAbsoluteError: 0.0526403300464153 | Loss: 0.0054817858617753 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0542922504246235 | Loss: 0.0052713009242255 | Epoch: 97 | \n\n\nMeanAbsoluteError: 0.0473384484648705 | Loss: 0.0045072990559185 | Epoch: 98 | \n\n\nMeanAbsoluteError: 0.0608610734343529 | Loss: 0.0064461884052994 | Epoch: 99 | \n\n\nMeanAbsoluteError: 0.0499427132308483 | Loss: 0.0037945707310708 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0538518317043781 | Loss: 0.0050005594340081 | Epoch: 101 | \n\n\nMeanAbsoluteError: 0.0501347817480564 | Loss: 0.0049663779880995 | Epoch: 102 | \n\n\nMeanAbsoluteError: 0.0459623783826828 | Loss: 0.0040788084152155 | Epoch: 103 | \n\n\nMeanAbsoluteError: 0.0490634478628635 | Loss: 0.0042704719100864 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0447152964770794 | Loss: 0.0037376483042653 | Epoch: 105 | \n\n\nMeanAbsoluteError: 0.0512279905378819 | Loss: 0.0046411088148419 | Epoch: 106 | \n\n\nMeanAbsoluteError: 0.0491465888917446 | Loss: 0.0042402162216604 | Epoch: 107 | \n\n\nMeanAbsoluteError: 0.0403543524444103 | Loss: 0.0036511152508095 | Early stopping at epoch 106\nFold: 4\nEpoch: 1 | \n\n\nMeanAbsoluteError: 0.1295451074838638 | Loss: 0.0272414047414294 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1183208152651787 | Loss: 0.0222997483439170 | Epoch: 3 | \n\n\nMeanAbsoluteError: 0.1002089232206345 | Loss: 0.0163654213986145 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1075942441821098 | Loss: 0.0184795561318214 | Epoch: 5 | \n\n\nMeanAbsoluteError: 0.1071042418479919 | Loss: 0.0170085478860598 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.0898475646972656 | Loss: 0.0130752814360536 | Epoch: 7 | \n\n\nMeanAbsoluteError: 0.0723304003477097 | Loss: 0.0088025706175428 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.0894422531127930 | Loss: 0.0127169148853192 | Epoch: 9 | \n\n\nMeanAbsoluteError: 0.0690239742398262 | Loss: 0.0076910901250533 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.0693888366222382 | Loss: 0.0077950891572982 | Epoch: 11 | \n\n\nMeanAbsoluteError: 0.0673896297812462 | Loss: 0.0088123113651258 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0725035071372986 | Loss: 0.0101033912505955 | Epoch: 13 | \n\n\nMeanAbsoluteError: 0.0741401314735413 | Loss: 0.0090441355087723 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.0624107010662556 | Loss: 0.0072377403624929 | Epoch: 15 | \n\n\nMeanAbsoluteError: 0.0669770687818527 | Loss: 0.0079951703440971 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0638877898454666 | Loss: 0.0075621591845097 | Epoch: 17 | \n\n\nMeanAbsoluteError: 0.0595755763351917 | Loss: 0.0060776871754430 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.0560981258749962 | Loss: 0.0061995738920254 | Epoch: 19 | \n\n\nMeanAbsoluteError: 0.0699021220207214 | Loss: 0.0078442511148751 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0516253896057606 | Loss: 0.0048236191371241 | Epoch: 21 | \n\n\nMeanAbsoluteError: 0.0624761395156384 | Loss: 0.0062396306938563 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.0583943687379360 | Loss: 0.0068559857631604 | Epoch: 23 | \n\n\nMeanAbsoluteError: 0.0552933216094971 | Loss: 0.0055146651620117 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0618971027433872 | Loss: 0.0064183540212420 | Epoch: 25 | \n\n\nMeanAbsoluteError: 0.0573176890611649 | Loss: 0.0066179173031392 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.0562461055815220 | Loss: 0.0073116907867818 | Epoch: 27 | \n\n\nMeanAbsoluteError: 0.0582503899931908 | Loss: 0.0061750186857982 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0691982656717300 | Loss: 0.0078855943866074 | Epoch: 29 | \n\n\nMeanAbsoluteError: 0.0626812130212784 | Loss: 0.0079393817398411 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.0683097764849663 | Loss: 0.0072557106255912 | Epoch: 31 | \n\n\nMeanAbsoluteError: 0.0615784749388695 | Loss: 0.0074442105654341 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0657906085252762 | Loss: 0.0085333293447128 | Epoch: 33 | \n\n\nMeanAbsoluteError: 0.0676696375012398 | Loss: 0.0097765120534370 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.0589211955666542 | Loss: 0.0076892381072797 | Epoch: 35 | \n\n\nMeanAbsoluteError: 0.0655896440148354 | Loss: 0.0080647191140228 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0619228407740593 | Loss: 0.0071563028837912 | Epoch: 37 | \n\n\nMeanAbsoluteError: 0.0490450374782085 | Loss: 0.0052634437709527 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.0684744566679001 | Loss: 0.0090208841272845 | Epoch: 39 | \n\n\nMeanAbsoluteError: 0.0569478906691074 | Loss: 0.0062485853335462 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0566121488809586 | Loss: 0.0052171697159513 | Epoch: 41 | \n\n\nMeanAbsoluteError: 0.0655281692743301 | Loss: 0.0079020174900786 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.0657156929373741 | Loss: 0.0104661396740434 | Epoch: 43 | \n\n\nMeanAbsoluteError: 0.0632758513092995 | Loss: 0.0082156969270167 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0610276944935322 | Loss: 0.0077409764501051 | Epoch: 45 | \n\n\nMeanAbsoluteError: 0.0619404613971710 | Loss: 0.0070734306847533 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.0610121972858906 | Loss: 0.0078730695218278 | Epoch: 47 | \n\n\nMeanAbsoluteError: 0.0559825673699379 | Loss: 0.0062714051007508 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0519441589713097 | Loss: 0.0064051867140314 | Epoch: 49 | \n\n\nMeanAbsoluteError: 0.0645272284746170 | Loss: 0.0070560539338308 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.0599462874233723 | Loss: 0.0075418464772296 | Epoch: 51 | \n\n\nMeanAbsoluteError: 0.0584816038608551 | Loss: 0.0069132885859849 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0562576390802860 | Loss: 0.0064391957040733 | Epoch: 53 | \n\n\nMeanAbsoluteError: 0.0532143004238605 | Loss: 0.0056180908219316 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0551778785884380 | Loss: 0.0073204944447543 | Epoch: 55 | \n\n\nMeanAbsoluteError: 0.0625424012541771 | Loss: 0.0083447203281909 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0632655173540115 | Loss: 0.0068185985661470 | Epoch: 57 | \n\n\nMeanAbsoluteError: 0.0580197051167488 | Loss: 0.0072162480254729 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.0602153092622757 | Loss: 0.0070842142001941 | Epoch: 59 | \n\n\nMeanAbsoluteError: 0.0688938051462173 | Loss: 0.0086791840417740 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0563457868993282 | Loss: 0.0060200112997196 | Epoch: 61 | \n\n\nMeanAbsoluteError: 0.0546776913106441 | Loss: 0.0055651518766983 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.0707466453313828 | Loss: 0.0103909698082134 | Epoch: 63 | \n\n\nMeanAbsoluteError: 0.0629545599222183 | Loss: 0.0080110639727746 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0576688498258591 | Loss: 0.0063994185587105 | Epoch: 65 | \n\n\nMeanAbsoluteError: 0.0674580931663513 | Loss: 0.0083285059350041 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.0621929466724396 | Loss: 0.0075349050860565 | Epoch: 67 | \n\n\nMeanAbsoluteError: 0.0637649893760681 | Loss: 0.0072835296159610 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0740604326128960 | Loss: 0.0108737735292659 | Epoch: 69 | \n\n\nMeanAbsoluteError: 0.0665446892380714 | Loss: 0.0085426762413520 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.0562667325139046 | Loss: 0.0087572373897554 | Epoch: 71 | \n\n\nMeanAbsoluteError: 0.0622893273830414 | Loss: 0.0059471748446902 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0603600181639194 | Loss: 0.0067792724137409 | Epoch: 73 | \n\n\nMeanAbsoluteError: 0.0574296861886978 | Loss: 0.0058086103526875 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.0739354789257050 | Loss: 0.0094917233946035 | Epoch: 75 | \n\n\nMeanAbsoluteError: 0.0568188875913620 | Loss: 0.0068219453275490 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0624318681657314 | Loss: 0.0062757639321857 | Epoch: 77 | \n\n\nMeanAbsoluteError: 0.0616847984492779 | Loss: 0.0091757299109864 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.0618060082197189 | Loss: 0.0068656064552040 | Epoch: 79 | \n\n\nMeanAbsoluteError: 0.0674304291605949 | Loss: 0.0084046331556657 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0626163259148598 | Loss: 0.0070494890320473 | Epoch: 81 | \n\n\nMeanAbsoluteError: 0.0688972696661949 | Loss: 0.0099171768927660 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0574288927018642 | Loss: 0.0057765903587167 | Epoch: 83 | \n\n\nMeanAbsoluteError: 0.0658561363816261 | Loss: 0.0075338723030514 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0488736815750599 | Loss: 0.0046499772355534 | Epoch: 85 | \n\n\nMeanAbsoluteError: 0.0619428381323814 | Loss: 0.0067468569882644 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.0598901361227036 | Loss: 0.0064820895562522 | Epoch: 87 | \n\n\nMeanAbsoluteError: 0.0498002395033836 | Loss: 0.0056425807549833 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0650264769792557 | Loss: 0.0102781606479906 | Epoch: 89 | \n\n\nMeanAbsoluteError: 0.0649131238460541 | Loss: 0.0085441264001509 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.0555118992924690 | Loss: 0.0079851931704280 | Epoch: 91 | \n\n\nMeanAbsoluteError: 0.0621955431997776 | Loss: 0.0067046598280565 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0527708977460861 | Loss: 0.0059728263340031 | Epoch: 93 | \n\n\nMeanAbsoluteError: 0.0623390488326550 | Loss: 0.0077111749515797 | Epoch: 94 | \n\n\nMeanAbsoluteError: 0.0606537610292435 | Loss: 0.0082690201234072 | Epoch: 95 | \n\n\nMeanAbsoluteError: 0.0601039603352547 | Loss: 0.0073759287434558 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0660987794399261 | Loss: 0.0090548777863240 | Epoch: 97 | \n\n\nMeanAbsoluteError: 0.0597432628273964 | Loss: 0.0054017025619172 | Epoch: 98 | \n\n\nMeanAbsoluteError: 0.0637103468179703 | Loss: 0.0082476272319372 | Epoch: 99 | \n\n\nMeanAbsoluteError: 0.0575288385152817 | Loss: 0.0077752613489373 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0707779079675674 | Loss: 0.0072257667421721 | Epoch: 101 | \n\n\nMeanAbsoluteError: 0.0487475693225861 | Loss: 0.0041552447678091 | Epoch: 102 | \n\n\nMeanAbsoluteError: 0.0548985712230206 | Loss: 0.0062958171519522 | Epoch: 103 | \n\n\nMeanAbsoluteError: 0.0529961250722408 | Loss: 0.0055664941543140 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0575807318091393 | Loss: 0.0071776412570706 | Epoch: 105 | \n\n\nMeanAbsoluteError: 0.0558836832642555 | Loss: 0.0066403083664437 | Epoch: 106 | \n\n\nMeanAbsoluteError: 0.0516363084316254 | Loss: 0.0051114483727500 | Epoch: 107 | \n\n\nMeanAbsoluteError: 0.0603631548583508 | Loss: 0.0086196033561674 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0536136776208878 | Loss: 0.0047195951740902 | Epoch: 109 | \n\n\nMeanAbsoluteError: 0.0613483339548111 | Loss: 0.0076270293301115 | Epoch: 110 | \n\n\nMeanAbsoluteError: 0.0521180778741837 | Loss: 0.0049957467305761 | Epoch: 111 | \n\n\nMeanAbsoluteError: 0.0665871277451515 | Loss: 0.0079336863172312 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0695098191499710 | Loss: 0.0102030189636235 | Epoch: 113 | \n\n\nMeanAbsoluteError: 0.0585582070052624 | Loss: 0.0068062680612247 | Epoch: 114 | \n\n\nMeanAbsoluteError: 0.0582700818777084 | Loss: 0.0070643280095492 | Epoch: 115 | \n\n\nMeanAbsoluteError: 0.0693532004952431 | Loss: 0.0091827564980262 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0535466969013214 | Loss: 0.0053293238787983 | Epoch: 117 | \n\n\nMeanAbsoluteError: 0.0591988675296307 | Loss: 0.0069440027716785 | Epoch: 118 | \n\n\nMeanAbsoluteError: 0.0656429976224899 | Loss: 0.0080397590063512 | Epoch: 119 | \n\n\nMeanAbsoluteError: 0.0472447685897350 | Loss: 0.0043877920923898 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0621988549828529 | Loss: 0.0086647627087167 | Epoch: 121 | \n\n\nMeanAbsoluteError: 0.0562786422669888 | Loss: 0.0057370731374249 | Epoch: 122 | \n\n\nMeanAbsoluteError: 0.0575303360819817 | Loss: 0.0069958989759191 | Epoch: 123 | \n\n\nMeanAbsoluteError: 0.0578312575817108 | Loss: 0.0060690940948776 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0698709636926651 | Loss: 0.0084867699501606 | Epoch: 125 | \n\n\nMeanAbsoluteError: 0.0714220702648163 | Loss: 0.0100764955143229 | Epoch: 126 | \n\n\nMeanAbsoluteError: 0.0822219476103783 | Loss: 0.0107800839647937 | Epoch: 127 | \n\n\nMeanAbsoluteError: 0.0585623420774937 | Loss: 0.0060803227198239 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0560715720057487 | Loss: 0.0078814978747127 | Epoch: 129 | \n\n\nMeanAbsoluteError: 0.0623379945755005 | Loss: 0.0067101854723520 | Epoch: 130 | \n\n\nMeanAbsoluteError: 0.0723433047533035 | Loss: 0.0087963947488998 | Epoch: 131 | \n\n\nMeanAbsoluteError: 0.0642264857888222 | Loss: 0.0079667904378416 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.0564659647643566 | Loss: 0.0062483401046708 | Epoch: 133 | \n\n\nMeanAbsoluteError: 0.0621905028820038 | Loss: 0.0075422709879394 | Epoch: 134 | \n\n\nMeanAbsoluteError: 0.0642918571829796 | Loss: 0.0107546728126741 | Epoch: 135 | \n\n\nMeanAbsoluteError: 0.0466957986354828 | Loss: 0.0044473974222246 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.0727990567684174 | Loss: 0.0093120436112468 | Epoch: 137 | \n\n\nMeanAbsoluteError: 0.0642651990056038 | Loss: 0.0078210648722374 | Epoch: 138 | \n\n\nMeanAbsoluteError: 0.0581276901066303 | Loss: 0.0078498151404067 | Epoch: 139 | \n\n\nMeanAbsoluteError: 0.0617445372045040 | Loss: 0.0088144409416530 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.0547469407320023 | Loss: 0.0054381818850883 | Epoch: 141 | \n\n\nMeanAbsoluteError: 0.0610641725361347 | Loss: 0.0061539752149166 | Epoch: 142 | \n\n\nMeanAbsoluteError: 0.0514637380838394 | Loss: 0.0059797720437368 | Epoch: 143 | \n\n\nMeanAbsoluteError: 0.0618896782398224 | Loss: 0.0072932114976888 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.0578461848199368 | Loss: 0.0069905480620666 | Epoch: 145 | \n\n\nMeanAbsoluteError: 0.0658600553870201 | Loss: 0.0068217953308844 | Epoch: 146 | \n\n\nMeanAbsoluteError: 0.0526573322713375 | Loss: 0.0066572105189642 | Epoch: 147 | \n\n\nMeanAbsoluteError: 0.0636313930153847 | Loss: 0.0087815399030940 | Epoch: 148 | \n\n\nMeanAbsoluteError: 0.0524257943034172 | Loss: 0.0058487363052196 | Epoch: 149 | \n\n\nMeanAbsoluteError: 0.0648193061351776 | Loss: 0.0096481845338041 | Epoch: 150 | \n\n\nMeanAbsoluteError: 0.0627068355679512 | Loss: 0.0077445944681620 | Epoch: 151 | \n\n\nMeanAbsoluteError: 0.0566737353801727 | Loss: 0.0067711727192196 | Epoch: 152 | \n\n\nMeanAbsoluteError: 0.0566287562251091 | Loss: 0.0062360209681524 | Epoch: 153 | \n\n\nMeanAbsoluteError: 0.0585543438792229 | Loss: 0.0079902020157673 | Epoch: 154 | \n\n\nMeanAbsoluteError: 0.0634030699729919 | Loss: 0.0072014157552845 | Epoch: 155 | \n\n\nMeanAbsoluteError: 0.0516289621591568 | Loss: 0.0051601562165440 | Epoch: 156 | \n\n\nMeanAbsoluteError: 0.0631784126162529 | Loss: 0.0073931910760271 | Epoch: 157 | \n\n\nMeanAbsoluteError: 0.0605923421680927 | Loss: 0.0060760460686512 | Epoch: 158 | \n\n\nMeanAbsoluteError: 0.0534964166581631 | Loss: 0.0061520030095170 | Epoch: 159 | \n\n\nMeanAbsoluteError: 0.0544124692678452 | Loss: 0.0054971754568844 | Epoch: 160 | \n\n\nMeanAbsoluteError: 0.0571031272411346 | Loss: 0.0056480065059776 | Epoch: 161 | \n\n\nMeanAbsoluteError: 0.0576127469539642 | Loss: 0.0067516019603667 | Epoch: 162 | \n\n\nMeanAbsoluteError: 0.0555339716374874 | Loss: 0.0052380962476421 | Epoch: 163 | \n\n\nMeanAbsoluteError: 0.0541806519031525 | Loss: 0.0068097904933473 | Epoch: 164 | \n\n\nMeanAbsoluteError: 0.0530400760471821 | Loss: 0.0058178929253840 | Epoch: 165 | \n\n\nMeanAbsoluteError: 0.0546691939234734 | Loss: 0.0058342379634269 | Early stopping at epoch 164\nFold: 5\nEpoch: 1 | \n\n\nMeanAbsoluteError: 0.1361552029848099 | Loss: 0.0267998285305042 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1328321695327759 | Loss: 0.0265433172910259 | Epoch: 3 | \n\n\nMeanAbsoluteError: 0.1406835317611694 | Loss: 0.0322584607996620 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1145121157169342 | Loss: 0.0200128537387802 | Epoch: 5 | \n\n\nMeanAbsoluteError: 0.1091193854808807 | Loss: 0.0181491009246271 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.0876618549227715 | Loss: 0.0129994561430067 | Epoch: 7 | \n\n\nMeanAbsoluteError: 0.1071730330586433 | Loss: 0.0160453595722524 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.0826943963766098 | Loss: 0.0102598698666463 | Epoch: 9 | \n\n\nMeanAbsoluteError: 0.0619466081261635 | Loss: 0.0056277949613734 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.0769614875316620 | Loss: 0.0108484982226331 | Epoch: 11 | \n\n\nMeanAbsoluteError: 0.0772276893258095 | Loss: 0.0102185603374472 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0634743645787239 | Loss: 0.0067072745878249 | Epoch: 13 | \n\n\nMeanAbsoluteError: 0.0581631436944008 | Loss: 0.0064111218536989 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.0580578818917274 | Loss: 0.0074157179333270 | Epoch: 15 | \n\n\nMeanAbsoluteError: 0.0652111247181892 | Loss: 0.0069929958643535 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0644076243042946 | Loss: 0.0082875687557344 | Epoch: 17 | \n\n\nMeanAbsoluteError: 0.0670819357037544 | Loss: 0.0068411454871798 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.0629865527153015 | Loss: 0.0069009244191245 | Epoch: 19 | \n\n\nMeanAbsoluteError: 0.0689855292439461 | Loss: 0.0081027412088588 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0546162761747837 | Loss: 0.0059179343650332 | Epoch: 21 | \n\n\nMeanAbsoluteError: 0.0614307671785355 | Loss: 0.0068012423580512 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.0541259497404099 | Loss: 0.0048594806307497 | Epoch: 23 | \n\n\nMeanAbsoluteError: 0.0599815510213375 | Loss: 0.0065047015937475 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0669004619121552 | Loss: 0.0082095228851988 | Epoch: 25 | \n\n\nMeanAbsoluteError: 0.0529720894992352 | Loss: 0.0049976353408196 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.0654290318489075 | Loss: 0.0080959100741893 | Epoch: 27 | \n\n\nMeanAbsoluteError: 0.0494590550661087 | Loss: 0.0045636794664181 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0561246648430824 | Loss: 0.0061059021480525 | Epoch: 29 | \n\n\nMeanAbsoluteError: 0.0637632831931114 | Loss: 0.0063100725125808 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.0584956258535385 | Loss: 0.0051714474550233 | Epoch: 31 | \n\n\nMeanAbsoluteError: 0.0591779090464115 | Loss: 0.0062096126180572 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0626657605171204 | Loss: 0.0077657487619525 | Epoch: 33 | \n\n\nMeanAbsoluteError: 0.0589084252715111 | Loss: 0.0067757866927423 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.0661469176411629 | Loss: 0.0083455499440718 | Epoch: 35 | \n\n\nMeanAbsoluteError: 0.0620181895792484 | Loss: 0.0071387847598929 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0630823820829391 | Loss: 0.0075631533444931 | Epoch: 37 | \n\n\nMeanAbsoluteError: 0.0533480644226074 | Loss: 0.0053683710284531 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.0633722841739655 | Loss: 0.0061204978802170 | Epoch: 39 | \n\n\nMeanAbsoluteError: 0.0667449161410332 | Loss: 0.0074129124942164 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0557881817221642 | Loss: 0.0050288043253554 | Epoch: 41 | \n\n\nMeanAbsoluteError: 0.0548548810184002 | Loss: 0.0050975992165219 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.0605497919023037 | Loss: 0.0061281452385279 | Epoch: 43 | \n\n\nMeanAbsoluteError: 0.0606228299438953 | Loss: 0.0077136508010041 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0621675290167332 | Loss: 0.0082937794862888 | Epoch: 45 | \n\n\nMeanAbsoluteError: 0.0674062445759773 | Loss: 0.0096488059229719 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.0555711649358273 | Loss: 0.0063959068972438 | Epoch: 47 | \n\n\nMeanAbsoluteError: 0.0650061666965485 | Loss: 0.0067495234812108 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0606169030070305 | Loss: 0.0079865059457146 | Epoch: 49 | \n\n\nMeanAbsoluteError: 0.0591282472014427 | Loss: 0.0059487207595689 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.0536581501364708 | Loss: 0.0070126633643388 | Epoch: 51 | \n\n\nMeanAbsoluteError: 0.0536542460322380 | Loss: 0.0053854854150604 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0504921637475491 | Loss: 0.0052837029618856 | Epoch: 53 | \n\n\nMeanAbsoluteError: 0.0600005164742470 | Loss: 0.0064991895431796 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0644682198762894 | Loss: 0.0069325850255644 | Epoch: 55 | \n\n\nMeanAbsoluteError: 0.0612165257334709 | Loss: 0.0060773845045612 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0624553523957729 | Loss: 0.0080526142936343 | Epoch: 57 | \n\n\nMeanAbsoluteError: 0.0589342638850212 | Loss: 0.0057518723949145 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.0639610067009926 | Loss: 0.0073440935856734 | Epoch: 59 | \n\n\nMeanAbsoluteError: 0.0675812885165215 | Loss: 0.0074772588335551 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0577390566468239 | Loss: 0.0063332542956162 | Epoch: 61 | \n\n\nMeanAbsoluteError: 0.0656770542263985 | Loss: 0.0082202855533419 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.0667214319109917 | Loss: 0.0075346161342727 | Epoch: 63 | \n\n\nMeanAbsoluteError: 0.0651335418224335 | Loss: 0.0076186429404725 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0596385672688484 | Loss: 0.0077807327168277 | Epoch: 65 | \n\n\nMeanAbsoluteError: 0.0574615038931370 | Loss: 0.0052049049856858 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.0671263337135315 | Loss: 0.0080702483582382 | Epoch: 67 | \n\n\nMeanAbsoluteError: 0.0688181519508362 | Loss: 0.0099071421159001 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0771731212735176 | Loss: 0.0114378734008194 | Epoch: 69 | \n\n\nMeanAbsoluteError: 0.0526800453662872 | Loss: 0.0048718708477771 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.0676365867257118 | Loss: 0.0072827638270190 | Epoch: 71 | \n\n\nMeanAbsoluteError: 0.0566409304738045 | Loss: 0.0055190260307147 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0657590478658676 | Loss: 0.0072742536699829 | Epoch: 73 | \n\n\nMeanAbsoluteError: 0.0500295385718346 | Loss: 0.0056130883436149 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.0679776743054390 | Loss: 0.0075295525602996 | Epoch: 75 | \n\n\nMeanAbsoluteError: 0.0624774992465973 | Loss: 0.0083467047661543 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0597581863403320 | Loss: 0.0063915929685418 | Epoch: 77 | \n\n\nMeanAbsoluteError: 0.0622355639934540 | Loss: 0.0074757002114963 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.0574176013469696 | Loss: 0.0072421511778465 | Epoch: 79 | \n\n\nMeanAbsoluteError: 0.0566970854997635 | Loss: 0.0057352280799443 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0638126432895660 | Loss: 0.0086297295951786 | Epoch: 81 | \n\n\nMeanAbsoluteError: 0.0706021711230278 | Loss: 0.0093391999924699 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0660186037421227 | Loss: 0.0082837347962105 | Epoch: 83 | \n\n\nMeanAbsoluteError: 0.0669088289141655 | Loss: 0.0090411271219357 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0695334449410439 | Loss: 0.0092465610852322 | Epoch: 85 | \n\n\nMeanAbsoluteError: 0.0627877637743950 | Loss: 0.0080320802159034 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.0545798242092133 | Loss: 0.0057458673657563 | Epoch: 87 | \n\n\nMeanAbsoluteError: 0.0709275081753731 | Loss: 0.0085482039273931 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0592638887465000 | Loss: 0.0056711107384987 | Epoch: 89 | \n\n\nMeanAbsoluteError: 0.0668169930577278 | Loss: 0.0074872000125022 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.0653130263090134 | Loss: 0.0068964539991262 | Epoch: 91 | \n\n\nMeanAbsoluteError: 0.0512141436338425 | Loss: 0.0048526836594997 | Early stopping at epoch 90\nFold: 6\nEpoch: 1 | \n\n\nMeanAbsoluteError: 0.1687397956848145 | Loss: 0.0419434167158145 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1438588201999664 | Loss: 0.0321013969727434 | Epoch: 3 | \n\n\nMeanAbsoluteError: 0.1118831038475037 | Loss: 0.0200412760202128 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1114128753542900 | Loss: 0.0179224965305856 | Epoch: 5 | \n\n\nMeanAbsoluteError: 0.1048199683427811 | Loss: 0.0198306036898150 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.0860336422920227 | Loss: 0.0141687317070766 | Epoch: 7 | \n\n\nMeanAbsoluteError: 0.0737045183777809 | Loss: 0.0102022227007323 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.0864967554807663 | Loss: 0.0121758653280827 | Epoch: 9 | \n\n\nMeanAbsoluteError: 0.0723257362842560 | Loss: 0.0100778177595482 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.0923158898949623 | Loss: 0.0142352185749377 | Epoch: 11 | \n\n\nMeanAbsoluteError: 0.0729380622506142 | Loss: 0.0104741860234823 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0714887008070946 | Loss: 0.0114961807759335 | Epoch: 13 | \n\n\nMeanAbsoluteError: 0.0884828343987465 | Loss: 0.0135756659572228 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.0735440701246262 | Loss: 0.0107202648471754 | Epoch: 15 | \n\n\nMeanAbsoluteError: 0.0727549493312836 | Loss: 0.0112216053172373 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0744273364543915 | Loss: 0.0100192390113639 | Epoch: 17 | \n\n\nMeanAbsoluteError: 0.0778321921825409 | Loss: 0.0117446687609817 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.0823405757546425 | Loss: 0.0136173363201893 | Epoch: 19 | \n\n\nMeanAbsoluteError: 0.0620370395481586 | Loss: 0.0081003202985112 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0753317177295685 | Loss: 0.0107288867271004 | Epoch: 21 | \n\n\nMeanAbsoluteError: 0.0657018274068832 | Loss: 0.0076335753457477 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.0645192191004753 | Loss: 0.0094378124295662 | Epoch: 23 | \n\n\nMeanAbsoluteError: 0.0766975656151772 | Loss: 0.0101348665507080 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0753081440925598 | Loss: 0.0103996323529058 | Epoch: 25 | \n\n\nMeanAbsoluteError: 0.0821129530668259 | Loss: 0.0131948809855832 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.0655679777264595 | Loss: 0.0101938976440579 | Epoch: 27 | \n\n\nMeanAbsoluteError: 0.0709491968154907 | Loss: 0.0087304362633194 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0754971206188202 | Loss: 0.0103168717692964 | Epoch: 29 | \n\n\nMeanAbsoluteError: 0.0737641155719757 | Loss: 0.0118334973051857 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.0751876756548882 | Loss: 0.0109401545666445 | Epoch: 31 | \n\n\nMeanAbsoluteError: 0.0620058886706829 | Loss: 0.0084949043991331 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0730844512581825 | Loss: 0.0099987929436163 | Epoch: 33 | \n\n\nMeanAbsoluteError: 0.0748436376452446 | Loss: 0.0116911731982747 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.0761922672390938 | Loss: 0.0113754312400348 | Epoch: 35 | \n\n\nMeanAbsoluteError: 0.0735652819275856 | Loss: 0.0092267719670557 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0723667070269585 | Loss: 0.0097238862743744 | Epoch: 37 | \n\n\nMeanAbsoluteError: 0.0881867110729218 | Loss: 0.0137190944156968 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.0801322013139725 | Loss: 0.0129004515026911 | Epoch: 39 | \n\n\nMeanAbsoluteError: 0.0654073059558868 | Loss: 0.0083479121094570 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0813074484467506 | Loss: 0.0101445348073657 | Epoch: 41 | \n\n\nMeanAbsoluteError: 0.0734921768307686 | Loss: 0.0107646540541632 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.0796309262514114 | Loss: 0.0125134825133360 | Epoch: 43 | \n\n\nMeanAbsoluteError: 0.0726558715105057 | Loss: 0.0098443691654561 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0709386020898819 | Loss: 0.0108330083849768 | Epoch: 45 | \n\n\nMeanAbsoluteError: 0.0691694244742393 | Loss: 0.0103270626859739 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.0773601084947586 | Loss: 0.0122848690057603 | Epoch: 47 | \n\n\nMeanAbsoluteError: 0.0703622698783875 | Loss: 0.0090291710310759 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0935762673616409 | Loss: 0.0157880431327682 | Epoch: 49 | \n\n\nMeanAbsoluteError: 0.0656713619828224 | Loss: 0.0091561876738874 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.0644260570406914 | Loss: 0.0091625319686360 | Epoch: 51 | \n\n\nMeanAbsoluteError: 0.0708733871579170 | Loss: 0.0092668853198680 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0599307268857956 | Loss: 0.0070307350789125 | Epoch: 53 | \n\n\nMeanAbsoluteError: 0.0593818724155426 | Loss: 0.0060952323524711 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0710178911685944 | Loss: 0.0088173968968197 | Epoch: 55 | \n\n\nMeanAbsoluteError: 0.0668576881289482 | Loss: 0.0084888606887454 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0624177604913712 | Loss: 0.0069881569182214 | Epoch: 57 | \n\n\nMeanAbsoluteError: 0.0842244923114777 | Loss: 0.0151139607008260 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.0565596371889114 | Loss: 0.0065817183438832 | Epoch: 59 | \n\n\nMeanAbsoluteError: 0.0731229186058044 | Loss: 0.0100347363175108 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0703742802143097 | Loss: 0.0083045214593697 | Epoch: 61 | \n\n\nMeanAbsoluteError: 0.0658114776015282 | Loss: 0.0086522144623674 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.0763604789972305 | Loss: 0.0113644704330139 | Epoch: 63 | \n\n\nMeanAbsoluteError: 0.0633325576782227 | Loss: 0.0069827745811870 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0688078999519348 | Loss: 0.0090006189599920 | Epoch: 65 | \n\n\nMeanAbsoluteError: 0.0723842456936836 | Loss: 0.0087957826908678 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.0723970457911491 | Loss: 0.0097305797792685 | Epoch: 67 | \n\n\nMeanAbsoluteError: 0.0748954340815544 | Loss: 0.0109762814916814 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0688213407993317 | Loss: 0.0101632511135764 | Epoch: 69 | \n\n\nMeanAbsoluteError: 0.0692326053977013 | Loss: 0.0097161832158096 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.0641695633530617 | Loss: 0.0071397859495706 | Epoch: 71 | \n\n\nMeanAbsoluteError: 0.0686443820595741 | Loss: 0.0084079927048431 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0686290860176086 | Loss: 0.0097180653560477 | Epoch: 73 | \n\n\nMeanAbsoluteError: 0.0660501345992088 | Loss: 0.0079874557783254 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.0748088583350182 | Loss: 0.0123910168857457 | Epoch: 75 | \n\n\nMeanAbsoluteError: 0.0680134072899818 | Loss: 0.0082702030105373 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0646824091672897 | Loss: 0.0085770358564332 | Epoch: 77 | \n\n\nMeanAbsoluteError: 0.0733272507786751 | Loss: 0.0094549433740142 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.0807291790843010 | Loss: 0.0108019224314306 | Epoch: 79 | \n\n\nMeanAbsoluteError: 0.0672891959547997 | Loss: 0.0084546569627351 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0640650242567062 | Loss: 0.0091712002445442 | Epoch: 81 | \n\n\nMeanAbsoluteError: 0.0692371428012848 | Loss: 0.0104408962562537 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0757210701704025 | Loss: 0.0107036830517105 | Epoch: 83 | \n\n\nMeanAbsoluteError: 0.0630327612161636 | Loss: 0.0069625818869099 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0627010315656662 | Loss: 0.0076692725281016 | Epoch: 85 | \n\n\nMeanAbsoluteError: 0.0745348632335663 | Loss: 0.0125566263539860 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.0752035826444626 | Loss: 0.0117147015676332 | Epoch: 87 | \n\n\nMeanAbsoluteError: 0.0730999261140823 | Loss: 0.0118230740683010 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0631376281380653 | Loss: 0.0099571059690788 | Epoch: 89 | \n\n\nMeanAbsoluteError: 0.0782348811626434 | Loss: 0.0111023086266449 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.0700057744979858 | Loss: 0.0104786289963298 | Epoch: 91 | \n\n\nMeanAbsoluteError: 0.0739642307162285 | Loss: 0.0096973564404135 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0867365673184395 | Loss: 0.0138106746957279 | Epoch: 93 | \n\n\nMeanAbsoluteError: 0.0710059180855751 | Loss: 0.0109723070815492 | Epoch: 94 | \n\n\nMeanAbsoluteError: 0.0689287409186363 | Loss: 0.0086445900826500 | Epoch: 95 | \n\n\nMeanAbsoluteError: 0.0946742817759514 | Loss: 0.0165264192395485 | Epoch: 96 | MeanAbsoluteError: 0.0555862821638584 | Loss: 0.0061838531221908 | Epoch: 97 | \n\n\nMeanAbsoluteError: 0.0756095722317696 | Loss: 0.0120667549065099 | Epoch: 98 | MeanAbsoluteError: 0.0710805356502533 | Loss: 0.0087832010064561 | Epoch: 99 | \n\n\nMeanAbsoluteError: 0.0730921104550362 | Loss: 0.0121865652215022 | Epoch: 100 | MeanAbsoluteError: 0.0746627599000931 | Loss: 0.0129335359759772 | Epoch: 101 | \n\n\nMeanAbsoluteError: 0.0587774738669395 | Loss: 0.0075627814915676 | Epoch: 102 | MeanAbsoluteError: 0.0704366192221642 | Loss: 0.0123351868958427 | Epoch: 103 | \n\n\nMeanAbsoluteError: 0.0792333111166954 | Loss: 0.0113896587863564 | Epoch: 104 | MeanAbsoluteError: 0.0647129788994789 | Loss: 0.0082308712970609 | Epoch: 105 | \n\n\nMeanAbsoluteError: 0.0678270012140274 | Loss: 0.0090675767009648 | Epoch: 106 | MeanAbsoluteError: 0.0829065814614296 | Loss: 0.0134242704281440 | Epoch: 107 | \n\n\nMeanAbsoluteError: 0.0809627622365952 | Loss: 0.0110001991359660 | Epoch: 108 | MeanAbsoluteError: 0.0647657364606857 | Loss: 0.0085742356291471 | Epoch: 109 | \n\n\nMeanAbsoluteError: 0.0878271311521530 | Loss: 0.0115513896856170 | Epoch: 110 | MeanAbsoluteError: 0.0813744291663170 | Loss: 0.0117496162867890 | Epoch: 111 | \n\n\nMeanAbsoluteError: 0.0702436715364456 | Loss: 0.0093992128878689 | Epoch: 112 | MeanAbsoluteError: 0.0701827108860016 | Loss: 0.0105562177307617 | Epoch: 113 | \n\n\nMeanAbsoluteError: 0.0682451128959656 | Loss: 0.0090680931909726 | Epoch: 114 | MeanAbsoluteError: 0.0736105740070343 | Loss: 0.0120328899580412 | Epoch: 115 | \n\n\nMeanAbsoluteError: 0.0685036852955818 | Loss: 0.0114603217135972 | Epoch: 116 | MeanAbsoluteError: 0.0728426575660706 | Loss: 0.0099598460723288 | Epoch: 117 | \n\n\nMeanAbsoluteError: 0.0681050270795822 | Loss: 0.0081563349275921 | Early stopping at epoch 116\nFold: 7\nEpoch: 1 | MeanAbsoluteError: 0.1234529688954353 | Loss: 0.0249044565627208 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1038545295596123 | Loss: 0.0168582241838941 | Epoch: 3 | MeanAbsoluteError: 0.0991577431559563 | Loss: 0.0152549712847059 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.0781348422169685 | Loss: 0.0099544279241505 | Epoch: 5 | MeanAbsoluteError: 0.0797237530350685 | Loss: 0.0100774518572367 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.0788219869136810 | Loss: 0.0109040756853154 | Epoch: 7 | MeanAbsoluteError: 0.0640989840030670 | Loss: 0.0066896020703448 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.0587595850229263 | Loss: 0.0058723579859361 | Epoch: 9 | MeanAbsoluteError: 0.0612723529338837 | Loss: 0.0062809117460767 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.0642153769731522 | Loss: 0.0061107083367041 | Epoch: 11 | MeanAbsoluteError: 0.0516264140605927 | Loss: 0.0054061538539827 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0735944435000420 | Loss: 0.0117343266208011 | Epoch: 13 | MeanAbsoluteError: 0.0572927296161652 | Loss: 0.0055189608426120 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.0474083572626114 | Loss: 0.0041399858712863 | Epoch: 15 | MeanAbsoluteError: 0.0493845567107201 | Loss: 0.0047832043626561 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0576296672224998 | Loss: 0.0055381332416661 | Epoch: 17 | MeanAbsoluteError: 0.0525424443185329 | Loss: 0.0056656119324124 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.0460553057491779 | Loss: 0.0042171976573837 | Epoch: 19 | MeanAbsoluteError: 0.0545976459980011 | Loss: 0.0055385928224915 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0409192480146885 | Loss: 0.0055191134496664 | Epoch: 21 | MeanAbsoluteError: 0.0568563789129257 | Loss: 0.0072470700475745 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.0532716661691666 | Loss: 0.0046066995172833 | Epoch: 23 | MeanAbsoluteError: 0.0569432675838470 | Loss: 0.0063396688425340 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0487894304096699 | Loss: 0.0035054896098490 | Epoch: 25 | MeanAbsoluteError: 0.0494885444641113 | Loss: 0.0057162300665648 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.0501844063401222 | Loss: 0.0044406677345530 | Epoch: 27 | MeanAbsoluteError: 0.0448846109211445 | Loss: 0.0038774333213671 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0563614405691624 | Loss: 0.0046932324453687 | Epoch: 29 | MeanAbsoluteError: 0.0483613386750221 | Loss: 0.0044310343935369 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.0461526662111282 | Loss: 0.0035709590436174 | Epoch: 31 | MeanAbsoluteError: 0.0468697324395180 | Loss: 0.0042593442619993 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0457247644662857 | Loss: 0.0047903065390598 | Epoch: 33 | MeanAbsoluteError: 0.0455576144158840 | Loss: 0.0040041821507307 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.0401184074580669 | Loss: 0.0029499816211263 | Epoch: 35 | MeanAbsoluteError: 0.0381462574005127 | Loss: 0.0033523708071488 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0389190763235092 | Loss: 0.0036981517489319 | Epoch: 37 | MeanAbsoluteError: 0.0439930669963360 | Loss: 0.0041552085755169 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.0377919040620327 | Loss: 0.0023027646820992 | Epoch: 39 | MeanAbsoluteError: 0.0374928824603558 | Loss: 0.0032095179463235 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0487146526575089 | Loss: 0.0037016681252191 | Epoch: 41 | MeanAbsoluteError: 0.0423346050083637 | Loss: 0.0039789595265085 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.0564791150391102 | Loss: 0.0056157050511012 | Epoch: 43 | MeanAbsoluteError: 0.0508621931076050 | Loss: 0.0058118996989483 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0452247522771358 | Loss: 0.0045196546304326 | Epoch: 45 | MeanAbsoluteError: 0.0340731777250767 | Loss: 0.0026496864391867 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.0517174825072289 | Loss: 0.0045760708699863 | Epoch: 47 | MeanAbsoluteError: 0.0554341264069080 | Loss: 0.0058302731038286 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0328364893794060 | Loss: 0.0025018940359587 | Epoch: 49 | MeanAbsoluteError: 0.0356037057936192 | Loss: 0.0023765830779806 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.0547443628311157 | Loss: 0.0063064400679790 | Epoch: 51 | MeanAbsoluteError: 0.0442257262766361 | Loss: 0.0038362858960262 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0484699495136738 | Loss: 0.0034682735747013 | Epoch: 53 | MeanAbsoluteError: 0.0577573962509632 | Loss: 0.0056292304518418 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0444532968103886 | Loss: 0.0035817397650904 | Epoch: 55 | MeanAbsoluteError: 0.0407717935740948 | Loss: 0.0035433305937868 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0360505543649197 | Loss: 0.0027970480956495 | Epoch: 57 | MeanAbsoluteError: 0.0359478853642941 | Loss: 0.0026567549211904 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.0484889782965183 | Loss: 0.0041775326513184 | Epoch: 59 | MeanAbsoluteError: 0.0419568642973900 | Loss: 0.0030939692022422 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0522609241306782 | Loss: 0.0058391781237263 | Epoch: 61 | MeanAbsoluteError: 0.0422049909830093 | Loss: 0.0033781847921021 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.0503936260938644 | Loss: 0.0055510910415950 | Epoch: 63 | MeanAbsoluteError: 0.0401714555919170 | Loss: 0.0029583583147122 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0354229100048542 | Loss: 0.0024252321544247 | Epoch: 65 | MeanAbsoluteError: 0.0536620281636715 | Loss: 0.0068040221768360 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.0494814924895763 | Loss: 0.0059000731159288 | Epoch: 67 | MeanAbsoluteError: 0.0408871471881866 | Loss: 0.0044325031474448 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0391681343317032 | Loss: 0.0034944072338896 | Epoch: 69 | MeanAbsoluteError: 0.0527789741754532 | Loss: 0.0060810027464938 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.0705540254712105 | Loss: 0.0075786768351323 | Epoch: 71 | MeanAbsoluteError: 0.0563827641308308 | Loss: 0.0048352292237373 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0358445718884468 | Loss: 0.0033215981182786 | Epoch: 73 | MeanAbsoluteError: 0.0480762049555779 | Loss: 0.0057199996002824 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.0504364222288132 | Loss: 0.0058538611942472 | Epoch: 75 | MeanAbsoluteError: 0.0365949720144272 | Loss: 0.0027130718092219 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0443592593073845 | Loss: 0.0044535474600987 | Epoch: 77 | MeanAbsoluteError: 0.0526016503572464 | Loss: 0.0048430594705189 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.0354474447667599 | Loss: 0.0027845271397382 | Epoch: 79 | MeanAbsoluteError: 0.0492106117308140 | Loss: 0.0056205726130149 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0365525595843792 | Loss: 0.0031965620851574 | Epoch: 81 | MeanAbsoluteError: 0.0578516498208046 | Loss: 0.0063402578100347 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0336575284600258 | Loss: 0.0020239861310424 | Epoch: 83 | MeanAbsoluteError: 0.0490442179143429 | Loss: 0.0051656309604788 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0544814243912697 | Loss: 0.0055255101223548 | Epoch: 85 | MeanAbsoluteError: 0.0357393734157085 | Loss: 0.0029585231991055 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.0380671136081219 | Loss: 0.0032675984727505 | Epoch: 87 | MeanAbsoluteError: 0.0470429845154285 | Loss: 0.0064883712240352 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0367677249014378 | Loss: 0.0027727800748275 | Epoch: 89 | MeanAbsoluteError: 0.0368092507123947 | Loss: 0.0028880982874678 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.0475561171770096 | Loss: 0.0054648117511533 | Epoch: 91 | MeanAbsoluteError: 0.0388941988348961 | Loss: 0.0030668997563995 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0526519827544689 | Loss: 0.0045800907537341 | Epoch: 93 | MeanAbsoluteError: 0.0444494821131229 | Loss: 0.0049096564672744 | Epoch: 94 | \n\n\nMeanAbsoluteError: 0.0510944426059723 | Loss: 0.0042312434077478 | Epoch: 95 | MeanAbsoluteError: 0.0442903861403465 | Loss: 0.0034301365286900 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0447144173085690 | Loss: 0.0039329349887199 | Epoch: 97 | MeanAbsoluteError: 0.0468128547072411 | Loss: 0.0042726785446016 | Epoch: 98 | \n\n\nMeanAbsoluteError: 0.0477880239486694 | Loss: 0.0037935624579684 | Epoch: 99 | MeanAbsoluteError: 0.0457170456647873 | Loss: 0.0046678203193901 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0394593365490437 | Loss: 0.0029358030296862 | Epoch: 101 | MeanAbsoluteError: 0.0331653207540512 | Loss: 0.0022359068747252 | Epoch: 102 | \n\n\nMeanAbsoluteError: 0.0365569107234478 | Loss: 0.0024651363223361 | Epoch: 103 | MeanAbsoluteError: 0.0377732664346695 | Loss: 0.0022013049366741 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0394034758210182 | Loss: 0.0036934225040918 | Epoch: 105 | MeanAbsoluteError: 0.0374852307140827 | Loss: 0.0028381473384798 | Epoch: 106 | \n\n\nMeanAbsoluteError: 0.0436474978923798 | Loss: 0.0034223307783787 | Epoch: 107 | MeanAbsoluteError: 0.0436349026858807 | Loss: 0.0034465638365453 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0361289866268635 | Loss: 0.0026315108272963 | Epoch: 109 | MeanAbsoluteError: 0.0433286912739277 | Loss: 0.0056258574074421 | Epoch: 110 | \n\n\nMeanAbsoluteError: 0.0549255236983299 | Loss: 0.0051065725095284 | Epoch: 111 | MeanAbsoluteError: 0.0439934097230434 | Loss: 0.0033806428501311 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0553181916475296 | Loss: 0.0051941743765313 | Epoch: 113 | MeanAbsoluteError: 0.0515906289219856 | Loss: 0.0051014289134540 | Epoch: 114 | \n\n\nMeanAbsoluteError: 0.0376896187663078 | Loss: 0.0027275716621751 | Epoch: 115 | MeanAbsoluteError: 0.0344583503901958 | Loss: 0.0033765093607004 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0410987995564938 | Loss: 0.0047440549387829 | Epoch: 117 | MeanAbsoluteError: 0.0491783432662487 | Loss: 0.0038025497208134 | Epoch: 118 | \n\n\nMeanAbsoluteError: 0.0453727133572102 | Loss: 0.0039183908957057 | Epoch: 119 | MeanAbsoluteError: 0.0513066612184048 | Loss: 0.0050800543260546 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0373566038906574 | Loss: 0.0022003292240417 | Epoch: 121 | MeanAbsoluteError: 0.0448829270899296 | Loss: 0.0053441669129265 | Epoch: 122 | \n\n\nMeanAbsoluteError: 0.0424305908381939 | Loss: 0.0031255250662350 | Epoch: 123 | MeanAbsoluteError: 0.0419652946293354 | Loss: 0.0038598484377592 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0509141683578491 | Loss: 0.0046875998825551 | Epoch: 125 | MeanAbsoluteError: 0.0400668606162071 | Loss: 0.0036713067540684 | Epoch: 126 | \n\n\nMeanAbsoluteError: 0.0457096211612225 | Loss: 0.0032970412911919 | Epoch: 127 | MeanAbsoluteError: 0.0392726622521877 | Loss: 0.0024810428507268 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0372281633317471 | Loss: 0.0032616691085143 | Epoch: 129 | MeanAbsoluteError: 0.0437358617782593 | Loss: 0.0035158209054946 | Epoch: 130 | \n\n\nMeanAbsoluteError: 0.0388302244246006 | Loss: 0.0032250383832993 | Epoch: 131 | MeanAbsoluteError: 0.0558539628982544 | Loss: 0.0059379891653617 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.0335950329899788 | Loss: 0.0023380350836445 | Epoch: 133 | MeanAbsoluteError: 0.0429355092346668 | Loss: 0.0032321999673373 | Epoch: 134 | \n\n\nMeanAbsoluteError: 0.0404422283172607 | Loss: 0.0029470132296690 | Epoch: 135 | MeanAbsoluteError: 0.0321686565876007 | Loss: 0.0020353326421733 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.0467959605157375 | Loss: 0.0058724564195682 | Epoch: 137 | MeanAbsoluteError: 0.0544436611235142 | Loss: 0.0045378479855851 | Epoch: 138 | \n\n\nMeanAbsoluteError: 0.0451936237514019 | Loss: 0.0039274568284432 | Epoch: 139 | MeanAbsoluteError: 0.0504657663404942 | Loss: 0.0039783585010670 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.0423080734908581 | Loss: 0.0036047038537128 | Epoch: 141 | MeanAbsoluteError: 0.0407872088253498 | Loss: 0.0029375329345035 | Epoch: 142 | \n\n\nMeanAbsoluteError: 0.0471210777759552 | Loss: 0.0048701082750295 | Epoch: 143 | MeanAbsoluteError: 0.0393749475479126 | Loss: 0.0036275973610687 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.0530727244913578 | Loss: 0.0055365790612996 | Epoch: 145 | MeanAbsoluteError: 0.0395344011485577 | Loss: 0.0036785151963928 | Epoch: 146 | \n\n\nMeanAbsoluteError: 0.0337132252752781 | Loss: 0.0027919317586706 | Early stopping at epoch 145\nFold: 8\nEpoch: 1 | MeanAbsoluteError: 0.1374367326498032 | Loss: 0.0276733073047720 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1390063911676407 | Loss: 0.0313035062729166 | Epoch: 3 | MeanAbsoluteError: 0.1228149384260178 | Loss: 0.0225088084832980 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1165016144514084 | Loss: 0.0208441738684017 | Epoch: 5 | MeanAbsoluteError: 0.1039877831935883 | Loss: 0.0167490679842348 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.1050908118486404 | Loss: 0.0169619152394052 | Epoch: 7 | MeanAbsoluteError: 0.1028598621487617 | Loss: 0.0172543398892650 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.0958747118711472 | Loss: 0.0142227670607659 | Epoch: 9 | MeanAbsoluteError: 0.0762315541505814 | Loss: 0.0102392567608219 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.0782990977168083 | Loss: 0.0106577894363839 | Epoch: 11 | MeanAbsoluteError: 0.0721391513943672 | Loss: 0.0084201721116327 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0812473371624947 | Loss: 0.0112918595640132 | Epoch: 13 | MeanAbsoluteError: 0.0799352079629898 | Loss: 0.0116113626505606 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.0627360641956329 | Loss: 0.0083883302734019 | Epoch: 15 | MeanAbsoluteError: 0.0713859423995018 | Loss: 0.0105617310350331 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0521201565861702 | Loss: 0.0062449821360553 | Epoch: 17 | MeanAbsoluteError: 0.0530716180801392 | Loss: 0.0051360322619215 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.0552141182124615 | Loss: 0.0061222131256587 | Epoch: 19 | MeanAbsoluteError: 0.0455452166497707 | Loss: 0.0049348883778573 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0502795167267323 | Loss: 0.0045776761880216 | Epoch: 21 | MeanAbsoluteError: 0.0605783164501190 | Loss: 0.0071129889424460 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.0564852207899094 | Loss: 0.0066572120771385 | Epoch: 23 | MeanAbsoluteError: 0.0478540621697903 | Loss: 0.0050416749951322 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0550281964242458 | Loss: 0.0056745168717148 | Epoch: 25 | MeanAbsoluteError: 0.0476697124540806 | Loss: 0.0051195334392385 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.0486089475452900 | Loss: 0.0050424468578198 | Epoch: 27 | MeanAbsoluteError: 0.0495666638016701 | Loss: 0.0048659047461115 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0524696595966816 | Loss: 0.0057415462685570 | Epoch: 29 | MeanAbsoluteError: 0.0591526739299297 | Loss: 0.0060021128200998 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.0526151470839977 | Loss: 0.0060109875854463 | Epoch: 31 | MeanAbsoluteError: 0.0591466762125492 | Loss: 0.0058168928103092 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0536059886217117 | Loss: 0.0049851682717697 | Epoch: 33 | MeanAbsoluteError: 0.0530237965285778 | Loss: 0.0051375592324453 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.0450921878218651 | Loss: 0.0039257599542347 | Epoch: 35 | MeanAbsoluteError: 0.0441803075373173 | Loss: 0.0044924994267953 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0603981763124466 | Loss: 0.0062080144237440 | Epoch: 37 | MeanAbsoluteError: 0.0481781400740147 | Loss: 0.0048966460700075 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.0492395386099815 | Loss: 0.0047535401446602 | Epoch: 39 | MeanAbsoluteError: 0.0385847613215446 | Loss: 0.0028055859998298 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0453119426965714 | Loss: 0.0051713044432780 | Epoch: 41 | MeanAbsoluteError: 0.0522523149847984 | Loss: 0.0056147109135054 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.0561820045113564 | Loss: 0.0061866163025395 | Epoch: 43 | MeanAbsoluteError: 0.0575414896011353 | Loss: 0.0050006594616347 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0504678264260292 | Loss: 0.0041746502975002 | Epoch: 45 | MeanAbsoluteError: 0.0519217140972614 | Loss: 0.0051643128032223 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.0528410337865353 | Loss: 0.0052294864928207 | Epoch: 47 | MeanAbsoluteError: 0.0497024208307266 | Loss: 0.0045403993482558 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0471315160393715 | Loss: 0.0050758966020882 | Epoch: 49 | MeanAbsoluteError: 0.0533564649522305 | Loss: 0.0060109125199513 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.0476179309189320 | Loss: 0.0042286198496675 | Epoch: 51 | MeanAbsoluteError: 0.0497152842581272 | Loss: 0.0048878072008777 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0415020957589149 | Loss: 0.0033957575058314 | Epoch: 53 | MeanAbsoluteError: 0.0455947034060955 | Loss: 0.0040764433797449 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0441358089447021 | Loss: 0.0034798114614275 | Epoch: 55 | MeanAbsoluteError: 0.0544273741543293 | Loss: 0.0062134879766605 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0460820756852627 | Loss: 0.0035012842585834 | Epoch: 57 | MeanAbsoluteError: 0.0418362952768803 | Loss: 0.0033521454091757 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.0535659752786160 | Loss: 0.0060466511103396 | Epoch: 59 | MeanAbsoluteError: 0.0517647899687290 | Loss: 0.0064737589832825 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0548863969743252 | Loss: 0.0054845792653326 | Epoch: 61 | MeanAbsoluteError: 0.0486875213682652 | Loss: 0.0072097262510887 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.0448541603982449 | Loss: 0.0037351494180397 | Epoch: 63 | MeanAbsoluteError: 0.0503941401839256 | Loss: 0.0051556638567350 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0422532781958580 | Loss: 0.0043949073923823 | Epoch: 65 | MeanAbsoluteError: 0.0598120354115963 | Loss: 0.0050990031853032 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.0450459606945515 | Loss: 0.0039988526416262 | Epoch: 67 | MeanAbsoluteError: 0.0494425073266029 | Loss: 0.0059742075525439 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0342030040919781 | Loss: 0.0027071595406876 | Epoch: 69 | MeanAbsoluteError: 0.0425083450973034 | Loss: 0.0034793110680766 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.0382314473390579 | Loss: 0.0035234291863162 | Epoch: 71 | MeanAbsoluteError: 0.0498653501272202 | Loss: 0.0055197439483331 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0549232661724091 | Loss: 0.0058399401067828 | Epoch: 73 | MeanAbsoluteError: 0.0376896560192108 | Loss: 0.0031669991608046 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.0535896196961403 | Loss: 0.0054246669214290 | Epoch: 75 | MeanAbsoluteError: 0.0418596118688583 | Loss: 0.0040838344362923 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0440928116440773 | Loss: 0.0050924530928919 | Epoch: 77 | MeanAbsoluteError: 0.0486115925014019 | Loss: 0.0049189191060857 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.0610706768929958 | Loss: 0.0092137591967073 | Epoch: 79 | MeanAbsoluteError: 0.0505536496639252 | Loss: 0.0044411463806262 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0509396046400070 | Loss: 0.0064060338907159 | Epoch: 81 | MeanAbsoluteError: 0.0419340506196022 | Loss: 0.0042424191487953 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0448743812739849 | Loss: 0.0054702499944072 | Epoch: 83 | MeanAbsoluteError: 0.0444181524217129 | Loss: 0.0047663162068392 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0485614426434040 | Loss: 0.0052036129726240 | Epoch: 85 | MeanAbsoluteError: 0.0601375401020050 | Loss: 0.0066140032677840 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.0448865853250027 | Loss: 0.0058775612896380 | Epoch: 87 | MeanAbsoluteError: 0.0435478501021862 | Loss: 0.0040994359140034 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0461949743330479 | Loss: 0.0044238010439305 | Epoch: 89 | MeanAbsoluteError: 0.0470159426331520 | Loss: 0.0049365379202824 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.0472462698817253 | Loss: 0.0048080984135875 | Epoch: 91 | MeanAbsoluteError: 0.0414232648909092 | Loss: 0.0034515138585658 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0431792102754116 | Loss: 0.0037989639274131 | Epoch: 93 | MeanAbsoluteError: 0.0409427024424076 | Loss: 0.0055556198349223 | Epoch: 94 | \n\n\nMeanAbsoluteError: 0.0457900911569595 | Loss: 0.0045389852826842 | Epoch: 95 | MeanAbsoluteError: 0.0638579428195953 | Loss: 0.0061307188589126 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0379286892712116 | Loss: 0.0024256667999837 | Epoch: 97 | MeanAbsoluteError: 0.0553378947079182 | Loss: 0.0044150534802331 | Epoch: 98 | \n\n\nMeanAbsoluteError: 0.0431468784809113 | Loss: 0.0033814949117816 | Epoch: 99 | MeanAbsoluteError: 0.0453700572252274 | Loss: 0.0042995193059771 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0414680242538452 | Loss: 0.0042912478137833 | Epoch: 101 | MeanAbsoluteError: 0.0346412397921085 | Loss: 0.0023212575783523 | Epoch: 102 | \n\n\nMeanAbsoluteError: 0.0542751103639603 | Loss: 0.0055082364795873 | Epoch: 103 | MeanAbsoluteError: 0.0365798175334930 | Loss: 0.0027034001317448 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0474616996943951 | Loss: 0.0061960281925097 | Epoch: 105 | MeanAbsoluteError: 0.0405123978853226 | Loss: 0.0049118613712311 | Epoch: 106 | \n\n\nMeanAbsoluteError: 0.0445686355233192 | Loss: 0.0031057852678574 | Epoch: 107 | MeanAbsoluteError: 0.0443137399852276 | Loss: 0.0046561018501122 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0413777939975262 | Loss: 0.0030654202564619 | Epoch: 109 | MeanAbsoluteError: 0.0446576513350010 | Loss: 0.0038773163939298 | Epoch: 110 | \n\n\nMeanAbsoluteError: 0.0639613494277000 | Loss: 0.0091518558776723 | Epoch: 111 | MeanAbsoluteError: 0.0468776039779186 | Loss: 0.0045507954558931 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0457145981490612 | Loss: 0.0043308084400801 | Epoch: 113 | MeanAbsoluteError: 0.0436295419931412 | Loss: 0.0041056001970831 | Epoch: 114 | \n\n\nMeanAbsoluteError: 0.0384459123015404 | Loss: 0.0028264022145707 | Epoch: 115 | MeanAbsoluteError: 0.0463306196033955 | Loss: 0.0053519903068753 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0412202402949333 | Loss: 0.0041576946790044 | Epoch: 117 | MeanAbsoluteError: 0.0436875261366367 | Loss: 0.0049596982381235 | Epoch: 118 | \n\n\nMeanAbsoluteError: 0.0422445051372051 | Loss: 0.0028906311177147 | Epoch: 119 | MeanAbsoluteError: 0.0532399527728558 | Loss: 0.0073262086236635 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0488372072577477 | Loss: 0.0052501538392300 | Epoch: 121 | MeanAbsoluteError: 0.0422287955880165 | Loss: 0.0043223010859667 | Epoch: 122 | \n\n\nMeanAbsoluteError: 0.0373729616403580 | Loss: 0.0034499593692509 | Epoch: 123 | MeanAbsoluteError: 0.0474605672061443 | Loss: 0.0036761421137131 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0453069843351841 | Loss: 0.0052212213989920 | Epoch: 125 | MeanAbsoluteError: 0.0412573814392090 | Loss: 0.0034237841654641 | Epoch: 126 | \n\n\nMeanAbsoluteError: 0.0419635139405727 | Loss: 0.0033356145946667 | Epoch: 127 | MeanAbsoluteError: 0.0341780520975590 | Loss: 0.0025264015286946 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0486839972436428 | Loss: 0.0047928956516374 | Epoch: 129 | MeanAbsoluteError: 0.0458146035671234 | Loss: 0.0039721411930469 | Epoch: 130 | \n\n\nMeanAbsoluteError: 0.0409040302038193 | Loss: 0.0027256227599887 | Epoch: 131 | MeanAbsoluteError: 0.0486680567264557 | Loss: 0.0052228406062708 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.0426811464130878 | Loss: 0.0033414501813240 | Epoch: 133 | MeanAbsoluteError: 0.0472925566136837 | Loss: 0.0042101508275104 | Epoch: 134 | \n\n\nMeanAbsoluteError: 0.0543481782078743 | Loss: 0.0056336631622309 | Epoch: 135 | MeanAbsoluteError: 0.0578053034842014 | Loss: 0.0065258792870177 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.0475800000131130 | Loss: 0.0053940598714130 | Epoch: 137 | MeanAbsoluteError: 0.0574687346816063 | Loss: 0.0049907394165460 | Epoch: 138 | \n\n\nMeanAbsoluteError: 0.0450430102646351 | Loss: 0.0053579945983634 | Epoch: 139 | MeanAbsoluteError: 0.0654336735606194 | Loss: 0.0077571248492369 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.0444375053048134 | Loss: 0.0042635182393357 | Epoch: 141 | MeanAbsoluteError: 0.0441319420933723 | Loss: 0.0043576211184980 | Epoch: 142 | \n\n\nMeanAbsoluteError: 0.0448317229747772 | Loss: 0.0040902775676491 | Epoch: 143 | MeanAbsoluteError: 0.0414509437978268 | Loss: 0.0043184327122827 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.0493579655885696 | Loss: 0.0083418514651175 | Epoch: 145 | MeanAbsoluteError: 0.0416471585631371 | Loss: 0.0033527194531276 | Epoch: 146 | \n\n\nMeanAbsoluteError: 0.0423944182693958 | Loss: 0.0043267280830500 | Epoch: 147 | MeanAbsoluteError: 0.0485891811549664 | Loss: 0.0055575829196291 | Epoch: 148 | \n\n\nMeanAbsoluteError: 0.0490230210125446 | Loss: 0.0061657967520520 | Epoch: 149 | MeanAbsoluteError: 0.0501714125275612 | Loss: 0.0058471960006640 | Epoch: 150 | \n\n\nMeanAbsoluteError: 0.0492983497679234 | Loss: 0.0048654340565778 | Epoch: 151 | MeanAbsoluteError: 0.0341732874512672 | Loss: 0.0027275965325176 | Epoch: 152 | \n\n\nMeanAbsoluteError: 0.0411529392004013 | Loss: 0.0029691223843166 | Epoch: 153 | MeanAbsoluteError: 0.0538726300001144 | Loss: 0.0077968105100668 | Epoch: 154 | \n\n\nMeanAbsoluteError: 0.0610259100794792 | Loss: 0.0066377737631018 | Epoch: 155 | MeanAbsoluteError: 0.0493384264409542 | Loss: 0.0051357757407599 | Epoch: 156 | \n\n\nMeanAbsoluteError: 0.0597255080938339 | Loss: 0.0058650378531848 | Epoch: 157 | MeanAbsoluteError: 0.0480600930750370 | Loss: 0.0050502338921293 | Epoch: 158 | \n\n\nMeanAbsoluteError: 0.0419135913252831 | Loss: 0.0037836879670907 | Epoch: 159 | MeanAbsoluteError: 0.0431661233305931 | Loss: 0.0041264550079806 | Epoch: 160 | \n\n\nMeanAbsoluteError: 0.0523506030440331 | Loss: 0.0065521920332685 | Epoch: 161 | MeanAbsoluteError: 0.0500298552215099 | Loss: 0.0034480882504095 | Epoch: 162 | \n\n\nMeanAbsoluteError: 0.0454096905887127 | Loss: 0.0064389453260586 | Epoch: 163 | MeanAbsoluteError: 0.0495575517416000 | Loss: 0.0052246547426106 | Epoch: 164 | \n\n\nMeanAbsoluteError: 0.0434758886694908 | Loss: 0.0038923176426369 | Epoch: 165 | MeanAbsoluteError: 0.0441629402339458 | Loss: 0.0057434859336354 | Early stopping at epoch 164\nFold: 9\nEpoch: 1 | \n\n\nMeanAbsoluteError: 0.1172357872128487 | Loss: 0.0249835757467036 | Epoch: 2 | MeanAbsoluteError: 0.1176900267601013 | Loss: 0.0207602613104078 | Epoch: 3 | \n\n\nMeanAbsoluteError: 0.1113067641854286 | Loss: 0.0186770497463070 | Epoch: 4 | MeanAbsoluteError: 0.0939953029155731 | Loss: 0.0143798219684798 | Epoch: 5 | \n\n\nMeanAbsoluteError: 0.0813096538186073 | Loss: 0.0114311742811249 | Epoch: 6 | MeanAbsoluteError: 0.0798245444893837 | Loss: 0.0110427977230686 | Epoch: 7 | \n\n\nMeanAbsoluteError: 0.0671087503433228 | Loss: 0.0082737542867947 | Epoch: 8 | MeanAbsoluteError: 0.0613413080573082 | Loss: 0.0059121912703491 | Epoch: 9 | \n\n\nMeanAbsoluteError: 0.0646580606698990 | Loss: 0.0088822115391779 | Epoch: 10 | MeanAbsoluteError: 0.0670200511813164 | Loss: 0.0078414877016957 | Epoch: 11 | \n\n\nMeanAbsoluteError: 0.0714023634791374 | Loss: 0.0089594070226527 | Epoch: 12 | MeanAbsoluteError: 0.0602418892085552 | Loss: 0.0057436228067113 | Epoch: 13 | \n\n\nMeanAbsoluteError: 0.0542251095175743 | Loss: 0.0055464076576754 | Epoch: 14 | MeanAbsoluteError: 0.0720131322741508 | Loss: 0.0103554116347088 | Epoch: 15 | \n\n\nMeanAbsoluteError: 0.0671845450997353 | Loss: 0.0086342555721505 | Epoch: 16 | MeanAbsoluteError: 0.0649741441011429 | Loss: 0.0071626252924594 | Epoch: 17 | \n\n\nMeanAbsoluteError: 0.0466409884393215 | Loss: 0.0046988147078082 | Epoch: 18 | MeanAbsoluteError: 0.0518569089472294 | Loss: 0.0049739499115206 | Epoch: 19 | \n\n\nMeanAbsoluteError: 0.0514013804495335 | Loss: 0.0051210914523556 | Epoch: 20 | MeanAbsoluteError: 0.0595673136413097 | Loss: 0.0069045534518619 | Epoch: 21 | \n\n\nMeanAbsoluteError: 0.0546700805425644 | Loss: 0.0053499130926167 | Epoch: 22 | MeanAbsoluteError: 0.0588247776031494 | Loss: 0.0061621812190144 | Epoch: 23 | \n\n\nMeanAbsoluteError: 0.0472579449415207 | Loss: 0.0053202606057032 | Epoch: 24 | MeanAbsoluteError: 0.0660508424043655 | Loss: 0.0066615240827489 | Epoch: 25 | \n\n\nMeanAbsoluteError: 0.0487492270767689 | Loss: 0.0056381624621841 | Epoch: 26 | MeanAbsoluteError: 0.0540363267064095 | Loss: 0.0047193045835369 | Epoch: 27 | \n\n\nMeanAbsoluteError: 0.0600768662989140 | Loss: 0.0062504985119001 | Epoch: 28 | MeanAbsoluteError: 0.0614775530993938 | Loss: 0.0072241395201462 | Epoch: 29 | \n\n\nMeanAbsoluteError: 0.0688777044415474 | Loss: 0.0086399933430724 | Epoch: 30 | MeanAbsoluteError: 0.0453533902764320 | Loss: 0.0038017217034044 | Epoch: 31 | \n\n\nMeanAbsoluteError: 0.0565037913620472 | Loss: 0.0064447741024196 | Epoch: 32 | MeanAbsoluteError: 0.0453887172043324 | Loss: 0.0045412421548882 | Epoch: 33 | \n\n\nMeanAbsoluteError: 0.0565603002905846 | Loss: 0.0064440210385678 | Epoch: 34 | MeanAbsoluteError: 0.0484452471137047 | Loss: 0.0050650956288267 | Epoch: 35 | \n\n\nMeanAbsoluteError: 0.0482646934688091 | Loss: 0.0057019653031603 | Epoch: 36 | MeanAbsoluteError: 0.0522206388413906 | Loss: 0.0061434559219588 | Epoch: 37 | \n\n\nMeanAbsoluteError: 0.0543848313391209 | Loss: 0.0059008503270049 | Epoch: 38 | MeanAbsoluteError: 0.0555062592029572 | Loss: 0.0050477143466616 | Epoch: 39 | \n\n\nMeanAbsoluteError: 0.0456202216446400 | Loss: 0.0037747517120666 | Epoch: 40 | MeanAbsoluteError: 0.0565228275954723 | Loss: 0.0065373061672569 | Epoch: 41 | \n\n\nMeanAbsoluteError: 0.0546152591705322 | Loss: 0.0059266959836420 | Epoch: 42 | MeanAbsoluteError: 0.0539576783776283 | Loss: 0.0066020853835373 | Epoch: 43 | \n\n\nMeanAbsoluteError: 0.0495820157229900 | Loss: 0.0047458462739507 | Epoch: 44 | MeanAbsoluteError: 0.0479259938001633 | Loss: 0.0041566754001766 | Epoch: 45 | \n\n\nMeanAbsoluteError: 0.0475148968398571 | Loss: 0.0048315568755452 | Epoch: 46 | MeanAbsoluteError: 0.0508032180368900 | Loss: 0.0053253833007497 | Epoch: 47 | \n\n\nMeanAbsoluteError: 0.0501035600900650 | Loss: 0.0072306615497487 | Epoch: 48 | MeanAbsoluteError: 0.0537414662539959 | Loss: 0.0056393831484736 | Epoch: 49 | \n\n\nMeanAbsoluteError: 0.0543368831276894 | Loss: 0.0048661268710230 | Epoch: 50 | MeanAbsoluteError: 0.0612048134207726 | Loss: 0.0063472468979084 | Epoch: 51 | \n\n\nMeanAbsoluteError: 0.0492692515254021 | Loss: 0.0051712006366310 | Epoch: 52 | MeanAbsoluteError: 0.0557113178074360 | Loss: 0.0056005708980732 | Epoch: 53 | \n\n\nMeanAbsoluteError: 0.0924686416983604 | Loss: 0.0130016946305449 | Epoch: 54 | MeanAbsoluteError: 0.0470180734992027 | Loss: 0.0052343755512713 | Epoch: 55 | \n\n\nMeanAbsoluteError: 0.0467957034707069 | Loss: 0.0052141738795819 | Epoch: 56 | MeanAbsoluteError: 0.0396084487438202 | Loss: 0.0033381350826508 | Epoch: 57 | \n\n\nMeanAbsoluteError: 0.0507704205811024 | Loss: 0.0047121578021548 | Epoch: 58 | MeanAbsoluteError: 0.0529200881719589 | Loss: 0.0050268140352833 | Epoch: 59 | \n\n\nMeanAbsoluteError: 0.0417566187679768 | Loss: 0.0037192617892288 | Epoch: 60 | MeanAbsoluteError: 0.0558184497058392 | Loss: 0.0068769277198813 | Epoch: 61 | \n\n\nMeanAbsoluteError: 0.0464020855724812 | Loss: 0.0037783216595506 | Epoch: 62 | MeanAbsoluteError: 0.0420752801001072 | Loss: 0.0035042487747537 | Epoch: 63 | \n\n\nMeanAbsoluteError: 0.0598996430635452 | Loss: 0.0062348953931807 | Epoch: 64 | MeanAbsoluteError: 0.0485216863453388 | Loss: 0.0043180285720155 | Epoch: 65 | \n\n\nMeanAbsoluteError: 0.0478619299829006 | Loss: 0.0047819742109053 | Epoch: 66 | MeanAbsoluteError: 0.0554016865789890 | Loss: 0.0063305946890838 | Epoch: 67 | \n\n\nMeanAbsoluteError: 0.0407041013240814 | Loss: 0.0044883761908680 | Epoch: 68 | MeanAbsoluteError: 0.0597236827015877 | Loss: 0.0067309127099669 | Epoch: 69 | \n\n\nMeanAbsoluteError: 0.0564291179180145 | Loss: 0.0072854008221139 | Epoch: 70 | MeanAbsoluteError: 0.0517698153853416 | Loss: 0.0048715012112203 | Epoch: 71 | \n\n\nMeanAbsoluteError: 0.0468033328652382 | Loss: 0.0045538153499365 | Epoch: 72 | MeanAbsoluteError: 0.0450678095221519 | Loss: 0.0040227638629193 | Epoch: 73 | \n\n\nMeanAbsoluteError: 0.0465100482106209 | Loss: 0.0039960070369908 | Epoch: 74 | MeanAbsoluteError: 0.0410478636622429 | Loss: 0.0031151708737105 | Epoch: 75 | \n\n\nMeanAbsoluteError: 0.0464675910770893 | Loss: 0.0051114853793898 | Epoch: 76 | MeanAbsoluteError: 0.0618213638663292 | Loss: 0.0078967330045998 | Epoch: 77 | \n\n\nMeanAbsoluteError: 0.0508961342275143 | Loss: 0.0048677994424137 | Epoch: 78 | MeanAbsoluteError: 0.0436435490846634 | Loss: 0.0032569752164328 | Epoch: 79 | \n\n\nMeanAbsoluteError: 0.0489352606236935 | Loss: 0.0042037598502177 | Epoch: 80 | MeanAbsoluteError: 0.0441811084747314 | Loss: 0.0035441406877138 | Epoch: 81 | \n\n\nMeanAbsoluteError: 0.0624358355998993 | Loss: 0.0066246739230477 | Epoch: 82 | MeanAbsoluteError: 0.0427246317267418 | Loss: 0.0038989507078301 | Epoch: 83 | \n\n\nMeanAbsoluteError: 0.0474096164107323 | Loss: 0.0036441601203898 | Epoch: 84 | MeanAbsoluteError: 0.0479999966919422 | Loss: 0.0054871087553553 | Epoch: 85 | \n\n\nMeanAbsoluteError: 0.0512699708342552 | Loss: 0.0063003661478153 | Epoch: 86 | MeanAbsoluteError: 0.0528414919972420 | Loss: 0.0062243858925425 | Epoch: 87 | \n\n\nMeanAbsoluteError: 0.0521373413503170 | Loss: 0.0049522076196109 | Epoch: 88 | MeanAbsoluteError: 0.0497848317027092 | Loss: 0.0046505618598670 | Epoch: 89 | \n\n\nMeanAbsoluteError: 0.0359752997756004 | Loss: 0.0020881516706700 | Epoch: 90 | MeanAbsoluteError: 0.0487183816730976 | Loss: 0.0052991386798497 | Epoch: 91 | \n\n\nMeanAbsoluteError: 0.0516067445278168 | Loss: 0.0059479763149284 | Epoch: 92 | MeanAbsoluteError: 0.0463106445968151 | Loss: 0.0039035263292205 | Epoch: 93 | \n\n\nMeanAbsoluteError: 0.0510158985853195 | Loss: 0.0048276947614235 | Epoch: 94 | MeanAbsoluteError: 0.0563014075160027 | Loss: 0.0080752968788147 | Epoch: 95 | \n\n\nMeanAbsoluteError: 0.0478503406047821 | Loss: 0.0043723333331470 | Epoch: 96 | MeanAbsoluteError: 0.0444415062665939 | Loss: 0.0031841035412911 | Epoch: 97 | \n\n\nMeanAbsoluteError: 0.0531083531677723 | Loss: 0.0054405292925926 | Epoch: 98 | MeanAbsoluteError: 0.0476004891097546 | Loss: 0.0042962702438952 | Epoch: 99 | \n\n\nMeanAbsoluteError: 0.0483026206493378 | Loss: 0.0044812720130162 | Epoch: 100 | MeanAbsoluteError: 0.0558529198169708 | Loss: 0.0056762207353201 | Epoch: 101 | \n\n\nMeanAbsoluteError: 0.0448809638619423 | Loss: 0.0036075529373752 | Epoch: 102 | MeanAbsoluteError: 0.0360305681824684 | Loss: 0.0030324829590077 | Epoch: 103 | \n\n\nMeanAbsoluteError: 0.0463160164654255 | Loss: 0.0042163833456400 | Epoch: 104 | MeanAbsoluteError: 0.0609929189085960 | Loss: 0.0075359920917365 | Epoch: 105 | \n\n\nMeanAbsoluteError: 0.0538906604051590 | Loss: 0.0048217514816385 | Epoch: 106 | MeanAbsoluteError: 0.0502893812954426 | Loss: 0.0052310034292392 | Epoch: 107 | \n\n\nMeanAbsoluteError: 0.0439229719340801 | Loss: 0.0044760416449907 | Epoch: 108 | MeanAbsoluteError: 0.0487011633813381 | Loss: 0.0051579959153269 | Epoch: 109 | \n\n\nMeanAbsoluteError: 0.0452546887099743 | Loss: 0.0042040133177159 | Epoch: 110 | MeanAbsoluteError: 0.0517486371099949 | Loss: 0.0058140354768301 | Epoch: 111 | \n\n\nMeanAbsoluteError: 0.0526556856930256 | Loss: 0.0066339000074479 | Epoch: 112 | MeanAbsoluteError: 0.0530944913625717 | Loss: 0.0049834412773355 | Epoch: 113 | \n\n\nMeanAbsoluteError: 0.0464283861219883 | Loss: 0.0040240021547876 | Epoch: 114 | MeanAbsoluteError: 0.0528395287692547 | Loss: 0.0060732415519082 | Epoch: 115 | \n\n\nMeanAbsoluteError: 0.0465971976518631 | Loss: 0.0052212273899036 | Epoch: 116 | MeanAbsoluteError: 0.0487754829227924 | Loss: 0.0052203410842384 | Epoch: 117 | \n\n\nMeanAbsoluteError: 0.0534561239182949 | Loss: 0.0049539765593811 | Epoch: 118 | MeanAbsoluteError: 0.0550290569663048 | Loss: 0.0063904139634150 | Epoch: 119 | \n\n\nMeanAbsoluteError: 0.0473691858351231 | Loss: 0.0046087102964520 | Epoch: 120 | MeanAbsoluteError: 0.0561778321862221 | Loss: 0.0054850269628402 | Epoch: 121 | \n\n\nMeanAbsoluteError: 0.0436565876007080 | Loss: 0.0042375482690449 | Epoch: 122 | MeanAbsoluteError: 0.0518955104053020 | Loss: 0.0041868724108029 | Epoch: 123 | \n\n\nMeanAbsoluteError: 0.0487163625657558 | Loss: 0.0043449540703128 | Epoch: 124 | MeanAbsoluteError: 0.0486333332955837 | Loss: 0.0052497073775157 | Epoch: 125 | \n\n\nMeanAbsoluteError: 0.0428322777152061 | Loss: 0.0039432850093223 | Epoch: 126 | MeanAbsoluteError: 0.0435788854956627 | Loss: 0.0042715150271901 | Epoch: 127 | \n\n\nMeanAbsoluteError: 0.0519715212285519 | Loss: 0.0045505898127046 | Epoch: 128 | MeanAbsoluteError: 0.0569702684879303 | Loss: 0.0076628555140745 | Epoch: 129 | \n\n\nMeanAbsoluteError: 0.0534894987940788 | Loss: 0.0040752554509359 | Epoch: 130 | MeanAbsoluteError: 0.0510935261845589 | Loss: 0.0044685782631859 | Epoch: 131 | \n\n\nMeanAbsoluteError: 0.0465764142572880 | Loss: 0.0051492451614020 | Epoch: 132 | MeanAbsoluteError: 0.0543086342513561 | Loss: 0.0055871133226901 | Epoch: 133 | \n\n\nMeanAbsoluteError: 0.0437445268034935 | Loss: 0.0043377997308898 | Epoch: 134 | MeanAbsoluteError: 0.0544043108820915 | Loss: 0.0080531465153819 | Epoch: 135 | \n\n\nMeanAbsoluteError: 0.0603490732610226 | Loss: 0.0071512160088437 | Epoch: 136 | MeanAbsoluteError: 0.0583204478025436 | Loss: 0.0067128082647776 | Epoch: 137 | \n\n\nMeanAbsoluteError: 0.0465017929673195 | Loss: 0.0044587488608578 | Epoch: 138 | MeanAbsoluteError: 0.0430762581527233 | Loss: 0.0036485167309785 | Epoch: 139 | \n\n\nMeanAbsoluteError: 0.0464083775877953 | Loss: 0.0035119662509085 | Epoch: 140 | MeanAbsoluteError: 0.0414862819015980 | Loss: 0.0029543110711249 | Epoch: 141 | \n\n\nMeanAbsoluteError: 0.0482460297644138 | Loss: 0.0043778997899678 | Epoch: 142 | MeanAbsoluteError: 0.0459075160324574 | Loss: 0.0042296499103451 | Epoch: 143 | \n\n\nMeanAbsoluteError: 0.0548679158091545 | Loss: 0.0065204262231978 | Epoch: 144 | MeanAbsoluteError: 0.0403414331376553 | Loss: 0.0029830855939788 | Epoch: 145 | \n\n\nMeanAbsoluteError: 0.0451600067317486 | Loss: 0.0050097795767495 | Epoch: 146 | MeanAbsoluteError: 0.0509104356169701 | Loss: 0.0055364152425542 | Epoch: 147 | \n\n\nMeanAbsoluteError: 0.0495515614748001 | Loss: 0.0051681337555727 | Epoch: 148 | MeanAbsoluteError: 0.0410659089684486 | Loss: 0.0036358770986016 | Epoch: 149 | \n\n\nMeanAbsoluteError: 0.0464085713028908 | Loss: 0.0049674599521005 | Epoch: 150 | MeanAbsoluteError: 0.0482429042458534 | Loss: 0.0049821607087954 | Epoch: 151 | \n\n\nMeanAbsoluteError: 0.0409724153578281 | Loss: 0.0032816150745090 | Epoch: 152 | MeanAbsoluteError: 0.0451047196984291 | Loss: 0.0049972926767973 | Epoch: 153 | \n\n\nMeanAbsoluteError: 0.0474792830646038 | Loss: 0.0053181373693336 | Early stopping at epoch 152\nFold: 10\nEpoch: 1 | MeanAbsoluteError: 0.1505321860313416 | Loss: 0.0373814047242586 | Epoch: 2 | \n\n\nMeanAbsoluteError: 0.1341463625431061 | Loss: 0.0259419717611029 | Epoch: 3 | MeanAbsoluteError: 0.1318567395210266 | Loss: 0.0257625442284804 | Epoch: 4 | \n\n\nMeanAbsoluteError: 0.1223548352718353 | Loss: 0.0225599115320410 | Epoch: 5 | MeanAbsoluteError: 0.1095115020871162 | Loss: 0.0181513176872753 | Epoch: 6 | \n\n\nMeanAbsoluteError: 0.1038170158863068 | Loss: 0.0158053003967955 | Epoch: 7 | MeanAbsoluteError: 0.0967821776866913 | Loss: 0.0154923522988191 | Epoch: 8 | \n\n\nMeanAbsoluteError: 0.1030098348855972 | Loss: 0.0177184024539131 | Epoch: 9 | MeanAbsoluteError: 0.0851003527641296 | Loss: 0.0121158836678100 | Epoch: 10 | \n\n\nMeanAbsoluteError: 0.0841116607189178 | Loss: 0.0116929876653907 | Epoch: 11 | MeanAbsoluteError: 0.0833694711327553 | Loss: 0.0122488431739979 | Epoch: 12 | \n\n\nMeanAbsoluteError: 0.0790107548236847 | Loss: 0.0100675169915821 | Epoch: 13 | MeanAbsoluteError: 0.0722560584545135 | Loss: 0.0103165860144565 | Epoch: 14 | \n\n\nMeanAbsoluteError: 0.0818924456834793 | Loss: 0.0105986275638525 | Epoch: 15 | MeanAbsoluteError: 0.0629078373312950 | Loss: 0.0075509873881506 | Epoch: 16 | \n\n\nMeanAbsoluteError: 0.0669349581003189 | Loss: 0.0071675748241922 | Epoch: 17 | MeanAbsoluteError: 0.0618290454149246 | Loss: 0.0069578931971381 | Epoch: 18 | \n\n\nMeanAbsoluteError: 0.0751151964068413 | Loss: 0.0098576115515943 | Epoch: 19 | MeanAbsoluteError: 0.0613186359405518 | Loss: 0.0067733345517459 | Epoch: 20 | \n\n\nMeanAbsoluteError: 0.0663515180349350 | Loss: 0.0080084589202530 | Epoch: 21 | MeanAbsoluteError: 0.0687954425811768 | Loss: 0.0095473083559997 | Epoch: 22 | \n\n\nMeanAbsoluteError: 0.0773321390151978 | Loss: 0.0092854219703720 | Epoch: 23 | MeanAbsoluteError: 0.0771270319819450 | Loss: 0.0108107581842118 | Epoch: 24 | \n\n\nMeanAbsoluteError: 0.0718091279268265 | Loss: 0.0080439731783162 | Epoch: 25 | MeanAbsoluteError: 0.0613524466753006 | Loss: 0.0071173546155198 | Epoch: 26 | \n\n\nMeanAbsoluteError: 0.0573399104177952 | Loss: 0.0063334542971391 | Epoch: 27 | MeanAbsoluteError: 0.0700055509805679 | Loss: 0.0088278031012473 | Epoch: 28 | \n\n\nMeanAbsoluteError: 0.0640623047947884 | Loss: 0.0081864244913539 | Epoch: 29 | MeanAbsoluteError: 0.0539837591350079 | Loss: 0.0053091593838942 | Epoch: 30 | \n\n\nMeanAbsoluteError: 0.0624148733913898 | Loss: 0.0069707521918015 | Epoch: 31 | MeanAbsoluteError: 0.0640761926770210 | Loss: 0.0063724293498895 | Epoch: 32 | \n\n\nMeanAbsoluteError: 0.0668332800269127 | Loss: 0.0085788459087221 | Epoch: 33 | MeanAbsoluteError: 0.0648071467876434 | Loss: 0.0078724879198349 | Epoch: 34 | \n\n\nMeanAbsoluteError: 0.0666542649269104 | Loss: 0.0089765380208309 | Epoch: 35 | MeanAbsoluteError: 0.0711570233106613 | Loss: 0.0101047093944194 | Epoch: 36 | \n\n\nMeanAbsoluteError: 0.0832560062408447 | Loss: 0.0106807602748561 | Epoch: 37 | MeanAbsoluteError: 0.0617659762501717 | Loss: 0.0077144893853424 | Epoch: 38 | \n\n\nMeanAbsoluteError: 0.0603848397731781 | Loss: 0.0082489315492029 | Epoch: 39 | MeanAbsoluteError: 0.0657170787453651 | Loss: 0.0083947160746902 | Epoch: 40 | \n\n\nMeanAbsoluteError: 0.0593610182404518 | Loss: 0.0072780268607088 | Epoch: 41 | MeanAbsoluteError: 0.0652022287249565 | Loss: 0.0082148000454673 | Epoch: 42 | \n\n\nMeanAbsoluteError: 0.0627646297216415 | Loss: 0.0075689001868551 | Epoch: 43 | MeanAbsoluteError: 0.0655858963727951 | Loss: 0.0079774127897018 | Epoch: 44 | \n\n\nMeanAbsoluteError: 0.0690128505229950 | Loss: 0.0082447546749161 | Epoch: 45 | MeanAbsoluteError: 0.0618119798600674 | Loss: 0.0057822479866445 | Epoch: 46 | \n\n\nMeanAbsoluteError: 0.0705023780465126 | Loss: 0.0082117386493617 | Epoch: 47 | MeanAbsoluteError: 0.0633399486541748 | Loss: 0.0069897700405608 | Epoch: 48 | \n\n\nMeanAbsoluteError: 0.0697820335626602 | Loss: 0.0099418859642286 | Epoch: 49 | MeanAbsoluteError: 0.0630035400390625 | Loss: 0.0077295498519491 | Epoch: 50 | \n\n\nMeanAbsoluteError: 0.0680048689246178 | Loss: 0.0091645862334050 | Epoch: 51 | MeanAbsoluteError: 0.0702456980943680 | Loss: 0.0097272736289037 | Epoch: 52 | \n\n\nMeanAbsoluteError: 0.0706589296460152 | Loss: 0.0084058777720202 | Epoch: 53 | MeanAbsoluteError: 0.0615943185985088 | Loss: 0.0089384379993694 | Epoch: 54 | \n\n\nMeanAbsoluteError: 0.0630500316619873 | Loss: 0.0070425344613166 | Epoch: 55 | MeanAbsoluteError: 0.0690006911754608 | Loss: 0.0087276098784059 | Epoch: 56 | \n\n\nMeanAbsoluteError: 0.0559046268463135 | Loss: 0.0064056628056838 | Epoch: 57 | MeanAbsoluteError: 0.0706591755151749 | Loss: 0.0097768523891528 | Epoch: 58 | \n\n\nMeanAbsoluteError: 0.0682418793439865 | Loss: 0.0077322459434911 | Epoch: 59 | MeanAbsoluteError: 0.0615696385502815 | Loss: 0.0064023870509118 | Epoch: 60 | \n\n\nMeanAbsoluteError: 0.0786560699343681 | Loss: 0.0117046675071693 | Epoch: 61 | MeanAbsoluteError: 0.0584018267691135 | Loss: 0.0064635155095647 | Epoch: 62 | \n\n\nMeanAbsoluteError: 0.0628165975213051 | Loss: 0.0067022277463156 | Epoch: 63 | MeanAbsoluteError: 0.0535288155078888 | Loss: 0.0054070224286988 | Epoch: 64 | \n\n\nMeanAbsoluteError: 0.0583375915884972 | Loss: 0.0067849711765750 | Epoch: 65 | MeanAbsoluteError: 0.0680346041917801 | Loss: 0.0085715753062127 | Epoch: 66 | \n\n\nMeanAbsoluteError: 0.0646111294627190 | Loss: 0.0074380665945892 | Epoch: 67 | MeanAbsoluteError: 0.0636407658457756 | Loss: 0.0073027254144948 | Epoch: 68 | \n\n\nMeanAbsoluteError: 0.0707308799028397 | Loss: 0.0092306548478798 | Epoch: 69 | MeanAbsoluteError: 0.0560751445591450 | Loss: 0.0067608992282588 | Epoch: 70 | \n\n\nMeanAbsoluteError: 0.0701136663556099 | Loss: 0.0083142416193508 | Epoch: 71 | MeanAbsoluteError: 0.0647647902369499 | Loss: 0.0080659033354515 | Epoch: 72 | \n\n\nMeanAbsoluteError: 0.0507827997207642 | Loss: 0.0045101065591622 | Epoch: 73 | MeanAbsoluteError: 0.0579609386622906 | Loss: 0.0068010390610792 | Epoch: 74 | \n\n\nMeanAbsoluteError: 0.0696533322334290 | Loss: 0.0091086368327244 | Epoch: 75 | MeanAbsoluteError: 0.0559084713459015 | Loss: 0.0060561009360334 | Epoch: 76 | \n\n\nMeanAbsoluteError: 0.0692183524370193 | Loss: 0.0108235795588161 | Epoch: 77 | MeanAbsoluteError: 0.0593423359096050 | Loss: 0.0073173083544064 | Epoch: 78 | \n\n\nMeanAbsoluteError: 0.0678133890032768 | Loss: 0.0070227453879152 | Epoch: 79 | MeanAbsoluteError: 0.0604270659387112 | Loss: 0.0060417440433342 | Epoch: 80 | \n\n\nMeanAbsoluteError: 0.0586117804050446 | Loss: 0.0059867004613177 | Epoch: 81 | MeanAbsoluteError: 0.0667390525341034 | Loss: 0.0078547348250420 | Epoch: 82 | \n\n\nMeanAbsoluteError: 0.0638829544186592 | Loss: 0.0083475649947874 | Epoch: 83 | MeanAbsoluteError: 0.0528729707002640 | Loss: 0.0052136133711499 | Epoch: 84 | \n\n\nMeanAbsoluteError: 0.0609128624200821 | Loss: 0.0076482850037372 | Epoch: 85 | MeanAbsoluteError: 0.0684368386864662 | Loss: 0.0087851436916166 | Epoch: 86 | \n\n\nMeanAbsoluteError: 0.0734503194689751 | Loss: 0.0096601159132730 | Epoch: 87 | MeanAbsoluteError: 0.0681354403495789 | Loss: 0.0082501356060115 | Epoch: 88 | \n\n\nMeanAbsoluteError: 0.0493830740451813 | Loss: 0.0047398798096065 | Epoch: 89 | MeanAbsoluteError: 0.0572699494659901 | Loss: 0.0058901847537177 | Epoch: 90 | \n\n\nMeanAbsoluteError: 0.0622331127524376 | Loss: 0.0086819474370434 | Epoch: 91 | MeanAbsoluteError: 0.0577282793819904 | Loss: 0.0062062093414939 | Epoch: 92 | \n\n\nMeanAbsoluteError: 0.0612105466425419 | Loss: 0.0062189466642359 | Epoch: 93 | MeanAbsoluteError: 0.0709921419620514 | Loss: 0.0109754472594851 | Epoch: 94 | \n\n\nMeanAbsoluteError: 0.0606449507176876 | Loss: 0.0087385109733217 | Epoch: 95 | MeanAbsoluteError: 0.0505150929093361 | Loss: 0.0046290151725631 | Epoch: 96 | \n\n\nMeanAbsoluteError: 0.0539205446839333 | Loss: 0.0073986137017178 | Epoch: 97 | MeanAbsoluteError: 0.0784206166863441 | Loss: 0.0106560549913691 | Epoch: 98 | \n\n\nMeanAbsoluteError: 0.0485214181244373 | Loss: 0.0044869695154305 | Epoch: 99 | MeanAbsoluteError: 0.0581676103174686 | Loss: 0.0067219713416237 | Epoch: 100 | \n\n\nMeanAbsoluteError: 0.0621945485472679 | Loss: 0.0093873145434862 | Epoch: 101 | MeanAbsoluteError: 0.0546179153025150 | Loss: 0.0071760267377473 | Epoch: 102 | \n\n\nMeanAbsoluteError: 0.0651561096310616 | Loss: 0.0079482584349954 | Epoch: 103 | MeanAbsoluteError: 0.0591971538960934 | Loss: 0.0063113694688162 | Epoch: 104 | \n\n\nMeanAbsoluteError: 0.0676085948944092 | Loss: 0.0104056000172232 | Epoch: 105 | MeanAbsoluteError: 0.0624341852962971 | Loss: 0.0076865756060355 | Epoch: 106 | \n\n\nMeanAbsoluteError: 0.0454358756542206 | Loss: 0.0050972095878723 | Epoch: 107 | MeanAbsoluteError: 0.0485964976251125 | Loss: 0.0050770252217682 | Epoch: 108 | \n\n\nMeanAbsoluteError: 0.0666709393262863 | Loss: 0.0084688071555530 | Epoch: 109 | MeanAbsoluteError: 0.0527016893029213 | Loss: 0.0066421621662672 | Epoch: 110 | \n\n\nMeanAbsoluteError: 0.0547427386045456 | Loss: 0.0078699588238333 | Epoch: 111 | MeanAbsoluteError: 0.0458312667906284 | Loss: 0.0057629656467515 | Epoch: 112 | \n\n\nMeanAbsoluteError: 0.0698763057589531 | Loss: 0.0098522913221341 | Epoch: 113 | MeanAbsoluteError: 0.0526641309261322 | Loss: 0.0073285429523541 | Epoch: 114 | \n\n\nMeanAbsoluteError: 0.0660217255353928 | Loss: 0.0078132123005791 | Epoch: 115 | MeanAbsoluteError: 0.0546588711440563 | Loss: 0.0070991210209636 | Epoch: 116 | \n\n\nMeanAbsoluteError: 0.0677618533372879 | Loss: 0.0080614539687164 | Epoch: 117 | MeanAbsoluteError: 0.0762702822685242 | Loss: 0.0103523537265853 | Epoch: 118 | \n\n\nMeanAbsoluteError: 0.0617055930197239 | Loss: 0.0091148895936875 | Epoch: 119 | MeanAbsoluteError: 0.0444798246026039 | Loss: 0.0040319842787889 | Epoch: 120 | \n\n\nMeanAbsoluteError: 0.0531220324337482 | Loss: 0.0057402588415873 | Epoch: 121 | MeanAbsoluteError: 0.0594215430319309 | Loss: 0.0081256164076666 | Epoch: 122 | \n\n\nMeanAbsoluteError: 0.0642738640308380 | Loss: 0.0089899015594990 | Epoch: 123 | MeanAbsoluteError: 0.0596221834421158 | Loss: 0.0075925287539856 | Epoch: 124 | \n\n\nMeanAbsoluteError: 0.0463717132806778 | Loss: 0.0044699984942921 | Epoch: 125 | MeanAbsoluteError: 0.0634471103549004 | Loss: 0.0076092549492247 | Epoch: 126 | \n\n\nMeanAbsoluteError: 0.0587493628263474 | Loss: 0.0090625123120844 | Epoch: 127 | MeanAbsoluteError: 0.0574749745428562 | Loss: 0.0077728753676638 | Epoch: 128 | \n\n\nMeanAbsoluteError: 0.0512195192277431 | Loss: 0.0052919355624069 | Epoch: 129 | MeanAbsoluteError: 0.0544948391616344 | Loss: 0.0059775251205652 | Epoch: 130 | \n\n\nMeanAbsoluteError: 0.0605409704148769 | Loss: 0.0071401181630790 | Epoch: 131 | MeanAbsoluteError: 0.0491442233324051 | Loss: 0.0052664549439214 | Epoch: 132 | \n\n\nMeanAbsoluteError: 0.0697740018367767 | Loss: 0.0088925149757415 | Epoch: 133 | MeanAbsoluteError: 0.0496185831725597 | Loss: 0.0054818421664934 | Epoch: 134 | \n\n\nMeanAbsoluteError: 0.0606070011854172 | Loss: 0.0085087748638426 | Epoch: 135 | MeanAbsoluteError: 0.0693370029330254 | Loss: 0.0098103206210698 | Epoch: 136 | \n\n\nMeanAbsoluteError: 0.0557470209896564 | Loss: 0.0076378000712094 | Epoch: 137 | MeanAbsoluteError: 0.0617778524756432 | Loss: 0.0091340900980867 | Epoch: 138 | \n\n\nMeanAbsoluteError: 0.0530001595616341 | Loss: 0.0067366497644868 | Epoch: 139 | MeanAbsoluteError: 0.0576068721711636 | Loss: 0.0062904597009317 | Epoch: 140 | \n\n\nMeanAbsoluteError: 0.0554134137928486 | Loss: 0.0050486571406229 | Epoch: 141 | MeanAbsoluteError: 0.0582154169678688 | Loss: 0.0073206364804019 | Epoch: 142 | \n\n\nMeanAbsoluteError: 0.0617595613002777 | Loss: 0.0080126418391816 | Epoch: 143 | MeanAbsoluteError: 0.0523391850292683 | Loss: 0.0054652053804602 | Epoch: 144 | \n\n\nMeanAbsoluteError: 0.0576252043247223 | Loss: 0.0067443104299645 | Epoch: 145 | MeanAbsoluteError: 0.0547550171613693 | Loss: 0.0063412425358995 | Epoch: 146 | \n\n\nMeanAbsoluteError: 0.0529801957309246 | Loss: 0.0057318464073782 | Epoch: 147 | MeanAbsoluteError: 0.0648513957858086 | Loss: 0.0101379164575169 | Epoch: 148 | \n\n\nMeanAbsoluteError: 0.0556635186076164 | Loss: 0.0078285316032214 | Epoch: 149 | MeanAbsoluteError: 0.0604388900101185 | Loss: 0.0071302779699461 | Epoch: 150 | \n\n\nMeanAbsoluteError: 0.0509907193481922 | Loss: 0.0064202406121275 | Epoch: 151 | MeanAbsoluteError: 0.0605017952620983 | Loss: 0.0072964496570281 | Epoch: 152 | \n\n\nMeanAbsoluteError: 0.0557976141571999 | Loss: 0.0071903436193959 | Epoch: 153 | MeanAbsoluteError: 0.0451186262071133 | Loss: 0.0043828652974648 | Epoch: 154 | \n\n\nMeanAbsoluteError: 0.0589866079390049 | Loss: 0.0073176596331625 | Epoch: 155 | MeanAbsoluteError: 0.0498058609664440 | Loss: 0.0044265147835876 | Epoch: 156 | \n\n\nMeanAbsoluteError: 0.0500610694289207 | Loss: 0.0057274116910636 | Epoch: 157 | MeanAbsoluteError: 0.0578924790024757 | Loss: 0.0070981555260145 | Epoch: 158 | \n\n\nMeanAbsoluteError: 0.0581887625157833 | Loss: 0.0082233631446098 | Epoch: 159 | MeanAbsoluteError: 0.0577226169407368 | Loss: 0.0068361365343802 | Epoch: 160 | \n\n\nMeanAbsoluteError: 0.0655587688088417 | Loss: 0.0090491244067939 | Epoch: 161 | MeanAbsoluteError: 0.0562661141157150 | Loss: 0.0068999080629930 | Epoch: 162 | \n\n\nMeanAbsoluteError: 0.0539579428732395 | Loss: 0.0058281296291030 | Epoch: 163 | MeanAbsoluteError: 0.0531446710228920 | Loss: 0.0056307544045222 | Epoch: 164 | \n\n\nMeanAbsoluteError: 0.0472165234386921 | Loss: 0.0040199885574671 | Epoch: 165 | MeanAbsoluteError: 0.0568644702434540 | Loss: 0.0057435796076718 | Epoch: 166 | \n\n\nMeanAbsoluteError: 0.0567265450954437 | Loss: 0.0085076527634206 | Epoch: 167 | MeanAbsoluteError: 0.0710207298398018 | Loss: 0.0102570988193083 | Epoch: 168 | \n\n\nMeanAbsoluteError: 0.0655366852879524 | Loss: 0.0089176510138294 | Epoch: 169 | MeanAbsoluteError: 0.0601331479847431 | Loss: 0.0071742431833767 | Epoch: 170 | \n\n\nMeanAbsoluteError: 0.0565233714878559 | Loss: 0.0076024002091099 | Epoch: 171 | MeanAbsoluteError: 0.0530958138406277 | Loss: 0.0062768047120279 | Epoch: 172 | \n\n\nMeanAbsoluteError: 0.0642050355672836 | Loss: 0.0092392557461818 | Epoch: 173 | MeanAbsoluteError: 0.0584961026906967 | Loss: 0.0058836199773046 | Epoch: 174 | \n\n\nMeanAbsoluteError: 0.0630785748362541 | Loss: 0.0080914592343526 | Epoch: 175 | MeanAbsoluteError: 0.0747937560081482 | Loss: 0.0105462714743156 | Epoch: 176 | \n\n\nMeanAbsoluteError: 0.0575501024723053 | Loss: 0.0076192394615366 | Epoch: 177 | MeanAbsoluteError: 0.0582688003778458 | Loss: 0.0057276208490993 | Epoch: 178 | \n\n\nMeanAbsoluteError: 0.0648411065340042 | Loss: 0.0095820181573240 | Epoch: 179 | MeanAbsoluteError: 0.0508357100188732 | Loss: 0.0060943401812647 | Epoch: 180 | \n\n\nMeanAbsoluteError: 0.0470551289618015 | Loss: 0.0049145281932747 | Epoch: 181 | MeanAbsoluteError: 0.0570837371051311 | Loss: 0.0075251042144373 | Epoch: 182 | \n\n\nMeanAbsoluteError: 0.0590297840535641 | Loss: 0.0073386507037167 | Epoch: 183 | MeanAbsoluteError: 0.0459917336702347 | Loss: 0.0044231302498911 | Epoch: 184 | \n\n\nMeanAbsoluteError: 0.0554594099521637 | Loss: 0.0077036757675859 | Epoch: 185 | MeanAbsoluteError: 0.0540508665144444 | Loss: 0.0082310874444934 | Epoch: 186 | \n\n\nMeanAbsoluteError: 0.0571001805365086 | Loss: 0.0083645958757888 | Epoch: 187 | MeanAbsoluteError: 0.0576010122895241 | Loss: 0.0064072902045714 | Epoch: 188 | \n\n\nMeanAbsoluteError: 0.0510868653655052 | Loss: 0.0071918462412074 | Epoch: 189 | MeanAbsoluteError: 0.0491816475987434 | Loss: 0.0052853475857634 | Epoch: 190 | \n\n\nMeanAbsoluteError: 0.0552592910826206 | Loss: 0.0076937429571094 | Epoch: 191 | MeanAbsoluteError: 0.0652271881699562 | Loss: 0.0085445723066536 | Epoch: 192 | \n\n\nMeanAbsoluteError: 0.0573950335383415 | Loss: 0.0074705811298022 | Epoch: 193 | MeanAbsoluteError: 0.0525457821786404 | Loss: 0.0052819058585625 | Epoch: 194 | \n\n\nMeanAbsoluteError: 0.0637385919690132 | Loss: 0.0089423751565986 | Epoch: 195 | MeanAbsoluteError: 0.0621828138828278 | Loss: 0.0080446264115520 | Epoch: 196 | \n\n\nMeanAbsoluteError: 0.0668711364269257 | Loss: 0.0094538261093056 | Epoch: 197 | MeanAbsoluteError: 0.0544425956904888 | Loss: 0.0073158619119428 | Epoch: 198 | \n\n\nMeanAbsoluteError: 0.0518801398575306 | Loss: 0.0051131541258655 | Epoch: 199 | MeanAbsoluteError: 0.0581603534519672 | Loss: 0.0080036634373335 | Epoch: 200 | \n\n\nMeanAbsoluteError: 0.0528272949159145 | Loss: 0.0054203358640035 | Epoch: 201 | MeanAbsoluteError: 0.0623189657926559 | Loss: 0.0085625981804557 | Epoch: 202 | \n\n\nMeanAbsoluteError: 0.0665741711854935 | Loss: 0.0096596153542543 | Epoch: 203 | MeanAbsoluteError: 0.0588388629257679 | Loss: 0.0070134257861915 | Epoch: 204 | \n\n\nMeanAbsoluteError: 0.0521865040063858 | Loss: 0.0064430783878869 | Epoch: 205 | MeanAbsoluteError: 0.0577110238373280 | Loss: 0.0087991928090700 | Epoch: 206 | \n\n\nMeanAbsoluteError: 0.0596932359039783 | Loss: 0.0080662838702735 | Epoch: 207 | MeanAbsoluteError: 0.0502521134912968 | Loss: 0.0064236160072211 | Epoch: 208 | \n\n\nMeanAbsoluteError: 0.0694956034421921 | Loss: 0.0101866935236523 | Epoch: 209 | MeanAbsoluteError: 0.0490633100271225 | Loss: 0.0068002084556680 | Epoch: 210 | \n\n\nMeanAbsoluteError: 0.0494897179305553 | Loss: 0.0059542137269790 | Epoch: 211 | MeanAbsoluteError: 0.0518540814518929 | Loss: 0.0067431628972722 | Epoch: 212 | \n\n\nMeanAbsoluteError: 0.0645281672477722 | Loss: 0.0098363255920748 | Epoch: 213 | MeanAbsoluteError: 0.0498803555965424 | Loss: 0.0047947534610732 | Epoch: 214 | \n\n\nMeanAbsoluteError: 0.0534256696701050 | Loss: 0.0066781547043670 | Epoch: 215 | MeanAbsoluteError: 0.0576695948839188 | Loss: 0.0066426375325626 | Epoch: 216 | \n\n\nMeanAbsoluteError: 0.0636638253927231 | Loss: 0.0092967670464602 | Epoch: 217 | MeanAbsoluteError: 0.0731270313262939 | Loss: 0.0102826469094278 | Epoch: 218 | \n\n\nMeanAbsoluteError: 0.0717559009790421 | Loss: 0.0110538774617733 | Epoch: 219 | MeanAbsoluteError: 0.0523790977895260 | Loss: 0.0056376980635552 | Epoch: 220 | \n\n\nMeanAbsoluteError: 0.0652142018079758 | Loss: 0.0088677995909865 | Epoch: 221 | MeanAbsoluteError: 0.0758368521928787 | Loss: 0.0099349946965678 | Epoch: 222 | \n\n\nMeanAbsoluteError: 0.0554857254028320 | Loss: 0.0052060993128599 | Epoch: 223 | MeanAbsoluteError: 0.0649882182478905 | Loss: 0.0075567372931311 | Epoch: 224 | \n\n\nMeanAbsoluteError: 0.0553612522780895 | Loss: 0.0061959373639323 | Epoch: 225 | MeanAbsoluteError: 0.0542000718414783 | Loss: 0.0066479889210314 | Epoch: 226 | \n\n\nMeanAbsoluteError: 0.0745222643017769 | Loss: 0.0129887371485193 | Epoch: 227 | MeanAbsoluteError: 0.0597912371158600 | Loss: 0.0072799518328303 | Epoch: 228 | \n\n\nMeanAbsoluteError: 0.0568351075053215 | Loss: 0.0073156528927099 | Early stopping at epoch 227\n\n\n\nmetric_name = type(fun_control[\"metric_torch\"]).__name__\nprint(f\"loss: {df_eval}, Cross-validated {metric_name}: {df_metrics}\")\n\nloss: 0.005507583225754878, Cross-validated MeanAbsoluteError: 0.04974936693906784\n\n\n\n\n19.10.4 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\npatience:  100.0\noptimizer:  3.0605820382321167\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n19.10.5 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n                                                \nParallel coordinates plots"
  },
  {
    "objectID": "24_spot_torch_regression.html#sec-summary-24",
    "href": "24_spot_torch_regression.html#sec-summary-24",
    "title": "19  HPT PyTorch: Regression",
    "section": "19.11 Summary and Outlook",
    "text": "19.11 Summary and Outlook\nThis tutorial presents the hyperparameter tuning open source software spotPython for PyTorch. Some of the advantages of spotPython are:\n\nNumerical and categorical hyperparameters.\nPowerful surrogate models.\nFlexible approach and easy to use.\nSimple JSON files for the specification of the hyperparameters.\nExtension of default and user specified network classes.\nNoise handling techniques.\nOnline visualization of the hyperparameter tuning process with tensorboard.\n\nCurrently, only rudimentary parallel and distributed neural network training is possible, but these capabilities will be extended in the future. The next version of spotPython will also include a more detailed documentation and more examples.\n\n\n\n\n\n\nImportant\n\n\n\nImportant: This tutorial does not present a complete benchmarking study (Bartz-Beielstein et al. 2020). The results are only preliminary and highly dependent on the local configuration (hard- and software). Our goal is to provide a first impression of the performance of the hyperparameter tuning package spotPython. The results should be interpreted with care.\n\n\n\n\n\n\nBartz-Beielstein, Thomas, Carola Doerr, Jakob Bossek, Sowmya Chandrasekaran, Tome Eftimov, Andreas Fischbach, Pascal Kerschke, et al. 2020. “Benchmarking in Optimization: Best Practice and Open Issues.” arXiv. https://arxiv.org/abs/2007.03488."
  },
  {
    "objectID": "25_spot_torch_vbdp.html#sec-setup-25",
    "href": "25_spot_torch_vbdp.html#sec-setup-25",
    "title": "20  HPT: PyTorch With VBDP",
    "section": "20.1 Step 1: Setup",
    "text": "20.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size and the device that is used.\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nThe device can be selected by setting the variable DEVICE.\nSince we are using a simple neural net, the setting \"cpu\" is preferred (on Mac).\nIf you have a GPU, you can use \"cuda:0\" instead.\nIf DEVICE is set to None, spotPython will automatically select the device.\n\nThis might result in \"mps\" on Macs, which is not the best choice for simple neural nets.\n\n\n\n\n\nMAX_TIME = 1\nINIT_SIZE = 5\nDEVICE = None # \"cpu\" # \"cuda:0\"\n\n\nfrom spotPython.utils.device import getDevice\nDEVICE = getDevice(DEVICE)\nprint(DEVICE)\n\nmps\n\n\n\nimport os\nimport copy\nimport socket\nfrom datetime import datetime\nfrom dateutil.tz import tzlocal\nstart_time = datetime.now(tzlocal())\nHOSTNAME = socket.gethostname().split(\".\")[0]\nexperiment_name = '25-torch' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\nexperiment_name = experiment_name.replace(':', '-')\nprint(experiment_name)\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')\n\n25-torch_maans03_1min_5init_2023-06-28_04-46-12"
  },
  {
    "objectID": "25_spot_torch_vbdp.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "25_spot_torch_vbdp.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "20  HPT: PyTorch With VBDP",
    "section": "20.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "20.2 Step 2: Initialization of the fun_control Dictionary\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process, which was described in Section 14.2, see Initialization of the fun_control Dictionary in the documentation.\n\nfrom spotPython.utils.init import fun_control_init\nfun_control = fun_control_init(task=\"classification\",\n    tensorboard_path=\"runs/25_spot_torch_vbdp\",\n    device=DEVICE)"
  },
  {
    "objectID": "25_spot_torch_vbdp.html#sec-data-loading-25",
    "href": "25_spot_torch_vbdp.html#sec-data-loading-25",
    "title": "20  HPT: PyTorch With VBDP",
    "section": "20.3 Step 3: PyTorch Data Loading",
    "text": "20.3 Step 3: PyTorch Data Loading\n\n20.3.1 1. Load VBDP Data\n\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\ntrain_df = pd.read_csv('./data/VBDP/train.csv')\n# remove the id column\ntrain_df = train_df.drop(columns=['id'])\nn_samples = train_df.shape[0]\nn_features = train_df.shape[1] - 1\ntarget_column = \"prognosis\"\n# Encode our prognosis labels as integers for easier decoding later\nenc = OrdinalEncoder()\ntrain_df[target_column] = enc.fit_transform(train_df[[target_column]])\n# convert all entries to int for faster processing\ntrain_df = train_df.astype(int)\n\n\nAdd logical combinations (AND, OR, XOR) of the features to the data set:\n\n\nfrom spotPython.utils.convert import add_logical_columns\ndf_new = train_df.copy()\n# save the target column using \"target_column\" as the column name\ntarget = train_df[target_column]\n# remove the target column\ndf_new = df_new.drop(columns=[target_column])\ntrain_df = add_logical_columns(df_new)\n# add the target column back\ntrain_df[target_column] = target\ntrain_df = train_df.astype(int)\n\n\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nn_samples = train_df.shape[0]\nn_features = train_df.shape[1] - 1\ntrain_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n\n\n\n20.3.2 Check content of the target column\n\ntrain_df[target_column].head()\n\n0     3\n1     7\n2     3\n3    10\n4     6\nName: prognosis, dtype: int64\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n                                                    random_state=42,\n                                                    test_size=0.25,\n                                                    stratify=train_df[target_column])\ntrainset = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\ntestset = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\ntrainset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\ntestset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\nprint(train_df.shape)\nprint(trainset.shape)\nprint(testset.shape)\n\n(707, 6113)\n(530, 6113)\n(177, 6113)\n\n\n\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom spotPython.torch.dataframedataset import DataFrameDataset\ndtype_x = torch.float32\ndtype_y = torch.long\ntrain_df = DataFrameDataset(train_df, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\ntrain = DataFrameDataset(trainset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\ntest = DataFrameDataset(testset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\nn_samples = len(train)\n\n\n# add the dataset to the fun_control\nfun_control.update({\"data\": train_df, # full dataset,\n               \"train\": train,\n               \"test\": test,\n               \"n_samples\": n_samples,\n               \"target_column\": target_column})"
  },
  {
    "objectID": "25_spot_torch_vbdp.html#sec-specification-of-preprocessing-model-25",
    "href": "25_spot_torch_vbdp.html#sec-specification-of-preprocessing-model-25",
    "title": "20  HPT: PyTorch With VBDP",
    "section": "20.4 Step 4: Specification of the Preprocessing Model",
    "text": "20.4 Step 4: Specification of the Preprocessing Model\nAfter the training and test data are specified and added to the fun_control dictionary, spotPython allows the specification of a data preprocessing pipeline, e.g., for the scaling of the data or for the one-hot encoding of categorical variables, see Section 14.4. This feature is not used here, so we do not change the default value (which is None)."
  },
  {
    "objectID": "25_spot_torch_vbdp.html#sec-selection-of-the-algorithm-25",
    "href": "25_spot_torch_vbdp.html#sec-selection-of-the-algorithm-25",
    "title": "20  HPT: PyTorch With VBDP",
    "section": "20.5 Step 5: Select algorithm and core_model_hyper_dict",
    "text": "20.5 Step 5: Select algorithm and core_model_hyper_dict\n\n20.5.1 Implementing a Configurable Neural Network With spotPython\nspotPython includes the Net_vbdp class which is implemented in the file netvbdp.py. The class is imported here.\nThis class inherits from the class Net_Core which is implemented in the file netcore.py, see Section 14.5.1.\n\n\n20.5.2 Add the NN Model to the fun_control Dictionary\n\nfrom spotPython.torch.netvbdp import Net_vbdp\nfrom spotPython.data.torch_hyper_dict import TorchHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfun_control = add_core_model_to_fun_control(core_model=Net_vbdp,\n                              fun_control=fun_control,\n                              hyper_dict=TorchHyperDict)\n\nThe corresponding entries for the core_model class are shown below.\n\nfun_control['core_model_hyper_dict']\n\n{'_L0': {'type': 'int',\n  'default': 64,\n  'transform': 'None',\n  'lower': 64,\n  'upper': 64},\n 'l1': {'type': 'int',\n  'default': 8,\n  'transform': 'transform_power_2_int',\n  'lower': 8,\n  'upper': 16},\n 'dropout_prob': {'type': 'float',\n  'default': 0.01,\n  'transform': 'None',\n  'lower': 0.0,\n  'upper': 0.9},\n 'lr_mult': {'type': 'float',\n  'default': 1.0,\n  'transform': 'None',\n  'lower': 0.1,\n  'upper': 10.0},\n 'batch_size': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 1,\n  'upper': 4},\n 'epochs': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 4,\n  'upper': 9},\n 'k_folds': {'type': 'int',\n  'default': 1,\n  'transform': 'None',\n  'lower': 1,\n  'upper': 1},\n 'patience': {'type': 'int',\n  'default': 2,\n  'transform': 'transform_power_2_int',\n  'lower': 1,\n  'upper': 5},\n 'optimizer': {'levels': ['Adadelta',\n   'Adagrad',\n   'Adam',\n   'AdamW',\n   'SparseAdam',\n   'Adamax',\n   'ASGD',\n   'NAdam',\n   'RAdam',\n   'RMSprop',\n   'Rprop',\n   'SGD'],\n  'type': 'factor',\n  'default': 'SGD',\n  'transform': 'None',\n  'class_name': 'torch.optim',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 12},\n 'sgd_momentum': {'type': 'float',\n  'default': 0.0,\n  'transform': 'None',\n  'lower': 0.0,\n  'upper': 1.0}}"
  },
  {
    "objectID": "25_spot_torch_vbdp.html#sec-modification-of-hyperparameters-25",
    "href": "25_spot_torch_vbdp.html#sec-modification-of-hyperparameters-25",
    "title": "20  HPT: PyTorch With VBDP",
    "section": "20.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "20.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in Section 14.6.\n\n\n\n\n\n\nCaution: Small number of epochs for demonstration purposes\n\n\n\n\nepochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nfun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[7, 9]) and\nfun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[2, 7])\n\n\n\n\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n\nfun_control = modify_hyper_parameter_bounds(fun_control, \"_L0\", bounds=[n_features, n_features])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[6, 13])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[2, 3])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[2, 2])\n\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_levels\nfun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n# fun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\", [\"Adam\"])\n# fun_control[\"core_model_hyper_dict\"]\n\n\n20.6.1 Optimizers\nOptimizers are described in Section 14.6.1.\n\nfun_control = modify_hyper_parameter_bounds(fun_control,\n    \"lr_mult\", bounds=[1e-3, 1e-3])\nfun_control = modify_hyper_parameter_bounds(fun_control,\n    \"sgd_momentum\", bounds=[0.9, 0.9])"
  },
  {
    "objectID": "25_spot_torch_vbdp.html#step-7-selection-of-the-objective-loss-function",
    "href": "25_spot_torch_vbdp.html#step-7-selection-of-the-objective-loss-function",
    "title": "20  HPT: PyTorch With VBDP",
    "section": "20.7 Step 7: Selection of the Objective (Loss) Function",
    "text": "20.7 Step 7: Selection of the Objective (Loss) Function\n\n20.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set (see Section 14.7.1)\nthe loss function (and a metric).\n\n\n\n20.7.2 Loss Functions and Metrics\nThe loss function is specified by the key \"loss_function\". We will use CrossEntropy loss for the multiclass-classification task.\n\nfrom torch.nn import CrossEntropyLoss\nloss_function = CrossEntropyLoss()\nfun_control.update({\"loss_function\": loss_function})\n\n\n\n20.7.3 Metric\n\nWe will use the MAP@k metric for the evaluation of the model. Here is an example how this metric is calculated.\n\n\nfrom spotPython.torch.mapk import MAPK\nimport torch\nmapk = MAPK(k=2)\ntarget = torch.tensor([0, 1, 2, 2])\npreds = torch.tensor(\n    [\n        [0.5, 0.2, 0.2],  # 0 is in top 2\n        [0.3, 0.4, 0.2],  # 1 is in top 2\n        [0.2, 0.4, 0.3],  # 2 is in top 2\n        [0.7, 0.2, 0.1],  # 2 isn't in top 2\n    ]\n)\nmapk.update(preds, target)\nprint(mapk.compute()) # tensor(0.6250)\n\ntensor(0.6250)\n\n\n\nfrom spotPython.torch.mapk import MAPK\nimport torchmetrics\nmetric_torch = MAPK(k=3)\nfun_control.update({\"metric_torch\": metric_torch})"
  },
  {
    "objectID": "25_spot_torch_vbdp.html#step-8-calling-the-spot-function",
    "href": "25_spot_torch_vbdp.html#step-8-calling-the-spot-function",
    "title": "20  HPT: PyTorch With VBDP",
    "section": "20.8 Step 8: Calling the SPOT Function",
    "text": "20.8 Step 8: Calling the SPOT Function\n\n20.8.1 Preparing the SPOT Call\nThe following code passes the information about the parameter ranges and bounds to spot.\n\n# extract the variable types, names, and bounds\nfrom spotPython.hyperparameters.values import (get_bound_values,\n    get_var_name,\n    get_var_type,)\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table generates a design table as follows:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name         | type   | default   |    lower |    upper | transform             |\n|--------------|--------|-----------|----------|----------|-----------------------|\n| _L0          | int    | 64        | 6112     | 6112     | None                  |\n| l1           | int    | 8         |    6     |   13     | transform_power_2_int |\n| dropout_prob | float  | 0.01      |    0     |    0.9   | None                  |\n| lr_mult      | float  | 1.0       |    0.001 |    0.001 | None                  |\n| batch_size   | int    | 4         |    1     |    4     | transform_power_2_int |\n| epochs       | int    | 4         |    2     |    3     | transform_power_2_int |\n| k_folds      | int    | 1         |    1     |    1     | None                  |\n| patience     | int    | 2         |    2     |    2     | transform_power_2_int |\n| optimizer    | factor | SGD       |    0     |    3     | None                  |\n| sgd_momentum | float  | 0.0       |    0.9   |    0.9   | None                  |\n\n\nThis allows to check if all information is available and if the information is correct.\n\n\n20.8.2 The Objective Function fun_torch\nThe objective function fun_torch is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.fun.hypertorch import HyperTorch\nfun = HyperTorch().fun_torch\n\n\nfrom spotPython.hyperparameters.values import get_default_hyperparameters_as_array\nhyper_dict=TorchHyperDict().load()\nX_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\n\n\n\n20.8.3 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function as described in Section 14.8.4.\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom math import inf\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run(X_start=X_start)\n\n\nconfig: {'_L0': 6112, 'l1': 2048, 'dropout_prob': 0.17031221661559992, 'lr_mult': 0.001, 'batch_size': 16, 'epochs': 8, 'k_folds': 1, 'patience': 4, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMAPK: 0.1540178507566452 | Loss: 2.3976302146911621 | Acc: 0.0849056603773585.\nEpoch: 2 | \n\n\nMAPK: 0.1569940596818924 | Loss: 2.3976029668535506 | Acc: 0.0849056603773585.\nEpoch: 3 | \n\n\nMAPK: 0.1696428805589676 | Loss: 2.3975278139114380 | Acc: 0.0849056603773585.\nEpoch: 4 | \n\n\nMAPK: 0.1755952686071396 | Loss: 2.3974441800798689 | Acc: 0.0849056603773585.\nEpoch: 5 | \n\n\nMAPK: 0.1711309701204300 | Loss: 2.3975015878677368 | Acc: 0.0849056603773585.\nEpoch: 6 | \n\n\nMAPK: 0.1763392984867096 | Loss: 2.3974156209400723 | Acc: 0.0849056603773585.\nEpoch: 7 | \n\n\nMAPK: 0.1815476119518280 | Loss: 2.3972805397851125 | Acc: 0.0849056603773585.\nEpoch: 8 | \n\n\nMAPK: 0.1815476268529892 | Loss: 2.3972591842923845 | Acc: 0.0849056603773585.\nReturned to Spot: Validation loss: 2.3972591842923845\n\nconfig: {'_L0': 6112, 'l1': 256, 'dropout_prob': 0.19379790035512987, 'lr_mult': 0.001, 'batch_size': 8, 'epochs': 4, 'k_folds': 1, 'patience': 4, 'optimizer': 'Adamax', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMAPK: 0.1820987761020660 | Loss: 2.3971086519735829 | Acc: 0.1084905660377359.\nEpoch: 2 | \n\n\nMAPK: 0.1851851791143417 | Loss: 2.3970684740278454 | Acc: 0.1037735849056604.\nEpoch: 3 | \n\n\nMAPK: 0.1774691641330719 | Loss: 2.3970886071523032 | Acc: 0.0943396226415094.\nEpoch: 4 | \n\n\nMAPK: 0.1805555522441864 | Loss: 2.3971756652549461 | Acc: 0.1084905660377359.\nReturned to Spot: Validation loss: 2.397175665254946\n\nconfig: {'_L0': 6112, 'l1': 4096, 'dropout_prob': 0.6759063718076167, 'lr_mult': 0.001, 'batch_size': 2, 'epochs': 8, 'k_folds': 1, 'patience': 4, 'optimizer': 'NAdam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMAPK: 0.1863207966089249 | Loss: 2.3976315979687675 | Acc: 0.0990566037735849.\nEpoch: 2 | \n\n\nMAPK: 0.1926100701093674 | Loss: 2.3974278827883162 | Acc: 0.1037735849056604.\nEpoch: 3 | \n\n\nMAPK: 0.2075471878051758 | Loss: 2.3971737353306897 | Acc: 0.1320754716981132.\nEpoch: 4 | \n\n\nMAPK: 0.1808176040649414 | Loss: 2.3970814223559396 | Acc: 0.1132075471698113.\nEpoch: 5 | \n\n\nMAPK: 0.1768868118524551 | Loss: 2.3969526155939640 | Acc: 0.1132075471698113.\nEpoch: 6 | \n\n\nMAPK: 0.1737421303987503 | Loss: 2.3967153801108307 | Acc: 0.1132075471698113.\nEpoch: 7 | \n\n\nMAPK: 0.1729559749364853 | Loss: 2.3964492105088144 | Acc: 0.1132075471698113.\nEpoch: 8 | \n\n\nMAPK: 0.1713836640119553 | Loss: 2.3960586201469853 | Acc: 0.1132075471698113.\nReturned to Spot: Validation loss: 2.3960586201469853\n\nconfig: {'_L0': 6112, 'l1': 128, 'dropout_prob': 0.37306669346546995, 'lr_mult': 0.001, 'batch_size': 4, 'epochs': 4, 'k_folds': 1, 'patience': 4, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMAPK: 0.2075471580028534 | Loss: 2.3964649506334990 | Acc: 0.1084905660377359.\nEpoch: 2 | \n\n\nMAPK: 0.2122641503810883 | Loss: 2.3964470287538924 | Acc: 0.1084905660377359.\nEpoch: 3 | \n\n\nMAPK: 0.2083333283662796 | Loss: 2.3964999261892066 | Acc: 0.1084905660377359.\nEpoch: 4 | \n\n\nMAPK: 0.2091194689273834 | Loss: 2.3964180181611261 | Acc: 0.1084905660377359.\nReturned to Spot: Validation loss: 2.396418018161126\n\nconfig: {'_L0': 6112, 'l1': 1024, 'dropout_prob': 0.870137281216666, 'lr_mult': 0.001, 'batch_size': 8, 'epochs': 8, 'k_folds': 1, 'patience': 4, 'optimizer': 'Adam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMAPK: 0.1836419701576233 | Loss: 2.3978879275145353 | Acc: 0.0990566037735849.\nEpoch: 2 | \n\n\nMAPK: 0.1751543283462524 | Loss: 2.3980012911337392 | Acc: 0.0943396226415094.\nEpoch: 3 | \n\n\nMAPK: 0.1689814776182175 | Loss: 2.3977711112410933 | Acc: 0.0943396226415094.\nEpoch: 4 | \n\n\nMAPK: 0.1728395074605942 | Loss: 2.3980036488285772 | Acc: 0.0896226415094340.\nEpoch: 5 | \n\n\nMAPK: 0.1882716268301010 | Loss: 2.3978758829611317 | Acc: 0.0990566037735849.\nEpoch: 6 | \n\n\nMAPK: 0.1766975373029709 | Loss: 2.3980953869996249 | Acc: 0.1037735849056604.\nEpoch: 7 | \n\n\nMAPK: 0.1728395223617554 | Loss: 2.3980912102593317 | Acc: 0.0990566037735849.\nEarly stopping at epoch 6\nReturned to Spot: Validation loss: 2.3980912102593317\n\n\n\nconfig: {'_L0': 6112, 'l1': 4096, 'dropout_prob': 0.6451395692472426, 'lr_mult': 0.001, 'batch_size': 2, 'epochs': 8, 'k_folds': 1, 'patience': 4, 'optimizer': 'NAdam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMAPK: 0.1218553408980370 | Loss: 2.3982275364533909 | Acc: 0.0518867924528302.\nEpoch: 2 | \n\n\nMAPK: 0.1627358645200729 | Loss: 2.3976766995663912 | Acc: 0.0613207547169811.\nEpoch: 3 | \n\n\nMAPK: 0.1422956138849258 | Loss: 2.3977168101184771 | Acc: 0.0660377358490566.\nEpoch: 4 | \n\n\nMAPK: 0.1627358645200729 | Loss: 2.3974515649507628 | Acc: 0.0566037735849057.\nEpoch: 5 | \n\n\nMAPK: 0.1572327464818954 | Loss: 2.3971563870052122 | Acc: 0.0566037735849057.\nEpoch: 6 | \n\n\nMAPK: 0.1650943458080292 | Loss: 2.3967831089811504 | Acc: 0.0613207547169811.\nEpoch: 7 | \n\n\nMAPK: 0.1643081903457642 | Loss: 2.3961733962005041 | Acc: 0.0613207547169811.\nEpoch: 8 | \n\n\nMAPK: 0.1603773534297943 | Loss: 2.3953707015739298 | Acc: 0.0707547169811321.\nReturned to Spot: Validation loss: 2.39537070157393\nspotPython tuning: 2.39537070157393 [##########] 96.02% \n\n\n\nconfig: {'_L0': 6112, 'l1': 4096, 'dropout_prob': 0.46046466104295497, 'lr_mult': 0.001, 'batch_size': 2, 'epochs': 8, 'k_folds': 1, 'patience': 4, 'optimizer': 'NAdam', 'sgd_momentum': 0.9}\nEpoch: 1 | \n\n\nMAPK: 0.1784591376781464 | Loss: 2.3977366865805858 | Acc: 0.0990566037735849.\nEpoch: 2 | \n\n\nMAPK: 0.2130503207445145 | Loss: 2.3973819917103030 | Acc: 0.1367924528301887.\nEpoch: 3 | \n\n\nMAPK: 0.2256288826465607 | Loss: 2.3970227016592927 | Acc: 0.1415094339622641.\nEpoch: 4 | \n\n\nMAPK: 0.2232704013586044 | Loss: 2.3964527103136168 | Acc: 0.1367924528301887.\nEpoch: 5 | \n\n\nMAPK: 0.2311320602893829 | Loss: 2.3956421141354545 | Acc: 0.1320754716981132.\nEpoch: 6 | \n\n\nMAPK: 0.2193395793437958 | Loss: 2.3947185772769854 | Acc: 0.1273584905660377.\nEpoch: 7 | \n\n\nMAPK: 0.2240566015243530 | Loss: 2.3934073943012164 | Acc: 0.1179245283018868.\nEpoch: 8 | \n\n\nMAPK: 0.2248427569866180 | Loss: 2.3915365304587022 | Acc: 0.1226415094339623.\nReturned to Spot: Validation loss: 2.391536530458702\nspotPython tuning: 2.391536530458702 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x189ae71f0&gt;"
  },
  {
    "objectID": "25_spot_torch_vbdp.html#sec-tensorboard-25",
    "href": "25_spot_torch_vbdp.html#sec-tensorboard-25",
    "title": "20  HPT: PyTorch With VBDP",
    "section": "20.9 Step 9: Tensorboard",
    "text": "20.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard as described in Section 14.9, see also the description in the documentation: Tensorboard."
  },
  {
    "objectID": "25_spot_torch_vbdp.html#sec-results-25",
    "href": "25_spot_torch_vbdp.html#sec-results-25",
    "title": "20  HPT: PyTorch With VBDP",
    "section": "20.10 Step 10: Results",
    "text": "20.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed as described in Section 14.10.\n\nspot_tuner.plot_progress(log_y=False, \n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name         | type   | default   |   lower |   upper |               tuned | transform             |   importance | stars   |\n|--------------|--------|-----------|---------|---------|---------------------|-----------------------|--------------|---------|\n| _L0          | int    | 64        |  6112.0 |  6112.0 |              6112.0 | None                  |         0.00 |         |\n| l1           | int    | 8         |     6.0 |    13.0 |                12.0 | transform_power_2_int |         0.14 | .       |\n| dropout_prob | float  | 0.01      |     0.0 |     0.9 | 0.46046466104295497 | None                  |         3.06 | *       |\n| lr_mult      | float  | 1.0       |   0.001 |   0.001 |               0.001 | None                  |         0.00 |         |\n| batch_size   | int    | 4         |     1.0 |     4.0 |                 1.0 | transform_power_2_int |       100.00 | ***     |\n| epochs       | int    | 4         |     2.0 |     3.0 |                 3.0 | transform_power_2_int |         0.19 | .       |\n| k_folds      | int    | 1         |     1.0 |     1.0 |                 1.0 | None                  |         0.00 |         |\n| patience     | int    | 2         |     2.0 |     2.0 |                 2.0 | transform_power_2_int |         0.00 |         |\n| optimizer    | factor | SGD       |     0.0 |     3.0 |                 3.0 | None                  |         4.72 | *       |\n| sgd_momentum | float  | 0.0       |     0.9 |     0.9 |                 0.9 | None                  |         0.00 |         |\n\n\n\nspot_tuner.plot_importance(threshold=0.025,\n    filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n20.10.1 Get the Tuned Architecture\n\nfrom spotPython.hyperparameters.values import get_one_core_model_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nmodel_spot = get_one_core_model_from_X(X, fun_control)\nmodel_spot\n\nNet_vbdp(\n  (fc1): Linear(in_features=6112, out_features=4096, bias=True)\n  (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n  (fc3): Linear(in_features=2048, out_features=1024, bias=True)\n  (fc4): Linear(in_features=1024, out_features=512, bias=True)\n  (fc5): Linear(in_features=512, out_features=11, bias=True)\n  (relu): ReLU()\n  (softmax): Softmax(dim=1)\n  (dropout1): Dropout(p=0.46046466104295497, inplace=False)\n  (dropout2): Dropout(p=0.23023233052147749, inplace=False)\n)\n\n\n\n\n20.10.2 Evaluation of the Tuned Architecture\n\nfrom spotPython.torch.traintest import (\n    train_tuned,\n    test_tuned,\n    )\ntrain_tuned(net=model_spot, train_dataset=train,\n        loss_function=fun_control[\"loss_function\"],\n        metric=fun_control[\"metric_torch\"],\n        shuffle=True,\n        device = fun_control[\"device\"],\n        path=None,\n        task=fun_control[\"task\"],)\n\nEpoch: 1 | \n\n\nMAPK: 0.1627358347177505 | Loss: 2.3976274706282705 | Acc: 0.0990566037735849.\nEpoch: 2 | \n\n\nMAPK: 0.1611635237932205 | Loss: 2.3973149983388073 | Acc: 0.0943396226415094.\nEpoch: 3 | \n\n\nMAPK: 0.1933961957693100 | Loss: 2.3969106044409410 | Acc: 0.1132075471698113.\nEpoch: 4 | \n\n\nMAPK: 0.2099056541919708 | Loss: 2.3965457700333506 | Acc: 0.1037735849056604.\nEpoch: 5 | \n\n\nMAPK: 0.2091194689273834 | Loss: 2.3956661404303783 | Acc: 0.1037735849056604.\nEpoch: 6 | \n\n\nMAPK: 0.1863207519054413 | Loss: 2.3948831828135364 | Acc: 0.0943396226415094.\nEpoch: 7 | \n\n\nMAPK: 0.1965408921241760 | Loss: 2.3936074229906188 | Acc: 0.0990566037735849.\nEpoch: 8 | \n\n\nMAPK: 0.1973270177841187 | Loss: 2.3920769286605545 | Acc: 0.0990566037735849.\nReturned to Spot: Validation loss: 2.3920769286605545\n\n\nIf path is set to a filename, e.g., path = \"model_spot_trained.pt\", the weights of the trained model will be loaded from this file.\n\ntest_tuned(net=model_spot, test_dataset=test,\n            shuffle=False,\n            loss_function=fun_control[\"loss_function\"],\n            metric=fun_control[\"metric_torch\"],\n            device = fun_control[\"device\"],\n            task=fun_control[\"task\"],)\n\nMAPK: 0.2219101339578629 | Loss: 2.3883381907859547 | Acc: 0.1186440677966102.\nFinal evaluation: Validation loss: 2.3883381907859547\nFinal evaluation: Validation metric: 0.22191013395786285\n----------------------------------------------\n\n\n(2.3883381907859547, nan, tensor(0.2219))\n\n\n\n\n20.10.3 Cross-validated Evaluations\n\nThis is the evaluation that will be used in the comparison.\n\n\n\n\n\n\n\nCaution: Cross-validated Evaluations\n\n\n\n\nThe number of folds is set to 1 by default.\nHere it was changed to 3 for demonstration purposes.\nSet the number of folds to a reasonable value, e.g., 10.\nThis can be done by setting the k_folds attribute of the model as follows:\nsetattr(model_spot, \"k_folds\",  10)\n\n\n\n\nfrom spotPython.torch.traintest import evaluate_cv\n# modify k-kolds:\nsetattr(model_spot, \"k_folds\",  3)\ndf_eval, df_preds, df_metrics = evaluate_cv(net=model_spot,\n    dataset=fun_control[\"data\"],\n    loss_function=fun_control[\"loss_function\"],\n    metric=fun_control[\"metric_torch\"],\n    task=fun_control[\"task\"],\n    writer=fun_control[\"writer\"],\n    writerId=\"model_spot_cv\",\n    device = fun_control[\"device\"])\n\nFold: 1\nEpoch: 1 | \n\n\nMAPK: 0.1631355732679367 | Loss: 2.3976899385452271 | Acc: 0.0720338983050847.\nEpoch: 2 | \n\n\nMAPK: 0.1935028284788132 | Loss: 2.3970449900223039 | Acc: 0.0593220338983051.\nEpoch: 3 | \n\n\nMAPK: 0.2153954058885574 | Loss: 2.3962083509412864 | Acc: 0.0974576271186441.\nEpoch: 4 | \n\n\nMAPK: 0.2104519307613373 | Loss: 2.3953836327892239 | Acc: 0.1144067796610169.\nEpoch: 5 | \n\n\nMAPK: 0.2224575728178024 | Loss: 2.3935029122789029 | Acc: 0.1483050847457627.\nEpoch: 6 | \n\n\nMAPK: 0.2097457051277161 | Loss: 2.3908023712998729 | Acc: 0.1313559322033898.\nEpoch: 7 | \n\n\nMAPK: 0.2125705927610397 | Loss: 2.3872123269711509 | Acc: 0.1271186440677966.\nEpoch: 8 | \n\n\nMAPK: 0.2196327298879623 | Loss: 2.3841707443786881 | Acc: 0.1271186440677966.\nFold: 2\nEpoch: 1 | \n\n\nMAPK: 0.2387005239725113 | Loss: 2.3975351341700151 | Acc: 0.1737288135593220.\nEpoch: 2 | \n\n\nMAPK: 0.2817796170711517 | Loss: 2.3968791274701133 | Acc: 0.1991525423728814.\nEpoch: 3 | \n\n\nMAPK: 0.2761299014091492 | Loss: 2.3960167250390780 | Acc: 0.1694915254237288.\nEpoch: 4 | \n\n\nMAPK: 0.2803671956062317 | Loss: 2.3947743743152943 | Acc: 0.1652542372881356.\nEpoch: 5 | \n\n\nMAPK: 0.2676552832126617 | Loss: 2.3924352536767217 | Acc: 0.1398305084745763.\nEpoch: 6 | \n\n\nMAPK: 0.2563558816909790 | Loss: 2.3891501164032243 | Acc: 0.1271186440677966.\nEpoch: 7 | \n\n\nMAPK: 0.2789547741413116 | Loss: 2.3855787293385653 | Acc: 0.1694915254237288.\nEpoch: 8 | \n\n\nMAPK: 0.2874293327331543 | Loss: 2.3826769650992699 | Acc: 0.1822033898305085.\nFold: 3\nEpoch: 1 | \n\n\nMAPK: 0.1765536814928055 | Loss: 2.3975534519906772 | Acc: 0.1063829787234043.\nEpoch: 2 | \n\n\nMAPK: 0.1786722987890244 | Loss: 2.3971492678432140 | Acc: 0.0936170212765957.\nEpoch: 3 | \n\n\nMAPK: 0.1850282549858093 | Loss: 2.3965534961829751 | Acc: 0.1191489361702128.\nEpoch: 4 | \n\n\nMAPK: 0.1822033971548080 | Loss: 2.3956931001048978 | Acc: 0.1021276595744681.\nEpoch: 5 | \n\n\nMAPK: 0.1927965879440308 | Loss: 2.3940545199281078 | Acc: 0.1191489361702128.\nEpoch: 6 | \n\n\nMAPK: 0.1857344359159470 | Loss: 2.3918487924640464 | Acc: 0.1063829787234043.\nEpoch: 7 | \n\n\nMAPK: 0.1807909309864044 | Loss: 2.3880215681205361 | Acc: 0.0851063829787234.\nEpoch: 8 | \n\n\nMAPK: 0.2323445975780487 | Loss: 2.3856813564138899 | Acc: 0.1319148936170213.\n\n\n\nmetric_name = type(fun_control[\"metric_torch\"]).__name__\nprint(f\"loss: {df_eval}, Cross-validated {metric_name}: {df_metrics}\")\n\nloss: 2.384176355297283, Cross-validated MAPK: 0.2464689016342163\n\n\n\n\n20.10.4 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nl1:  0.14155091712282247\ndropout_prob:  3.0557577535882814\nbatch_size:  100.0\nepochs:  0.1857261662232338\noptimizer:  4.718527868292797\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n20.10.5 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n                                                \nParallel coordinates plots\n\n\n\n# close tensorbaoard writer\nif fun_control[\"writer\"] is not None:\n    fun_control[\"writer\"].close()\n\n\n\n20.10.6 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
  },
  {
    "objectID": "30_spot_lightning_csv.html#sec-setup-30",
    "href": "30_spot_lightning_csv.html#sec-setup-30",
    "title": "21  HPT PyTorch Lightning: VBDP",
    "section": "21.1 Step 1: Setup",
    "text": "21.1 Step 1: Setup\nBefore we consider the detailed experimental setup, we select the parameters that affect run time, initial design size and the device that is used.\n\n\n\n\n\n\nCaution: Run time and initial design size should be increased for real experiments\n\n\n\n\nMAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\nINIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\nWORKERS is set to 0 for demonstration purposes. For real experiments, this should be increased. See the warnings that are printed when the number of workers is set to 0.\n\n\n\n\n\n\n\n\n\nNote: Device selection\n\n\n\n\nThe device can be selected by setting the variable DEVICE.\nSince we are using a simple neural net, the setting \"cpu\" is preferred (on Mac).\nIf you have a GPU, you can use \"cuda:0\" instead.\nIf DEVICE is set to \"auto\" or None, spotPython will automatically select the device.\n\nThis might result in \"mps\" on Macs, which is not the best choice for simple neural nets.\n\n\n\n\n\n\n\n\n\n\nNote: Prefix\n\n\n\n\nThe prefix PREFIX is used for the experiment name and the name of the log file.\n\n\n\n\nMAX_TIME = 1\nINIT_SIZE = 5\nDEVICE = \"auto\" #\"cpu\" # \"cuda:0\"\nWORKERS = 0\nPREFIX=\"30\"\n\n\nfrom spotPython.utils.device import getDevice\nDEVICE = getDevice(DEVICE)\nprint(DEVICE)\n\nmps\n\n\n\nimport os\nif not os.path.exists('./figures'):\n    os.makedirs('./figures')"
  },
  {
    "objectID": "30_spot_lightning_csv.html#step-2-initialization-of-the-fun_control-dictionary",
    "href": "30_spot_lightning_csv.html#step-2-initialization-of-the-fun_control-dictionary",
    "title": "21  HPT PyTorch Lightning: VBDP",
    "section": "21.2 Step 2: Initialization of the fun_control Dictionary",
    "text": "21.2 Step 2: Initialization of the fun_control Dictionary\n\n\n\n\n\n\nCaution: Tensorboard does not work under Windows\n\n\n\n\nSince tensorboard does not work under Windows, we recommend setting the parameter tensorboard_path to None if you are working under Windows.\n\n\n\nspotPython uses a Python dictionary for storing the information required for the hyperparameter tuning process, which was described in Section 14.2, see Initialization of the fun_control Dictionary in the documentation.\n\nfrom spotPython.utils.init import fun_control_init\nfrom spotPython.utils.file import get_experiment_name\nexperiment_name = get_experiment_name(prefix=PREFIX)\nfun_control = fun_control_init(task=\"classification\",\n    tensorboard_path=\"./runs/\" + experiment_name,\n    num_workers=WORKERS,\n    device=DEVICE)"
  },
  {
    "objectID": "30_spot_lightning_csv.html#sec-data-loading-30",
    "href": "30_spot_lightning_csv.html#sec-data-loading-30",
    "title": "21  HPT PyTorch Lightning: VBDP",
    "section": "21.3 Step 3: PyTorch Data Loading",
    "text": "21.3 Step 3: PyTorch Data Loading\n\n21.3.1 Lightning Dataset and DataModule\nThe data loading and preprocessing is handled by Lightning and PyTorch. It comprehends the following classes:\n\nCSVDataset: A class that loads the data from a CSV file. [SOURCE]\nCSVDataModule: A class that prepares the data for training and testing. [SOURCE]\n\n\n21.3.1.1 Taking a Look at the Data\n\nimport torch\nfrom spotPython.light.csvdataset import CSVDataset\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor\n\n# Create an instance of CSVDataset\ndataset = CSVDataset(csv_file=\"./data/VBDP/train.csv\", train=True)\n# show the dimensions of the input data\nprint(dataset[0][0].shape)\n# show the first element of the input data\nprint(dataset[0][0])\n# show the size of the dataset\nprint(f\"Dataset Size: {len(dataset)}\")\n\ntorch.Size([64])\ntensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\nDataset Size: 707\n\n\n\n# Set batch size for DataLoader\nbatch_size = 3\n# Create DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Iterate over the data in the DataLoader\nfor batch in dataloader:\n    inputs, targets = batch\n    print(f\"Batch Size: {inputs.size(0)}\")\n    print(\"---------------\")\n    print(f\"Inputs: {inputs}\")\n    print(f\"Targets: {targets}\")\n    break\n\nBatch Size: 3\n---------------\nInputs: tensor([[1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n         1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n         1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n         1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n         1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n         1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n         1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n         0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\nTargets: tensor([4, 3, 2])\n\n\n\n\n\n\n\n\nCaution: Data Loading in Lightning\n\n\n\n\nData loading is handled independently from the fun_control dictionary by Lightning and PyTorch.\nIn contrast to spotPython with torch, river and sklearn, the data sets are not added to the fun_control dictionary."
  },
  {
    "objectID": "30_spot_lightning_csv.html#sec-specification-of-preprocessing-model-30",
    "href": "30_spot_lightning_csv.html#sec-specification-of-preprocessing-model-30",
    "title": "21  HPT PyTorch Lightning: VBDP",
    "section": "21.4 Step 4: Specification of the Preprocessing Model",
    "text": "21.4 Step 4: Specification of the Preprocessing Model\nThe fun_control dictionary, the torch, sklearnand river versions of spotPython allow the specification of a data preprocessing pipeline, e.g., for the scaling of the data or for the one-hot encoding of categorical variables, see Section 14.4. This feature is not used in the Lightning version.\n\n\n\n\n\n\nCaution: Data preprocessing in Lightning\n\n\n\nLightning allows the data preprocessing to be specified in the LightningDataModule class. It is not considered here, because it should be computed at one location only."
  },
  {
    "objectID": "30_spot_lightning_csv.html#sec-selection-of-the-algorithm-30",
    "href": "30_spot_lightning_csv.html#sec-selection-of-the-algorithm-30",
    "title": "21  HPT PyTorch Lightning: VBDP",
    "section": "21.5 Step 5: Select the NN Model (algorithm) and core_model_hyper_dict",
    "text": "21.5 Step 5: Select the NN Model (algorithm) and core_model_hyper_dict\n\n21.5.1 Implementing a Configurable Neural Network With spotPython\nspotPython includes the NetLightBase class [SOURCE] for configurable neural networks. The class is imported here. It inherits from the class Lightning.LightningModule, which is the base class for all models in Lightning. Lightning.LightningModule is a subclass of torch.nn.Module and provides additional functionality for the training and testing of neural networks. The class Lightning.LightningModule is described in the Lightning documentation.\n\n\n21.5.2 Add the NN Model to the fun_control Dictionary\n\nfrom spotPython.light.netlightbase import NetLightBase \nfrom spotPython.data.light_hyper_dict import LightHyperDict\nfrom spotPython.hyperparameters.values import add_core_model_to_fun_control\nfun_control = add_core_model_to_fun_control(core_model=NetLightBase,\n                              fun_control=fun_control,\n                              hyper_dict= LightHyperDict)\n\nThe default entries for the core_model class are shown below.\n\nfun_control['core_model_hyper_dict']\n\n{'l1': {'type': 'int',\n  'default': 3,\n  'transform': 'transform_power_2_int',\n  'lower': 3,\n  'upper': 8},\n 'epochs': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 4,\n  'upper': 9},\n 'batch_size': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 1,\n  'upper': 4},\n 'act_fn': {'levels': ['Sigmoid', 'Tanh', 'ReLU', 'LeakyReLU', 'ELU', 'Swish'],\n  'type': 'factor',\n  'default': 'ReLU',\n  'transform': 'None',\n  'class_name': 'spotPython.torch.activation',\n  'core_model_parameter_type': 'instance()',\n  'lower': 0,\n  'upper': 5},\n 'optimizer': {'levels': ['Adadelta',\n   'Adagrad',\n   'Adam',\n   'AdamW',\n   'SparseAdam',\n   'Adamax',\n   'ASGD',\n   'NAdam',\n   'RAdam',\n   'RMSprop',\n   'Rprop',\n   'SGD'],\n  'type': 'factor',\n  'default': 'SGD',\n  'transform': 'None',\n  'class_name': 'torch.optim',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 11},\n 'dropout_prob': {'type': 'float',\n  'default': 0.01,\n  'transform': 'None',\n  'lower': 0.0,\n  'upper': 0.1},\n 'lr_mult': {'type': 'float',\n  'default': 1.0,\n  'transform': 'None',\n  'lower': 0.1,\n  'upper': 10.0},\n 'patience': {'type': 'int',\n  'default': 2,\n  'transform': 'transform_power_2_int',\n  'lower': 2,\n  'upper': 6},\n 'initialization': {'levels': ['Default', 'Kaiming', 'Xavier'],\n  'type': 'factor',\n  'default': 'Default',\n  'transform': 'None',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 2}}\n\n\nThe NetLightBase is a configurable neural network. The hyperparameters of the model are specified in the core_model_hyper_dict dictionary [SOURCE]."
  },
  {
    "objectID": "30_spot_lightning_csv.html#sec-modification-of-hyperparameters-30",
    "href": "30_spot_lightning_csv.html#sec-modification-of-hyperparameters-30",
    "title": "21  HPT PyTorch Lightning: VBDP",
    "section": "21.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model",
    "text": "21.6 Step 6: Modify hyper_dict Hyperparameters for the Selected Algorithm aka core_model\nspotPython provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in Section 14.6.\n\n\n\n\n\n\nCaution: Small number of epochs for demonstration purposes\n\n\n\n\nepochs and patience are set to small values for demonstration purposes. These values are too small for a real application.\nMore resonable values are, e.g.:\n\nfun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[7, 9]) and\nfun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[2, 7])\n\n\n\n\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n\nfun_control = modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[7,10])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[5,7])\nfun_control = modify_hyper_parameter_bounds(fun_control, \"batch_size\", bounds=[3, 8])\n\n\nfrom spotPython.hyperparameters.values import modify_hyper_parameter_levels\nfun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n# fun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\", [\"Adam\"])\n\nThe updated fun_control dictionary is shown below.\n\nfun_control[\"core_model_hyper_dict\"]\n\n{'l1': {'type': 'int',\n  'default': 3,\n  'transform': 'transform_power_2_int',\n  'lower': 7,\n  'upper': 10},\n 'epochs': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 5,\n  'upper': 7},\n 'batch_size': {'type': 'int',\n  'default': 4,\n  'transform': 'transform_power_2_int',\n  'lower': 3,\n  'upper': 8},\n 'act_fn': {'levels': ['Sigmoid', 'Tanh', 'ReLU', 'LeakyReLU', 'ELU', 'Swish'],\n  'type': 'factor',\n  'default': 'ReLU',\n  'transform': 'None',\n  'class_name': 'spotPython.torch.activation',\n  'core_model_parameter_type': 'instance()',\n  'lower': 0,\n  'upper': 5},\n 'optimizer': {'levels': ['Adam', 'AdamW', 'Adamax', 'NAdam'],\n  'type': 'factor',\n  'default': 'SGD',\n  'transform': 'None',\n  'class_name': 'torch.optim',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 3},\n 'dropout_prob': {'type': 'float',\n  'default': 0.01,\n  'transform': 'None',\n  'lower': 0.0,\n  'upper': 0.1},\n 'lr_mult': {'type': 'float',\n  'default': 1.0,\n  'transform': 'None',\n  'lower': 0.1,\n  'upper': 10.0},\n 'patience': {'type': 'int',\n  'default': 2,\n  'transform': 'transform_power_2_int',\n  'lower': 2,\n  'upper': 6},\n 'initialization': {'levels': ['Default', 'Kaiming', 'Xavier'],\n  'type': 'factor',\n  'default': 'Default',\n  'transform': 'None',\n  'core_model_parameter_type': 'str',\n  'lower': 0,\n  'upper': 2}}"
  },
  {
    "objectID": "30_spot_lightning_csv.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "href": "30_spot_lightning_csv.html#step-7-data-splitting-the-objective-loss-function-and-the-metric",
    "title": "21  HPT PyTorch Lightning: VBDP",
    "section": "21.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric",
    "text": "21.7 Step 7: Data Splitting, the Objective (Loss) Function and the Metric\n\n21.7.1 Evaluation\nThe evaluation procedure requires the specification of two elements:\n\nthe way how the data is split into a train and a test set (see Section 14.7.1)\nthe loss function (and a metric).\n\n\n\n\n\n\n\nCaution: Data Splitting in Lightning\n\n\n\n\nThe data splitting is handled by Lightning.\n\n\n\n\n\n21.7.2 Loss Functions and Metrics\nThe loss function is specified in the configurable network class [SOURCE] We will use CrossEntropy loss for the multiclass-classification task.\n\n\n21.7.3 Metric\n\nWe will use the MAP@k metric [SOURCE] for the evaluation of the model. Here is an example how this metric is calculated.\n\n\nfrom spotPython.torch.mapk import MAPK\nimport torch\nmapk = MAPK(k=2)\ntarget = torch.tensor([0, 1, 2, 2])\npreds = torch.tensor(\n    [\n        [0.5, 0.2, 0.2],  # 0 is in top 2\n        [0.3, 0.4, 0.2],  # 1 is in top 2\n        [0.2, 0.4, 0.3],  # 2 is in top 2\n        [0.7, 0.2, 0.1],  # 2 isn't in top 2\n    ]\n)\nmapk.update(preds, target)\nprint(mapk.compute()) # tensor(0.6250)\n\ntensor(0.6250)\n\n\nSimilar to the loss function, the metric is specified in the configurable network class [SOURCE].\n\n\n\n\n\n\nCaution: Loss Function and Metric in Lightning\n\n\n\n\nThe loss function and the metric are not hyperparameters that can be tuned with spotPython.\nThey are handled by Lightning."
  },
  {
    "objectID": "30_spot_lightning_csv.html#step-8-calling-the-spot-function",
    "href": "30_spot_lightning_csv.html#step-8-calling-the-spot-function",
    "title": "21  HPT PyTorch Lightning: VBDP",
    "section": "21.8 Step 8: Calling the SPOT Function",
    "text": "21.8 Step 8: Calling the SPOT Function\n\n21.8.1 Preparing the SPOT Call\nThe following code passes the information about the parameter ranges and bounds to spot. It extracts the variable types, names, and bounds\n\nfrom spotPython.hyperparameters.values import (get_bound_values,\n    get_var_name,\n    get_var_type,)\nvar_type = get_var_type(fun_control)\nvar_name = get_var_name(fun_control)\nfun_control.update({\"var_type\": var_type,\n                    \"var_name\": var_name})\nlower = get_bound_values(fun_control, \"lower\")\nupper = get_bound_values(fun_control, \"upper\")\n\nNow, the dictionary fun_control contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method gen_design_table [SOURCE] generates a design table as follows:\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control))\n\n| name           | type   | default   |   lower |   upper | transform             |\n|----------------|--------|-----------|---------|---------|-----------------------|\n| l1             | int    | 3         |     7   |    10   | transform_power_2_int |\n| epochs         | int    | 4         |     5   |     7   | transform_power_2_int |\n| batch_size     | int    | 4         |     3   |     8   | transform_power_2_int |\n| act_fn         | factor | ReLU      |     0   |     5   | None                  |\n| optimizer      | factor | SGD       |     0   |     3   | None                  |\n| dropout_prob   | float  | 0.01      |     0   |     0.1 | None                  |\n| lr_mult        | float  | 1.0       |     0.1 |    10   | None                  |\n| patience       | int    | 2         |     2   |     6   | transform_power_2_int |\n| initialization | factor | Default   |     0   |     2   | None                  |\n\n\nThis allows to check if all information is available and if the information is correct.\n\n\n21.8.2 The Objective Function fun\nThe objective function fun from the class HyperLight [SOURCE] is selected next. It implements an interface from PyTorch’s training, validation, and testing methods to spotPython.\n\nfrom spotPython.light.hyperlight import HyperLight\nfun = HyperLight().fun\n\n\n\n21.8.3 Starting the Hyperparameter Tuning\nThe spotPython hyperparameter tuning is started by calling the Spot function [SOURCE] as described in Section 14.8.4.\n\nimport numpy as np\nfrom spotPython.spot import spot\nfrom math import inf\nspot_tuner = spot.Spot(fun=fun,\n                   lower = lower,\n                   upper = upper,\n                   fun_evals = inf,\n                   fun_repeats = 1,\n                   max_time = MAX_TIME,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type = var_type,\n                   var_name = var_name,\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models= False,\n                   show_progress= True,\n                   fun_control = fun_control,\n                   design_control={\"init_size\": INIT_SIZE,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": True,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": len(var_name),\n                                      \"model_fun_evals\": 10_000,\n                                      \"log_level\": 50\n                                      })\nspot_tuner.run()\n\n\nconfig: {'l1': 1024, 'epochs': 128, 'batch_size': 64, 'act_fn': ReLU(), 'optimizer': 'AdamW', 'dropout_prob': 0.04375810986688453, 'lr_mult': 4.211776903906428, 'patience': 16, 'initialization': 'Default'}\nmodel: NetLightBase(\n  (act_fn): ReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=1024, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.04375810986688453, inplace=False)\n    (3): Linear(in_features=1024, out_features=512, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.04375810986688453, inplace=False)\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.04375810986688453, inplace=False)\n    (9): Linear(in_features=512, out_features=256, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.04375810986688453, inplace=False)\n    (12): Linear(in_features=256, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.3234684467315674     │\n│          val_acc          │    0.21908126771450043    │\n│         val_loss          │    2.3234684467315674     │\n│        valid_mapk         │    0.28458720445632935    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.28458720445632935, 'val_loss': 2.3234684467315674, 'val_acc': 0.21908126771450043, 'hp_metric': 2.3234684467315674}\n\nconfig: {'l1': 128, 'epochs': 32, 'batch_size': 256, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.005170658955305807, 'lr_mult': 0.832718394912432, 'patience': 8, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005170658955305807, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005170658955305807, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005170658955305807, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005170658955305807, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.2648348808288574     │\n│          val_acc          │    0.27915194630622864    │\n│         val_loss          │    2.2648348808288574     │\n│        valid_mapk         │    0.3821132183074951     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.3821132183074951, 'val_loss': 2.2648348808288574, 'val_acc': 0.27915194630622864, 'hp_metric': 2.2648348808288574}\n\nconfig: {'l1': 512, 'epochs': 64, 'batch_size': 16, 'act_fn': Swish(), 'optimizer': 'NAdam', 'dropout_prob': 0.08834550718769361, 'lr_mult': 7.65501078489161, 'patience': 64, 'initialization': 'Xavier'}\nmodel: NetLightBase(\n  (act_fn): Swish()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=512, bias=True)\n    (1): Swish()\n    (2): Dropout(p=0.08834550718769361, inplace=False)\n    (3): Linear(in_features=512, out_features=256, bias=True)\n    (4): Swish()\n    (5): Dropout(p=0.08834550718769361, inplace=False)\n    (6): Linear(in_features=256, out_features=256, bias=True)\n    (7): Swish()\n    (8): Dropout(p=0.08834550718769361, inplace=False)\n    (9): Linear(in_features=256, out_features=128, bias=True)\n    (10): Swish()\n    (11): Dropout(p=0.08834550718769361, inplace=False)\n    (12): Linear(in_features=128, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.4511682987213135     │\n│          val_acc          │    0.09187278896570206    │\n│         val_loss          │    2.4511682987213135     │\n│        valid_mapk         │    0.1778198629617691     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.1778198629617691, 'val_loss': 2.4511682987213135, 'val_acc': 0.09187278896570206, 'hp_metric': 2.4511682987213135}\n\nconfig: {'l1': 256, 'epochs': 64, 'batch_size': 16, 'act_fn': Sigmoid(), 'optimizer': 'Adam', 'dropout_prob': 0.07563714253500024, 'lr_mult': 2.3450676871382794, 'patience': 32, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): Sigmoid()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=256, bias=True)\n    (1): Sigmoid()\n    (2): Dropout(p=0.07563714253500024, inplace=False)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Sigmoid()\n    (5): Dropout(p=0.07563714253500024, inplace=False)\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): Sigmoid()\n    (8): Dropout(p=0.07563714253500024, inplace=False)\n    (9): Linear(in_features=128, out_features=64, bias=True)\n    (10): Sigmoid()\n    (11): Dropout(p=0.07563714253500024, inplace=False)\n    (12): Linear(in_features=64, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.2770135402679443     │\n│          val_acc          │    0.2614840865135193     │\n│         val_loss          │    2.2770135402679443     │\n│        valid_mapk         │    0.34932658076286316    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.34932658076286316, 'val_loss': 2.2770135402679443, 'val_acc': 0.2614840865135193, 'hp_metric': 2.2770135402679443}\n\nconfig: {'l1': 256, 'epochs': 128, 'batch_size': 128, 'act_fn': ReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.02833523179697884, 'lr_mult': 9.528945328733357, 'patience': 4, 'initialization': 'Xavier'}\nmodel: NetLightBase(\n  (act_fn): ReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.02833523179697884, inplace=False)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.02833523179697884, inplace=False)\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.02833523179697884, inplace=False)\n    (9): Linear(in_features=128, out_features=64, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.02833523179697884, inplace=False)\n    (12): Linear(in_features=64, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2.302140951156616     │\n│          val_acc          │    0.23674911260604858    │\n│         val_loss          │     2.302140951156616     │\n│        valid_mapk         │    0.3054752051830292     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.3054752051830292, 'val_loss': 2.302140951156616, 'val_acc': 0.23674911260604858, 'hp_metric': 2.302140951156616}\n\n\n\nconfig: {'l1': 128, 'epochs': 32, 'batch_size': 128, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.00147908939268937, 'lr_mult': 1.0752266899289973, 'patience': 16, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.00147908939268937, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.00147908939268937, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.00147908939268937, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.00147908939268937, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.2760536670684814     │\n│          val_acc          │    0.24381625652313232    │\n│         val_loss          │    2.2760536670684814     │\n│        valid_mapk         │     0.385239839553833     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.385239839553833, 'val_loss': 2.2760536670684814, 'val_acc': 0.24381625652313232, 'hp_metric': 2.2760536670684814}\n\n\nspotPython tuning: 2.2648348808288574 [----------] 4.54% \n\n\n\nconfig: {'l1': 128, 'epochs': 64, 'batch_size': 256, 'act_fn': LeakyReLU(), 'optimizer': 'AdamW', 'dropout_prob': 0.0, 'lr_mult': 6.5400267229367035, 'patience': 4, 'initialization': 'Default'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.0, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.0, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.0, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.0, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.2655727863311768     │\n│          val_acc          │    0.26501765847206116    │\n│         val_loss          │    2.2655727863311768     │\n│        valid_mapk         │    0.3856698274612427     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.3856698274612427, 'val_loss': 2.2655727863311768, 'val_acc': 0.26501765847206116, 'hp_metric': 2.2655727863311768}\n\n\nspotPython tuning: 2.2648348808288574 [#---------] 10.61% \n\n\n\nconfig: {'l1': 128, 'epochs': 32, 'batch_size': 256, 'act_fn': Sigmoid(), 'optimizer': 'Adam', 'dropout_prob': 0.1, 'lr_mult': 0.10000000000000053, 'patience': 8, 'initialization': 'Kaiming'}\n\n\n\nmodel: NetLightBase(\n  (act_fn): Sigmoid()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): Sigmoid()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): Sigmoid()\n    (5): Dropout(p=0.1, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): Sigmoid()\n    (8): Dropout(p=0.1, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): Sigmoid()\n    (11): Dropout(p=0.1, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.3945162296295166     │\n│          val_acc          │    0.11307420581579208    │\n│         val_loss          │    2.3945162296295166     │\n│        valid_mapk         │    0.2007981240749359     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.2007981240749359, 'val_loss': 2.3945162296295166, 'val_acc': 0.11307420581579208, 'hp_metric': 2.3945162296295166}\n\n\nspotPython tuning: 2.2648348808288574 [##--------] 16.76% \n\n\n\nconfig: {'l1': 128, 'epochs': 32, 'batch_size': 256, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.1, 'lr_mult': 0.10000000000000009, 'patience': 32, 'initialization': 'Default'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.1, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.1, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.1, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.3974363803863525     │\n│          val_acc          │    0.15547703206539154    │\n│         val_loss          │    2.3974363803863525     │\n│        valid_mapk         │    0.16636526584625244    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.16636526584625244, 'val_loss': 2.3974363803863525, 'val_acc': 0.15547703206539154, 'hp_metric': 2.3974363803863525}\n\n\nspotPython tuning: 2.2648348808288574 [##--------] 20.96% \n\n\n\nconfig: {'l1': 128, 'epochs': 64, 'batch_size': 256, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.005410283967863825, 'lr_mult': 6.748234140091307, 'patience': 32, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2.235407590866089     │\n│          val_acc          │    0.30035334825515747    │\n│         val_loss          │     2.235407590866089     │\n│        valid_mapk         │    0.4498577117919922     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.4498577117919922, 'val_loss': 2.235407590866089, 'val_acc': 0.30035334825515747, 'hp_metric': 2.235407590866089}\n\n\nspotPython tuning: 2.235407590866089 [###-------] 25.23% \n\n\n\nconfig: {'l1': 256, 'epochs': 64, 'batch_size': 16, 'act_fn': LeakyReLU(), 'optimizer': 'AdamW', 'dropout_prob': 0.01092951864482135, 'lr_mult': 10.0, 'patience': 32, 'initialization': 'Default'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=256, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.01092951864482135, inplace=False)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.01092951864482135, inplace=False)\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.01092951864482135, inplace=False)\n    (9): Linear(in_features=128, out_features=64, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.01092951864482135, inplace=False)\n    (12): Linear(in_features=64, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2.401698350906372     │\n│          val_acc          │    0.14134275913238525    │\n│         val_loss          │     2.401698350906372     │\n│        valid_mapk         │    0.22632575035095215    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.22632575035095215, 'val_loss': 2.401698350906372, 'val_acc': 0.14134275913238525, 'hp_metric': 2.401698350906372}\n\n\nspotPython tuning: 2.235407590866089 [####------] 35.39% \n\n\n\nconfig: {'l1': 128, 'epochs': 64, 'batch_size': 128, 'act_fn': LeakyReLU(), 'optimizer': 'NAdam', 'dropout_prob': 0.00899518733143736, 'lr_mult': 10.0, 'patience': 32, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.00899518733143736, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.00899518733143736, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.00899518733143736, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.00899518733143736, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.4260430335998535     │\n│          val_acc          │    0.11660777032375336    │\n│         val_loss          │    2.4260430335998535     │\n│        valid_mapk         │    0.20526300370693207    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.20526300370693207, 'val_loss': 2.4260430335998535, 'val_acc': 0.11660777032375336, 'hp_metric': 2.4260430335998535}\n\n\nspotPython tuning: 2.235407590866089 [####------] 39.41% \n\n\n\nconfig: {'l1': 256, 'epochs': 64, 'batch_size': 128, 'act_fn': LeakyReLU(), 'optimizer': 'AdamW', 'dropout_prob': 0.010835406635652793, 'lr_mult': 6.906652374057626, 'patience': 64, 'initialization': 'Default'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=256, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.010835406635652793, inplace=False)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.010835406635652793, inplace=False)\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.010835406635652793, inplace=False)\n    (9): Linear(in_features=128, out_features=64, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.010835406635652793, inplace=False)\n    (12): Linear(in_features=64, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.3303024768829346     │\n│          val_acc          │    0.21201413869857788    │\n│         val_loss          │    2.3303024768829346     │\n│        valid_mapk         │    0.3230452537536621     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.3230452537536621, 'val_loss': 2.3303024768829346, 'val_acc': 0.21201413869857788, 'hp_metric': 2.3303024768829346}\n\n\nspotPython tuning: 2.235407590866089 [#####-----] 46.70% \n\n\n\nconfig: {'l1': 256, 'epochs': 32, 'batch_size': 128, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.010168422249530674, 'lr_mult': 0.3979310825141637, 'patience': 4, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=256, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.010168422249530674, inplace=False)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.010168422249530674, inplace=False)\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.010168422249530674, inplace=False)\n    (9): Linear(in_features=128, out_features=64, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.010168422249530674, inplace=False)\n    (12): Linear(in_features=64, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.2467353343963623     │\n│          val_acc          │    0.30388692021369934    │\n│         val_loss          │    2.2467353343963623     │\n│        valid_mapk         │    0.4136767089366913     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.4136767089366913, 'val_loss': 2.2467353343963623, 'val_acc': 0.30388692021369934, 'hp_metric': 2.2467353343963623}\n\n\nspotPython tuning: 2.235407590866089 [#####-----] 50.64% \n\n\n\nconfig: {'l1': 128, 'epochs': 64, 'batch_size': 16, 'act_fn': ReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.0, 'lr_mult': 6.251645303354199, 'patience': 8, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): ReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.0, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.0, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.0, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.0, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2.317075490951538     │\n│          val_acc          │    0.21554769575595856    │\n│         val_loss          │     2.317075490951538     │\n│        valid_mapk         │    0.3116056025028229     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.3116056025028229, 'val_loss': 2.317075490951538, 'val_acc': 0.21554769575595856, 'hp_metric': 2.317075490951538}\n\n\nspotPython tuning: 2.235407590866089 [######----] 55.26% \n\n\n\nconfig: {'l1': 256, 'epochs': 32, 'batch_size': 128, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.017032402495548216, 'lr_mult': 0.1, 'patience': 4, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=256, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.017032402495548216, inplace=False)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.017032402495548216, inplace=False)\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.017032402495548216, inplace=False)\n    (9): Linear(in_features=128, out_features=64, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.017032402495548216, inplace=False)\n    (12): Linear(in_features=64, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2.329146146774292     │\n│          val_acc          │    0.25088340044021606    │\n│         val_loss          │     2.329146146774292     │\n│        valid_mapk         │    0.34129050374031067    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.34129050374031067, 'val_loss': 2.329146146774292, 'val_acc': 0.25088340044021606, 'hp_metric': 2.329146146774292}\n\n\nspotPython tuning: 2.235407590866089 [######----] 58.92% \n\n\n\nconfig: {'l1': 1024, 'epochs': 64, 'batch_size': 128, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.008891593286542805, 'lr_mult': 0.4334837589006342, 'patience': 8, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=1024, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.008891593286542805, inplace=False)\n    (3): Linear(in_features=1024, out_features=512, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.008891593286542805, inplace=False)\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.008891593286542805, inplace=False)\n    (9): Linear(in_features=512, out_features=256, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.008891593286542805, inplace=False)\n    (12): Linear(in_features=256, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.2644355297088623     │\n│          val_acc          │     0.268551230430603     │\n│         val_loss          │    2.2644355297088623     │\n│        valid_mapk         │    0.40575167536735535    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.40575167536735535, 'val_loss': 2.2644355297088623, 'val_acc': 0.268551230430603, 'hp_metric': 2.2644355297088623}\n\n\nspotPython tuning: 2.235407590866089 [######----] 62.85% \n\n\n\nconfig: {'l1': 512, 'epochs': 64, 'batch_size': 16, 'act_fn': Sigmoid(), 'optimizer': 'AdamW', 'dropout_prob': 0.003937815969805521, 'lr_mult': 6.755223507633317, 'patience': 32, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): Sigmoid()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=512, bias=True)\n    (1): Sigmoid()\n    (2): Dropout(p=0.003937815969805521, inplace=False)\n    (3): Linear(in_features=512, out_features=256, bias=True)\n    (4): Sigmoid()\n    (5): Dropout(p=0.003937815969805521, inplace=False)\n    (6): Linear(in_features=256, out_features=256, bias=True)\n    (7): Sigmoid()\n    (8): Dropout(p=0.003937815969805521, inplace=False)\n    (9): Linear(in_features=256, out_features=128, bias=True)\n    (10): Sigmoid()\n    (11): Dropout(p=0.003937815969805521, inplace=False)\n    (12): Linear(in_features=128, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2.292844295501709     │\n│          val_acc          │    0.21908126771450043    │\n│         val_loss          │     2.292844295501709     │\n│        valid_mapk         │     0.310079962015152     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.310079962015152, 'val_loss': 2.292844295501709, 'val_acc': 0.21908126771450043, 'hp_metric': 2.292844295501709}\n\n\nspotPython tuning: 2.235407590866089 [########--] 79.67% \n\n\n\nconfig: {'l1': 1024, 'epochs': 64, 'batch_size': 128, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.00969894222013007, 'lr_mult': 0.4584612086889453, 'patience': 8, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=1024, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.00969894222013007, inplace=False)\n    (3): Linear(in_features=1024, out_features=512, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.00969894222013007, inplace=False)\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.00969894222013007, inplace=False)\n    (9): Linear(in_features=512, out_features=256, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.00969894222013007, inplace=False)\n    (12): Linear(in_features=256, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.2968060970306396     │\n│          val_acc          │    0.24028268456459045    │\n│         val_loss          │    2.2968060970306396     │\n│        valid_mapk         │    0.3265174925327301     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.3265174925327301, 'val_loss': 2.2968060970306396, 'val_acc': 0.24028268456459045, 'hp_metric': 2.2968060970306396}\n\n\nspotPython tuning: 2.235407590866089 [########--] 84.30% \n\n\n\nconfig: {'l1': 1024, 'epochs': 64, 'batch_size': 32, 'act_fn': LeakyReLU(), 'optimizer': 'AdamW', 'dropout_prob': 0.0030486518813622513, 'lr_mult': 6.651775910569058, 'patience': 8, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=1024, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.0030486518813622513, inplace=False)\n    (3): Linear(in_features=1024, out_features=512, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.0030486518813622513, inplace=False)\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.0030486518813622513, inplace=False)\n    (9): Linear(in_features=512, out_features=256, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.0030486518813622513, inplace=False)\n    (12): Linear(in_features=256, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.4547014236450195     │\n│          val_acc          │    0.08833922445774078    │\n│         val_loss          │    2.4547014236450195     │\n│        valid_mapk         │    0.17118912935256958    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.17118912935256958, 'val_loss': 2.4547014236450195, 'val_acc': 0.08833922445774078, 'hp_metric': 2.4547014236450195}\n\n\nspotPython tuning: 2.235407590866089 [#########-] 88.97% \n\n\n\nconfig: {'l1': 512, 'epochs': 128, 'batch_size': 256, 'act_fn': LeakyReLU(), 'optimizer': 'NAdam', 'dropout_prob': 0.0, 'lr_mult': 4.0047452006296504, 'patience': 32, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=512, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.0, inplace=False)\n    (3): Linear(in_features=512, out_features=256, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.0, inplace=False)\n    (6): Linear(in_features=256, out_features=256, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.0, inplace=False)\n    (9): Linear(in_features=256, out_features=128, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.0, inplace=False)\n    (12): Linear(in_features=128, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2.321242570877075     │\n│          val_acc          │    0.2226148396730423     │\n│         val_loss          │     2.321242570877075     │\n│        valid_mapk         │     0.298382043838501     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.298382043838501, 'val_loss': 2.321242570877075, 'val_acc': 0.2226148396730423, 'hp_metric': 2.321242570877075}\n\n\nspotPython tuning: 2.235407590866089 [#########-] 94.39% \n\n\n\nconfig: {'l1': 1024, 'epochs': 64, 'batch_size': 128, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.005473793889002229, 'lr_mult': 0.44915068254485657, 'patience': 8, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=1024, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005473793889002229, inplace=False)\n    (3): Linear(in_features=1024, out_features=512, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005473793889002229, inplace=False)\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005473793889002229, inplace=False)\n    (9): Linear(in_features=512, out_features=256, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005473793889002229, inplace=False)\n    (12): Linear(in_features=256, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.2467963695526123     │\n│          val_acc          │    0.2968197762966156     │\n│         val_loss          │    2.2467963695526123     │\n│        valid_mapk         │     0.377861350774765     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.377861350774765, 'val_loss': 2.2467963695526123, 'val_acc': 0.2968197762966156, 'hp_metric': 2.2467963695526123}\n\n\nspotPython tuning: 2.235407590866089 [##########] 99.95% \n\n\n\nconfig: {'l1': 1024, 'epochs': 64, 'batch_size': 256, 'act_fn': LeakyReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.005739155710200516, 'lr_mult': 6.552194949351319, 'patience': 4, 'initialization': 'Kaiming'}\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=1024, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005739155710200516, inplace=False)\n    (3): Linear(in_features=1024, out_features=512, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005739155710200516, inplace=False)\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005739155710200516, inplace=False)\n    (9): Linear(in_features=512, out_features=256, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005739155710200516, inplace=False)\n    (12): Linear(in_features=256, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2.444101095199585     │\n│          val_acc          │    0.0989399328827858     │\n│         val_loss          │     2.444101095199585     │\n│        valid_mapk         │    0.15774497389793396    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.15774497389793396, 'val_loss': 2.444101095199585, 'val_acc': 0.0989399328827858, 'hp_metric': 2.444101095199585}\n\n\nspotPython tuning: 2.235407590866089 [##########] 100.00% Done...\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x18ddddb40&gt;"
  },
  {
    "objectID": "30_spot_lightning_csv.html#sec-tensorboard-30",
    "href": "30_spot_lightning_csv.html#sec-tensorboard-30",
    "title": "21  HPT PyTorch Lightning: VBDP",
    "section": "21.9 Step 9: Tensorboard",
    "text": "21.9 Step 9: Tensorboard\nThe textual output shown in the console (or code cell) can be visualized with Tensorboard as described in Section 14.9, see also the description in the documentation: Tensorboard."
  },
  {
    "objectID": "30_spot_lightning_csv.html#sec-results-30",
    "href": "30_spot_lightning_csv.html#sec-results-30",
    "title": "21  HPT PyTorch Lightning: VBDP",
    "section": "21.10 Step 10: Results",
    "text": "21.10 Step 10: Results\nAfter the hyperparameter tuning run is finished, the results can be analyzed as described in Section 14.10.\n\nspot_tuner.plot_progress(log_y=False,\n    filename=\"./figures/\" + experiment_name+\"_progress.png\")\n\n\n\n\nProgress plot. Black dots denote results from the initial design. Red dots illustrate the improvement found by the surrogate model based optimization.\n\n\n\n\n\nfrom spotPython.utils.eda import gen_design_table\nprint(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n\n| name           | type   | default   |   lower |   upper |                tuned | transform             |   importance | stars   |\n|----------------|--------|-----------|---------|---------|----------------------|-----------------------|--------------|---------|\n| l1             | int    | 3         |     7.0 |    10.0 |                  7.0 | transform_power_2_int |         0.22 | .       |\n| epochs         | int    | 4         |     5.0 |     7.0 |                  6.0 | transform_power_2_int |         0.00 |         |\n| batch_size     | int    | 4         |     3.0 |     8.0 |                  8.0 | transform_power_2_int |         0.00 |         |\n| act_fn         | factor | ReLU      |     0.0 |     5.0 |                  3.0 | None                  |       100.00 | ***     |\n| optimizer      | factor | SGD       |     0.0 |     3.0 |                  2.0 | None                  |         0.00 |         |\n| dropout_prob   | float  | 0.01      |     0.0 |     0.1 | 0.005410283967863825 | None                  |         7.34 | *       |\n| lr_mult        | float  | 1.0       |     0.1 |    10.0 |    6.748234140091307 | None                  |         5.47 | *       |\n| patience       | int    | 2         |     2.0 |     6.0 |                  5.0 | transform_power_2_int |         0.00 |         |\n| initialization | factor | Default   |     0.0 |     2.0 |                  1.0 | None                  |         0.00 |         |\n\n\n\nspot_tuner.plot_importance(threshold=0.025,\n    filename=\"./figures/\" + experiment_name+\"_importance.png\")\n\n\n\n\nVariable importance plot, threshold 0.025.\n\n\n\n\n\n21.10.1 Get the Tuned Architecture\n\nfrom spotPython.hyperparameters.values import get_one_config_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nconfig = get_one_config_from_X(X, fun_control)\nconfig\n\n{'l1': 128,\n 'epochs': 64,\n 'batch_size': 256,\n 'act_fn': LeakyReLU(),\n 'optimizer': 'Adamax',\n 'dropout_prob': 0.005410283967863825,\n 'lr_mult': 6.748234140091307,\n 'patience': 32,\n 'initialization': 'Kaiming'}\n\n\n\nTest on the full data set\n\n\nfrom spotPython.light.traintest import test_model\ntest_model(config, fun_control)\n\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│      test_mapk_epoch      │    0.5770321488380432     │\n│          val_acc          │    0.5261669158935547     │\n│         val_loss          │    2.0120155811309814     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntest_model result: {'test_mapk_epoch': 0.5770321488380432, 'val_loss': 2.0120155811309814, 'val_acc': 0.5261669158935547}\n\n\n(2.0120155811309814, 0.5261669158935547)\n\n\n\n\n21.10.2 Cross Validation With Lightning\n\nThe KFold class from sklearn.model_selection is used to generate the folds for cross-validation.\nThese mechanism is used to generate the folds for the final evaluation of the model.\nThe CrossValidationDataModule class [SOURCE] is used to generate the folds for the hyperparameter tuning process.\nIt is called from the cv_model function [SOURCE].\n\n\nfrom spotPython.light.traintest import cv_model\ncv_model(config, fun_control)\n\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\nk: 0\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.2361419200897217     │\n│          val_acc          │    0.30985915660858154    │\n│         val_loss          │    2.2361419200897217     │\n│        valid_mapk         │    0.4154929518699646     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.4154929518699646, 'val_loss': 2.2361419200897217, 'val_acc': 0.30985915660858154, 'hp_metric': 2.2361419200897217}\nk: 1\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     2.023916244506836     │\n│          val_acc          │    0.5352112650871277     │\n│         val_loss          │     2.023916244506836     │\n│        valid_mapk         │    0.6103286147117615     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.6103286147117615, 'val_loss': 2.023916244506836, 'val_acc': 0.5352112650871277, 'hp_metric': 2.023916244506836}\nk: 2\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    2.0173938274383545     │\n│          val_acc          │    0.5352112650871277     │\n│         val_loss          │    2.0173938274383545     │\n│        valid_mapk         │    0.6197183132171631     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.6197183132171631, 'val_loss': 2.0173938274383545, 'val_acc': 0.5352112650871277, 'hp_metric': 2.0173938274383545}\nk: 3\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    1.8894219398498535     │\n│          val_acc          │    0.6338028311729431     │\n│         val_loss          │    1.8894219398498535     │\n│        valid_mapk         │    0.7159624695777893     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.7159624695777893, 'val_loss': 1.8894219398498535, 'val_acc': 0.6338028311729431, 'hp_metric': 1.8894219398498535}\nk: 4\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    1.8175771236419678     │\n│          val_acc          │    0.7183098793029785     │\n│         val_loss          │    1.8175771236419678     │\n│        valid_mapk         │    0.7652581930160522     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.7652581930160522, 'val_loss': 1.8175771236419678, 'val_acc': 0.7183098793029785, 'hp_metric': 1.8175771236419678}\nk: 5\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    1.8304204940795898     │\n│          val_acc          │    0.7183098793029785     │\n│         val_loss          │    1.8304204940795898     │\n│        valid_mapk         │    0.7558685541152954     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.7558685541152954, 'val_loss': 1.8304204940795898, 'val_acc': 0.7183098793029785, 'hp_metric': 1.8304204940795898}\nk: 6\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    1.7933049201965332     │\n│          val_acc          │    0.7605633735656738     │\n│         val_loss          │    1.7933049201965332     │\n│        valid_mapk         │    0.7981220483779907     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.7981220483779907, 'val_loss': 1.7933049201965332, 'val_acc': 0.7605633735656738, 'hp_metric': 1.7933049201965332}\nk: 7\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    1.6774734258651733     │\n│          val_acc          │    0.8714285492897034     │\n│         val_loss          │    1.6774734258651733     │\n│        valid_mapk         │    0.8809523582458496     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.8809523582458496, 'val_loss': 1.6774734258651733, 'val_acc': 0.8714285492897034, 'hp_metric': 1.6774734258651733}\nk: 8\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │    1.7264482975006104     │\n│          val_acc          │    0.8142856955528259     │\n│         val_loss          │    1.7264482975006104     │\n│        valid_mapk         │    0.8428571224212646     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.8428571224212646, 'val_loss': 1.7264482975006104, 'val_acc': 0.8142856955528259, 'hp_metric': 1.7264482975006104}\nk: 9\nmodel: NetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃      Validate metric      ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         hp_metric         │     1.755602240562439     │\n│          val_acc          │    0.7857142686843872     │\n│         val_loss          │     1.755602240562439     │\n│        valid_mapk         │    0.8166666626930237     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\ntrain_model result: {'valid_mapk': 0.8166666626930237, 'val_loss': 1.755602240562439, 'val_acc': 0.7857142686843872, 'hp_metric': 1.755602240562439}\ncv_model mapk result: 0.7221227288246155\n\n\n0.7221227288246155\n\n\n\n\n\n\n\n\nNote: Evaluation for the Final Comaprison\n\n\n\n\nThis is the evaluation that will be used in the comparison.\n\n\n\n\n\n21.10.3 Detailed Hyperparameter Plots\n\nfilename = \"./figures/\" + experiment_name\nspot_tuner.plot_important_hyperparameter_contour(filename=filename)\n\nl1:  0.21696178534277133\nact_fn:  100.00000000000001\ndropout_prob:  7.338197881872086\nlr_mult:  5.466810662183265\n\n\n\n\n\nContour plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n21.10.4 Parallel Coordinates Plot\n\nspot_tuner.parallel_plot()\n\n\n                                                \nParallel coordinates plots\n\n\n\n\n21.10.5 Plot all Combinations of Hyperparameters\n\nWarning: this may take a while.\n\n\nPLOT_ALL = False\nif PLOT_ALL:\n    n = spot_tuner.k\n    for i in range(n-1):\n        for j in range(i+1, n):\n            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)\n\n\n\n21.10.6 Visualizing the Activation Distribution\n\n\n\n\n\n\nReference:\n\n\n\n\nThe following code is based on [PyTorch Lightning TUTORIAL 2: ACTIVATION FUNCTIONS], Author: Phillip Lippe, License: [CC BY-SA], Generated: 2023-03-15T09:52:39.179933.\n\n\n\nAfter we have trained the models, we can look at the actual activation values that find inside the model. For instance, how many neurons are set to zero in ReLU? Where do we find most values in Tanh? To answer these questions, we can write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activations inside the network:\n\nfrom spotPython.torch.activation import Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Swish\nact_fn_by_name = {\"sigmoid\": Sigmoid, \"tanh\": Tanh, \"relu\": ReLU, \"leakyrelu\": LeakyReLU, \"elu\": ELU, \"swish\": Swish}\n\n\nfrom spotPython.hyperparameters.values import get_one_config_from_X\nX = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\nconfig = get_one_config_from_X(X, fun_control)\nmodel = fun_control[\"core_model\"](**config, _L_in=64, _L_out=11)\nmodel\n\nNetLightBase(\n  (act_fn): LeakyReLU()\n  (train_mapk): MAPK()\n  (valid_mapk): MAPK()\n  (test_mapk): MAPK()\n  (layers): Sequential(\n    (0): Linear(in_features=64, out_features=128, bias=True)\n    (1): LeakyReLU()\n    (2): Dropout(p=0.005410283967863825, inplace=False)\n    (3): Linear(in_features=128, out_features=64, bias=True)\n    (4): LeakyReLU()\n    (5): Dropout(p=0.005410283967863825, inplace=False)\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): LeakyReLU()\n    (8): Dropout(p=0.005410283967863825, inplace=False)\n    (9): Linear(in_features=64, out_features=32, bias=True)\n    (10): LeakyReLU()\n    (11): Dropout(p=0.005410283967863825, inplace=False)\n    (12): Linear(in_features=32, out_features=11, bias=True)\n  )\n)\n\n\n\nfrom spotPython.utils.eda import visualize_activations\nvisualize_activations(model, device=\"cpu\", color=f\"C{0}\")"
  },
  {
    "objectID": "99_spot_doc.html#example-spot",
    "href": "99_spot_doc.html#example-spot",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.1 Example: spot",
    "text": "22.1 Example: spot\n\nimport numpy as np\nfrom math import inf\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nfrom scipy.optimize import shgo\nfrom scipy.optimize import direct\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\n\n\n22.1.1 The Objective Function\nThe spotPython package provides several classes of objective functions. We will use an analytical objective function, i.e., a function that can be described by a (closed) formula: \\[f(x) = x^2\\]\n\nfun = analytical().fun_sphere\n\n\nx = np.linspace(-1,1,100).reshape(-1,1)\ny = fun(x)\nplt.figure()\nplt.plot(x,y, \"k\")\nplt.show()\n\n\n\n\n\nspot_1 = spot.Spot(fun=fun,\n                   lower = np.array([-10]),\n                   upper = np.array([100]),\n                   fun_evals = 7,\n                   fun_repeats = 1,\n                   max_time = inf,\n                   noise = False,\n                   tolerance_x = np.sqrt(np.spacing(1)),\n                   var_type=[\"num\"],\n                   infill_criterion = \"y\",\n                   n_points = 1,\n                   seed=123,\n                   log_level = 50,\n                   show_models=True,\n                   fun_control = {},\n                   design_control={\"init_size\": 5,\n                                   \"repeats\": 1},\n                   surrogate_control={\"noise\": False,\n                                      \"cod_type\": \"norm\",\n                                      \"min_theta\": -4,\n                                      \"max_theta\": 3,\n                                      \"n_theta\": 1,\n                                      \"model_optimizer\": differential_evolution,\n                                      \"model_fun_evals\": 1000,\n                                      })\n\nspot’s __init__ method sets the control parameters. There are two parameter groups:\n\nexternal parameters can be specified by the user\ninternal parameters, which are handled by spot.\n\n\n\n22.1.2 External Parameters\n\n\n\n\n\n\n\n\n\n\nexternal parameter\ntype\ndescription\ndefault\nmandatory\n\n\n\n\nfun\nobject\nobjective function\n\nyes\n\n\nlower\narray\nlower bound\n\nyes\n\n\nupper\narray\nupper bound\n\nyes\n\n\nfun_evals\nint\nnumber of function evaluations\n15\nno\n\n\nfun_evals\nint\nnumber of function evaluations\n15\nno\n\n\nfun_control\ndict\nnoise etc.\n{}\nn\n\n\nmax_time\nint\nmax run time budget\ninf\nno\n\n\nnoise\nbool\nif repeated evaluations of fun results in different values, then noise should be set to True.\nFalse\nno\n\n\ntolerance_x\nfloat\ntolerance for new x solutions. Minimum distance of new solutions, generated by suggest_new_X, to already existing solutions. If zero (which is the default), every new solution is accepted.\n0\nno\n\n\nvar_type\nlist\nlist of type information, can be either \"num\" or \"factor\"\n[\"num\"]\nno\n\n\ninfill_criterion\nstring\nCan be \"y\", \"s\", \"ei\" (negative expected improvement), or \"all\"\n\"y\"\nno\n\n\nn_points\nint\nnumber of infill points\n1\nno\n\n\nseed\nint\ninitial seed. If Spot.run() is called twice, different results will be generated. To reproduce results, the seed can be used.\n123\nno\n\n\nlog_level\nint\nlog level with the following settings: NOTSET (0), DEBUG (10: Detailed information, typically of interest only when diagnosing problems.), INFO (20: Confirmation that things are working as expected.), WARNING (30: An indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected.), ERROR (40: Due to a more serious problem, the software has not been able to perform some function.), and CRITICAL (50: A serious error, indicating that the program itself may be unable to continue running.)\n50\nno\n\n\nshow_models\nbool\nPlot model. Currently only 1-dim functions are supported\nFalse\nno\n\n\ndesign\nobject\nexperimental design\nNone\nno\n\n\ndesign_control\ndict\ncontrol parameters\nsee below\nno\n\n\nsurrogate\n\nsurrogate model\nkriging\nno\n\n\nsurrogate_control\ndict\ncontrol parameters\nsee below\nno\n\n\noptimizer\nobject\noptimizer\nsee below\nno\n\n\noptimizer_control\ndict\ncontrol parameters\nsee below\nno\n\n\n\n\nBesides these single parameters, the following parameter dictionaries can be specified by the user:\n\nfun_control\ndesign_control\nsurrogate_control\noptimizer_control"
  },
  {
    "objectID": "99_spot_doc.html#the-fun_control-dictionary",
    "href": "99_spot_doc.html#the-fun_control-dictionary",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.2 The fun_control Dictionary",
    "text": "22.2 The fun_control Dictionary\n\n\n\nexternal parameter\ntype\ndescription\ndefault\nmandatory\n\n\n\n\nsigma\nfloat\nnoise: standard deviation\n0\nyes\n\n\nseed\nint\nseed for rng\n124\nyes"
  },
  {
    "objectID": "99_spot_doc.html#the-design_control-dictionary",
    "href": "99_spot_doc.html#the-design_control-dictionary",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.3 The design_control Dictionary",
    "text": "22.3 The design_control Dictionary\n\n\n\n\n\n\n\n\n\n\nexternal parameter\ntype\ndescription\ndefault\nmandatory\n\n\n\n\ninit_size\nint\ninitial sample size\n10\nyes\n\n\nrepeats\nint\nnumber of repeats of the initial sammples\n1\nyes"
  },
  {
    "objectID": "99_spot_doc.html#the-surrogate_control-dictionary",
    "href": "99_spot_doc.html#the-surrogate_control-dictionary",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.4 The surrogate_control Dictionary",
    "text": "22.4 The surrogate_control Dictionary\n\n\n\n\n\n\n\n\n\n\nexternal parameter\ntype\ndescription\ndefault\nmandatory\n\n\n\n\nnoise\n\n\n\n\n\n\nmodel_optimizer\nobject\noptimizer\ndifferential_evolution\nno\n\n\nmodel_fun_evals\n\n\n\n\n\n\nmin_theta\n\n\n-3.\n\n\n\nmax_theta\n\n\n3.\n\n\n\nn_theta\n\n\n1\n\n\n\nn_p\n\n\n1\n\n\n\noptim_p\n\n\nFalse\n\n\n\ncod_type\n\n\n\"norm\"\n\n\n\nvar_type\n\n\n\n\n\n\nuse_cod_y\nbool\n\nFalse"
  },
  {
    "objectID": "99_spot_doc.html#the-optimizer_control-dictionary",
    "href": "99_spot_doc.html#the-optimizer_control-dictionary",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.5 The optimizer_control Dictionary",
    "text": "22.5 The optimizer_control Dictionary\n\n\n\n\n\n\n\n\n\n\nexternal parameter\ntype\ndescription\ndefault\nmandatory\n\n\n\n\nmax_iter\nint\nmax number of iterations. Note: these are the cheap evaluations on the surrogate.\n1000\nno"
  },
  {
    "objectID": "99_spot_doc.html#run",
    "href": "99_spot_doc.html#run",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.6 Run",
    "text": "22.6 Run\n\nspot_1.run()\n\n\n\n\n\n\n\n\n\n\n&lt;spotPython.spot.spot.Spot at 0x108523fa0&gt;"
  },
  {
    "objectID": "99_spot_doc.html#print-the-results",
    "href": "99_spot_doc.html#print-the-results",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.7 Print the Results",
    "text": "22.7 Print the Results\n\nspot_1.print_results()\n\nmin y: 0.29847171516431314\nx0: -0.5463256493743572\n\n\n[['x0', -0.5463256493743572]]"
  },
  {
    "objectID": "99_spot_doc.html#show-the-progress",
    "href": "99_spot_doc.html#show-the-progress",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.8 Show the Progress",
    "text": "22.8 Show the Progress\n\nspot_1.plot_progress()"
  },
  {
    "objectID": "99_spot_doc.html#visualize-the-surrogate",
    "href": "99_spot_doc.html#visualize-the-surrogate",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.9 Visualize the Surrogate",
    "text": "22.9 Visualize the Surrogate\n\nThe plot method of the kriging surrogate is used.\nNote: the plot uses the interval defined by the ranges of the natural variables.\n\n\nspot_1.surrogate.plot()\n\n&lt;Figure size 864x576 with 0 Axes&gt;"
  },
  {
    "objectID": "99_spot_doc.html#init-build-initial-design",
    "href": "99_spot_doc.html#init-build-initial-design",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.10 Init: Build Initial Design",
    "text": "22.10 Init: Build Initial Design\n\nfrom spotPython.design.spacefilling import spacefilling\nfrom spotPython.build.kriging import Kriging\nfrom spotPython.fun.objectivefunctions import analytical\ngen = spacefilling(2)\nrng = np.random.RandomState(1)\nlower = np.array([-5,-0])\nupper = np.array([10,15])\nfun = analytical().fun_branin\nfun_control = {\"sigma\": 0,\n               \"seed\": 123}\n\nX = gen.scipy_lhd(10, lower=lower, upper = upper)\nprint(X)\ny = fun(X, fun_control=fun_control)\nprint(y)\n\n[[ 8.97647221 13.41926847]\n [ 0.66946019  1.22344228]\n [ 5.23614115 13.78185824]\n [ 5.6149825  11.5851384 ]\n [-1.72963184  1.66516096]\n [-4.26945568  7.1325531 ]\n [ 1.26363761 10.17935555]\n [ 2.88779942  8.05508969]\n [-3.39111089  4.15213772]\n [ 7.30131231  5.22275244]]\n[128.95676449  31.73474356 172.89678121 126.71295908  64.34349975\n  70.16178611  48.71407916  31.77322887  76.91788181  30.69410529]"
  },
  {
    "objectID": "99_spot_doc.html#replicability",
    "href": "99_spot_doc.html#replicability",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.11 Replicability",
    "text": "22.11 Replicability\nSeed\n\ngen = spacefilling(2, seed=123)\nX0 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=345)\nX1 = gen.scipy_lhd(3)\nX2 = gen.scipy_lhd(3)\ngen = spacefilling(2, seed=123)\nX3 = gen.scipy_lhd(3)\nX0, X1, X2, X3\n\n(array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]),\n array([[0.78373509, 0.86811887],\n        [0.06692621, 0.6058029 ],\n        [0.41374778, 0.00525456]]),\n array([[0.121357  , 0.69043832],\n        [0.41906219, 0.32838498],\n        [0.86742658, 0.52910374]]),\n array([[0.77254938, 0.31539299],\n        [0.59321338, 0.93854273],\n        [0.27469803, 0.3959685 ]]))"
  },
  {
    "objectID": "99_spot_doc.html#surrogates",
    "href": "99_spot_doc.html#surrogates",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.12 Surrogates",
    "text": "22.12 Surrogates\n\n22.12.1 A Simple Predictor\nThe code below shows how to use a simple model for prediction. Assume that only two (very costly) measurements are available:\n\nf(0) = 0.5\nf(2) = 2.5\n\nWe are interested in the value at \\(x_0 = 1\\), i.e., \\(f(x_0 = 1)\\), but cannot run an additional, third experiment.\n\nfrom sklearn import linear_model\nX = np.array([[0], [2]])\ny = np.array([0.5, 2.5])\nS_lm = linear_model.LinearRegression()\nS_lm = S_lm.fit(X, y)\nX0 = np.array([[1]])\ny0 = S_lm.predict(X0)\nprint(y0)\n\n[1.5]\n\n\nCentral Idea: Evaluation of the surrogate model S_lm is much cheaper (or / and much faster) than running the real-world experiment \\(f\\)."
  },
  {
    "objectID": "99_spot_doc.html#demotest-objective-function-fails",
    "href": "99_spot_doc.html#demotest-objective-function-fails",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.13 Demo/Test: Objective Function Fails",
    "text": "22.13 Demo/Test: Objective Function Fails\nSPOT expects np.nan values from failed objective function values. These are handled. Note: SPOT’s counter considers only successful executions of the objective function.\n\nimport numpy as np\nfrom spotPython.fun.objectivefunctions import analytical\nfrom spotPython.spot import spot\nimport numpy as np\nfrom math import inf\n# number of initial points:\nni = 20\n# number of points\nn = 30\n\nfun = analytical().fun_random_error\nlower = np.array([-1])\nupper = np.array([1])\ndesign_control={\"init_size\": ni}\n\nspot_1 = spot.Spot(fun=fun,\n            lower = lower,\n            upper= upper,\n            fun_evals = n,\n            show_progress=False,\n            design_control=design_control,)\nspot_1.run()\n# To check whether the run was successfully completed,\n# we compare the number of evaluated points to the specified\n# number of points.\nassert spot_1.y.shape[0] == n\n\n[ 0.53176481 -0.9053821  -0.02203599 -0.21843718  0.78240941 -0.58120945\n -0.3923345   0.67234256  0.31802454 -0.68898927 -0.75129705  0.97550354\n  0.41757584  0.0786237          nan         nan         nan -0.82319082\n -0.17991251  0.1481835 ]\n[-1.]\n\n\n[-0.47259301]\n[nan]\n\n\n[0.95541987]\n[0.17335968]\n\n\n[-0.58552368]\n[nan]\n\n\n[-0.20126111]\n[-0.60100809]\n\n\n[-0.97897336]\n\n\n[-0.2748985]\n[0.8359486]\n\n\n[0.99035591]\n[0.01641232]\n\n\n[0.5629346]"
  },
  {
    "objectID": "99_spot_doc.html#sec-detailed-data-splitting",
    "href": "99_spot_doc.html#sec-detailed-data-splitting",
    "title": "22  Documentation of the Sequential Parameter Optimization",
    "section": "22.14 PyTorch: Detailed Description of the Data Splitting",
    "text": "22.14 PyTorch: Detailed Description of the Data Splitting\n\n22.14.1 Description of the \"train_hold_out\" Setting\nThe \"train_hold_out\" setting is used by default. It uses the loss function specfied in fun_control and the metric specified in fun_control.\n\nFirst, the method HyperTorch().fun_torch is called.\nfun_torc(), which is implemented in the file hypertorch.py, calls evaluate_hold_out() as follows:\n\ndf_eval, _ = evaluate_hold_out(\n    model,\n    train_dataset=fun_control[\"train\"],\n    shuffle=self.fun_control[\"shuffle\"],\n    loss_function=self.fun_control[\"loss_function\"],\n    metric=self.fun_control[\"metric_torch\"],\n    device=self.fun_control[\"device\"],\n    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n    path=self.fun_control[\"path\"],\n    task=self.fun_control[\"task\"],\n    writer=self.fun_control[\"writer\"],\n    writerId=config_id,\n)\nNote: Only the data set fun_control[\"train\"] is used for training and validation. It is used in evaluate_hold_out as follows:\ntrainloader, valloader = create_train_val_data_loaders(\n                dataset=train_dataset, batch_size=batch_size_instance, shuffle=shuffle\n            )\ncreate_train_val_data_loaders() splits the train_dataset into trainloader and valloader using torch.utils.data.random_split() as follows:\ndef create_train_val_data_loaders(dataset, batch_size, shuffle, num_workers=0):\n    test_abs = int(len(dataset) * 0.6)\n    train_subset, val_subset = random_split(dataset, [test_abs, len(dataset) - test_abs])\n    trainloader = torch.utils.data.DataLoader(\n        train_subset, batch_size=int(batch_size), shuffle=shuffle, num_workers=num_workers\n    )\n    valloader = torch.utils.data.DataLoader(\n        val_subset, batch_size=int(batch_size), shuffle=shuffle, num_workers=num_workers\n    )\n    return trainloader, valloader\nThe optimizer is set up as follows:\noptimizer_instance = net.optimizer\nlr_mult_instance = net.lr_mult\nsgd_momentum_instance = net.sgd_momentum\noptimizer = optimizer_handler(\n    optimizer_name=optimizer_instance,\n    params=net.parameters(),\n    lr_mult=lr_mult_instance,\n    sgd_momentum=sgd_momentum_instance,\n)\n\nevaluate_hold_out() sets the net attributes such as epochs, batch_size, optimizer, and patience. For each epoch, the methods train_one_epoch() and validate_one_epoch() are called, the former for training and the latter for validation and early stopping. The validation loss from the last epoch (not the best validation loss) is returned from evaluate_hold_out.\nThe method train_one_epoch() is implemented as follows:\n\ndef train_one_epoch(\n    net,\n    trainloader,\n    batch_size,\n    loss_function,\n    optimizer,\n    device,\n    show_batch_interval=10_000,\n    task=None,\n):\n    running_loss = 0.0\n    epoch_steps = 0\n    for batch_nr, data in enumerate(trainloader, 0):\n        input, target = data\n        input, target = input.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = net(input)\n        if task == \"regression\":\n            target = target.unsqueeze(1)\n            if target.shape == output.shape:\n                loss = loss_function(output, target)\n            else:\n                raise ValueError(f\"Shapes of target and output do not match:\n                 {target.shape} vs {output.shape}\")\n        elif task == \"classification\":\n            loss = loss_function(output, target)\n        else:\n            raise ValueError(f\"Unknown task: {task}\")\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n        optimizer.step()\n        running_loss += loss.item()\n        epoch_steps += 1\n        if batch_nr % show_batch_interval == (show_batch_interval - 1):  \n            print(\n                \"Batch: %5d. Batch Size: %d. Training Loss (running): %.3f\"\n                % (batch_nr + 1, int(batch_size), running_loss / epoch_steps)\n            )\n            running_loss = 0.0\n    return loss.item()\n\nThe method validate_one_epoch() is implemented as follows:\n\ndef validate_one_epoch(net, valloader, loss_function, metric, device, task):\n    val_loss = 0.0\n    val_steps = 0\n    total = 0\n    correct = 0\n    metric.reset()\n    for i, data in enumerate(valloader, 0):\n        # get batches\n        with torch.no_grad():\n            input, target = data\n            input, target = input.to(device), target.to(device)\n            output = net(input)\n            # print(f\"target: {target}\")\n            # print(f\"output: {output}\")\n            if task == \"regression\":\n                target = target.unsqueeze(1)\n                if target.shape == output.shape:\n                    loss = loss_function(output, target)\n                else:\n                    raise ValueError(f\"Shapes of target and output \n                        do not match: {target.shape} vs {output.shape}\")\n                metric_value = metric.update(output, target)\n            elif task == \"classification\":\n                loss = loss_function(output, target)\n                metric_value = metric.update(output, target)\n                _, predicted = torch.max(output.data, 1)\n                total += target.size(0)\n                correct += (predicted == target).sum().item()\n            else:\n                raise ValueError(f\"Unknown task: {task}\")\n            val_loss += loss.cpu().numpy()\n            val_steps += 1\n    loss = val_loss / val_steps\n    print(f\"Loss on hold-out set: {loss}\")\n    if task == \"classification\":\n        accuracy = correct / total\n        print(f\"Accuracy on hold-out set: {accuracy}\")\n    # metric on all batches using custom accumulation\n    metric_value = metric.compute()\n    metric_name = type(metric).__name__\n    print(f\"{metric_name} value on hold-out data: {metric_value}\")\n    return metric_value, loss\n\n22.14.1.1 Description of the \"test_hold_out\" Setting\nIt uses the loss function specfied in fun_control and the metric specified in fun_control.\n\nFirst, the method HyperTorch().fun_torch is called.\nfun_torc() calls spotPython.torch.traintest.evaluate_hold_out() similar to the \"train_hold_out\" setting with one exception: It passes an additional test data set to evaluate_hold_out() as follows:\n\ntest_dataset=fun_control[\"test\"]\nevaluate_hold_out() calls create_train_test_data_loaders instead of create_train_val_data_loaders: The two data sets are used in create_train_test_data_loaders as follows:\ndef create_train_test_data_loaders(dataset, batch_size, shuffle, test_dataset, \n        num_workers=0):\n    trainloader = torch.utils.data.DataLoader(\n        dataset, batch_size=int(batch_size), shuffle=shuffle, \n        num_workers=num_workers\n    )\n    testloader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=int(batch_size), shuffle=shuffle, \n        num_workers=num_workers\n    )\n    return trainloader, testloader\n\nThe following steps are identical to the \"train_hold_out\" setting. Only a different data loader is used for testing.\n\n\n\n22.14.1.2 Detailed Description of the \"train_cv\" Setting\nIt uses the loss function specfied in fun_control and the metric specified in fun_control.\n\nFirst, the method HyperTorch().fun_torch is called.\nfun_torc() calls spotPython.torch.traintest.evaluate_cv() as follows (Note: Only the data set fun_control[\"train\"] is used for CV.):\n\ndf_eval, _ = evaluate_cv(\n    model,\n    dataset=fun_control[\"train\"],\n    shuffle=self.fun_control[\"shuffle\"],\n    device=self.fun_control[\"device\"],\n    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n    task=self.fun_control[\"task\"],\n    writer=self.fun_control[\"writer\"],\n    writerId=config_id,\n)\n\nIn `evaluate_cv(), the following steps are performed: The optimizer is set up as follows:\n\noptimizer_instance = net.optimizer\nlr_instance = net.lr\nsgd_momentum_instance = net.sgd_momentum\noptimizer = optimizer_handler(optimizer_name=optimizer_instance,\n     params=net.parameters(), lr_mult=lr_mult_instance)\nevaluate_cv() sets the net attributes such as epochs, batch_size, optimizer, and patience. CV is implemented as follows:\ndef evaluate_cv(\n    net,\n    dataset,\n    shuffle=False,\n    loss_function=None,\n    num_workers=0,\n    device=None,\n    show_batch_interval=10_000,\n    metric=None,\n    path=None,\n    task=None,\n    writer=None,\n    writerId=None,\n):\n    lr_mult_instance = net.lr_mult\n    epochs_instance = net.epochs\n    batch_size_instance = net.batch_size\n    k_folds_instance = net.k_folds\n    optimizer_instance = net.optimizer\n    patience_instance = net.patience\n    sgd_momentum_instance = net.sgd_momentum\n    removed_attributes, net = get_removed_attributes_and_base_net(net)\n    metric_values = {}\n    loss_values = {}\n    try:\n        device = getDevice(device=device)\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n            if torch.cuda.device_count() &gt; 1:\n                print(\"We will use\", torch.cuda.device_count(), \"GPUs!\")\n                net = nn.DataParallel(net)\n        net.to(device)\n        optimizer = optimizer_handler(\n            optimizer_name=optimizer_instance,\n            params=net.parameters(),\n            lr_mult=lr_mult_instance,\n            sgd_momentum=sgd_momentum_instance,\n        )\n        kfold = KFold(n_splits=k_folds_instance, shuffle=shuffle)\n        for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n            print(f\"Fold: {fold + 1}\")\n            train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n            val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n            trainloader = torch.utils.data.DataLoader(\n                dataset, batch_size=batch_size_instance, \n                sampler=train_subsampler, num_workers=num_workers\n            )\n            valloader = torch.utils.data.DataLoader(\n                dataset, batch_size=batch_size_instance, \n                sampler=val_subsampler, num_workers=num_workers\n            )\n            # each fold starts with new weights:\n            reset_weights(net)\n            # Early stopping parameters\n            best_val_loss = float(\"inf\")\n            counter = 0\n            for epoch in range(epochs_instance):\n                print(f\"Epoch: {epoch + 1}\")\n                # training loss from one epoch:\n                training_loss = train_one_epoch(\n                    net=net,\n                    trainloader=trainloader,\n                    batch_size=batch_size_instance,\n                    loss_function=loss_function,\n                    optimizer=optimizer,\n                    device=device,\n                    show_batch_interval=show_batch_interval,\n                    task=task,\n                )\n                # Early stopping check. Calculate validation loss from one epoch:\n                metric_values[fold], loss_values[fold] = validate_one_epoch(\n                    net, valloader=valloader, loss_function=loss_function, \n                    metric=metric, device=device, task=task\n                )\n                # Log the running loss averaged per batch\n                metric_name = \"Metric\"\n                if metric is None:\n                    metric_name = type(metric).__name__\n                    print(f\"{metric_name} value on hold-out data: \n                        {metric_values[fold]}\")\n                if writer is not None:\n                    writer.add_scalars(\n                        \"evaluate_cv fold:\" + str(fold + 1) + \n                        \". Train & Val Loss and Val Metric\" + writerId,\n                        {\"Train loss\": training_loss, \"Val loss\": \n                        loss_values[fold], metric_name: metric_values[fold]},\n                        epoch + 1,\n                    )\n                    writer.flush()\n                if loss_values[fold] &lt; best_val_loss:\n                    best_val_loss = loss_values[fold]\n                    counter = 0\n                    # save model:\n                    if path is not None:\n                        torch.save(net.state_dict(), path)\n                else:\n                    counter += 1\n                    if counter &gt;= patience_instance:\n                        print(f\"Early stopping at epoch {epoch}\")\n                        break\n        df_eval = sum(loss_values.values()) / len(loss_values.values())\n        df_metrics = sum(metric_values.values()) / len(metric_values.values())\n        df_preds = np.nan\n    except Exception as err:\n        print(f\"Error in Net_Core. Call to evaluate_cv() failed. {err=}, \n            {type(err)=}\")\n        df_eval = np.nan\n        df_preds = np.nan\n    add_attributes(net, removed_attributes)\n    if writer is not None:\n        metric_name = \"Metric\"\n        if metric is None:\n            metric_name = type(metric).__name__\n        writer.add_scalars(\n            \"CV: Val Loss and Val Metric\" + writerId,\n            {\"CV-loss\": df_eval, metric_name: df_metrics},\n            epoch + 1,\n        )\n        writer.flush()\n    return df_eval, df_preds, df_metrics\n\nThe method train_fold() is implemented as shown above.\nThe method validate_one_epoch() is implemented as shown above. In contrast to the hold-out setting, it is called for each of the \\(k\\) folds. The results are stored in a dictionaries metric_values and loss_values. The results are averaged over the \\(k\\) folds and returned as df_eval.\n\n\n\n22.14.1.3 Detailed Description of the \"test_cv\" Setting\nIt uses the loss function specfied in fun_control and the metric specified in fun_control.\n\nFirst, the method HyperTorch().fun_torch is called.\nfun_torc() calls spotPython.torch.traintest.evaluate_cv() as follows:\n\ndf_eval, _ = evaluate_cv(\n    model,\n    dataset=fun_control[\"test\"],\n    shuffle=self.fun_control[\"shuffle\"],\n    device=self.fun_control[\"device\"],\n    show_batch_interval=self.fun_control[\"show_batch_interval\"],\n    task=self.fun_control[\"task\"],\n    writer=self.fun_control[\"writer\"],\n    writerId=config_id,\n)\nNote: The data set fun_control[\"test\"] is used for CV. The rest is the same as for the \"train_cv\" setting.\n\n\n22.14.1.4 Detailed Description of the Final Model Training and Evaluation\nThere are two methods that can be used for the final evaluation of a Pytorch model:\n\n\"train_tuned and\n\"test_tuned\".\n\ntrain_tuned() is just a wrapper to evaluate_hold_out using the train data set. It is implemented as follows:\ndef train_tuned(\n    net,\n    train_dataset,\n    shuffle,\n    loss_function,\n    metric,\n    device=None,\n    show_batch_interval=10_000,\n    path=None,\n    task=None,\n    writer=None,\n):\n    evaluate_hold_out(\n        net=net,\n        train_dataset=train_dataset,\n        shuffle=shuffle,\n        test_dataset=None,\n        loss_function=loss_function,\n        metric=metric,\n        device=device,\n        show_batch_interval=show_batch_interval,\n        path=path,\n        task=task,\n        writer=writer,\n    )\nThe test_tuned() procedure is implemented as follows:\ndef test_tuned(net, shuffle, test_dataset=None, loss_function=None,\n    metric=None, device=None, path=None, task=None):\n    batch_size_instance = net.batch_size\n    removed_attributes, net = get_removed_attributes_and_base_net(net)\n    if path is not None:\n        net.load_state_dict(torch.load(path))\n        net.eval()\n    try:\n        device = getDevice(device=device)\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n            if torch.cuda.device_count() &gt; 1:\n                print(\"We will use\", torch.cuda.device_count(), \"GPUs!\")\n                net = nn.DataParallel(net)\n        net.to(device)\n        valloader = torch.utils.data.DataLoader(\n            test_dataset, batch_size=int(batch_size_instance),\n            shuffle=shuffle, \n            num_workers=0\n        )\n        metric_value, loss = validate_one_epoch(\n            net, valloader=valloader, loss_function=loss_function,\n            metric=metric, device=device, task=task\n        )\n        df_eval = loss\n        df_metric = metric_value\n        df_preds = np.nan\n    except Exception as err:\n        print(f\"Error in Net_Core. Call to test_tuned() failed. {err=}, \n            {type(err)=}\")\n        df_eval = np.nan\n        df_metric = np.nan\n        df_preds = np.nan\n    add_attributes(net, removed_attributes)\n    print(f\"Final evaluation: Validation loss: {df_eval}\")\n    print(f\"Final evaluation: Validation metric: {df_metric}\")\n    print(\"----------------------------------------------\")\n    return df_eval, df_preds, df_metric"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bartz, Eva, Thomas Bartz-Beielstein, Martin Zaefferer, and Olaf\nMersmann, eds. 2022. Hyperparameter Tuning for\nMachine and Deep Learning with R - A Practical Guide.\nSpringer.\n\n\nBartz-Beielstein, Thomas. 2023. “PyTorch\nHyperparameter Tuning with SPOT: Comparison with Ray\nTuner and Default Hyperparameters on\nCIFAR10.” https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb.\n\n\nBartz-Beielstein, Thomas, Jürgen Branke, Jörn Mehnen, and Olaf Mersmann.\n2014. “Evolutionary Algorithms.” Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge Discovery 4\n(3): 178–95.\n\n\nBartz-Beielstein, Thomas, Carola Doerr, Jakob Bossek, Sowmya\nChandrasekaran, Tome Eftimov, Andreas Fischbach, Pascal Kerschke, et al.\n2020. “Benchmarking in Optimization: Best Practice and Open\nIssues.” arXiv. https://arxiv.org/abs/2007.03488.\n\n\nBartz-Beielstein, Thomas, Christian Lasarczyk, and Mike Preuss. 2005.\n“Sequential Parameter Optimization.” In\nProceedings 2005 Congress on Evolutionary\nComputation (CEC’05), Edinburgh, Scotland, edited by B McKay\net al., 773–80. Piscataway NJ: IEEE Press.\n\n\nLewis, R M, V Torczon, and M W Trosset. 2000. “Direct search methods: Then and now.”\nJournal of Computational and Applied Mathematics 124 (1–2):\n191–207.\n\n\nLi, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and\nAmeet Talwalkar. 2016. “Hyperband: A Novel\nBandit-Based Approach to Hyperparameter Optimization.”\narXiv e-Prints, March, arXiv:1603.06560.\n\n\nMeignan, David, Sigrid Knust, Jean-Marc Frayet, Gilles Pesant, and\nNicolas Gaud. 2015. “A Review and Taxonomy of\nInteractive Optimization Methods in Operations Research.”\nACM Transactions on Interactive Intelligent Systems, September.\n\n\nMontiel, Jacob, Max Halford, Saulo Martiello Mastelini, Geoffrey\nBolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, et al. 2021.\n“River: Machine Learning for Streaming Data in Python.”\n\n\nPyTorch. 2023a. “Hyperparameter Tuning with Ray Tune.” https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html.\n\n\n———. 2023b. “Training a Classifier.” https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html."
  }
]