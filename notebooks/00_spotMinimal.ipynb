{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1432eab7",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  cache: false\n",
    "  eval: true\n",
    "  echo: true\n",
    "  warning: false\n",
    "title: 'HPT PyTorch Lightning: User Specified Data Set and Regression Model'\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56423565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Loading\n",
    "import numpy as np\n",
    "from math import inf\n",
    "\n",
    "from spotPython.utils.device import getDevice\n",
    "from spotPython.utils.init import fun_control_init\n",
    "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotPython.hyperparameters.values import set_control_hyperparameter_value\n",
    "from spotPython.utils.eda import gen_design_table\n",
    "from spotPython.utils.init import design_control_init, surrogate_control_init\n",
    "from spotPython.fun.hyperlight import HyperLight\n",
    "from spotPython.spot import spot\n",
    "from spotPython.utils.eda import gen_design_table\n",
    "from spotPython.hyperparameters.values import get_tuned_architecture\n",
    "from spotPython.light.loadmodel import load_light_from_checkpoint\n",
    "from spotPython.light.cvmodel import cv_model\n",
    "from torch.utils.data import DataLoader\n",
    "from spotPython.utils.init import fun_control_init\n",
    "from spotPython.hyperparameters.values import set_control_key_value\n",
    "from spotPython.data.diabetes import Diabetes\n",
    "from spotPython.light.regression.netlightregression import NetLightRegression\n",
    "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotPython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotPython.plot.xai import (get_activations, get_gradients, get_weights,\n",
    "                                 plot_nn_values_hist, plot_nn_values_scatter, visualize_weights,\n",
    "                                 visualize_gradients, visualize_activations, visualize_gradient_distributions,\n",
    "                                 visualize_weights_distributions)\n",
    "from pyhcf.data.param_list_generator import (\n",
    "    load_all_features_param_list,\n",
    "    load_thermo_features_param_list,\n",
    "    load_man_most_significant,\n",
    "    load_man_significant,\n",
    ")\n",
    "from spotPython.light.predictmodel import predict_model\n",
    "from spotPython.utils.file import save_experiment, load_experiment\n",
    "from spotPython.data.lightdatamodule import LightDataModule\n",
    "from spotPython.light.regression.netlightregression2 import NetLightRegression2\n",
    "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from pyhcf.data.loadHcfData import loadFeaturesFromPkl\n",
    "from pyhcf.data.param_list_generator import load_relevante_aero_variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX=\"RELEVANTE_AERO_VARIABLEN_3551a_04a\" # Prefix unbedingt ändern!\n",
    "DATA_PKL_NAME = \"RELEVANTE_AERO_VARIABLEN_DATA.pickle\"\n",
    "MAX_TIME = 300\n",
    "FUN_EVALS = inf\n",
    "FUN_REPEATS = 2\n",
    "NOISE = False\n",
    "OCBA_DELTA = 0\n",
    "REPEATS = 2\n",
    "INIT_SIZE = 20\n",
    "WORKERS = 0\n",
    "DEVICE = getDevice()\n",
    "DEVICES = 1\n",
    "TEST_SIZE = 0.3\n",
    "K_FOLDS = 5\n",
    "param_list= load_relevante_aero_variablen()\n",
    "# param_list = load_all_features_param_list()\n",
    "target = \"N\"\n",
    "rmNA=True\n",
    "rmMF=True\n",
    "scale_data=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=len(param_list),\n",
    "    _L_out=1,\n",
    "    PREFIX=PREFIX,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    device=DEVICE,\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=FUN_EVALS,\n",
    "    fun_repeats=FUN_REPEATS,\n",
    "    log_level=50,\n",
    "    max_time=MAX_TIME,\n",
    "    num_workers=WORKERS,\n",
    "    ocba_delta = OCBA_DELTA,\n",
    "    show_progress=True,\n",
    "    test_size=TEST_SIZE,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    verbosity=1,\n",
    "    noise=NOISE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = DATA_PKL_NAME\n",
    "dataset = loadFeaturesFromPkl(param_list=param_list, filename=filename, return_X_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cfb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_core_model_to_fun_control(fun_control=fun_control,\n",
    "                              core_model=NetLightRegression2,\n",
    "                              hyper_dict=LightHyperDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ändern der Default Hyperparameter \n",
    "\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [5, 9])\n",
    "set_control_hyperparameter_value(fun_control, \"epochs\", [8, 12])\n",
    "set_control_hyperparameter_value(fun_control, \"batch_size\", [8, 10])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [ \n",
    "    \"Adagrad\", \"Adam\"])\n",
    "set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.001, 0.1])\n",
    "set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.01, 4])\n",
    "set_control_hyperparameter_value(fun_control, \"patience\", [4, 7])\n",
    "set_control_hyperparameter_value(fun_control, \"act_fn\",[\n",
    "                \"Sigmoid\",\n",
    "                \"ReLU\",\n",
    "                \"LeakyReLU\",\n",
    "             ] )\n",
    "set_control_hyperparameter_value(fun_control, \"initialization\",[\"Default\"] )\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = HyperLight(log_level=50).fun\n",
    "\n",
    "design_control = design_control_init(init_size=INIT_SIZE,\n",
    "                                     repeats=REPEATS,)\n",
    "\n",
    "surrogate_control = surrogate_control_init(noise=True,\n",
    "                                            n_theta=2,\n",
    "                                            min_Lambda=1e-6,\n",
    "                                            max_Lambda=10,\n",
    "                                            log_level=50,)\n",
    "\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       fun_control=fun_control,\n",
    "                       design_control=design_control,\n",
    "                       surrogate_control=surrogate_control)\n",
    "spot_tuner.run()\n",
    "\n",
    "SPOT_PKL_NAME = save_experiment(spot_tuner, fun_control)\n",
    "\n",
    "# tensorboard --logdir=\"runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spot_tuner.noise:\n",
    "    print(spot_tuner.min_mean_X)\n",
    "    print(spot_tuner.min_mean_y)\n",
    "else:\n",
    "    print(spot_tuner.min_X)\n",
    "    print(spot_tuner.min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf6cf9",
   "metadata": {
    "fig-label": "fig-progress-33"
   },
   "outputs": [],
   "source": [
    "spot_tuner.plot_progress(log_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729fdbc",
   "metadata": {
    "fig-label": "tbl-results-33"
   },
   "outputs": [],
   "source": [
    "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Tuned Architecture \n",
    "\n",
    "config = get_tuned_architecture(spot_tuner, fun_control)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abacf8ba",
   "metadata": {},
   "source": [
    "# Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0afac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotPython.utils.file import load_experiment\n",
    "from spotPython.light.predictmodel import predict_model\n",
    "from spotPython.hyperparameters.values import get_one_config_from_X\n",
    "from spotPython.plot.validation import plot_actual_vs_predicted\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix unbedingt anpassen, wenn ein altes Experiment geladen wird!\n",
    "# Ansonsten wird das oben für das Tuning erstellte PREFIX verwendet.\n",
    "# PREFIX=\"033allbartz09d\" \n",
    "spot_tuner, fun_control = load_experiment(SPOT_PKL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"noise: {spot_tuner.noise}\")\n",
    "if spot_tuner.noise:\n",
    "    print(spot_tuner.min_mean_X)\n",
    "    X = spot_tuner.to_all_dim(spot_tuner.min_mean_X.reshape(1,-1))\n",
    "    print(spot_tuner.min_mean_y)\n",
    "else:\n",
    "    print(spot_tuner.min_X)\n",
    "    X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n",
    "    print(spot_tuner.min_y)\n",
    "\n",
    "print(f\"X: {X}\")\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "print(f\"config: {config}\")\n",
    "batch_size = config[\"batch_size\"]\n",
    "print(f\"batch_size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b01a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training des Netzes mit den besten Hyperparametern\n",
    "res = predict_model(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraktion und Konvertieren der Ergebnisse in Numpy Arrays\n",
    "x = res[0][0]\n",
    "y = res[0][1]\n",
    "yhat = res[0][2]\n",
    "y_test = y.numpy().flatten()\n",
    "print(y_test.shape)\n",
    "y_pred = yhat.numpy().flatten()\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe24b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actual_vs_predicted(y_test=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306950d",
   "metadata": {},
   "source": [
    "# Captum Analye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffbf252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pyhcf.data.hcfDataModule import HCFDataModule\n",
    "from pyhcf.data.loadHcfData import load_hcf_data\n",
    "from pyhcf.utils.names import get_full_parameter_names\n",
    "from spotPython.data.pkldataset import PKLDataset\n",
    "from captum.attr import IntegratedGradients\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from pyhcf.data.loadHcfData import loadFeaturesFromPkl\n",
    "from pyhcf.data.param_list_generator import load_relevante_aero_variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list= load_relevante_aero_variablen()\n",
    "print(param_list)\n",
    "len(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"aero_features.pickle\"\n",
    "dataset = loadFeaturesFromPkl(A=True, H=True, param_list=param_list, target=\"N\", rmNA=True, rmMF=True,  scale_data=True,filename=filename, return_X_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfacc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "data_module = HCFDataModule(batch_size=batch_size, dataset = dataset, test_size=0.3)\n",
    "train_set = data_module.train_dataloader()\n",
    "test_set = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from spotPython.utils.math import generate_div2_list\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        l1 = 256\n",
    "        _L_in = len(param_list)\n",
    "        _L_out = 1\n",
    "        n_low = _L_in // 4\n",
    "        # ensure that n_high is larger than n_low\n",
    "        n_high = max(l1, 2 * n_low)\n",
    "        hidden_sizes = generate_div2_list(n_high, n_low)\n",
    "        dropout_prob = 0.03691049560954292\n",
    "\n",
    "        # Create the network based on the specified hidden sizes\n",
    "        layers = []\n",
    "        layer_sizes = [_L_in] + hidden_sizes\n",
    "        layer_size_last = layer_sizes[0]\n",
    "        for layer_size in layer_sizes[1:]:\n",
    "            layers += [\n",
    "                nn.Linear(layer_size_last, layer_size),\n",
    "                nn.BatchNorm1d(layer_size),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout_prob),\n",
    "            ]\n",
    "            layer_size_last = layer_size\n",
    "        print(f\"layer_sizes w/o last, which is 1: {layer_sizes}\")\n",
    "        layers += [nn.Linear(layer_sizes[-1], _L_out)]\n",
    "        # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A tensor containing a batch of input data.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the output of the model.\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CustomModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "lr_mult = 2.4863701285514677\n",
    "# MANUALLY adjust lr according to the transformations shown in optimizer_handler from spotPython.hyperparameters.optimizer  \n",
    "optimizer = optim.Adagrad(model.parameters(), lr=lr_mult * 0.01)\n",
    "\n",
    "# For testing set epoch to 100\n",
    "# epochs = 100\n",
    "epochs = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c5e9c",
   "metadata": {},
   "source": [
    "## Training des Netzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotPython.utils.device import getDevice\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = getDevice(\"cpu\")\n",
    "# device = getDevice()\n",
    "print(device)\n",
    "model.to(device)\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    print('epochs {}/{}'.format(epoch+1,epochs))\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for idx, (inputs,labels) in enumerate(train_set):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs.float())\n",
    "        labels = labels.view(len(labels), 1)\n",
    "        loss = criterion(preds,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "\n",
    "    train_loss = running_loss/len(train_set)\n",
    "    # put the train loss tensor on the cpu and convert it to numpy:\n",
    "    train_losses.append(train_loss.detach().cpu().numpy())\n",
    "\n",
    "    print(f'train_loss {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03364026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| label: fig-plot-train-aero-captum\n",
    "#| fig-cap: Verlauf des Trainings-Losses\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f22b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| label: aero-sig\n",
    "n_rel = 31\n",
    "def get_n_most_sig_features(model, test_set, batch_size, n_rel = n_rel, verbose=False):\n",
    "    \n",
    "    model.eval()\n",
    "    total_attributions = None\n",
    "    integrated_gradients = IntegratedGradients(model)\n",
    "\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(test_set):\n",
    "        #inputs = inputs.unsqueeze(0)\n",
    "\n",
    "        # Ensure that the last batch is not smaller than the batch size\n",
    "        # and only \"full\" batches are used for the analysis!\n",
    "        if inputs.shape[0] != batch_size:\n",
    "            continue    \n",
    "        attributions, delta = integrated_gradients.attribute(inputs, return_convergence_delta=True)\n",
    "\n",
    "        if total_attributions is None:\n",
    "            total_attributions = attributions\n",
    "        else:\n",
    "            total_attributions += attributions\n",
    "\n",
    "    # Calculation of average attribution across all batches\n",
    "    avg_attributions = total_attributions.mean(dim=0).detach().numpy()\n",
    "    \n",
    "    # take the absolute value of the attributions\n",
    "    abs_avg_attributions = np.abs(avg_attributions)\n",
    "\n",
    "    # Get indices of the most important features\n",
    "    top_n_indices = abs_avg_attributions.argsort()[-n_rel:][::-1]\n",
    "\n",
    "    # Get the importance values for the top features\n",
    "    top_n_importances = avg_attributions[top_n_indices]\n",
    "\n",
    "    # Print the indices and importance values of the top features\n",
    "    selected_indices = []\n",
    "    if verbose:\n",
    "        print(f\"Die {n_rel} wichtigsten Features aus der Captum Analyse sind:\")\n",
    "    i = 1\n",
    "    for idx, importance in zip(top_n_indices, top_n_importances):\n",
    "        selected_indices.append(idx)\n",
    "        if verbose:\n",
    "            print(f\"{i}. Feature Index: {idx}, Importance: {importance}\")\n",
    "        i += 1\n",
    "\n",
    "    selected_significants = [param_list[i] for i in selected_indices]\n",
    "    important = get_full_parameter_names(selected_significants)\n",
    "    # print the elements of the list \"important\" in a single line\n",
    "    if verbose:\n",
    "        print(\"Die Namen der wichtigsten Features nach der Captum-Analyse sind: \", end=\"\\n\")\n",
    "        for i in range(len(important)):\n",
    "            if i < len(important) - 1:\n",
    "                print(i+1, end=\". \")\n",
    "                print(important[i], end=\"\\n\")\n",
    "            else:\n",
    "                print(i+1, end=\". \")\n",
    "                print(important[i])\n",
    "\n",
    "    # Final, total print:\n",
    "    print(f\"Die {n_rel} wichtigsten Features aus der Captum Analyse sind:\")\n",
    "    i = 1\n",
    "    for idx, importance in zip(top_n_indices, top_n_importances):\n",
    "        # selected_indices.append(idx)\n",
    "        print(f\"{i}. Feature: {important[i-1]}.  Index: {idx}, Importance: {importance}\")\n",
    "        i += 1\n",
    "    \n",
    "\n",
    "    return top_n_indices, top_n_importances, avg_attributions, selected_indices, important, selected_significants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ed0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_indices, top_n_importances, avg_attributions, selected_indices, important, selected_significants = get_n_most_sig_features(model, test_set, batch_size, n_rel = n_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_aero = selected_significants\n",
    "print(param_list_aero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sig_features(top_n_indices, top_n_importances, avg_attributions):\n",
    "    # Visualize attributions using a bar plot\n",
    "    # features = range(1, len(avg_attributions) + 1)\n",
    "    # TODO: Check Indices!\n",
    "    features = range(len(avg_attributions))\n",
    "    plt.bar(features, avg_attributions)\n",
    "    plt.title('Integrated Gradients - Relevanz der Variablen für die Vorhersage der Amplitude')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Attribution Score')\n",
    "    # add a grid each 5th feature\n",
    "    plt.xticks(features[::5])\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b0684",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sig_features(top_n_indices, top_n_importances, avg_attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347099a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
