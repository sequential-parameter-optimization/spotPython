{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "title: 'spotPython Tests'\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# fun_control_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n",
        "fun_control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def class_attributes_to_dataframe(class_obj):\n",
        "    # Get the attributes and their values of the class object\n",
        "    attributes = [attr for attr in dir(class_obj) if not callable(getattr(class_obj, attr)) and not attr.startswith(\"__\")]\n",
        "    values = [getattr(class_obj, attr) for attr in attributes]\n",
        "    \n",
        "    # Create a DataFrame from the attributes and values\n",
        "    df = pd.DataFrame({'Attribute Name': attributes, 'Attribute Value': values})\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "class MyClass:\n",
        "    def __init__(self):\n",
        "        self.name = \"John\"\n",
        "        self.age = 30\n",
        "        self.salary = 50000\n",
        "\n",
        "my_instance = MyClass()\n",
        "df = class_attributes_to_dataframe(my_instance)\n",
        "print(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import inf\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "from spotPython.spot import spot\n",
        "# number of initial points:\n",
        "ni = 7\n",
        "# number of points\n",
        "n = 10\n",
        "\n",
        "fun = analytical().fun_sphere\n",
        "lower = np.array([-1])\n",
        "upper = np.array([1])\n",
        "design_control={\"init_size\": ni}\n",
        "\n",
        "spot_1 = spot.Spot(fun=fun,\n",
        "            lower = lower,\n",
        "            upper= upper,\n",
        "            fun_evals = n,\n",
        "            show_progress=True,\n",
        "            design_control=design_control,)\n",
        "spot_1.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sys import stdout\n",
        "df = spot_1.class_attributes_to_dataframe()\n",
        "stdout.write(df.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from river import datasets\n",
        "from river import evaluate\n",
        "from river.linear_model import LogisticRegression\n",
        "from river import metrics\n",
        "from river import optim\n",
        "from river import preprocessing\n",
        "\n",
        "dataset = datasets.Phishing()\n",
        "\n",
        "model = (\n",
        "    preprocessing.StandardScaler() |\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "metric = metrics.Accuracy()\n",
        "\n",
        "evaluate.progressive_val_score(dataset, model, metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.csvdataset import CSVDataset\n",
        "# dataset = CSVDataset(csv_file='./data/spotPython/data.csv', target_column='prognosis')\n",
        "dataset = CSVDataset(target_column='prognosis')\n",
        "print(dataset.data.shape)\n",
        "print(dataset.targets.shape)            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.extra_repr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 3\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSV Data set VBDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the csv_file='./data/spotPython/data.csv' as a pandas df and save it as a pickle file\n",
        "import pandas as pd\n",
        "df = pd.read_csv('./data/spotPython/data.csv')\n",
        "df.to_pickle('./data/spotPython/data.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyHcf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyhcf.data.daten_sensitive import DatenSensitive\n",
        "from pyhcf.utils.names import get_short_parameter_names\n",
        "daten = DatenSensitive()\n",
        "df = daten.load()\n",
        "names =  df.columns\n",
        "names = get_short_parameter_names(names)\n",
        "# rename columns with short names\n",
        "df.columns = names\n",
        "df.head()\n",
        "# save the df as a csv file\n",
        "df.to_csv('./data/spotPython/data_sensitive.csv', index=False)\n",
        "# save the df as a pickle file\n",
        "df.to_pickle('./data/spotPython/data_sensitive.pkl')\n",
        "# remove all rows with NaN values\n",
        "df = df.dropna()\n",
        "# save the df as a csv file\n",
        "df.to_csv('./data/spotPython/data_sensitive_rmNA.csv', index=False)\n",
        "# save the df as a pickle file\n",
        "df.to_pickle('./data/spotPython/data_sensitive_rmNA.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyHcf data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from spotPython.light.csvdataset import CSVDataset\n",
        "# import torch\n",
        "# dataset = CSVDataset(csv_file='./data/spotPython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# # Set batch size for DataLoader\n",
        "# batch_size = 5000\n",
        "# # Create DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Iterate over the data in the DataLoader\n",
        "# for batch in dataloader:\n",
        "#     inputs, targets = batch\n",
        "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
        "#     print(\"---------------\")\n",
        "#     # print(f\"Inputs: {inputs}\")\n",
        "#     print(f\"Targets: {targets}\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from spotPython.light.csvdataset import CSVDataset\n",
        "# import torch\n",
        "# dataset = CSVDataset(csv_file='./data/spotPython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# # Set batch size for DataLoader\n",
        "# batch_size = 5000\n",
        "# # Create DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Iterate over the data in the DataLoader\n",
        "# for batch in dataloader:\n",
        "#     inputs, targets = batch\n",
        "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
        "#     print(\"---------------\")\n",
        "#     # print(f\"Inputs: {inputs}\")\n",
        "#     print(f\"Targets: {targets}\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pickle data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
        "dataset.feature_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Sensitive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = PKLDataset(pkl_file='./data/spotPython/data_sensitive.pkl', target_column='A', feature_type=torch.long, rmNA=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# # Set batch size for DataLoader\n",
        "# batch_size = 5\n",
        "# # Create DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Iterate over the data in the DataLoader\n",
        "# for batch in dataloader:\n",
        "#     inputs, targets = batch\n",
        "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
        "#     print(\"---------------\")\n",
        "#     print(f\"Inputs: {inputs}\")\n",
        "#     print(f\"Targets: {targets}\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/spotPython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test lightdatamodule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "from spotPython.data.csvdataset import CSVDataset\n",
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "# dataset = PKLDataset(directory=\"./data/spotPython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_module.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Training set size: {len(data_module.data_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Validation set size: {len(data_module.data_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Test set size: {len(data_module.data_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set the DataModule in fun_control "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperparameters.values import set_data_module\n",
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "from spotPython.data.csvdataset import CSVDataset\n",
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "fun_control = fun_control_init()\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=7)\n",
        "dm.setup()\n",
        "set_data_module(fun_control=fun_control,\n",
        "                data_module=dm)\n",
        "data_module = fun_control[\"data_module\"]\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## same with the sensitive data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperparameters.values import set_data_module\n",
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "fun_control = fun_control_init()\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/spotPython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
        "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
        "dm.setup()\n",
        "set_data_module(fun_control=fun_control,\n",
        "                data_module=dm)\n",
        "data_module = fun_control[\"data_module\"]\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## same, but VBDO data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperparameters.values import set_data_module\n",
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "from spotPython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "fun_control = fun_control_init()\n",
        "dataset = CSVDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/VBDP/\", filename=\"train.csv\",target_column='prognosis', feature_type=torch.long)\n",
        "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
        "dm.setup()\n",
        "set_data_module(fun_control=fun_control,\n",
        "                data_module=dm)\n",
        "data_module = fun_control[\"data_module\"]\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# load Hyperdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "lhd = LightHyperDict()\n",
        "lhd.hyper_dict\n",
        "user_lhd = LightHyperDict(filename=\"user_hyper_dict.json\", directory=\"./hyperdict/\")\n",
        "user_lhd.hyper_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diabetes data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes  \n",
        "import torch\n",
        "\n",
        "# Load the diabetes dataset\n",
        "feature_df, target_df = load_diabetes(return_X_y=True, as_frame=True)\n",
        "feature_tensor = torch.tensor(feature_df.values, dtype=torch.float32)\n",
        "target_tensor = torch.tensor(target_df.values, dtype=torch.float32)\n",
        "feature_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.diabetes import Diabetes\n",
        "dataset = Diabetes()\n",
        "print(dataset.data.shape)\n",
        "print(dataset.targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# add core model to fun control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.netlightbase import NetLightBase\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightBase,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "fun_control[\"core_model\"].__name__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if the fun_control[\"core_model_hyper_dict\"] is a LightHyperDict\n",
        "isinstance(fun_control[\"core_model_hyper_dict\"], dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test check_X_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.light.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotPython.fun.hyperlight import HyperLight\n",
        "from spotPython.hyperparameters.values import get_var_name\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "hyper_light = HyperLight(seed=126, log_level=50)\n",
        "n_hyperparams = len(get_var_name(fun_control))\n",
        "# generate a random np.array X with shape (2, n_hyperparams)\n",
        "X = np.random.rand(2, n_hyperparams)\n",
        "X == hyper_light.check_X_shape(X, fun_control)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test hyperlight fun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.light.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
        "from spotPython.fun.hyperlight import HyperLight\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "from spotPython.hyperparameters.values import set_data_set\n",
        "import numpy as np\n",
        "fun_control = fun_control_init(\n",
        "    _L_in=10,\n",
        "    _L_out=1,)\n",
        "\n",
        "dataset = Diabetes()\n",
        "set_data_set(fun_control=fun_control,\n",
        "                data_set=dataset)\n",
        "\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "hyper_light = HyperLight(seed=126, log_level=50)\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
        "X = np.vstack((X, X))\n",
        "y = hyper_light.fun(X, fun_control)\n",
        "y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test getVarName()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperparameters.values import get_var_name\n",
        "fun_control = {\"core_model_hyper_dict\":{\n",
        "            \"leaf_prediction\": {\n",
        "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": \"mean\",\n",
        "                \"core_model_parameter_type\": \"str\"},\n",
        "            \"leaf_model\": {\n",
        "                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": \"LinearRegression\",\n",
        "                \"core_model_parameter_type\": \"instance\"},\n",
        "            \"splitter\": {\n",
        "                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": \"EBSTSplitter\",\n",
        "                \"core_model_parameter_type\": \"instance()\"},\n",
        "            \"binary_split\": {\n",
        "                \"levels\": [0, 1],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": 0,\n",
        "                \"core_model_parameter_type\": \"bool\"},\n",
        "            \"stop_mem_management\": {\n",
        "                \"levels\": [0, 1],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": 0,\n",
        "                \"core_model_parameter_type\": \"bool\"}}}\n",
        "len(get_var_name(fun_control))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test netlightregression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [32, 133] | [32, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| name           | type   | default   |   lower |   upper | transform             |\n",
            "|----------------|--------|-----------|---------|---------|-----------------------|\n",
            "| l1             | int    | 3         |     5   |    8    | transform_power_2_int |\n",
            "| epochs         | int    | 4         |     2   |    3    | transform_power_2_int |\n",
            "| batch_size     | int    | 4         |     2   |    8    | transform_power_2_int |\n",
            "| act_fn         | factor | ReLU      |     0   |    5    | None                  |\n",
            "| optimizer      | factor | SGD       |     0   |    3    | None                  |\n",
            "| dropout_prob   | float  | 0.01      |     0   |    0.25 | None                  |\n",
            "| lr_mult        | float  | 1.0       |     0.1 |   10    | None                  |\n",
            "| patience       | int    | 2         |     2   |    6    | transform_power_2_int |\n",
            "| initialization | factor | Default   |     0   |    2    | None                  |\n",
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "`Trainer.fit` stopped: `max_epochs=8` reached.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes   | Out sizes\n",
            "---------------------------------------------------------------\n",
            "0 | layers | Sequential | 5.2 K  | [256, 133] | [256, 1] \n",
            "---------------------------------------------------------------\n",
            "5.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "5.2 K     Total params\n",
            "0.021     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric            192.0380096435547\n",
            "        val_loss             192.0380096435547\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 192.0380096435547, 'hp_metric': 192.0380096435547}\n",
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "`Trainer.fit` stopped: `max_epochs=4` reached.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes | Out sizes\n",
            "-------------------------------------------------------------\n",
            "0 | layers | Sequential | 31.7 K | [8, 133] | [8, 1]   \n",
            "-------------------------------------------------------------\n",
            "31.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "31.7 K    Total params\n",
            "0.127     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric               188761008.0\n",
            "        val_loss                188761008.0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 188761008.0, 'hp_metric': 188761008.0}\n",
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "`Trainer.fit` stopped: `max_epochs=4` reached.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 12.3 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "12.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "12.3 K    Total params\n",
            "0.049     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric            326.2699279785156\n",
            "        val_loss             326.2699279785156\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 326.2699279785156, 'hp_metric': 326.2699279785156}\n",
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 12.3 K | [64, 133] | [64, 1]  \n",
            "--------------------------------------------------------------\n",
            "12.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "12.3 K    Total params\n",
            "0.049     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                637604.0\n",
            "        val_loss                 637604.0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 637604.0, 'hp_metric': 637604.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes | Out sizes\n",
            "-------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [4, 133] | [4, 1]   \n",
            "-------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric             6045.1591796875\n",
            "        val_loss              6045.1591796875\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 6045.1591796875, 'hp_metric': 6045.1591796875}\n",
            "spotPython tuning: 192.0380096435547 [##--------] 19.24% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes   | Out sizes\n",
            "---------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [256, 133] | [256, 1] \n",
            "---------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric               117985112.0\n",
            "        val_loss                117985112.0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 117985112.0, 'hp_metric': 117985112.0}\n",
            "spotPython tuning: 192.0380096435547 [##--------] 21.70% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes   | Out sizes\n",
            "---------------------------------------------------------------\n",
            "0 | layers | Sequential | 31.7 K | [256, 133] | [256, 1] \n",
            "---------------------------------------------------------------\n",
            "31.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "31.7 K    Total params\n",
            "0.127     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric               55424216.0\n",
            "        val_loss                55424216.0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 55424216.0, 'hp_metric': 55424216.0}\n",
            "spotPython tuning: 192.0380096435547 [##--------] 23.59% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric            350.8734436035156\n",
            "        val_loss             350.8734436035156\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 350.8734436035156, 'hp_metric': 350.8734436035156}\n",
            "spotPython tuning: 192.0380096435547 [###-------] 29.59% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 5.2 K  | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "5.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "5.2 K     Total params\n",
            "0.021     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric              204251.328125\n",
            "        val_loss               204251.328125\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 204251.328125, 'hp_metric': 204251.328125}\n",
            "spotPython tuning: 192.0380096435547 [####------] 35.57% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes | Out sizes\n",
            "-------------------------------------------------------------\n",
            "0 | layers | Sequential | 5.2 K  | [4, 133] | [4, 1]   \n",
            "-------------------------------------------------------------\n",
            "5.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "5.2 K     Total params\n",
            "0.021     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                7630090.5\n",
            "        val_loss                 7630090.5\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 7630090.5, 'hp_metric': 7630090.5}\n",
            "spotPython tuning: 192.0380096435547 [######----] 56.10% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [32, 133] | [32, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric            402.1930847167969\n",
            "        val_loss             402.1930847167969\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 402.1930847167969, 'hp_metric': 402.1930847167969}\n",
            "spotPython tuning: 192.0380096435547 [######----] 59.80% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes | Out sizes\n",
            "-------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [8, 133] | [8, 1]   \n",
            "-------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric           150.07376098632812\n",
            "        val_loss            150.07376098632812\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': 150.07376098632812, 'hp_metric': 150.07376098632812}\n",
            "spotPython tuning: 150.07376098632812 [#######---] 69.54% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [#######---] 72.12% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [#######---] 74.15% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [########--] 76.20% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [########--] 78.28% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [########--] 80.41% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [########--] 82.47% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [########--] 84.61% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [#########-] 86.78% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [#########-] 88.91% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [#########-] 90.95% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [#########-] 93.01% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [##########] 95.07% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [##########] 97.29% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [##########] 99.37% \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory runs/lightning_logs/2399856865645210810/checkpoints exists and is not empty.\n",
            "\n",
            "  | Name   | Type       | Params | In sizes  | Out sizes\n",
            "--------------------------------------------------------------\n",
            "0 | layers | Sequential | 92.0 K | [16, 133] | [16, 1]  \n",
            "--------------------------------------------------------------\n",
            "92.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "92.0 K    Total params\n",
            "0.368     Total estimated model params size (MB)\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_model(): Test set size: 1429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        hp_metric                   nan\n",
            "        val_loss                    nan\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "train_model result: {'val_loss': nan, 'hp_metric': nan}\n",
            "spotPython tuning: 150.07376098632812 [##########] 100.00% Done...\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<spotPython.spot.spot.Spot at 0x2d319f3d0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from spotPython.spot import spot\n",
        "from math import inf\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
        "from spotPython.utils.device import getDevice\n",
        "from spotPython.hyperparameters.values import set_data_set\n",
        "from spotPython.data.pkldataset import PKLDataset\n",
        "from spotPython.light.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n",
        "from spotPython.hyperparameters.values import modify_hyper_parameter_levels\n",
        "from spotPython.fun.hyperlight import HyperLight\n",
        "from spotPython.hyperparameters.values import (get_bound_values,\n",
        "    get_var_name,\n",
        "    get_var_type,)\n",
        "from spotPython.utils.eda import gen_design_table\n",
        "from spotPython.light.utils import get_tuned_architecture\n",
        "from spotPython.light.traintest import test_model\n",
        "from spotPython.light.traintest import load_light_from_checkpoint\n",
        "\n",
        "MAX_TIME = 1\n",
        "INIT_SIZE = 5\n",
        "WORKERS = 0\n",
        "PREFIX=\"031\"\n",
        "\n",
        "experiment_name = get_experiment_name(prefix=PREFIX)\n",
        "fun_control = fun_control_init(\n",
        "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
        "    num_workers=WORKERS,\n",
        "    device=getDevice(),\n",
        "    _L_in=133,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True)\n",
        "\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/spotPython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
        "set_data_set(fun_control=fun_control,\n",
        "                data_set=dataset)\n",
        "\n",
        "\n",
        "\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[5,8])\n",
        "modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[2,3])\n",
        "modify_hyper_parameter_bounds(fun_control, \"batch_size\", bounds=[2, 8])\n",
        "modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
        "\n",
        "print(gen_design_table(fun_control))\n",
        "\n",
        "var_type = get_var_type(fun_control)\n",
        "var_name = get_var_name(fun_control)\n",
        "lower = get_bound_values(fun_control, \"lower\")\n",
        "upper = get_bound_values(fun_control, \"upper\")\n",
        "fun = HyperLight(log_level=50).fun\n",
        "spot_tuner = spot.Spot(fun=fun,\n",
        "                       log_level=50,\n",
        "                   lower = lower,\n",
        "                   upper = upper,\n",
        "                   fun_evals = inf,\n",
        "                   max_time = MAX_TIME,\n",
        "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                   var_type = var_type,\n",
        "                   var_name = var_name,\n",
        "                   show_progress= True,\n",
        "                   fun_control = fun_control,\n",
        "                   design_control={\"init_size\": INIT_SIZE},\n",
        "                   surrogate_control={\"noise\": True,\n",
        "                                      \"min_theta\": -4,\n",
        "                                      \"max_theta\": 3,\n",
        "                                      \"n_theta\": len(var_name),\n",
        "                                      \"model_fun_evals\": 10_000,\n",
        "                                      })\n",
        "spot_tuner.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_progress(log_y=False, filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_importance(threshold=0.025, filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_tuned_architecture(spot_tuner, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_model(config, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_loaded = load_light_from_checkpoint(config, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_important_hyperparameter_contour(filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.parallel_plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.traintest import cv_model\n",
        "# set the number of folds to 10\n",
        "fun_control[\"k_folds\"] = 10\n",
        "cv_model(config, fun_control)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
