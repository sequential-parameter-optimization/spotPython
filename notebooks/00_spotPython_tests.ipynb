{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  cache: false\n",
    "  eval: true\n",
    "  echo: true\n",
    "  warning: false\n",
    "title: 'spotpython Tests'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fun_control_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n",
    "fun_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def class_attributes_to_dataframe(class_obj):\n",
    "    # Get the attributes and their values of the class object\n",
    "    attributes = [attr for attr in dir(class_obj) if not callable(getattr(class_obj, attr)) and not attr.startswith(\"__\")]\n",
    "    values = [getattr(class_obj, attr) for attr in attributes]\n",
    "    \n",
    "    # Create a DataFrame from the attributes and values\n",
    "    df = pd.DataFrame({'Attribute Name': attributes, 'Attribute Value': values})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.name = \"John\"\n",
    "        self.age = 30\n",
    "        self.salary = 50000\n",
    "\n",
    "my_instance = MyClass()\n",
    "df = class_attributes_to_dataframe(my_instance)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "n = 10\n",
    "\n",
    "fun = analytical().fun_sphere\n",
    "lower = np.array([-1])\n",
    "upper = np.array([1])\n",
    "design_control={\"init_size\": ni}\n",
    "\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            lower = lower,\n",
    "            upper= upper,\n",
    "            fun_evals = n,\n",
    "            show_progress=True,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "df = spot_1.class_attributes_to_dataframe()\n",
    "stdout.write(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river.linear_model import LogisticRegression\n",
    "from river import metrics\n",
    "from river import optim\n",
    "from river import preprocessing\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset, model, metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.csvdataset import CSVDataset\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data.csv', target_column='prognosis')\n",
    "dataset = CSVDataset(target_column='prognosis')\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.extra_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 3\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Data set VBDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv_file='./data/spotpython/data.csv' as a pandas df and save it as a pickle file\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/spotpython/data.csv')\n",
    "df.to_pickle('./data/spotpython/data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyHcf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.data.daten_sensitive import DatenSensitive\n",
    "from pyhcf.utils.names import get_short_parameter_names\n",
    "daten = DatenSensitive()\n",
    "df = daten.load()\n",
    "names =  df.columns\n",
    "names = get_short_parameter_names(names)\n",
    "# rename columns with short names\n",
    "df.columns = names\n",
    "df.head()\n",
    "# save the df as a csv file\n",
    "df.to_csv('./data/spotpython/data_sensitive.csv', index=False)\n",
    "# save the df as a pickle file\n",
    "df.to_pickle('./data/spotpython/data_sensitive.pkl')\n",
    "# remove all rows with NaN values\n",
    "df = df.dropna()\n",
    "# save the df as a csv file\n",
    "df.to_csv('./data/spotpython/data_sensitive_rmNA.csv', index=False)\n",
    "# save the df as a pickle file\n",
    "df.to_pickle('./data/spotpython/data_sensitive_rmNA.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyHcf data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.light.csvdataset import CSVDataset\n",
    "# import torch\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5000\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     # print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.light.csvdataset import CSVDataset\n",
    "# import torch\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5000\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     # print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset.feature_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(pkl_file='./data/spotpython/data_sensitive.pkl', target_column='A', feature_type=torch.long, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test lightdatamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "# dataset = PKLDataset(directory=\"./data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set size: {len(data_module.data_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation set size: {len(data_module.data_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the DataModule in fun_control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=7)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same with the sensitive data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same, but VBDO data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = CSVDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/VBDP/\", filename=\"train.csv\",target_column='prognosis', feature_type=torch.long)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Hyperdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "lhd = LightHyperDict()\n",
    "lhd.hyper_dict\n",
    "user_lhd = LightHyperDict(filename=\"user_hyper_dict.json\", directory=\"./hyperdict/\")\n",
    "user_lhd.hyper_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes  \n",
    "import torch\n",
    "\n",
    "# Load the diabetes dataset\n",
    "feature_df, target_df = load_diabetes(return_X_y=True, as_frame=True)\n",
    "feature_tensor = torch.tensor(feature_df.values, dtype=torch.float32)\n",
    "target_tensor = torch.tensor(target_df.values, dtype=torch.float32)\n",
    "feature_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "dataset = Diabetes()\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add core model to fun control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.netlightregressione import NetLightRegression\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "fun_control[\"core_model\"].__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the fun_control[\"core_model_hyper_dict\"] is a LightHyperDict\n",
    "isinstance(fun_control[\"core_model_hyper_dict\"], dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test check_X_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "n_hyperparams = len(get_var_name(fun_control))\n",
    "# generate a random np.array X with shape (2, n_hyperparams)\n",
    "X = np.random.rand(2, n_hyperparams)\n",
    "X == hyper_light.check_X_shape(X, fun_control)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test hyperlight fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                    key=\"data_set\",\n",
    "                    value=dataset)\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "X = np.vstack((X, X))\n",
    "y = hyper_light.fun(X, fun_control)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test  NetLightRegression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader)) \n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests optimizer_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "lr_mult=0.1\n",
    "\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adam', dropout_prob=0.1, lr_mult=lr_mult,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# Adam uses a lr which is calculated as lr=lr_mult * 0.001, so this value\n",
    "# should be 0.1 * 0.001 = 0.0001 \n",
    "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*0.001\n",
    "\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adadelta', dropout_prob=0.1, lr_mult=lr_mult,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# Adadelta uses a lr which is calculated as lr=lr_mult * 1.0, so this value\n",
    "# should be 1.0 * 0.1 = 0.1 \n",
    "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
    "from spotpython.light.traintest import train_model, test_model\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "for config in generate_one_config_from_var_dict(var_dict, fun_control):\n",
    "    y_train = train_model(config, fun_control)\n",
    "    y_test = test_model(config, fun_control)\n",
    "    break\n",
    "print(y_train)\n",
    "print(y_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
    "from spotpython.light.traintest import test_model\n",
    "\n",
    "\n",
    "def test_traintest_test_model():\n",
    "    fun_control = fun_control_init(\n",
    "        _L_in=10,\n",
    "        _L_out=1,)\n",
    "\n",
    "    dataset = Diabetes()\n",
    "    set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "    add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                                fun_control=fun_control,\n",
    "                                hyper_dict=LightHyperDict)\n",
    "    X = get_default_hyperparameters_as_array(fun_control)\n",
    "    var_dict = assign_values(X, get_var_name(fun_control))\n",
    "    for vals in generate_one_config_from_var_dict(var_dict, fun_control):\n",
    "        y_test = test_model(test_config=vals,\n",
    "                            fun_control=fun_control)\n",
    "        break\n",
    "    # check if y is a float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test getVarName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = {\"core_model_hyper_dict\":{\n",
    "            \"leaf_prediction\": {\n",
    "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"mean\",\n",
    "                \"core_model_parameter_type\": \"str\"},\n",
    "            \"leaf_model\": {\n",
    "                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"LinearRegression\",\n",
    "                \"core_model_parameter_type\": \"instance\"},\n",
    "            \"splitter\": {\n",
    "                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"EBSTSplitter\",\n",
    "                \"core_model_parameter_type\": \"instance()\"},\n",
    "            \"binary_split\": {\n",
    "                \"levels\": [0, 1],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": 0,\n",
    "                \"core_model_parameter_type\": \"bool\"},\n",
    "            \"stop_mem_management\": {\n",
    "                \"levels\": [0, 1],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": 0,\n",
    "                \"core_model_parameter_type\": \"bool\"}}}\n",
    "len(get_var_name(fun_control))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test netlightregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from math import inf\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import modify_hyper_parameter_bounds\n",
    "from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import (get_bound_values,\n",
    "    get_var_name,\n",
    "    get_var_type,)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.hyperparameters.values import get_tuned_architecture\n",
    "from spotpython.light.testmodel import test_model\n",
    "from spotpython.light.loadmodel import load_light_from_checkpoint\n",
    "\n",
    "MAX_TIME = 1\n",
    "INIT_SIZE = 5\n",
    "WORKERS = 0\n",
    "PREFIX=\"031\"\n",
    "\n",
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    num_workers=WORKERS,\n",
    "    device=getDevice(),\n",
    "    _L_in=133,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True)\n",
    "\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[5,8])\n",
    "modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[3,5])\n",
    "modify_hyper_parameter_bounds(fun_control, \"batch_size\", bounds=[2, 8])\n",
    "modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
    "\n",
    "print(gen_design_table(fun_control))\n",
    "\n",
    "var_type = get_var_type(fun_control)\n",
    "var_name = get_var_name(fun_control)\n",
    "lower = get_bound_values(fun_control, \"lower\")\n",
    "upper = get_bound_values(fun_control, \"upper\")\n",
    "fun = HyperLight(log_level=50).fun\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       log_level=50,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   max_time = MAX_TIME,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type = var_type,\n",
    "                   var_name = var_name,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": INIT_SIZE},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": len(var_name),\n",
    "                                      \"model_fun_evals\": 10_000,\n",
    "                                      })\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_progress(log_y=False, filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_importance(threshold=0.025, filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_tuned_architecture(spot_tuner, fun_control)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = load_light_from_checkpoint(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_important_hyperparameter_contour(filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.parallel_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.cvmodel import cv_model\n",
    "# set the number of folds to 10\n",
    "fun_control[\"k_folds\"] = 10\n",
    "cv_model(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "fun = analytical(sigma=1.0, seed=123)\n",
    "fun.add_noise(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "print(np.array([1, 2, 3, 4, 5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(seed=4321, sigma=0.1)\n",
    "fun = analytical(seed=222, sigma=0.0).fun_sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_1 = spot.Spot(fun=fun,\n",
    "                   lower = np.array([-10]),\n",
    "                   upper = np.array([100]),\n",
    "                   fun_evals = 100,\n",
    "                   fun_repeats = 3,\n",
    "                   max_time = inf,\n",
    "                   noise = True,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type=[\"num\"],\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=111,\n",
    "                   log_level = 10,\n",
    "                   show_models=False,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": 5,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": 1,\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 1000,\n",
    "                                      })\n",
    "spot_1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def squared_euclidean_distance(X_0, X, theta):\n",
    "    return np.sum(theta*(X_0 - X)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
    "from numpy.linalg import cholesky, solve\n",
    "from numpy.random import multivariate_normal\n",
    "def build_Psi(X, theta):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    D = zeros((k, n, n))\n",
    "    for l in range(k):\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
    "    D = sum(D)\n",
    "    D = D + D.T\n",
    "    return exp(-D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0, 1.0])\n",
    "X = np.array([[1.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_Psi(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n",
    "fun = analytical()\n",
    "fun.fun_branin_factor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pi = np.pi\n",
    "X = np.array([[0,0], [-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
    "fun = analytical()\n",
    "fun.fun_branin(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "pi = np.pi\n",
    "X_0 = np.array([[0, 0]])\n",
    "X_1 = np.array([[-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
    "X_2 = np.array([[0,0,0], [0,0,1], [0,0,2]])\n",
    "fun = analytical()\n",
    "y_0 = fun.fun_branin(X_0)\n",
    "y_1 = fun.fun_branin(X_1)\n",
    "y_2 = fun.fun_branin_factor(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(y_1[0], 2) == round(y_1[1],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "round(y_1[0], 2) == round(y_1[2],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[0] == y_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[1] == y_0 + 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[2] == y_0 - 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "import numpy as np\n",
    "n = 100\n",
    "X = np.linspace(0, 10, n, endpoint=False).reshape(-1,1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
    "from numpy.linalg import cholesky, solve\n",
    "from numpy.random import multivariate_normal\n",
    "def build_Psi(X, theta):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    D = zeros((k, n, n))\n",
    "    for l in range(k):\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
    "    D = sum(D)\n",
    "    D = D + D.T\n",
    "    return exp(-D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0])\n",
    "Psi = build_Psi(X, theta)\n",
    "np.round(Psi[:3,:], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = (3, 1, 1), check_valid=\"raise\")\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Y to a 3 x 100 array\n",
    "Y = np.squeeze(Y)\n",
    "Y.shape\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 samples from the GP as a function of X\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, Y.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = 3, check_valid=\"raise\")\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 samples from the GP as a function of X\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, Y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Targets Shape: {targets.shape}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test HyperLight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "n_hyperparams = len(get_var_name(fun_control))\n",
    "# generate a random np.array X with shape (2, n_hyperparams)\n",
    "X = np.random.rand(2, n_hyperparams)\n",
    "X == hyper_light.check_X_shape(X, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n",
    "    get_default_hyperparameters_as_array)\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "# so that two values are returned\n",
    "X = np.vstack((X, X))\n",
    "hyper_light.fun(X, fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test pkldataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\",\n",
    "                    filename=\"data_sensitive.pkl\",\n",
    "                    target_column='N',\n",
    "                    feature_type=torch.float32,\n",
    "                    target_type=torch.float32,\n",
    "                    rmNA=True)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_bound_values\n",
    "from spotpython.hyperparameters.values import get_control_key_value, set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_type_from_var_name\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "set_control_key_value(control_dict=fun_control, key=\"var_type\", value=[\"int\", \"float\", \"str\"], replace=True)\n",
    "set_control_key_value(control_dict=fun_control, key=\"var_name\", value=[\"max_depth\", \"learning_rate\", \"model_type\"], replace=True)\n",
    "\n",
    "print(fun_control)\n",
    "\n",
    "# Test with existing var_name\n",
    "assert get_var_type_from_var_name(var_name=\"max_depth\", fun_control=fun_control) == \"int\"\n",
    "assert get_var_type_from_var_name(var_name=\"learning_rate\", fun_control=fun_control) == \"float\"\n",
    "assert get_var_type_from_var_name(var_name=\"model_type\", fun_control=fun_control) == \"str\"\n",
    "\n",
    "# Test with non-existing var_name\n",
    "with pytest.raises(ValueError):\n",
    "    get_var_type_from_var_name(var_name=\"non_existing\", fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_control_key_value\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import get_var_type_from_var_name\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                    fun_control=fun_control,\n",
    "                    hyper_dict=LightHyperDict)\n",
    "var_type = get_control_key_value(control_dict=fun_control, key=\"var_type\")\n",
    "var_name = get_control_key_value(control_dict=fun_control, key=\"var_name\")\n",
    "print(var_type)\n",
    "print(var_name)\n",
    "vn = \"l1\"\n",
    "get_var_type_from_var_name(fun_control=fun_control, var_name=vn)\n",
    "\n",
    "assert var_type[var_name.index(vn)] == \"int\"\n",
    "assert get_var_type_from_var_name(fun_control, vn) == \"int\"\n",
    "vn = \"initialization\"\n",
    "assert var_type[var_name.index(vn)] == \"factor\"\n",
    "assert var_type[var_name.index(vn)] == \"factor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_control_key_value\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                    fun_control=fun_control,\n",
    "                    hyper_dict=LightHyperDict)\n",
    "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"l1\", value=[1,7])\n",
    "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"initialization\", value=[\"xavier2\", \"kaiming2\"])\n",
    "print(fun_control)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry(dictionary, key, i):\n",
    "    if key in dictionary:\n",
    "        if 'levels' in dictionary[key]:\n",
    "            if i < len(dictionary[key]['levels']):\n",
    "                return dictionary[key]['levels'][i]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from spotpython.data.pkldataset_intern import PKLDataset\n",
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from math import inf\n",
    "\n",
    "MAX_TIME = 60\n",
    "FUN_EVALS = inf\n",
    "INIT_SIZE = 25\n",
    "WORKERS = 0\n",
    "PREFIX=\"031\"\n",
    "DEVICE = getDevice()\n",
    "\n",
    "\n",
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    device=DEVICE,\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=FUN_EVALS,\n",
    "    log_level=10,\n",
    "    max_time=MAX_TIME,\n",
    "    num_workers=WORKERS,\n",
    "    show_progress=True,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "\n",
    "dataset = Diabetes()\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True, rmMF=True)\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"_L_in\",\n",
    "                        value=133,\n",
    "                        replace=True)\n",
    "\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "# from spotpython.hyperparameters.values import modify_hyper_parameter_bounds\n",
    "\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
    "set_control_hyperparameter_value(fun_control, \"epochs\", [4,9])\n",
    "set_control_hyperparameter_value(fun_control, \"batch_size\", [1, 4])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry(dictionary, key, i):\n",
    "    if 'core_model_hyper_dict' in dictionary:\n",
    "        if key in dictionary['core_model_hyper_dict']:\n",
    "            if 'levels' in dictionary['core_model_hyper_dict'][key]:\n",
    "                if i < len(dictionary['core_model_hyper_dict'][key]['levels']):\n",
    "                    return dictionary['core_model_hyper_dict'][key]['levels'][i]\n",
    "    return None\n",
    "print(get_entry(fun_control, \"optimizer\", 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "import numpy as np\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "experiment_name = get_experiment_name(prefix=\"000\")\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    device=getDevice(),\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=15,\n",
    "    log_level=10,\n",
    "    max_time=1,\n",
    "    num_workers=0,\n",
    "    show_progress=True,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
    "assert get_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0) == \"Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_timestamp(only_int=True):\n",
    "    dt = datetime.datetime.now().isoformat(sep=\" \", timespec=\"microseconds\")\n",
    "    if only_int:\n",
    "        # remove - . : and space\n",
    "        dt = dt.replace(\"-\", \"\")\n",
    "        dt = dt.replace(\".\", \"\")\n",
    "        dt = dt.replace(\":\", \"\")\n",
    "        dt = dt.replace(\" \", \"\")\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, surrogate_control_init, design_control_init\n",
    ")\n",
    "\n",
    "def test_plot_progress():\n",
    "    # number of initial points:\n",
    "    ni = 7\n",
    "    # number of points\n",
    "    fun_evals = 10\n",
    "    fun = analytical().fun_sphere\n",
    "    fun_control = fun_control_init(\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        fun_evals=fun_evals,\n",
    "        tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "    design_control=design_control_init(init_size=ni)\n",
    "    surrogate_control=surrogate_control_init(n_theta=3)\n",
    "    S = spot.Spot(fun=fun,\n",
    "                    fun_control=fun_control,\n",
    "                    design_control=design_control,\n",
    "                    surrogate_control=surrogate_control,)\n",
    "    S.run()\n",
    "\n",
    "    # Test plot_progress with different parameters\n",
    "    S.plot_progress(show=False)  # Test with show=False\n",
    "    S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "    S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "    S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename\n",
    "    # add NaN to S.y at position 2\n",
    "    S.y[2] = np.nan\n",
    "    S.plot_progress(show=False)  # Test with show=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, surrogate_control_init, design_control_init\n",
    ")\n",
    "\n",
    "\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "                fun_control=fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "\n",
    "# remove points from S.y so that there are less than ni points\n",
    "S.y = S.y[:3]\n",
    "# Test plot_progress with different parameters\n",
    "S.plot_progress(show=False)  # Test with show=False\n",
    "S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import differential_evolution\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "def objective_function(X, fun_control=None):\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X)\n",
    "    if X.shape[1] != 2:\n",
    "        raise Exception\n",
    "    x0 = X[:, 0]\n",
    "    x1 = X[:, 1]\n",
    "    y = x0**2 + 10*x1**2\n",
    "    return y\n",
    "fun_control = fun_control_init(\n",
    "            lower = np.array([0, 0]),\n",
    "            upper = np.array([10, 10]),\n",
    "            fun_evals=8,\n",
    "            fun_repeats=1,\n",
    "            max_time=inf,\n",
    "            noise=True,\n",
    "            tolerance_x=0,\n",
    "            ocba_delta=0,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            infill_criterion=\"ei\",\n",
    "            n_points=1,\n",
    "            seed=123,\n",
    "            log_level=10,\n",
    "            show_models=False,\n",
    "            show_progress=True)\n",
    "design_control = design_control_init(\n",
    "            init_size=5,\n",
    "            repeats=1)\n",
    "surrogate_control = surrogate_control_init(\n",
    "            log_level=10,\n",
    "            model_optimizer=differential_evolution,\n",
    "            model_fun_evals=10000,\n",
    "            min_theta=-3,\n",
    "            max_theta=3,\n",
    "            n_theta=2,\n",
    "            theta_init_zero=True,\n",
    "            n_p=1,\n",
    "            optim_p=False,\n",
    "            noise=True,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            seed=124, \n",
    "            min_Lambda=1,\n",
    "            max_Lambda=10)\n",
    "optimizer_control = optimizer_control_init(\n",
    "            max_iter=1000,\n",
    "            seed=125)\n",
    "spot = spot.Spot(fun=objective_function,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,\n",
    "            optimizer_control=optimizer_control\n",
    "            )\n",
    "spot.run()\n",
    "spot.plot_progress()\n",
    "spot.plot_contour(i=0, j=1)\n",
    "spot.plot_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n",
    "\n",
    "fun = analytical().fun_branin\n",
    "fun_control = fun_control_init(lower = np.array([-5, 0]),\n",
    "                               upper = np.array([10, 15]),\n",
    "                               fun_evals=20)\n",
    "design_control = design_control_init(init_size=10)\n",
    "surrogate_control = surrogate_control_init(n_theta=2)\n",
    "S = spot.Spot(fun=fun, fun_control=fun_control, design_control=design_control)\n",
    "S.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.plot_progress(log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.surrogate.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = analytical().fun_sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,100).reshape(-1,1)\n",
    "y = fun(x)\n",
    "plt.figure()\n",
    "plt.plot(x,y, \"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "                   fun_control=fun_control_init(\n",
    "                        lower = np.array([-10]),\n",
    "                        upper = np.array([100]),\n",
    "                        fun_evals = 7,\n",
    "                        fun_repeats = 1,\n",
    "                        max_time = inf,\n",
    "                        noise = False,\n",
    "                        tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                        var_type=[\"num\"],\n",
    "                        infill_criterion = \"y\",\n",
    "                        n_points = 1,\n",
    "                        seed=123,\n",
    "                        log_level = 50),\n",
    "                   design_control=design_control_init(\n",
    "                        init_size=5,\n",
    "                        repeats=1),\n",
    "                   surrogate_control=surrogate_control_init(\n",
    "                        noise=False,\n",
    "                        min_theta=-4,\n",
    "                        max_theta=3,\n",
    "                        n_theta=1,\n",
    "                        model_optimizer=differential_evolution,\n",
    "                        model_fun_evals=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "fun = analytical().fun_sphere\n",
    "from spotpython.design.spacefilling import spacefilling\n",
    "design = spacefilling(2)\n",
    "from scipy.optimize import differential_evolution\n",
    "optimizer = differential_evolution\n",
    "from spotpython.build.kriging import Kriging\n",
    "surrogate = Kriging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\n",
    "fun_control=fun_control_init(lower=np.array([-1, -1]),\n",
    "                            upper=np.array([1, 1]))\n",
    "design_control=design_control_init()\n",
    "optimizer_control=optimizer_control_init()\n",
    "surrogate_control=surrogate_control_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       fun_control=fun_control,\n",
    "                       design_control=design_control,\n",
    "                       optimizer_control=optimizer_control,\n",
    "                       surrogate_control=surrogate_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytest\n",
    "import torch\n",
    "from pyhcf.data.loadHcfData import build_df, load_hcf_data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list=[\"L\", \"AQ\", \"AS\"]\n",
    "dataset = load_hcf_data(param_list=p_list, target=\"T\",\n",
    "                        rmNA=True, rmMF=True,\n",
    "                        load_all_features=False,\n",
    "                        load_thermo_features=False,\n",
    "                        scale_data=True,\n",
    "                        return_X_y=False)\n",
    "assert isinstance(dataset, torch.utils.data.TensorDataset)\n",
    "assert len(dataset) > 0\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader    \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    assert inputs.size(0) == batch_size\n",
    "    print(f\"Inputs Shape: {inputs.shape[1]}\")\n",
    "    print(f\"P List: {p_list}\")\n",
    "    print(f\"P List Length: {len(p_list)}\")\n",
    "    # input is p_list + 1 (for target)\n",
    "    # p_list = [\"L\", \"AQ\", \"AS\"] plus target \"N\"\n",
    "    assert inputs.shape[1] + 1 == len(p_list)\n",
    "    print(f\"Targets Shape: {targets.shape[0]}\")\n",
    "    assert targets.shape[0] == batch_size\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "# data.csv is simple csv file with 11 samples\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup(stage=\"predict\")\n",
    "print(f\"Predict set size: {len(data_module.data_predict)}\")\n",
    "for batch in data_module.predict_dataloader():\n",
    "    inputs, targets = batch\n",
    "    print(f\"inputs: {inputs}\")\n",
    "    print(f\"targets: {targets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_module.data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_div2_list(n, n_min):\n",
    "    result = []\n",
    "    current = n\n",
    "    while current >= n_min:\n",
    "        result.extend([current] * (n // current))\n",
    "        current = current // 2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_L_in = 128\n",
    "l1 = \n",
    "\n",
    "n_low = _L_in // 4\n",
    "# ensure that n_high is larger than n_low\n",
    "n_high = max(l1, 2 * n_low)\n",
    "generate_div2_list(n_high, n_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.math import generate_div2_list\n",
    "generate_div2_list(64, 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.transformer.positionalEncoding import PositionalEncoding\n",
    "import torch\n",
    "# number of tensors\n",
    "n = 3\n",
    "# dimension of each tensor\n",
    "k = 32\n",
    "pe = PositionalEncoding(d_model=k, dropout_prob=0, verbose=False)\n",
    "input = torch.zeros(1, n, k)\n",
    "# Generate a tensor of size (1, 10, 4) with values from 1 to 10\n",
    "for i in range(n):\n",
    "    input[0, i, :] = i\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Input: {input}\")\n",
    "output = pe(input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.transformer.skiplinear import SkipLinear\n",
    "import torch\n",
    "n_in = 2\n",
    "n_out = 4\n",
    "sl = SkipLinear(n_in, n_out)\n",
    "input = torch.zeros(1, n_in)\n",
    "for i in range(n_in):\n",
    "    input[0, i] = i\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Input: {input}\")\n",
    "output = sl(input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(sl.lst_modules)\n",
    "for i in sl.lst_modules:\n",
    "    print(f\"weights: {i.weights}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Example from J. Caffrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_income_transformer.py\n",
    "# predict income from sex, age, city, politics\n",
    "# PyTorch 2.0.0-CPU Anaconda3-2022.10  Python 3.9.13\n",
    "# Windows 10/11 \n",
    "\n",
    "# Transformer component for regression\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "\n",
    "device = T.device('cpu')  # apply to Tensor or Module\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class PeopleDataset(T.utils.data.Dataset):\n",
    "  def __init__(self, src_file):\n",
    "    # sex age   state   income   politics\n",
    "    # -1  0.27  0 1 0   0.7610   0 0 1\n",
    "    # +1  0.19  0 0 1   0.6550   1 0 0\n",
    "\n",
    "    tmp_x = np.loadtxt(src_file, usecols=[0,1,2,3,4,6,7,8],\n",
    "      delimiter=\",\", comments=\"#\", dtype=np.float32)\n",
    "    tmp_y = np.loadtxt(src_file, usecols=5, delimiter=\",\",\n",
    "      comments=\"#\", dtype=np.float32)\n",
    "    tmp_y = tmp_y.reshape(-1,1)  # 2D required\n",
    "\n",
    "    self.x_data = T.tensor(tmp_x, dtype=T.float32).to(device)\n",
    "    self.y_data = T.tensor(tmp_y, dtype=T.float32).to(device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    preds = self.x_data[idx]\n",
    "    incom = self.y_data[idx] \n",
    "    return (preds, incom)  # as a tuple\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class SkipLinear(T.nn.Module):\n",
    "\n",
    "  # -----\n",
    "\n",
    "  class Core(T.nn.Module):\n",
    "    def __init__(self, n):\n",
    "      super().__init__()\n",
    "      # 1 node to n nodes, n gte 2\n",
    "      self.weights = T.nn.Parameter(T.zeros((n,1),\n",
    "        dtype=T.float32))\n",
    "      self.biases = T.nn.Parameter(T.tensor(n,\n",
    "        dtype=T.float32))\n",
    "      lim = 0.01\n",
    "      T.nn.init.uniform_(self.weights, -lim, lim)\n",
    "      T.nn.init.zeros_(self.biases)\n",
    "\n",
    "    def forward(self, x):\n",
    "      wx= T.mm(x, self.weights.t())\n",
    "      v = T.add(wx, self.biases)\n",
    "      return v\n",
    "\n",
    "  # -----\n",
    "\n",
    "  def __init__(self, n_in, n_out):\n",
    "    super().__init__()\n",
    "    self.n_in = n_in; self.n_out = n_out\n",
    "    if n_out  % n_in != 0:\n",
    "      print(\"FATAL: n_out must be divisible by n_in\")\n",
    "    n = n_out // n_in  # num nodes per input\n",
    "\n",
    "    self.lst_modules = \\\n",
    "      T.nn.ModuleList([SkipLinear.Core(n) for \\\n",
    "        i in range(n_in)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    lst_nodes = []\n",
    "    for i in range(self.n_in):\n",
    "      xi = x[:,i].reshape(-1,1)\n",
    "      oupt = self.lst_modules[i](xi)\n",
    "      lst_nodes.append(oupt)\n",
    "    result = T.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "    for i in range(2,self.n_in):\n",
    "      result = T.cat((result, lst_nodes[i]), 1)\n",
    "    result = result.reshape(-1, self.n_out)\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class PositionalEncoding(T.nn.Module):  # documentation code\n",
    "  def __init__(self, d_model: int, dropout: float=0.1,\n",
    "   max_len: int=5000):\n",
    "    super(PositionalEncoding, self).__init__()  # old syntax\n",
    "    self.dropout = T.nn.Dropout(p=dropout)\n",
    "    pe = T.zeros(max_len, d_model)  # like 10x4\n",
    "    position = \\\n",
    "      T.arange(0, max_len, dtype=T.float).unsqueeze(1)\n",
    "    div_term = T.exp(T.arange(0, d_model, 2).float() * \\\n",
    "      (-np.log(10_000.0) / d_model))\n",
    "    pe[:, 0::2] = T.sin(position * div_term)\n",
    "    pe[:, 1::2] = T.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    self.register_buffer('pe', pe)  # allows state-save\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:x.size(0), :]\n",
    "    return self.dropout(x)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class TransformerNet(T.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TransformerNet, self).__init__()\n",
    "    self.embed = SkipLinear(8, 32)  # 8 inputs, each goes to 4 \n",
    "    self.pos_enc = \\\n",
    "      PositionalEncoding(4, dropout=0.20)  # positional\n",
    "    self.enc_layer = T.nn.TransformerEncoderLayer(d_model=4,\n",
    "      nhead=2, dim_feedforward=10, \n",
    "      batch_first=True)  # d_model divisible by nhead\n",
    "    self.trans_enc = T.nn.TransformerEncoder(self.enc_layer,\n",
    "      num_layers=2)  # 6 layers default\n",
    "\n",
    "    self.fc1 = T.nn.Linear(32, 10)  # 8--32-T-10-1\n",
    "    self.fc2 = T.nn.Linear(10, 1)\n",
    "\n",
    "    # default weight and bias initialization\n",
    "\n",
    "  def forward(self, x):\n",
    "    z = self.embed(x)  # 8 inpts to 32 embed\n",
    "    z = z.reshape(-1, 8, 4)  # bat seq embed\n",
    "    z = self.pos_enc(z) \n",
    "    z = self.trans_enc(z) \n",
    "    z = z.reshape(-1, 32)  # torch.Size([bs, xxx])\n",
    "    z = T.tanh(self.fc1(z))\n",
    "    z = self.fc2(z)  # regression: no activation\n",
    "    return z\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def accuracy(model, ds, pct_close):\n",
    "  # assumes model.eval()\n",
    "  # correct within pct of true income\n",
    "  n_correct = 0; n_wrong = 0\n",
    "\n",
    "  for i in range(len(ds)):\n",
    "    X = ds[i][0].reshape(1,-1)  # make it a batch\n",
    "    Y = ds[i][1].reshape(1)\n",
    "    with T.no_grad():\n",
    "      oupt = model(X)         # computed income\n",
    "\n",
    "    if T.abs(oupt - Y) <= T.abs(pct_close * Y):\n",
    "      n_correct += 1\n",
    "    else:\n",
    "      n_wrong += 1\n",
    "  acc = (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "  return acc\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def accuracy_x(model, ds, pct_close):\n",
    "  # all-at-once (quick)\n",
    "  # assumes model.eval()\n",
    "  X = ds.x_data  # all inputs\n",
    "  Y = ds.y_data  # all targets\n",
    "  n_items = len(X)\n",
    "  with T.no_grad():\n",
    "    pred = model(X)  # all predicted incomes\n",
    " \n",
    "  n_correct = T.sum((T.abs(pred - Y) <= \\\n",
    "    T.abs(pct_close * Y)))\n",
    "  result = (n_correct.item() / n_items)  # scalar\n",
    "  return result  \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def train(model, ds, bs, lr, me, le, test_ds):\n",
    "  # dataset, bat_size, lrn_rate, max_epochs, log interval\n",
    "  train_ldr = T.utils.data.DataLoader(ds, batch_size=bs,\n",
    "    shuffle=True)\n",
    "  loss_func = T.nn.MSELoss()\n",
    "  optimizer = T.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "  for epoch in range(0, me):\n",
    "    epoch_loss = 0.0  # for one full epoch\n",
    "    for (b_idx, batch) in enumerate(train_ldr):\n",
    "      X = batch[0]  # predictors\n",
    "      y = batch[1]  # target income\n",
    "      optimizer.zero_grad()\n",
    "      oupt = model(X)\n",
    "      loss_val = loss_func(oupt, y)  # a tensor\n",
    "      epoch_loss += loss_val.item()  # accumulate\n",
    "      loss_val.backward()  # compute gradients\n",
    "      optimizer.step()     # update weights\n",
    "\n",
    "    if epoch % le == 0:\n",
    "      print(\"epoch = %4d  |  loss = %0.4f\" % \\\n",
    "        (epoch, epoch_loss))\n",
    "      # model.eval()\n",
    "      # print(\"-------------\")\n",
    "      # acc_train = accuracy(model, ds, 0.10)\n",
    "      # print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
    "      # acc_test = accuracy(model, test_ds, 0.10) \n",
    "      # print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
    "      # model.train()\n",
    "      # print(\"-------------\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "  # 0. get started\n",
    "  print(\"\\nBegin People predict income using Transformer \")\n",
    "  T.manual_seed(0)\n",
    "  np.random.seed(0)\n",
    "  \n",
    "\n",
    "\n",
    "  # 1. create Dataset objects\n",
    "  print(\"\\nCreating People Dataset objects \")\n",
    "  train_file = \"../src/spotpython/data/people_train.csv\"\n",
    "  train_ds = PeopleDataset(train_file)  # 200 rows\n",
    "\n",
    "  test_file = \"../src/spotpython/data/people_test.csv\"\n",
    "  test_ds = PeopleDataset(test_file)  # 40 rows\n",
    "\n",
    "  # 2. create network\n",
    "  print(\"\\nCreating (8--32)-T-10-1 neural network \")\n",
    "  net = TransformerNet().to(device)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 3. train model\n",
    "  print(\"\\nbat_size = 10 \")\n",
    "  print(\"loss = MSELoss() \")\n",
    "  print(\"optimizer = Adam \")\n",
    "  print(\"lrn_rate = 0.01 \")\n",
    "\n",
    "  print(\"\\nStarting training\")\n",
    "  net.train()\n",
    "  train(net, train_ds, bs=10, lr=0.01, me=300,\n",
    "    le=50, test_ds=test_ds)\n",
    "  print(\"Done \")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 4. evaluate model accuracy\n",
    "  print(\"\\nComputing model accuracy (within 0.10 of true) \")\n",
    "  net.eval()\n",
    "  acc_train = accuracy(net, train_ds, 0.10)  # item-by-item\n",
    "  print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
    "\n",
    "  acc_test = accuracy_x(net, test_ds, 0.10)  # all-at-once\n",
    "  print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 5. make a prediction\n",
    "  print(\"\\nPredicting income for M 34 Oklahoma moderate: \")\n",
    "  x = np.array([[-1, 0.34, 0,0,1,  0,1,0]],\n",
    "    dtype=np.float32)\n",
    "  x = T.tensor(x, dtype=T.float32).to(device) \n",
    "\n",
    "  with T.no_grad():\n",
    "    pred_inc = net(x)\n",
    "  pred_inc = pred_inc.item()  # scalar\n",
    "  print(\"$%0.2f\" % (pred_inc * 100_000))  # un-normalized\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 6. save model (state_dict approach)\n",
    "  print(\"\\nSaving trained model state\")\n",
    "  fn = \".\\\\Models\\\\people_income_model.pt\"\n",
    "  T.save(net.state_dict(), fn)\n",
    "\n",
    "  # model = Net()\n",
    "  # model.load_state_dict(T.load(fn))\n",
    "  # use model to make prediction(s)\n",
    "\n",
    "  print(\"\\nEnd People income demo \")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SkipLinear(torch.nn.Module):\n",
    "    class Core(torch.nn.Module):\n",
    "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
    "\n",
    "        def __init__(self, n):\n",
    "            \"\"\"\n",
    "            Initialize the layer.\n",
    "\n",
    "            Args:\n",
    "                n (int): The number of output nodes.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
    "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
    "            lim = 0.01\n",
    "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
    "\n",
    "        def forward(self, x)->torch.Tensor:\n",
    "            \"\"\"\n",
    "            Forward pass through the layer.\n",
    "\n",
    "            Args:\n",
    "                x (torch.Tensor): The input tensor.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The output of the layer.\n",
    "            \"\"\"\n",
    "            return x @ self.weights.t() + self.biases\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        if n_out % n_in != 0:\n",
    "            raise ValueError(\"n_out % n_in != 0\")\n",
    "        n = n_out // n_in  # num nodes per input\n",
    "\n",
    "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        lst_nodes = []\n",
    "        for i in range(self.n_in):\n",
    "            xi = x[:, i].reshape(-1, 1)\n",
    "            oupt = self.lst_modules[i](xi)\n",
    "            lst_nodes.append(oupt)\n",
    "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "        for i in range(2, self.n_in):\n",
    "            result = torch.cat((result, lst_nodes[i]), 1)\n",
    "        result = result.reshape(-1, self.n_out)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipLinear(torch.nn.Module):\n",
    "\n",
    "    class Core(torch.nn.Module):\n",
    "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
    "\n",
    "        def __init__(self, n):\n",
    "            \"\"\"\n",
    "            Initialize the layer.\n",
    "\n",
    "            Args:\n",
    "                n (int): The number of output nodes.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
    "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
    "            lim = 0.01\n",
    "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
    "\n",
    "        def forward(self, x) -> torch.Tensor:\n",
    "            \"\"\"\n",
    "            Forward pass through the layer.\n",
    "\n",
    "            Args:\n",
    "                x (torch.Tensor): The input tensor.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The output of the layer.\n",
    "            \"\"\"\n",
    "            return x @ self.weights.t() + self.biases\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        if n_out % n_in != 0:\n",
    "            raise ValueError(\"n_out % n_in != 0\")\n",
    "        n = n_out // n_in  # num nodes per input\n",
    "\n",
    "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        lst_nodes = []\n",
    "        for i in range(self.n_in):\n",
    "            xi = x[:, i].reshape(-1, 1)\n",
    "            oupt = self.lst_modules[i](xi)\n",
    "            lst_nodes.append(oupt)\n",
    "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "        for i in range(2, self.n_in):\n",
    "            result = torch.cat((result, lst_nodes[i]), 1)\n",
    "        result = result.reshape(-1, self.n_out)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spotGUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import get_default_values, get_bound_values\n",
    "lhd = LightHyperDict()\n",
    "# generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
    "fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
    "get_default_values(fun_control)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import json\n",
    "from spotpython.hyperparameters.values import get_default_values, get_bound_values\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "\n",
    "\n",
    "def create_gui(model):\n",
    "    lhd = LightHyperDict()\n",
    "    # generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
    "    fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
    "\n",
    "    # Apply the functions to the dictionary\n",
    "    default_values = get_default_values(fun_control)\n",
    "    lower_bound_values = get_bound_values(fun_control, \"lower\")\n",
    "    upper_bound_values = get_bound_values(fun_control, \"upper\")\n",
    "\n",
    "    # Create a tkinter window\n",
    "    root = tk.Tk()\n",
    "\n",
    "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
    "    for i, (key, value) in enumerate(lhd.hyper_dict['NetLightRegression'].items()):\n",
    "            # Create a label with the key as text\n",
    "            label = tk.Label(root, text=key)\n",
    "            label.grid(row=i, column=0, sticky=\"W\")\n",
    "\n",
    "            # Create an entry with the default value as the default text\n",
    "            default_entry = tk.Entry(root)\n",
    "            default_entry.insert(0, value)\n",
    "            default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "        # add the lower bound values in column 2\n",
    "            lower_bound_entry = tk.Entry(root)\n",
    "            lower_bound_entry.insert(0, lower_bound_values[i])\n",
    "            lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
    "        # add the upper bound values in column 3\n",
    "            upper_bound_entry = tk.Entry(root)\n",
    "            upper_bound_entry.insert(0, upper_bound_values[i])\n",
    "            upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
    "\n",
    "    # Run the tkinter main loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Call the function to create the GUI\n",
    "create_gui(model = 'NetLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "\n",
    "\n",
    "def create_gui(model):\n",
    "    lhd = LightHyperDict()\n",
    "    dict =  lhd.hyper_dict[model]\n",
    "\n",
    "    \n",
    "    # Create a tkinter window\n",
    "    root = tk.Tk()\n",
    "\n",
    "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
    "    # TODO: Add labels to the column headers\n",
    "    for i, (key, value) in enumerate(dict.items()):            \n",
    "            if dict[key][\"type\"] == \"int\" or dict[key][\"type\"] == \"float\":\n",
    "                # Create a label with the key as text\n",
    "                label = tk.Label(root, text=key)\n",
    "                label.grid(row=i, column=0, sticky=\"W\")\n",
    "                # Create an entry with the default value as the default text\n",
    "                default_entry = tk.Entry(root)\n",
    "                default_entry.insert(0, dict[key][\"default\"])\n",
    "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "                # add the lower bound values in column 2\n",
    "                lower_bound_entry = tk.Entry(root)                \n",
    "                lower_bound_entry.insert(0, dict[key][\"lower\"])\n",
    "                lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
    "                # add the upper bound values in column 3\n",
    "                upper_bound_entry = tk.Entry(root)\n",
    "                upper_bound_entry.insert(0, dict[key][\"upper\"])\n",
    "                upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
    "            if dict[key][\"type\"] == \"factor\":        \n",
    "                # Create a label with the key as text\n",
    "                label = tk.Label(root, text=key)\n",
    "                label.grid(row=i, column=0, sticky=\"W\")\n",
    "                # Create an entry with the default value as the default text\n",
    "                default_entry = tk.Entry(root)\n",
    "                default_entry.insert(0, dict[key][\"default\"])\n",
    "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "                # add the lower bound values in column 2\n",
    "                factor_level_entry = tk.Entry(root)\n",
    "                # add a comma to each level\n",
    "                dict[key][\"levels\"] = \", \".join(dict[key][\"levels\"])                                \n",
    "                factor_level_entry.insert(0, dict[key][\"levels\"])\n",
    "                # TODO: Fix columnspan\n",
    "                factor_level_entry.grid(row=i, column=2, columnspan=2, sticky=\"W\")\n",
    "\n",
    "    # Run the tkinter main loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Call the function to create the GUI\n",
    "create_gui(model = 'NetLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gui(model = 'TransformerLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save Load Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spotpython.utils.file import save_experiment, load_experiment\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "\n",
    "def test_file_save_load():\n",
    "    fun = analytical().fun_branin\n",
    "\n",
    "    fun_control = fun_control_init(\n",
    "                PREFIX=\"branin\",\n",
    "                SUMMARY_WRITER=False,\n",
    "                lower = np.array([0, 0]),\n",
    "                upper = np.array([10, 10]),\n",
    "                fun_evals=8,\n",
    "                fun_repeats=1,\n",
    "                max_time=inf,\n",
    "                noise=False,\n",
    "                tolerance_x=0,\n",
    "                ocba_delta=0,\n",
    "                var_type=[\"num\", \"num\"],\n",
    "                infill_criterion=\"ei\",\n",
    "                n_points=1,\n",
    "                seed=123,\n",
    "                log_level=20,\n",
    "                show_models=False,\n",
    "                show_progress=True)\n",
    "    design_control = design_control_init(\n",
    "                init_size=5,\n",
    "                repeats=1)\n",
    "    surrogate_control = surrogate_control_init(\n",
    "                model_fun_evals=10000,\n",
    "                min_theta=-3,\n",
    "                max_theta=3,\n",
    "                n_theta=2,\n",
    "                theta_init_zero=True,\n",
    "                n_p=1,\n",
    "                optim_p=False,\n",
    "                var_type=[\"num\", \"num\"],\n",
    "                seed=124)\n",
    "    optimizer_control = optimizer_control_init(\n",
    "                max_iter=1000,\n",
    "                seed=125)\n",
    "    spot_tuner = spot.Spot(fun=fun,\n",
    "                fun_control=fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control,\n",
    "                optimizer_control=optimizer_control)\n",
    "    # Call the save_experiment function\n",
    "    filename = save_experiment(\n",
    "        spot_tuner=spot_tuner,\n",
    "        fun_control=fun_control,\n",
    "        design_control=None,\n",
    "        surrogate_control=None,\n",
    "        optimizer_control=None\n",
    "    )\n",
    "\n",
    "    # Verify that the pickle file is created\n",
    "    assert os.path.exists(filename)\n",
    "\n",
    "    # Call the load_experiment function\n",
    "    spot_tuner_1, fun_control_1, design_control_1, surrogate_control_1, optimizer_control_1 = load_experiment(filename)\n",
    "\n",
    "    # Verify the name of the pickle file\n",
    "    assert filename == f\"spot_{fun_control['PREFIX']}experiment.pickle\"\n",
    "\n",
    "    # Clean up the temporary directory\n",
    "    os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_save_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netlightregression2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression2 import NetLightRegression2\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "BATCH_SIZE = 8\n",
    "dataset = Diabetes()\n",
    "train1_set, test_set = torch.utils.data.random_split(dataset, [0.6, 0.4])\n",
    "train_set, val_set = torch.utils.data.random_split(train1_set, [0.6, 0.4])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(f\"batch_x.shape: {batch_x.shape}\")\n",
    "print(f\"batch_y.shape: {batch_y.shape}\")\n",
    "net_light_base = NetLightRegression2(l1=128,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='Default',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=10,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=10,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "# data.csv is simple csv file with 11 samples\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from spotpython.utils.file import save_experiment, load_experiment\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "fun = analytical().fun_branin\n",
    "fun_control = fun_control_init(\n",
    "            PREFIX=\"branin\",\n",
    "            SUMMARY_WRITER=False,\n",
    "            lower = np.array([0, 0]),\n",
    "            upper = np.array([10, 10]),\n",
    "            fun_evals=8,\n",
    "            fun_repeats=1,\n",
    "            max_time=inf,\n",
    "            noise=False,\n",
    "            tolerance_x=0,\n",
    "            ocba_delta=0,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            infill_criterion=\"ei\",\n",
    "            n_points=1,\n",
    "            seed=123,\n",
    "            log_level=20,\n",
    "            show_models=False,\n",
    "            show_progress=True)\n",
    "design_control = design_control_init(\n",
    "            init_size=5,\n",
    "            repeats=1)\n",
    "surrogate_control = surrogate_control_init(\n",
    "            model_fun_evals=10000,\n",
    "            min_theta=-3,\n",
    "            max_theta=3,\n",
    "            n_theta=2,\n",
    "            theta_init_zero=True,\n",
    "            n_p=1,\n",
    "            optim_p=False,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            seed=124)\n",
    "optimizer_control = optimizer_control_init(\n",
    "            max_iter=1000,\n",
    "            seed=125)\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,\n",
    "            optimizer_control=optimizer_control)\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters(fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tuned Hyperparameters from a Machine/Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.device import getDevice\n",
    "from math import inf\n",
    "from spotpython.utils.init import fun_control_init\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "\n",
    "MAX_TIME = 1\n",
    "FUN_EVALS = 10\n",
    "INIT_SIZE = 5\n",
    "WORKERS = 0\n",
    "PREFIX=\"037\"\n",
    "DEVICE = getDevice()\n",
    "DEVICES = 1\n",
    "TEST_SIZE = 0.4\n",
    "TORCH_METRIC = \"mean_squared_error\"\n",
    "dataset = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    _torchmetric=TORCH_METRIC,\n",
    "    PREFIX=PREFIX,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    data_set=dataset,\n",
    "    device=DEVICE,\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=FUN_EVALS,\n",
    "    log_level=50,\n",
    "    max_time=MAX_TIME,\n",
    "    num_workers=WORKERS,\n",
    "    show_progress=True,\n",
    "    test_size=TEST_SIZE,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "add_core_model_to_fun_control(fun_control=fun_control,\n",
    "                              core_model=NetLightRegression,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n",
    "set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n",
    "set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\n",
    "                \"Adam\",\n",
    "                \"RAdam\",\n",
    "            ])\n",
    "set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n",
    "set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n",
    "set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n",
    "set_control_hyperparameter_value(fun_control, \"act_fn\",[\n",
    "                \"ReLU\",\n",
    "                \"LeakyReLU\"\n",
    "            ] )\n",
    "from spotpython.utils.init import design_control_init, surrogate_control_init\n",
    "design_control = design_control_init(init_size=INIT_SIZE)\n",
    "\n",
    "surrogate_control = surrogate_control_init(noise=True,\n",
    "                                            n_theta=2)\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "fun = HyperLight(log_level=50).fun\n",
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       fun_control=fun_control,\n",
    "                       design_control=design_control,\n",
    "                       surrogate_control=surrogate_control)\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors\n",
    "\n",
    "* Example from https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/012_num_spot_ei.html#factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.design.spacefilling import spacefilling\n",
    "from spotpython.build.kriging import Kriging\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = spacefilling(2)\n",
    "n = 30\n",
    "rng = np.random.RandomState(1)\n",
    "lower = np.array([-5,-0])\n",
    "upper = np.array([10,15])\n",
    "fun = analytical().fun_branin_factor\n",
    "#fun = analytical(sigma=0).fun_sphere\n",
    "\n",
    "X0 = gen.scipy_lhd(n, lower=lower, upper = upper)\n",
    "X1 = np.random.randint(low=0, high=3, size=(n,))\n",
    "X = np.c_[X0, X1]\n",
    "y = fun(X)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "S = Kriging(name='kriging',  seed=123, log_level=10, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\n",
    "S.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sf = Kriging(name='kriging',  seed=123, log_level=10, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"factor\"])\n",
    "# Sf = Kriging(name='kriging',  seed=123, log_level=50, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\n",
    "Sf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0\n",
    "for _ in range(100):\n",
    "    n = 100\n",
    "    X0 = gen.scipy_lhd(n, lower=lower, upper = upper)\n",
    "    X1 = np.random.randint(low=0, high=3, size=(n,))\n",
    "    X = np.c_[X0, X1]\n",
    "    y = fun(X)\n",
    "    s=np.sum(np.abs(S.predict(X) - y))\n",
    "    sf=np.sum(np.abs(Sf.predict(X) - y))\n",
    "    res = res + (sf - s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def select_distant_points(X, y, k):\n",
    "    \"\"\"\n",
    "    Selects k points that are distant from each other using a clustering approach.\n",
    "    \n",
    "    :param X: np.array of shape (n, k), with n points in k-dimensional space.\n",
    "    :param y: np.array of length n, with values corresponding to each point in X.\n",
    "    :param k: The number of distant points to select.\n",
    "    :return: Selected k points from X and their corresponding y values.\n",
    "    \"\"\"\n",
    "    # Perform k-means clustering to find k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n",
    "    \n",
    "    # Find the closest point in X to each cluster center\n",
    "    selected_points = np.array([X[np.argmin(np.linalg.norm(X - center, axis=1))] for center in kmeans.cluster_centers_])\n",
    "    \n",
    "    # Find indices of the selected points in the original X array\n",
    "    indices = np.array([np.where(np.all(X==point, axis=1))[0][0] for point in selected_points])\n",
    "    \n",
    "    # Select the corresponding y values\n",
    "    selected_y = y[indices]\n",
    "    \n",
    "    return selected_points, selected_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 2)  # Generate some random points\n",
    "y = np.random.rand(100)     # Random corresponding y values\n",
    "k = 5\n",
    "\n",
    "selected_points, selected_y = select_distant_points(X, y, k)\n",
    "print(\"Selected Points:\", selected_points)\n",
    "print(\"Corresponding y values:\", selected_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 5\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1, -1]),\n",
    "    upper = np.array([1, 1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "S.plot_important_hyperparameter_contour(max_imp=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n",
    "\n",
    "# Sorting the array in descending order by the second element of each sub-list\n",
    "sorted_array = sorted(array, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(sorted_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_second_and_return_indices(array):\n",
    "    \"\"\"\n",
    "    Sorts an array of arrays based on the second values in descending order and returns\n",
    "    the indices of the original array entries.\n",
    "\n",
    "    :param array: List of lists, where each inner list has at least two elements.\n",
    "    :return: Indices of the original array entries after sorting by the second value.\n",
    "             Returns an empty list if the input is empty or None.\n",
    "    :raises ValueError: If any sub-array is improperly structured.\n",
    "    \"\"\"\n",
    "    if not array:\n",
    "        return []\n",
    "\n",
    "    # Check for improperly structured sub-arrays\n",
    "    for item in array:\n",
    "        if not isinstance(item, list) or len(item) < 2:\n",
    "            raise ValueError(\"All sub-arrays must be lists with at least two elements.\")\n",
    "\n",
    "    # Enumerate the array to keep track of original indices, then sort by the second item\n",
    "    sorted_indices = [index for index, value in sorted(enumerate(array), key=lambda x: x[1][1], reverse=True)]\n",
    "\n",
    "    return sorted_indices\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n",
    "    indices = sort_by_second_and_return_indices(array)\n",
    "    print(\"Indices of the sorted elements:\", indices)\n",
    "except ValueError as error:\n",
    "    print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Core Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree import HoeffdingAdaptiveTreeRegressor\n",
    "from spotriver.data.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_for_core_model, get_default_values\n",
    "fun_control = {}\n",
    "add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=RiverHyperDict,\n",
    "    filename=None)\n",
    "values = get_default_values(fun_control)\n",
    "print(values)\n",
    "# get_default_hyperparameters_for_core_model(fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytest\n",
    "import pprint\n",
    "from spotpython.plot.xai import (get_activations, get_gradients, get_weights, plot_nn_values_hist, plot_nn_values_scatter, visualize_weights, visualize_gradients, visualize_activations, visualize_activations_distributions, visualize_gradient_distributions, visualize_weights_distributions)\n",
    "\n",
    "def test_plot_nn_values_scatter_reshaped_values():\n",
    "    # Mock data for testing\n",
    "    nn_values = {\n",
    "        'layer0': np.random.rand(10),  # 10 values suggesting padding for a 4x4\n",
    "        'layer1': np.random.rand(64),  # 64 values suggesting a perfect square (8x8)\n",
    "        'layer2': np.random.rand(32),  # 32 values suggesting  padding for a 6x6\n",
    "        'layer3': np.random.rand(16),  # 16 values suggesting a perfect square (4x4)\n",
    "    }\n",
    "\n",
    "    # Use the modified function that returns reshaped_values for testing\n",
    "    reshaped_values = plot_nn_values_scatter(nn_values, 'Test NN', return_reshaped=True)    \n",
    "\n",
    "    pprint.pprint(nn_values)\n",
    "    pprint.pprint(reshaped_values)\n",
    "    \n",
    "\n",
    "    # Assert for layer0: Checks if reshaping is correct for perfect square\n",
    "    assert reshaped_values['layer0'].shape == (4, 4)\n",
    "    # Assert for layer1: Checks if reshaping is correct for non-square\n",
    "    assert reshaped_values['layer1'].shape == (8, 8)\n",
    "    assert reshaped_values['layer2'].shape == (6, 6)\n",
    "    assert reshaped_values['layer3'].shape == (4, 4)\n",
    "\n",
    "\n",
    "\n",
    "test_plot_nn_values_scatter_reshaped_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.convert import set_dataset_target_type\n",
    "import pandas as pd\n",
    "dataset = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9], \"y\": [True, False, True]})\n",
    "print(dataset)\n",
    "dataset = set_dataset_target_type(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import river.tree\n",
    "core_model_name = \"tree.HoeffdingTreeRegressor\"\n",
    "core_model_module = core_model_name.split(\".\")[0]\n",
    "coremodel = core_model_name.split(\".\")[1]\n",
    "core_model_instance = getattr(getattr(river, core_model_module), coremodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.friedman import FriedmanDriftDataset\n",
    "import matplotlib.pyplot as plt\n",
    "data_generator = FriedmanDriftDataset(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\n",
    "data = [data for data in data_generator]\n",
    "indices = [i for _, _, i in data]\n",
    "values = {f\"x{i}\": [] for i in range(5)}\n",
    "values[\"y\"] = []\n",
    "for x, y, _ in data:\n",
    "    for i in range(5):\n",
    "        values[f\"x{i}\"].append(x[i])\n",
    "    values[\"y\"].append(y)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for label, series in values.items():\n",
    "    plt.plot(indices, series, label=label)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('')\n",
    "plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n",
    "plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scaler for Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from spotpython.utils.scaler import TorchStandardScaler\n",
    "import torch\n",
    "\n",
    "scaler=TorchStandardScaler()\n",
    "\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.float64)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "import numpy as np\n",
    "np.max(diabetes.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "dataset = Diabetes()\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 1\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "dataset = CaliforniaHousing()\n",
    "print(dataset.get_names())\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "dataset = CaliforniaHousing()\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "import torch\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "import torch\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=2, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n",
    "# print the first batch of the training set from data_module.data_train\n",
    "print(next(iter(data_module.train_dataloader())))\n",
    "# print the first batch of the training set from data_module.data_train as a numpy array\n",
    "print(next(iter(data_module.train_dataloader()))[0].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.utils.scaler import TorchStandardScaler, TorchMinMaxScaler\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "\n",
    "\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "scaler = TorchMinMaxScaler()\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "\n",
    "loader = data_module.train_dataloader\n",
    "\n",
    "total_sum = None\n",
    "total_count = 0\n",
    "\n",
    "# Iterate over batches in the DataLoader\n",
    "for batch in loader():\n",
    "    inputs, targets = batch\n",
    "    \n",
    "\n",
    "total_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "def test_net_light_regression_class():\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "    dataset = Diabetes()\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    net_light_regression = NetLightRegression(\n",
    "        l1=128,\n",
    "        epochs=10,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        initialization=\"Default\",\n",
    "        act_fn=nn.ReLU(),\n",
    "        optimizer=\"Adam\",\n",
    "        dropout_prob=0.1,\n",
    "        lr_mult=0.1,\n",
    "        patience=5,\n",
    "        _L_in=10,\n",
    "        _L_out=1,\n",
    "        _torchmetric=\"mean_squared_error\",\n",
    "    )\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=2,\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer.fit(net_light_regression, train_loader, val_loader)\n",
    "    res = trainer.test(net_light_regression, test_loader)\n",
    "    # test if the entry 'hp_metric' is in the res dict\n",
    "    assert \"hp_metric\" in res[0].keys()\n",
    "\n",
    "test_net_light_regression_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_int_hyperparameter_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_int_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"forest.AMFRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_int_hyperparameter_values(fun_control, \"n_estimators\", 2, 5)\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_factor_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_factor_hyperparameter_values(fun_control, \"leaf_model\", ['LinearRegression',\n",
    "                                                     'Perceptron'])\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "\n",
    "set_factor_hyperparameter_values(fun_control, \"leaf_model\", [\"LinearRegression\",\n",
    "                                                                \"Perceptron\"])\n",
    "\n",
    "# Access updated hyperparameters\n",
    "updated_hyperparameters = fun_control[\"core_model_hyper_dict\"]\n",
    "print(updated_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_boolean_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"forest.AMFRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_boolean_hyperparameter_values(fun_control, \"use_aggregation\", 0, 0)\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, full_dataset, train_size=0.8, batch_size=32, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.dataset = full_dataset\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Split the dataset\n",
    "        train_len = int(len(self.dataset) * self.train_size)\n",
    "        val_len = len(self.dataset) - train_len\n",
    "        self.train_set, self.val_set = random_split(self.dataset, [train_len, val_len])\n",
    "        \n",
    "        # Fit scaler on training data\n",
    "        train_data = torch.stack([item[0] for item in self.train_set])\n",
    "        print(f\"train_data before scaling\\n: {train_data}\")  \n",
    "        self.scaler.fit(train_data)\n",
    "       \n",
    "        # Transform training data\n",
    "        scaled_train_data = self.scaler.transform(train_data)\n",
    "        self.train_set = self._update_dataset(self.train_set, scaled_train_data)\n",
    "        print(f\"train_data after scaling\\n: {self.train_set}\")  \n",
    "        \n",
    "        # Transform validation data\n",
    "        val_data = torch.stack([item[0] for item in self.val_set])\n",
    "        scaled_val_data = self.scaler.transform(val_data)\n",
    "        self.val_set = self._update_dataset(self.val_set, scaled_val_data)\n",
    "\n",
    "    def _update_dataset(self, original_dataset, scaled_data):\n",
    "        updated_dataset = []\n",
    "        for i, (data, label) in enumerate(original_dataset):\n",
    "            updated_dataset.append((torch.tensor(scaled_data[i]), label))\n",
    "        return updated_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_data = torch.stack([item[0] for item in self.test_set])\n",
    "        scaled_test_data = self.scaler.transform(test_data)\n",
    "        self.test_set = self._update_dataset(self.test_set, scaled_test_data)\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Here you can download datasets if needed\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a 3-dimensional tensor with 1000 samples\n",
    "n = 10\n",
    "data = torch.rand((n, 3))\n",
    "print(f\"data: {data}\")\n",
    "labels = torch.tensor([i % 2 for i in range(n)], dtype=torch.float32)\n",
    "print(f\"labels: {labels}\")\n",
    "full_dataset = MyDataset(data, labels)\n",
    "\n",
    "# Creating DataModule instance\n",
    "data_module = MyDataModule(full_dataset)\n",
    "\n",
    "# Setup the data module\n",
    "data_module.setup()\n",
    "\n",
    "# Example of fetching a single batch\n",
    "train_loader = data_module.train_dataloader()\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch data shape: {batch[0].shape}\")\n",
    "    x, y = batch\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, do not delete the following imports, they are needed for the function add_core_model_to_fun_control\n",
    "import river\n",
    "from river import forest, tree, linear_model, rules\n",
    "from river import preprocessing\n",
    "import sklearn.metrics\n",
    "import spotpython\n",
    "from spotpython.light import regression\n",
    "\n",
    "def get_core_model_from_name(core_model_name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns the river core model name and instance from a core model name.\n",
    "\n",
    "    Args:\n",
    "        core_model_name (str): The full name of the core model in the format 'module.Model'.\n",
    "\n",
    "    Returns:\n",
    "        (str, object): A tuple containing the core model name and an instance of the core model.\n",
    "    \"\"\"\n",
    "    # Split the model name into its components\n",
    "    name_parts = core_model_name.split(\".\")\n",
    "    \n",
    "    if len(name_parts) < 2:\n",
    "        raise ValueError(f\"Invalid core model name: {core_model_name}. Expected format: 'module.ModelName'.\")\n",
    "\n",
    "    module_name = name_parts[0]\n",
    "    model_name = name_parts[1]\n",
    "    \n",
    "    try:\n",
    "        # Try to get the model from the river library\n",
    "        core_model_instance = getattr(getattr(river, module_name), model_name)\n",
    "        return model_name, core_model_instance\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            # Try to get the model from the spotpython library\n",
    "            submodule_name = name_parts[1]\n",
    "            model_name = name_parts[2] if len(name_parts) == 3 else model_name\n",
    "            print(f\"module_name: {module_name}\")\n",
    "            print(f\"submodule_name: {submodule_name}\")\n",
    "            print(f\"model_name: {model_name}\")\n",
    "            core_model_instance = getattr(getattr(getattr(spotpython, module_name), submodule_name), model_name)\n",
    "            return model_name, core_model_instance\n",
    "        except AttributeError:\n",
    "            raise ValueError(f\"Model '{core_model_name}' not found in either 'river' or 'spotpython' libraries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of usage\n",
    "model_name, model_instance = get_core_model_from_name('tree.HoeffdingTreeRegressor')\n",
    "print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, model_instance = get_core_model_from_name(\"light.regression.NNLinearRegressor\")\n",
    "print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 8\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=10,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes(return_X_y=False, as_frame=True)\n",
    "# svaing the data to a csv file\n",
    "data.frame.to_csv('~/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "n_features = 2\n",
    "n_samples = 500\n",
    "target_column = \"y\"\n",
    "ds =  make_moons(n_samples, noise=0.5, random_state=0)\n",
    "X, y = ds\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1))))\n",
    "test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1))))\n",
    "train.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "test.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "train.head()\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.concat([train, test])\n",
    "data.to_csv('moon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_classification(n_samples=1000, n_features=20,  n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('binary_classification.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_classification(n_samples=1000, n_features=20,  n_informative=9, n_redundant=2, n_repeated=0, n_classes=10, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('multiple_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('regression.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "data = load_iris(as_frame=True)\n",
    "data.frame.to_csv('iris.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotGUI.tuner.spotRun import get_report_file_name\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(PREFIX=\"test\")\n",
    "get_report_file_name(fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotGUI.tuner.spotRun import get_scenario_dict\n",
    "import pprint\n",
    "dic = get_scenario_dict(\"sklearn\")\n",
    "pprint.pprint(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.utils.io import load_hcf_dataframe, hcf_df2tensor\n",
    "from pyhcf.utils.names import load_all_features_N_regression_list\n",
    "from torch.utils.data import DataLoader\n",
    "df = load_hcf_dataframe(A=True,\n",
    "    H=True,\n",
    "    param_list=load_all_features_N_regression_list(),\n",
    "    target='N',\n",
    "    rmNA=True,\n",
    "    rmMF=True,\n",
    "    rmV=4,\n",
    "    min_freq=1000,\n",
    "    incl_drossel=False)\n",
    "dataset = hcf_df2tensor(df, target='N', return_X_y=False)\n",
    "print(type(dataset))\n",
    "print(len(dataset))\n",
    "# save the 'TensorDataset' object to a pkl file\n",
    "# import pickle\n",
    "# with open('hcf_dataset.pkl', 'wb') as f:\n",
    "#     pickle.dump(dataset, f)\n",
    "# load the 'TensorDataset' object from the pkl file\n",
    "# with open('hcf_dataset.pkl', 'rb') as f:\n",
    "#     dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0)\n",
    "# get the dimensions of the first sample\n",
    "dataset.__getitem__(0)[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "X0, y0 = S.generate_random_point()\n",
    "print(f\"X0: {X0}\")\n",
    "print(f\"y0: {y0}\")\n",
    "assert X0.size == 2\n",
    "assert y0.size == 1\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "assert y0 >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import get_spot_tensorboard_path\n",
    "get_spot_tensorboard_path(\"00_ubuntu_2021-08-31_14-30-00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import get_experiment_name\n",
    "get_experiment_name(prefix=\"00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 5\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1, -1]),\n",
    "    upper = np.array([1, 1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "S.plot_important_hyperparameter_contour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import design_control_init\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "design_control = design_control_init(init_size=3)\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "S = spot.Spot(fun = analytical().fun_sphere,\n",
    "              fun_control = fun_control,\n",
    "              design_control = design_control)\n",
    "X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n",
    "assert X.shape[0] == 3\n",
    "assert X.shape[1] == 2\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import inf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from spotriver.utils.data_conversion import convert_to_df\n",
    "from river.datasets import synth\n",
    "import warnings\n",
    "if not os.path.exists('./figures'):\n",
    "    os.makedirs('./figures')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "PREFIX=\"TEST_SAVE\"\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from river.datasets import synth\n",
    "from spotriver.utils.data_conversion import convert_to_df\n",
    "from math import inf\n",
    "import numpy as np\n",
    "from spotriver.fun.hyperriver import HyperRiver\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "\n",
    "\n",
    "target_column = \"y\"\n",
    "metric = mean_absolute_error\n",
    "horizon = 7*24\n",
    "n_train = horizon\n",
    "p_1 = int(n_train/4)\n",
    "p_2 = int(n_train/2)\n",
    "position=(p_1, p_2)\n",
    "dataset_train = synth.FriedmanDrift(\n",
    "   drift_type='gra',\n",
    "   position=position,\n",
    "   seed=123\n",
    ")\n",
    "\n",
    "train = convert_to_df(dataset_train, n_total=n_train)\n",
    "train.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n",
    "\n",
    "\n",
    "n_val = 10_000\n",
    "p_1 = int(n_val/4)\n",
    "p_2 = int(n_val/2)\n",
    "position=(p_1, p_2)\n",
    "dataset_val = synth.FriedmanDrift(\n",
    "   drift_type='gra',\n",
    "   position=position,\n",
    "   seed=124\n",
    ")\n",
    "val = convert_to_df(dataset_val, n_total=n_val)\n",
    "val.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n",
    "\n",
    "from math import inf\n",
    "import numpy as np\n",
    "from spotriver.fun.hyperriver import HyperRiver\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "\n",
    "fun = HyperRiver().fun_oml_horizon\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    TENSORBOARD_CLEAN=False,\n",
    "    tensorboard_start=False,\n",
    "    tensorboard_stop=False,\n",
    "    fun_evals=inf,\n",
    "    max_time=0.1,\n",
    "\n",
    "    prep_model_name=\"StandardScaler\",\n",
    "    test=val, # tuner uses the validation set as test set\n",
    "    train=train,\n",
    "    target_column=target_column,\n",
    "\n",
    "    metric_sklearn_name=\"mean_absolute_error\",\n",
    "    horizon=7*24,\n",
    "    oml_grace_period=7*24,\n",
    "    weight_coeff=0.0,\n",
    "    weights=np.array([100, 0.1, 0.1]),\n",
    "\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    "   )\n",
    "\n",
    "\n",
    "design_control = design_control_init(\n",
    "    init_size=3,\n",
    ")\n",
    "\n",
    "surrogate_control = surrogate_control_init(\n",
    "    noise=True,\n",
    "    n_theta=2,\n",
    "    min_Lambda=0.001,\n",
    "    max_Lambda=100,\n",
    ")\n",
    "\n",
    "optimizer_control = optimizer_control_init()\n",
    "\n",
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control,\n",
    "    surrogate_control=surrogate_control,\n",
    "    optimizer_control=optimizer_control,\n",
    ")\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.file import load_and_run_spot_python_experiment\n",
    "spot_tuner = load_and_run_spot_python_experiment(\"spot_000_experiment.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_div2_list(n, n_min) -> list:\n",
    "    \"\"\"\n",
    "    Generate a list of numbers from n to n_min (inclusive) by dividing n by 2\n",
    "    until the result is less than n_min.\n",
    "    This function starts with n and keeps dividing it by 2 until n_min is reached.\n",
    "    The number of times each value is added to the list is determined by n // current.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number to start with.\n",
    "        n_min (int): The minimum number to stop at.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of numbers from n to n_min (inclusive).\n",
    "\n",
    "    Examples:\n",
    "        _generate_div2_list(10, 1)\n",
    "        [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        _generate_div2_list(10, 2)\n",
    "        [10, 5, 5, 2, 2, 2, 2, 2]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    current = n\n",
    "    repeats = 1\n",
    "    max_repeats = 4\n",
    "    while current >= n_min:\n",
    "        result.extend([current] * min(repeats, max_repeats))\n",
    "        current = current // 2\n",
    "        repeats = repeats + 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_generate_div2_list(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_generate_div2_list(128, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_sizes(_L_in = 80, l1=2**9):\n",
    "    n_low = _L_in // 4\n",
    "    n_high = max(l1, 2 * n_low)\n",
    "    hidden_sizes = _generate_div2_list(n_high, n_low)\n",
    "    return hidden_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_hidden_sizes(l1=2**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.data.daten_lightv2 import DatenLightV2\n",
    "from pyhcf.utils.io import hcf_df2tensor\n",
    "df = DatenLightV2().load()\n",
    "print(f\"Datensatz der Größe {df.shape} erfolgreich geladen.\")\n",
    "print(df.columns.to_list())\n",
    "dataset = hcf_df2tensor(df, target='N', return_X_y=False)\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Targets Shape: {targets.shape}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Network Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we generate an 80-dim dataframe with 10000 samples, where the first two columns are random integers and the rest are random floats.\n",
    "* Then, we generate a target variable as the sum of the squared values.\n",
    "* The dataframe is converted to a tensor and split into a training, validation, and testing set. The corresponding data loaders are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "np.random.seed(42)\n",
    "n_samples = 10_000\n",
    "n_int = 2\n",
    "n_float = 76\n",
    "input_dim = n_int + n_float\n",
    "output_dim = 1\n",
    "data = np.random.rand(n_samples, n_float)\n",
    "data = np.hstack((np.random.randint(0, 10, (n_samples, n_int)), data))\n",
    "df = pd.DataFrame(data)\n",
    "df['y'] = np.sum(df.iloc[:, 2:]**2, axis=1)\n",
    "df.head()\n",
    "X = torch.tensor(df.iloc[:, :-1].values, dtype=torch.float32)\n",
    "y = torch.tensor(df.iloc[:, -1].values, dtype=torch.float32)\n",
    "dataset = TensorDataset(X, y)\n",
    "print(f\"Dataset with input tensor shape: {dataset.tensors[0].shape}\")\n",
    "print(f\"Dataset with target tensor shape: {dataset.tensors[1].shape}\")\n",
    "# print(dataset[0][0])\n",
    "# print(dataset[0][1])\n",
    "train_size_0 = int(0.8 * len(dataset))\n",
    "train_size = int(0.8 * train_size_0)\n",
    "val_size = train_size_0 - train_size\n",
    "test_size = len(dataset) - train_size_0\n",
    "train_dataset_0, test_dataset = random_split(dataset, [train_size_0, test_size])\n",
    "train_dataset, val_dataset = random_split(train_dataset_0, [train_size, val_size])\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NNLinearRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                    batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=input_dim,\n",
    "                                    _L_out=output_dim,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from spotpython.hyperparameters.optimizer import optimizer_handler\n",
    "import torchmetrics.functional.regression\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "\n",
    "class SettingsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset to handle settings-based data.\"\"\"\n",
    "    def __init__(self, dataframe, settings_columns, target_column):\n",
    "        self.dataframe = dataframe\n",
    "        self.settings_columns = settings_columns\n",
    "        self.target_column = target_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        settings = tuple(self.dataframe.iloc[idx][self.settings_columns])\n",
    "        features = self.dataframe.iloc[idx].drop(self.settings_columns + [self.target_column]).values\n",
    "        target = self.dataframe.iloc[idx][self.target_column]\n",
    "        return settings, torch.tensor(features, dtype=torch.float32), torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "class FilteredNNLinearRegressor:\n",
    "    def __init__(self, settings_columns, data, target_column='target', **nn_kwargs):\n",
    "        self.settings_columns = settings_columns\n",
    "        self.models = {}\n",
    "        self.nn_kwargs = nn_kwargs\n",
    "        self.data = data\n",
    "        self.target_column = target_column\n",
    "        self.prepare_networks()\n",
    "\n",
    "    def prepare_networks(self):\n",
    "        settings_combinations = self.data[self.settings_columns].drop_duplicates().to_records(index=False)\n",
    "        i = 0\n",
    "        for combination in settings_combinations:\n",
    "            print(f\"Combination {i}: {combination}\")\n",
    "            i += 1\n",
    "            self.models[combination] = NNLinearRegressor(**self.nn_kwargs)\n",
    "\n",
    "    def feature_filter(self, settings):\n",
    "        \"\"\"Filter the data based on given settings tuple.\"\"\"\n",
    "        df_filtered = self.data[(self.data[self.settings_columns] == pd.Series(settings)).all(axis=1)]\n",
    "        print(f\"df_filtered: {df_filtered}\")\n",
    "        return df_filtered\n",
    "\n",
    "    def train(self, trainer_kwargs):\n",
    "        # Split data and train each model separately\n",
    "        for settings, model in self.models.items():\n",
    "            filtered_data = self.feature_filter(settings)\n",
    "            dataset = SettingsDataset(filtered_data, self.settings_columns, self.target_column)\n",
    "            train_loader = DataLoader(dataset, batch_size=self.nn_kwargs['batch_size'])\n",
    "            trainer = L.Trainer(**trainer_kwargs)\n",
    "            trainer.fit(model, train_loader)\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        predictions = {}\n",
    "        for settings, model in self.models.items():\n",
    "            filtered_data = self.feature_filter(settings)\n",
    "            if not filtered_data.empty:\n",
    "                dataset = SettingsDataset(filtered_data, self.settings_columns, self.target_column)\n",
    "                test_loader = DataLoader(dataset, batch_size=self.nn_kwargs['batch_size'])\n",
    "                trainer = L.Trainer()\n",
    "                preds = trainer.predict(model, test_loader)\n",
    "                predictions[settings] = preds\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'setting1': [-1, -1, 1, 1],\n",
    "    'setting2': ['A', 'B', 'A', 'B'],\n",
    "    'feature1': [0.1, 0.2, 0.3, 0.4],\n",
    "    'feature2': [0.5, 0.6, 0.7, 0.8],\n",
    "    'target': [1, 2, 3, 4]\n",
    "})\n",
    "\n",
    "settings_columns = ['setting1', 'setting2']\n",
    "target_column = 'target'\n",
    "nn_kwargs = {\n",
    "    'l1': 16,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 2,\n",
    "    'initialization': 'xavier',\n",
    "    'act_fn': nn.ReLU(),\n",
    "    'optimizer': 'Adam',\n",
    "    'dropout_prob': 0.1,\n",
    "    'lr_mult': 0.1,\n",
    "    'patience': 2,\n",
    "    'batch_norm': True,\n",
    "    '_L_in': 2,  # For this example, 2 features besides settings\n",
    "    '_L_out': 1,\n",
    "    '_torchmetric': \"mean_squared_error\",\n",
    "}\n",
    "\n",
    "multi_network = FilteredNNLinearRegressor(settings_columns, data, target_column, **nn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = {'max_epochs': 2,  'enable_progress_bar': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_network.train(trainer_kwargs)\n",
    "predictions = multi_network.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explanation:\n",
    "1. SettingsDataset: A custom dataset class that includes settings as part of the data. Each row has a tuple of settings, feature values, and the target value.\n",
    "\n",
    "2. FilteredNNLinearRegressor: An umbrella class that handles setting combinations and assigns each its own `NNLinearRegressor` model instance. It trains these models using only relevant data filtered by the `feature_filter()` function.\n",
    "\n",
    "3. Feature Filtering: The `feature_filter()` function uses Pandas to filter rows based on their relevant setting information before creating dataset and loader instances for each unique settings combination.\n",
    "\n",
    "4. Training and Prediction: We generate and train separate models for each settings combination and then predict using test data filtered similarly using the defined `feature_filter()`.\n",
    "\n",
    "This approach provides modularity as each model is logically separated based on settings, while utilizing your existing class structure to individually specify training processes and handle data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.plot.xai import get_gradients\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "gradients = get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "# assert that the gradients are a dictionary with keys that contain the string 'layers' and values that are arrays\n",
    "assert all([key in gradients.keys() for key in gradients.keys()])\n",
    "assert all([isinstance(value, np.ndarray) for value in gradients.values()])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.plot.xai import get_gradients\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CondNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConditionalLayer(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, output_dim):\n",
    "        super(ConditionalLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.condition_fc = nn.Linear(condition_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # Basic linear transformation\n",
    "        base_output = self.fc(x)\n",
    "        # Compute a condition-dependent transformation\n",
    "        condition_output = self.condition_fc(condition)\n",
    "        # Modulate the output by adding the condition-dependent transformation\n",
    "        output = base_output + condition_output\n",
    "        return F.relu(output)\n",
    "\n",
    "class ConditionalNet(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, hidden_dim, output_dim):\n",
    "        super(ConditionalNet, self).__init__()\n",
    "        self.cond_layer1 = ConditionalLayer(input_dim, condition_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        x = self.cond_layer1(x, condition)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_dim = 10\n",
    "condition_dim = 2  # For instance, if you have two conditional features like region and season\n",
    "hidden_dim = 20\n",
    "output_dim = 1\n",
    "\n",
    "model = ConditionalNet(input_dim, condition_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Example data\n",
    "x = torch.randn(5, input_dim)\n",
    "condition = torch.randn(5, condition_dim)\n",
    "\n",
    "output = model(x, condition)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.light.regression import NNCondNetRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 64\n",
    "# generate data\n",
    "num_samples = 1_000\n",
    "input_dim = 10\n",
    "cond_dim = 2\n",
    "X = torch.randn(num_samples, input_dim)  # random data for example\n",
    "Y = torch.randn(num_samples, 1)  # random target for example\n",
    "data_set = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNCondNetRegressor(l1=128,\n",
    "                                batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_cond=cond_dim,\n",
    "                                    _L_in=input_dim - cond_dim,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# validation and test should give the same result, because the data is the same\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CondNet Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.file import get_experiment_filename\n",
    "from math import inf\n",
    "from spotpython.hyperparameters.values import set_hyperparameter\n",
    "\n",
    "PREFIX=\"4000\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "input_dim = 10\n",
    "output_dim = 1\n",
    "cond_dim = 2\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    fun_evals=inf,\n",
    "    max_time=1,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNCondNetRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=input_dim - cond_dim,\n",
    "    _L_out=1,\n",
    "    _L_cond=cond_dim,)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "\n",
    "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
    "set_hyperparameter(fun_control, \"l1\", [3,4])\n",
    "set_hyperparameter(fun_control, \"epochs\", [3,7])\n",
    "set_hyperparameter(fun_control, \"batch_size\", [4,5])\n",
    "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
    "set_hyperparameter(fun_control, \"patience\", [2,3])\n",
    "set_hyperparameter(fun_control, \"lr_mult\", [0.1, 20.0])\n",
    "\n",
    "design_control = design_control_init(init_size=10)\n",
    "\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_weights(net, return_index=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the weights of a neural network and the size of each layer.\n",
    "\n",
    "    Args:\n",
    "        net (object):\n",
    "            A neural network.\n",
    "        return_index (bool, optional):\n",
    "            Whether to return the index. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            A tuple containing:\n",
    "            - weights: A dictionary with the weights of the neural network.\n",
    "            - index: The layer index list (only if return_index is True).\n",
    "            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "\n",
    "    Examples:\n",
    "        # Example usage (as described in the original function's docstring)\n",
    "    \"\"\"\n",
    "    weights = {}\n",
    "    index = []\n",
    "    layer_sizes = {}\n",
    "    \n",
    "    for name, param in net.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            continue\n",
    "        \n",
    "        # Extract layer number\n",
    "        layer_number = int(name.split(\".\")[1])\n",
    "        index.append(layer_number)\n",
    "        \n",
    "        # Create dictionary key for this layer\n",
    "        key_name = f\"Layer {layer_number}\"\n",
    "        \n",
    "        # Store weight information\n",
    "        weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
    "        \n",
    "        # Store layer size as a NumPy array\n",
    "        layer_sizes[key_name] = np.array(param.size())\n",
    "    \n",
    "    if return_index:\n",
    "        return weights, index, layer_sizes\n",
    "    else:\n",
    "        return weights, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.plot.xai import get_weights\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "weights, layer_sizes = get_weights(net=model)\n",
    "weights, layer_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import math\n",
    "\n",
    "def plot_nn_values_scatter(\n",
    "    nn_values,\n",
    "    layer_sizes,\n",
    "    nn_values_names=\"\",\n",
    "    absolute=True,\n",
    "    cmap=\"gray\",\n",
    "    figsize=(6, 6),\n",
    "    return_reshaped=False,\n",
    "    show=True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Plot the values of a neural network including a marker for padding values.\n",
    "    For simplicity, this example will annotate 'P' directly on the plot for padding values\n",
    "    using a unique marker value approach.\n",
    "\n",
    "    Args:\n",
    "        nn_values (dict):\n",
    "            A dictionary with the values of the neural network. For example,\n",
    "            the weights, gradients, or activations.\n",
    "        layer_sizes (dict):\n",
    "            A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "        nn_values_names (str, optional):\n",
    "            The name of the values. Defaults to \"\".\n",
    "        absolute (bool, optional):\n",
    "            Whether to use the absolute values. Defaults to True.\n",
    "        cmap (str, optional):\n",
    "            The colormap to use. Defaults to \"gray\".\n",
    "        figsize (tuple, optional):\n",
    "            The figure size. Defaults to (6, 6).\n",
    "        return_reshaped (bool, optional):\n",
    "            Whether to return the reshaped values. Defaults to False.\n",
    "        show (bool, optional):\n",
    "            Whether to show the plot. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the reshaped values.\n",
    "    \"\"\"\n",
    "    if cmap == \"gray\":\n",
    "        cmap = \"gray\"\n",
    "    elif cmap == \"BlueWhiteRed\":\n",
    "        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"blue\", \"white\", \"red\"])\n",
    "    elif cmap == \"GreenYellowRed\":\n",
    "        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"green\", \"yellow\", \"red\"])\n",
    "    else:\n",
    "        cmap = \"viridis\"\n",
    "\n",
    "    res = {}\n",
    "    padding_marker = np.nan  # Use NaN as a special marker for padding\n",
    "    for layer, values in nn_values.items():\n",
    "        if layer not in layer_sizes:\n",
    "            print(f\"Layer {layer} size not defined, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        layer_shape = layer_sizes[layer]\n",
    "        height, width = layer_shape if len(layer_shape) == 2 else (layer_shape[0], 1)  # Support linear layers\n",
    "        \n",
    "        print(f\"{len(values)} values in Layer {layer}. Geometry: ({height}, {width})\")\n",
    "        \n",
    "        total_size = height * width\n",
    "        if len(values) < total_size:\n",
    "            padding_needed = total_size - len(values)\n",
    "            print(f\"{padding_needed} padding values added to Layer {layer}.\")\n",
    "            values = np.append(values, [padding_marker] * padding_needed)  # Append padding values\n",
    "\n",
    "        if absolute:\n",
    "            reshaped_values = np.abs(values).reshape((height, width))\n",
    "            # Mark padding values distinctly by setting them back to NaN\n",
    "            reshaped_values[reshaped_values == np.abs(padding_marker)] = np.nan\n",
    "        else:\n",
    "            reshaped_values = values.reshape((height, width))\n",
    "\n",
    "        _, ax = plt.subplots(figsize=figsize)\n",
    "        cax = ax.imshow(reshaped_values, cmap=cmap, interpolation=\"nearest\")\n",
    "\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                if np.isnan(reshaped_values[i, j]):\n",
    "                    ax.text(j, i, \"P\", ha=\"center\", va=\"center\", color=\"red\")\n",
    "        \n",
    "        plt.colorbar(cax, label=\"Value\")\n",
    "        plt.title(f\"{nn_values_names} Plot for {layer}\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "        # Add reshaped_values to the dictionary res\n",
    "        res[layer] = reshaped_values\n",
    "    if return_reshaped:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, layer_sizes = get_weights(net=model)\n",
    "plot_nn_values_scatter(nn_values=weights, layer_sizes=layer_sizes, nn_values_names=\"Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "\n",
    "def get_gradients(net, fun_control, batch_size, device=\"cpu\", normalize=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the gradients of a neural network and the size of each layer.\n",
    "\n",
    "    Args:\n",
    "        net (object):\n",
    "            A neural network.\n",
    "        fun_control (dict):\n",
    "            A dictionary with the function control.\n",
    "        batch_size (int, optional):\n",
    "            The batch size.\n",
    "        device (str, optional):\n",
    "            The device to use. Defaults to \"cpu\".\n",
    "        normalize (bool, optional):\n",
    "            Whether to normalize the input data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - grads: A dictionary with the gradients of the neural network.\n",
    "            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "\n",
    "    Examples:\n",
    "        # Example usage to compute gradients\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    dataset = fun_control[\"data_set\"]\n",
    "    data_module = LightDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        test_size=fun_control[\"test_size\"],\n",
    "        scaler=fun_control[\"scaler\"],\n",
    "        verbosity=10,\n",
    "    )\n",
    "    data_module.setup(stage=\"fit\")\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    inputs, targets = next(iter(train_loader))\n",
    "    if normalize:\n",
    "        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    net.zero_grad()\n",
    "    preds = net(inputs)\n",
    "    preds = preds.squeeze(-1)  # Remove the last dimension if it's 1\n",
    "    loss = F.mse_loss(preds, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = {}\n",
    "    layer_sizes = {}\n",
    "    for name, params in net.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            # Collect gradient information\n",
    "            grads[name] = params.grad.view(-1).cpu().clone().numpy()\n",
    "            # Collect size information\n",
    "            layer_sizes[name] = np.array(params.size())\n",
    "\n",
    "    net.zero_grad()\n",
    "    return grads, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients, layer_sizes = get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "plot_nn_values_scatter(nn_values=gradients, layer_sizes=layer_sizes, nn_values_names=\"Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(data, layer_index) -> bool:\n",
    "    \"\"\"Checks for NaN values in the tensor data.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The tensor to check for NaN values.\n",
    "        layer_index (int): The index of the layer for logging purposes.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if NaNs are found, False otherwise.\n",
    "    \"\"\"\n",
    "    if torch.isnan(data).any():\n",
    "        print(f\"NaN detected after layer {layer_index}\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(net, fun_control, batch_size, device=\"cpu\", normalize=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Computes the activations for each layer of the network, the mean activations,\n",
    "    and the sizes of the activations for each layer.\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The neural network model.\n",
    "        fun_control (dict): A dictionary containing the dataset.\n",
    "        batch_size (int): The batch size for the data loader.\n",
    "        device (str): The device to run the model on. Defaults to \"cpu\".\n",
    "        normalize (bool): Whether to normalize the input data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the activations, mean activations, and layer sizes for each layer.\n",
    "\n",
    "    Examples:\n",
    "        from spotpython.plot.xai import get_activations\n",
    "            activations, mean_activations, layer_sizes = get_activations(net, fun_control)\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    mean_activations = {}\n",
    "    layer_sizes = {}\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    dataset = fun_control[\"data_set\"]\n",
    "    data_module = LightDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        test_size=fun_control[\"test_size\"],\n",
    "        scaler=fun_control[\"scaler\"],\n",
    "        verbosity=10,\n",
    "    )\n",
    "    data_module.setup(stage=\"fit\")\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    inputs, _ = next(iter(train_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    if normalize:\n",
    "        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        # Loop through all layers\n",
    "        for layer_index, layer in enumerate(net.layers[:-1]):\n",
    "            inputs = layer(inputs)  # Forward pass through the layer\n",
    "\n",
    "            # Check for NaNs\n",
    "            if check_for_nans(inputs, layer_index):\n",
    "                break\n",
    "\n",
    "            # Collect activations for Linear layers\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                activations[layer_index] = inputs.view(-1).cpu().numpy()\n",
    "                mean_activations[layer_index] = inputs.mean(dim=0).cpu().numpy()\n",
    "                # Record the size of the activations\n",
    "                layer_sizes[layer_index] = np.array(inputs.size())\n",
    "\n",
    "    return activations, mean_activations, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations, mean_activations, layer_sizes = get_activations(net=model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "plot_nn_values_scatter(nn_values=activations, layer_sizes=layer_sizes, nn_values_names=\"Activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def extract_linear_dims(model):\n",
    "    dims = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Append input and output features of the Linear layer\n",
    "            dims.append(layer.in_features)\n",
    "            dims.append(layer.out_features)\n",
    "    return np.array(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_linear_dims(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "def viz_net(net,\n",
    "            device=\"cpu\",\n",
    "            show_attrs=False,\n",
    "            show_saved=False,\n",
    "            filename=\"model_architecture\",\n",
    "            format=\"png\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the architecture of a linear neural network.\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The neural network model.\n",
    "        device (str, optional): The device to use. Defaults to \"cpu\".\n",
    "        show_attrs (bool, optional): Whether to show the attributes. Defaults to False.\n",
    "        show_saved (bool, optional): Whether to show the saved. Defaults to False.\n",
    "        filename (str, optional): The filename. Defaults to \"model_architecture\".\n",
    "        format (str, optional): The format. Defaults to \"png\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the model does not have a linear layer.\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "       dim=extract_linear_dims(net)\n",
    "    except:\n",
    "        error_message = \"The model does not have a linear layer.\"\n",
    "        raise ValueError(error_message)\n",
    "    x = torch.randn(1, dim[0]).requires_grad_(True)\n",
    "    x = x.to(device)\n",
    "    output = net(x)\n",
    "    dot = make_dot(output, params=dict(net.named_parameters()), show_attrs=show_attrs , show_saved=show_saved)\n",
    "    dot.render(filename, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_net(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.plot.xai import viz_net\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "_L_in=10\n",
    "_L_out=1\n",
    "_torchmetric=\"mean_squared_error\"\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=_L_in,\n",
    "    _L_out=_L_out,\n",
    "    _torchmetric=_torchmetric,\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "viz_net(net=model, device=\"cpu\", show_attrs=True, show_saved=True, filename=\"model_architecture\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "S.exp_imp(1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "# assert S.exp_imp(0.0, 1.0) == 1/np.sqrt(2*np.pi)\n",
    "# which is approx. 0.3989422804014327\n",
    "S.exp_imp(0.0, 1.0)\n",
    "0.3989422804014327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_de_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract_from_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "\n",
    "# Define the number of theta and p parameters\n",
    "num_theta = 2\n",
    "num_p = 3\n",
    "\n",
    "# Initialize the Kriging model\n",
    "kriging_model = Kriging(\n",
    "    name='kriging',\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=False\n",
    ")\n",
    "\n",
    "# Extract parameters from given bounds\n",
    "# Assumes 'extract_from_bounds' will split the array into `theta` and `p` based on `n_theta`.\n",
    "bounds_array = np.array([1, 2, 3, 4, 5])\n",
    "kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "\n",
    "# Validate the expected values for theta and p\n",
    "# Convert theta and p to lists if they are numpy arrays\n",
    "theta_list = list(kriging_model.theta)\n",
    "p_list = list(kriging_model.p)\n",
    "\n",
    "assert theta_list == [1, 2], f\"Expected theta to be [1, 2] but got {theta_list}\"\n",
    "assert p_list == [3, 4, 5], f\"Expected p to be [3] but got {p_list}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "# Define the number of theta and p parameters\n",
    "num_theta = 2\n",
    "num_p = 3\n",
    "# Initialize the Kriging model\n",
    "kriging_model = Kriging(\n",
    "    name='kriging',\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=True\n",
    ")\n",
    "# Create bounds array\n",
    "bounds_array = np.array([1, 2, 3, 4, 5, 6])\n",
    "# Extract parameters from given bounds\n",
    "kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "# Assertions to check if parameters are correctly extracted\n",
    "assert np.array_equal(kriging_model.theta, [1, 2]), f\"Expected theta to be [1, 2] but got {kriging_model.theta}\"\n",
    "assert np.array_equal(kriging_model.p, [3, 4, 5]), f\"Expected p to be [3, 4, 5] but got {kriging_model.p}\"\n",
    "assert kriging_model.Lambda == 6, f\"Expected Lambda to be 6 but got {kriging_model.Lambda}\"\n",
    "print(\"All assertions passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "print(new_theta_p_Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.update_log()\n",
    "print(S.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 0], [1, 0]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.fit(nat_X, nat_y)\n",
    "print(S.Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "print(f\"S.nat_X: {S.nat_X}\")\n",
    "print(f\"S.nat_y: {S.nat_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "assert S.var_type == ['num', 'num']\n",
    "assert S.var_type == ['num', 'num']\n",
    "assert S.num_mask.all() == True\n",
    "assert S.factor_mask.all() == False\n",
    "assert S.int_mask.all() == False\n",
    "assert S.ordered_mask.all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "assert S.theta.all() == array([0., 0.]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import log, var\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "nat_y = np.array([1, 2, 3])\n",
    "n=3\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "# if var(self.nat_y) is > 0, then self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n",
    "# else self.pen_val = self.n * var(self.nat_y) + 1e4\n",
    "assert S.pen_val == nat_X.shape[0] * log(var(S.nat_y)) + 1e4\n",
    "assert S.Psi.shape == (n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")\n",
    "S.build_U()\n",
    "print(f\"S.U:{S.U}\")\n",
    "S.likelihood()\n",
    "S.negLnLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "from numpy import power\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "print(S.__is_any__(power(10.0, S.theta), 0))\n",
    "print(S.__is_any__(S.theta, 0))\n",
    "S.theta: [0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")\n",
    "S.build_U()\n",
    "print(f\"S.U:{S.U}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1], [2]])\n",
    "nat_y = np.array([5, 10])\n",
    "n=2\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.build_Psi()\n",
    "S.build_U()\n",
    "S.likelihood()\n",
    "# assert S.mu is close to 7.5 with a tolerance of 1e-6\n",
    "assert np.allclose(S.mu, 7.5, atol=1e-6)\n",
    "E = np.exp(1)\n",
    "sigma2 = E/(E**2 -1) * (25/4 + 25/4*E)\n",
    "# asssert S.SigmaSqr is close to sigma2 with a tolerance of 1e-6\n",
    "assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)\n",
    "print(f\"S.LnDetPsi:{S.LnDetPsi}\")\n",
    "print(f\"S.self.negLnLike:{S.negLnLike}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# 1-dimensional example\n",
    "fun = analytical().fun_sphere\n",
    "fun_control=fun_control_init(lower = np.array([-1]),\n",
    "                            upper = np.array([1]),\n",
    "                            noise=False)\n",
    "design_control=design_control_init(init_size=10)\n",
    "S = spot.Spot(fun=fun,\n",
    "              fun_control=fun_control,\n",
    "              design_control=design_control)\n",
    "S.initialize_design()\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.surrogate.plot()\n",
    "# 2-dimensional example\n",
    "fun = analytical().fun_sphere\n",
    "fun_control=fun_control_init(lower = np.array([-1, -1]),\n",
    "                            upper = np.array([1, 1]),\n",
    "                            noise=False)\n",
    "design_control=design_control_init(init_size=10)\n",
    "S = spot.Spot(fun=fun,\n",
    "              fun_control=fun_control,\n",
    "              design_control=design_control)\n",
    "S.initialize_design()\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.surrogate.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_0).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "print(f\"mean_prediction: {mean_prediction}\")\n",
    "print(f\"std_prediction: {std_prediction}\")\n",
    "print(f\"s_ei: {s_ei}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "    from numpy import linspace, arange, empty\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=10).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "n = X.shape[0]\n",
    "y = empty(n, dtype=float)\n",
    "s = empty(n, dtype=float)\n",
    "ei = empty(n, dtype=float)\n",
    "for i in range(n):\n",
    "    y_coded, s_coded, ei_coded = S.predict_coded(X[i, :])\n",
    "    y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n",
    "    s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n",
    "    ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n",
    "print(f\"y: {y}\")\n",
    "print(f\"s: {s}\")\n",
    "print(f\"ei: {-1.0*ei}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_psi_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "X_train = np.array([[1., 2.],\n",
    "                    [2., 4.],\n",
    "                    [3., 6.]])\n",
    "y_train = np.array([1., 2., 3.])\n",
    "S = Kriging(name='kriging',\n",
    "            seed=123,\n",
    "            log_level=50,\n",
    "            n_theta=1,\n",
    "            noise=False,\n",
    "            cod_type=\"norm\")\n",
    "S.fit(X_train, y_train)\n",
    "# force theta to simple values:\n",
    "S.theta = np.array([0.0])\n",
    "nat_X = np.array([1., 0.])\n",
    "S.psi = np.zeros((S.n, 1))\n",
    "S.build_psi_vec(nat_X)\n",
    "res = np.array([[np.exp(-4)],\n",
    "    [np.exp(-17)],\n",
    "    [np.exp(-40)]])\n",
    "assert np.array_equal(S.psi, res)\n",
    "print(f\"S.psi: {S.psi}\")\n",
    "print(f\"Control value res: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Log Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import inf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.file import get_experiment_filename\n",
    "\n",
    "PREFIX=\"00_TEST\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    save_experiment=True,\n",
    "    fun_evals=inf,\n",
    "    max_time=10,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    tensorboard_log=True,\n",
    "    show_config=True,)\n",
    "\n",
    "design_control = design_control_init(init_size=5)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNLinearRegressor Teest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "torch.manual_seed(0)\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 64\n",
    "# generate data\n",
    "num_samples = 1_000\n",
    "input_dim = 10\n",
    "X = torch.randn(num_samples, input_dim)  # random data for example\n",
    "Y = torch.randn(num_samples, 1)  # random target for example\n",
    "data_set = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=input_dim,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# validation and test should give the same result, because the data is the same\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Objective Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [-1, -1, -1]])\n",
    "fun = analytical()\n",
    "fun.fun_cubed(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([np.zeros(10), np.ones(10)])\n",
    "fun = analytical()\n",
    "fun.fun_wingwt(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.architecture import generate_div2_list\n",
    "# call the function with all integer values between 5 and 10\n",
    "for n in range(5, 21):\n",
    "    print(generate_div2_list(n, n_min=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.architecture import get_hidden_sizes\n",
    "_L_in = 10\n",
    "max_n = 10\n",
    "for l1 in range(5, 20):    \n",
    "    print(get_hidden_sizes(_L_in, l1, max_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def get_three_layers(_L_in, l1) -> list:\n",
    "    \"\"\"\n",
    "    Calculate three layers based on input values.\n",
    "\n",
    "    Args:\n",
    "        _L_in (float): The input value to be multiplied.\n",
    "        l1 (float): The multiplier for the layers.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing three calculated layers [a, b, c] where:\n",
    "            - a = 3 * l1 * _L_in\n",
    "            - b = 2 * l1 * _L_in\n",
    "            - c = l1 * _L_in\n",
    "\n",
    "    Examples:\n",
    "        from spotpython.hyperparameters.architecture import get_three_layers\n",
    "            _L_in = 10\n",
    "            l1 = 20\n",
    "            get_three_layers(_L_in, l1)\n",
    "            [600, 400, 200]\n",
    "    \"\"\"\n",
    "    a = 3 * l1 * _L_in\n",
    "    b = 2 * l1 * _L_in\n",
    "    c = ceil(l1/2) * _L_in\n",
    "    return [a, b, a, b, b, c, c] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_L_in = 20\n",
    "l1 = 4\n",
    "get_three_layers(_L_in, l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for 0.20.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_spot_attributes_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1]),\n",
    "    upper = np.array([1]),\n",
    "    fun_evals=n)\n",
    "design_control=design_control_init(init_size=ni)\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n",
    "df = spot_1.get_spot_attributes_as_df()\n",
    "df[\"Attribute Name\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_red_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "# number of points\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals = n)\n",
    "design_control=design_control_init(init_size=ni)\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n",
    "assert spot_1.lower.size == 2\n",
    "assert spot_1.upper.size == 2\n",
    "assert len(spot_1.var_type) == 2\n",
    "assert spot_1.red_dim == False\n",
    "spot_1.lower = np.array([-1, -1])\n",
    "spot_1.upper = np.array([-1, -1])\n",
    "spot_1.to_red_dim()\n",
    "assert spot_1.lower.size == 0\n",
    "assert spot_1.upper.size == 0\n",
    "assert len(spot_1.var_type) == 0\n",
    "assert spot_1.red_dim == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_all_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n",
    "lower = np.array([-1, -1, 0, 0])\n",
    "upper = np.array([1, -1, 0, 5])  # Second and third dimensions are fixed\n",
    "fun_evals = 10\n",
    "var_type = ['float', 'int', 'float', 'int']\n",
    "var_name = ['x1', 'x2', 'x3', 'x4']\n",
    "spot_instance = spot.Spot(\n",
    "    fun = Analytical().fun_sphere, \n",
    "    fun_control=fun_control_init(lower=lower, upper=upper, fun_evals=fun_evals)\n",
    ")\n",
    "X0 = np.array([[2.5, 3.5], [4.5, 5.5]])\n",
    "X_full_dim = spot_instance.to_all_dim(X0)\n",
    "print(X_full_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_new_X0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "            n_points=10,\n",
    "            ocba_delta=0,\n",
    "            lower = np.array([-1, -1]),\n",
    "            upper = np.array([1, 1])\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "             fun_control=fun_control,\n",
    "             design_control=design_control,\n",
    ")\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "X0 = S.get_new_X0()\n",
    "assert X0.shape[0] == S.n_points\n",
    "assert X0.shape[1] == S.lower.size\n",
    "# assert new points are in the interval [lower, upper]\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "# print using 20 digits precision\n",
    "np.set_printoptions(precision=20)\n",
    "print(f\"X0: {X0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython import Analytical\n",
    "from spotpython import Spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write_tensorboard_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    tensorboard_log=True,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    lower = np.array([-1]),\n",
    "    upper = np.array([1])\n",
    "    )\n",
    "fun = Analytical().fun_sphere\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "S.initialize_design()\n",
    "S.write_tensorboard_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize_design_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "fun = Analytical().fun_sphere\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "X_start = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "S.initialize_design_matrix(X_start)\n",
    "print(f\"Design matrix: {S.X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate_initial_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    lower=np.array([-1, -1]),\n",
    "    upper=np.array([1, 1])\n",
    ")\n",
    "fun = Analytical().fun_sphere\n",
    "S = Spot(fun=fun, fun_control=fun_control)\n",
    "X0 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "S.initialize_design_matrix(X_start=X0)\n",
    "S.evaluate_initial_design()\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Experiment:\n",
    "    def save_experiment(self, filename=None, path=None, overwrite=True) -> None:\n",
    "        \"\"\"\n",
    "        Save the experiment to a file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename of the experiment file.\n",
    "            path (str): The path to the experiment file.\n",
    "            overwrite (bool): If `True`, the file will be overwritten if it already exists. Default is `True`.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Ensure we don't accidentally try to pickle unpicklable components\n",
    "        self.close_and_del_spot_writer()\n",
    "        self.remove_logger_handlers()\n",
    "\n",
    "        # Create deep copies of control dictionaries\n",
    "        fun_control = copy.deepcopy(self.fun_control)\n",
    "        optimizer_control = copy.deepcopy(self.optimizer_control)\n",
    "        surrogate_control = copy.deepcopy(self.surrogate_control)\n",
    "        design_control = copy.deepcopy(self.design_control)\n",
    "\n",
    "        # Prepare an experiment dictionary excluding any explicitly unpickable components\n",
    "        experiment = {\n",
    "            \"design_control\": design_control,\n",
    "            \"fun_control\": fun_control,\n",
    "            \"optimizer_control\": optimizer_control,\n",
    "            \"spot_tuner\": self._get_pickle_safe_spot_tuner(),\n",
    "            \"surrogate_control\": surrogate_control,\n",
    "        }\n",
    "\n",
    "        # Determine the filename based on PREFIX if not provided\n",
    "        PREFIX = fun_control.get(\"PREFIX\", \"experiment\")\n",
    "        if filename is None:\n",
    "            filename = self.get_experiment_filename(PREFIX)\n",
    "\n",
    "        if path is not None:\n",
    "            filename = os.path.join(path, filename)\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if filename is not None and os.path.exists(filename) and not overwrite:\n",
    "            print(f\"Error: File {filename} already exists. Use overwrite=True to overwrite the file.\")\n",
    "            return\n",
    "\n",
    "        # Serialize the experiment dictionary to the pickle file\n",
    "        if filename is not None:\n",
    "            with open(filename, \"wb\") as handle:\n",
    "                try:\n",
    "                    pickle.dump(experiment, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during pickling: {e}\")\n",
    "                    raise e\n",
    "            print(f\"Experiment saved to {filename}\")\n",
    "\n",
    "    def remove_logger_handlers(self) -> None:\n",
    "        \"\"\"\n",
    "        Remove handlers from the logger to avoid pickling issues.\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        for handler in list(logger.handlers):  # Copy the list to avoid modification during iteration\n",
    "            logger.removeHandler(handler)\n",
    "\n",
    "    def close_and_del_spot_writer(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete the spot_writer attribute from the object\n",
    "        if it exists and close the writer.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n",
    "            self.spot_writer.flush()\n",
    "            self.spot_writer.close()\n",
    "            del self.spot_writer\n",
    "\n",
    "    def _get_pickle_safe_spot_tuner(self):\n",
    "        \"\"\"\n",
    "        Create a copy of self excluding unpickleable components for safe pickling.\n",
    "        This ensures no unpicklable components are passed to pickle.dump().\n",
    "        \"\"\"\n",
    "        # Make a deepcopy and manually remove unpickleable components\n",
    "        spot_tuner = copy.deepcopy(self)\n",
    "        for attr in ['spot_writer']:\n",
    "            if hasattr(spot_tuner, attr):\n",
    "                delattr(spot_tuner, attr)\n",
    "        return spot_tuner\n",
    "\n",
    "    def get_experiment_filename(self, prefix):\n",
    "        \"\"\"\n",
    "        Generate a filename based on a given prefix with additional unique identifiers or timestamps.\n",
    "        \"\"\"\n",
    "        # Implement the logic to generate a filename\n",
    "        return f\"{prefix}_experiment.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform_hyper_parameter_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.transform import transform_hyper_parameter_values\n",
    "fun_control = {\n",
    "    \"core_model_hyper_dict\": {\n",
    "        \"leaf_prediction\": {\n",
    "                \"type\": \"factor\",\n",
    "                \"transform\": \"None\",\n",
    "                \"default\": \"mean\",\n",
    "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
    "                \"core_model_parameter_type\": \"str\"\n",
    "                            },\n",
    "        \"max_depth\": {\n",
    "                \"type\": \"int\",\n",
    "                \"default\": 20,\n",
    "                \"transform\": \"transform_power_2\",\n",
    "                \"lower\": 2,\n",
    "                \"upper\": 20}\n",
    "            }\n",
    "    }\n",
    "hyper_parameter_values = {\n",
    "        'max_depth': 2,\n",
    "        'leaf_prediction': 'mean'}\n",
    "transform_hyper_parameter_values(fun_control, hyper_parameter_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.transform import transform_hyper_parameter_values\n",
    "fun_control = {\n",
    "    \"core_model_hyper_dict\": {\n",
    "        \"l1\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 3,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 3,\n",
    "            \"upper\": 8\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 4,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 4,\n",
    "            \"upper\": 9\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 4,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 1,\n",
    "            \"upper\": 4\n",
    "        },\n",
    "        \"act_fn\": {\n",
    "            \"levels\": [\n",
    "                \"Sigmoid\",\n",
    "                \"Tanh\",\n",
    "                \"ReLU\",\n",
    "                \"LeakyReLU\",\n",
    "                \"ELU\",\n",
    "                \"Swish\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"ReLU\",\n",
    "            \"transform\": \"None\",\n",
    "            \"class_name\": \"spotpython.torch.activation\",\n",
    "            \"core_model_parameter_type\": \"instance()\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 5\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"levels\": [\n",
    "                \"Adadelta\",\n",
    "                \"Adagrad\",\n",
    "                \"Adam\",\n",
    "                \"AdamW\",\n",
    "                \"SparseAdam\",\n",
    "                \"Adamax\",\n",
    "                \"ASGD\",\n",
    "                \"NAdam\",\n",
    "                \"RAdam\",\n",
    "                \"RMSprop\",\n",
    "                \"Rprop\",\n",
    "                \"SGD\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"SGD\",\n",
    "            \"transform\": \"None\",\n",
    "            \"class_name\": \"torch.optim\",\n",
    "            \"core_model_parameter_type\": \"str\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 11\n",
    "        },\n",
    "        \"dropout_prob\": {\n",
    "            \"type\": \"float\",\n",
    "            \"default\": 0.01,\n",
    "            \"transform\": \"None\",\n",
    "            \"lower\": 0.0,\n",
    "            \"upper\": 0.25\n",
    "        },\n",
    "        \"lr_mult\": {\n",
    "            \"type\": \"float\",\n",
    "            \"default\": 1.0,\n",
    "            \"transform\": \"None\",\n",
    "            \"lower\": 0.1,\n",
    "            \"upper\": 10.0\n",
    "        },\n",
    "        \"patience\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 2,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 2,\n",
    "            \"upper\": 6\n",
    "        },\n",
    "        \"batch_norm\": {\n",
    "            \"levels\": [\n",
    "                0,\n",
    "                1\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": 0,\n",
    "            \"transform\": \"None\",\n",
    "            \"core_model_parameter_type\": \"bool\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 1\n",
    "        },\n",
    "        \"initialization\": {\n",
    "            \"levels\": [\n",
    "                \"Default\",\n",
    "                \"kaiming_uniform\",\n",
    "                \"kaiming_normal\",\n",
    "                \"xavier_uniform\",\n",
    "                \"xavier_normal\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"Default\",\n",
    "            \"transform\": \"None\",\n",
    "            \"core_model_parameter_type\": \"str\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 4\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "hyper_parameter_values = {\n",
    "        'l1': 2,\n",
    "        'epochs': 3,\n",
    "        'batch_size': 4,\n",
    "        'act_fn': 'ReLU',\n",
    "        'optimizer': 'SGD',\n",
    "        'dropout_prob': 0.01,\n",
    "        'lr_mult': 1.0,\n",
    "        'patience': 3,\n",
    "        'batch_norm': 0,\n",
    "        'initialization': 'Default',        \n",
    "    }\n",
    "transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import assign_values\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "var_list = ['a', 'b']\n",
    "result = assign_values(X, var_list)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from spotpythonspot import Spot\n",
    "from spotpythonfunobjectivefunctions import Analytical\n",
    "from spotpythonutilsinit import fun_control_init, design_control_init\n",
    "from spotpythonutilsfile import load_result\n",
    "import pprint\n",
    "\n",
    "def _compare_dicts(dict1, dict2, ignore_keys=None):\n",
    "    \"\"\"\n",
    "    Compare two dictionaries, including element-wise comparison for numpy arrays\n",
    "    Print missing elements (keys) if the dictionaries do not match\n",
    "\n",
    "    Args:\n",
    "        dict1 (dict): First dictionary to compare\n",
    "        dict2 (dict): Second dictionary to compare\n",
    "        ignore_keys (list, optional): List of keys to ignore during comparison Default is None\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the dictionaries match, False otherwise\n",
    "    \"\"\"\n",
    "    if ignore_keys is None:\n",
    "        ignore_keys = []\n",
    "    # ensure that ignore_keys is a list\n",
    "    if not isinstance(ignore_keys, list):\n",
    "        ignore_keys = [ignore_keys]\n",
    "\n",
    "    keys1 = set(dict1keys()) - set(ignore_keys)\n",
    "    keys2 = set(dict2keys()) - set(ignore_keys)\n",
    "\n",
    "    if keys1 != keys2:\n",
    "        missing_in_dict1 = keys2 - keys1\n",
    "        missing_in_dict2 = keys1 - keys2\n",
    "        print(f\"Missing in dict1: {missing_in_dict1}\")\n",
    "        print(f\"Missing in dict2: {missing_in_dict2}\")\n",
    "        return False\n",
    "\n",
    "    for key in keys1:\n",
    "        if isinstance(dict1[key], npndarray) and isinstance(dict2[key], npndarray):\n",
    "            if not nparray_equal(dict1[key], dict2[key]):\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "        else:\n",
    "            if dict1[key] != dict2[key]:\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_save_and_load_experiment(tmp_path):\n",
    "    PREFIX = \"test_02\"\n",
    "    # Initialize function control\n",
    "    fun_control = fun_control_init(\n",
    "        PREFIX=PREFIX,\n",
    "        lower=nparray([-1, -1]),\n",
    "        upper=nparray([1, 1]),\n",
    "        verbosity=1\n",
    "    )\n",
    "    \n",
    "    design_control = design_control_init(init_size=7)\n",
    "\n",
    "    fun = Analytical()fun_sphere\n",
    "        \n",
    "    S = Spot(\n",
    "        fun=fun,\n",
    "        fun_control=fun_control,\n",
    "        design_control=design_control,\n",
    "    )\n",
    "    \n",
    "    X_start = nparray([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    S.run(X_start=X_start)\n",
    "\n",
    "    # Load the experiment\n",
    "    S_loaded = load_result(PREFIX)\n",
    "    print(f\"S: {S}\")    \n",
    "    print(f\"S_loaded: {S_loaded}\")\n",
    "    pprint.pprint(S_loaded)\n",
    "    loaded_fun_control = S_loaded.fun_control\n",
    "    loaded_design_control = S_loaded.design_control\n",
    "    loaded_surrogate_control = S_loaded.surrogate_control\n",
    "    loaded_optimizer_control = S_loaded.optimizer_control\n",
    "    \n",
    "    # Check if the loaded data matches the original data\n",
    "    # It is ok if the counter is different, because it is increased during the run\n",
    "    assert _compare_dicts(loaded_fun_control, fun_control, ignore_keys=\"counter\"), \"Loaded fun_control should match the original fun_control.\"\n",
    "    assert _compare_dicts(loaded_design_control, design_control), \"Loaded design_control should match the original design_control.\"\n",
    "    assert _compare_dicts(loaded_surrogate_control, S.surrogate_control), \"Loaded surrogate_control should match the original surrogate_control.\"\n",
    "    assert _compare_dicts(loaded_optimizer_control, S.optimizer_control), \"Loaded optimizer_control should match the original optimizer_control.\"\n",
    "\n",
    "    # Check if the S_loaded is an instance of Spot\n",
    "    assert isinstance(S_loaded, Spot), \"Loaded S_loaded should be an instance of Spot.\"\n",
    "\n",
    "    # Check if the design matrix and response vector are equal\n",
    "    # if there are differences, print the differences\n",
    "    # Differences are OK\n",
    "    # if not np.array_equal(S_loaded.X, S.X):\n",
    "    #     print(f\"Design matrix mismatch: {S_loaded.X} != {S.X}\")\n",
    "    # if not np.array_equal(S_loaded.y, S.y):\n",
    "    #     print(f\"Response vector mismatch: {S_loaded.y} != {S.y}\")\n",
    "\n",
    "\n",
    "test_save_and_load_experiment(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n",
    "\n",
    "ni = 7\n",
    "PREFIX = \"test_plot_progress_05\"\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "PREFIX=PREFIX,\n",
    "lower=np.array([-1, -1]),\n",
    "upper=np.array([1, 1]),\n",
    "fun_evals=fun_evals,\n",
    "tolerance_x=np.sqrt(np.spacing(1))\n",
    ")\n",
    "design_control = design_control_init(init_size=ni)\n",
    "surrogate_control = surrogate_control_init(n_theta=3)\n",
    "S = Spot(\n",
    "fun=fun,\n",
    "fun_control=fun_control,\n",
    "design_control=design_control,\n",
    "surrogate_control=surrogate_control,\n",
    ")\n",
    "S = S.run()\n",
    "\n",
    "# Test plot_progress with different parameters\n",
    "S.plot_progress(show=False)  # Test with show=False\n",
    "S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename\n",
    "# add NaN to S.y at position 2\n",
    "S.y[2] = np.nan\n",
    "S.plot_progress(show=False)  # Test with NaN in S.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot.spot import Spot\n",
    "from spotpython.utils.repair import repair_non_numeric\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    ")\n",
    "\n",
    "fun = Analytical().fun_branin_factor\n",
    "ni = 12\n",
    "spot_test = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control_init(\n",
    "        lower=np.array([-5, -0, 1]), upper=np.array([10, 15, 3]), var_type=[\"num\", \"num\", \"factor\"]\n",
    "    ),\n",
    "    design_control=design_control_init(init_size=ni),\n",
    ")\n",
    "spot_test.run()\n",
    "# 3rd variable should be a rounded float, because it was labeled as a factor\n",
    "assert spot_test.min_X[2] == round(spot_test.min_X[2])\n",
    "\n",
    "spot_test.X = spot_test.generate_design(\n",
    "    size=spot_test.design_control[\"init_size\"],\n",
    "    repeats=spot_test.design_control[\"repeats\"],\n",
    "    lower=spot_test.lower,\n",
    "    upper=spot_test.upper,\n",
    ")\n",
    "spot_test.X = repair_non_numeric(spot_test.X, spot_test.var_type)\n",
    "assert spot_test.X.ndim == 2\n",
    "assert spot_test.X.shape[0] == ni\n",
    "assert spot_test.X.shape[1] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spot_test.get_spot_attributes_as_df()\n",
    "list(df['Attribute Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "    \n",
    "# Setup: Configure initial parameters\n",
    "ni = 7\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX= \"test_get_spot_attributes_as_df\",\n",
    "    lower=np.array([-1]),\n",
    "    upper=np.array([1]),\n",
    "    fun_evals=n\n",
    ")\n",
    "design_control = design_control_init(init_size=ni)\n",
    "\n",
    "# Create instance of the Spot class\n",
    "S = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control\n",
    ")\n",
    "\n",
    "# Run the optimization\n",
    "S.run()\n",
    "\n",
    "# Get the attributes as a DataFrame\n",
    "df = S.get_spot_attributes_as_df()\n",
    "\n",
    "# Define expected attribute names (ensure these match your Spot class' attributes)\n",
    "expected_attributes = ['X',\n",
    "                        'all_lower',\n",
    "                        'all_upper',\n",
    "                        'all_var_name',\n",
    "                        'all_var_type',\n",
    "                        'counter',\n",
    "                        'de_bounds',\n",
    "                        'design',\n",
    "                        'design_control',\n",
    "                        'eps',\n",
    "                        'fun_control',\n",
    "                        'fun_evals',\n",
    "                        'fun_repeats',\n",
    "                        'ident',\n",
    "                        'infill_criterion',\n",
    "                        'k',\n",
    "                        'log_level',\n",
    "                        'lower',\n",
    "                        'max_surrogate_points',\n",
    "                        'max_time',\n",
    "                        'mean_X',\n",
    "                        'mean_y',\n",
    "                        'min_X',\n",
    "                        'min_mean_X',\n",
    "                        'min_mean_y',\n",
    "                        'min_y',\n",
    "                        'n_points',\n",
    "                        'noise',\n",
    "                        'ocba_delta',\n",
    "                        'optimizer_control',\n",
    "                        'progress_file',\n",
    "                        'red_dim',\n",
    "                        'rng',\n",
    "                        'show_models',\n",
    "                        'show_progress',\n",
    "                        'spot_writer',\n",
    "                        'surrogate',\n",
    "                        'surrogate_control',\n",
    "                        'tkagg',\n",
    "                        'tolerance_x',\n",
    "                        'upper',\n",
    "                        'var_name',\n",
    "                        'var_type',\n",
    "                        'var_y',\n",
    "                        'verbosity',\n",
    "                        'y']\n",
    "\n",
    "# Check that the DataFrame has the correct attributes\n",
    "assert list(df['Attribute Name']) == expected_attributes\n",
    "\n",
    "# Further checks can be done for specific attribute values\n",
    "# Example: Check that 'fun_evals' has the expected value\n",
    "fun_evals_row = df.query(\"`Attribute Name` == 'fun_evals'\")\n",
    "assert not fun_evals_row.empty and fun_evals_row['Attribute Value'].values[0] == n\n",
    "\n",
    "# Example: Check that 'lower' has the expected value\n",
    "lower_row = df.query(\"`Attribute Name` == 'lower'\")\n",
    "assert not lower_row.empty and lower_row['Attribute Value'].values[0] == [-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate_dic_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import iterate_dict_values\n",
    "var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n",
    "print(var_dict)\n",
    "list(iterate_dict_values(var_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.values import convert_keys\n",
    "d = {'a': 1, 'b': 2.1, 'c': 3}\n",
    "var_type = [\"int\", \"num\", \"int\"]\n",
    "convert_keys(d, var_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_dict_with_levels_and_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "from spotpython.hyperparameters.values import assign_values, get_var_name, iterate_dict_values, convert_keys, get_dict_with_levels_and_types, get_core_model_from_name, add_core_model_to_fun_control\n",
    "import pprint\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "coremodel, core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "for values in iterate_dict_values(var_dict):\n",
    "    values = convert_keys(values, fun_control[\"var_type\"])\n",
    "    pprint.pprint(values)\n",
    "    # pprint.pprint(fun_control)\n",
    "    values = get_dict_with_levels_and_types(fun_control=fun_control, v=values)\n",
    "    pprint.pprint(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_one_config_from_var_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import  get_core_model_from_name, add_core_model_to_fun_control, generate_one_config_from_var_dict\n",
    "import pprint\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "_ , core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "var_dict = {'l1': np.array([3.]),\n",
    "            'epochs': np.array([4.]),\n",
    "            'batch_size': np.array([4.]),\n",
    "            'act_fn': np.array([2.]),\n",
    "            'optimizer': np.array([11.]),\n",
    "            'dropout_prob': np.array([0.01]),\n",
    "            'lr_mult': np.array([1.]),\n",
    "            'patience': np.array([2.]),\n",
    "            'batch_norm': np.array([0.]),\n",
    "            'initialization': np.array([0.])}\n",
    "g = generate_one_config_from_var_dict(var_dict=var_dict, fun_control=fun_control)\n",
    "# Since g is an iterator, we need to call next to get the values\n",
    "values = next(g)\n",
    "pprint.pprint(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return_conf_list_from_var_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import  get_core_model_from_name, add_core_model_to_fun_control, return_conf_list_from_var_dict\n",
    "import pprint\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "_ , core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "var_dict = {'l1': np.array([3., 4.]),\n",
    "            'epochs': np.array([4., 3.]),\n",
    "            'batch_size': np.array([4., 4.]),\n",
    "            'act_fn': np.array([2., 1.]),\n",
    "            'optimizer': np.array([11., 10.]),\n",
    "            'dropout_prob': np.array([0.01, 0.]),\n",
    "            'lr_mult': np.array([1., 1.1]),\n",
    "            'patience': np.array([2., 3.]),\n",
    "            'batch_norm': np.array([0., 1.]),\n",
    "            'initialization': np.array([0., 1.])}\n",
    "return_conf_list_from_var_dict(var_dict=var_dict, fun_control=fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_one_config_from_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import get_core_model_from_name, add_core_model_to_fun_control, get_one_config_from_X\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "coremodel, core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "X = np.array([[3.0e+00, 4.0e+00, 4.0e+00, 2.0e+00, 1.1e+01, 1.0e-02, 1.0e+00, 2.0e+00, 0.0e+00,\n",
    " 0.0e+00]])\n",
    "print(X)\n",
    "get_one_config_from_X(X, fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_tuned_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.spot import Spot\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_hyperparameter, get_tuned_architecture\n",
    "\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    force_run=False,\n",
    "    PREFIX=\"get_one_config_from_X\",\n",
    "    save_experiment=True,\n",
    "    fun_evals=10,\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "set_hyperparameter(fun_control, \"epochs\", [2,2])\n",
    "set_hyperparameter(fun_control, \"patience\", [1,2])\n",
    "design_control = design_control_init(init_size=5)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "S = Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "S.run()\n",
    "get_tuned_architecture(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_and_save_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import pprint\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.utils.file import load_experiment, load_result\n",
    "\n",
    "def _compare_dicts(dict1, dict2, ignore_keys=None):\n",
    "    \"\"\"\n",
    "    Compare two dictionaries, including element-wise comparison for numpy arrays.\n",
    "    Print missing elements (keys) if the dictionaries do not match.\n",
    "\n",
    "    Args:\n",
    "        dict1 (dict): First dictionary to compare.\n",
    "        dict2 (dict): Second dictionary to compare.\n",
    "        ignore_keys (list, optional): List of keys to ignore during comparison. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the dictionaries match, False otherwise.\n",
    "    \"\"\"\n",
    "    if ignore_keys is None:\n",
    "        ignore_keys = []\n",
    "    # ensure that ignore_keys is a list\n",
    "    if not isinstance(ignore_keys, list):\n",
    "        ignore_keys = [ignore_keys]\n",
    "\n",
    "    keys1 = set(dict1.keys()) - set(ignore_keys)\n",
    "    keys2 = set(dict2.keys()) - set(ignore_keys)\n",
    "\n",
    "    if keys1 != keys2:\n",
    "        missing_in_dict1 = keys2 - keys1\n",
    "        missing_in_dict2 = keys1 - keys2\n",
    "        print(f\"Missing in dict1: {missing_in_dict1}\")\n",
    "        print(f\"Missing in dict2: {missing_in_dict2}\")\n",
    "        return False\n",
    "\n",
    "    for key in keys1:\n",
    "        if isinstance(dict1[key], np.ndarray) and isinstance(dict2[key], np.ndarray):\n",
    "            if not np.array_equal(dict1[key], dict2[key]):\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "        else:\n",
    "            if dict1[key] != dict2[key]:\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "PREFIX = \"test_save_and_load_experiment_04\"\n",
    "# Initialize function control\n",
    "fun_control = fun_control_init(\n",
    "    save_experiment=True,\n",
    "    PREFIX=PREFIX,\n",
    "    lower=np.array([-1, -1]),\n",
    "    upper=np.array([1, 1]),\n",
    "    verbosity=2,\n",
    "    log_level=50\n",
    ")\n",
    "\n",
    "design_control = design_control_init(init_size=7)\n",
    "\n",
    "fun = Analytical().fun_sphere\n",
    "    \n",
    "S = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control\n",
    ")\n",
    "\n",
    "# Load the experiment\n",
    "S_loaded = load_experiment(PREFIX)\n",
    "print(f\"S: {S}\")    \n",
    "print(f\"S_loaded: {S_loaded}\")\n",
    "pprint.pprint(S_loaded)\n",
    "loaded_fun_control = S_loaded.fun_control\n",
    "# pprint.pprint(loaded_fun_control)\n",
    "loaded_design_control = S_loaded.design_control\n",
    "loaded_surrogate_control = S_loaded.surrogate_control\n",
    "loaded_optimizer_control = S_loaded.optimizer_control\n",
    "\n",
    "# Check if the loaded data matches the original data\n",
    "# It is ok if the counter is different, because it is increased during the run\n",
    "assert _compare_dicts(loaded_fun_control, fun_control, ignore_keys=\"counter\"), \"Loaded fun_control should match the original fun_control.\"\n",
    "assert _compare_dicts(loaded_design_control, design_control), \"Loaded design_control should match the original design_control.\"\n",
    "assert _compare_dicts(loaded_surrogate_control, S.surrogate_control), \"Loaded surrogate_control should match the original surrogate_control.\"\n",
    "assert _compare_dicts(loaded_optimizer_control, S.optimizer_control), \"Loaded optimizer_control should match the original optimizer_control.\"\n",
    "\n",
    "# Check if the S_loaded is an instance of Spot\n",
    "assert isinstance(S_loaded, Spot), \"Loaded S_loaded should be an instance of Spot.\"\n",
    "\n",
    "# Check if the design matrix and response vector are equal\n",
    "# if there are differences, print the differences\n",
    "# Differences are OK\n",
    "if not np.array_equal(S_loaded.X, S.X):\n",
    "    print(f\"Design matrix mismatch: {S_loaded.X} != {S.X}\")\n",
    "if not np.array_equal(S_loaded.y, S.y):\n",
    "    print(f\"Response vector mismatch: {S_loaded.y} != {S.y}\")\n",
    "\n",
    "S_loaded.run()\n",
    "\n",
    "S.run()\n",
    "S_loaded_2 = load_result(PREFIX)\n",
    "S_loaded_2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_exp_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.eda import print_exp_table, print_res_table\n",
    "from spotpython.hyperparameters.values import set_hyperparameter\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=\"show_exp_table\",\n",
    "    fun_evals=5,\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
    "set_hyperparameter(fun_control, \"l1\", [1,2])\n",
    "set_hyperparameter(fun_control, \"epochs\", [2,2])\n",
    "set_hyperparameter(fun_control, \"batch_size\", [4,11])\n",
    "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
    "set_hyperparameter(fun_control, \"patience\", [1,2])\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "print_exp_table(fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_res_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.eda import print_res_table\n",
    "from spotpython.hyperparameters.values import set_hyperparameter\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=\"show_exp_table\",\n",
    "    fun_evals=5,\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
    "set_hyperparameter(fun_control, \"l1\", [1,2])\n",
    "set_hyperparameter(fun_control, \"epochs\", [2,2])\n",
    "set_hyperparameter(fun_control, \"batch_size\", [4,11])\n",
    "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
    "set_hyperparameter(fun_control, \"patience\", [1,2])\n",
    "design_control = design_control_init(init_size=3)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "S = Spot(fun=fun, fun_control=fun_control, design_control=design_control)\n",
    "\n",
    "S.run()\n",
    "\n",
    "print_res_table(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_new_X0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "            seed=1,\n",
    "            n_points=10,\n",
    "            ocba_delta=0,\n",
    "            lower = np.array([-1, -1]),\n",
    "            upper = np.array([1, 1])\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    ")\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "X0 = S.get_new_X0()\n",
    "assert X0.shape[0] == S.n_points\n",
    "assert X0.shape[1] == S.lower.size\n",
    "# assert new points are in the interval [lower, upper]\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "# print using 20 digits precision\n",
    "np.set_printoptions(precision=20)\n",
    "print(f\"X0: {X0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init\n",
    "nn = 10\n",
    "fun_sphere = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        n_points=nn,\n",
    "        )\n",
    "spot_1 = spot.Spot(\n",
    "    fun=fun_sphere,\n",
    "    fun_control=fun_control,\n",
    "    )\n",
    "# (S-2) Initial Design:\n",
    "spot_1.X = spot_1.design.scipy_lhd(\n",
    "    spot_1.design_control[\"init_size\"], lower=spot_1.lower, upper=spot_1.upper\n",
    ")\n",
    "print(f\"spot_1.X: {spot_1.X}\")\n",
    "# (S-3): Eval initial design:\n",
    "spot_1.y = spot_1.fun(spot_1.X)\n",
    "print(f\"spot_1.y: {spot_1.y}\")\n",
    "spot_1.fit_surrogate()\n",
    "spot_1.suggest_X0()\n",
    "X0 = spot_1.X0\n",
    "print(f\"X0: {X0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate_mean_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.aggregate import aggregate_mean_var\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4], [1, 2]])\n",
    "y = np.array([1, 2, 3])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1,2]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.ones((2, 3))\n",
    "y_1 = np.sum(X_1, axis=1)\n",
    "y_2 = 2 * y_1\n",
    "X_2 = np.append(X_1, 2 * X_1, axis=0)\n",
    "X = np.append(X_2, X_1, axis=0)\n",
    "y = np.append(y_1, y_2, axis=0)\n",
    "y = np.append(y, y_2, axis=0)\n",
    "print(X)\n",
    "print(y)\n",
    "Z = aggregate_mean_var(X, y, var_empirical=True)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.ones((2, 3))\n",
    "y_1 = np.sum(X_1, axis=1)\n",
    "y_2 = 2 * y_1\n",
    "X_2 = np.append(X_1, 2 * X_1, axis=0)\n",
    "X = np.append(X_2, X_1, axis=0)\n",
    "y = np.append(y_1, y_2, axis=0)\n",
    "y = np.append(y, y_2, axis=0)\n",
    "print(X)\n",
    "print(y)\n",
    "Z = aggregate_mean_var(X, y, var_empirical=False)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.aggregate import aggregate_mean_var\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4], [1, 2]])\n",
    "y = np.array([1, 2, 3])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y, var_empirical=True)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1,2]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y, var_empirical=True)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit_surrogate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# number of initial points:\n",
    "ni = 0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select_distant_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.aggregate import select_distant_points\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "selected_points, selected_y = select_distant_points(X, y, 3)\n",
    "print(selected_points)\n",
    "print(selected_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# number of initial points:\n",
    "ni = 0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.surrogate.predict(np.array([[0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.spot import spot\n",
    "# number of initial points:\n",
    "ni = 0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "X_shape_before = S.X.shape\n",
    "print(f\"X_shape_before: {X_shape_before}\")\n",
    "print(f\"y_size_before: {S.y.size}\")\n",
    "y_size_before = S.y.size\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.update_design()\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "print(f\"S.n_points: {S.n_points}\")\n",
    "print(f\"X_shape_after: {S.X.shape}\")\n",
    "print(f\"y_size_after: {S.y.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "        sigma=0.02,\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        noise=True,\n",
    "        ocba_delta=1,\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni, repeats=2)\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            design_control=design_control,\n",
    "            fun_control=fun_control\n",
    ")\n",
    "S.initialize_design(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "X_shape_before = S.X.shape\n",
    "print(f\"X_shape_before: {X_shape_before}\")\n",
    "print(f\"y_size_before: {S.y.size}\")\n",
    "y_size_before = S.y.size\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.update_design()\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "print(f\"S.n_points: {S.n_points}\")\n",
    "print(f\"S.ocba_delta: {S.ocba_delta}\")\n",
    "print(f\"X_shape_after: {S.X.shape}\")\n",
    "print(f\"y_size_after: {S.y.size}\")\n",
    "# compare the shapes of the X and y values before and after the update_design method\n",
    "assert X_shape_before[0] + S.ocba_delta == S.X.shape[0]\n",
    "assert X_shape_before[1] == S.X.shape[1]\n",
    "assert y_size_before + S.ocba_delta == S.y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_ocba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.budget.ocba import get_ocba\n",
    "from spotpython.utils import fun_control_init, design_control_init, surrogate_control_init\n",
    "# Example is based on the example from the book:\n",
    "# Chun-Hung Chen and Loo Hay Lee:\n",
    "#     Stochastic Simulation Optimization: An Optimal Computer Budget Allocation,\n",
    "#     pp. 49 and pp. 215\n",
    "#     p. 49:\n",
    "#     mean_y = np.array([1,2,3,4,5])\n",
    "#     var_y = np.array([1,1,9,9,4])\n",
    "#     get_ocba(mean_y, var_y, 50)\n",
    "#     [11  9 19  9  2]\n",
    "fun = Analytical().fun_linear\n",
    "fun_control = fun_control_init(\n",
    "                lower = np.array([-1]),\n",
    "                upper = np.array([1]),\n",
    "                fun_evals = 20,\n",
    "                fun_repeats = 2,\n",
    "                noise = True,\n",
    "                ocba_delta=1,\n",
    "                seed=123,\n",
    "                show_models=False,\n",
    "                sigma=0.001,\n",
    "                )\n",
    "design_control = design_control_init(init_size=3, repeats=2)\n",
    "surrogate_control = surrogate_control_init(noise=True)\n",
    "spot_1_noisy = Spot(fun=fun,                \n",
    "                fun_control = fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control)\n",
    "spot_1_noisy.run()\n",
    "spot_2 = copy.deepcopy(spot_1_noisy)\n",
    "spot_2.mean_y = np.array([1,2,3,4,5])\n",
    "spot_2.var_y = np.array([1,1,9,9,4])\n",
    "n = 50\n",
    "o = get_ocba(spot_2.mean_y, spot_2.var_y, n)\n",
    "assert sum(o) == 50\n",
    "assert (o == np.array([[11, 9, 19, 9, 2]])).all()\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_ocba_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.budget.ocba import get_ocba_X\n",
    "from spotpython.utils.aggregate import aggregate_mean_var\n",
    "import numpy as np\n",
    "X = np.array([[1,2,3],\n",
    "            [1,2,3],\n",
    "            [4,5,6],\n",
    "            [4,5,6],\n",
    "            [4,5,6],\n",
    "            [7,8,9],\n",
    "            [7,8,9],])\n",
    "y = np.array([1,2,30,40, 40, 500, 600  ])\n",
    "Z = aggregate_mean_var(X=X, y=y)\n",
    "mean_X = Z[0]\n",
    "mean_y = Z[1]\n",
    "var_y = Z[2]\n",
    "print(f\"X: {X}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"mean_X: {mean_X}\")\n",
    "print(f\"mean_y: {mean_y}\")\n",
    "print(f\"var_y: {var_y}\")\n",
    "delta = 5\n",
    "X_new = get_ocba_X(X=mean_X, means=mean_y, vars=var_y, delta=delta,verbose=True)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## suggest_new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init\n",
    "nn = 3\n",
    "fun_sphere = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        n_points=nn,\n",
    "        )\n",
    "S = Spot(\n",
    "    fun=fun_sphere,\n",
    "    fun_control=fun_control,\n",
    "    )\n",
    "S.X = S.design.scipy_lhd(\n",
    "    S.design_control[\"init_size\"], lower=S.lower, upper=S.upper\n",
    ")\n",
    "print(f\"S.X: {S.X}\")\n",
    "S.y = S.fun(S.X)\n",
    "print(f\"S.y: {S.y}\")\n",
    "S.fit_surrogate()\n",
    "X0 = S.suggest_new_X()\n",
    "print(f\"X0: {X0}\")\n",
    "assert X0.size == S.n_points * S.k\n",
    "assert X0.ndim == 2\n",
    "assert X0.shape[0] == nn\n",
    "assert X0.shape[1] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.optimize import dual_annealing\n",
    "from scipy.optimize import basinhopping\n",
    "    \n",
    "nn = 2\n",
    "fun_sphere = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower=np.array([-1, -1]),\n",
    "    upper=np.array([1, 1]),\n",
    "    n_points=nn,\n",
    ")\n",
    "design_control = design_control_init(init_size=10)\n",
    "surrogate_control = surrogate_control_init()\n",
    "\n",
    "# optimizers = [dual_annealing, differential_evolution, direct, shgo, basinhopping]\n",
    "optimizers = [differential_evolution, dual_annealing, direct, shgo]\n",
    "\n",
    "for optimizer_name in optimizers:\n",
    "    optimizer_control = optimizer_control_init()\n",
    "\n",
    "    S = Spot(\n",
    "        fun=fun_sphere,\n",
    "        fun_control=fun_control,\n",
    "        design_control=design_control,\n",
    "        optimizer_control=optimizer_control,\n",
    "        surrogate_control=surrogate_control,\n",
    "        optimizer=optimizer_name\n",
    "    )\n",
    "    \n",
    "    S.X = S.design.scipy_lhd(\n",
    "        S.design_control[\"init_size\"], lower=S.lower, upper=S.upper\n",
    "    )\n",
    "    S.y = S.fun(S.X)\n",
    "    S.fit_surrogate()\n",
    "    X0 = S.suggest_new_X()\n",
    "    print(f\"X0: {X0}\")\n",
    "\n",
    "    assert X0.size <= S.n_points * S.k\n",
    "    assert X0.ndim == 2\n",
    "    assert X0.shape[0] <= nn\n",
    "    assert X0.shape[1] == 2\n",
    "    assert np.all(X0 >= S.lower)\n",
    "    assert np.all(X0 <= S.upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=100, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.plot(X, s_ei, label=\"Expected Improvement\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, -s_ei, label=\"Negative Expected Improvement\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 0, 0], [1, 0, 0]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.fit(nat_X, nat_y)\n",
    "print(S.Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4], [1,2]])\n",
    "nat_y = np.array([1, 2, 11])\n",
    "S = Kriging()\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "print(f\"S.nat_X: {S.nat_X}\")\n",
    "print(f\"S.nat_y: {S.nat_y}\")\n",
    "print(f\"S.aggregated_mean_y: {S.aggregated_mean_y}\")\n",
    "print(f\"S.min_X: {S.min_X}\")\n",
    "print(f\"S.max_X: {S.max_X}\")\n",
    "print(f\"S.n: {S.n}\")\n",
    "print(f\"S.k: {S.k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "assert S.var_type == ['num', 'num']\n",
    "assert S.num_mask.all() == True\n",
    "assert S.factor_mask.all() == False\n",
    "assert S.int_mask.all() == False\n",
    "assert S.ordered_mask.all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "nat_y = np.array([1, 2, 3])\n",
    "var_type = [\"num\", \"int\", \"float\"]\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(var_type=var_type, seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "assert S.var_type == [\"num\", \"int\", \"float\"]\n",
    "assert S.num_mask.all() == False\n",
    "assert S.factor_mask.all() == False\n",
    "assert S.int_mask.all() == False\n",
    "assert S.ordered_mask.all() == True\n",
    "assert np.all(S.num_mask == np.array([True, False, False]))\n",
    "assert np.all(S.int_mask == np.array([False, True, False]))\n",
    "assert np.all(S.ordered_mask == np.array([True, True, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "# Create a Kriging instance with var_type shorter than k\n",
    "var_type = [\"num\"]\n",
    "S = Kriging(var_type=var_type, seed=124, n_theta=2, n_p=2, optim_p=True, noise=True)\n",
    "\n",
    "# Initialize variables\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "nat_y = np.array([1, 2, 3])\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "\n",
    "# Set variable types\n",
    "S._set_variable_types()\n",
    "\n",
    "print(f\"S.var_type: {S.var_type}\")\n",
    "\n",
    "# Check if the variable types are defaulted to 'num' and masks are set correctly\n",
    "assert S.var_type == [\"num\", \"num\"]\n",
    "assert np.all(S.num_mask == np.array([True, True]))\n",
    "assert np.all(S.int_mask == np.array([False, False]))\n",
    "assert np.all(S.ordered_mask == np.array([True, True]))\n",
    "assert np.all(S.factor_mask == np.array([False, False]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "n = nat_X.shape[0]\n",
    "k = nat_X.shape[1]\n",
    "nat_y = np.array([1, 2])\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "assert S.theta.all() == array([0., 0.]).all()\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "t = np.ones(n_theta, dtype=float) * n / (100 * k)\n",
    "assert S.theta.all() == t.all()\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "n = nat_X.shape[0]\n",
    "k = nat_X.shape[1]\n",
    "nat_y = np.array([1, 2, 3])\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "assert S.theta.all() == array([0., 0.]).all()\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "t = np.ones(n_theta, dtype=float) * n / (100 * k)\n",
    "assert S.theta.all() == t.all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "from numpy import log, var\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "nat_y = np.array([1, 2, 3])\n",
    "n = nat_X.shape[0]\n",
    "k = nat_X.shape[1]\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "S._initialize_matrices()\n",
    "assert np.all(S.p == 2.0 * np.ones(n_p))\n",
    "# if var(self.nat_y) is > 0, then self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n",
    "# else self.pen_val = self.n * var(self.nat_y) + 1e4\n",
    "assert S.pen_val == nat_X.shape[0] * log(var(S.nat_y)) + 1e4\n",
    "assert S.Psi.shape == (n, n)\n",
    "assert S.psi.shape == (n, 1)\n",
    "assert S.one.shape == (n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "S = Kriging()\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "S = Kriging(n_theta=2, n_p=2, optim_p=True)\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "S = Kriging(n_theta=2, n_p=2, optim_p=True, noise=True)\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "S = Kriging(n_theta=2, n_p=2, noise=True)\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "S._initialize_matrices()\n",
    "S._set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "assert  len(new_theta_p_Lambda) == n_theta + n_p + 1\n",
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "S._initialize_matrices()\n",
    "S._set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "assert len(new_theta_p_Lambda) == n_theta + n_p\n",
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n_theta=2\n",
    "n_p=1\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "S._initialize_matrices()\n",
    "S._set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "assert  len(new_theta_p_Lambda) == n_theta + n_p\n",
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n_theta=1\n",
    "n_p=1\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=False, noise=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "S._initialize_matrices()\n",
    "S._set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "assert  len(new_theta_p_Lambda) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract_from_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build import Kriging\n",
    "num_theta = 2\n",
    "num_p = 3\n",
    "S = Kriging(\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=True\n",
    ")\n",
    "bounds_array = np.array([1, 2, 3, 4, 5, 6])\n",
    "S.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "assert np.array_equal(S.theta,\n",
    "    [1, 2]), f\"Expected theta to be [1, 2] but got {S.theta}\"\n",
    "assert np.array_equal(S.p,\n",
    "    [3, 4, 5]), f\"Expected p to be [3, 4, 5] but got {S.p}\"\n",
    "assert S.Lambda == 6, f\"Expected Lambda to be 6 but got {S.Lambda}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build import Kriging\n",
    "num_theta = 1\n",
    "num_p = 1\n",
    "S = Kriging(\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=False,\n",
    "    noise=False\n",
    ")\n",
    "bounds_array = np.array([1])\n",
    "S.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "assert np.array_equal(S.theta,\n",
    "    [1]), f\"Expected theta to be [1] but got {S.theta}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build import Kriging\n",
    "num_theta = 1\n",
    "num_p = 2\n",
    "S = Kriging(\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=True\n",
    ")\n",
    "bounds_array = np.array([1, 2, 3, 4])\n",
    "S.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "assert np.array_equal(S.theta,\n",
    "    [1]), f\"Expected theta to be [1, 2] but got {S.theta}\"\n",
    "assert np.array_equal(S.p,\n",
    "    [2, 3]), f\"Expected p to be [3, 4, 5] but got {S.p}\"\n",
    "assert S.Lambda == 4, f\"Expected Lambda to be 6 but got {S.Lambda}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build_Psi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S._set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S._initialize_matrices()\n",
    "S._set_de_bounds()\n",
    "new_theta_p_Lambda = S._optimize_model()\n",
    "S._extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")\n",
    "print(f\"S.cnd_Psi: {S.cnd_Psi}\")\n",
    "print(f\"S.inf_Psi: {S.inf_Psi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all_estimates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"/Users/bartz/workspace/ki-for-hcf/KI-basierte_Verdichterauslegung/\"\n",
    "import pandas as pd\n",
    "\n",
    "def load_pkl_files(dirname=\"\"):\n",
    "    df_x1 = pd.read_pickle(dirname+'x_1.pkl')\n",
    "    df_x2 = pd.read_pickle(dirname+'x_2.pkl')\n",
    "    df_y = pd.read_pickle(dirname+'y.pkl')\n",
    "    # combine x_1 and x_2 and  y\n",
    "    df = pd.concat([df_x1, df_x2, df_y], axis=1)\n",
    "    df_x1x2 = pd.concat([df_x1, df_x2], axis=1)\n",
    "    df_x1y = pd.concat([df_x1, df_y], axis=1)\n",
    "    df_x2y = pd.concat([df_x2, df_y], axis=1)\n",
    "    return df, df_x1, df_x2, df_x1x2, df_x1y, df_x2y, df_y\n",
    "df, df_x1, df_x2, df_x1x2, df_x1y, df_x2y, df_y = load_pkl_files(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_vars(crude):\n",
    "    \"\"\"Utility function to extract variables from a formula.\n",
    "    \n",
    "    Args:\n",
    "        crude (str): A formula.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of variables.\n",
    "        \n",
    "    Examples:\n",
    "        >>> all_vars(\"y ~ x1 + x2\")\n",
    "        ['y', 'x1', 'x2']\n",
    "        >>> all_vars(\"y ~ x1 + x2 + x3\")\n",
    "        ['y', 'x1', 'x2', 'x3']\n",
    "    \"\"\"    \n",
    "    # Split the formula into the dependent and independent variables\n",
    "    dependent, independent = crude.split(\"~\")\n",
    "    # Strip whitespace and split the independent variables by '+'\n",
    "    independent_vars = independent.strip().split(\"+\")\n",
    "    # Combine the dependent variable with the independent variables\n",
    "    return [dependent.strip()] + [var.strip() for var in independent_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df_y.iloc[:, 0], df_x2], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars(\"AR ~ AK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import norm\n",
    "from statsmodels.tools.eval_measures import aic\n",
    "\n",
    "def all_lm(crude, xlist, data, remove_na=True):\n",
    "    # Prepare the data frame\n",
    "    data = copy.deepcopy(data)\n",
    "    data = data[all_vars(crude) + xlist]\n",
    "    if remove_na:\n",
    "        data = data.dropna()\n",
    "    print(data.head())\n",
    "    # Crude model\n",
    "    mod_0 = ols(crude, data=data).fit()\n",
    "    p = mod_0.pvalues.iloc[1]\n",
    "    print(f\"p-values: {p}\")\n",
    "    estimate = mod_0.params.iloc[1]\n",
    "    print(f\"estimate: {estimate}\")  \n",
    "    conf_int = mod_0.conf_int().iloc[1]\n",
    "    print(f\"conf_int: {conf_int}\")\n",
    "    aic_value = mod_0.aic\n",
    "    print(f\"aic: {aic_value}\")\n",
    "    n = len(mod_0.resid)\n",
    "    df_0 = pd.DataFrame([[\"Crude\", estimate, conf_int[0], conf_int[1], p, aic_value, n]], \n",
    "                        columns=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\"])\n",
    "    \n",
    "    # All combinations model\n",
    "    comb_lst = list(itertools.chain.from_iterable(itertools.combinations(xlist, r) for r in range(1, len(xlist)+1)))\n",
    "    models = [ols(f\"{crude} + {' + '.join(comb)}\", data=data).fit() for comb in comb_lst]\n",
    "\n",
    "    df_list = []\n",
    "    for i, model in enumerate(models):\n",
    "        p = model.pvalues.iloc[1]\n",
    "        estimate = model.params.iloc[1]\n",
    "        conf_int = model.conf_int().iloc[1]\n",
    "        aic_value = model.aic\n",
    "        n = len(model.resid)\n",
    "        comb_str = \", \".join(comb_lst[i])\n",
    "        df_list.append([comb_str, estimate, conf_int[0], conf_int[1], p, aic_value, n])\n",
    "    \n",
    "    df_coef = pd.DataFrame(df_list, columns=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\"])\n",
    "    estimates = pd.concat([df_0, df_coef], ignore_index=True)\n",
    "    return {\"estimate\": estimates, \"xlist\": xlist, \"fun\": \"all_lm\", \"crude\": crude, \"family\": \"lm\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlist2 = [\"AL\", \"AM\", \"AN\", \"AO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = all_lm(crude = \"AR ~ AK\", xlist = vlist2, data = df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_plot(data, xlabels=None, xlim=(0, 1), xlab=\"P value\", ylim=None, ylab=None, xscale_log=True, yscale_log=False, title=None):\n",
    "    data = copy.deepcopy(data)\n",
    "    if xlabels is None:\n",
    "        xlabels = [0, 0.001, 0.01, 0.05, 0.2, 0.5, 1]\n",
    "    xbreaks = np.power(xlabels, np.log(0.5) / np.log(0.05))\n",
    "    \n",
    "    result_df = data[\"estimate\"]\n",
    "    if ylab is None:\n",
    "        ylab = \"Coefficient\" if data[\"fun\"] == \"all_lm\" else \"Effect estimates\"\n",
    "    hline = 0 if data[\"fun\"] == \"all_lm\" else 1\n",
    "    \n",
    "    result_df['p_value'] = np.power(result_df['p'], np.log(0.5) / np.log(0.05))\n",
    "    if ylim is None:\n",
    "        maxv = max(result_df['estimate'].max(), abs(result_df['estimate'].min()))\n",
    "        ylim = (-maxv, maxv) if data[\"fun\"] == \"all_lm\" else (1/maxv, maxv)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=result_df, x=\"p_value\", y=\"estimate\")\n",
    "    if xscale_log:\n",
    "        plt.xscale('log')\n",
    "    if yscale_log:\n",
    "        plt.yscale('log')\n",
    "    plt.xticks(ticks=xbreaks, labels=xlabels)\n",
    "    plt.axvline(x=0.5, linestyle='--')\n",
    "    plt.axhline(y=hline, linestyle='--')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def all_plot2(data, xlabels=None, xlim=(0, 1), xlab=\"P value\", ylim=None, ylab=None, yscale_log=False, title=None, grid=True, ncol=2):\n",
    "    \"\"\"\n",
    "    Generates a panel of scatter plots with effect estimates of all possible models against p-values.\n",
    "    Each plot includes effect estimates from all models including a specific variable.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary containing the following keys:\n",
    "            - estimate (pd.DataFrame): A DataFrame containing the estimates.\n",
    "            - xlist (list): A list of variables.\n",
    "            - fun (str): The function name.\n",
    "            - family (str): The family of the model.\n",
    "        xlabels (list): A list of x-axis labels.\n",
    "        xlim (tuple): The x-axis limits.\n",
    "        xlab (str): The x-axis label.\n",
    "        ylim (tuple): The y-axis limits.\n",
    "        ylab (str): The y-axis label.\n",
    "        yscale_log (bool): Whether to scale y-axis to log10. Default is False.\n",
    "        title (str): The title of the plot.\n",
    "        grid (bool): Whether to display gridlines. Default is True.\n",
    "        ncol (int): Number of columns in the plot grid. Default is 2.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "               \n",
    "    Examples:\n",
    "        data = {\n",
    "            \"estimate\": pd.DataFrame({\n",
    "                \"variables\": [\"Crude\", \"AL\", \"AM\", \"AN\", \"AO\"],\n",
    "                \"estimate\": [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                \"conf_low\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                \"conf_high\": [0.9, 1.0, 1.1, 1.2, 1.3],\n",
    "                \"p\": [0.01, 0.02, 0.03, 0.04, 0.05],\n",
    "                \"aic\": [100, 200, 300, 400, 500],\n",
    "                \"n\": [10, 20, 30, 40, 50]\n",
    "            }),\n",
    "            \"xlist\": [\"AL\", \"AM\", \"AN\", \"AO\"],\n",
    "            \"fun\": \"all_lm\"\n",
    "        }\n",
    "        all_plot2(data)\n",
    "    \"\"\"\n",
    "    if xlabels is None:\n",
    "        xlabels = [0, 0.001, 0.01, 0.05, 0.2, 0.5, 1]\n",
    "    xbreaks = np.power(xlabels, np.log(0.5) / np.log(0.05))\n",
    "    \n",
    "    result_df = data[\"estimate\"]\n",
    "    if ylab is None:\n",
    "        ylab = {\n",
    "            \"all_lm\": \"Coefficient\",\n",
    "            \"poisson\": \"Rate ratio\",\n",
    "            \"binomial\": \"Odds ratio\"\n",
    "        }.get(data.get(\"fun\"), \"Effect estimates\")\n",
    "    \n",
    "    hline = 0 if data[\"fun\"] == \"all_lm\" else 1\n",
    "    \n",
    "    result_df[\"p_value\"] = np.power(result_df[\"p\"], np.log(0.5) / np.log(0.05))\n",
    "    if ylim is None:\n",
    "        maxv = max(result_df[\"estimate\"].max(), abs(result_df[\"estimate\"].min()))\n",
    "        if data[\"fun\"] == \"all_lm\":\n",
    "            ylim = (-maxv, maxv)\n",
    "        else:\n",
    "            ylim = (1 / maxv, maxv)\n",
    "\n",
    "    # Create a DataFrame to mark inclusion of variables\n",
    "    mark_df = pd.DataFrame({x: result_df[\"variables\"].str.contains(x).astype(int) for x in data[\"xlist\"]})\n",
    "    df_scatter = pd.concat([result_df, mark_df], axis=1)\n",
    "    \n",
    "    # Melt the DataFrame for plotting\n",
    "    df_long = df_scatter.melt(id_vars=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\", \"p_value\"], \n",
    "                              value_vars=data[\"xlist\"],\n",
    "                              var_name=\"variable\", value_name=\"inclusion\")\n",
    "    df_long[\"inclusion\"] = df_long[\"inclusion\"].apply(lambda x: \"Included\" if x > 0 else \"Not included\")\n",
    "\n",
    "    # Calculate number of rows based on number of columns\n",
    "    nrow = int(np.ceil(len(data[\"xlist\"]) / ncol))\n",
    "\n",
    "    # Plotting\n",
    "    g = sns.FacetGrid(df_long, col=\"variable\", hue=\"inclusion\", palette={\"Included\": \"blue\", \"Not included\": \"orange\"},\n",
    "                      col_wrap=ncol, height=4, sharex=False, sharey=False)\n",
    "    g.map(sns.scatterplot, \"p_value\", \"estimate\")\n",
    "    g.add_legend()\n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_xticks(xbreaks)\n",
    "        ax.set_xticklabels(xlabels)\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.axvline(x=0.5, linestyle='--', linewidth=1.5, color='black')  # Black dashed vertical line\n",
    "        ax.axhline(y=hline, linestyle='--', linewidth=1.5, color='black')  # Black dashed horizontal line\n",
    "        if grid:\n",
    "            ax.grid(True)\n",
    "    if yscale_log:\n",
    "        g.set(yscale=\"log\")\n",
    "    g.set_axis_labels(xlab, ylab)\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    if title:\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        g.figure.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot2(res, ylim=(-0.2,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def all_plot2_old(data, xlabels=None, xlim=(0, 1), xlab=\"P value\", ylim=None, ylab=None, title=None, grid=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict): A dictionary containing the following keys:\n",
    "            - estimate (pd.DataFrame): A DataFrame containing the estimates.\n",
    "            - xlist (list): A list of variables.\n",
    "            - fun (str): The function name.\n",
    "            - family (str): The family of the model.\n",
    "        xlabels (list): A list of x-axis labels.\n",
    "        xlim (tuple): The x-axis limits.\n",
    "        xlab (str): The x-axis label.\n",
    "        ylim (tuple): The y-axis limits.\n",
    "        ylab (str): The y-axis label.\n",
    "        title (str): The title of the plot.\n",
    "        grid (bool): Whether to display gridlines. Default is True.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "               \n",
    "    Examples:\n",
    "        data = {\n",
    "            \"estimate\": pd.DataFrame({\n",
    "                \"variables\": [\"Crude\", \"AL\", \"AM\", \"AN\", \"AO\"],\n",
    "                \"estimate\": [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                \"conf_low\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                \"conf_high\": [0.9, 1.0, 1.1, 1.2, 1.3],\n",
    "                \"p\": [0.01, 0.02, 0.03, 0.04, 0.05],\n",
    "                \"aic\": [100, 200, 300, 400, 500],\n",
    "                \"n\": [10, 20, 30, 40, 50]\n",
    "            }),\n",
    "            \"xlist\": [\"AL\", \"AM\", \"AN\", \"AO\"],\n",
    "            \"fun\": \"all_lm\"\n",
    "        }\n",
    "    \n",
    "    \"\"\"\n",
    "    if xlabels is None:\n",
    "        xlabels = [0, 0.001, 0.01, 0.05, 0.2, 0.5, 1]\n",
    "    xbreaks = np.power(xlabels, np.log(0.5) / np.log(0.05))\n",
    "    \n",
    "    result_df = data[\"estimate\"]\n",
    "    if ylab is None:\n",
    "        if data[\"fun\"] == \"all_lm\":\n",
    "            ylab = \"Coefficient\"\n",
    "        elif data[\"family\"] == \"poisson\":\n",
    "            ylab = \"Rate ratio\"\n",
    "        elif data[\"family\"] == \"binomial\":\n",
    "            ylab = \"Odds ratio\"\n",
    "        else:\n",
    "            ylab = \"Effect estimates\"\n",
    "    hline = 0 if data[\"fun\"] == \"all_lm\" else 1\n",
    "    \n",
    "    result_df['p_value'] = np.power(result_df['p'], np.log(0.5) / np.log(0.05))\n",
    "    if ylim is None:\n",
    "        if data[\"fun\"] == \"all_lm\":\n",
    "            maxv = max(result_df['estimate'].max(), abs(result_df['estimate'].min()))\n",
    "            ylim = (-maxv, maxv)\n",
    "        else:\n",
    "            maxv = max(max(1 / result_df['estimate']), max(result_df['estimate']))\n",
    "            ylim = (1 / maxv, maxv)\n",
    "\n",
    "    # Create a DataFrame to mark inclusion of variables\n",
    "    mark_df = pd.DataFrame({x: result_df['variables'].str.contains(x).astype(int) for x in data['xlist']})\n",
    "    df_scatter = pd.concat([result_df, mark_df], axis=1)\n",
    "    \n",
    "    # Melt the DataFrame for plotting\n",
    "    df_long = df_scatter.melt(id_vars=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\", \"p_value\"], \n",
    "                              value_vars=data['xlist'],\n",
    "                              var_name=\"variable\", value_name=\"inclusion\")\n",
    "    df_long['inclusion'] = df_long['inclusion'].apply(lambda x: \"Included\" if x > 0 else \"Not included\")\n",
    "\n",
    "    # Plotting\n",
    "    g = sns.FacetGrid(df_long, col=\"variable\", hue=\"inclusion\", height=4, sharex=False, sharey=False)\n",
    "    g.map(sns.scatterplot, \"p_value\", \"estimate\")\n",
    "    g.add_legend()\n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_xticks(xbreaks)\n",
    "        ax.set_xticklabels(xlabels)\n",
    "        if grid:\n",
    "            ax.grid(True)\n",
    "    g.set_axis_labels(xlab, ylab)\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    if title:\n",
    "        plt.subplots_adjust(top=0.8)\n",
    "        g.fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests for fit_all_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           y        x1        x2        x3\n",
      "0   6.160434  0.374540  0.031429  0.642032\n",
      "1   8.782900  0.950714  0.636410  0.084140\n",
      "2   8.711113  0.731994  0.314356  0.161629\n",
      "3  18.088468  0.598658  0.508571  0.898554\n",
      "4  10.250555  0.156019  0.907566  0.606429\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_data(n_samples=100, b0=1, b1=2, b2=3, b3=4, b12=5, b13=6, b23=7, b123=8, noise_std=1):\n",
    "    \"\"\"\n",
    "    Generate data for the linear formula y ~ b0 + b1*x1 + b2*x2 + b3*x3 + b12*x1*x2 + b13*x1*x3 + b23*x2*x3 + b123*x1*x2*x3.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): Number of samples to generate.\n",
    "        b0 (float): Coefficient for the intercept.\n",
    "        b1 (float): Coefficient for x1.\n",
    "        b2 (float): Coefficient for x2.\n",
    "        b3 (float): Coefficient for x3.\n",
    "        b12 (float): Coefficient for the interaction term x1*x2.\n",
    "        b13 (float): Coefficient for the interaction term x1*x3.\n",
    "        b23 (float): Coefficient for the interaction term x2*x3.\n",
    "        b123 (float): Coefficient for the interaction term x1*x2*x3.\n",
    "        noise_std (float): Standard deviation of the Gaussian noise added to y.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the generated data with columns ['x1', 'x2', 'x3', 'y'].\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    x1 = np.random.uniform(0, 1, n_samples)\n",
    "    x2 = np.random.uniform(0, 1, n_samples)\n",
    "    x3 = np.random.uniform(0, 1, n_samples)\n",
    "    \n",
    "    y = (b0 + b1*x1 + b2*x2 + b3*x3 + b12*x1*x2 + b13*x1*x3 + b23*x2*x3 + b123*x1*x2*x3 +\n",
    "         np.random.normal(0, noise_std, n_samples))\n",
    "    \n",
    "    data = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "data = generate_data()\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.840428</td>\n",
       "      <td>0.374540</td>\n",
       "      <td>0.031429</td>\n",
       "      <td>0.642032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.140514</td>\n",
       "      <td>0.950714</td>\n",
       "      <td>0.636410</td>\n",
       "      <td>0.084140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.492225</td>\n",
       "      <td>0.731994</td>\n",
       "      <td>0.314356</td>\n",
       "      <td>0.161629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.218119</td>\n",
       "      <td>0.598658</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>0.898554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.195167</td>\n",
       "      <td>0.156019</td>\n",
       "      <td>0.907566</td>\n",
       "      <td>0.606429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           y        x1        x2        x3\n",
       "0   6.840428  0.374540  0.031429  0.642032\n",
       "1   1.140514  0.950714  0.636410  0.084140\n",
       "2   4.492225  0.731994  0.314356  0.161629\n",
       "3  10.218119  0.598658  0.508571  0.898554\n",
       "4   4.195167  0.156019  0.907566  0.606429"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = generate_data(b0=0, b1=1, b2=0, b3=10, b12=0, b13=0, b23=0, b123=0, noise_std=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def fit_ols_model(formula, data):\n",
    "    \"\"\"\n",
    "    Fit an OLS model using the given formula and data, and print the results.\n",
    "\n",
    "    Args:\n",
    "        formula (str): The formula for the OLS model.\n",
    "        data (pd.DataFrame): The data frame containing the variables.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the p-values, estimates, confidence intervals, and AIC value.\n",
    "    \"\"\"\n",
    "    mod_0 = smf.ols(formula=formula, data=data).fit()\n",
    "    p = mod_0.pvalues.iloc[1]\n",
    "    estimate = mod_0.params.iloc[1]\n",
    "    conf_int = mod_0.conf_int().iloc[1]\n",
    "    aic_value = mod_0.aic\n",
    "\n",
    "    print(f\"p-values: {p}\")\n",
    "    print(f\"estimate: {estimate}\")\n",
    "    print(f\"conf_int: {conf_int}\")\n",
    "    print(f\"aic: {aic_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-values: 0.34343741859526267\n",
      "estimate: 1.025306391110114\n",
      "conf_int: 0   -1.111963\n",
      "1    3.162575\n",
      "Name: x1, dtype: float64\n",
      "aic: 517.6397392012537\n",
      "p-values: 0.3637511850778461\n",
      "estimate: 0.9810502049698089\n",
      "conf_int: 0   -1.152698\n",
      "1    3.114798\n",
      "Name: x1, dtype: float64\n",
      "aic: 518.1426513151566\n",
      "p-values: 4.9467606744218404e-05\n",
      "estimate: 1.4077923469421165\n",
      "conf_int: 0    0.750106\n",
      "1    2.065479\n",
      "Name: x1, dtype: float64\n",
      "aic: 282.73524524532\n",
      "p-values: 4.849840959643538e-05\n",
      "estimate: 1.4159292625696247\n",
      "conf_int: 0    0.755494\n",
      "1    2.076364\n",
      "Name: x1, dtype: float64\n",
      "aic: 284.34665447613634\n"
     ]
    }
   ],
   "source": [
    "fit_ols_model(\"y ~ x1\", data)\n",
    "fit_ols_model(\"y ~ x1 + x2\", data)\n",
    "fit_ols_model(\"y ~ x1 + x3\", data)\n",
    "fit_ols_model(\"y ~ x1 + x2 + x3\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The basic model is: y ~ x1\n",
      "The following features will be used for fitting the basic model: Index(['x3', 'x2', 'y', 'x1'], dtype='object')\n",
      "p-values: 0.34343741859526267\n",
      "estimate: 1.025306391110114\n",
      "conf_int: 0   -1.111963\n",
      "1    3.162575\n",
      "Name: x1, dtype: float64\n",
      "aic: 517.6397392012537\n",
      "Combinations: [('x2',), ('x3',), ('x2', 'x3')]\n",
      "  variables  estimate  conf_low  conf_high         p         aic    n\n",
      "0     basic  1.025306 -1.111963   3.162575  0.343437  517.639739  100\n",
      "1        x2  0.981050 -1.152698   3.114798  0.363751  518.142651  100\n",
      "2        x3  1.407792  0.750106   2.065479  0.000049  282.735245  100\n",
      "3    x2, x3  1.415929  0.755494   2.076364  0.000048  284.346654  100\n"
     ]
    }
   ],
   "source": [
    "from spotpython.utils.stats import fit_all_lm, plot_coeff_vs_pvals, plot_coeff_vs_pvals_by_included\n",
    "res = fit_all_lm(\"y ~ x1\", [\"x2\", \"x3\"], data)\n",
    "print(res[\"estimate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 937.875x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_coeff_vs_pvals_by_included(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a 5x5 correlation matrix, generate a DataFrame with samples drawn from a multivariate normal distribution with the given correlation matrix. Since the correlation matrix is symmetric, we only need to specify the upper triangular part of the matrix. Write a function that accepts the upper triangular part of the correlation matrix and the number of samples to generate. The function should return a DataFrame with the generated samples.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_samples_from_correlation_matrix(upper_tri, n_samples):\n",
    "    \"\"\"\n",
    "    Generate samples from a multivariate normal distribution with a given correlation matrix.\n",
    "\n",
    "    Args:\n",
    "        upper_tri (list): A list containing the upper triangular part of the 5x5 correlation matrix.\n",
    "        n_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the generated samples.\n",
    "    \"\"\"\n",
    "    # Create an empty 5x5 correlation matrix\n",
    "    corr_matrix = np.zeros((5, 5))\n",
    "\n",
    "    # Fill the upper triangular part of the correlation matrix\n",
    "    upper_tri_indices = np.triu_indices(5, k=1)\n",
    "    corr_matrix[upper_tri_indices] = upper_tri\n",
    "\n",
    "    # Make the correlation matrix symmetric\n",
    "    corr_matrix = corr_matrix + corr_matrix.T\n",
    "\n",
    "    # Set the diagonal elements to 1\n",
    "    np.fill_diagonal(corr_matrix, 1)\n",
    "\n",
    "    # Generate samples from a multivariate normal distribution\n",
    "    mean = np.zeros(5)\n",
    "    samples = np.random.multivariate_normal(mean, corr_matrix, size=n_samples)\n",
    "\n",
    "    # Create a DataFrame with the generated samples\n",
    "    columns = [f'x{i+1}' for i in range(4)]\n",
    "    columns = ['y'] + columns\n",
    "    df_samples = pd.DataFrame(samples, columns=columns)\n",
    "\n",
    "    return df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          y        x1        x2        x3        x4\n",
      "0  0.044997 -0.003430  0.444511  0.872654  0.143217\n",
      "1  2.029283  1.223798 -0.845555  0.575638  1.311293\n",
      "2 -1.767282 -0.529397  1.343997 -1.022553 -2.252436\n",
      "3  0.412404  0.086523  0.422435  0.372841  0.592150\n",
      "4  0.490212 -0.722110  0.030818  2.308572  1.759333\n",
      "5  0.638286  0.583984  1.189120  1.164792  1.038973\n",
      "6  1.113330 -0.190182 -0.045483  1.716791  0.850343\n",
      "7 -0.355967 -0.822201  0.225335 -0.316512  0.064009\n",
      "8 -0.024064  0.777673  0.385850 -1.738148 -0.824541\n",
      "9 -0.675368 -1.194853 -0.495401  0.091217  0.440580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dw/pvtj6mt91znd0hftcztqb0k00000gn/T/ipykernel_17783/3929617165.py:31: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  samples = np.random.multivariate_normal(mean, corr_matrix, size=n_samples)\n"
     ]
    }
   ],
   "source": [
    "upper_tri = [1, 0.5, 0.5, 0.5,\n",
    "                .5, .0, .0,\n",
    "                    .0, .0,\n",
    "                        .5]\n",
    "n_samples = 10\n",
    "df_samples = generate_samples_from_correlation_matrix(upper_tri, n_samples)\n",
    "print(df_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The basic model is: y ~ x1\n",
      "The following features will be used for fitting the basic model: Index(['y', 'x3', 'x2', 'x1', 'x4'], dtype='object')\n",
      "p-values: 0.050556345571168976\n",
      "estimate: 0.8429665769634175\n",
      "conf_int: 0   -0.002599\n",
      "1    1.688532\n",
      "Name: x1, dtype: float64\n",
      "aic: 26.815750239627118\n",
      "Combinations: [('x2',), ('x3',), ('x4',), ('x2', 'x3'), ('x2', 'x4'), ('x3', 'x4'), ('x2', 'x3', 'x4')]\n",
      "    variables  estimate  conf_low  conf_high         p        aic   n\n",
      "0       basic  0.842967 -0.002599   1.688532  0.050556  26.815750  10\n",
      "1          x2  0.825363  0.156607   1.494120  0.022392  22.280665  10\n",
      "2          x3  0.934877  0.387128   1.482625  0.004959  18.175378  10\n",
      "3          x4  0.716014  0.357954   1.074075  0.002136   9.618227  10\n",
      "4      x2, x3  0.906808  0.599447   1.214169  0.000358   6.365858  10\n",
      "5      x2, x4  0.729113  0.391717   1.066510  0.001851   8.167146  10\n",
      "6      x3, x4  0.740879  0.314010   1.167748  0.005399  11.381842  10\n",
      "7  x2, x3, x4  0.831241  0.481187   1.181296  0.001709   5.797141  10\n"
     ]
    }
   ],
   "source": [
    "from spotpython.utils.stats import fit_all_lm, plot_coeff_vs_pvals, plot_coeff_vs_pvals_by_included\n",
    "res = fit_all_lm(\"y ~ x1\", [\"x2\", \"x3\", \"x4\"], df_samples)\n",
    "print(res[\"estimate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 937.875x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_coeff_vs_pvals_by_included(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spot312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
