{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "title: 'spotpython Tests'\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# fun_control_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init\n",
        "fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n",
        "fun_control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def class_attributes_to_dataframe(class_obj):\n",
        "    # Get the attributes and their values of the class object\n",
        "    attributes = [attr for attr in dir(class_obj) if not callable(getattr(class_obj, attr)) and not attr.startswith(\"__\")]\n",
        "    values = [getattr(class_obj, attr) for attr in attributes]\n",
        "    \n",
        "    # Create a DataFrame from the attributes and values\n",
        "    df = pd.DataFrame({'Attribute Name': attributes, 'Attribute Value': values})\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "class MyClass:\n",
        "    def __init__(self):\n",
        "        self.name = \"John\"\n",
        "        self.age = 30\n",
        "        self.salary = 50000\n",
        "\n",
        "my_instance = MyClass()\n",
        "df = class_attributes_to_dataframe(my_instance)\n",
        "print(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import inf\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "from spotpython.spot import spot\n",
        "# number of initial points:\n",
        "ni = 7\n",
        "# number of points\n",
        "n = 10\n",
        "\n",
        "fun = analytical().fun_sphere\n",
        "lower = np.array([-1])\n",
        "upper = np.array([1])\n",
        "design_control={\"init_size\": ni}\n",
        "\n",
        "spot_1 = spot.Spot(fun=fun,\n",
        "            lower = lower,\n",
        "            upper= upper,\n",
        "            fun_evals = n,\n",
        "            show_progress=True,\n",
        "            design_control=design_control,)\n",
        "spot_1.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sys import stdout\n",
        "df = spot_1.class_attributes_to_dataframe()\n",
        "stdout.write(df.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from river import datasets\n",
        "from river import evaluate\n",
        "from river.linear_model import LogisticRegression\n",
        "from river import metrics\n",
        "from river import optim\n",
        "from river import preprocessing\n",
        "\n",
        "dataset = datasets.Phishing()\n",
        "\n",
        "model = (\n",
        "    preprocessing.StandardScaler() |\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "metric = metrics.Accuracy()\n",
        "\n",
        "evaluate.progressive_val_score(dataset, model, metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.csvdataset import CSVDataset\n",
        "# dataset = CSVDataset(csv_file='./data/spotpython/data.csv', target_column='prognosis')\n",
        "dataset = CSVDataset(target_column='prognosis')\n",
        "print(dataset.data.shape)\n",
        "print(dataset.targets.shape)            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.extra_repr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 3\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSV Data set VBDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the csv_file='./data/spotpython/data.csv' as a pandas df and save it as a pickle file\n",
        "import pandas as pd\n",
        "df = pd.read_csv('./data/spotpython/data.csv')\n",
        "df.to_pickle('./data/spotpython/data.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyHcf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyhcf.data.daten_sensitive import DatenSensitive\n",
        "from pyhcf.utils.names import get_short_parameter_names\n",
        "daten = DatenSensitive()\n",
        "df = daten.load()\n",
        "names =  df.columns\n",
        "names = get_short_parameter_names(names)\n",
        "# rename columns with short names\n",
        "df.columns = names\n",
        "df.head()\n",
        "# save the df as a csv file\n",
        "df.to_csv('./data/spotpython/data_sensitive.csv', index=False)\n",
        "# save the df as a pickle file\n",
        "df.to_pickle('./data/spotpython/data_sensitive.pkl')\n",
        "# remove all rows with NaN values\n",
        "df = df.dropna()\n",
        "# save the df as a csv file\n",
        "df.to_csv('./data/spotpython/data_sensitive_rmNA.csv', index=False)\n",
        "# save the df as a pickle file\n",
        "df.to_pickle('./data/spotpython/data_sensitive_rmNA.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyHcf data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from spotpython.light.csvdataset import CSVDataset\n",
        "# import torch\n",
        "# dataset = CSVDataset(csv_file='./data/spotpython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# # Set batch size for DataLoader\n",
        "# batch_size = 5000\n",
        "# # Create DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Iterate over the data in the DataLoader\n",
        "# for batch in dataloader:\n",
        "#     inputs, targets = batch\n",
        "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
        "#     print(\"---------------\")\n",
        "#     # print(f\"Inputs: {inputs}\")\n",
        "#     print(f\"Targets: {targets}\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from spotpython.light.csvdataset import CSVDataset\n",
        "# import torch\n",
        "# dataset = CSVDataset(csv_file='./data/spotpython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# # Set batch size for DataLoader\n",
        "# batch_size = 5000\n",
        "# # Create DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Iterate over the data in the DataLoader\n",
        "# for batch in dataloader:\n",
        "#     inputs, targets = batch\n",
        "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
        "#     print(\"---------------\")\n",
        "#     # print(f\"Inputs: {inputs}\")\n",
        "#     print(f\"Targets: {targets}\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pickle data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
        "dataset.feature_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Sensitive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.light.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = PKLDataset(pkl_file='./data/spotpython/data_sensitive.pkl', target_column='A', feature_type=torch.long, rmNA=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# # Set batch size for DataLoader\n",
        "# batch_size = 5\n",
        "# # Create DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Iterate over the data in the DataLoader\n",
        "# for batch in dataloader:\n",
        "#     inputs, targets = batch\n",
        "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
        "#     print(\"---------------\")\n",
        "#     print(f\"Inputs: {inputs}\")\n",
        "#     print(f\"Targets: {targets}\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test lightdatamodule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.csvdataset import CSVDataset\n",
        "from spotpython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "# dataset = PKLDataset(directory=\"./data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_module.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Training set size: {len(data_module.data_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Validation set size: {len(data_module.data_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Test set size: {len(data_module.data_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set the DataModule in fun_control "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.csvdataset import CSVDataset\n",
        "from spotpython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "fun_control = fun_control_init()\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=7)\n",
        "dm.setup()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                key=\"data_module\",\n",
        "                value=dm, replace=True)\n",
        "data_module = fun_control[\"data_module\"]\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## same with the sensitive data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "fun_control = fun_control_init()\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
        "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
        "dm.setup()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                key=\"data_module\",\n",
        "                value=dm, replace=True)\n",
        "data_module = fun_control[\"data_module\"]\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## same, but VBDO data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "fun_control = fun_control_init()\n",
        "dataset = CSVDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/VBDP/\", filename=\"train.csv\",target_column='prognosis', feature_type=torch.long)\n",
        "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
        "dm.setup()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                key=\"data_module\",\n",
        "                value=dm, replace=True)\n",
        "data_module = fun_control[\"data_module\"]\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# load Hyperdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "lhd = LightHyperDict()\n",
        "lhd.hyper_dict\n",
        "user_lhd = LightHyperDict(filename=\"user_hyper_dict.json\", directory=\"./hyperdict/\")\n",
        "user_lhd.hyper_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diabetes data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes  \n",
        "import torch\n",
        "\n",
        "# Load the diabetes dataset\n",
        "feature_df, target_df = load_diabetes(return_X_y=True, as_frame=True)\n",
        "feature_tensor = torch.tensor(feature_df.values, dtype=torch.float32)\n",
        "target_tensor = torch.tensor(target_df.values, dtype=torch.float32)\n",
        "feature_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.diabetes import Diabetes\n",
        "dataset = Diabetes()\n",
        "print(dataset.data.shape)\n",
        "print(dataset.targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# add core model to fun control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.light.netlightregressione import NetLightRegression\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "fun_control[\"core_model\"].__name__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if the fun_control[\"core_model_hyper_dict\"] is a LightHyperDict\n",
        "isinstance(fun_control[\"core_model_hyper_dict\"], dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test check_X_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.light.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.hyperparameters.values import get_var_name\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "hyper_light = HyperLight(seed=126, log_level=50)\n",
        "n_hyperparams = len(get_var_name(fun_control))\n",
        "# generate a random np.array X with shape (2, n_hyperparams)\n",
        "X = np.random.rand(2, n_hyperparams)\n",
        "X == hyper_light.check_X_shape(X, fun_control)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test hyperlight fun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.light.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "import numpy as np\n",
        "fun_control = fun_control_init(\n",
        "    _L_in=10,\n",
        "    _L_out=1,)\n",
        "\n",
        "dataset = Diabetes()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                    key=\"data_set\",\n",
        "                    value=dataset)\n",
        "\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "hyper_light = HyperLight(seed=126, log_level=50)\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
        "X = np.vstack((X, X))\n",
        "y = hyper_light.fun(X, fun_control)\n",
        "y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test  NetLightRegression Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.light.netlightregression import NetLightRegression\n",
        "from torch import nn\n",
        "import lightning as L\n",
        "PATH_DATASETS = './data'\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "dataset = Diabetes()\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "batch_x, batch_y = next(iter(train_loader)) \n",
        "print(batch_x.shape)\n",
        "print(batch_y.shape)\n",
        "\n",
        "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
        "                                initialization='xavier', act_fn=nn.ReLU(),\n",
        "                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n",
        "                                patience=5, _L_in=10, _L_out=1)\n",
        "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
        "trainer.fit(net_light_base, train_loader)\n",
        "trainer.validate(net_light_base, val_loader)\n",
        "trainer.test(net_light_base, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# tests optimizer_handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.light.netlightregression import NetLightRegression\n",
        "from torch import nn\n",
        "import lightning as L\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "lr_mult=0.1\n",
        "\n",
        "dataset = Diabetes()\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
        "                                initialization='xavier', act_fn=nn.ReLU(),\n",
        "                                optimizer='Adam', dropout_prob=0.1, lr_mult=lr_mult,\n",
        "                                patience=5, _L_in=10, _L_out=1)\n",
        "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
        "trainer.fit(net_light_base, train_loader)\n",
        "# Adam uses a lr which is calculated as lr=lr_mult * 0.001, so this value\n",
        "# should be 0.1 * 0.001 = 0.0001 \n",
        "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*0.001\n",
        "\n",
        "\n",
        "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
        "                                initialization='xavier', act_fn=nn.ReLU(),\n",
        "                                optimizer='Adadelta', dropout_prob=0.1, lr_mult=lr_mult,\n",
        "                                patience=5, _L_in=10, _L_out=1)\n",
        "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
        "trainer.fit(net_light_base, train_loader)\n",
        "# Adadelta uses a lr which is calculated as lr=lr_mult * 1.0, so this value\n",
        "# should be 1.0 * 0.1 = 0.1 \n",
        "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test train_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.light.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "from spotpython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
        "from spotpython.light.traintest import train_model, test_model\n",
        "fun_control = fun_control_init(\n",
        "    _L_in=10,\n",
        "    _L_out=1,)\n",
        "\n",
        "dataset = Diabetes()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset)\n",
        "\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "var_dict = assign_values(X, get_var_name(fun_control))\n",
        "for config in generate_one_config_from_var_dict(var_dict, fun_control):\n",
        "    y_train = train_model(config, fun_control)\n",
        "    y_test = test_model(config, fun_control)\n",
        "    break\n",
        "print(y_train)\n",
        "print(y_test[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.light.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "from spotpython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
        "from spotpython.light.traintest import test_model\n",
        "\n",
        "\n",
        "def test_traintest_test_model():\n",
        "    fun_control = fun_control_init(\n",
        "        _L_in=10,\n",
        "        _L_out=1,)\n",
        "\n",
        "    dataset = Diabetes()\n",
        "    set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset)\n",
        "\n",
        "    add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                                fun_control=fun_control,\n",
        "                                hyper_dict=LightHyperDict)\n",
        "    X = get_default_hyperparameters_as_array(fun_control)\n",
        "    var_dict = assign_values(X, get_var_name(fun_control))\n",
        "    for vals in generate_one_config_from_var_dict(var_dict, fun_control):\n",
        "        y_test = test_model(test_config=vals,\n",
        "                            fun_control=fun_control)\n",
        "        break\n",
        "    # check if y is a float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test getVarName()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.hyperparameters.values import get_var_name\n",
        "fun_control = {\"core_model_hyper_dict\":{\n",
        "            \"leaf_prediction\": {\n",
        "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": \"mean\",\n",
        "                \"core_model_parameter_type\": \"str\"},\n",
        "            \"leaf_model\": {\n",
        "                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": \"LinearRegression\",\n",
        "                \"core_model_parameter_type\": \"instance\"},\n",
        "            \"splitter\": {\n",
        "                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": \"EBSTSplitter\",\n",
        "                \"core_model_parameter_type\": \"instance()\"},\n",
        "            \"binary_split\": {\n",
        "                \"levels\": [0, 1],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": 0,\n",
        "                \"core_model_parameter_type\": \"bool\"},\n",
        "            \"stop_mem_management\": {\n",
        "                \"levels\": [0, 1],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": 0,\n",
        "                \"core_model_parameter_type\": \"bool\"}}}\n",
        "len(get_var_name(fun_control))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test netlightregression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from spotpython.spot import spot\n",
        "from math import inf\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
        "from spotpython.utils.device import getDevice\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "from spotpython.data.pkldataset import PKLDataset\n",
        "from spotpython.light.regression.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotpython.hyperparameters.values import modify_hyper_parameter_bounds\n",
        "from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.hyperparameters.values import (get_bound_values,\n",
        "    get_var_name,\n",
        "    get_var_type,)\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "from spotpython.hyperparameters.values import get_tuned_architecture\n",
        "from spotpython.light.testmodel import test_model\n",
        "from spotpython.light.loadmodel import load_light_from_checkpoint\n",
        "\n",
        "MAX_TIME = 1\n",
        "INIT_SIZE = 5\n",
        "WORKERS = 0\n",
        "PREFIX=\"031\"\n",
        "\n",
        "experiment_name = get_experiment_name(prefix=PREFIX)\n",
        "fun_control = fun_control_init(\n",
        "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
        "    num_workers=WORKERS,\n",
        "    device=getDevice(),\n",
        "    _L_in=133,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True)\n",
        "\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset)\n",
        "\n",
        "\n",
        "\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[5,8])\n",
        "modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[3,5])\n",
        "modify_hyper_parameter_bounds(fun_control, \"batch_size\", bounds=[2, 8])\n",
        "modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
        "\n",
        "print(gen_design_table(fun_control))\n",
        "\n",
        "var_type = get_var_type(fun_control)\n",
        "var_name = get_var_name(fun_control)\n",
        "lower = get_bound_values(fun_control, \"lower\")\n",
        "upper = get_bound_values(fun_control, \"upper\")\n",
        "fun = HyperLight(log_level=50).fun\n",
        "spot_tuner = spot.Spot(fun=fun,\n",
        "                       log_level=50,\n",
        "                   lower = lower,\n",
        "                   upper = upper,\n",
        "                   fun_evals = inf,\n",
        "                   max_time = MAX_TIME,\n",
        "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                   var_type = var_type,\n",
        "                   var_name = var_name,\n",
        "                   show_progress= True,\n",
        "                   fun_control = fun_control,\n",
        "                   design_control={\"init_size\": INIT_SIZE},\n",
        "                   surrogate_control={\"noise\": True,\n",
        "                                      \"min_theta\": -4,\n",
        "                                      \"max_theta\": 3,\n",
        "                                      \"n_theta\": len(var_name),\n",
        "                                      \"model_fun_evals\": 10_000,\n",
        "                                      })\n",
        "spot_tuner.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_progress(log_y=False, filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_importance(threshold=0.025, filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_tuned_architecture(spot_tuner, fun_control)\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_model(config, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_loaded = load_light_from_checkpoint(config, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_important_hyperparameter_contour(filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.parallel_plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.light.cvmodel import cv_model\n",
        "# set the number of folds to 10\n",
        "fun_control[\"k_folds\"] = 10\n",
        "cv_model(config, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# fun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.fun.objectivefunctions import analytical\n",
        "import numpy as np\n",
        "y = np.array([1, 2, 3, 4, 5])\n",
        "fun = analytical(sigma=1.0, seed=123)\n",
        "fun.add_noise(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.fun.objectivefunctions import analytical\n",
        "import numpy as np\n",
        "print(np.array([1, 2, 3, 4, 5]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import inf\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "from spotpython.spot import spot\n",
        "from scipy.optimize import shgo\n",
        "from scipy.optimize import direct\n",
        "from scipy.optimize import differential_evolution\n",
        "import matplotlib.pyplot as plt\n",
        "from spotpython.utils.init import fun_control_init\n",
        "fun_control = fun_control_init(seed=4321, sigma=0.1)\n",
        "fun = analytical(seed=222, sigma=0.0).fun_sphere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_1 = spot.Spot(fun=fun,\n",
        "                   lower = np.array([-10]),\n",
        "                   upper = np.array([100]),\n",
        "                   fun_evals = 100,\n",
        "                   fun_repeats = 3,\n",
        "                   max_time = inf,\n",
        "                   noise = True,\n",
        "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                   var_type=[\"num\"],\n",
        "                   infill_criterion = \"y\",\n",
        "                   n_points = 1,\n",
        "                   seed=111,\n",
        "                   log_level = 10,\n",
        "                   show_models=False,\n",
        "                   fun_control = fun_control,\n",
        "                   design_control={\"init_size\": 5,\n",
        "                                   \"repeats\": 1},\n",
        "                   surrogate_control={\"noise\": True,\n",
        "                                      \"cod_type\": \"norm\",\n",
        "                                      \"min_theta\": -4,\n",
        "                                      \"max_theta\": 3,\n",
        "                                      \"n_theta\": 1,\n",
        "                                      \"model_optimizer\": differential_evolution,\n",
        "                                      \"model_fun_evals\": 1000,\n",
        "                                      })\n",
        "spot_1.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def squared_euclidean_distance(X_0, X, theta):\n",
        "    return np.sum(theta*(X_0 - X)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
        "from numpy.linalg import cholesky, solve\n",
        "from numpy.random import multivariate_normal\n",
        "def build_Psi(X, theta):\n",
        "    n = X.shape[0]\n",
        "    k = X.shape[1]\n",
        "    D = zeros((k, n, n))\n",
        "    for l in range(k):\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
        "    D = sum(D)\n",
        "    D = D + D.T\n",
        "    return exp(-D)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "theta = np.array([1.0, 1.0])\n",
        "X = np.array([[1.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n",
        "print(X.shape)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "build_Psi(X, theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.fun.objectivefunctions import analytical\n",
        "import numpy as np\n",
        "X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n",
        "fun = analytical()\n",
        "fun.fun_branin_factor(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "pi = np.pi\n",
        "X = np.array([[0,0], [-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
        "fun = analytical()\n",
        "fun.fun_branin(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.fun.objectivefunctions import analytical\n",
        "import numpy as np\n",
        "pi = np.pi\n",
        "X_0 = np.array([[0, 0]])\n",
        "X_1 = np.array([[-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
        "X_2 = np.array([[0,0,0], [0,0,1], [0,0,2]])\n",
        "fun = analytical()\n",
        "y_0 = fun.fun_branin(X_0)\n",
        "y_1 = fun.fun_branin(X_1)\n",
        "y_2 = fun.fun_branin_factor(X_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "round(y_1[0], 2) == round(y_1[1],2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "round(y_1[0], 2) == round(y_1[2],2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_2[0] == y_0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_2[1] == y_0 + 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_2[2] == y_0 - 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy.random import multivariate_normal\n",
        "import numpy as np\n",
        "n = 100\n",
        "X = np.linspace(0, 10, n, endpoint=False).reshape(-1,1)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
        "from numpy.linalg import cholesky, solve\n",
        "from numpy.random import multivariate_normal\n",
        "def build_Psi(X, theta):\n",
        "    n = X.shape[0]\n",
        "    k = X.shape[1]\n",
        "    D = zeros((k, n, n))\n",
        "    for l in range(k):\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
        "    D = sum(D)\n",
        "    D = D + D.T\n",
        "    return exp(-D)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "theta = np.array([1.0])\n",
        "Psi = build_Psi(X, theta)\n",
        "np.round(Psi[:3,:], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = (3, 1, 1), check_valid=\"raise\")\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert Y to a 3 x 100 array\n",
        "Y = np.squeeze(Y)\n",
        "Y.shape\n",
        "Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot 3 samples from the GP as a function of X\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X, Y.T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = 3, check_valid=\"raise\")\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot 3 samples from the GP as a function of X\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X, Y.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(f\"Inputs Shape: {inputs.shape}\")\n",
        "    print(f\"Targets Shape: {targets.shape}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test HyperLight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.light.regression.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.hyperparameters.values import get_var_name\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                            fun_control=fun_control,\n",
        "                            hyper_dict=LightHyperDict)\n",
        "hyper_light = HyperLight(seed=126, log_level=50)\n",
        "n_hyperparams = len(get_var_name(fun_control))\n",
        "# generate a random np.array X with shape (2, n_hyperparams)\n",
        "X = np.random.rand(2, n_hyperparams)\n",
        "X == hyper_light.check_X_shape(X, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.light.regression.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n",
        "    get_default_hyperparameters_as_array)\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "import numpy as np\n",
        "fun_control = fun_control_init(\n",
        "    _L_in=10,\n",
        "    _L_out=1,)\n",
        "dataset = Diabetes()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset)\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                            fun_control=fun_control,\n",
        "                            hyper_dict=LightHyperDict)\n",
        "hyper_light = HyperLight(seed=126, log_level=50)\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
        "# so that two values are returned\n",
        "X = np.vstack((X, X))\n",
        "hyper_light.fun(X, fun_control)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## test pkldataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\",\n",
        "                    filename=\"data_sensitive.pkl\",\n",
        "                    target_column='N',\n",
        "                    feature_type=torch.float32,\n",
        "                    target_type=torch.float32,\n",
        "                    rmNA=True)\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytest\n",
        "import numpy as np\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperparameters.values import get_bound_values\n",
        "from spotpython.hyperparameters.values import get_control_key_value, set_control_key_value\n",
        "from spotpython.hyperparameters.values import get_var_type_from_var_name\n",
        "\n",
        "fun_control = fun_control_init()\n",
        "set_control_key_value(control_dict=fun_control, key=\"var_type\", value=[\"int\", \"float\", \"str\"], replace=True)\n",
        "set_control_key_value(control_dict=fun_control, key=\"var_name\", value=[\"max_depth\", \"learning_rate\", \"model_type\"], replace=True)\n",
        "\n",
        "print(fun_control)\n",
        "\n",
        "# Test with existing var_name\n",
        "assert get_var_type_from_var_name(var_name=\"max_depth\", fun_control=fun_control) == \"int\"\n",
        "assert get_var_type_from_var_name(var_name=\"learning_rate\", fun_control=fun_control) == \"float\"\n",
        "assert get_var_type_from_var_name(var_name=\"model_type\", fun_control=fun_control) == \"str\"\n",
        "\n",
        "# Test with non-existing var_name\n",
        "with pytest.raises(ValueError):\n",
        "    get_var_type_from_var_name(var_name=\"non_existing\", fun_control=fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperparameters.values import get_control_key_value\n",
        "from spotpython.light.regression.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotpython.hyperparameters.values import get_var_type_from_var_name\n",
        "\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                    fun_control=fun_control,\n",
        "                    hyper_dict=LightHyperDict)\n",
        "var_type = get_control_key_value(control_dict=fun_control, key=\"var_type\")\n",
        "var_name = get_control_key_value(control_dict=fun_control, key=\"var_name\")\n",
        "print(var_type)\n",
        "print(var_name)\n",
        "vn = \"l1\"\n",
        "get_var_type_from_var_name(fun_control=fun_control, var_name=vn)\n",
        "\n",
        "assert var_type[var_name.index(vn)] == \"int\"\n",
        "assert get_var_type_from_var_name(fun_control, vn) == \"int\"\n",
        "vn = \"initialization\"\n",
        "assert var_type[var_name.index(vn)] == \"factor\"\n",
        "assert var_type[var_name.index(vn)] == \"factor\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperparameters.values import get_control_key_value\n",
        "from spotpython.light.regression.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
        "\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                    fun_control=fun_control,\n",
        "                    hyper_dict=LightHyperDict)\n",
        "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"l1\", value=[1,7])\n",
        "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"initialization\", value=[\"xavier2\", \"kaiming2\"])\n",
        "print(fun_control)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## get names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_entry(dictionary, key, i):\n",
        "    if key in dictionary:\n",
        "        if 'levels' in dictionary[key]:\n",
        "            if i < len(dictionary[key]['levels']):\n",
        "                return dictionary[key]['levels'][i]\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from spotpython.data.pkldataset_intern import PKLDataset\n",
        "from spotpython.utils.device import getDevice\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
        "import numpy as np\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.light.regression.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "from math import inf\n",
        "\n",
        "MAX_TIME = 60\n",
        "FUN_EVALS = inf\n",
        "INIT_SIZE = 25\n",
        "WORKERS = 0\n",
        "PREFIX=\"031\"\n",
        "DEVICE = getDevice()\n",
        "\n",
        "\n",
        "experiment_name = get_experiment_name(prefix=PREFIX)\n",
        "fun_control = fun_control_init(\n",
        "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    device=DEVICE,\n",
        "    enable_progress_bar=False,\n",
        "    fun_evals=FUN_EVALS,\n",
        "    log_level=10,\n",
        "    max_time=MAX_TIME,\n",
        "    num_workers=WORKERS,\n",
        "    show_progress=True,\n",
        "    tolerance_x=np.sqrt(np.spacing(1)),\n",
        "    )\n",
        "\n",
        "dataset = Diabetes()\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True, rmMF=True)\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset,\n",
        "                        replace=True)\n",
        "\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"_L_in\",\n",
        "                        value=133,\n",
        "                        replace=True)\n",
        "\n",
        "\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "# from spotpython.hyperparameters.values import modify_hyper_parameter_bounds\n",
        "\n",
        "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
        "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
        "set_control_hyperparameter_value(fun_control, \"epochs\", [4,9])\n",
        "set_control_hyperparameter_value(fun_control, \"batch_size\", [1, 4])\n",
        "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fun_control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_entry(dictionary, key, i):\n",
        "    if 'core_model_hyper_dict' in dictionary:\n",
        "        if key in dictionary['core_model_hyper_dict']:\n",
        "            if 'levels' in dictionary['core_model_hyper_dict'][key]:\n",
        "                if i < len(dictionary['core_model_hyper_dict'][key]['levels']):\n",
        "                    return dictionary['core_model_hyper_dict'][key]['levels'][i]\n",
        "    return None\n",
        "print(get_entry(fun_control, \"optimizer\", 0)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.device import getDevice\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
        "import numpy as np\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.light.regression.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
        "experiment_name = get_experiment_name(prefix=\"000\")\n",
        "fun_control = fun_control_init(\n",
        "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    device=getDevice(),\n",
        "    enable_progress_bar=False,\n",
        "    fun_evals=15,\n",
        "    log_level=10,\n",
        "    max_time=1,\n",
        "    num_workers=0,\n",
        "    show_progress=True,\n",
        "    tolerance_x=np.sqrt(np.spacing(1)),\n",
        "    )\n",
        "dataset = Diabetes()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset,\n",
        "                        replace=True)\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                            fun_control=fun_control,\n",
        "                            hyper_dict=LightHyperDict)\n",
        "\n",
        "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
        "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
        "assert get_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0) == \"Adam\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def get_timestamp(only_int=True):\n",
        "    dt = datetime.datetime.now().isoformat(sep=\" \", timespec=\"microseconds\")\n",
        "    if only_int:\n",
        "        # remove - . : and space\n",
        "        dt = dt.replace(\"-\", \"\")\n",
        "        dt = dt.replace(\".\", \"\")\n",
        "        dt = dt.replace(\":\", \"\")\n",
        "        dt = dt.replace(\" \", \"\")\n",
        "    return dt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytest\n",
        "import numpy as np\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "from spotpython.spot import spot\n",
        "from spotpython.utils.init import (\n",
        "    fun_control_init, surrogate_control_init, design_control_init\n",
        ")\n",
        "\n",
        "def test_plot_progress():\n",
        "    # number of initial points:\n",
        "    ni = 7\n",
        "    # number of points\n",
        "    fun_evals = 10\n",
        "    fun = analytical().fun_sphere\n",
        "    fun_control = fun_control_init(\n",
        "        lower = np.array([-1, -1]),\n",
        "        upper = np.array([1, 1]),\n",
        "        fun_evals=fun_evals,\n",
        "        tolerance_x = np.sqrt(np.spacing(1))\n",
        "    )\n",
        "    design_control=design_control_init(init_size=ni)\n",
        "    surrogate_control=surrogate_control_init(n_theta=3)\n",
        "    S = spot.Spot(fun=fun,\n",
        "                    fun_control=fun_control,\n",
        "                    design_control=design_control,\n",
        "                    surrogate_control=surrogate_control,)\n",
        "    S.run()\n",
        "\n",
        "    # Test plot_progress with different parameters\n",
        "    S.plot_progress(show=False)  # Test with show=False\n",
        "    S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
        "    S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
        "    S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename\n",
        "    # add NaN to S.y at position 2\n",
        "    S.y[2] = np.nan\n",
        "    S.plot_progress(show=False)  # Test with show=False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytest\n",
        "import numpy as np\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "from spotpython.spot import spot\n",
        "from spotpython.utils.init import (\n",
        "    fun_control_init, surrogate_control_init, design_control_init\n",
        ")\n",
        "\n",
        "\n",
        "# number of initial points:\n",
        "ni = 7\n",
        "# number of points\n",
        "fun_evals = 10\n",
        "fun = analytical().fun_sphere\n",
        "fun_control = fun_control_init(\n",
        "    lower = np.array([-1, -1]),\n",
        "    upper = np.array([1, 1]),\n",
        "    fun_evals=fun_evals,\n",
        "    tolerance_x = np.sqrt(np.spacing(1))\n",
        ")\n",
        "design_control=design_control_init(init_size=ni)\n",
        "surrogate_control=surrogate_control_init(n_theta=3)\n",
        "S = spot.Spot(fun=fun,\n",
        "                fun_control=fun_control,\n",
        "                design_control=design_control,\n",
        "                surrogate_control=surrogate_control,)\n",
        "S.run()\n",
        "\n",
        "# remove points from S.y so that there are less than ni points\n",
        "S.y = S.y[:3]\n",
        "# Test plot_progress with different parameters\n",
        "S.plot_progress(show=False)  # Test with show=False\n",
        "S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
        "S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
        "S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import inf\n",
        "from spotpython.spot import spot\n",
        "from scipy.optimize import differential_evolution\n",
        "from spotpython.utils.init import (\n",
        "    fun_control_init,\n",
        "    design_control_init,\n",
        "    surrogate_control_init,\n",
        "    optimizer_control_init)\n",
        "def objective_function(X, fun_control=None):\n",
        "    if not isinstance(X, np.ndarray):\n",
        "        X = np.array(X)\n",
        "    if X.shape[1] != 2:\n",
        "        raise Exception\n",
        "    x0 = X[:, 0]\n",
        "    x1 = X[:, 1]\n",
        "    y = x0**2 + 10*x1**2\n",
        "    return y\n",
        "fun_control = fun_control_init(\n",
        "            lower = np.array([0, 0]),\n",
        "            upper = np.array([10, 10]),\n",
        "            fun_evals=8,\n",
        "            fun_repeats=1,\n",
        "            max_time=inf,\n",
        "            noise=True,\n",
        "            tolerance_x=0,\n",
        "            ocba_delta=0,\n",
        "            var_type=[\"num\", \"num\"],\n",
        "            infill_criterion=\"ei\",\n",
        "            n_points=1,\n",
        "            seed=123,\n",
        "            log_level=10,\n",
        "            show_models=False,\n",
        "            show_progress=True)\n",
        "design_control = design_control_init(\n",
        "            init_size=5,\n",
        "            repeats=1)\n",
        "surrogate_control = surrogate_control_init(\n",
        "            log_level=10,\n",
        "            model_optimizer=differential_evolution,\n",
        "            model_fun_evals=10000,\n",
        "            min_theta=-3,\n",
        "            max_theta=3,\n",
        "            n_theta=2,\n",
        "            theta_init_zero=True,\n",
        "            n_p=1,\n",
        "            optim_p=False,\n",
        "            noise=True,\n",
        "            var_type=[\"num\", \"num\"],\n",
        "            seed=124, \n",
        "            min_Lambda=1,\n",
        "            max_Lambda=10)\n",
        "optimizer_control = optimizer_control_init(\n",
        "            max_iter=1000,\n",
        "            seed=125)\n",
        "spot = spot.Spot(fun=objective_function,\n",
        "            fun_control=fun_control,\n",
        "            design_control=design_control,\n",
        "            surrogate_control=surrogate_control,\n",
        "            optimizer_control=optimizer_control\n",
        "            )\n",
        "spot.run()\n",
        "spot.plot_progress()\n",
        "spot.plot_contour(i=0, j=1)\n",
        "spot.plot_importance()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotpython.spot import spot\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n",
        "\n",
        "fun = analytical().fun_branin\n",
        "fun_control = fun_control_init(lower = np.array([-5, 0]),\n",
        "                               upper = np.array([10, 15]),\n",
        "                               fun_evals=20)\n",
        "design_control = design_control_init(init_size=10)\n",
        "surrogate_control = surrogate_control_init(n_theta=2)\n",
        "S = spot.Spot(fun=fun, fun_control=fun_control, design_control=design_control)\n",
        "S.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "S.print_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "S.plot_progress(log_y=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "S.surrogate.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import inf\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "from spotpython.spot import spot\n",
        "from scipy.optimize import shgo\n",
        "from scipy.optimize import direct\n",
        "from scipy.optimize import differential_evolution\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fun = analytical().fun_sphere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.linspace(-1,1,100).reshape(-1,1)\n",
        "y = fun(x)\n",
        "plt.figure()\n",
        "plt.plot(x,y, \"k\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
        "spot_1 = spot.Spot(fun=fun,\n",
        "                   fun_control=fun_control_init(\n",
        "                        lower = np.array([-10]),\n",
        "                        upper = np.array([100]),\n",
        "                        fun_evals = 7,\n",
        "                        fun_repeats = 1,\n",
        "                        max_time = inf,\n",
        "                        noise = False,\n",
        "                        tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                        var_type=[\"num\"],\n",
        "                        infill_criterion = \"y\",\n",
        "                        n_points = 1,\n",
        "                        seed=123,\n",
        "                        log_level = 50),\n",
        "                   design_control=design_control_init(\n",
        "                        init_size=5,\n",
        "                        repeats=1),\n",
        "                   surrogate_control=surrogate_control_init(\n",
        "                        noise=False,\n",
        "                        min_theta=-4,\n",
        "                        max_theta=3,\n",
        "                        n_theta=1,\n",
        "                        model_optimizer=differential_evolution,\n",
        "                        model_fun_evals=10000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_1.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.fun.objectivefunctions import analytical\n",
        "fun = analytical().fun_sphere\n",
        "from spotpython.design.spacefilling import spacefilling\n",
        "design = spacefilling(2)\n",
        "from scipy.optimize import differential_evolution\n",
        "optimizer = differential_evolution\n",
        "from spotpython.build.kriging import Kriging\n",
        "surrogate = Kriging()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\n",
        "fun_control=fun_control_init(lower=np.array([-1, -1]),\n",
        "                            upper=np.array([1, 1]))\n",
        "design_control=design_control_init()\n",
        "optimizer_control=optimizer_control_init()\n",
        "surrogate_control=surrogate_control_init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.spot import spot\n",
        "spot_tuner = spot.Spot(fun=fun,\n",
        "                       fun_control=fun_control,\n",
        "                       design_control=design_control,\n",
        "                       optimizer_control=optimizer_control,\n",
        "                       surrogate_control=surrogate_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pytest\n",
        "import torch\n",
        "from pyhcf.data.loadHcfData import build_df, load_hcf_data\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_list=[\"L\", \"AQ\", \"AS\"]\n",
        "dataset = load_hcf_data(param_list=p_list, target=\"T\",\n",
        "                        rmNA=True, rmMF=True,\n",
        "                        load_all_features=False,\n",
        "                        load_thermo_features=False,\n",
        "                        scale_data=True,\n",
        "                        return_X_y=False)\n",
        "assert isinstance(dataset, torch.utils.data.TensorDataset)\n",
        "assert len(dataset) > 0\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader    \n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    assert inputs.size(0) == batch_size\n",
        "    print(f\"Inputs Shape: {inputs.shape[1]}\")\n",
        "    print(f\"P List: {p_list}\")\n",
        "    print(f\"P List Length: {len(p_list)}\")\n",
        "    # input is p_list + 1 (for target)\n",
        "    # p_list = [\"L\", \"AQ\", \"AS\"] plus target \"N\"\n",
        "    assert inputs.shape[1] + 1 == len(p_list)\n",
        "    print(f\"Targets Shape: {targets.shape[0]}\")\n",
        "    assert targets.shape[0] == batch_size\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "# data.csv is simple csv file with 11 samples\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "data_module.setup()\n",
        "print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "data_module.setup(stage=\"predict\")\n",
        "print(f\"Predict set size: {len(data_module.data_predict)}\")\n",
        "for batch in data_module.predict_dataloader():\n",
        "    inputs, targets = batch\n",
        "    print(f\"inputs: {inputs}\")\n",
        "    print(f\"targets: {targets}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(data_module.data_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_div2_list(n, n_min):\n",
        "    result = []\n",
        "    current = n\n",
        "    while current >= n_min:\n",
        "        result.extend([current] * (n // current))\n",
        "        current = current // 2\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_L_in = 128\n",
        "l1 = \n",
        "\n",
        "n_low = _L_in // 4\n",
        "# ensure that n_high is larger than n_low\n",
        "n_high = max(l1, 2 * n_low)\n",
        "generate_div2_list(n_high, n_low)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.math import generate_div2_list\n",
        "generate_div2_list(64, 63)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.light.transformer.positionalEncoding import PositionalEncoding\n",
        "import torch\n",
        "# number of tensors\n",
        "n = 3\n",
        "# dimension of each tensor\n",
        "k = 32\n",
        "pe = PositionalEncoding(d_model=k, dropout_prob=0, verbose=False)\n",
        "input = torch.zeros(1, n, k)\n",
        "# Generate a tensor of size (1, 10, 4) with values from 1 to 10\n",
        "for i in range(n):\n",
        "    input[0, i, :] = i\n",
        "print(f\"Input shape: {input.shape}\")\n",
        "print(f\"Input: {input}\")\n",
        "output = pe(input)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output: {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.light.transformer.skiplinear import SkipLinear\n",
        "import torch\n",
        "n_in = 2\n",
        "n_out = 4\n",
        "sl = SkipLinear(n_in, n_out)\n",
        "input = torch.zeros(1, n_in)\n",
        "for i in range(n_in):\n",
        "    input[0, i] = i\n",
        "print(f\"Input shape: {input.shape}\")\n",
        "print(f\"Input: {input}\")\n",
        "output = sl(input)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output: {output}\")\n",
        "print(sl.lst_modules)\n",
        "for i in sl.lst_modules:\n",
        "    print(f\"weights: {i.weights}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Example from J. Caffrey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# people_income_transformer.py\n",
        "# predict income from sex, age, city, politics\n",
        "# PyTorch 2.0.0-CPU Anaconda3-2022.10  Python 3.9.13\n",
        "# Windows 10/11 \n",
        "\n",
        "# Transformer component for regression\n",
        "\n",
        "import numpy as np\n",
        "import torch as T\n",
        "\n",
        "device = T.device('cpu')  # apply to Tensor or Module\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class PeopleDataset(T.utils.data.Dataset):\n",
        "  def __init__(self, src_file):\n",
        "    # sex age   state   income   politics\n",
        "    # -1  0.27  0 1 0   0.7610   0 0 1\n",
        "    # +1  0.19  0 0 1   0.6550   1 0 0\n",
        "\n",
        "    tmp_x = np.loadtxt(src_file, usecols=[0,1,2,3,4,6,7,8],\n",
        "      delimiter=\",\", comments=\"#\", dtype=np.float32)\n",
        "    tmp_y = np.loadtxt(src_file, usecols=5, delimiter=\",\",\n",
        "      comments=\"#\", dtype=np.float32)\n",
        "    tmp_y = tmp_y.reshape(-1,1)  # 2D required\n",
        "\n",
        "    self.x_data = T.tensor(tmp_x, dtype=T.float32).to(device)\n",
        "    self.y_data = T.tensor(tmp_y, dtype=T.float32).to(device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    preds = self.x_data[idx]\n",
        "    incom = self.y_data[idx] \n",
        "    return (preds, incom)  # as a tuple\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class SkipLinear(T.nn.Module):\n",
        "\n",
        "  # -----\n",
        "\n",
        "  class Core(T.nn.Module):\n",
        "    def __init__(self, n):\n",
        "      super().__init__()\n",
        "      # 1 node to n nodes, n gte 2\n",
        "      self.weights = T.nn.Parameter(T.zeros((n,1),\n",
        "        dtype=T.float32))\n",
        "      self.biases = T.nn.Parameter(T.tensor(n,\n",
        "        dtype=T.float32))\n",
        "      lim = 0.01\n",
        "      T.nn.init.uniform_(self.weights, -lim, lim)\n",
        "      T.nn.init.zeros_(self.biases)\n",
        "\n",
        "    def forward(self, x):\n",
        "      wx= T.mm(x, self.weights.t())\n",
        "      v = T.add(wx, self.biases)\n",
        "      return v\n",
        "\n",
        "  # -----\n",
        "\n",
        "  def __init__(self, n_in, n_out):\n",
        "    super().__init__()\n",
        "    self.n_in = n_in; self.n_out = n_out\n",
        "    if n_out  % n_in != 0:\n",
        "      print(\"FATAL: n_out must be divisible by n_in\")\n",
        "    n = n_out // n_in  # num nodes per input\n",
        "\n",
        "    self.lst_modules = \\\n",
        "      T.nn.ModuleList([SkipLinear.Core(n) for \\\n",
        "        i in range(n_in)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    lst_nodes = []\n",
        "    for i in range(self.n_in):\n",
        "      xi = x[:,i].reshape(-1,1)\n",
        "      oupt = self.lst_modules[i](xi)\n",
        "      lst_nodes.append(oupt)\n",
        "    result = T.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
        "    for i in range(2,self.n_in):\n",
        "      result = T.cat((result, lst_nodes[i]), 1)\n",
        "    result = result.reshape(-1, self.n_out)\n",
        "    return result\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class PositionalEncoding(T.nn.Module):  # documentation code\n",
        "  def __init__(self, d_model: int, dropout: float=0.1,\n",
        "   max_len: int=5000):\n",
        "    super(PositionalEncoding, self).__init__()  # old syntax\n",
        "    self.dropout = T.nn.Dropout(p=dropout)\n",
        "    pe = T.zeros(max_len, d_model)  # like 10x4\n",
        "    position = \\\n",
        "      T.arange(0, max_len, dtype=T.float).unsqueeze(1)\n",
        "    div_term = T.exp(T.arange(0, d_model, 2).float() * \\\n",
        "      (-np.log(10_000.0) / d_model))\n",
        "    pe[:, 0::2] = T.sin(position * div_term)\n",
        "    pe[:, 1::2] = T.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "    self.register_buffer('pe', pe)  # allows state-save\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0), :]\n",
        "    return self.dropout(x)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class TransformerNet(T.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TransformerNet, self).__init__()\n",
        "    self.embed = SkipLinear(8, 32)  # 8 inputs, each goes to 4 \n",
        "    self.pos_enc = \\\n",
        "      PositionalEncoding(4, dropout=0.20)  # positional\n",
        "    self.enc_layer = T.nn.TransformerEncoderLayer(d_model=4,\n",
        "      nhead=2, dim_feedforward=10, \n",
        "      batch_first=True)  # d_model divisible by nhead\n",
        "    self.trans_enc = T.nn.TransformerEncoder(self.enc_layer,\n",
        "      num_layers=2)  # 6 layers default\n",
        "\n",
        "    self.fc1 = T.nn.Linear(32, 10)  # 8--32-T-10-1\n",
        "    self.fc2 = T.nn.Linear(10, 1)\n",
        "\n",
        "    # default weight and bias initialization\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.embed(x)  # 8 inpts to 32 embed\n",
        "    z = z.reshape(-1, 8, 4)  # bat seq embed\n",
        "    z = self.pos_enc(z) \n",
        "    z = self.trans_enc(z) \n",
        "    z = z.reshape(-1, 32)  # torch.Size([bs, xxx])\n",
        "    z = T.tanh(self.fc1(z))\n",
        "    z = self.fc2(z)  # regression: no activation\n",
        "    return z\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def accuracy(model, ds, pct_close):\n",
        "  # assumes model.eval()\n",
        "  # correct within pct of true income\n",
        "  n_correct = 0; n_wrong = 0\n",
        "\n",
        "  for i in range(len(ds)):\n",
        "    X = ds[i][0].reshape(1,-1)  # make it a batch\n",
        "    Y = ds[i][1].reshape(1)\n",
        "    with T.no_grad():\n",
        "      oupt = model(X)         # computed income\n",
        "\n",
        "    if T.abs(oupt - Y) <= T.abs(pct_close * Y):\n",
        "      n_correct += 1\n",
        "    else:\n",
        "      n_wrong += 1\n",
        "  acc = (n_correct * 1.0) / (n_correct + n_wrong)\n",
        "  return acc\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def accuracy_x(model, ds, pct_close):\n",
        "  # all-at-once (quick)\n",
        "  # assumes model.eval()\n",
        "  X = ds.x_data  # all inputs\n",
        "  Y = ds.y_data  # all targets\n",
        "  n_items = len(X)\n",
        "  with T.no_grad():\n",
        "    pred = model(X)  # all predicted incomes\n",
        " \n",
        "  n_correct = T.sum((T.abs(pred - Y) <= \\\n",
        "    T.abs(pct_close * Y)))\n",
        "  result = (n_correct.item() / n_items)  # scalar\n",
        "  return result  \n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def train(model, ds, bs, lr, me, le, test_ds):\n",
        "  # dataset, bat_size, lrn_rate, max_epochs, log interval\n",
        "  train_ldr = T.utils.data.DataLoader(ds, batch_size=bs,\n",
        "    shuffle=True)\n",
        "  loss_func = T.nn.MSELoss()\n",
        "  optimizer = T.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(0, me):\n",
        "    epoch_loss = 0.0  # for one full epoch\n",
        "    for (b_idx, batch) in enumerate(train_ldr):\n",
        "      X = batch[0]  # predictors\n",
        "      y = batch[1]  # target income\n",
        "      optimizer.zero_grad()\n",
        "      oupt = model(X)\n",
        "      loss_val = loss_func(oupt, y)  # a tensor\n",
        "      epoch_loss += loss_val.item()  # accumulate\n",
        "      loss_val.backward()  # compute gradients\n",
        "      optimizer.step()     # update weights\n",
        "\n",
        "    if epoch % le == 0:\n",
        "      print(\"epoch = %4d  |  loss = %0.4f\" % \\\n",
        "        (epoch, epoch_loss))\n",
        "      # model.eval()\n",
        "      # print(\"-------------\")\n",
        "      # acc_train = accuracy(model, ds, 0.10)\n",
        "      # print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
        "      # acc_test = accuracy(model, test_ds, 0.10) \n",
        "      # print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
        "      # model.train()\n",
        "      # print(\"-------------\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def main():\n",
        "  # 0. get started\n",
        "  print(\"\\nBegin People predict income using Transformer \")\n",
        "  T.manual_seed(0)\n",
        "  np.random.seed(0)\n",
        "  \n",
        "\n",
        "\n",
        "  # 1. create Dataset objects\n",
        "  print(\"\\nCreating People Dataset objects \")\n",
        "  train_file = \"../src/spotpython/data/people_train.csv\"\n",
        "  train_ds = PeopleDataset(train_file)  # 200 rows\n",
        "\n",
        "  test_file = \"../src/spotpython/data/people_test.csv\"\n",
        "  test_ds = PeopleDataset(test_file)  # 40 rows\n",
        "\n",
        "  # 2. create network\n",
        "  print(\"\\nCreating (8--32)-T-10-1 neural network \")\n",
        "  net = TransformerNet().to(device)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "  # 3. train model\n",
        "  print(\"\\nbat_size = 10 \")\n",
        "  print(\"loss = MSELoss() \")\n",
        "  print(\"optimizer = Adam \")\n",
        "  print(\"lrn_rate = 0.01 \")\n",
        "\n",
        "  print(\"\\nStarting training\")\n",
        "  net.train()\n",
        "  train(net, train_ds, bs=10, lr=0.01, me=300,\n",
        "    le=50, test_ds=test_ds)\n",
        "  print(\"Done \")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "  # 4. evaluate model accuracy\n",
        "  print(\"\\nComputing model accuracy (within 0.10 of true) \")\n",
        "  net.eval()\n",
        "  acc_train = accuracy(net, train_ds, 0.10)  # item-by-item\n",
        "  print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
        "\n",
        "  acc_test = accuracy_x(net, test_ds, 0.10)  # all-at-once\n",
        "  print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "  # 5. make a prediction\n",
        "  print(\"\\nPredicting income for M 34 Oklahoma moderate: \")\n",
        "  x = np.array([[-1, 0.34, 0,0,1,  0,1,0]],\n",
        "    dtype=np.float32)\n",
        "  x = T.tensor(x, dtype=T.float32).to(device) \n",
        "\n",
        "  with T.no_grad():\n",
        "    pred_inc = net(x)\n",
        "  pred_inc = pred_inc.item()  # scalar\n",
        "  print(\"$%0.2f\" % (pred_inc * 100_000))  # un-normalized\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "  # 6. save model (state_dict approach)\n",
        "  print(\"\\nSaving trained model state\")\n",
        "  fn = \".\\\\Models\\\\people_income_model.pt\"\n",
        "  T.save(net.state_dict(), fn)\n",
        "\n",
        "  # model = Net()\n",
        "  # model.load_state_dict(T.load(fn))\n",
        "  # use model to make prediction(s)\n",
        "\n",
        "  print(\"\\nEnd People income demo \")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skip Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class SkipLinear(torch.nn.Module):\n",
        "    class Core(torch.nn.Module):\n",
        "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
        "\n",
        "        def __init__(self, n):\n",
        "            \"\"\"\n",
        "            Initialize the layer.\n",
        "\n",
        "            Args:\n",
        "                n (int): The number of output nodes.\n",
        "            \"\"\"\n",
        "            super().__init__()\n",
        "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
        "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
        "            lim = 0.01\n",
        "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
        "\n",
        "        def forward(self, x)->torch.Tensor:\n",
        "            \"\"\"\n",
        "            Forward pass through the layer.\n",
        "\n",
        "            Args:\n",
        "                x (torch.Tensor): The input tensor.\n",
        "\n",
        "            Returns:\n",
        "                torch.Tensor: The output of the layer.\n",
        "            \"\"\"\n",
        "            return x @ self.weights.t() + self.biases\n",
        "\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super().__init__()\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        if n_out % n_in != 0:\n",
        "            raise ValueError(\"n_out % n_in != 0\")\n",
        "        n = n_out // n_in  # num nodes per input\n",
        "\n",
        "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        lst_nodes = []\n",
        "        for i in range(self.n_in):\n",
        "            xi = x[:, i].reshape(-1, 1)\n",
        "            oupt = self.lst_modules[i](xi)\n",
        "            lst_nodes.append(oupt)\n",
        "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
        "        for i in range(2, self.n_in):\n",
        "            result = torch.cat((result, lst_nodes[i]), 1)\n",
        "        result = result.reshape(-1, self.n_out)\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SkipLinear(torch.nn.Module):\n",
        "\n",
        "    class Core(torch.nn.Module):\n",
        "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
        "\n",
        "        def __init__(self, n):\n",
        "            \"\"\"\n",
        "            Initialize the layer.\n",
        "\n",
        "            Args:\n",
        "                n (int): The number of output nodes.\n",
        "            \"\"\"\n",
        "            super().__init__()\n",
        "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
        "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
        "            lim = 0.01\n",
        "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
        "\n",
        "        def forward(self, x) -> torch.Tensor:\n",
        "            \"\"\"\n",
        "            Forward pass through the layer.\n",
        "\n",
        "            Args:\n",
        "                x (torch.Tensor): The input tensor.\n",
        "\n",
        "            Returns:\n",
        "                torch.Tensor: The output of the layer.\n",
        "            \"\"\"\n",
        "            return x @ self.weights.t() + self.biases\n",
        "\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super().__init__()\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        if n_out % n_in != 0:\n",
        "            raise ValueError(\"n_out % n_in != 0\")\n",
        "        n = n_out // n_in  # num nodes per input\n",
        "\n",
        "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        lst_nodes = []\n",
        "        for i in range(self.n_in):\n",
        "            xi = x[:, i].reshape(-1, 1)\n",
        "            oupt = self.lst_modules[i](xi)\n",
        "            lst_nodes.append(oupt)\n",
        "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
        "        for i in range(2, self.n_in):\n",
        "            result = torch.cat((result, lst_nodes[i]), 1)\n",
        "        result = result.reshape(-1, self.n_out)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# spotGUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import get_default_values, get_bound_values\n",
        "lhd = LightHyperDict()\n",
        "# generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
        "fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
        "get_default_values(fun_control)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tkinter as tk\n",
        "import json\n",
        "from spotpython.hyperparameters.values import get_default_values, get_bound_values\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "\n",
        "\n",
        "def create_gui(model):\n",
        "    lhd = LightHyperDict()\n",
        "    # generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
        "    fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
        "\n",
        "    # Apply the functions to the dictionary\n",
        "    default_values = get_default_values(fun_control)\n",
        "    lower_bound_values = get_bound_values(fun_control, \"lower\")\n",
        "    upper_bound_values = get_bound_values(fun_control, \"upper\")\n",
        "\n",
        "    # Create a tkinter window\n",
        "    root = tk.Tk()\n",
        "\n",
        "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
        "    for i, (key, value) in enumerate(lhd.hyper_dict['NetLightRegression'].items()):\n",
        "            # Create a label with the key as text\n",
        "            label = tk.Label(root, text=key)\n",
        "            label.grid(row=i, column=0, sticky=\"W\")\n",
        "\n",
        "            # Create an entry with the default value as the default text\n",
        "            default_entry = tk.Entry(root)\n",
        "            default_entry.insert(0, value)\n",
        "            default_entry.grid(row=i, column=1, sticky=\"W\")\n",
        "        # add the lower bound values in column 2\n",
        "            lower_bound_entry = tk.Entry(root)\n",
        "            lower_bound_entry.insert(0, lower_bound_values[i])\n",
        "            lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
        "        # add the upper bound values in column 3\n",
        "            upper_bound_entry = tk.Entry(root)\n",
        "            upper_bound_entry.insert(0, upper_bound_values[i])\n",
        "            upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
        "\n",
        "    # Run the tkinter main loop\n",
        "    root.mainloop()\n",
        "\n",
        "# Call the function to create the GUI\n",
        "create_gui(model = 'NetLightRegression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tkinter as tk\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "\n",
        "\n",
        "def create_gui(model):\n",
        "    lhd = LightHyperDict()\n",
        "    dict =  lhd.hyper_dict[model]\n",
        "\n",
        "    \n",
        "    # Create a tkinter window\n",
        "    root = tk.Tk()\n",
        "\n",
        "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
        "    # TODO: Add labels to the column headers\n",
        "    for i, (key, value) in enumerate(dict.items()):            \n",
        "            if dict[key][\"type\"] == \"int\" or dict[key][\"type\"] == \"float\":\n",
        "                # Create a label with the key as text\n",
        "                label = tk.Label(root, text=key)\n",
        "                label.grid(row=i, column=0, sticky=\"W\")\n",
        "                # Create an entry with the default value as the default text\n",
        "                default_entry = tk.Entry(root)\n",
        "                default_entry.insert(0, dict[key][\"default\"])\n",
        "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
        "                # add the lower bound values in column 2\n",
        "                lower_bound_entry = tk.Entry(root)                \n",
        "                lower_bound_entry.insert(0, dict[key][\"lower\"])\n",
        "                lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
        "                # add the upper bound values in column 3\n",
        "                upper_bound_entry = tk.Entry(root)\n",
        "                upper_bound_entry.insert(0, dict[key][\"upper\"])\n",
        "                upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
        "            if dict[key][\"type\"] == \"factor\":        \n",
        "                # Create a label with the key as text\n",
        "                label = tk.Label(root, text=key)\n",
        "                label.grid(row=i, column=0, sticky=\"W\")\n",
        "                # Create an entry with the default value as the default text\n",
        "                default_entry = tk.Entry(root)\n",
        "                default_entry.insert(0, dict[key][\"default\"])\n",
        "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
        "                # add the lower bound values in column 2\n",
        "                factor_level_entry = tk.Entry(root)\n",
        "                # add a comma to each level\n",
        "                dict[key][\"levels\"] = \", \".join(dict[key][\"levels\"])                                \n",
        "                factor_level_entry.insert(0, dict[key][\"levels\"])\n",
        "                # TODO: Fix columnspan\n",
        "                factor_level_entry.grid(row=i, column=2, columnspan=2, sticky=\"W\")\n",
        "\n",
        "    # Run the tkinter main loop\n",
        "    root.mainloop()\n",
        "\n",
        "# Call the function to create the GUI\n",
        "create_gui(model = 'NetLightRegression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_gui(model = 'TransformerLightRegression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# save Load Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from spotpython.utils.file import save_experiment, load_experiment\n",
        "import numpy as np\n",
        "from math import inf\n",
        "from spotpython.spot import spot\n",
        "from spotpython.utils.init import (\n",
        "    fun_control_init,\n",
        "    design_control_init,\n",
        "    surrogate_control_init,\n",
        "    optimizer_control_init)\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "\n",
        "def test_file_save_load():\n",
        "    fun = analytical().fun_branin\n",
        "\n",
        "    fun_control = fun_control_init(\n",
        "                PREFIX=\"branin\",\n",
        "                SUMMARY_WRITER=False,\n",
        "                lower = np.array([0, 0]),\n",
        "                upper = np.array([10, 10]),\n",
        "                fun_evals=8,\n",
        "                fun_repeats=1,\n",
        "                max_time=inf,\n",
        "                noise=False,\n",
        "                tolerance_x=0,\n",
        "                ocba_delta=0,\n",
        "                var_type=[\"num\", \"num\"],\n",
        "                infill_criterion=\"ei\",\n",
        "                n_points=1,\n",
        "                seed=123,\n",
        "                log_level=20,\n",
        "                show_models=False,\n",
        "                show_progress=True)\n",
        "    design_control = design_control_init(\n",
        "                init_size=5,\n",
        "                repeats=1)\n",
        "    surrogate_control = surrogate_control_init(\n",
        "                model_fun_evals=10000,\n",
        "                min_theta=-3,\n",
        "                max_theta=3,\n",
        "                n_theta=2,\n",
        "                theta_init_zero=True,\n",
        "                n_p=1,\n",
        "                optim_p=False,\n",
        "                var_type=[\"num\", \"num\"],\n",
        "                seed=124)\n",
        "    optimizer_control = optimizer_control_init(\n",
        "                max_iter=1000,\n",
        "                seed=125)\n",
        "    spot_tuner = spot.Spot(fun=fun,\n",
        "                fun_control=fun_control,\n",
        "                design_control=design_control,\n",
        "                surrogate_control=surrogate_control,\n",
        "                optimizer_control=optimizer_control)\n",
        "    # Call the save_experiment function\n",
        "    pkl_name = save_experiment(\n",
        "        spot_tuner=spot_tuner,\n",
        "        fun_control=fun_control,\n",
        "        design_control=None,\n",
        "        surrogate_control=None,\n",
        "        optimizer_control=None\n",
        "    )\n",
        "\n",
        "    # Verify that the pickle file is created\n",
        "    assert os.path.exists(pkl_name)\n",
        "\n",
        "    # Call the load_experiment function\n",
        "    spot_tuner_1, fun_control_1, design_control_1, surrogate_control_1, optimizer_control_1 = load_experiment(pkl_name)\n",
        "\n",
        "    # Verify the name of the pickle file\n",
        "    assert pkl_name == f\"spot_{fun_control['PREFIX']}experiment.pickle\"\n",
        "\n",
        "    # Clean up the temporary directory\n",
        "    os.remove(pkl_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_file_save_load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Netlightregression2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.light.regression.netlightregression2 import NetLightRegression2\n",
        "from torch import nn\n",
        "import lightning as L\n",
        "import torch\n",
        "BATCH_SIZE = 8\n",
        "dataset = Diabetes()\n",
        "train1_set, test_set = torch.utils.data.random_split(dataset, [0.6, 0.4])\n",
        "train_set, val_set = torch.utils.data.random_split(train1_set, [0.6, 0.4])\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
        "batch_x, batch_y = next(iter(train_loader))\n",
        "print(f\"batch_x.shape: {batch_x.shape}\")\n",
        "print(f\"batch_y.shape: {batch_y.shape}\")\n",
        "net_light_base = NetLightRegression2(l1=128,\n",
        "                                    epochs=10,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    initialization='Default',\n",
        "                                    act_fn=nn.ReLU(),\n",
        "                                    optimizer='Adam',\n",
        "                                    dropout_prob=0.1,\n",
        "                                    lr_mult=0.1,\n",
        "                                    patience=5,\n",
        "                                    _L_in=10,\n",
        "                                    _L_out=1,\n",
        "                                    _torchmetric=\"mean_squared_error\",)\n",
        "trainer = L.Trainer(max_epochs=10,  enable_progress_bar=False)\n",
        "trainer.fit(net_light_base, train_loader)\n",
        "trainer.validate(net_light_base, val_loader)\n",
        "trainer.test(net_light_base, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LightDataModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "# data.csv is simple csv file with 11 samples\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "data_module.setup()\n",
        "print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Tuned Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "from spotpython.utils.file import save_experiment, load_experiment\n",
        "import numpy as np\n",
        "from math import inf\n",
        "from spotpython.spot import spot\n",
        "from spotpython.utils.init import (\n",
        "    fun_control_init,\n",
        "    design_control_init,\n",
        "    surrogate_control_init,\n",
        "    optimizer_control_init)\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "fun = analytical().fun_branin\n",
        "fun_control = fun_control_init(\n",
        "            PREFIX=\"branin\",\n",
        "            SUMMARY_WRITER=False,\n",
        "            lower = np.array([0, 0]),\n",
        "            upper = np.array([10, 10]),\n",
        "            fun_evals=8,\n",
        "            fun_repeats=1,\n",
        "            max_time=inf,\n",
        "            noise=False,\n",
        "            tolerance_x=0,\n",
        "            ocba_delta=0,\n",
        "            var_type=[\"num\", \"num\"],\n",
        "            infill_criterion=\"ei\",\n",
        "            n_points=1,\n",
        "            seed=123,\n",
        "            log_level=20,\n",
        "            show_models=False,\n",
        "            show_progress=True)\n",
        "design_control = design_control_init(\n",
        "            init_size=5,\n",
        "            repeats=1)\n",
        "surrogate_control = surrogate_control_init(\n",
        "            model_fun_evals=10000,\n",
        "            min_theta=-3,\n",
        "            max_theta=3,\n",
        "            n_theta=2,\n",
        "            theta_init_zero=True,\n",
        "            n_p=1,\n",
        "            optim_p=False,\n",
        "            var_type=[\"num\", \"num\"],\n",
        "            seed=124)\n",
        "optimizer_control = optimizer_control_init(\n",
        "            max_iter=1000,\n",
        "            seed=125)\n",
        "spot_tuner = spot.Spot(fun=fun,\n",
        "            fun_control=fun_control,\n",
        "            design_control=design_control,\n",
        "            surrogate_control=surrogate_control,\n",
        "            optimizer_control=optimizer_control)\n",
        "spot_tuner.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.get_tuned_hyperparameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.get_tuned_hyperparameters(fun_control=fun_control)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Tuned Hyperparameters from a Machine/Deep Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.device import getDevice\n",
        "from math import inf\n",
        "from spotpython.utils.init import fun_control_init\n",
        "import numpy as np\n",
        "from spotpython.hyperparameters.values import set_control_key_value\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "\n",
        "MAX_TIME = 1\n",
        "FUN_EVALS = 10\n",
        "INIT_SIZE = 5\n",
        "WORKERS = 0\n",
        "PREFIX=\"037\"\n",
        "DEVICE = getDevice()\n",
        "DEVICES = 1\n",
        "TEST_SIZE = 0.4\n",
        "TORCH_METRIC = \"mean_squared_error\"\n",
        "dataset = Diabetes()\n",
        "\n",
        "fun_control = fun_control_init(\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    _torchmetric=TORCH_METRIC,\n",
        "    PREFIX=PREFIX,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    data_set=dataset,\n",
        "    device=DEVICE,\n",
        "    enable_progress_bar=False,\n",
        "    fun_evals=FUN_EVALS,\n",
        "    log_level=50,\n",
        "    max_time=MAX_TIME,\n",
        "    num_workers=WORKERS,\n",
        "    show_progress=True,\n",
        "    test_size=TEST_SIZE,\n",
        "    tolerance_x=np.sqrt(np.spacing(1)),\n",
        "    )\n",
        "from spotpython.light.regression.netlightregression import NetLightRegression\n",
        "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
        "add_core_model_to_fun_control(fun_control=fun_control,\n",
        "                              core_model=NetLightRegression,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
        "\n",
        "set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n",
        "set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n",
        "set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n",
        "set_control_hyperparameter_value(fun_control, \"optimizer\", [\n",
        "                \"Adam\",\n",
        "                \"RAdam\",\n",
        "            ])\n",
        "set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n",
        "set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n",
        "set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n",
        "set_control_hyperparameter_value(fun_control, \"act_fn\",[\n",
        "                \"ReLU\",\n",
        "                \"LeakyReLU\"\n",
        "            ] )\n",
        "from spotpython.utils.init import design_control_init, surrogate_control_init\n",
        "design_control = design_control_init(init_size=INIT_SIZE)\n",
        "\n",
        "surrogate_control = surrogate_control_init(noise=True,\n",
        "                                            n_theta=2)\n",
        "from spotpython.fun.hyperlight import HyperLight\n",
        "fun = HyperLight(log_level=50).fun\n",
        "from spotpython.spot import spot\n",
        "spot_tuner = spot.Spot(fun=fun,\n",
        "                       fun_control=fun_control,\n",
        "                       design_control=design_control,\n",
        "                       surrogate_control=surrogate_control)\n",
        "spot_tuner.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.get_tuned_hyperparameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.print_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Factors\n",
        "\n",
        "* Example from https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/012_num_spot_ei.html#factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.design.spacefilling import spacefilling\n",
        "from spotpython.build.kriging import Kriging\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen = spacefilling(2)\n",
        "n = 30\n",
        "rng = np.random.RandomState(1)\n",
        "lower = np.array([-5,-0])\n",
        "upper = np.array([10,15])\n",
        "fun = analytical().fun_branin_factor\n",
        "#fun = analytical(sigma=0).fun_sphere\n",
        "\n",
        "X0 = gen.scipy_lhd(n, lower=lower, upper = upper)\n",
        "X1 = np.random.randint(low=0, high=3, size=(n,))\n",
        "X = np.c_[X0, X1]\n",
        "y = fun(X)\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "S = Kriging(name='kriging',  seed=123, log_level=10, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\n",
        "S.fit(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "Sf = Kriging(name='kriging',  seed=123, log_level=10, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"factor\"])\n",
        "# Sf = Kriging(name='kriging',  seed=123, log_level=50, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\n",
        "Sf.fit(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = 0\n",
        "for _ in range(100):\n",
        "    n = 100\n",
        "    X0 = gen.scipy_lhd(n, lower=lower, upper = upper)\n",
        "    X1 = np.random.randint(low=0, high=3, size=(n,))\n",
        "    X = np.c_[X0, X1]\n",
        "    y = fun(X)\n",
        "    s=np.sum(np.abs(S.predict(X) - y))\n",
        "    sf=np.sum(np.abs(Sf.predict(X) - y))\n",
        "    res = res + (sf - s)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Subset Select"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def select_distant_points(X, y, k):\n",
        "    \"\"\"\n",
        "    Selects k points that are distant from each other using a clustering approach.\n",
        "    \n",
        "    :param X: np.array of shape (n, k), with n points in k-dimensional space.\n",
        "    :param y: np.array of length n, with values corresponding to each point in X.\n",
        "    :param k: The number of distant points to select.\n",
        "    :return: Selected k points from X and their corresponding y values.\n",
        "    \"\"\"\n",
        "    # Perform k-means clustering to find k clusters\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n",
        "    \n",
        "    # Find the closest point in X to each cluster center\n",
        "    selected_points = np.array([X[np.argmin(np.linalg.norm(X - center, axis=1))] for center in kmeans.cluster_centers_])\n",
        "    \n",
        "    # Find indices of the selected points in the original X array\n",
        "    indices = np.array([np.where(np.all(X==point, axis=1))[0][0] for point in selected_points])\n",
        "    \n",
        "    # Select the corresponding y values\n",
        "    selected_y = y[indices]\n",
        "    \n",
        "    return selected_points, selected_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.random.rand(100, 2)  # Generate some random points\n",
        "y = np.random.rand(100)     # Random corresponding y values\n",
        "k = 5\n",
        "\n",
        "selected_points, selected_y = select_distant_points(X, y, k)\n",
        "print(\"Selected Points:\", selected_points)\n",
        "print(\"Corresponding y values:\", selected_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "from spotpython.spot import spot\n",
        "from spotpython.utils.init import (\n",
        "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
        "    )\n",
        "# number of initial points:\n",
        "ni = 5\n",
        "# number of points\n",
        "fun_evals = 10\n",
        "fun = analytical().fun_sphere\n",
        "fun_control = fun_control_init(\n",
        "    lower = np.array([-1, -1, -1]),\n",
        "    upper = np.array([1, 1, 1]),\n",
        "    fun_evals=fun_evals,\n",
        "    tolerance_x = np.sqrt(np.spacing(1))\n",
        "    )\n",
        "design_control=design_control_init(init_size=ni)\n",
        "surrogate_control=surrogate_control_init(n_theta=3)\n",
        "S = spot.Spot(fun=fun,\n",
        "            fun_control=fun_control,\n",
        "            design_control=design_control,\n",
        "            surrogate_control=surrogate_control,)\n",
        "S.run()\n",
        "S.plot_important_hyperparameter_contour(max_imp=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n",
        "\n",
        "# Sorting the array in descending order by the second element of each sub-list\n",
        "sorted_array = sorted(array, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(sorted_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sort_by_second_and_return_indices(array):\n",
        "    \"\"\"\n",
        "    Sorts an array of arrays based on the second values in descending order and returns\n",
        "    the indices of the original array entries.\n",
        "\n",
        "    :param array: List of lists, where each inner list has at least two elements.\n",
        "    :return: Indices of the original array entries after sorting by the second value.\n",
        "             Returns an empty list if the input is empty or None.\n",
        "    :raises ValueError: If any sub-array is improperly structured.\n",
        "    \"\"\"\n",
        "    if not array:\n",
        "        return []\n",
        "\n",
        "    # Check for improperly structured sub-arrays\n",
        "    for item in array:\n",
        "        if not isinstance(item, list) or len(item) < 2:\n",
        "            raise ValueError(\"All sub-arrays must be lists with at least two elements.\")\n",
        "\n",
        "    # Enumerate the array to keep track of original indices, then sort by the second item\n",
        "    sorted_indices = [index for index, value in sorted(enumerate(array), key=lambda x: x[1][1], reverse=True)]\n",
        "\n",
        "    return sorted_indices\n",
        "\n",
        "# Example usage\n",
        "try:\n",
        "    array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n",
        "    indices = sort_by_second_and_return_indices(array)\n",
        "    print(\"Indices of the sorted elements:\", indices)\n",
        "except ValueError as error:\n",
        "    print(f\"Error: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Core Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from river.tree import HoeffdingAdaptiveTreeRegressor\n",
        "from spotriver.data.river_hyper_dict import RiverHyperDict\n",
        "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_for_core_model, get_default_values\n",
        "fun_control = {}\n",
        "add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n",
        "    fun_control=fun_control,\n",
        "    hyper_dict=RiverHyperDict,\n",
        "    filename=None)\n",
        "values = get_default_values(fun_control)\n",
        "print(values)\n",
        "# get_default_hyperparameters_for_core_model(fun_control)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pytest\n",
        "import pprint\n",
        "from spotpython.plot.xai import (get_activations, get_gradients, get_weights, plot_nn_values_hist, plot_nn_values_scatter, visualize_weights, visualize_gradients, visualize_activations, visualize_activations_distributions, visualize_gradient_distributions, visualize_weights_distributions)\n",
        "\n",
        "def test_plot_nn_values_scatter_reshaped_values():\n",
        "    # Mock data for testing\n",
        "    nn_values = {\n",
        "        'layer0': np.random.rand(10),  # 10 values suggesting padding for a 4x4\n",
        "        'layer1': np.random.rand(64),  # 64 values suggesting a perfect square (8x8)\n",
        "        'layer2': np.random.rand(32),  # 32 values suggesting  padding for a 6x6\n",
        "        'layer3': np.random.rand(16),  # 16 values suggesting a perfect square (4x4)\n",
        "    }\n",
        "\n",
        "    # Use the modified function that returns reshaped_values for testing\n",
        "    reshaped_values = plot_nn_values_scatter(nn_values, 'Test NN', return_reshaped=True)    \n",
        "\n",
        "    pprint.pprint(nn_values)\n",
        "    pprint.pprint(reshaped_values)\n",
        "    \n",
        "\n",
        "    # Assert for layer0: Checks if reshaping is correct for perfect square\n",
        "    assert reshaped_values['layer0'].shape == (4, 4)\n",
        "    # Assert for layer1: Checks if reshaping is correct for non-square\n",
        "    assert reshaped_values['layer1'].shape == (8, 8)\n",
        "    assert reshaped_values['layer2'].shape == (6, 6)\n",
        "    assert reshaped_values['layer3'].shape == (4, 4)\n",
        "\n",
        "\n",
        "\n",
        "test_plot_nn_values_scatter_reshaped_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.convert import set_dataset_target_type\n",
        "import pandas as pd\n",
        "dataset = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9], \"y\": [True, False, True]})\n",
        "print(dataset)\n",
        "dataset = set_dataset_target_type(dataset)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import river.tree\n",
        "core_model_name = \"tree.HoeffdingTreeRegressor\"\n",
        "core_model_module = core_model_name.split(\".\")[0]\n",
        "coremodel = core_model_name.split(\".\")[1]\n",
        "core_model_instance = getattr(getattr(river, core_model_module), coremodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.friedman import FriedmanDriftDataset\n",
        "import matplotlib.pyplot as plt\n",
        "data_generator = FriedmanDriftDataset(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\n",
        "data = [data for data in data_generator]\n",
        "indices = [i for _, _, i in data]\n",
        "values = {f\"x{i}\": [] for i in range(5)}\n",
        "values[\"y\"] = []\n",
        "for x, y, _ in data:\n",
        "    for i in range(5):\n",
        "        values[f\"x{i}\"].append(x[i])\n",
        "    values[\"y\"].append(y)\n",
        "plt.figure(figsize=(10, 6))\n",
        "for label, series in values.items():\n",
        "    plt.plot(indices, series, label=label)\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "plt.title('')\n",
        "plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n",
        "plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Scaler for Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.csvdataset import CSVDataset\n",
        "from spotpython.data.pkldataset import PKLDataset\n",
        "from spotpython.utils.scaler import TorchStandardScaler\n",
        "import torch\n",
        "\n",
        "scaler=TorchStandardScaler()\n",
        "\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.float64)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
        "data_module.setup()\n",
        "print(f\"Training set size: {len(data_module.data_train)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "diabetes = load_diabetes()\n",
        "import numpy as np\n",
        "np.max(diabetes.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.csvdataset import CSVDataset\n",
        "from spotpython.data.pkldataset import PKLDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "dataset = Diabetes()\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 1\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "housing.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.california_housing import CaliforniaHousing\n",
        "dataset = CaliforniaHousing()\n",
        "print(dataset.get_names())\n",
        "print(len(dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.california_housing import CaliforniaHousing\n",
        "dataset = CaliforniaHousing()\n",
        "print(dataset.data.shape)\n",
        "print(dataset.targets.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from spotpython.data.california_housing import CaliforniaHousing\n",
        "import torch\n",
        "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.california_housing import CaliforniaHousing\n",
        "import torch\n",
        "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=2, test_size=0.5, scaler=scaler)\n",
        "data_module.setup()\n",
        "print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n",
        "# print the first batch of the training set from data_module.data_train\n",
        "print(next(iter(data_module.train_dataloader())))\n",
        "# print the first batch of the training set from data_module.data_train as a numpy array\n",
        "print(next(iter(data_module.train_dataloader()))[0].detach().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "from spotpython.spot import spot\n",
        "from spotpython.utils.init import (\n",
        "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
        "    )\n",
        "# number of initial points:\n",
        "ni = 7\n",
        "# start point X_0\n",
        "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "fun = analytical().fun_sphere\n",
        "fun_control = fun_control_init(\n",
        "    lower = np.array([-1, -1]),\n",
        "    upper = np.array([1, 1]))\n",
        "design_control=design_control_init(init_size=ni)\n",
        "S = spot.Spot(fun=fun,\n",
        "            fun_control=fun_control,\n",
        "            design_control=design_control,)\n",
        "S.run(X_start=X_start)\n",
        "print(f\"S.X: {S.X}\")\n",
        "print(f\"S.y: {S.y}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from spotpython.data.lightdatamodule import LightDataModule\n",
        "from spotpython.data.csvdataset import CSVDataset\n",
        "from spotpython.utils.scaler import TorchStandardScaler, TorchMinMaxScaler\n",
        "from spotpython.data.california_housing import CaliforniaHousing\n",
        "\n",
        "\n",
        "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
        "scaler = TorchMinMaxScaler()\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
        "data_module.setup()\n",
        "\n",
        "loader = data_module.train_dataloader\n",
        "\n",
        "total_sum = None\n",
        "total_count = 0\n",
        "\n",
        "# Iterate over batches in the DataLoader\n",
        "for batch in loader():\n",
        "    inputs, targets = batch\n",
        "    \n",
        "\n",
        "total_sum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.light.regression.netlightregression import NetLightRegression\n",
        "from torch import nn\n",
        "import lightning as L\n",
        "\n",
        "\n",
        "def test_net_light_regression_class():\n",
        "    BATCH_SIZE = 8\n",
        "\n",
        "    dataset = Diabetes()\n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    net_light_regression = NetLightRegression(\n",
        "        l1=128,\n",
        "        epochs=10,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        initialization=\"Default\",\n",
        "        act_fn=nn.ReLU(),\n",
        "        optimizer=\"Adam\",\n",
        "        dropout_prob=0.1,\n",
        "        lr_mult=0.1,\n",
        "        patience=5,\n",
        "        _L_in=10,\n",
        "        _L_out=1,\n",
        "        _torchmetric=\"mean_squared_error\",\n",
        "    )\n",
        "    trainer = L.Trainer(\n",
        "        max_epochs=2,\n",
        "        enable_progress_bar=False,\n",
        "    )\n",
        "    trainer.fit(net_light_regression, train_loader, val_loader)\n",
        "    res = trainer.test(net_light_regression, test_loader)\n",
        "    # test if the entry 'hp_metric' is in the res dict\n",
        "    assert \"hp_metric\" in res[0].keys()\n",
        "\n",
        "test_net_light_regression_class()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## set_int_hyperparameter_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperparameters.values import set_int_hyperparameter_values\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "fun_control = fun_control_init(\n",
        "    core_model_name=\"forest.AMFRegressor\",\n",
        "    hyperdict=RiverHyperDict,\n",
        ")\n",
        "print(\"Before modification:\")\n",
        "print(gen_design_table(fun_control))\n",
        "set_int_hyperparameter_values(fun_control, \"n_estimators\", 2, 5)\n",
        "print(\"After modification:\")\n",
        "print(gen_design_table(fun_control))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pprint\n",
        "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperparameters.values import set_factor_hyperparameter_values\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "fun_control = fun_control_init(\n",
        "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
        "    hyperdict=RiverHyperDict,\n",
        ")\n",
        "print(\"Before modification:\")\n",
        "print(gen_design_table(fun_control))\n",
        "set_factor_hyperparameter_values(fun_control, \"leaf_model\", ['LinearRegression',\n",
        "                                                     'Perceptron'])\n",
        "print(\"After modification:\")\n",
        "print(gen_design_table(fun_control))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fun_control = fun_control_init(\n",
        "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
        "    hyperdict=RiverHyperDict,\n",
        ")\n",
        "\n",
        "set_factor_hyperparameter_values(fun_control, \"leaf_model\", [\"LinearRegression\",\n",
        "                                                                \"Perceptron\"])\n",
        "\n",
        "# Access updated hyperparameters\n",
        "updated_hyperparameters = fun_control[\"core_model_hyper_dict\"]\n",
        "print(updated_hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
        "from spotpython.utils.init import fun_control_init\n",
        "from spotpython.hyperparameters.values import set_boolean_hyperparameter_values\n",
        "from spotpython.utils.eda import gen_design_table\n",
        "fun_control = fun_control_init(\n",
        "    core_model_name=\"forest.AMFRegressor\",\n",
        "    hyperdict=RiverHyperDict,\n",
        ")\n",
        "print(\"Before modification:\")\n",
        "print(gen_design_table(fun_control))\n",
        "set_boolean_hyperparameter_values(fun_control, \"use_aggregation\", 0, 0)\n",
        "print(\"After modification:\")\n",
        "print(gen_design_table(fun_control))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "class MyDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, full_dataset, train_size=0.8, batch_size=32, num_workers=4):\n",
        "        super().__init__()\n",
        "        self.dataset = full_dataset\n",
        "        self.train_size = train_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Split the dataset\n",
        "        train_len = int(len(self.dataset) * self.train_size)\n",
        "        val_len = len(self.dataset) - train_len\n",
        "        self.train_set, self.val_set = random_split(self.dataset, [train_len, val_len])\n",
        "        \n",
        "        # Fit scaler on training data\n",
        "        train_data = torch.stack([item[0] for item in self.train_set])\n",
        "        print(f\"train_data before scaling\\n: {train_data}\")  \n",
        "        self.scaler.fit(train_data)\n",
        "       \n",
        "        # Transform training data\n",
        "        scaled_train_data = self.scaler.transform(train_data)\n",
        "        self.train_set = self._update_dataset(self.train_set, scaled_train_data)\n",
        "        print(f\"train_data after scaling\\n: {self.train_set}\")  \n",
        "        \n",
        "        # Transform validation data\n",
        "        val_data = torch.stack([item[0] for item in self.val_set])\n",
        "        scaled_val_data = self.scaler.transform(val_data)\n",
        "        self.val_set = self._update_dataset(self.val_set, scaled_val_data)\n",
        "\n",
        "    def _update_dataset(self, original_dataset, scaled_data):\n",
        "        updated_dataset = []\n",
        "        for i, (data, label) in enumerate(original_dataset):\n",
        "            updated_dataset.append((torch.tensor(scaled_data[i]), label))\n",
        "        return updated_dataset\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_data = torch.stack([item[0] for item in self.test_set])\n",
        "        scaled_test_data = self.scaler.transform(test_data)\n",
        "        self.test_set = self._update_dataset(self.test_set, scaled_test_data)\n",
        "        return DataLoader(self.test_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Here you can download datasets if needed\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate a 3-dimensional tensor with 1000 samples\n",
        "n = 10\n",
        "data = torch.rand((n, 3))\n",
        "print(f\"data: {data}\")\n",
        "labels = torch.tensor([i % 2 for i in range(n)], dtype=torch.float32)\n",
        "print(f\"labels: {labels}\")\n",
        "full_dataset = MyDataset(data, labels)\n",
        "\n",
        "# Creating DataModule instance\n",
        "data_module = MyDataModule(full_dataset)\n",
        "\n",
        "# Setup the data module\n",
        "data_module.setup()\n",
        "\n",
        "# Example of fetching a single batch\n",
        "train_loader = data_module.train_dataloader()\n",
        "for batch in train_loader:\n",
        "    print(f\"Batch data shape: {batch[0].shape}\")\n",
        "    x, y = batch\n",
        "    print(x)\n",
        "    print(y)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Model Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Important, do not delete the following imports, they are needed for the function add_core_model_to_fun_control\n",
        "import river\n",
        "from river import forest, tree, linear_model, rules\n",
        "from river import preprocessing\n",
        "import sklearn.metrics\n",
        "import spotpython\n",
        "from spotpython.light import regression\n",
        "\n",
        "def get_core_model_from_name(core_model_name: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Returns the river core model name and instance from a core model name.\n",
        "\n",
        "    Args:\n",
        "        core_model_name (str): The full name of the core model in the format 'module.Model'.\n",
        "\n",
        "    Returns:\n",
        "        (str, object): A tuple containing the core model name and an instance of the core model.\n",
        "    \"\"\"\n",
        "    # Split the model name into its components\n",
        "    name_parts = core_model_name.split(\".\")\n",
        "    \n",
        "    if len(name_parts) < 2:\n",
        "        raise ValueError(f\"Invalid core model name: {core_model_name}. Expected format: 'module.ModelName'.\")\n",
        "\n",
        "    module_name = name_parts[0]\n",
        "    model_name = name_parts[1]\n",
        "    \n",
        "    try:\n",
        "        # Try to get the model from the river library\n",
        "        core_model_instance = getattr(getattr(river, module_name), model_name)\n",
        "        return model_name, core_model_instance\n",
        "    except AttributeError:\n",
        "        try:\n",
        "            # Try to get the model from the spotpython library\n",
        "            submodule_name = name_parts[1]\n",
        "            model_name = name_parts[2] if len(name_parts) == 3 else model_name\n",
        "            print(f\"module_name: {module_name}\")\n",
        "            print(f\"submodule_name: {submodule_name}\")\n",
        "            print(f\"model_name: {model_name}\")\n",
        "            core_model_instance = getattr(getattr(getattr(spotpython, module_name), submodule_name), model_name)\n",
        "            return model_name, core_model_instance\n",
        "        except AttributeError:\n",
        "            raise ValueError(f\"Model '{core_model_name}' not found in either 'river' or 'spotpython' libraries.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Example of usage\n",
        "model_name, model_instance = get_core_model_from_name('tree.HoeffdingTreeRegressor')\n",
        "print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name, model_instance = get_core_model_from_name(\"light.regression.NNLinearRegressor\")\n",
        "print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from spotpython.data.diabetes import Diabetes\n",
        "from spotpython.light.regression import NNLinearRegressor\n",
        "from torch import nn\n",
        "import lightning as L\n",
        "PATH_DATASETS = './data'\n",
        "BATCH_SIZE = 8\n",
        "dataset = Diabetes()\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "batch_x, batch_y = next(iter(train_loader))\n",
        "print(batch_x.shape)\n",
        "print(batch_y.shape)\n",
        "net_light_base = NNLinearRegressor(l1=128,\n",
        "                                    epochs=10,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    initialization='xavier',\n",
        "                                    act_fn=nn.ReLU(),\n",
        "                                    optimizer='Adam',\n",
        "                                    dropout_prob=0.1,\n",
        "                                    lr_mult=0.1,\n",
        "                                    patience=5,\n",
        "                                    _L_in=10,\n",
        "                                    _L_out=1,\n",
        "                                    _torchmetric=\"mean_squared_error\",)\n",
        "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
        "trainer.fit(net_light_base, train_loader)\n",
        "trainer.validate(net_light_base, val_loader)\n",
        "trainer.test(net_light_base, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "data = load_diabetes(return_X_y=False, as_frame=True)\n",
        "# svaing the data to a csv file\n",
        "data.frame.to_csv('~/data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Moons Data Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "n_features = 2\n",
        "n_samples = 500\n",
        "target_column = \"y\"\n",
        "ds =  make_moons(n_samples, noise=0.5, random_state=0)\n",
        "X, y = ds\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1))))\n",
        "test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1))))\n",
        "train.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "test.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "train.head()\n",
        "# combine the training and test data and save to a csv file\n",
        "data = pd.concat([train, test])\n",
        "data.to_csv('moon.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sklearn Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "X, y = make_classification(n_samples=1000, n_features=20,  n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
        "# combine the training and test data and save to a csv file\n",
        "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
        "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
        "data.to_csv('binary_classification.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "X, y = make_classification(n_samples=1000, n_features=20,  n_informative=9, n_redundant=2, n_repeated=0, n_classes=10, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
        "# combine the training and test data and save to a csv file\n",
        "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
        "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
        "data.to_csv('multiple_classification.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
        "# combine the training and test data and save to a csv file\n",
        "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
        "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
        "data.to_csv('regression.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "data = load_iris(as_frame=True)\n",
        "data.frame.to_csv('iris.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotGUI.tuner.spotRun import get_report_file_name\n",
        "from spotpython.utils.init import fun_control_init\n",
        "fun_control = fun_control_init(PREFIX=\"test\")\n",
        "get_report_file_name(fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotGUI.tuner.spotRun import get_scenario_dict\n",
        "import pprint\n",
        "dic = get_scenario_dict(\"sklearn\")\n",
        "pprint.pprint(dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyhcf.utils.io import load_hcf_dataframe, hcf_df2tensor\n",
        "from pyhcf.utils.names import load_all_features_N_regression_list\n",
        "from torch.utils.data import DataLoader\n",
        "df = load_hcf_dataframe(A=True,\n",
        "    H=True,\n",
        "    param_list=load_all_features_N_regression_list(),\n",
        "    target='N',\n",
        "    rmNA=True,\n",
        "    rmMF=True,\n",
        "    rmV=4,\n",
        "    min_freq=1000,\n",
        "    incl_drossel=False)\n",
        "dataset = hcf_df2tensor(df, target='N', return_X_y=False)\n",
        "print(type(dataset))\n",
        "print(len(dataset))\n",
        "# save the 'TensorDataset' object to a pkl file\n",
        "# import pickle\n",
        "# with open('hcf_dataset.pkl', 'wb') as f:\n",
        "#     pickle.dump(dataset, f)\n",
        "# load the 'TensorDataset' object from the pkl file\n",
        "# with open('hcf_dataset.pkl', 'rb') as f:\n",
        "#     dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.__getitem__(0)\n",
        "# get the dimensions of the first sample\n",
        "dataset.__getitem__(0)[0].shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Random Point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "from spotpython.spot import spot\n",
        "from spotpython.utils.init import fun_control_init\n",
        "fun = analytical().fun_sphere\n",
        "fun_control = fun_control_init(\n",
        "    lower = np.array([-1, -1]),\n",
        "    upper = np.array([1, 1])\n",
        "    )\n",
        "S = spot.Spot(fun=fun,\n",
        "            fun_control=fun_control,\n",
        "            )\n",
        "X0, y0 = S.generate_random_point()\n",
        "print(f\"X0: {X0}\")\n",
        "print(f\"y0: {y0}\")\n",
        "assert X0.size == 2\n",
        "assert y0.size == 1\n",
        "assert np.all(X0 >= S.lower)\n",
        "assert np.all(X0 <= S.upper)\n",
        "assert y0 >= 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tensorboard Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import get_spot_tensorboard_path\n",
        "get_spot_tensorboard_path(\"00_ubuntu_2021-08-31_14-30-00\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.init import get_experiment_name\n",
        "get_experiment_name(prefix=\"00\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot Contour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "from spotpython.spot import spot\n",
        "from spotpython.utils.init import (\n",
        "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
        "    )\n",
        "# number of initial points:\n",
        "ni = 5\n",
        "# number of points\n",
        "fun_evals = 10\n",
        "fun = analytical().fun_sphere\n",
        "fun_control = fun_control_init(\n",
        "    lower = np.array([-1, -1, -1]),\n",
        "    upper = np.array([1, 1, 1]),\n",
        "    fun_evals=fun_evals,\n",
        "    tolerance_x = np.sqrt(np.spacing(1))\n",
        "    )\n",
        "design_control=design_control_init(init_size=ni)\n",
        "surrogate_control=surrogate_control_init(n_theta=3)\n",
        "S = spot.Spot(fun=fun,\n",
        "            fun_control=fun_control,\n",
        "            design_control=design_control,\n",
        "            surrogate_control=surrogate_control,)\n",
        "S.run()\n",
        "S.plot_important_hyperparameter_contour()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotpython.spot import spot\n",
        "from spotpython.utils.init import design_control_init\n",
        "from spotpython.fun.objectivefunctions import analytical\n",
        "design_control = design_control_init(init_size=3)\n",
        "fun_control = fun_control_init(\n",
        "    lower = np.array([-1, -1]),\n",
        "    upper = np.array([1, 1]),\n",
        "    fun_evals=fun_evals,\n",
        "    tolerance_x = np.sqrt(np.spacing(1))\n",
        "    )\n",
        "S = spot.Spot(fun = analytical().fun_sphere,\n",
        "              fun_control = fun_control,\n",
        "              design_control = design_control)\n",
        "X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n",
        "assert X.shape[0] == 3\n",
        "assert X.shape[1] == 2\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import inf\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from spotriver.utils.data_conversion import convert_to_df\n",
        "from river.datasets import synth\n",
        "import warnings\n",
        "if not os.path.exists('./figures'):\n",
        "    os.makedirs('./figures')\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "PREFIX=\"TEST_SAVE\"\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from river.datasets import synth\n",
        "from spotriver.utils.data_conversion import convert_to_df\n",
        "from math import inf\n",
        "import numpy as np\n",
        "from spotriver.fun.hyperriver import HyperRiver\n",
        "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
        "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
        "\n",
        "\n",
        "target_column = \"y\"\n",
        "metric = mean_absolute_error\n",
        "horizon = 7*24\n",
        "n_train = horizon\n",
        "p_1 = int(n_train/4)\n",
        "p_2 = int(n_train/2)\n",
        "position=(p_1, p_2)\n",
        "dataset_train = synth.FriedmanDrift(\n",
        "   drift_type='gra',\n",
        "   position=position,\n",
        "   seed=123\n",
        ")\n",
        "\n",
        "train = convert_to_df(dataset_train, n_total=n_train)\n",
        "train.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n",
        "\n",
        "\n",
        "n_val = 10_000\n",
        "p_1 = int(n_val/4)\n",
        "p_2 = int(n_val/2)\n",
        "position=(p_1, p_2)\n",
        "dataset_val = synth.FriedmanDrift(\n",
        "   drift_type='gra',\n",
        "   position=position,\n",
        "   seed=124\n",
        ")\n",
        "val = convert_to_df(dataset_val, n_total=n_val)\n",
        "val.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n",
        "\n",
        "from math import inf\n",
        "import numpy as np\n",
        "from spotriver.fun.hyperriver import HyperRiver\n",
        "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
        "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
        "\n",
        "fun = HyperRiver().fun_oml_horizon\n",
        "\n",
        "fun_control = fun_control_init(\n",
        "    PREFIX=PREFIX,\n",
        "    TENSORBOARD_CLEAN=False,\n",
        "    tensorboard_start=False,\n",
        "    tensorboard_stop=False,\n",
        "    fun_evals=inf,\n",
        "    max_time=0.1,\n",
        "\n",
        "    prep_model_name=\"StandardScaler\",\n",
        "    test=val, # tuner uses the validation set as test set\n",
        "    train=train,\n",
        "    target_column=target_column,\n",
        "\n",
        "    metric_sklearn_name=\"mean_absolute_error\",\n",
        "    horizon=7*24,\n",
        "    oml_grace_period=7*24,\n",
        "    weight_coeff=0.0,\n",
        "    weights=np.array([100, 0.1, 0.1]),\n",
        "\n",
        "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
        "    hyperdict=RiverHyperDict,\n",
        "   )\n",
        "\n",
        "\n",
        "design_control = design_control_init(\n",
        "    init_size=3,\n",
        ")\n",
        "\n",
        "surrogate_control = surrogate_control_init(\n",
        "    noise=True,\n",
        "    n_theta=2,\n",
        "    min_Lambda=0.001,\n",
        "    max_Lambda=100,\n",
        ")\n",
        "\n",
        "optimizer_control = optimizer_control_init()\n",
        "\n",
        "from spotpython.spot import spot\n",
        "spot_tuner = spot.Spot(\n",
        "    fun=fun,\n",
        "    fun_control=fun_control,\n",
        "    design_control=design_control,\n",
        "    surrogate_control=surrogate_control,\n",
        "    optimizer_control=optimizer_control,\n",
        ")\n",
        "res = spot_tuner.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotpython.utils.file import load_and_run_spot_python_experiment\n",
        "spot_tuner = load_and_run_spot_python_experiment(\"spot_000_experiment.pickle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lightning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _generate_div2_list(n, n_min) -> list:\n",
        "    \"\"\"\n",
        "    Generate a list of numbers from n to n_min (inclusive) by dividing n by 2\n",
        "    until the result is less than n_min.\n",
        "    This function starts with n and keeps dividing it by 2 until n_min is reached.\n",
        "    The number of times each value is added to the list is determined by n // current.\n",
        "\n",
        "    Args:\n",
        "        n (int): The number to start with.\n",
        "        n_min (int): The minimum number to stop at.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of numbers from n to n_min (inclusive).\n",
        "\n",
        "    Examples:\n",
        "        _generate_div2_list(10, 1)\n",
        "        [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "        _generate_div2_list(10, 2)\n",
        "        [10, 5, 5, 2, 2, 2, 2, 2]\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    current = n\n",
        "    repeats = 1\n",
        "    max_repeats = 4\n",
        "    while current >= n_min:\n",
        "        result.extend([current] * min(repeats, max_repeats))\n",
        "        current = current // 2\n",
        "        repeats = repeats + 1\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10, 5, 5]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_generate_div2_list(10, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[128, 64, 64, 32, 32, 32]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_generate_div2_list(128, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
