{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  cache: false\n",
    "  eval: true\n",
    "  echo: true\n",
    "  warning: false\n",
    "title: 'spotpython Tests'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fun_control_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n",
    "fun_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def class_attributes_to_dataframe(class_obj):\n",
    "    # Get the attributes and their values of the class object\n",
    "    attributes = [attr for attr in dir(class_obj) if not callable(getattr(class_obj, attr)) and not attr.startswith(\"__\")]\n",
    "    values = [getattr(class_obj, attr) for attr in attributes]\n",
    "    \n",
    "    # Create a DataFrame from the attributes and values\n",
    "    df = pd.DataFrame({'Attribute Name': attributes, 'Attribute Value': values})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.name = \"John\"\n",
    "        self.age = 30\n",
    "        self.salary = 50000\n",
    "\n",
    "my_instance = MyClass()\n",
    "df = class_attributes_to_dataframe(my_instance)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "n = 10\n",
    "\n",
    "fun = analytical().fun_sphere\n",
    "lower = np.array([-1])\n",
    "upper = np.array([1])\n",
    "design_control={\"init_size\": ni}\n",
    "\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            lower = lower,\n",
    "            upper= upper,\n",
    "            fun_evals = n,\n",
    "            show_progress=True,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "df = spot_1.class_attributes_to_dataframe()\n",
    "stdout.write(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river.linear_model import LogisticRegression\n",
    "from river import metrics\n",
    "from river import optim\n",
    "from river import preprocessing\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset, model, metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.csvdataset import CSVDataset\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data.csv', target_column='prognosis')\n",
    "dataset = CSVDataset(target_column='prognosis')\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.extra_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 3\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Data set VBDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv_file='./data/spotpython/data.csv' as a pandas df and save it as a pickle file\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/spotpython/data.csv')\n",
    "df.to_pickle('./data/spotpython/data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyHcf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.data.daten_sensitive import DatenSensitive\n",
    "from pyhcf.utils.names import get_short_parameter_names\n",
    "daten = DatenSensitive()\n",
    "df = daten.load()\n",
    "names =  df.columns\n",
    "names = get_short_parameter_names(names)\n",
    "# rename columns with short names\n",
    "df.columns = names\n",
    "df.head()\n",
    "# save the df as a csv file\n",
    "df.to_csv('./data/spotpython/data_sensitive.csv', index=False)\n",
    "# save the df as a pickle file\n",
    "df.to_pickle('./data/spotpython/data_sensitive.pkl')\n",
    "# remove all rows with NaN values\n",
    "df = df.dropna()\n",
    "# save the df as a csv file\n",
    "df.to_csv('./data/spotpython/data_sensitive_rmNA.csv', index=False)\n",
    "# save the df as a pickle file\n",
    "df.to_pickle('./data/spotpython/data_sensitive_rmNA.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyHcf data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.light.csvdataset import CSVDataset\n",
    "# import torch\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5000\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     # print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.light.csvdataset import CSVDataset\n",
    "# import torch\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5000\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     # print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset.feature_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(pkl_file='./data/spotpython/data_sensitive.pkl', target_column='A', feature_type=torch.long, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test lightdatamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "# dataset = PKLDataset(directory=\"./data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set size: {len(data_module.data_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation set size: {len(data_module.data_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the DataModule in fun_control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=7)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same with the sensitive data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same, but VBDO data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = CSVDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/VBDP/\", filename=\"train.csv\",target_column='prognosis', feature_type=torch.long)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Hyperdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "lhd = LightHyperDict()\n",
    "lhd.hyper_dict\n",
    "user_lhd = LightHyperDict(filename=\"user_hyper_dict.json\", directory=\"./hyperdict/\")\n",
    "user_lhd.hyper_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes  \n",
    "import torch\n",
    "\n",
    "# Load the diabetes dataset\n",
    "feature_df, target_df = load_diabetes(return_X_y=True, as_frame=True)\n",
    "feature_tensor = torch.tensor(feature_df.values, dtype=torch.float32)\n",
    "target_tensor = torch.tensor(target_df.values, dtype=torch.float32)\n",
    "feature_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "dataset = Diabetes()\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add core model to fun control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.netlightregressione import NetLightRegression\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "fun_control[\"core_model\"].__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the fun_control[\"core_model_hyper_dict\"] is a LightHyperDict\n",
    "isinstance(fun_control[\"core_model_hyper_dict\"], dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test check_X_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "n_hyperparams = len(get_var_name(fun_control))\n",
    "# generate a random np.array X with shape (2, n_hyperparams)\n",
    "X = np.random.rand(2, n_hyperparams)\n",
    "X == hyper_light.check_X_shape(X, fun_control)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test hyperlight fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                    key=\"data_set\",\n",
    "                    value=dataset)\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "X = np.vstack((X, X))\n",
    "y = hyper_light.fun(X, fun_control)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test  NetLightRegression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader)) \n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests optimizer_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "lr_mult=0.1\n",
    "\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adam', dropout_prob=0.1, lr_mult=lr_mult,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# Adam uses a lr which is calculated as lr=lr_mult * 0.001, so this value\n",
    "# should be 0.1 * 0.001 = 0.0001 \n",
    "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*0.001\n",
    "\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adadelta', dropout_prob=0.1, lr_mult=lr_mult,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# Adadelta uses a lr which is calculated as lr=lr_mult * 1.0, so this value\n",
    "# should be 1.0 * 0.1 = 0.1 \n",
    "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
    "from spotpython.light.traintest import train_model, test_model\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "for config in generate_one_config_from_var_dict(var_dict, fun_control):\n",
    "    y_train = train_model(config, fun_control)\n",
    "    y_test = test_model(config, fun_control)\n",
    "    break\n",
    "print(y_train)\n",
    "print(y_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
    "from spotpython.light.traintest import test_model\n",
    "\n",
    "\n",
    "def test_traintest_test_model():\n",
    "    fun_control = fun_control_init(\n",
    "        _L_in=10,\n",
    "        _L_out=1,)\n",
    "\n",
    "    dataset = Diabetes()\n",
    "    set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "    add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                                fun_control=fun_control,\n",
    "                                hyper_dict=LightHyperDict)\n",
    "    X = get_default_hyperparameters_as_array(fun_control)\n",
    "    var_dict = assign_values(X, get_var_name(fun_control))\n",
    "    for vals in generate_one_config_from_var_dict(var_dict, fun_control):\n",
    "        y_test = test_model(test_config=vals,\n",
    "                            fun_control=fun_control)\n",
    "        break\n",
    "    # check if y is a float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test getVarName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = {\"core_model_hyper_dict\":{\n",
    "            \"leaf_prediction\": {\n",
    "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"mean\",\n",
    "                \"core_model_parameter_type\": \"str\"},\n",
    "            \"leaf_model\": {\n",
    "                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"LinearRegression\",\n",
    "                \"core_model_parameter_type\": \"instance\"},\n",
    "            \"splitter\": {\n",
    "                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"EBSTSplitter\",\n",
    "                \"core_model_parameter_type\": \"instance()\"},\n",
    "            \"binary_split\": {\n",
    "                \"levels\": [0, 1],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": 0,\n",
    "                \"core_model_parameter_type\": \"bool\"},\n",
    "            \"stop_mem_management\": {\n",
    "                \"levels\": [0, 1],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": 0,\n",
    "                \"core_model_parameter_type\": \"bool\"}}}\n",
    "len(get_var_name(fun_control))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test netlightregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from math import inf\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import modify_hyper_parameter_bounds\n",
    "from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import (get_bound_values,\n",
    "    get_var_name,\n",
    "    get_var_type,)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.hyperparameters.values import get_tuned_architecture\n",
    "from spotpython.light.testmodel import test_model\n",
    "from spotpython.light.loadmodel import load_light_from_checkpoint\n",
    "\n",
    "MAX_TIME = 1\n",
    "INIT_SIZE = 5\n",
    "WORKERS = 0\n",
    "PREFIX=\"031\"\n",
    "\n",
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    num_workers=WORKERS,\n",
    "    device=getDevice(),\n",
    "    _L_in=133,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True)\n",
    "\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[5,8])\n",
    "modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[3,5])\n",
    "modify_hyper_parameter_bounds(fun_control, \"batch_size\", bounds=[2, 8])\n",
    "modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
    "\n",
    "print(gen_design_table(fun_control))\n",
    "\n",
    "var_type = get_var_type(fun_control)\n",
    "var_name = get_var_name(fun_control)\n",
    "lower = get_bound_values(fun_control, \"lower\")\n",
    "upper = get_bound_values(fun_control, \"upper\")\n",
    "fun = HyperLight(log_level=50).fun\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       log_level=50,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   max_time = MAX_TIME,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type = var_type,\n",
    "                   var_name = var_name,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": INIT_SIZE},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": len(var_name),\n",
    "                                      \"model_fun_evals\": 10_000,\n",
    "                                      })\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_progress(log_y=False, filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_importance(threshold=0.025, filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_tuned_architecture(spot_tuner, fun_control)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = load_light_from_checkpoint(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_important_hyperparameter_contour(filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.parallel_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.cvmodel import cv_model\n",
    "# set the number of folds to 10\n",
    "fun_control[\"k_folds\"] = 10\n",
    "cv_model(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "fun = analytical(sigma=1.0, seed=123)\n",
    "fun.add_noise(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "print(np.array([1, 2, 3, 4, 5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(seed=4321, sigma=0.1)\n",
    "fun = analytical(seed=222, sigma=0.0).fun_sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_1 = spot.Spot(fun=fun,\n",
    "                   lower = np.array([-10]),\n",
    "                   upper = np.array([100]),\n",
    "                   fun_evals = 100,\n",
    "                   fun_repeats = 3,\n",
    "                   max_time = inf,\n",
    "                   noise = True,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type=[\"num\"],\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=111,\n",
    "                   log_level = 10,\n",
    "                   show_models=False,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": 5,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": 1,\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 1000,\n",
    "                                      })\n",
    "spot_1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def squared_euclidean_distance(X_0, X, theta):\n",
    "    return np.sum(theta*(X_0 - X)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
    "from numpy.linalg import cholesky, solve\n",
    "from numpy.random import multivariate_normal\n",
    "def build_Psi(X, theta):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    D = zeros((k, n, n))\n",
    "    for l in range(k):\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
    "    D = sum(D)\n",
    "    D = D + D.T\n",
    "    return exp(-D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0, 1.0])\n",
    "X = np.array([[1.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_Psi(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n",
    "fun = analytical()\n",
    "fun.fun_branin_factor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pi = np.pi\n",
    "X = np.array([[0,0], [-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
    "fun = analytical()\n",
    "fun.fun_branin(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "pi = np.pi\n",
    "X_0 = np.array([[0, 0]])\n",
    "X_1 = np.array([[-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
    "X_2 = np.array([[0,0,0], [0,0,1], [0,0,2]])\n",
    "fun = analytical()\n",
    "y_0 = fun.fun_branin(X_0)\n",
    "y_1 = fun.fun_branin(X_1)\n",
    "y_2 = fun.fun_branin_factor(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(y_1[0], 2) == round(y_1[1],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "round(y_1[0], 2) == round(y_1[2],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[0] == y_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[1] == y_0 + 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[2] == y_0 - 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "import numpy as np\n",
    "n = 100\n",
    "X = np.linspace(0, 10, n, endpoint=False).reshape(-1,1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
    "from numpy.linalg import cholesky, solve\n",
    "from numpy.random import multivariate_normal\n",
    "def build_Psi(X, theta):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    D = zeros((k, n, n))\n",
    "    for l in range(k):\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
    "    D = sum(D)\n",
    "    D = D + D.T\n",
    "    return exp(-D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0])\n",
    "Psi = build_Psi(X, theta)\n",
    "np.round(Psi[:3,:], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = (3, 1, 1), check_valid=\"raise\")\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Y to a 3 x 100 array\n",
    "Y = np.squeeze(Y)\n",
    "Y.shape\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 samples from the GP as a function of X\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, Y.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = 3, check_valid=\"raise\")\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 samples from the GP as a function of X\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, Y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Targets Shape: {targets.shape}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test HyperLight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "n_hyperparams = len(get_var_name(fun_control))\n",
    "# generate a random np.array X with shape (2, n_hyperparams)\n",
    "X = np.random.rand(2, n_hyperparams)\n",
    "X == hyper_light.check_X_shape(X, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n",
    "    get_default_hyperparameters_as_array)\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "# so that two values are returned\n",
    "X = np.vstack((X, X))\n",
    "hyper_light.fun(X, fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test pkldataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\",\n",
    "                    filename=\"data_sensitive.pkl\",\n",
    "                    target_column='N',\n",
    "                    feature_type=torch.float32,\n",
    "                    target_type=torch.float32,\n",
    "                    rmNA=True)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_bound_values\n",
    "from spotpython.hyperparameters.values import get_control_key_value, set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_type_from_var_name\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "set_control_key_value(control_dict=fun_control, key=\"var_type\", value=[\"int\", \"float\", \"str\"], replace=True)\n",
    "set_control_key_value(control_dict=fun_control, key=\"var_name\", value=[\"max_depth\", \"learning_rate\", \"model_type\"], replace=True)\n",
    "\n",
    "print(fun_control)\n",
    "\n",
    "# Test with existing var_name\n",
    "assert get_var_type_from_var_name(var_name=\"max_depth\", fun_control=fun_control) == \"int\"\n",
    "assert get_var_type_from_var_name(var_name=\"learning_rate\", fun_control=fun_control) == \"float\"\n",
    "assert get_var_type_from_var_name(var_name=\"model_type\", fun_control=fun_control) == \"str\"\n",
    "\n",
    "# Test with non-existing var_name\n",
    "with pytest.raises(ValueError):\n",
    "    get_var_type_from_var_name(var_name=\"non_existing\", fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_control_key_value\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import get_var_type_from_var_name\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                    fun_control=fun_control,\n",
    "                    hyper_dict=LightHyperDict)\n",
    "var_type = get_control_key_value(control_dict=fun_control, key=\"var_type\")\n",
    "var_name = get_control_key_value(control_dict=fun_control, key=\"var_name\")\n",
    "print(var_type)\n",
    "print(var_name)\n",
    "vn = \"l1\"\n",
    "get_var_type_from_var_name(fun_control=fun_control, var_name=vn)\n",
    "\n",
    "assert var_type[var_name.index(vn)] == \"int\"\n",
    "assert get_var_type_from_var_name(fun_control, vn) == \"int\"\n",
    "vn = \"initialization\"\n",
    "assert var_type[var_name.index(vn)] == \"factor\"\n",
    "assert var_type[var_name.index(vn)] == \"factor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_control_key_value\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                    fun_control=fun_control,\n",
    "                    hyper_dict=LightHyperDict)\n",
    "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"l1\", value=[1,7])\n",
    "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"initialization\", value=[\"xavier2\", \"kaiming2\"])\n",
    "print(fun_control)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry(dictionary, key, i):\n",
    "    if key in dictionary:\n",
    "        if 'levels' in dictionary[key]:\n",
    "            if i < len(dictionary[key]['levels']):\n",
    "                return dictionary[key]['levels'][i]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from spotpython.data.pkldataset_intern import PKLDataset\n",
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from math import inf\n",
    "\n",
    "MAX_TIME = 60\n",
    "FUN_EVALS = inf\n",
    "INIT_SIZE = 25\n",
    "WORKERS = 0\n",
    "PREFIX=\"031\"\n",
    "DEVICE = getDevice()\n",
    "\n",
    "\n",
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    device=DEVICE,\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=FUN_EVALS,\n",
    "    log_level=10,\n",
    "    max_time=MAX_TIME,\n",
    "    num_workers=WORKERS,\n",
    "    show_progress=True,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "\n",
    "dataset = Diabetes()\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True, rmMF=True)\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"_L_in\",\n",
    "                        value=133,\n",
    "                        replace=True)\n",
    "\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "# from spotpython.hyperparameters.values import modify_hyper_parameter_bounds\n",
    "\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
    "set_control_hyperparameter_value(fun_control, \"epochs\", [4,9])\n",
    "set_control_hyperparameter_value(fun_control, \"batch_size\", [1, 4])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry(dictionary, key, i):\n",
    "    if 'core_model_hyper_dict' in dictionary:\n",
    "        if key in dictionary['core_model_hyper_dict']:\n",
    "            if 'levels' in dictionary['core_model_hyper_dict'][key]:\n",
    "                if i < len(dictionary['core_model_hyper_dict'][key]['levels']):\n",
    "                    return dictionary['core_model_hyper_dict'][key]['levels'][i]\n",
    "    return None\n",
    "print(get_entry(fun_control, \"optimizer\", 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "import numpy as np\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "experiment_name = get_experiment_name(prefix=\"000\")\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    device=getDevice(),\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=15,\n",
    "    log_level=10,\n",
    "    max_time=1,\n",
    "    num_workers=0,\n",
    "    show_progress=True,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
    "assert get_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0) == \"Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_timestamp(only_int=True):\n",
    "    dt = datetime.datetime.now().isoformat(sep=\" \", timespec=\"microseconds\")\n",
    "    if only_int:\n",
    "        # remove - . : and space\n",
    "        dt = dt.replace(\"-\", \"\")\n",
    "        dt = dt.replace(\".\", \"\")\n",
    "        dt = dt.replace(\":\", \"\")\n",
    "        dt = dt.replace(\" \", \"\")\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, surrogate_control_init, design_control_init\n",
    ")\n",
    "\n",
    "def test_plot_progress():\n",
    "    # number of initial points:\n",
    "    ni = 7\n",
    "    # number of points\n",
    "    fun_evals = 10\n",
    "    fun = analytical().fun_sphere\n",
    "    fun_control = fun_control_init(\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        fun_evals=fun_evals,\n",
    "        tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "    design_control=design_control_init(init_size=ni)\n",
    "    surrogate_control=surrogate_control_init(n_theta=3)\n",
    "    S = spot.Spot(fun=fun,\n",
    "                    fun_control=fun_control,\n",
    "                    design_control=design_control,\n",
    "                    surrogate_control=surrogate_control,)\n",
    "    S.run()\n",
    "\n",
    "    # Test plot_progress with different parameters\n",
    "    S.plot_progress(show=False)  # Test with show=False\n",
    "    S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "    S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "    S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename\n",
    "    # add NaN to S.y at position 2\n",
    "    S.y[2] = np.nan\n",
    "    S.plot_progress(show=False)  # Test with show=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, surrogate_control_init, design_control_init\n",
    ")\n",
    "\n",
    "\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "                fun_control=fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "\n",
    "# remove points from S.y so that there are less than ni points\n",
    "S.y = S.y[:3]\n",
    "# Test plot_progress with different parameters\n",
    "S.plot_progress(show=False)  # Test with show=False\n",
    "S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import differential_evolution\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "def objective_function(X, fun_control=None):\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X)\n",
    "    if X.shape[1] != 2:\n",
    "        raise Exception\n",
    "    x0 = X[:, 0]\n",
    "    x1 = X[:, 1]\n",
    "    y = x0**2 + 10*x1**2\n",
    "    return y\n",
    "fun_control = fun_control_init(\n",
    "            lower = np.array([0, 0]),\n",
    "            upper = np.array([10, 10]),\n",
    "            fun_evals=8,\n",
    "            fun_repeats=1,\n",
    "            max_time=inf,\n",
    "            noise=True,\n",
    "            tolerance_x=0,\n",
    "            ocba_delta=0,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            infill_criterion=\"ei\",\n",
    "            n_points=1,\n",
    "            seed=123,\n",
    "            log_level=10,\n",
    "            show_models=False,\n",
    "            show_progress=True)\n",
    "design_control = design_control_init(\n",
    "            init_size=5,\n",
    "            repeats=1)\n",
    "surrogate_control = surrogate_control_init(\n",
    "            log_level=10,\n",
    "            model_optimizer=differential_evolution,\n",
    "            model_fun_evals=10000,\n",
    "            min_theta=-3,\n",
    "            max_theta=3,\n",
    "            n_theta=2,\n",
    "            theta_init_zero=True,\n",
    "            n_p=1,\n",
    "            optim_p=False,\n",
    "            noise=True,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            seed=124, \n",
    "            min_Lambda=1,\n",
    "            max_Lambda=10)\n",
    "optimizer_control = optimizer_control_init(\n",
    "            max_iter=1000,\n",
    "            seed=125)\n",
    "spot = spot.Spot(fun=objective_function,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,\n",
    "            optimizer_control=optimizer_control\n",
    "            )\n",
    "spot.run()\n",
    "spot.plot_progress()\n",
    "spot.plot_contour(i=0, j=1)\n",
    "spot.plot_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n",
    "\n",
    "fun = analytical().fun_branin\n",
    "fun_control = fun_control_init(lower = np.array([-5, 0]),\n",
    "                               upper = np.array([10, 15]),\n",
    "                               fun_evals=20)\n",
    "design_control = design_control_init(init_size=10)\n",
    "surrogate_control = surrogate_control_init(n_theta=2)\n",
    "S = spot.Spot(fun=fun, fun_control=fun_control, design_control=design_control)\n",
    "S.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.plot_progress(log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.surrogate.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = analytical().fun_sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,100).reshape(-1,1)\n",
    "y = fun(x)\n",
    "plt.figure()\n",
    "plt.plot(x,y, \"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "                   fun_control=fun_control_init(\n",
    "                        lower = np.array([-10]),\n",
    "                        upper = np.array([100]),\n",
    "                        fun_evals = 7,\n",
    "                        fun_repeats = 1,\n",
    "                        max_time = inf,\n",
    "                        noise = False,\n",
    "                        tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                        var_type=[\"num\"],\n",
    "                        infill_criterion = \"y\",\n",
    "                        n_points = 1,\n",
    "                        seed=123,\n",
    "                        log_level = 50),\n",
    "                   design_control=design_control_init(\n",
    "                        init_size=5,\n",
    "                        repeats=1),\n",
    "                   surrogate_control=surrogate_control_init(\n",
    "                        noise=False,\n",
    "                        min_theta=-4,\n",
    "                        max_theta=3,\n",
    "                        n_theta=1,\n",
    "                        model_optimizer=differential_evolution,\n",
    "                        model_fun_evals=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "fun = analytical().fun_sphere\n",
    "from spotpython.design.spacefilling import spacefilling\n",
    "design = spacefilling(2)\n",
    "from scipy.optimize import differential_evolution\n",
    "optimizer = differential_evolution\n",
    "from spotpython.build.kriging import Kriging\n",
    "surrogate = Kriging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\n",
    "fun_control=fun_control_init(lower=np.array([-1, -1]),\n",
    "                            upper=np.array([1, 1]))\n",
    "design_control=design_control_init()\n",
    "optimizer_control=optimizer_control_init()\n",
    "surrogate_control=surrogate_control_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       fun_control=fun_control,\n",
    "                       design_control=design_control,\n",
    "                       optimizer_control=optimizer_control,\n",
    "                       surrogate_control=surrogate_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytest\n",
    "import torch\n",
    "from pyhcf.data.loadHcfData import build_df, load_hcf_data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list=[\"L\", \"AQ\", \"AS\"]\n",
    "dataset = load_hcf_data(param_list=p_list, target=\"T\",\n",
    "                        rmNA=True, rmMF=True,\n",
    "                        load_all_features=False,\n",
    "                        load_thermo_features=False,\n",
    "                        scale_data=True,\n",
    "                        return_X_y=False)\n",
    "assert isinstance(dataset, torch.utils.data.TensorDataset)\n",
    "assert len(dataset) > 0\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader    \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    assert inputs.size(0) == batch_size\n",
    "    print(f\"Inputs Shape: {inputs.shape[1]}\")\n",
    "    print(f\"P List: {p_list}\")\n",
    "    print(f\"P List Length: {len(p_list)}\")\n",
    "    # input is p_list + 1 (for target)\n",
    "    # p_list = [\"L\", \"AQ\", \"AS\"] plus target \"N\"\n",
    "    assert inputs.shape[1] + 1 == len(p_list)\n",
    "    print(f\"Targets Shape: {targets.shape[0]}\")\n",
    "    assert targets.shape[0] == batch_size\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "# data.csv is simple csv file with 11 samples\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup(stage=\"predict\")\n",
    "print(f\"Predict set size: {len(data_module.data_predict)}\")\n",
    "for batch in data_module.predict_dataloader():\n",
    "    inputs, targets = batch\n",
    "    print(f\"inputs: {inputs}\")\n",
    "    print(f\"targets: {targets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_module.data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_div2_list(n, n_min):\n",
    "    result = []\n",
    "    current = n\n",
    "    while current >= n_min:\n",
    "        result.extend([current] * (n // current))\n",
    "        current = current // 2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_L_in = 128\n",
    "l1 = \n",
    "\n",
    "n_low = _L_in // 4\n",
    "# ensure that n_high is larger than n_low\n",
    "n_high = max(l1, 2 * n_low)\n",
    "generate_div2_list(n_high, n_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.math import generate_div2_list\n",
    "generate_div2_list(64, 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.transformer.positionalEncoding import PositionalEncoding\n",
    "import torch\n",
    "# number of tensors\n",
    "n = 3\n",
    "# dimension of each tensor\n",
    "k = 32\n",
    "pe = PositionalEncoding(d_model=k, dropout_prob=0, verbose=False)\n",
    "input = torch.zeros(1, n, k)\n",
    "# Generate a tensor of size (1, 10, 4) with values from 1 to 10\n",
    "for i in range(n):\n",
    "    input[0, i, :] = i\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Input: {input}\")\n",
    "output = pe(input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.transformer.skiplinear import SkipLinear\n",
    "import torch\n",
    "n_in = 2\n",
    "n_out = 4\n",
    "sl = SkipLinear(n_in, n_out)\n",
    "input = torch.zeros(1, n_in)\n",
    "for i in range(n_in):\n",
    "    input[0, i] = i\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Input: {input}\")\n",
    "output = sl(input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(sl.lst_modules)\n",
    "for i in sl.lst_modules:\n",
    "    print(f\"weights: {i.weights}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Example from J. Caffrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_income_transformer.py\n",
    "# predict income from sex, age, city, politics\n",
    "# PyTorch 2.0.0-CPU Anaconda3-2022.10  Python 3.9.13\n",
    "# Windows 10/11 \n",
    "\n",
    "# Transformer component for regression\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "\n",
    "device = T.device('cpu')  # apply to Tensor or Module\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class PeopleDataset(T.utils.data.Dataset):\n",
    "  def __init__(self, src_file):\n",
    "    # sex age   state   income   politics\n",
    "    # -1  0.27  0 1 0   0.7610   0 0 1\n",
    "    # +1  0.19  0 0 1   0.6550   1 0 0\n",
    "\n",
    "    tmp_x = np.loadtxt(src_file, usecols=[0,1,2,3,4,6,7,8],\n",
    "      delimiter=\",\", comments=\"#\", dtype=np.float32)\n",
    "    tmp_y = np.loadtxt(src_file, usecols=5, delimiter=\",\",\n",
    "      comments=\"#\", dtype=np.float32)\n",
    "    tmp_y = tmp_y.reshape(-1,1)  # 2D required\n",
    "\n",
    "    self.x_data = T.tensor(tmp_x, dtype=T.float32).to(device)\n",
    "    self.y_data = T.tensor(tmp_y, dtype=T.float32).to(device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    preds = self.x_data[idx]\n",
    "    incom = self.y_data[idx] \n",
    "    return (preds, incom)  # as a tuple\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class SkipLinear(T.nn.Module):\n",
    "\n",
    "  # -----\n",
    "\n",
    "  class Core(T.nn.Module):\n",
    "    def __init__(self, n):\n",
    "      super().__init__()\n",
    "      # 1 node to n nodes, n gte 2\n",
    "      self.weights = T.nn.Parameter(T.zeros((n,1),\n",
    "        dtype=T.float32))\n",
    "      self.biases = T.nn.Parameter(T.tensor(n,\n",
    "        dtype=T.float32))\n",
    "      lim = 0.01\n",
    "      T.nn.init.uniform_(self.weights, -lim, lim)\n",
    "      T.nn.init.zeros_(self.biases)\n",
    "\n",
    "    def forward(self, x):\n",
    "      wx= T.mm(x, self.weights.t())\n",
    "      v = T.add(wx, self.biases)\n",
    "      return v\n",
    "\n",
    "  # -----\n",
    "\n",
    "  def __init__(self, n_in, n_out):\n",
    "    super().__init__()\n",
    "    self.n_in = n_in; self.n_out = n_out\n",
    "    if n_out  % n_in != 0:\n",
    "      print(\"FATAL: n_out must be divisible by n_in\")\n",
    "    n = n_out // n_in  # num nodes per input\n",
    "\n",
    "    self.lst_modules = \\\n",
    "      T.nn.ModuleList([SkipLinear.Core(n) for \\\n",
    "        i in range(n_in)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    lst_nodes = []\n",
    "    for i in range(self.n_in):\n",
    "      xi = x[:,i].reshape(-1,1)\n",
    "      oupt = self.lst_modules[i](xi)\n",
    "      lst_nodes.append(oupt)\n",
    "    result = T.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "    for i in range(2,self.n_in):\n",
    "      result = T.cat((result, lst_nodes[i]), 1)\n",
    "    result = result.reshape(-1, self.n_out)\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class PositionalEncoding(T.nn.Module):  # documentation code\n",
    "  def __init__(self, d_model: int, dropout: float=0.1,\n",
    "   max_len: int=5000):\n",
    "    super(PositionalEncoding, self).__init__()  # old syntax\n",
    "    self.dropout = T.nn.Dropout(p=dropout)\n",
    "    pe = T.zeros(max_len, d_model)  # like 10x4\n",
    "    position = \\\n",
    "      T.arange(0, max_len, dtype=T.float).unsqueeze(1)\n",
    "    div_term = T.exp(T.arange(0, d_model, 2).float() * \\\n",
    "      (-np.log(10_000.0) / d_model))\n",
    "    pe[:, 0::2] = T.sin(position * div_term)\n",
    "    pe[:, 1::2] = T.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    self.register_buffer('pe', pe)  # allows state-save\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:x.size(0), :]\n",
    "    return self.dropout(x)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class TransformerNet(T.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TransformerNet, self).__init__()\n",
    "    self.embed = SkipLinear(8, 32)  # 8 inputs, each goes to 4 \n",
    "    self.pos_enc = \\\n",
    "      PositionalEncoding(4, dropout=0.20)  # positional\n",
    "    self.enc_layer = T.nn.TransformerEncoderLayer(d_model=4,\n",
    "      nhead=2, dim_feedforward=10, \n",
    "      batch_first=True)  # d_model divisible by nhead\n",
    "    self.trans_enc = T.nn.TransformerEncoder(self.enc_layer,\n",
    "      num_layers=2)  # 6 layers default\n",
    "\n",
    "    self.fc1 = T.nn.Linear(32, 10)  # 8--32-T-10-1\n",
    "    self.fc2 = T.nn.Linear(10, 1)\n",
    "\n",
    "    # default weight and bias initialization\n",
    "\n",
    "  def forward(self, x):\n",
    "    z = self.embed(x)  # 8 inpts to 32 embed\n",
    "    z = z.reshape(-1, 8, 4)  # bat seq embed\n",
    "    z = self.pos_enc(z) \n",
    "    z = self.trans_enc(z) \n",
    "    z = z.reshape(-1, 32)  # torch.Size([bs, xxx])\n",
    "    z = T.tanh(self.fc1(z))\n",
    "    z = self.fc2(z)  # regression: no activation\n",
    "    return z\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def accuracy(model, ds, pct_close):\n",
    "  # assumes model.eval()\n",
    "  # correct within pct of true income\n",
    "  n_correct = 0; n_wrong = 0\n",
    "\n",
    "  for i in range(len(ds)):\n",
    "    X = ds[i][0].reshape(1,-1)  # make it a batch\n",
    "    Y = ds[i][1].reshape(1)\n",
    "    with T.no_grad():\n",
    "      oupt = model(X)         # computed income\n",
    "\n",
    "    if T.abs(oupt - Y) <= T.abs(pct_close * Y):\n",
    "      n_correct += 1\n",
    "    else:\n",
    "      n_wrong += 1\n",
    "  acc = (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "  return acc\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def accuracy_x(model, ds, pct_close):\n",
    "  # all-at-once (quick)\n",
    "  # assumes model.eval()\n",
    "  X = ds.x_data  # all inputs\n",
    "  Y = ds.y_data  # all targets\n",
    "  n_items = len(X)\n",
    "  with T.no_grad():\n",
    "    pred = model(X)  # all predicted incomes\n",
    " \n",
    "  n_correct = T.sum((T.abs(pred - Y) <= \\\n",
    "    T.abs(pct_close * Y)))\n",
    "  result = (n_correct.item() / n_items)  # scalar\n",
    "  return result  \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def train(model, ds, bs, lr, me, le, test_ds):\n",
    "  # dataset, bat_size, lrn_rate, max_epochs, log interval\n",
    "  train_ldr = T.utils.data.DataLoader(ds, batch_size=bs,\n",
    "    shuffle=True)\n",
    "  loss_func = T.nn.MSELoss()\n",
    "  optimizer = T.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "  for epoch in range(0, me):\n",
    "    epoch_loss = 0.0  # for one full epoch\n",
    "    for (b_idx, batch) in enumerate(train_ldr):\n",
    "      X = batch[0]  # predictors\n",
    "      y = batch[1]  # target income\n",
    "      optimizer.zero_grad()\n",
    "      oupt = model(X)\n",
    "      loss_val = loss_func(oupt, y)  # a tensor\n",
    "      epoch_loss += loss_val.item()  # accumulate\n",
    "      loss_val.backward()  # compute gradients\n",
    "      optimizer.step()     # update weights\n",
    "\n",
    "    if epoch % le == 0:\n",
    "      print(\"epoch = %4d  |  loss = %0.4f\" % \\\n",
    "        (epoch, epoch_loss))\n",
    "      # model.eval()\n",
    "      # print(\"-------------\")\n",
    "      # acc_train = accuracy(model, ds, 0.10)\n",
    "      # print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
    "      # acc_test = accuracy(model, test_ds, 0.10) \n",
    "      # print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
    "      # model.train()\n",
    "      # print(\"-------------\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "  # 0. get started\n",
    "  print(\"\\nBegin People predict income using Transformer \")\n",
    "  T.manual_seed(0)\n",
    "  np.random.seed(0)\n",
    "  \n",
    "\n",
    "\n",
    "  # 1. create Dataset objects\n",
    "  print(\"\\nCreating People Dataset objects \")\n",
    "  train_file = \"../src/spotpython/data/people_train.csv\"\n",
    "  train_ds = PeopleDataset(train_file)  # 200 rows\n",
    "\n",
    "  test_file = \"../src/spotpython/data/people_test.csv\"\n",
    "  test_ds = PeopleDataset(test_file)  # 40 rows\n",
    "\n",
    "  # 2. create network\n",
    "  print(\"\\nCreating (8--32)-T-10-1 neural network \")\n",
    "  net = TransformerNet().to(device)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 3. train model\n",
    "  print(\"\\nbat_size = 10 \")\n",
    "  print(\"loss = MSELoss() \")\n",
    "  print(\"optimizer = Adam \")\n",
    "  print(\"lrn_rate = 0.01 \")\n",
    "\n",
    "  print(\"\\nStarting training\")\n",
    "  net.train()\n",
    "  train(net, train_ds, bs=10, lr=0.01, me=300,\n",
    "    le=50, test_ds=test_ds)\n",
    "  print(\"Done \")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 4. evaluate model accuracy\n",
    "  print(\"\\nComputing model accuracy (within 0.10 of true) \")\n",
    "  net.eval()\n",
    "  acc_train = accuracy(net, train_ds, 0.10)  # item-by-item\n",
    "  print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
    "\n",
    "  acc_test = accuracy_x(net, test_ds, 0.10)  # all-at-once\n",
    "  print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 5. make a prediction\n",
    "  print(\"\\nPredicting income for M 34 Oklahoma moderate: \")\n",
    "  x = np.array([[-1, 0.34, 0,0,1,  0,1,0]],\n",
    "    dtype=np.float32)\n",
    "  x = T.tensor(x, dtype=T.float32).to(device) \n",
    "\n",
    "  with T.no_grad():\n",
    "    pred_inc = net(x)\n",
    "  pred_inc = pred_inc.item()  # scalar\n",
    "  print(\"$%0.2f\" % (pred_inc * 100_000))  # un-normalized\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 6. save model (state_dict approach)\n",
    "  print(\"\\nSaving trained model state\")\n",
    "  fn = \".\\\\Models\\\\people_income_model.pt\"\n",
    "  T.save(net.state_dict(), fn)\n",
    "\n",
    "  # model = Net()\n",
    "  # model.load_state_dict(T.load(fn))\n",
    "  # use model to make prediction(s)\n",
    "\n",
    "  print(\"\\nEnd People income demo \")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SkipLinear(torch.nn.Module):\n",
    "    class Core(torch.nn.Module):\n",
    "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
    "\n",
    "        def __init__(self, n):\n",
    "            \"\"\"\n",
    "            Initialize the layer.\n",
    "\n",
    "            Args:\n",
    "                n (int): The number of output nodes.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
    "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
    "            lim = 0.01\n",
    "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
    "\n",
    "        def forward(self, x)->torch.Tensor:\n",
    "            \"\"\"\n",
    "            Forward pass through the layer.\n",
    "\n",
    "            Args:\n",
    "                x (torch.Tensor): The input tensor.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The output of the layer.\n",
    "            \"\"\"\n",
    "            return x @ self.weights.t() + self.biases\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        if n_out % n_in != 0:\n",
    "            raise ValueError(\"n_out % n_in != 0\")\n",
    "        n = n_out // n_in  # num nodes per input\n",
    "\n",
    "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        lst_nodes = []\n",
    "        for i in range(self.n_in):\n",
    "            xi = x[:, i].reshape(-1, 1)\n",
    "            oupt = self.lst_modules[i](xi)\n",
    "            lst_nodes.append(oupt)\n",
    "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "        for i in range(2, self.n_in):\n",
    "            result = torch.cat((result, lst_nodes[i]), 1)\n",
    "        result = result.reshape(-1, self.n_out)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipLinear(torch.nn.Module):\n",
    "\n",
    "    class Core(torch.nn.Module):\n",
    "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
    "\n",
    "        def __init__(self, n):\n",
    "            \"\"\"\n",
    "            Initialize the layer.\n",
    "\n",
    "            Args:\n",
    "                n (int): The number of output nodes.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
    "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
    "            lim = 0.01\n",
    "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
    "\n",
    "        def forward(self, x) -> torch.Tensor:\n",
    "            \"\"\"\n",
    "            Forward pass through the layer.\n",
    "\n",
    "            Args:\n",
    "                x (torch.Tensor): The input tensor.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The output of the layer.\n",
    "            \"\"\"\n",
    "            return x @ self.weights.t() + self.biases\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        if n_out % n_in != 0:\n",
    "            raise ValueError(\"n_out % n_in != 0\")\n",
    "        n = n_out // n_in  # num nodes per input\n",
    "\n",
    "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        lst_nodes = []\n",
    "        for i in range(self.n_in):\n",
    "            xi = x[:, i].reshape(-1, 1)\n",
    "            oupt = self.lst_modules[i](xi)\n",
    "            lst_nodes.append(oupt)\n",
    "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "        for i in range(2, self.n_in):\n",
    "            result = torch.cat((result, lst_nodes[i]), 1)\n",
    "        result = result.reshape(-1, self.n_out)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spotGUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import get_default_values, get_bound_values\n",
    "lhd = LightHyperDict()\n",
    "# generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
    "fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
    "get_default_values(fun_control)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import json\n",
    "from spotpython.hyperparameters.values import get_default_values, get_bound_values\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "\n",
    "\n",
    "def create_gui(model):\n",
    "    lhd = LightHyperDict()\n",
    "    # generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
    "    fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
    "\n",
    "    # Apply the functions to the dictionary\n",
    "    default_values = get_default_values(fun_control)\n",
    "    lower_bound_values = get_bound_values(fun_control, \"lower\")\n",
    "    upper_bound_values = get_bound_values(fun_control, \"upper\")\n",
    "\n",
    "    # Create a tkinter window\n",
    "    root = tk.Tk()\n",
    "\n",
    "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
    "    for i, (key, value) in enumerate(lhd.hyper_dict['NetLightRegression'].items()):\n",
    "            # Create a label with the key as text\n",
    "            label = tk.Label(root, text=key)\n",
    "            label.grid(row=i, column=0, sticky=\"W\")\n",
    "\n",
    "            # Create an entry with the default value as the default text\n",
    "            default_entry = tk.Entry(root)\n",
    "            default_entry.insert(0, value)\n",
    "            default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "        # add the lower bound values in column 2\n",
    "            lower_bound_entry = tk.Entry(root)\n",
    "            lower_bound_entry.insert(0, lower_bound_values[i])\n",
    "            lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
    "        # add the upper bound values in column 3\n",
    "            upper_bound_entry = tk.Entry(root)\n",
    "            upper_bound_entry.insert(0, upper_bound_values[i])\n",
    "            upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
    "\n",
    "    # Run the tkinter main loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Call the function to create the GUI\n",
    "create_gui(model = 'NetLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "\n",
    "\n",
    "def create_gui(model):\n",
    "    lhd = LightHyperDict()\n",
    "    dict =  lhd.hyper_dict[model]\n",
    "\n",
    "    \n",
    "    # Create a tkinter window\n",
    "    root = tk.Tk()\n",
    "\n",
    "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
    "    # TODO: Add labels to the column headers\n",
    "    for i, (key, value) in enumerate(dict.items()):            \n",
    "            if dict[key][\"type\"] == \"int\" or dict[key][\"type\"] == \"float\":\n",
    "                # Create a label with the key as text\n",
    "                label = tk.Label(root, text=key)\n",
    "                label.grid(row=i, column=0, sticky=\"W\")\n",
    "                # Create an entry with the default value as the default text\n",
    "                default_entry = tk.Entry(root)\n",
    "                default_entry.insert(0, dict[key][\"default\"])\n",
    "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "                # add the lower bound values in column 2\n",
    "                lower_bound_entry = tk.Entry(root)                \n",
    "                lower_bound_entry.insert(0, dict[key][\"lower\"])\n",
    "                lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
    "                # add the upper bound values in column 3\n",
    "                upper_bound_entry = tk.Entry(root)\n",
    "                upper_bound_entry.insert(0, dict[key][\"upper\"])\n",
    "                upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
    "            if dict[key][\"type\"] == \"factor\":        \n",
    "                # Create a label with the key as text\n",
    "                label = tk.Label(root, text=key)\n",
    "                label.grid(row=i, column=0, sticky=\"W\")\n",
    "                # Create an entry with the default value as the default text\n",
    "                default_entry = tk.Entry(root)\n",
    "                default_entry.insert(0, dict[key][\"default\"])\n",
    "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "                # add the lower bound values in column 2\n",
    "                factor_level_entry = tk.Entry(root)\n",
    "                # add a comma to each level\n",
    "                dict[key][\"levels\"] = \", \".join(dict[key][\"levels\"])                                \n",
    "                factor_level_entry.insert(0, dict[key][\"levels\"])\n",
    "                # TODO: Fix columnspan\n",
    "                factor_level_entry.grid(row=i, column=2, columnspan=2, sticky=\"W\")\n",
    "\n",
    "    # Run the tkinter main loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Call the function to create the GUI\n",
    "create_gui(model = 'NetLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gui(model = 'TransformerLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save Load Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spotpython.utils.file import save_experiment, load_experiment\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "\n",
    "def test_file_save_load():\n",
    "    fun = analytical().fun_branin\n",
    "\n",
    "    fun_control = fun_control_init(\n",
    "                PREFIX=\"branin\",\n",
    "                SUMMARY_WRITER=False,\n",
    "                lower = np.array([0, 0]),\n",
    "                upper = np.array([10, 10]),\n",
    "                fun_evals=8,\n",
    "                fun_repeats=1,\n",
    "                max_time=inf,\n",
    "                noise=False,\n",
    "                tolerance_x=0,\n",
    "                ocba_delta=0,\n",
    "                var_type=[\"num\", \"num\"],\n",
    "                infill_criterion=\"ei\",\n",
    "                n_points=1,\n",
    "                seed=123,\n",
    "                log_level=20,\n",
    "                show_models=False,\n",
    "                show_progress=True)\n",
    "    design_control = design_control_init(\n",
    "                init_size=5,\n",
    "                repeats=1)\n",
    "    surrogate_control = surrogate_control_init(\n",
    "                model_fun_evals=10000,\n",
    "                min_theta=-3,\n",
    "                max_theta=3,\n",
    "                n_theta=2,\n",
    "                theta_init_zero=True,\n",
    "                n_p=1,\n",
    "                optim_p=False,\n",
    "                var_type=[\"num\", \"num\"],\n",
    "                seed=124)\n",
    "    optimizer_control = optimizer_control_init(\n",
    "                max_iter=1000,\n",
    "                seed=125)\n",
    "    spot_tuner = spot.Spot(fun=fun,\n",
    "                fun_control=fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control,\n",
    "                optimizer_control=optimizer_control)\n",
    "    # Call the save_experiment function\n",
    "    filename = save_experiment(\n",
    "        spot_tuner=spot_tuner,\n",
    "        fun_control=fun_control,\n",
    "        design_control=None,\n",
    "        surrogate_control=None,\n",
    "        optimizer_control=None\n",
    "    )\n",
    "\n",
    "    # Verify that the pickle file is created\n",
    "    assert os.path.exists(filename)\n",
    "\n",
    "    # Call the load_experiment function\n",
    "    spot_tuner_1, fun_control_1, design_control_1, surrogate_control_1, optimizer_control_1 = load_experiment(filename)\n",
    "\n",
    "    # Verify the name of the pickle file\n",
    "    assert filename == f\"spot_{fun_control['PREFIX']}experiment.pickle\"\n",
    "\n",
    "    # Clean up the temporary directory\n",
    "    os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_save_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netlightregression2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression2 import NetLightRegression2\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "BATCH_SIZE = 8\n",
    "dataset = Diabetes()\n",
    "train1_set, test_set = torch.utils.data.random_split(dataset, [0.6, 0.4])\n",
    "train_set, val_set = torch.utils.data.random_split(train1_set, [0.6, 0.4])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(f\"batch_x.shape: {batch_x.shape}\")\n",
    "print(f\"batch_y.shape: {batch_y.shape}\")\n",
    "net_light_base = NetLightRegression2(l1=128,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='Default',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=10,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=10,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "# data.csv is simple csv file with 11 samples\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from spotpython.utils.file import save_experiment, load_experiment\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "fun = analytical().fun_branin\n",
    "fun_control = fun_control_init(\n",
    "            PREFIX=\"branin\",\n",
    "            SUMMARY_WRITER=False,\n",
    "            lower = np.array([0, 0]),\n",
    "            upper = np.array([10, 10]),\n",
    "            fun_evals=8,\n",
    "            fun_repeats=1,\n",
    "            max_time=inf,\n",
    "            noise=False,\n",
    "            tolerance_x=0,\n",
    "            ocba_delta=0,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            infill_criterion=\"ei\",\n",
    "            n_points=1,\n",
    "            seed=123,\n",
    "            log_level=20,\n",
    "            show_models=False,\n",
    "            show_progress=True)\n",
    "design_control = design_control_init(\n",
    "            init_size=5,\n",
    "            repeats=1)\n",
    "surrogate_control = surrogate_control_init(\n",
    "            model_fun_evals=10000,\n",
    "            min_theta=-3,\n",
    "            max_theta=3,\n",
    "            n_theta=2,\n",
    "            theta_init_zero=True,\n",
    "            n_p=1,\n",
    "            optim_p=False,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            seed=124)\n",
    "optimizer_control = optimizer_control_init(\n",
    "            max_iter=1000,\n",
    "            seed=125)\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,\n",
    "            optimizer_control=optimizer_control)\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters(fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tuned Hyperparameters from a Machine/Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.device import getDevice\n",
    "from math import inf\n",
    "from spotpython.utils.init import fun_control_init\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "\n",
    "MAX_TIME = 1\n",
    "FUN_EVALS = 10\n",
    "INIT_SIZE = 5\n",
    "WORKERS = 0\n",
    "PREFIX=\"037\"\n",
    "DEVICE = getDevice()\n",
    "DEVICES = 1\n",
    "TEST_SIZE = 0.4\n",
    "TORCH_METRIC = \"mean_squared_error\"\n",
    "dataset = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    _torchmetric=TORCH_METRIC,\n",
    "    PREFIX=PREFIX,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    data_set=dataset,\n",
    "    device=DEVICE,\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=FUN_EVALS,\n",
    "    log_level=50,\n",
    "    max_time=MAX_TIME,\n",
    "    num_workers=WORKERS,\n",
    "    show_progress=True,\n",
    "    test_size=TEST_SIZE,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "add_core_model_to_fun_control(fun_control=fun_control,\n",
    "                              core_model=NetLightRegression,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n",
    "set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n",
    "set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\n",
    "                \"Adam\",\n",
    "                \"RAdam\",\n",
    "            ])\n",
    "set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n",
    "set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n",
    "set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n",
    "set_control_hyperparameter_value(fun_control, \"act_fn\",[\n",
    "                \"ReLU\",\n",
    "                \"LeakyReLU\"\n",
    "            ] )\n",
    "from spotpython.utils.init import design_control_init, surrogate_control_init\n",
    "design_control = design_control_init(init_size=INIT_SIZE)\n",
    "\n",
    "surrogate_control = surrogate_control_init(noise=True,\n",
    "                                            n_theta=2)\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "fun = HyperLight(log_level=50).fun\n",
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       fun_control=fun_control,\n",
    "                       design_control=design_control,\n",
    "                       surrogate_control=surrogate_control)\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors\n",
    "\n",
    "* Example from https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/012_num_spot_ei.html#factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.design.spacefilling import spacefilling\n",
    "from spotpython.build.kriging import Kriging\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = spacefilling(2)\n",
    "n = 30\n",
    "rng = np.random.RandomState(1)\n",
    "lower = np.array([-5,-0])\n",
    "upper = np.array([10,15])\n",
    "fun = analytical().fun_branin_factor\n",
    "#fun = analytical(sigma=0).fun_sphere\n",
    "\n",
    "X0 = gen.scipy_lhd(n, lower=lower, upper = upper)\n",
    "X1 = np.random.randint(low=0, high=3, size=(n,))\n",
    "X = np.c_[X0, X1]\n",
    "y = fun(X)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "S = Kriging(name='kriging',  seed=123, log_level=10, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\n",
    "S.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sf = Kriging(name='kriging',  seed=123, log_level=10, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"factor\"])\n",
    "# Sf = Kriging(name='kriging',  seed=123, log_level=50, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\n",
    "Sf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0\n",
    "for _ in range(100):\n",
    "    n = 100\n",
    "    X0 = gen.scipy_lhd(n, lower=lower, upper = upper)\n",
    "    X1 = np.random.randint(low=0, high=3, size=(n,))\n",
    "    X = np.c_[X0, X1]\n",
    "    y = fun(X)\n",
    "    s=np.sum(np.abs(S.predict(X) - y))\n",
    "    sf=np.sum(np.abs(Sf.predict(X) - y))\n",
    "    res = res + (sf - s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def select_distant_points(X, y, k):\n",
    "    \"\"\"\n",
    "    Selects k points that are distant from each other using a clustering approach.\n",
    "    \n",
    "    :param X: np.array of shape (n, k), with n points in k-dimensional space.\n",
    "    :param y: np.array of length n, with values corresponding to each point in X.\n",
    "    :param k: The number of distant points to select.\n",
    "    :return: Selected k points from X and their corresponding y values.\n",
    "    \"\"\"\n",
    "    # Perform k-means clustering to find k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n",
    "    \n",
    "    # Find the closest point in X to each cluster center\n",
    "    selected_points = np.array([X[np.argmin(np.linalg.norm(X - center, axis=1))] for center in kmeans.cluster_centers_])\n",
    "    \n",
    "    # Find indices of the selected points in the original X array\n",
    "    indices = np.array([np.where(np.all(X==point, axis=1))[0][0] for point in selected_points])\n",
    "    \n",
    "    # Select the corresponding y values\n",
    "    selected_y = y[indices]\n",
    "    \n",
    "    return selected_points, selected_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 2)  # Generate some random points\n",
    "y = np.random.rand(100)     # Random corresponding y values\n",
    "k = 5\n",
    "\n",
    "selected_points, selected_y = select_distant_points(X, y, k)\n",
    "print(\"Selected Points:\", selected_points)\n",
    "print(\"Corresponding y values:\", selected_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 5\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1, -1]),\n",
    "    upper = np.array([1, 1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "S.plot_important_hyperparameter_contour(max_imp=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n",
    "\n",
    "# Sorting the array in descending order by the second element of each sub-list\n",
    "sorted_array = sorted(array, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(sorted_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_second_and_return_indices(array):\n",
    "    \"\"\"\n",
    "    Sorts an array of arrays based on the second values in descending order and returns\n",
    "    the indices of the original array entries.\n",
    "\n",
    "    :param array: List of lists, where each inner list has at least two elements.\n",
    "    :return: Indices of the original array entries after sorting by the second value.\n",
    "             Returns an empty list if the input is empty or None.\n",
    "    :raises ValueError: If any sub-array is improperly structured.\n",
    "    \"\"\"\n",
    "    if not array:\n",
    "        return []\n",
    "\n",
    "    # Check for improperly structured sub-arrays\n",
    "    for item in array:\n",
    "        if not isinstance(item, list) or len(item) < 2:\n",
    "            raise ValueError(\"All sub-arrays must be lists with at least two elements.\")\n",
    "\n",
    "    # Enumerate the array to keep track of original indices, then sort by the second item\n",
    "    sorted_indices = [index for index, value in sorted(enumerate(array), key=lambda x: x[1][1], reverse=True)]\n",
    "\n",
    "    return sorted_indices\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n",
    "    indices = sort_by_second_and_return_indices(array)\n",
    "    print(\"Indices of the sorted elements:\", indices)\n",
    "except ValueError as error:\n",
    "    print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Core Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree import HoeffdingAdaptiveTreeRegressor\n",
    "from spotriver.data.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_for_core_model, get_default_values\n",
    "fun_control = {}\n",
    "add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=RiverHyperDict,\n",
    "    filename=None)\n",
    "values = get_default_values(fun_control)\n",
    "print(values)\n",
    "# get_default_hyperparameters_for_core_model(fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytest\n",
    "import pprint\n",
    "from spotpython.plot.xai import (get_activations, get_gradients, get_weights, plot_nn_values_hist, plot_nn_values_scatter, visualize_weights, visualize_gradients, visualize_activations, visualize_activations_distributions, visualize_gradient_distributions, visualize_weights_distributions)\n",
    "\n",
    "def test_plot_nn_values_scatter_reshaped_values():\n",
    "    # Mock data for testing\n",
    "    nn_values = {\n",
    "        'layer0': np.random.rand(10),  # 10 values suggesting padding for a 4x4\n",
    "        'layer1': np.random.rand(64),  # 64 values suggesting a perfect square (8x8)\n",
    "        'layer2': np.random.rand(32),  # 32 values suggesting  padding for a 6x6\n",
    "        'layer3': np.random.rand(16),  # 16 values suggesting a perfect square (4x4)\n",
    "    }\n",
    "\n",
    "    # Use the modified function that returns reshaped_values for testing\n",
    "    reshaped_values = plot_nn_values_scatter(nn_values, 'Test NN', return_reshaped=True)    \n",
    "\n",
    "    pprint.pprint(nn_values)\n",
    "    pprint.pprint(reshaped_values)\n",
    "    \n",
    "\n",
    "    # Assert for layer0: Checks if reshaping is correct for perfect square\n",
    "    assert reshaped_values['layer0'].shape == (4, 4)\n",
    "    # Assert for layer1: Checks if reshaping is correct for non-square\n",
    "    assert reshaped_values['layer1'].shape == (8, 8)\n",
    "    assert reshaped_values['layer2'].shape == (6, 6)\n",
    "    assert reshaped_values['layer3'].shape == (4, 4)\n",
    "\n",
    "\n",
    "\n",
    "test_plot_nn_values_scatter_reshaped_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.convert import set_dataset_target_type\n",
    "import pandas as pd\n",
    "dataset = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9], \"y\": [True, False, True]})\n",
    "print(dataset)\n",
    "dataset = set_dataset_target_type(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import river.tree\n",
    "core_model_name = \"tree.HoeffdingTreeRegressor\"\n",
    "core_model_module = core_model_name.split(\".\")[0]\n",
    "coremodel = core_model_name.split(\".\")[1]\n",
    "core_model_instance = getattr(getattr(river, core_model_module), coremodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.friedman import FriedmanDriftDataset\n",
    "import matplotlib.pyplot as plt\n",
    "data_generator = FriedmanDriftDataset(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\n",
    "data = [data for data in data_generator]\n",
    "indices = [i for _, _, i in data]\n",
    "values = {f\"x{i}\": [] for i in range(5)}\n",
    "values[\"y\"] = []\n",
    "for x, y, _ in data:\n",
    "    for i in range(5):\n",
    "        values[f\"x{i}\"].append(x[i])\n",
    "    values[\"y\"].append(y)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for label, series in values.items():\n",
    "    plt.plot(indices, series, label=label)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('')\n",
    "plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n",
    "plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scaler for Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from spotpython.utils.scaler import TorchStandardScaler\n",
    "import torch\n",
    "\n",
    "scaler=TorchStandardScaler()\n",
    "\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.float64)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "import numpy as np\n",
    "np.max(diabetes.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "dataset = Diabetes()\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 1\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "dataset = CaliforniaHousing()\n",
    "print(dataset.get_names())\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "dataset = CaliforniaHousing()\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "import torch\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "import torch\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=2, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n",
    "# print the first batch of the training set from data_module.data_train\n",
    "print(next(iter(data_module.train_dataloader())))\n",
    "# print the first batch of the training set from data_module.data_train as a numpy array\n",
    "print(next(iter(data_module.train_dataloader()))[0].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.utils.scaler import TorchStandardScaler, TorchMinMaxScaler\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "\n",
    "\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "scaler = TorchMinMaxScaler()\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "\n",
    "loader = data_module.train_dataloader\n",
    "\n",
    "total_sum = None\n",
    "total_count = 0\n",
    "\n",
    "# Iterate over batches in the DataLoader\n",
    "for batch in loader():\n",
    "    inputs, targets = batch\n",
    "    \n",
    "\n",
    "total_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "def test_net_light_regression_class():\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "    dataset = Diabetes()\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    net_light_regression = NetLightRegression(\n",
    "        l1=128,\n",
    "        epochs=10,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        initialization=\"Default\",\n",
    "        act_fn=nn.ReLU(),\n",
    "        optimizer=\"Adam\",\n",
    "        dropout_prob=0.1,\n",
    "        lr_mult=0.1,\n",
    "        patience=5,\n",
    "        _L_in=10,\n",
    "        _L_out=1,\n",
    "        _torchmetric=\"mean_squared_error\",\n",
    "    )\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=2,\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer.fit(net_light_regression, train_loader, val_loader)\n",
    "    res = trainer.test(net_light_regression, test_loader)\n",
    "    # test if the entry 'hp_metric' is in the res dict\n",
    "    assert \"hp_metric\" in res[0].keys()\n",
    "\n",
    "test_net_light_regression_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_int_hyperparameter_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_int_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"forest.AMFRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_int_hyperparameter_values(fun_control, \"n_estimators\", 2, 5)\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_factor_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_factor_hyperparameter_values(fun_control, \"leaf_model\", ['LinearRegression',\n",
    "                                                     'Perceptron'])\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "\n",
    "set_factor_hyperparameter_values(fun_control, \"leaf_model\", [\"LinearRegression\",\n",
    "                                                                \"Perceptron\"])\n",
    "\n",
    "# Access updated hyperparameters\n",
    "updated_hyperparameters = fun_control[\"core_model_hyper_dict\"]\n",
    "print(updated_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_boolean_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"forest.AMFRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_boolean_hyperparameter_values(fun_control, \"use_aggregation\", 0, 0)\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, full_dataset, train_size=0.8, batch_size=32, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.dataset = full_dataset\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Split the dataset\n",
    "        train_len = int(len(self.dataset) * self.train_size)\n",
    "        val_len = len(self.dataset) - train_len\n",
    "        self.train_set, self.val_set = random_split(self.dataset, [train_len, val_len])\n",
    "        \n",
    "        # Fit scaler on training data\n",
    "        train_data = torch.stack([item[0] for item in self.train_set])\n",
    "        print(f\"train_data before scaling\\n: {train_data}\")  \n",
    "        self.scaler.fit(train_data)\n",
    "       \n",
    "        # Transform training data\n",
    "        scaled_train_data = self.scaler.transform(train_data)\n",
    "        self.train_set = self._update_dataset(self.train_set, scaled_train_data)\n",
    "        print(f\"train_data after scaling\\n: {self.train_set}\")  \n",
    "        \n",
    "        # Transform validation data\n",
    "        val_data = torch.stack([item[0] for item in self.val_set])\n",
    "        scaled_val_data = self.scaler.transform(val_data)\n",
    "        self.val_set = self._update_dataset(self.val_set, scaled_val_data)\n",
    "\n",
    "    def _update_dataset(self, original_dataset, scaled_data):\n",
    "        updated_dataset = []\n",
    "        for i, (data, label) in enumerate(original_dataset):\n",
    "            updated_dataset.append((torch.tensor(scaled_data[i]), label))\n",
    "        return updated_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_data = torch.stack([item[0] for item in self.test_set])\n",
    "        scaled_test_data = self.scaler.transform(test_data)\n",
    "        self.test_set = self._update_dataset(self.test_set, scaled_test_data)\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Here you can download datasets if needed\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a 3-dimensional tensor with 1000 samples\n",
    "n = 10\n",
    "data = torch.rand((n, 3))\n",
    "print(f\"data: {data}\")\n",
    "labels = torch.tensor([i % 2 for i in range(n)], dtype=torch.float32)\n",
    "print(f\"labels: {labels}\")\n",
    "full_dataset = MyDataset(data, labels)\n",
    "\n",
    "# Creating DataModule instance\n",
    "data_module = MyDataModule(full_dataset)\n",
    "\n",
    "# Setup the data module\n",
    "data_module.setup()\n",
    "\n",
    "# Example of fetching a single batch\n",
    "train_loader = data_module.train_dataloader()\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch data shape: {batch[0].shape}\")\n",
    "    x, y = batch\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, do not delete the following imports, they are needed for the function add_core_model_to_fun_control\n",
    "import river\n",
    "from river import forest, tree, linear_model, rules\n",
    "from river import preprocessing\n",
    "import sklearn.metrics\n",
    "import spotpython\n",
    "from spotpython.light import regression\n",
    "\n",
    "def get_core_model_from_name(core_model_name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns the river core model name and instance from a core model name.\n",
    "\n",
    "    Args:\n",
    "        core_model_name (str): The full name of the core model in the format 'module.Model'.\n",
    "\n",
    "    Returns:\n",
    "        (str, object): A tuple containing the core model name and an instance of the core model.\n",
    "    \"\"\"\n",
    "    # Split the model name into its components\n",
    "    name_parts = core_model_name.split(\".\")\n",
    "    \n",
    "    if len(name_parts) < 2:\n",
    "        raise ValueError(f\"Invalid core model name: {core_model_name}. Expected format: 'module.ModelName'.\")\n",
    "\n",
    "    module_name = name_parts[0]\n",
    "    model_name = name_parts[1]\n",
    "    \n",
    "    try:\n",
    "        # Try to get the model from the river library\n",
    "        core_model_instance = getattr(getattr(river, module_name), model_name)\n",
    "        return model_name, core_model_instance\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            # Try to get the model from the spotpython library\n",
    "            submodule_name = name_parts[1]\n",
    "            model_name = name_parts[2] if len(name_parts) == 3 else model_name\n",
    "            print(f\"module_name: {module_name}\")\n",
    "            print(f\"submodule_name: {submodule_name}\")\n",
    "            print(f\"model_name: {model_name}\")\n",
    "            core_model_instance = getattr(getattr(getattr(spotpython, module_name), submodule_name), model_name)\n",
    "            return model_name, core_model_instance\n",
    "        except AttributeError:\n",
    "            raise ValueError(f\"Model '{core_model_name}' not found in either 'river' or 'spotpython' libraries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of usage\n",
    "model_name, model_instance = get_core_model_from_name('tree.HoeffdingTreeRegressor')\n",
    "print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, model_instance = get_core_model_from_name(\"light.regression.NNLinearRegressor\")\n",
    "print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 8\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=10,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes(return_X_y=False, as_frame=True)\n",
    "# svaing the data to a csv file\n",
    "data.frame.to_csv('~/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "n_features = 2\n",
    "n_samples = 500\n",
    "target_column = \"y\"\n",
    "ds =  make_moons(n_samples, noise=0.5, random_state=0)\n",
    "X, y = ds\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1))))\n",
    "test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1))))\n",
    "train.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "test.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "train.head()\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.concat([train, test])\n",
    "data.to_csv('moon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_classification(n_samples=1000, n_features=20,  n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('binary_classification.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_classification(n_samples=1000, n_features=20,  n_informative=9, n_redundant=2, n_repeated=0, n_classes=10, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('multiple_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('regression.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "data = load_iris(as_frame=True)\n",
    "data.frame.to_csv('iris.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotGUI.tuner.spotRun import get_report_file_name\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(PREFIX=\"test\")\n",
    "get_report_file_name(fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotGUI.tuner.spotRun import get_scenario_dict\n",
    "import pprint\n",
    "dic = get_scenario_dict(\"sklearn\")\n",
    "pprint.pprint(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.utils.io import load_hcf_dataframe, hcf_df2tensor\n",
    "from pyhcf.utils.names import load_all_features_N_regression_list\n",
    "from torch.utils.data import DataLoader\n",
    "df = load_hcf_dataframe(A=True,\n",
    "    H=True,\n",
    "    param_list=load_all_features_N_regression_list(),\n",
    "    target='N',\n",
    "    rmNA=True,\n",
    "    rmMF=True,\n",
    "    rmV=4,\n",
    "    min_freq=1000,\n",
    "    incl_drossel=False)\n",
    "dataset = hcf_df2tensor(df, target='N', return_X_y=False)\n",
    "print(type(dataset))\n",
    "print(len(dataset))\n",
    "# save the 'TensorDataset' object to a pkl file\n",
    "# import pickle\n",
    "# with open('hcf_dataset.pkl', 'wb') as f:\n",
    "#     pickle.dump(dataset, f)\n",
    "# load the 'TensorDataset' object from the pkl file\n",
    "# with open('hcf_dataset.pkl', 'rb') as f:\n",
    "#     dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0)\n",
    "# get the dimensions of the first sample\n",
    "dataset.__getitem__(0)[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "X0, y0 = S.generate_random_point()\n",
    "print(f\"X0: {X0}\")\n",
    "print(f\"y0: {y0}\")\n",
    "assert X0.size == 2\n",
    "assert y0.size == 1\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "assert y0 >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import get_spot_tensorboard_path\n",
    "get_spot_tensorboard_path(\"00_ubuntu_2021-08-31_14-30-00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import get_experiment_name\n",
    "get_experiment_name(prefix=\"00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 5\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1, -1]),\n",
    "    upper = np.array([1, 1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "S.plot_important_hyperparameter_contour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import design_control_init\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "design_control = design_control_init(init_size=3)\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "S = spot.Spot(fun = analytical().fun_sphere,\n",
    "              fun_control = fun_control,\n",
    "              design_control = design_control)\n",
    "X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n",
    "assert X.shape[0] == 3\n",
    "assert X.shape[1] == 2\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import inf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from spotriver.utils.data_conversion import convert_to_df\n",
    "from river.datasets import synth\n",
    "import warnings\n",
    "if not os.path.exists('./figures'):\n",
    "    os.makedirs('./figures')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "PREFIX=\"TEST_SAVE\"\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from river.datasets import synth\n",
    "from spotriver.utils.data_conversion import convert_to_df\n",
    "from math import inf\n",
    "import numpy as np\n",
    "from spotriver.fun.hyperriver import HyperRiver\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "\n",
    "\n",
    "target_column = \"y\"\n",
    "metric = mean_absolute_error\n",
    "horizon = 7*24\n",
    "n_train = horizon\n",
    "p_1 = int(n_train/4)\n",
    "p_2 = int(n_train/2)\n",
    "position=(p_1, p_2)\n",
    "dataset_train = synth.FriedmanDrift(\n",
    "   drift_type='gra',\n",
    "   position=position,\n",
    "   seed=123\n",
    ")\n",
    "\n",
    "train = convert_to_df(dataset_train, n_total=n_train)\n",
    "train.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n",
    "\n",
    "\n",
    "n_val = 10_000\n",
    "p_1 = int(n_val/4)\n",
    "p_2 = int(n_val/2)\n",
    "position=(p_1, p_2)\n",
    "dataset_val = synth.FriedmanDrift(\n",
    "   drift_type='gra',\n",
    "   position=position,\n",
    "   seed=124\n",
    ")\n",
    "val = convert_to_df(dataset_val, n_total=n_val)\n",
    "val.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n",
    "\n",
    "from math import inf\n",
    "import numpy as np\n",
    "from spotriver.fun.hyperriver import HyperRiver\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "\n",
    "fun = HyperRiver().fun_oml_horizon\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    TENSORBOARD_CLEAN=False,\n",
    "    tensorboard_start=False,\n",
    "    tensorboard_stop=False,\n",
    "    fun_evals=inf,\n",
    "    max_time=0.1,\n",
    "\n",
    "    prep_model_name=\"StandardScaler\",\n",
    "    test=val, # tuner uses the validation set as test set\n",
    "    train=train,\n",
    "    target_column=target_column,\n",
    "\n",
    "    metric_sklearn_name=\"mean_absolute_error\",\n",
    "    horizon=7*24,\n",
    "    oml_grace_period=7*24,\n",
    "    weight_coeff=0.0,\n",
    "    weights=np.array([100, 0.1, 0.1]),\n",
    "\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    "   )\n",
    "\n",
    "\n",
    "design_control = design_control_init(\n",
    "    init_size=3,\n",
    ")\n",
    "\n",
    "surrogate_control = surrogate_control_init(\n",
    "    noise=True,\n",
    "    n_theta=2,\n",
    "    min_Lambda=0.001,\n",
    "    max_Lambda=100,\n",
    ")\n",
    "\n",
    "optimizer_control = optimizer_control_init()\n",
    "\n",
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control,\n",
    "    surrogate_control=surrogate_control,\n",
    "    optimizer_control=optimizer_control,\n",
    ")\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.file import load_and_run_spot_python_experiment\n",
    "spot_tuner = load_and_run_spot_python_experiment(\"spot_000_experiment.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_div2_list(n, n_min) -> list:\n",
    "    \"\"\"\n",
    "    Generate a list of numbers from n to n_min (inclusive) by dividing n by 2\n",
    "    until the result is less than n_min.\n",
    "    This function starts with n and keeps dividing it by 2 until n_min is reached.\n",
    "    The number of times each value is added to the list is determined by n // current.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number to start with.\n",
    "        n_min (int): The minimum number to stop at.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of numbers from n to n_min (inclusive).\n",
    "\n",
    "    Examples:\n",
    "        _generate_div2_list(10, 1)\n",
    "        [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        _generate_div2_list(10, 2)\n",
    "        [10, 5, 5, 2, 2, 2, 2, 2]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    current = n\n",
    "    repeats = 1\n",
    "    max_repeats = 4\n",
    "    while current >= n_min:\n",
    "        result.extend([current] * min(repeats, max_repeats))\n",
    "        current = current // 2\n",
    "        repeats = repeats + 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_generate_div2_list(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_generate_div2_list(128, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_sizes(_L_in = 80, l1=2**9):\n",
    "    n_low = _L_in // 4\n",
    "    n_high = max(l1, 2 * n_low)\n",
    "    hidden_sizes = _generate_div2_list(n_high, n_low)\n",
    "    return hidden_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_hidden_sizes(l1=2**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.data.daten_lightv2 import DatenLightV2\n",
    "from pyhcf.utils.io import hcf_df2tensor\n",
    "df = DatenLightV2().load()\n",
    "print(f\"Datensatz der Größe {df.shape} erfolgreich geladen.\")\n",
    "print(df.columns.to_list())\n",
    "dataset = hcf_df2tensor(df, target='N', return_X_y=False)\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Targets Shape: {targets.shape}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Network Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we generate an 80-dim dataframe with 10000 samples, where the first two columns are random integers and the rest are random floats.\n",
    "* Then, we generate a target variable as the sum of the squared values.\n",
    "* The dataframe is converted to a tensor and split into a training, validation, and testing set. The corresponding data loaders are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "np.random.seed(42)\n",
    "n_samples = 10_000\n",
    "n_int = 2\n",
    "n_float = 76\n",
    "input_dim = n_int + n_float\n",
    "output_dim = 1\n",
    "data = np.random.rand(n_samples, n_float)\n",
    "data = np.hstack((np.random.randint(0, 10, (n_samples, n_int)), data))\n",
    "df = pd.DataFrame(data)\n",
    "df['y'] = np.sum(df.iloc[:, 2:]**2, axis=1)\n",
    "df.head()\n",
    "X = torch.tensor(df.iloc[:, :-1].values, dtype=torch.float32)\n",
    "y = torch.tensor(df.iloc[:, -1].values, dtype=torch.float32)\n",
    "dataset = TensorDataset(X, y)\n",
    "print(f\"Dataset with input tensor shape: {dataset.tensors[0].shape}\")\n",
    "print(f\"Dataset with target tensor shape: {dataset.tensors[1].shape}\")\n",
    "# print(dataset[0][0])\n",
    "# print(dataset[0][1])\n",
    "train_size_0 = int(0.8 * len(dataset))\n",
    "train_size = int(0.8 * train_size_0)\n",
    "val_size = train_size_0 - train_size\n",
    "test_size = len(dataset) - train_size_0\n",
    "train_dataset_0, test_dataset = random_split(dataset, [train_size_0, test_size])\n",
    "train_dataset, val_dataset = random_split(train_dataset_0, [train_size, val_size])\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NNLinearRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                    batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=input_dim,\n",
    "                                    _L_out=output_dim,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from spotpython.hyperparameters.optimizer import optimizer_handler\n",
    "import torchmetrics.functional.regression\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "\n",
    "class SettingsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset to handle settings-based data.\"\"\"\n",
    "    def __init__(self, dataframe, settings_columns, target_column):\n",
    "        self.dataframe = dataframe\n",
    "        self.settings_columns = settings_columns\n",
    "        self.target_column = target_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        settings = tuple(self.dataframe.iloc[idx][self.settings_columns])\n",
    "        features = self.dataframe.iloc[idx].drop(self.settings_columns + [self.target_column]).values\n",
    "        target = self.dataframe.iloc[idx][self.target_column]\n",
    "        return settings, torch.tensor(features, dtype=torch.float32), torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "class FilteredNNLinearRegressor:\n",
    "    def __init__(self, settings_columns, data, target_column='target', **nn_kwargs):\n",
    "        self.settings_columns = settings_columns\n",
    "        self.models = {}\n",
    "        self.nn_kwargs = nn_kwargs\n",
    "        self.data = data\n",
    "        self.target_column = target_column\n",
    "        self.prepare_networks()\n",
    "\n",
    "    def prepare_networks(self):\n",
    "        settings_combinations = self.data[self.settings_columns].drop_duplicates().to_records(index=False)\n",
    "        i = 0\n",
    "        for combination in settings_combinations:\n",
    "            print(f\"Combination {i}: {combination}\")\n",
    "            i += 1\n",
    "            self.models[combination] = NNLinearRegressor(**self.nn_kwargs)\n",
    "\n",
    "    def feature_filter(self, settings):\n",
    "        \"\"\"Filter the data based on given settings tuple.\"\"\"\n",
    "        df_filtered = self.data[(self.data[self.settings_columns] == pd.Series(settings)).all(axis=1)]\n",
    "        print(f\"df_filtered: {df_filtered}\")\n",
    "        return df_filtered\n",
    "\n",
    "    def train(self, trainer_kwargs):\n",
    "        # Split data and train each model separately\n",
    "        for settings, model in self.models.items():\n",
    "            filtered_data = self.feature_filter(settings)\n",
    "            dataset = SettingsDataset(filtered_data, self.settings_columns, self.target_column)\n",
    "            train_loader = DataLoader(dataset, batch_size=self.nn_kwargs['batch_size'])\n",
    "            trainer = L.Trainer(**trainer_kwargs)\n",
    "            trainer.fit(model, train_loader)\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        predictions = {}\n",
    "        for settings, model in self.models.items():\n",
    "            filtered_data = self.feature_filter(settings)\n",
    "            if not filtered_data.empty:\n",
    "                dataset = SettingsDataset(filtered_data, self.settings_columns, self.target_column)\n",
    "                test_loader = DataLoader(dataset, batch_size=self.nn_kwargs['batch_size'])\n",
    "                trainer = L.Trainer()\n",
    "                preds = trainer.predict(model, test_loader)\n",
    "                predictions[settings] = preds\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'setting1': [-1, -1, 1, 1],\n",
    "    'setting2': ['A', 'B', 'A', 'B'],\n",
    "    'feature1': [0.1, 0.2, 0.3, 0.4],\n",
    "    'feature2': [0.5, 0.6, 0.7, 0.8],\n",
    "    'target': [1, 2, 3, 4]\n",
    "})\n",
    "\n",
    "settings_columns = ['setting1', 'setting2']\n",
    "target_column = 'target'\n",
    "nn_kwargs = {\n",
    "    'l1': 16,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 2,\n",
    "    'initialization': 'xavier',\n",
    "    'act_fn': nn.ReLU(),\n",
    "    'optimizer': 'Adam',\n",
    "    'dropout_prob': 0.1,\n",
    "    'lr_mult': 0.1,\n",
    "    'patience': 2,\n",
    "    'batch_norm': True,\n",
    "    '_L_in': 2,  # For this example, 2 features besides settings\n",
    "    '_L_out': 1,\n",
    "    '_torchmetric': \"mean_squared_error\",\n",
    "}\n",
    "\n",
    "multi_network = FilteredNNLinearRegressor(settings_columns, data, target_column, **nn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = {'max_epochs': 2,  'enable_progress_bar': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_network.train(trainer_kwargs)\n",
    "predictions = multi_network.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explanation:\n",
    "1. SettingsDataset: A custom dataset class that includes settings as part of the data. Each row has a tuple of settings, feature values, and the target value.\n",
    "\n",
    "2. FilteredNNLinearRegressor: An umbrella class that handles setting combinations and assigns each its own `NNLinearRegressor` model instance. It trains these models using only relevant data filtered by the `feature_filter()` function.\n",
    "\n",
    "3. Feature Filtering: The `feature_filter()` function uses Pandas to filter rows based on their relevant setting information before creating dataset and loader instances for each unique settings combination.\n",
    "\n",
    "4. Training and Prediction: We generate and train separate models for each settings combination and then predict using test data filtered similarly using the defined `feature_filter()`.\n",
    "\n",
    "This approach provides modularity as each model is logically separated based on settings, while utilizing your existing class structure to individually specify training processes and handle data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.plot.xai import get_gradients\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "gradients = get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "# assert that the gradients are a dictionary with keys that contain the string 'layers' and values that are arrays\n",
    "assert all([key in gradients.keys() for key in gradients.keys()])\n",
    "assert all([isinstance(value, np.ndarray) for value in gradients.values()])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.plot.xai import get_gradients\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CondNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConditionalLayer(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, output_dim):\n",
    "        super(ConditionalLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.condition_fc = nn.Linear(condition_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # Basic linear transformation\n",
    "        base_output = self.fc(x)\n",
    "        # Compute a condition-dependent transformation\n",
    "        condition_output = self.condition_fc(condition)\n",
    "        # Modulate the output by adding the condition-dependent transformation\n",
    "        output = base_output + condition_output\n",
    "        return F.relu(output)\n",
    "\n",
    "class ConditionalNet(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, hidden_dim, output_dim):\n",
    "        super(ConditionalNet, self).__init__()\n",
    "        self.cond_layer1 = ConditionalLayer(input_dim, condition_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        x = self.cond_layer1(x, condition)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_dim = 10\n",
    "condition_dim = 2  # For instance, if you have two conditional features like region and season\n",
    "hidden_dim = 20\n",
    "output_dim = 1\n",
    "\n",
    "model = ConditionalNet(input_dim, condition_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Example data\n",
    "x = torch.randn(5, input_dim)\n",
    "condition = torch.randn(5, condition_dim)\n",
    "\n",
    "output = model(x, condition)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.light.regression import NNCondNetRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 64\n",
    "# generate data\n",
    "num_samples = 1_000\n",
    "input_dim = 10\n",
    "cond_dim = 2\n",
    "X = torch.randn(num_samples, input_dim)  # random data for example\n",
    "Y = torch.randn(num_samples, 1)  # random target for example\n",
    "data_set = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNCondNetRegressor(l1=128,\n",
    "                                batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_cond=cond_dim,\n",
    "                                    _L_in=input_dim - cond_dim,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# validation and test should give the same result, because the data is the same\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CondNet Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.file import get_experiment_filename\n",
    "from math import inf\n",
    "from spotpython.hyperparameters.values import set_hyperparameter\n",
    "\n",
    "PREFIX=\"4000\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "input_dim = 10\n",
    "output_dim = 1\n",
    "cond_dim = 2\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    fun_evals=inf,\n",
    "    max_time=1,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNCondNetRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=input_dim - cond_dim,\n",
    "    _L_out=1,\n",
    "    _L_cond=cond_dim,)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "\n",
    "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
    "set_hyperparameter(fun_control, \"l1\", [3,4])\n",
    "set_hyperparameter(fun_control, \"epochs\", [3,7])\n",
    "set_hyperparameter(fun_control, \"batch_size\", [4,5])\n",
    "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
    "set_hyperparameter(fun_control, \"patience\", [2,3])\n",
    "set_hyperparameter(fun_control, \"lr_mult\", [0.1, 20.0])\n",
    "\n",
    "design_control = design_control_init(init_size=10)\n",
    "\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_weights(net, return_index=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the weights of a neural network and the size of each layer.\n",
    "\n",
    "    Args:\n",
    "        net (object):\n",
    "            A neural network.\n",
    "        return_index (bool, optional):\n",
    "            Whether to return the index. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            A tuple containing:\n",
    "            - weights: A dictionary with the weights of the neural network.\n",
    "            - index: The layer index list (only if return_index is True).\n",
    "            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "\n",
    "    Examples:\n",
    "        # Example usage (as described in the original function's docstring)\n",
    "    \"\"\"\n",
    "    weights = {}\n",
    "    index = []\n",
    "    layer_sizes = {}\n",
    "    \n",
    "    for name, param in net.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            continue\n",
    "        \n",
    "        # Extract layer number\n",
    "        layer_number = int(name.split(\".\")[1])\n",
    "        index.append(layer_number)\n",
    "        \n",
    "        # Create dictionary key for this layer\n",
    "        key_name = f\"Layer {layer_number}\"\n",
    "        \n",
    "        # Store weight information\n",
    "        weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
    "        \n",
    "        # Store layer size as a NumPy array\n",
    "        layer_sizes[key_name] = np.array(param.size())\n",
    "    \n",
    "    if return_index:\n",
    "        return weights, index, layer_sizes\n",
    "    else:\n",
    "        return weights, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.plot.xai import get_weights\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "weights, layer_sizes = get_weights(net=model)\n",
    "weights, layer_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import math\n",
    "\n",
    "def plot_nn_values_scatter(\n",
    "    nn_values,\n",
    "    layer_sizes,\n",
    "    nn_values_names=\"\",\n",
    "    absolute=True,\n",
    "    cmap=\"gray\",\n",
    "    figsize=(6, 6),\n",
    "    return_reshaped=False,\n",
    "    show=True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Plot the values of a neural network including a marker for padding values.\n",
    "    For simplicity, this example will annotate 'P' directly on the plot for padding values\n",
    "    using a unique marker value approach.\n",
    "\n",
    "    Args:\n",
    "        nn_values (dict):\n",
    "            A dictionary with the values of the neural network. For example,\n",
    "            the weights, gradients, or activations.\n",
    "        layer_sizes (dict):\n",
    "            A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "        nn_values_names (str, optional):\n",
    "            The name of the values. Defaults to \"\".\n",
    "        absolute (bool, optional):\n",
    "            Whether to use the absolute values. Defaults to True.\n",
    "        cmap (str, optional):\n",
    "            The colormap to use. Defaults to \"gray\".\n",
    "        figsize (tuple, optional):\n",
    "            The figure size. Defaults to (6, 6).\n",
    "        return_reshaped (bool, optional):\n",
    "            Whether to return the reshaped values. Defaults to False.\n",
    "        show (bool, optional):\n",
    "            Whether to show the plot. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the reshaped values.\n",
    "    \"\"\"\n",
    "    if cmap == \"gray\":\n",
    "        cmap = \"gray\"\n",
    "    elif cmap == \"BlueWhiteRed\":\n",
    "        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"blue\", \"white\", \"red\"])\n",
    "    elif cmap == \"GreenYellowRed\":\n",
    "        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"green\", \"yellow\", \"red\"])\n",
    "    else:\n",
    "        cmap = \"viridis\"\n",
    "\n",
    "    res = {}\n",
    "    padding_marker = np.nan  # Use NaN as a special marker for padding\n",
    "    for layer, values in nn_values.items():\n",
    "        if layer not in layer_sizes:\n",
    "            print(f\"Layer {layer} size not defined, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        layer_shape = layer_sizes[layer]\n",
    "        height, width = layer_shape if len(layer_shape) == 2 else (layer_shape[0], 1)  # Support linear layers\n",
    "        \n",
    "        print(f\"{len(values)} values in Layer {layer}. Geometry: ({height}, {width})\")\n",
    "        \n",
    "        total_size = height * width\n",
    "        if len(values) < total_size:\n",
    "            padding_needed = total_size - len(values)\n",
    "            print(f\"{padding_needed} padding values added to Layer {layer}.\")\n",
    "            values = np.append(values, [padding_marker] * padding_needed)  # Append padding values\n",
    "\n",
    "        if absolute:\n",
    "            reshaped_values = np.abs(values).reshape((height, width))\n",
    "            # Mark padding values distinctly by setting them back to NaN\n",
    "            reshaped_values[reshaped_values == np.abs(padding_marker)] = np.nan\n",
    "        else:\n",
    "            reshaped_values = values.reshape((height, width))\n",
    "\n",
    "        _, ax = plt.subplots(figsize=figsize)\n",
    "        cax = ax.imshow(reshaped_values, cmap=cmap, interpolation=\"nearest\")\n",
    "\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                if np.isnan(reshaped_values[i, j]):\n",
    "                    ax.text(j, i, \"P\", ha=\"center\", va=\"center\", color=\"red\")\n",
    "        \n",
    "        plt.colorbar(cax, label=\"Value\")\n",
    "        plt.title(f\"{nn_values_names} Plot for {layer}\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "        # Add reshaped_values to the dictionary res\n",
    "        res[layer] = reshaped_values\n",
    "    if return_reshaped:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, layer_sizes = get_weights(net=model)\n",
    "plot_nn_values_scatter(nn_values=weights, layer_sizes=layer_sizes, nn_values_names=\"Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "\n",
    "def get_gradients(net, fun_control, batch_size, device=\"cpu\", normalize=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the gradients of a neural network and the size of each layer.\n",
    "\n",
    "    Args:\n",
    "        net (object):\n",
    "            A neural network.\n",
    "        fun_control (dict):\n",
    "            A dictionary with the function control.\n",
    "        batch_size (int, optional):\n",
    "            The batch size.\n",
    "        device (str, optional):\n",
    "            The device to use. Defaults to \"cpu\".\n",
    "        normalize (bool, optional):\n",
    "            Whether to normalize the input data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - grads: A dictionary with the gradients of the neural network.\n",
    "            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "\n",
    "    Examples:\n",
    "        # Example usage to compute gradients\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    dataset = fun_control[\"data_set\"]\n",
    "    data_module = LightDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        test_size=fun_control[\"test_size\"],\n",
    "        scaler=fun_control[\"scaler\"],\n",
    "        verbosity=10,\n",
    "    )\n",
    "    data_module.setup(stage=\"fit\")\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    inputs, targets = next(iter(train_loader))\n",
    "    if normalize:\n",
    "        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    net.zero_grad()\n",
    "    preds = net(inputs)\n",
    "    preds = preds.squeeze(-1)  # Remove the last dimension if it's 1\n",
    "    loss = F.mse_loss(preds, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = {}\n",
    "    layer_sizes = {}\n",
    "    for name, params in net.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            # Collect gradient information\n",
    "            grads[name] = params.grad.view(-1).cpu().clone().numpy()\n",
    "            # Collect size information\n",
    "            layer_sizes[name] = np.array(params.size())\n",
    "\n",
    "    net.zero_grad()\n",
    "    return grads, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients, layer_sizes = get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "plot_nn_values_scatter(nn_values=gradients, layer_sizes=layer_sizes, nn_values_names=\"Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(data, layer_index) -> bool:\n",
    "    \"\"\"Checks for NaN values in the tensor data.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The tensor to check for NaN values.\n",
    "        layer_index (int): The index of the layer for logging purposes.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if NaNs are found, False otherwise.\n",
    "    \"\"\"\n",
    "    if torch.isnan(data).any():\n",
    "        print(f\"NaN detected after layer {layer_index}\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(net, fun_control, batch_size, device=\"cpu\", normalize=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Computes the activations for each layer of the network, the mean activations,\n",
    "    and the sizes of the activations for each layer.\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The neural network model.\n",
    "        fun_control (dict): A dictionary containing the dataset.\n",
    "        batch_size (int): The batch size for the data loader.\n",
    "        device (str): The device to run the model on. Defaults to \"cpu\".\n",
    "        normalize (bool): Whether to normalize the input data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the activations, mean activations, and layer sizes for each layer.\n",
    "\n",
    "    Examples:\n",
    "        from spotpython.plot.xai import get_activations\n",
    "            activations, mean_activations, layer_sizes = get_activations(net, fun_control)\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    mean_activations = {}\n",
    "    layer_sizes = {}\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    dataset = fun_control[\"data_set\"]\n",
    "    data_module = LightDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        test_size=fun_control[\"test_size\"],\n",
    "        scaler=fun_control[\"scaler\"],\n",
    "        verbosity=10,\n",
    "    )\n",
    "    data_module.setup(stage=\"fit\")\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    inputs, _ = next(iter(train_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    if normalize:\n",
    "        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        # Loop through all layers\n",
    "        for layer_index, layer in enumerate(net.layers[:-1]):\n",
    "            inputs = layer(inputs)  # Forward pass through the layer\n",
    "\n",
    "            # Check for NaNs\n",
    "            if check_for_nans(inputs, layer_index):\n",
    "                break\n",
    "\n",
    "            # Collect activations for Linear layers\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                activations[layer_index] = inputs.view(-1).cpu().numpy()\n",
    "                mean_activations[layer_index] = inputs.mean(dim=0).cpu().numpy()\n",
    "                # Record the size of the activations\n",
    "                layer_sizes[layer_index] = np.array(inputs.size())\n",
    "\n",
    "    return activations, mean_activations, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations, mean_activations, layer_sizes = get_activations(net=model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "plot_nn_values_scatter(nn_values=activations, layer_sizes=layer_sizes, nn_values_names=\"Activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def extract_linear_dims(model):\n",
    "    dims = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Append input and output features of the Linear layer\n",
    "            dims.append(layer.in_features)\n",
    "            dims.append(layer.out_features)\n",
    "    return np.array(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_linear_dims(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "def viz_net(net,\n",
    "            device=\"cpu\",\n",
    "            show_attrs=False,\n",
    "            show_saved=False,\n",
    "            filename=\"model_architecture\",\n",
    "            format=\"png\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the architecture of a linear neural network.\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The neural network model.\n",
    "        device (str, optional): The device to use. Defaults to \"cpu\".\n",
    "        show_attrs (bool, optional): Whether to show the attributes. Defaults to False.\n",
    "        show_saved (bool, optional): Whether to show the saved. Defaults to False.\n",
    "        filename (str, optional): The filename. Defaults to \"model_architecture\".\n",
    "        format (str, optional): The format. Defaults to \"png\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the model does not have a linear layer.\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "       dim=extract_linear_dims(net)\n",
    "    except:\n",
    "        error_message = \"The model does not have a linear layer.\"\n",
    "        raise ValueError(error_message)\n",
    "    x = torch.randn(1, dim[0]).requires_grad_(True)\n",
    "    x = x.to(device)\n",
    "    output = net(x)\n",
    "    dot = make_dot(output, params=dict(net.named_parameters()), show_attrs=show_attrs , show_saved=show_saved)\n",
    "    dot.render(filename, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_net(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.plot.xai import viz_net\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "_L_in=10\n",
    "_L_out=1\n",
    "_torchmetric=\"mean_squared_error\"\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=_L_in,\n",
    "    _L_out=_L_out,\n",
    "    _torchmetric=_torchmetric,\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "viz_net(net=model, device=\"cpu\", show_attrs=True, show_saved=True, filename=\"model_architecture\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "S.exp_imp(1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "# assert S.exp_imp(0.0, 1.0) == 1/np.sqrt(2*np.pi)\n",
    "# which is approx. 0.3989422804014327\n",
    "S.exp_imp(0.0, 1.0)\n",
    "0.3989422804014327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_de_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract_from_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "\n",
    "# Define the number of theta and p parameters\n",
    "num_theta = 2\n",
    "num_p = 3\n",
    "\n",
    "# Initialize the Kriging model\n",
    "kriging_model = Kriging(\n",
    "    name='kriging',\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=False\n",
    ")\n",
    "\n",
    "# Extract parameters from given bounds\n",
    "# Assumes 'extract_from_bounds' will split the array into `theta` and `p` based on `n_theta`.\n",
    "bounds_array = np.array([1, 2, 3, 4, 5])\n",
    "kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "\n",
    "# Validate the expected values for theta and p\n",
    "# Convert theta and p to lists if they are numpy arrays\n",
    "theta_list = list(kriging_model.theta)\n",
    "p_list = list(kriging_model.p)\n",
    "\n",
    "assert theta_list == [1, 2], f\"Expected theta to be [1, 2] but got {theta_list}\"\n",
    "assert p_list == [3, 4, 5], f\"Expected p to be [3] but got {p_list}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "# Define the number of theta and p parameters\n",
    "num_theta = 2\n",
    "num_p = 3\n",
    "# Initialize the Kriging model\n",
    "kriging_model = Kriging(\n",
    "    name='kriging',\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=True\n",
    ")\n",
    "# Create bounds array\n",
    "bounds_array = np.array([1, 2, 3, 4, 5, 6])\n",
    "# Extract parameters from given bounds\n",
    "kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "# Assertions to check if parameters are correctly extracted\n",
    "assert np.array_equal(kriging_model.theta, [1, 2]), f\"Expected theta to be [1, 2] but got {kriging_model.theta}\"\n",
    "assert np.array_equal(kriging_model.p, [3, 4, 5]), f\"Expected p to be [3, 4, 5] but got {kriging_model.p}\"\n",
    "assert kriging_model.Lambda == 6, f\"Expected Lambda to be 6 but got {kriging_model.Lambda}\"\n",
    "print(\"All assertions passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "print(new_theta_p_Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.update_log()\n",
    "print(S.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 0], [1, 0]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.fit(nat_X, nat_y)\n",
    "print(S.Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "print(f\"S.nat_X: {S.nat_X}\")\n",
    "print(f\"S.nat_y: {S.nat_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "assert S.var_type == ['num', 'num']\n",
    "assert S.var_type == ['num', 'num']\n",
    "assert S.num_mask.all() == True\n",
    "assert S.factor_mask.all() == False\n",
    "assert S.int_mask.all() == False\n",
    "assert S.ordered_mask.all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "assert S.theta.all() == array([0., 0.]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import log, var\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "nat_y = np.array([1, 2, 3])\n",
    "n=3\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "# if var(self.nat_y) is > 0, then self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n",
    "# else self.pen_val = self.n * var(self.nat_y) + 1e4\n",
    "assert S.pen_val == nat_X.shape[0] * log(var(S.nat_y)) + 1e4\n",
    "assert S.Psi.shape == (n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")\n",
    "S.build_U()\n",
    "print(f\"S.U:{S.U}\")\n",
    "S.likelihood()\n",
    "S.negLnLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "from numpy import power\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "print(S.__is_any__(power(10.0, S.theta), 0))\n",
    "print(S.__is_any__(S.theta, 0))\n",
    "S.theta: [0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")\n",
    "S.build_U()\n",
    "print(f\"S.U:{S.U}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1], [2]])\n",
    "nat_y = np.array([5, 10])\n",
    "n=2\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.build_Psi()\n",
    "S.build_U()\n",
    "S.likelihood()\n",
    "# assert S.mu is close to 7.5 with a tolerance of 1e-6\n",
    "assert np.allclose(S.mu, 7.5, atol=1e-6)\n",
    "E = np.exp(1)\n",
    "sigma2 = E/(E**2 -1) * (25/4 + 25/4*E)\n",
    "# asssert S.SigmaSqr is close to sigma2 with a tolerance of 1e-6\n",
    "assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)\n",
    "print(f\"S.LnDetPsi:{S.LnDetPsi}\")\n",
    "print(f\"S.self.negLnLike:{S.negLnLike}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# 1-dimensional example\n",
    "fun = analytical().fun_sphere\n",
    "fun_control=fun_control_init(lower = np.array([-1]),\n",
    "                            upper = np.array([1]),\n",
    "                            noise=False)\n",
    "design_control=design_control_init(init_size=10)\n",
    "S = spot.Spot(fun=fun,\n",
    "              fun_control=fun_control,\n",
    "              design_control=design_control)\n",
    "S.initialize_design()\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.surrogate.plot()\n",
    "# 2-dimensional example\n",
    "fun = analytical().fun_sphere\n",
    "fun_control=fun_control_init(lower = np.array([-1, -1]),\n",
    "                            upper = np.array([1, 1]),\n",
    "                            noise=False)\n",
    "design_control=design_control_init(init_size=10)\n",
    "S = spot.Spot(fun=fun,\n",
    "              fun_control=fun_control,\n",
    "              design_control=design_control)\n",
    "S.initialize_design()\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.surrogate.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_0).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "print(f\"mean_prediction: {mean_prediction}\")\n",
    "print(f\"std_prediction: {std_prediction}\")\n",
    "print(f\"s_ei: {s_ei}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "    from numpy import linspace, arange, empty\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=10).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "n = X.shape[0]\n",
    "y = empty(n, dtype=float)\n",
    "s = empty(n, dtype=float)\n",
    "ei = empty(n, dtype=float)\n",
    "for i in range(n):\n",
    "    y_coded, s_coded, ei_coded = S.predict_coded(X[i, :])\n",
    "    y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n",
    "    s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n",
    "    ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n",
    "print(f\"y: {y}\")\n",
    "print(f\"s: {s}\")\n",
    "print(f\"ei: {-1.0*ei}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_psi_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "X_train = np.array([[1., 2.],\n",
    "                    [2., 4.],\n",
    "                    [3., 6.]])\n",
    "y_train = np.array([1., 2., 3.])\n",
    "S = Kriging(name='kriging',\n",
    "            seed=123,\n",
    "            log_level=50,\n",
    "            n_theta=1,\n",
    "            noise=False,\n",
    "            cod_type=\"norm\")\n",
    "S.fit(X_train, y_train)\n",
    "# force theta to simple values:\n",
    "S.theta = np.array([0.0])\n",
    "nat_X = np.array([1., 0.])\n",
    "S.psi = np.zeros((S.n, 1))\n",
    "S.build_psi_vec(nat_X)\n",
    "res = np.array([[np.exp(-4)],\n",
    "    [np.exp(-17)],\n",
    "    [np.exp(-40)]])\n",
    "assert np.array_equal(S.psi, res)\n",
    "print(f\"S.psi: {S.psi}\")\n",
    "print(f\"Control value res: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Log Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import inf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.file import get_experiment_filename\n",
    "\n",
    "PREFIX=\"00_TEST\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    save_experiment=True,\n",
    "    fun_evals=inf,\n",
    "    max_time=10,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    tensorboard_log=True,\n",
    "    show_config=True,)\n",
    "\n",
    "design_control = design_control_init(init_size=5)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNLinearRegressor Teest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "torch.manual_seed(0)\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 64\n",
    "# generate data\n",
    "num_samples = 1_000\n",
    "input_dim = 10\n",
    "X = torch.randn(num_samples, input_dim)  # random data for example\n",
    "Y = torch.randn(num_samples, 1)  # random target for example\n",
    "data_set = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=input_dim,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# validation and test should give the same result, because the data is the same\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Objective Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [-1, -1, -1]])\n",
    "fun = analytical()\n",
    "fun.fun_cubed(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([np.zeros(10), np.ones(10)])\n",
    "fun = analytical()\n",
    "fun.fun_wingwt(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.architecture import generate_div2_list\n",
    "# call the function with all integer values between 5 and 10\n",
    "for n in range(5, 21):\n",
    "    print(generate_div2_list(n, n_min=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.architecture import get_hidden_sizes\n",
    "_L_in = 10\n",
    "max_n = 10\n",
    "for l1 in range(5, 20):    \n",
    "    print(get_hidden_sizes(_L_in, l1, max_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def get_three_layers(_L_in, l1) -> list:\n",
    "    \"\"\"\n",
    "    Calculate three layers based on input values.\n",
    "\n",
    "    Args:\n",
    "        _L_in (float): The input value to be multiplied.\n",
    "        l1 (float): The multiplier for the layers.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing three calculated layers [a, b, c] where:\n",
    "            - a = 3 * l1 * _L_in\n",
    "            - b = 2 * l1 * _L_in\n",
    "            - c = l1 * _L_in\n",
    "\n",
    "    Examples:\n",
    "        from spotpython.hyperparameters.architecture import get_three_layers\n",
    "            _L_in = 10\n",
    "            l1 = 20\n",
    "            get_three_layers(_L_in, l1)\n",
    "            [600, 400, 200]\n",
    "    \"\"\"\n",
    "    a = 3 * l1 * _L_in\n",
    "    b = 2 * l1 * _L_in\n",
    "    c = ceil(l1/2) * _L_in\n",
    "    return [a, b, a, b, b, c, c] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_L_in = 20\n",
    "l1 = 4\n",
    "get_three_layers(_L_in, l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for 0.20.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_spot_attributes_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1]),\n",
    "    upper = np.array([1]),\n",
    "    fun_evals=n)\n",
    "design_control=design_control_init(init_size=ni)\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n",
    "df = spot_1.get_spot_attributes_as_df()\n",
    "df[\"Attribute Name\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_red_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "# number of points\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals = n)\n",
    "design_control=design_control_init(init_size=ni)\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n",
    "assert spot_1.lower.size == 2\n",
    "assert spot_1.upper.size == 2\n",
    "assert len(spot_1.var_type) == 2\n",
    "assert spot_1.red_dim == False\n",
    "spot_1.lower = np.array([-1, -1])\n",
    "spot_1.upper = np.array([-1, -1])\n",
    "spot_1.to_red_dim()\n",
    "assert spot_1.lower.size == 0\n",
    "assert spot_1.upper.size == 0\n",
    "assert len(spot_1.var_type) == 0\n",
    "assert spot_1.red_dim == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_all_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n",
    "lower = np.array([-1, -1, 0, 0])\n",
    "upper = np.array([1, -1, 0, 5])  # Second and third dimensions are fixed\n",
    "fun_evals = 10\n",
    "var_type = ['float', 'int', 'float', 'int']\n",
    "var_name = ['x1', 'x2', 'x3', 'x4']\n",
    "spot_instance = spot.Spot(\n",
    "    fun = Analytical().fun_sphere, \n",
    "    fun_control=fun_control_init(lower=lower, upper=upper, fun_evals=fun_evals)\n",
    ")\n",
    "X0 = np.array([[2.5, 3.5], [4.5, 5.5]])\n",
    "X_full_dim = spot_instance.to_all_dim(X0)\n",
    "print(X_full_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_new_X0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "            n_points=10,\n",
    "            ocba_delta=0,\n",
    "            lower = np.array([-1, -1]),\n",
    "            upper = np.array([1, 1])\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "             fun_control=fun_control,\n",
    "             design_control=design_control,\n",
    ")\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "X0 = S.get_new_X0()\n",
    "assert X0.shape[0] == S.n_points\n",
    "assert X0.shape[1] == S.lower.size\n",
    "# assert new points are in the interval [lower, upper]\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "# print using 20 digits precision\n",
    "np.set_printoptions(precision=20)\n",
    "print(f\"X0: {X0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython import Analytical\n",
    "from spotpython import Spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write_tensorboard_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    tensorboard_log=True,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    lower = np.array([-1]),\n",
    "    upper = np.array([1])\n",
    "    )\n",
    "fun = Analytical().fun_sphere\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "S.initialize_design()\n",
    "S.write_tensorboard_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize_design_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "fun = Analytical().fun_sphere\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "X_start = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "S.initialize_design_matrix(X_start)\n",
    "print(f\"Design matrix: {S.X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate_initial_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    lower=np.array([-1, -1]),\n",
    "    upper=np.array([1, 1])\n",
    ")\n",
    "fun = Analytical().fun_sphere\n",
    "S = Spot(fun=fun, fun_control=fun_control)\n",
    "X0 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "S.initialize_design_matrix(X_start=X0)\n",
    "S.evaluate_initial_design()\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Experiment:\n",
    "    def save_experiment(self, filename=None, path=None, overwrite=True) -> None:\n",
    "        \"\"\"\n",
    "        Save the experiment to a file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename of the experiment file.\n",
    "            path (str): The path to the experiment file.\n",
    "            overwrite (bool): If `True`, the file will be overwritten if it already exists. Default is `True`.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Ensure we don't accidentally try to pickle unpicklable components\n",
    "        self.close_and_del_spot_writer()\n",
    "        self.remove_logger_handlers()\n",
    "\n",
    "        # Create deep copies of control dictionaries\n",
    "        fun_control = copy.deepcopy(self.fun_control)\n",
    "        optimizer_control = copy.deepcopy(self.optimizer_control)\n",
    "        surrogate_control = copy.deepcopy(self.surrogate_control)\n",
    "        design_control = copy.deepcopy(self.design_control)\n",
    "\n",
    "        # Prepare an experiment dictionary excluding any explicitly unpickable components\n",
    "        experiment = {\n",
    "            \"design_control\": design_control,\n",
    "            \"fun_control\": fun_control,\n",
    "            \"optimizer_control\": optimizer_control,\n",
    "            \"spot_tuner\": self._get_pickle_safe_spot_tuner(),\n",
    "            \"surrogate_control\": surrogate_control,\n",
    "        }\n",
    "\n",
    "        # Determine the filename based on PREFIX if not provided\n",
    "        PREFIX = fun_control.get(\"PREFIX\", \"experiment\")\n",
    "        if filename is None:\n",
    "            filename = self.get_experiment_filename(PREFIX)\n",
    "\n",
    "        if path is not None:\n",
    "            filename = os.path.join(path, filename)\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if filename is not None and os.path.exists(filename) and not overwrite:\n",
    "            print(f\"Error: File {filename} already exists. Use overwrite=True to overwrite the file.\")\n",
    "            return\n",
    "\n",
    "        # Serialize the experiment dictionary to the pickle file\n",
    "        if filename is not None:\n",
    "            with open(filename, \"wb\") as handle:\n",
    "                try:\n",
    "                    pickle.dump(experiment, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during pickling: {e}\")\n",
    "                    raise e\n",
    "            print(f\"Experiment saved to {filename}\")\n",
    "\n",
    "    def remove_logger_handlers(self) -> None:\n",
    "        \"\"\"\n",
    "        Remove handlers from the logger to avoid pickling issues.\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        for handler in list(logger.handlers):  # Copy the list to avoid modification during iteration\n",
    "            logger.removeHandler(handler)\n",
    "\n",
    "    def close_and_del_spot_writer(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete the spot_writer attribute from the object\n",
    "        if it exists and close the writer.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n",
    "            self.spot_writer.flush()\n",
    "            self.spot_writer.close()\n",
    "            del self.spot_writer\n",
    "\n",
    "    def _get_pickle_safe_spot_tuner(self):\n",
    "        \"\"\"\n",
    "        Create a copy of self excluding unpickleable components for safe pickling.\n",
    "        This ensures no unpicklable components are passed to pickle.dump().\n",
    "        \"\"\"\n",
    "        # Make a deepcopy and manually remove unpickleable components\n",
    "        spot_tuner = copy.deepcopy(self)\n",
    "        for attr in ['spot_writer']:\n",
    "            if hasattr(spot_tuner, attr):\n",
    "                delattr(spot_tuner, attr)\n",
    "        return spot_tuner\n",
    "\n",
    "    def get_experiment_filename(self, prefix):\n",
    "        \"\"\"\n",
    "        Generate a filename based on a given prefix with additional unique identifiers or timestamps.\n",
    "        \"\"\"\n",
    "        # Implement the logic to generate a filename\n",
    "        return f\"{prefix}_experiment.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform_hyper_parameter_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.transform import transform_hyper_parameter_values\n",
    "fun_control = {\n",
    "    \"core_model_hyper_dict\": {\n",
    "        \"leaf_prediction\": {\n",
    "                \"type\": \"factor\",\n",
    "                \"transform\": \"None\",\n",
    "                \"default\": \"mean\",\n",
    "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
    "                \"core_model_parameter_type\": \"str\"\n",
    "                            },\n",
    "        \"max_depth\": {\n",
    "                \"type\": \"int\",\n",
    "                \"default\": 20,\n",
    "                \"transform\": \"transform_power_2\",\n",
    "                \"lower\": 2,\n",
    "                \"upper\": 20}\n",
    "            }\n",
    "    }\n",
    "hyper_parameter_values = {\n",
    "        'max_depth': 2,\n",
    "        'leaf_prediction': 'mean'}\n",
    "transform_hyper_parameter_values(fun_control, hyper_parameter_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.transform import transform_hyper_parameter_values\n",
    "fun_control = {\n",
    "    \"core_model_hyper_dict\": {\n",
    "        \"l1\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 3,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 3,\n",
    "            \"upper\": 8\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 4,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 4,\n",
    "            \"upper\": 9\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 4,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 1,\n",
    "            \"upper\": 4\n",
    "        },\n",
    "        \"act_fn\": {\n",
    "            \"levels\": [\n",
    "                \"Sigmoid\",\n",
    "                \"Tanh\",\n",
    "                \"ReLU\",\n",
    "                \"LeakyReLU\",\n",
    "                \"ELU\",\n",
    "                \"Swish\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"ReLU\",\n",
    "            \"transform\": \"None\",\n",
    "            \"class_name\": \"spotpython.torch.activation\",\n",
    "            \"core_model_parameter_type\": \"instance()\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 5\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"levels\": [\n",
    "                \"Adadelta\",\n",
    "                \"Adagrad\",\n",
    "                \"Adam\",\n",
    "                \"AdamW\",\n",
    "                \"SparseAdam\",\n",
    "                \"Adamax\",\n",
    "                \"ASGD\",\n",
    "                \"NAdam\",\n",
    "                \"RAdam\",\n",
    "                \"RMSprop\",\n",
    "                \"Rprop\",\n",
    "                \"SGD\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"SGD\",\n",
    "            \"transform\": \"None\",\n",
    "            \"class_name\": \"torch.optim\",\n",
    "            \"core_model_parameter_type\": \"str\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 11\n",
    "        },\n",
    "        \"dropout_prob\": {\n",
    "            \"type\": \"float\",\n",
    "            \"default\": 0.01,\n",
    "            \"transform\": \"None\",\n",
    "            \"lower\": 0.0,\n",
    "            \"upper\": 0.25\n",
    "        },\n",
    "        \"lr_mult\": {\n",
    "            \"type\": \"float\",\n",
    "            \"default\": 1.0,\n",
    "            \"transform\": \"None\",\n",
    "            \"lower\": 0.1,\n",
    "            \"upper\": 10.0\n",
    "        },\n",
    "        \"patience\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 2,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 2,\n",
    "            \"upper\": 6\n",
    "        },\n",
    "        \"batch_norm\": {\n",
    "            \"levels\": [\n",
    "                0,\n",
    "                1\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": 0,\n",
    "            \"transform\": \"None\",\n",
    "            \"core_model_parameter_type\": \"bool\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 1\n",
    "        },\n",
    "        \"initialization\": {\n",
    "            \"levels\": [\n",
    "                \"Default\",\n",
    "                \"kaiming_uniform\",\n",
    "                \"kaiming_normal\",\n",
    "                \"xavier_uniform\",\n",
    "                \"xavier_normal\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"Default\",\n",
    "            \"transform\": \"None\",\n",
    "            \"core_model_parameter_type\": \"str\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 4\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "hyper_parameter_values = {\n",
    "        'l1': 2,\n",
    "        'epochs': 3,\n",
    "        'batch_size': 4,\n",
    "        'act_fn': 'ReLU',\n",
    "        'optimizer': 'SGD',\n",
    "        'dropout_prob': 0.01,\n",
    "        'lr_mult': 1.0,\n",
    "        'patience': 3,\n",
    "        'batch_norm': 0,\n",
    "        'initialization': 'Default',        \n",
    "    }\n",
    "transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import assign_values\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "var_list = ['a', 'b']\n",
    "result = assign_values(X, var_list)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from spotpythonspot import Spot\n",
    "from spotpythonfunobjectivefunctions import Analytical\n",
    "from spotpythonutilsinit import fun_control_init, design_control_init\n",
    "from spotpythonutilsfile import load_result\n",
    "import pprint\n",
    "\n",
    "def _compare_dicts(dict1, dict2, ignore_keys=None):\n",
    "    \"\"\"\n",
    "    Compare two dictionaries, including element-wise comparison for numpy arrays\n",
    "    Print missing elements (keys) if the dictionaries do not match\n",
    "\n",
    "    Args:\n",
    "        dict1 (dict): First dictionary to compare\n",
    "        dict2 (dict): Second dictionary to compare\n",
    "        ignore_keys (list, optional): List of keys to ignore during comparison Default is None\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the dictionaries match, False otherwise\n",
    "    \"\"\"\n",
    "    if ignore_keys is None:\n",
    "        ignore_keys = []\n",
    "    # ensure that ignore_keys is a list\n",
    "    if not isinstance(ignore_keys, list):\n",
    "        ignore_keys = [ignore_keys]\n",
    "\n",
    "    keys1 = set(dict1keys()) - set(ignore_keys)\n",
    "    keys2 = set(dict2keys()) - set(ignore_keys)\n",
    "\n",
    "    if keys1 != keys2:\n",
    "        missing_in_dict1 = keys2 - keys1\n",
    "        missing_in_dict2 = keys1 - keys2\n",
    "        print(f\"Missing in dict1: {missing_in_dict1}\")\n",
    "        print(f\"Missing in dict2: {missing_in_dict2}\")\n",
    "        return False\n",
    "\n",
    "    for key in keys1:\n",
    "        if isinstance(dict1[key], npndarray) and isinstance(dict2[key], npndarray):\n",
    "            if not nparray_equal(dict1[key], dict2[key]):\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "        else:\n",
    "            if dict1[key] != dict2[key]:\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_save_and_load_experiment(tmp_path):\n",
    "    PREFIX = \"test_02\"\n",
    "    # Initialize function control\n",
    "    fun_control = fun_control_init(\n",
    "        PREFIX=PREFIX,\n",
    "        lower=nparray([-1, -1]),\n",
    "        upper=nparray([1, 1]),\n",
    "        verbosity=1\n",
    "    )\n",
    "    \n",
    "    design_control = design_control_init(init_size=7)\n",
    "\n",
    "    fun = Analytical()fun_sphere\n",
    "        \n",
    "    S = Spot(\n",
    "        fun=fun,\n",
    "        fun_control=fun_control,\n",
    "        design_control=design_control,\n",
    "    )\n",
    "    \n",
    "    X_start = nparray([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    S.run(X_start=X_start)\n",
    "\n",
    "    # Load the experiment\n",
    "    S_loaded = load_result(PREFIX)\n",
    "    print(f\"S: {S}\")    \n",
    "    print(f\"S_loaded: {S_loaded}\")\n",
    "    pprint.pprint(S_loaded)\n",
    "    loaded_fun_control = S_loaded.fun_control\n",
    "    loaded_design_control = S_loaded.design_control\n",
    "    loaded_surrogate_control = S_loaded.surrogate_control\n",
    "    loaded_optimizer_control = S_loaded.optimizer_control\n",
    "    \n",
    "    # Check if the loaded data matches the original data\n",
    "    # It is ok if the counter is different, because it is increased during the run\n",
    "    assert _compare_dicts(loaded_fun_control, fun_control, ignore_keys=\"counter\"), \"Loaded fun_control should match the original fun_control.\"\n",
    "    assert _compare_dicts(loaded_design_control, design_control), \"Loaded design_control should match the original design_control.\"\n",
    "    assert _compare_dicts(loaded_surrogate_control, S.surrogate_control), \"Loaded surrogate_control should match the original surrogate_control.\"\n",
    "    assert _compare_dicts(loaded_optimizer_control, S.optimizer_control), \"Loaded optimizer_control should match the original optimizer_control.\"\n",
    "\n",
    "    # Check if the S_loaded is an instance of Spot\n",
    "    assert isinstance(S_loaded, Spot), \"Loaded S_loaded should be an instance of Spot.\"\n",
    "\n",
    "    # Check if the design matrix and response vector are equal\n",
    "    # if there are differences, print the differences\n",
    "    # Differences are OK\n",
    "    # if not np.array_equal(S_loaded.X, S.X):\n",
    "    #     print(f\"Design matrix mismatch: {S_loaded.X} != {S.X}\")\n",
    "    # if not np.array_equal(S_loaded.y, S.y):\n",
    "    #     print(f\"Response vector mismatch: {S_loaded.y} != {S.y}\")\n",
    "\n",
    "\n",
    "test_save_and_load_experiment(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n",
    "\n",
    "ni = 7\n",
    "PREFIX = \"test_plot_progress_05\"\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "PREFIX=PREFIX,\n",
    "lower=np.array([-1, -1]),\n",
    "upper=np.array([1, 1]),\n",
    "fun_evals=fun_evals,\n",
    "tolerance_x=np.sqrt(np.spacing(1))\n",
    ")\n",
    "design_control = design_control_init(init_size=ni)\n",
    "surrogate_control = surrogate_control_init(n_theta=3)\n",
    "S = Spot(\n",
    "fun=fun,\n",
    "fun_control=fun_control,\n",
    "design_control=design_control,\n",
    "surrogate_control=surrogate_control,\n",
    ")\n",
    "S = S.run()\n",
    "\n",
    "# Test plot_progress with different parameters\n",
    "S.plot_progress(show=False)  # Test with show=False\n",
    "S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename\n",
    "# add NaN to S.y at position 2\n",
    "S.y[2] = np.nan\n",
    "S.plot_progress(show=False)  # Test with NaN in S.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot.spot import Spot\n",
    "from spotpython.utils.repair import repair_non_numeric\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    ")\n",
    "\n",
    "fun = Analytical().fun_branin_factor\n",
    "ni = 12\n",
    "spot_test = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control_init(\n",
    "        lower=np.array([-5, -0, 1]), upper=np.array([10, 15, 3]), var_type=[\"num\", \"num\", \"factor\"]\n",
    "    ),\n",
    "    design_control=design_control_init(init_size=ni),\n",
    ")\n",
    "spot_test.run()\n",
    "# 3rd variable should be a rounded float, because it was labeled as a factor\n",
    "assert spot_test.min_X[2] == round(spot_test.min_X[2])\n",
    "\n",
    "spot_test.X = spot_test.generate_design(\n",
    "    size=spot_test.design_control[\"init_size\"],\n",
    "    repeats=spot_test.design_control[\"repeats\"],\n",
    "    lower=spot_test.lower,\n",
    "    upper=spot_test.upper,\n",
    ")\n",
    "spot_test.X = repair_non_numeric(spot_test.X, spot_test.var_type)\n",
    "assert spot_test.X.ndim == 2\n",
    "assert spot_test.X.shape[0] == ni\n",
    "assert spot_test.X.shape[1] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spot_test.get_spot_attributes_as_df()\n",
    "list(df['Attribute Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "    \n",
    "# Setup: Configure initial parameters\n",
    "ni = 7\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX= \"test_get_spot_attributes_as_df\",\n",
    "    lower=np.array([-1]),\n",
    "    upper=np.array([1]),\n",
    "    fun_evals=n\n",
    ")\n",
    "design_control = design_control_init(init_size=ni)\n",
    "\n",
    "# Create instance of the Spot class\n",
    "S = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control\n",
    ")\n",
    "\n",
    "# Run the optimization\n",
    "S.run()\n",
    "\n",
    "# Get the attributes as a DataFrame\n",
    "df = S.get_spot_attributes_as_df()\n",
    "\n",
    "# Define expected attribute names (ensure these match your Spot class' attributes)\n",
    "expected_attributes = ['X',\n",
    "                        'all_lower',\n",
    "                        'all_upper',\n",
    "                        'all_var_name',\n",
    "                        'all_var_type',\n",
    "                        'counter',\n",
    "                        'de_bounds',\n",
    "                        'design',\n",
    "                        'design_control',\n",
    "                        'eps',\n",
    "                        'fun_control',\n",
    "                        'fun_evals',\n",
    "                        'fun_repeats',\n",
    "                        'ident',\n",
    "                        'infill_criterion',\n",
    "                        'k',\n",
    "                        'log_level',\n",
    "                        'lower',\n",
    "                        'max_surrogate_points',\n",
    "                        'max_time',\n",
    "                        'mean_X',\n",
    "                        'mean_y',\n",
    "                        'min_X',\n",
    "                        'min_mean_X',\n",
    "                        'min_mean_y',\n",
    "                        'min_y',\n",
    "                        'n_points',\n",
    "                        'noise',\n",
    "                        'ocba_delta',\n",
    "                        'optimizer_control',\n",
    "                        'progress_file',\n",
    "                        'red_dim',\n",
    "                        'rng',\n",
    "                        'show_models',\n",
    "                        'show_progress',\n",
    "                        'spot_writer',\n",
    "                        'surrogate',\n",
    "                        'surrogate_control',\n",
    "                        'tkagg',\n",
    "                        'tolerance_x',\n",
    "                        'upper',\n",
    "                        'var_name',\n",
    "                        'var_type',\n",
    "                        'var_y',\n",
    "                        'verbosity',\n",
    "                        'y']\n",
    "\n",
    "# Check that the DataFrame has the correct attributes\n",
    "assert list(df['Attribute Name']) == expected_attributes\n",
    "\n",
    "# Further checks can be done for specific attribute values\n",
    "# Example: Check that 'fun_evals' has the expected value\n",
    "fun_evals_row = df.query(\"`Attribute Name` == 'fun_evals'\")\n",
    "assert not fun_evals_row.empty and fun_evals_row['Attribute Value'].values[0] == n\n",
    "\n",
    "# Example: Check that 'lower' has the expected value\n",
    "lower_row = df.query(\"`Attribute Name` == 'lower'\")\n",
    "assert not lower_row.empty and lower_row['Attribute Value'].values[0] == [-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate_dic_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import iterate_dict_values\n",
    "var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n",
    "print(var_dict)\n",
    "list(iterate_dict_values(var_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.values import convert_keys\n",
    "d = {'a': 1, 'b': 2.1, 'c': 3}\n",
    "var_type = [\"int\", \"num\", \"int\"]\n",
    "convert_keys(d, var_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_dict_with_levels_and_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "from spotpython.hyperparameters.values import assign_values, get_var_name, iterate_dict_values, convert_keys, get_dict_with_levels_and_types, get_core_model_from_name, add_core_model_to_fun_control\n",
    "import pprint\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "coremodel, core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "for values in iterate_dict_values(var_dict):\n",
    "    values = convert_keys(values, fun_control[\"var_type\"])\n",
    "    pprint.pprint(values)\n",
    "    # pprint.pprint(fun_control)\n",
    "    values = get_dict_with_levels_and_types(fun_control=fun_control, v=values)\n",
    "    pprint.pprint(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_one_config_from_var_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import  get_core_model_from_name, add_core_model_to_fun_control, generate_one_config_from_var_dict\n",
    "import pprint\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "_ , core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "var_dict = {'l1': np.array([3.]),\n",
    "            'epochs': np.array([4.]),\n",
    "            'batch_size': np.array([4.]),\n",
    "            'act_fn': np.array([2.]),\n",
    "            'optimizer': np.array([11.]),\n",
    "            'dropout_prob': np.array([0.01]),\n",
    "            'lr_mult': np.array([1.]),\n",
    "            'patience': np.array([2.]),\n",
    "            'batch_norm': np.array([0.]),\n",
    "            'initialization': np.array([0.])}\n",
    "g = generate_one_config_from_var_dict(var_dict=var_dict, fun_control=fun_control)\n",
    "# Since g is an iterator, we need to call next to get the values\n",
    "values = next(g)\n",
    "pprint.pprint(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return_conf_list_from_var_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import  get_core_model_from_name, add_core_model_to_fun_control, return_conf_list_from_var_dict\n",
    "import pprint\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "_ , core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "var_dict = {'l1': np.array([3., 4.]),\n",
    "            'epochs': np.array([4., 3.]),\n",
    "            'batch_size': np.array([4., 4.]),\n",
    "            'act_fn': np.array([2., 1.]),\n",
    "            'optimizer': np.array([11., 10.]),\n",
    "            'dropout_prob': np.array([0.01, 0.]),\n",
    "            'lr_mult': np.array([1., 1.1]),\n",
    "            'patience': np.array([2., 3.]),\n",
    "            'batch_norm': np.array([0., 1.]),\n",
    "            'initialization': np.array([0., 1.])}\n",
    "return_conf_list_from_var_dict(var_dict=var_dict, fun_control=fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_one_config_from_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import get_core_model_from_name, add_core_model_to_fun_control, get_one_config_from_X\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "coremodel, core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "X = np.array([[3.0e+00, 4.0e+00, 4.0e+00, 2.0e+00, 1.1e+01, 1.0e-02, 1.0e+00, 2.0e+00, 0.0e+00,\n",
    " 0.0e+00]])\n",
    "print(X)\n",
    "get_one_config_from_X(X, fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_tuned_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.spot import Spot\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_hyperparameter, get_tuned_architecture\n",
    "\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    force_run=False,\n",
    "    PREFIX=\"get_one_config_from_X\",\n",
    "    save_experiment=True,\n",
    "    fun_evals=10,\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "set_hyperparameter(fun_control, \"epochs\", [2,2])\n",
    "set_hyperparameter(fun_control, \"patience\", [1,2])\n",
    "design_control = design_control_init(init_size=5)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "S = Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "S.run()\n",
    "get_tuned_architecture(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_and_save_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import pprint\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.utils.file import load_experiment, load_result\n",
    "\n",
    "def _compare_dicts(dict1, dict2, ignore_keys=None):\n",
    "    \"\"\"\n",
    "    Compare two dictionaries, including element-wise comparison for numpy arrays.\n",
    "    Print missing elements (keys) if the dictionaries do not match.\n",
    "\n",
    "    Args:\n",
    "        dict1 (dict): First dictionary to compare.\n",
    "        dict2 (dict): Second dictionary to compare.\n",
    "        ignore_keys (list, optional): List of keys to ignore during comparison. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the dictionaries match, False otherwise.\n",
    "    \"\"\"\n",
    "    if ignore_keys is None:\n",
    "        ignore_keys = []\n",
    "    # ensure that ignore_keys is a list\n",
    "    if not isinstance(ignore_keys, list):\n",
    "        ignore_keys = [ignore_keys]\n",
    "\n",
    "    keys1 = set(dict1.keys()) - set(ignore_keys)\n",
    "    keys2 = set(dict2.keys()) - set(ignore_keys)\n",
    "\n",
    "    if keys1 != keys2:\n",
    "        missing_in_dict1 = keys2 - keys1\n",
    "        missing_in_dict2 = keys1 - keys2\n",
    "        print(f\"Missing in dict1: {missing_in_dict1}\")\n",
    "        print(f\"Missing in dict2: {missing_in_dict2}\")\n",
    "        return False\n",
    "\n",
    "    for key in keys1:\n",
    "        if isinstance(dict1[key], np.ndarray) and isinstance(dict2[key], np.ndarray):\n",
    "            if not np.array_equal(dict1[key], dict2[key]):\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "        else:\n",
    "            if dict1[key] != dict2[key]:\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "PREFIX = \"test_save_and_load_experiment_04\"\n",
    "# Initialize function control\n",
    "fun_control = fun_control_init(\n",
    "    save_experiment=True,\n",
    "    PREFIX=PREFIX,\n",
    "    lower=np.array([-1, -1]),\n",
    "    upper=np.array([1, 1]),\n",
    "    verbosity=2,\n",
    "    log_level=50\n",
    ")\n",
    "\n",
    "design_control = design_control_init(init_size=7)\n",
    "\n",
    "fun = Analytical().fun_sphere\n",
    "    \n",
    "S = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control\n",
    ")\n",
    "\n",
    "# Load the experiment\n",
    "S_loaded = load_experiment(PREFIX)\n",
    "print(f\"S: {S}\")    \n",
    "print(f\"S_loaded: {S_loaded}\")\n",
    "pprint.pprint(S_loaded)\n",
    "loaded_fun_control = S_loaded.fun_control\n",
    "# pprint.pprint(loaded_fun_control)\n",
    "loaded_design_control = S_loaded.design_control\n",
    "loaded_surrogate_control = S_loaded.surrogate_control\n",
    "loaded_optimizer_control = S_loaded.optimizer_control\n",
    "\n",
    "# Check if the loaded data matches the original data\n",
    "# It is ok if the counter is different, because it is increased during the run\n",
    "assert _compare_dicts(loaded_fun_control, fun_control, ignore_keys=\"counter\"), \"Loaded fun_control should match the original fun_control.\"\n",
    "assert _compare_dicts(loaded_design_control, design_control), \"Loaded design_control should match the original design_control.\"\n",
    "assert _compare_dicts(loaded_surrogate_control, S.surrogate_control), \"Loaded surrogate_control should match the original surrogate_control.\"\n",
    "assert _compare_dicts(loaded_optimizer_control, S.optimizer_control), \"Loaded optimizer_control should match the original optimizer_control.\"\n",
    "\n",
    "# Check if the S_loaded is an instance of Spot\n",
    "assert isinstance(S_loaded, Spot), \"Loaded S_loaded should be an instance of Spot.\"\n",
    "\n",
    "# Check if the design matrix and response vector are equal\n",
    "# if there are differences, print the differences\n",
    "# Differences are OK\n",
    "if not np.array_equal(S_loaded.X, S.X):\n",
    "    print(f\"Design matrix mismatch: {S_loaded.X} != {S.X}\")\n",
    "if not np.array_equal(S_loaded.y, S.y):\n",
    "    print(f\"Response vector mismatch: {S_loaded.y} != {S.y}\")\n",
    "\n",
    "S_loaded.run()\n",
    "\n",
    "S.run()\n",
    "S_loaded_2 = load_result(PREFIX)\n",
    "S_loaded_2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_exp_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.eda import print_exp_table, print_res_table\n",
    "from spotpython.hyperparameters.values import set_hyperparameter\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=\"show_exp_table\",\n",
    "    fun_evals=5,\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
    "set_hyperparameter(fun_control, \"l1\", [1,2])\n",
    "set_hyperparameter(fun_control, \"epochs\", [2,2])\n",
    "set_hyperparameter(fun_control, \"batch_size\", [4,11])\n",
    "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
    "set_hyperparameter(fun_control, \"patience\", [1,2])\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "print_exp_table(fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_res_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.eda import print_res_table\n",
    "from spotpython.hyperparameters.values import set_hyperparameter\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=\"show_exp_table\",\n",
    "    fun_evals=5,\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
    "set_hyperparameter(fun_control, \"l1\", [1,2])\n",
    "set_hyperparameter(fun_control, \"epochs\", [2,2])\n",
    "set_hyperparameter(fun_control, \"batch_size\", [4,11])\n",
    "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
    "set_hyperparameter(fun_control, \"patience\", [1,2])\n",
    "design_control = design_control_init(init_size=3)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "S = Spot(fun=fun, fun_control=fun_control, design_control=design_control)\n",
    "\n",
    "S.run()\n",
    "\n",
    "print_res_table(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_new_X0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "            seed=1,\n",
    "            n_points=10,\n",
    "            ocba_delta=0,\n",
    "            lower = np.array([-1, -1]),\n",
    "            upper = np.array([1, 1])\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    ")\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "X0 = S.get_new_X0()\n",
    "assert X0.shape[0] == S.n_points\n",
    "assert X0.shape[1] == S.lower.size\n",
    "# assert new points are in the interval [lower, upper]\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "# print using 20 digits precision\n",
    "np.set_printoptions(precision=20)\n",
    "print(f\"X0: {X0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init\n",
    "nn = 10\n",
    "fun_sphere = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        n_points=nn,\n",
    "        )\n",
    "spot_1 = spot.Spot(\n",
    "    fun=fun_sphere,\n",
    "    fun_control=fun_control,\n",
    "    )\n",
    "# (S-2) Initial Design:\n",
    "spot_1.X = spot_1.design.scipy_lhd(\n",
    "    spot_1.design_control[\"init_size\"], lower=spot_1.lower, upper=spot_1.upper\n",
    ")\n",
    "print(f\"spot_1.X: {spot_1.X}\")\n",
    "# (S-3): Eval initial design:\n",
    "spot_1.y = spot_1.fun(spot_1.X)\n",
    "print(f\"spot_1.y: {spot_1.y}\")\n",
    "spot_1.fit_surrogate()\n",
    "spot_1.suggest_X0()\n",
    "X0 = spot_1.X0\n",
    "print(f\"X0: {X0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate_mean_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.aggregate import aggregate_mean_var\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4], [1, 2]])\n",
    "y = np.array([1, 2, 3])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1,2]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.ones((2, 3))\n",
    "y_1 = np.sum(X_1, axis=1)\n",
    "y_2 = 2 * y_1\n",
    "X_2 = np.append(X_1, 2 * X_1, axis=0)\n",
    "X = np.append(X_2, X_1, axis=0)\n",
    "y = np.append(y_1, y_2, axis=0)\n",
    "y = np.append(y, y_2, axis=0)\n",
    "print(X)\n",
    "print(y)\n",
    "Z = aggregate_mean_var(X, y, var_empirical=True)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.ones((2, 3))\n",
    "y_1 = np.sum(X_1, axis=1)\n",
    "y_2 = 2 * y_1\n",
    "X_2 = np.append(X_1, 2 * X_1, axis=0)\n",
    "X = np.append(X_2, X_1, axis=0)\n",
    "y = np.append(y_1, y_2, axis=0)\n",
    "y = np.append(y, y_2, axis=0)\n",
    "print(X)\n",
    "print(y)\n",
    "Z = aggregate_mean_var(X, y, var_empirical=False)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.aggregate import aggregate_mean_var\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4], [1, 2]])\n",
    "y = np.array([1, 2, 3])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y, var_empirical=True)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1,2]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y, var_empirical=True)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit_surrogate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# number of initial points:\n",
    "ni = 0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select_distant_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.aggregate import select_distant_points\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "selected_points, selected_y = select_distant_points(X, y, 3)\n",
    "print(selected_points)\n",
    "print(selected_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# number of initial points:\n",
    "ni = 0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.surrogate.predict(np.array([[0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.spot import spot\n",
    "# number of initial points:\n",
    "ni = 0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "X_shape_before = S.X.shape\n",
    "print(f\"X_shape_before: {X_shape_before}\")\n",
    "print(f\"y_size_before: {S.y.size}\")\n",
    "y_size_before = S.y.size\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.update_design()\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "print(f\"S.n_points: {S.n_points}\")\n",
    "print(f\"X_shape_after: {S.X.shape}\")\n",
    "print(f\"y_size_after: {S.y.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "        sigma=0.02,\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        noise=True,\n",
    "        ocba_delta=1,\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni, repeats=2)\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            design_control=design_control,\n",
    "            fun_control=fun_control\n",
    ")\n",
    "S.initialize_design(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "X_shape_before = S.X.shape\n",
    "print(f\"X_shape_before: {X_shape_before}\")\n",
    "print(f\"y_size_before: {S.y.size}\")\n",
    "y_size_before = S.y.size\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.update_design()\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "print(f\"S.n_points: {S.n_points}\")\n",
    "print(f\"S.ocba_delta: {S.ocba_delta}\")\n",
    "print(f\"X_shape_after: {S.X.shape}\")\n",
    "print(f\"y_size_after: {S.y.size}\")\n",
    "# compare the shapes of the X and y values before and after the update_design method\n",
    "assert X_shape_before[0] + S.ocba_delta == S.X.shape[0]\n",
    "assert X_shape_before[1] == S.X.shape[1]\n",
    "assert y_size_before + S.ocba_delta == S.y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_ocba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.budget.ocba import get_ocba\n",
    "from spotpython.utils import fun_control_init, design_control_init, surrogate_control_init\n",
    "# Example is based on the example from the book:\n",
    "# Chun-Hung Chen and Loo Hay Lee:\n",
    "#     Stochastic Simulation Optimization: An Optimal Computer Budget Allocation,\n",
    "#     pp. 49 and pp. 215\n",
    "#     p. 49:\n",
    "#     mean_y = np.array([1,2,3,4,5])\n",
    "#     var_y = np.array([1,1,9,9,4])\n",
    "#     get_ocba(mean_y, var_y, 50)\n",
    "#     [11  9 19  9  2]\n",
    "fun = Analytical().fun_linear\n",
    "fun_control = fun_control_init(\n",
    "                lower = np.array([-1]),\n",
    "                upper = np.array([1]),\n",
    "                fun_evals = 20,\n",
    "                fun_repeats = 2,\n",
    "                noise = True,\n",
    "                ocba_delta=1,\n",
    "                seed=123,\n",
    "                show_models=False,\n",
    "                sigma=0.001,\n",
    "                )\n",
    "design_control = design_control_init(init_size=3, repeats=2)\n",
    "surrogate_control = surrogate_control_init(noise=True)\n",
    "spot_1_noisy = Spot(fun=fun,                \n",
    "                fun_control = fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control)\n",
    "spot_1_noisy.run()\n",
    "spot_2 = copy.deepcopy(spot_1_noisy)\n",
    "spot_2.mean_y = np.array([1,2,3,4,5])\n",
    "spot_2.var_y = np.array([1,1,9,9,4])\n",
    "n = 50\n",
    "o = get_ocba(spot_2.mean_y, spot_2.var_y, n)\n",
    "assert sum(o) == 50\n",
    "assert (o == np.array([[11, 9, 19, 9, 2]])).all()\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_ocba_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.budget.ocba import get_ocba_X\n",
    "from spotpython.utils.aggregate import aggregate_mean_var\n",
    "import numpy as np\n",
    "X = np.array([[1,2,3],\n",
    "            [1,2,3],\n",
    "            [4,5,6],\n",
    "            [4,5,6],\n",
    "            [4,5,6],\n",
    "            [7,8,9],\n",
    "            [7,8,9],])\n",
    "y = np.array([1,2,30,40, 40, 500, 600  ])\n",
    "Z = aggregate_mean_var(X=X, y=y)\n",
    "mean_X = Z[0]\n",
    "mean_y = Z[1]\n",
    "var_y = Z[2]\n",
    "print(f\"X: {X}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"mean_X: {mean_X}\")\n",
    "print(f\"mean_y: {mean_y}\")\n",
    "print(f\"var_y: {var_y}\")\n",
    "delta = 5\n",
    "X_new = get_ocba_X(X=mean_X, means=mean_y, vars=var_y, delta=delta,verbose=True)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## suggest_new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init\n",
    "nn = 3\n",
    "fun_sphere = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        n_points=nn,\n",
    "        )\n",
    "S = Spot(\n",
    "    fun=fun_sphere,\n",
    "    fun_control=fun_control,\n",
    "    )\n",
    "S.X = S.design.scipy_lhd(\n",
    "    S.design_control[\"init_size\"], lower=S.lower, upper=S.upper\n",
    ")\n",
    "print(f\"S.X: {S.X}\")\n",
    "S.y = S.fun(S.X)\n",
    "print(f\"S.y: {S.y}\")\n",
    "S.fit_surrogate()\n",
    "X0 = S.suggest_new_X()\n",
    "print(f\"X0: {X0}\")\n",
    "assert X0.size == S.n_points * S.k\n",
    "assert X0.ndim == 2\n",
    "assert X0.shape[0] == nn\n",
    "assert X0.shape[1] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.optimize import dual_annealing\n",
    "from scipy.optimize import basinhopping\n",
    "    \n",
    "nn = 2\n",
    "fun_sphere = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower=np.array([-1, -1]),\n",
    "    upper=np.array([1, 1]),\n",
    "    n_points=nn,\n",
    ")\n",
    "design_control = design_control_init(init_size=10)\n",
    "surrogate_control = surrogate_control_init()\n",
    "\n",
    "# optimizers = [dual_annealing, differential_evolution, direct, shgo, basinhopping]\n",
    "optimizers = [differential_evolution, dual_annealing, direct, shgo]\n",
    "\n",
    "for optimizer_name in optimizers:\n",
    "    optimizer_control = optimizer_control_init()\n",
    "\n",
    "    S = Spot(\n",
    "        fun=fun_sphere,\n",
    "        fun_control=fun_control,\n",
    "        design_control=design_control,\n",
    "        optimizer_control=optimizer_control,\n",
    "        surrogate_control=surrogate_control,\n",
    "        optimizer=optimizer_name\n",
    "    )\n",
    "    \n",
    "    S.X = S.design.scipy_lhd(\n",
    "        S.design_control[\"init_size\"], lower=S.lower, upper=S.upper\n",
    "    )\n",
    "    S.y = S.fun(S.X)\n",
    "    S.fit_surrogate()\n",
    "    X0 = S.suggest_new_X()\n",
    "    print(f\"X0: {X0}\")\n",
    "\n",
    "    assert X0.size <= S.n_points * S.k\n",
    "    assert X0.ndim == 2\n",
    "    assert X0.shape[0] <= nn\n",
    "    assert X0.shape[1] == 2\n",
    "    assert np.all(X0 >= S.lower)\n",
    "    assert np.all(X0 <= S.upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=100, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.plot(X, s_ei, label=\"Expected Improvement\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, -s_ei, label=\"Negative Expected Improvement\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000001 1.        ]\n",
      " [1.         1.00000001]]\n"
     ]
    }
   ],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 0, 0], [1, 0, 0]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.fit(nat_X, nat_y)\n",
    "print(S.Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S.nat_X: [[1 2]\n",
      " [3 4]\n",
      " [1 2]]\n",
      "S.nat_y: [ 1  2 11]\n",
      "S.aggregated_mean_y: [6. 2.]\n",
      "S.min_X: [1 2]\n",
      "S.max_X: [3 4]\n",
      "S.n: 3\n",
      "S.k: 2\n"
     ]
    }
   ],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4], [1,2]])\n",
    "nat_y = np.array([1, 2, 11])\n",
    "S = Kriging()\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "print(f\"S.nat_X: {S.nat_X}\")\n",
    "print(f\"S.nat_y: {S.nat_y}\")\n",
    "print(f\"S.aggregated_mean_y: {S.aggregated_mean_y}\")\n",
    "print(f\"S.min_X: {S.min_X}\")\n",
    "print(f\"S.max_X: {S.max_X}\")\n",
    "print(f\"S.n: {S.n}\")\n",
    "print(f\"S.k: {S.k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spot312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
