{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  cache: false\n",
    "  eval: true\n",
    "  echo: true\n",
    "  warning: false\n",
    "title: 'spotpython Tests'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fun_control_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n",
    "fun_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def class_attributes_to_dataframe(class_obj):\n",
    "    # Get the attributes and their values of the class object\n",
    "    attributes = [attr for attr in dir(class_obj) if not callable(getattr(class_obj, attr)) and not attr.startswith(\"__\")]\n",
    "    values = [getattr(class_obj, attr) for attr in attributes]\n",
    "    \n",
    "    # Create a DataFrame from the attributes and values\n",
    "    df = pd.DataFrame({'Attribute Name': attributes, 'Attribute Value': values})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.name = \"John\"\n",
    "        self.age = 30\n",
    "        self.salary = 50000\n",
    "\n",
    "my_instance = MyClass()\n",
    "df = class_attributes_to_dataframe(my_instance)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "n = 10\n",
    "\n",
    "fun = analytical().fun_sphere\n",
    "lower = np.array([-1])\n",
    "upper = np.array([1])\n",
    "design_control={\"init_size\": ni}\n",
    "\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            lower = lower,\n",
    "            upper= upper,\n",
    "            fun_evals = n,\n",
    "            show_progress=True,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "df = spot_1.class_attributes_to_dataframe()\n",
    "stdout.write(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river.linear_model import LogisticRegression\n",
    "from river import metrics\n",
    "from river import optim\n",
    "from river import preprocessing\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset, model, metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.csvdataset import CSVDataset\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data.csv', target_column='prognosis')\n",
    "dataset = CSVDataset(target_column='prognosis')\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.extra_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 3\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Data set VBDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv_file='./data/spotpython/data.csv' as a pandas df and save it as a pickle file\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/spotpython/data.csv')\n",
    "df.to_pickle('./data/spotpython/data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyHcf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.data.daten_sensitive import DatenSensitive\n",
    "from pyhcf.utils.names import get_short_parameter_names\n",
    "daten = DatenSensitive()\n",
    "df = daten.load()\n",
    "names =  df.columns\n",
    "names = get_short_parameter_names(names)\n",
    "# rename columns with short names\n",
    "df.columns = names\n",
    "df.head()\n",
    "# save the df as a csv file\n",
    "df.to_csv('./data/spotpython/data_sensitive.csv', index=False)\n",
    "# save the df as a pickle file\n",
    "df.to_pickle('./data/spotpython/data_sensitive.pkl')\n",
    "# remove all rows with NaN values\n",
    "df = df.dropna()\n",
    "# save the df as a csv file\n",
    "df.to_csv('./data/spotpython/data_sensitive_rmNA.csv', index=False)\n",
    "# save the df as a pickle file\n",
    "df.to_pickle('./data/spotpython/data_sensitive_rmNA.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyHcf data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.light.csvdataset import CSVDataset\n",
    "# import torch\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5000\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     # print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.light.csvdataset import CSVDataset\n",
    "# import torch\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5000\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     # print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset.feature_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(pkl_file='./data/spotpython/data_sensitive.pkl', target_column='A', feature_type=torch.long, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test lightdatamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "# dataset = PKLDataset(directory=\"./data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set size: {len(data_module.data_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation set size: {len(data_module.data_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the DataModule in fun_control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=7)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same with the sensitive data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same, but VBDO data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = CSVDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/VBDP/\", filename=\"train.csv\",target_column='prognosis', feature_type=torch.long)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Hyperdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "lhd = LightHyperDict()\n",
    "lhd.hyper_dict\n",
    "user_lhd = LightHyperDict(filename=\"user_hyper_dict.json\", directory=\"./hyperdict/\")\n",
    "user_lhd.hyper_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes  \n",
    "import torch\n",
    "\n",
    "# Load the diabetes dataset\n",
    "feature_df, target_df = load_diabetes(return_X_y=True, as_frame=True)\n",
    "feature_tensor = torch.tensor(feature_df.values, dtype=torch.float32)\n",
    "target_tensor = torch.tensor(target_df.values, dtype=torch.float32)\n",
    "feature_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "dataset = Diabetes()\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add core model to fun control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.netlightregressione import NetLightRegression\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "fun_control[\"core_model\"].__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the fun_control[\"core_model_hyper_dict\"] is a LightHyperDict\n",
    "isinstance(fun_control[\"core_model_hyper_dict\"], dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test check_X_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "n_hyperparams = len(get_var_name(fun_control))\n",
    "# generate a random np.array X with shape (2, n_hyperparams)\n",
    "X = np.random.rand(2, n_hyperparams)\n",
    "X == hyper_light.check_X_shape(X, fun_control)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test hyperlight fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                    key=\"data_set\",\n",
    "                    value=dataset)\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "X = np.vstack((X, X))\n",
    "y = hyper_light.fun(X, fun_control)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test  NetLightRegression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader)) \n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests optimizer_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "lr_mult=0.1\n",
    "\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adam', dropout_prob=0.1, lr_mult=lr_mult,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# Adam uses a lr which is calculated as lr=lr_mult * 0.001, so this value\n",
    "# should be 0.1 * 0.001 = 0.0001 \n",
    "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*0.001\n",
    "\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adadelta', dropout_prob=0.1, lr_mult=lr_mult,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# Adadelta uses a lr which is calculated as lr=lr_mult * 1.0, so this value\n",
    "# should be 1.0 * 0.1 = 0.1 \n",
    "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
    "from spotpython.light.traintest import train_model, test_model\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "for config in generate_one_config_from_var_dict(var_dict, fun_control):\n",
    "    y_train = train_model(config, fun_control)\n",
    "    y_test = test_model(config, fun_control)\n",
    "    break\n",
    "print(y_train)\n",
    "print(y_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
    "from spotpython.light.traintest import test_model\n",
    "\n",
    "\n",
    "def test_traintest_test_model():\n",
    "    fun_control = fun_control_init(\n",
    "        _L_in=10,\n",
    "        _L_out=1,)\n",
    "\n",
    "    dataset = Diabetes()\n",
    "    set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "    add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                                fun_control=fun_control,\n",
    "                                hyper_dict=LightHyperDict)\n",
    "    X = get_default_hyperparameters_as_array(fun_control)\n",
    "    var_dict = assign_values(X, get_var_name(fun_control))\n",
    "    for vals in generate_one_config_from_var_dict(var_dict, fun_control):\n",
    "        y_test = test_model(test_config=vals,\n",
    "                            fun_control=fun_control)\n",
    "        break\n",
    "    # check if y is a float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test getVarName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = {\"core_model_hyper_dict\":{\n",
    "            \"leaf_prediction\": {\n",
    "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"mean\",\n",
    "                \"core_model_parameter_type\": \"str\"},\n",
    "            \"leaf_model\": {\n",
    "                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"LinearRegression\",\n",
    "                \"core_model_parameter_type\": \"instance\"},\n",
    "            \"splitter\": {\n",
    "                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"EBSTSplitter\",\n",
    "                \"core_model_parameter_type\": \"instance()\"},\n",
    "            \"binary_split\": {\n",
    "                \"levels\": [0, 1],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": 0,\n",
    "                \"core_model_parameter_type\": \"bool\"},\n",
    "            \"stop_mem_management\": {\n",
    "                \"levels\": [0, 1],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": 0,\n",
    "                \"core_model_parameter_type\": \"bool\"}}}\n",
    "len(get_var_name(fun_control))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test netlightregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from math import inf\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import modify_hyper_parameter_bounds\n",
    "from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import (get_bound_values,\n",
    "    get_var_name,\n",
    "    get_var_type,)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.hyperparameters.values import get_tuned_architecture\n",
    "from spotpython.light.testmodel import test_model\n",
    "from spotpython.light.loadmodel import load_light_from_checkpoint\n",
    "\n",
    "MAX_TIME = 1\n",
    "INIT_SIZE = 5\n",
    "WORKERS = 0\n",
    "PREFIX=\"031\"\n",
    "\n",
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    num_workers=WORKERS,\n",
    "    device=getDevice(),\n",
    "    _L_in=133,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True)\n",
    "\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[5,8])\n",
    "modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[3,5])\n",
    "modify_hyper_parameter_bounds(fun_control, \"batch_size\", bounds=[2, 8])\n",
    "modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
    "\n",
    "print(gen_design_table(fun_control))\n",
    "\n",
    "var_type = get_var_type(fun_control)\n",
    "var_name = get_var_name(fun_control)\n",
    "lower = get_bound_values(fun_control, \"lower\")\n",
    "upper = get_bound_values(fun_control, \"upper\")\n",
    "fun = HyperLight(log_level=50).fun\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       log_level=50,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   max_time = MAX_TIME,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type = var_type,\n",
    "                   var_name = var_name,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": INIT_SIZE},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": len(var_name),\n",
    "                                      \"model_fun_evals\": 10_000,\n",
    "                                      })\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_progress(log_y=False, filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_importance(threshold=0.025, filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_tuned_architecture(spot_tuner, fun_control)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = load_light_from_checkpoint(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_important_hyperparameter_contour(filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.parallel_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.cvmodel import cv_model\n",
    "# set the number of folds to 10\n",
    "fun_control[\"k_folds\"] = 10\n",
    "cv_model(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "fun = analytical(sigma=1.0, seed=123)\n",
    "fun.add_noise(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "print(np.array([1, 2, 3, 4, 5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(seed=4321, sigma=0.1)\n",
    "fun = analytical(seed=222, sigma=0.0).fun_sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_1 = spot.Spot(fun=fun,\n",
    "                   lower = np.array([-10]),\n",
    "                   upper = np.array([100]),\n",
    "                   fun_evals = 100,\n",
    "                   fun_repeats = 3,\n",
    "                   max_time = inf,\n",
    "                   noise = True,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type=[\"num\"],\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=111,\n",
    "                   log_level = 10,\n",
    "                   show_models=False,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": 5,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": 1,\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 1000,\n",
    "                                      })\n",
    "spot_1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def squared_euclidean_distance(X_0, X, theta):\n",
    "    return np.sum(theta*(X_0 - X)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
    "from numpy.linalg import cholesky, solve\n",
    "from numpy.random import multivariate_normal\n",
    "def build_Psi(X, theta):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    D = zeros((k, n, n))\n",
    "    for l in range(k):\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
    "    D = sum(D)\n",
    "    D = D + D.T\n",
    "    return exp(-D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0, 1.0])\n",
    "X = np.array([[1.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_Psi(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n",
    "fun = analytical()\n",
    "fun.fun_branin_factor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pi = np.pi\n",
    "X = np.array([[0,0], [-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
    "fun = analytical()\n",
    "fun.fun_branin(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "pi = np.pi\n",
    "X_0 = np.array([[0, 0]])\n",
    "X_1 = np.array([[-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
    "X_2 = np.array([[0,0,0], [0,0,1], [0,0,2]])\n",
    "fun = analytical()\n",
    "y_0 = fun.fun_branin(X_0)\n",
    "y_1 = fun.fun_branin(X_1)\n",
    "y_2 = fun.fun_branin_factor(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(y_1[0], 2) == round(y_1[1],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "round(y_1[0], 2) == round(y_1[2],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[0] == y_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[1] == y_0 + 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[2] == y_0 - 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "import numpy as np\n",
    "n = 100\n",
    "X = np.linspace(0, 10, n, endpoint=False).reshape(-1,1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
    "from numpy.linalg import cholesky, solve\n",
    "from numpy.random import multivariate_normal\n",
    "def build_Psi(X, theta):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    D = zeros((k, n, n))\n",
    "    for l in range(k):\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
    "    D = sum(D)\n",
    "    D = D + D.T\n",
    "    return exp(-D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0])\n",
    "Psi = build_Psi(X, theta)\n",
    "np.round(Psi[:3,:], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = (3, 1, 1), check_valid=\"raise\")\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Y to a 3 x 100 array\n",
    "Y = np.squeeze(Y)\n",
    "Y.shape\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 samples from the GP as a function of X\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, Y.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = 3, check_valid=\"raise\")\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 samples from the GP as a function of X\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, Y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Targets Shape: {targets.shape}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test HyperLight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "n_hyperparams = len(get_var_name(fun_control))\n",
    "# generate a random np.array X with shape (2, n_hyperparams)\n",
    "X = np.random.rand(2, n_hyperparams)\n",
    "X == hyper_light.check_X_shape(X, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n",
    "    get_default_hyperparameters_as_array)\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "# so that two values are returned\n",
    "X = np.vstack((X, X))\n",
    "hyper_light.fun(X, fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test pkldataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\",\n",
    "                    filename=\"data_sensitive.pkl\",\n",
    "                    target_column='N',\n",
    "                    feature_type=torch.float32,\n",
    "                    target_type=torch.float32,\n",
    "                    rmNA=True)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_bound_values\n",
    "from spotpython.hyperparameters.values import get_control_key_value, set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_type_from_var_name\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "set_control_key_value(control_dict=fun_control, key=\"var_type\", value=[\"int\", \"float\", \"str\"], replace=True)\n",
    "set_control_key_value(control_dict=fun_control, key=\"var_name\", value=[\"max_depth\", \"learning_rate\", \"model_type\"], replace=True)\n",
    "\n",
    "print(fun_control)\n",
    "\n",
    "# Test with existing var_name\n",
    "assert get_var_type_from_var_name(var_name=\"max_depth\", fun_control=fun_control) == \"int\"\n",
    "assert get_var_type_from_var_name(var_name=\"learning_rate\", fun_control=fun_control) == \"float\"\n",
    "assert get_var_type_from_var_name(var_name=\"model_type\", fun_control=fun_control) == \"str\"\n",
    "\n",
    "# Test with non-existing var_name\n",
    "with pytest.raises(ValueError):\n",
    "    get_var_type_from_var_name(var_name=\"non_existing\", fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_control_key_value\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import get_var_type_from_var_name\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                    fun_control=fun_control,\n",
    "                    hyper_dict=LightHyperDict)\n",
    "var_type = get_control_key_value(control_dict=fun_control, key=\"var_type\")\n",
    "var_name = get_control_key_value(control_dict=fun_control, key=\"var_name\")\n",
    "print(var_type)\n",
    "print(var_name)\n",
    "vn = \"l1\"\n",
    "get_var_type_from_var_name(fun_control=fun_control, var_name=vn)\n",
    "\n",
    "assert var_type[var_name.index(vn)] == \"int\"\n",
    "assert get_var_type_from_var_name(fun_control, vn) == \"int\"\n",
    "vn = \"initialization\"\n",
    "assert var_type[var_name.index(vn)] == \"factor\"\n",
    "assert var_type[var_name.index(vn)] == \"factor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_control_key_value\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                    fun_control=fun_control,\n",
    "                    hyper_dict=LightHyperDict)\n",
    "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"l1\", value=[1,7])\n",
    "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"initialization\", value=[\"xavier2\", \"kaiming2\"])\n",
    "print(fun_control)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry(dictionary, key, i):\n",
    "    if key in dictionary:\n",
    "        if 'levels' in dictionary[key]:\n",
    "            if i < len(dictionary[key]['levels']):\n",
    "                return dictionary[key]['levels'][i]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from spotpython.data.pkldataset_intern import PKLDataset\n",
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from math import inf\n",
    "\n",
    "MAX_TIME = 60\n",
    "FUN_EVALS = inf\n",
    "INIT_SIZE = 25\n",
    "WORKERS = 0\n",
    "PREFIX=\"031\"\n",
    "DEVICE = getDevice()\n",
    "\n",
    "\n",
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    device=DEVICE,\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=FUN_EVALS,\n",
    "    log_level=10,\n",
    "    max_time=MAX_TIME,\n",
    "    num_workers=WORKERS,\n",
    "    show_progress=True,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "\n",
    "dataset = Diabetes()\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True, rmMF=True)\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"_L_in\",\n",
    "                        value=133,\n",
    "                        replace=True)\n",
    "\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "# from spotpython.hyperparameters.values import modify_hyper_parameter_bounds\n",
    "\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
    "set_control_hyperparameter_value(fun_control, \"epochs\", [4,9])\n",
    "set_control_hyperparameter_value(fun_control, \"batch_size\", [1, 4])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry(dictionary, key, i):\n",
    "    if 'core_model_hyper_dict' in dictionary:\n",
    "        if key in dictionary['core_model_hyper_dict']:\n",
    "            if 'levels' in dictionary['core_model_hyper_dict'][key]:\n",
    "                if i < len(dictionary['core_model_hyper_dict'][key]['levels']):\n",
    "                    return dictionary['core_model_hyper_dict'][key]['levels'][i]\n",
    "    return None\n",
    "print(get_entry(fun_control, \"optimizer\", 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "import numpy as np\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "experiment_name = get_experiment_name(prefix=\"000\")\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    device=getDevice(),\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=15,\n",
    "    log_level=10,\n",
    "    max_time=1,\n",
    "    num_workers=0,\n",
    "    show_progress=True,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
    "assert get_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0) == \"Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_timestamp(only_int=True):\n",
    "    dt = datetime.datetime.now().isoformat(sep=\" \", timespec=\"microseconds\")\n",
    "    if only_int:\n",
    "        # remove - . : and space\n",
    "        dt = dt.replace(\"-\", \"\")\n",
    "        dt = dt.replace(\".\", \"\")\n",
    "        dt = dt.replace(\":\", \"\")\n",
    "        dt = dt.replace(\" \", \"\")\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, surrogate_control_init, design_control_init\n",
    ")\n",
    "\n",
    "def test_plot_progress():\n",
    "    # number of initial points:\n",
    "    ni = 7\n",
    "    # number of points\n",
    "    fun_evals = 10\n",
    "    fun = analytical().fun_sphere\n",
    "    fun_control = fun_control_init(\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        fun_evals=fun_evals,\n",
    "        tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "    design_control=design_control_init(init_size=ni)\n",
    "    surrogate_control=surrogate_control_init(n_theta=3)\n",
    "    S = spot.Spot(fun=fun,\n",
    "                    fun_control=fun_control,\n",
    "                    design_control=design_control,\n",
    "                    surrogate_control=surrogate_control,)\n",
    "    S.run()\n",
    "\n",
    "    # Test plot_progress with different parameters\n",
    "    S.plot_progress(show=False)  # Test with show=False\n",
    "    S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "    S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "    S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename\n",
    "    # add NaN to S.y at position 2\n",
    "    S.y[2] = np.nan\n",
    "    S.plot_progress(show=False)  # Test with show=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, surrogate_control_init, design_control_init\n",
    ")\n",
    "\n",
    "\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "                fun_control=fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "\n",
    "# remove points from S.y so that there are less than ni points\n",
    "S.y = S.y[:3]\n",
    "# Test plot_progress with different parameters\n",
    "S.plot_progress(show=False)  # Test with show=False\n",
    "S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import differential_evolution\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "def objective_function(X, fun_control=None):\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X)\n",
    "    if X.shape[1] != 2:\n",
    "        raise Exception\n",
    "    x0 = X[:, 0]\n",
    "    x1 = X[:, 1]\n",
    "    y = x0**2 + 10*x1**2\n",
    "    return y\n",
    "fun_control = fun_control_init(\n",
    "            lower = np.array([0, 0]),\n",
    "            upper = np.array([10, 10]),\n",
    "            fun_evals=8,\n",
    "            fun_repeats=1,\n",
    "            max_time=inf,\n",
    "            noise=True,\n",
    "            tolerance_x=0,\n",
    "            ocba_delta=0,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            infill_criterion=\"ei\",\n",
    "            n_points=1,\n",
    "            seed=123,\n",
    "            log_level=10,\n",
    "            show_models=False,\n",
    "            show_progress=True)\n",
    "design_control = design_control_init(\n",
    "            init_size=5,\n",
    "            repeats=1)\n",
    "surrogate_control = surrogate_control_init(\n",
    "            log_level=10,\n",
    "            model_optimizer=differential_evolution,\n",
    "            model_fun_evals=10000,\n",
    "            min_theta=-3,\n",
    "            max_theta=3,\n",
    "            n_theta=2,\n",
    "            theta_init_zero=True,\n",
    "            n_p=1,\n",
    "            optim_p=False,\n",
    "            noise=True,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            seed=124, \n",
    "            min_Lambda=1,\n",
    "            max_Lambda=10)\n",
    "optimizer_control = optimizer_control_init(\n",
    "            max_iter=1000,\n",
    "            seed=125)\n",
    "spot = spot.Spot(fun=objective_function,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,\n",
    "            optimizer_control=optimizer_control\n",
    "            )\n",
    "spot.run()\n",
    "spot.plot_progress()\n",
    "spot.plot_contour(i=0, j=1)\n",
    "spot.plot_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n",
    "\n",
    "fun = analytical().fun_branin\n",
    "fun_control = fun_control_init(lower = np.array([-5, 0]),\n",
    "                               upper = np.array([10, 15]),\n",
    "                               fun_evals=20)\n",
    "design_control = design_control_init(init_size=10)\n",
    "surrogate_control = surrogate_control_init(n_theta=2)\n",
    "S = spot.Spot(fun=fun, fun_control=fun_control, design_control=design_control)\n",
    "S.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.plot_progress(log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.surrogate.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = analytical().fun_sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,100).reshape(-1,1)\n",
    "y = fun(x)\n",
    "plt.figure()\n",
    "plt.plot(x,y, \"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "                   fun_control=fun_control_init(\n",
    "                        lower = np.array([-10]),\n",
    "                        upper = np.array([100]),\n",
    "                        fun_evals = 7,\n",
    "                        fun_repeats = 1,\n",
    "                        max_time = inf,\n",
    "                        noise = False,\n",
    "                        tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                        var_type=[\"num\"],\n",
    "                        infill_criterion = \"y\",\n",
    "                        n_points = 1,\n",
    "                        seed=123,\n",
    "                        log_level = 50),\n",
    "                   design_control=design_control_init(\n",
    "                        init_size=5,\n",
    "                        repeats=1),\n",
    "                   surrogate_control=surrogate_control_init(\n",
    "                        noise=False,\n",
    "                        min_theta=-4,\n",
    "                        max_theta=3,\n",
    "                        n_theta=1,\n",
    "                        model_optimizer=differential_evolution,\n",
    "                        model_fun_evals=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "fun = analytical().fun_sphere\n",
    "from spotpython.design.spacefilling import spacefilling\n",
    "design = spacefilling(2)\n",
    "from scipy.optimize import differential_evolution\n",
    "optimizer = differential_evolution\n",
    "from spotpython.build.kriging import Kriging\n",
    "surrogate = Kriging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\n",
    "fun_control=fun_control_init(lower=np.array([-1, -1]),\n",
    "                            upper=np.array([1, 1]))\n",
    "design_control=design_control_init()\n",
    "optimizer_control=optimizer_control_init()\n",
    "surrogate_control=surrogate_control_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       fun_control=fun_control,\n",
    "                       design_control=design_control,\n",
    "                       optimizer_control=optimizer_control,\n",
    "                       surrogate_control=surrogate_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytest\n",
    "import torch\n",
    "from pyhcf.data.loadHcfData import build_df, load_hcf_data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list=[\"L\", \"AQ\", \"AS\"]\n",
    "dataset = load_hcf_data(param_list=p_list, target=\"T\",\n",
    "                        rmNA=True, rmMF=True,\n",
    "                        load_all_features=False,\n",
    "                        load_thermo_features=False,\n",
    "                        scale_data=True,\n",
    "                        return_X_y=False)\n",
    "assert isinstance(dataset, torch.utils.data.TensorDataset)\n",
    "assert len(dataset) > 0\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader    \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    assert inputs.size(0) == batch_size\n",
    "    print(f\"Inputs Shape: {inputs.shape[1]}\")\n",
    "    print(f\"P List: {p_list}\")\n",
    "    print(f\"P List Length: {len(p_list)}\")\n",
    "    # input is p_list + 1 (for target)\n",
    "    # p_list = [\"L\", \"AQ\", \"AS\"] plus target \"N\"\n",
    "    assert inputs.shape[1] + 1 == len(p_list)\n",
    "    print(f\"Targets Shape: {targets.shape[0]}\")\n",
    "    assert targets.shape[0] == batch_size\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "# data.csv is simple csv file with 11 samples\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup(stage=\"predict\")\n",
    "print(f\"Predict set size: {len(data_module.data_predict)}\")\n",
    "for batch in data_module.predict_dataloader():\n",
    "    inputs, targets = batch\n",
    "    print(f\"inputs: {inputs}\")\n",
    "    print(f\"targets: {targets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_module.data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_div2_list(n, n_min):\n",
    "    result = []\n",
    "    current = n\n",
    "    while current >= n_min:\n",
    "        result.extend([current] * (n // current))\n",
    "        current = current // 2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_L_in = 128\n",
    "l1 = \n",
    "\n",
    "n_low = _L_in // 4\n",
    "# ensure that n_high is larger than n_low\n",
    "n_high = max(l1, 2 * n_low)\n",
    "generate_div2_list(n_high, n_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.math import generate_div2_list\n",
    "generate_div2_list(64, 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.transformer.positionalEncoding import PositionalEncoding\n",
    "import torch\n",
    "# number of tensors\n",
    "n = 3\n",
    "# dimension of each tensor\n",
    "k = 32\n",
    "pe = PositionalEncoding(d_model=k, dropout_prob=0, verbose=False)\n",
    "input = torch.zeros(1, n, k)\n",
    "# Generate a tensor of size (1, 10, 4) with values from 1 to 10\n",
    "for i in range(n):\n",
    "    input[0, i, :] = i\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Input: {input}\")\n",
    "output = pe(input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.transformer.skiplinear import SkipLinear\n",
    "import torch\n",
    "n_in = 2\n",
    "n_out = 4\n",
    "sl = SkipLinear(n_in, n_out)\n",
    "input = torch.zeros(1, n_in)\n",
    "for i in range(n_in):\n",
    "    input[0, i] = i\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Input: {input}\")\n",
    "output = sl(input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(sl.lst_modules)\n",
    "for i in sl.lst_modules:\n",
    "    print(f\"weights: {i.weights}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Example from J. Caffrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_income_transformer.py\n",
    "# predict income from sex, age, city, politics\n",
    "# PyTorch 2.0.0-CPU Anaconda3-2022.10  Python 3.9.13\n",
    "# Windows 10/11 \n",
    "\n",
    "# Transformer component for regression\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "\n",
    "device = T.device('cpu')  # apply to Tensor or Module\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class PeopleDataset(T.utils.data.Dataset):\n",
    "  def __init__(self, src_file):\n",
    "    # sex age   state   income   politics\n",
    "    # -1  0.27  0 1 0   0.7610   0 0 1\n",
    "    # +1  0.19  0 0 1   0.6550   1 0 0\n",
    "\n",
    "    tmp_x = np.loadtxt(src_file, usecols=[0,1,2,3,4,6,7,8],\n",
    "      delimiter=\",\", comments=\"#\", dtype=np.float32)\n",
    "    tmp_y = np.loadtxt(src_file, usecols=5, delimiter=\",\",\n",
    "      comments=\"#\", dtype=np.float32)\n",
    "    tmp_y = tmp_y.reshape(-1,1)  # 2D required\n",
    "\n",
    "    self.x_data = T.tensor(tmp_x, dtype=T.float32).to(device)\n",
    "    self.y_data = T.tensor(tmp_y, dtype=T.float32).to(device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    preds = self.x_data[idx]\n",
    "    incom = self.y_data[idx] \n",
    "    return (preds, incom)  # as a tuple\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class SkipLinear(T.nn.Module):\n",
    "\n",
    "  # -----\n",
    "\n",
    "  class Core(T.nn.Module):\n",
    "    def __init__(self, n):\n",
    "      super().__init__()\n",
    "      # 1 node to n nodes, n gte 2\n",
    "      self.weights = T.nn.Parameter(T.zeros((n,1),\n",
    "        dtype=T.float32))\n",
    "      self.biases = T.nn.Parameter(T.tensor(n,\n",
    "        dtype=T.float32))\n",
    "      lim = 0.01\n",
    "      T.nn.init.uniform_(self.weights, -lim, lim)\n",
    "      T.nn.init.zeros_(self.biases)\n",
    "\n",
    "    def forward(self, x):\n",
    "      wx= T.mm(x, self.weights.t())\n",
    "      v = T.add(wx, self.biases)\n",
    "      return v\n",
    "\n",
    "  # -----\n",
    "\n",
    "  def __init__(self, n_in, n_out):\n",
    "    super().__init__()\n",
    "    self.n_in = n_in; self.n_out = n_out\n",
    "    if n_out  % n_in != 0:\n",
    "      print(\"FATAL: n_out must be divisible by n_in\")\n",
    "    n = n_out // n_in  # num nodes per input\n",
    "\n",
    "    self.lst_modules = \\\n",
    "      T.nn.ModuleList([SkipLinear.Core(n) for \\\n",
    "        i in range(n_in)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    lst_nodes = []\n",
    "    for i in range(self.n_in):\n",
    "      xi = x[:,i].reshape(-1,1)\n",
    "      oupt = self.lst_modules[i](xi)\n",
    "      lst_nodes.append(oupt)\n",
    "    result = T.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "    for i in range(2,self.n_in):\n",
    "      result = T.cat((result, lst_nodes[i]), 1)\n",
    "    result = result.reshape(-1, self.n_out)\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class PositionalEncoding(T.nn.Module):  # documentation code\n",
    "  def __init__(self, d_model: int, dropout: float=0.1,\n",
    "   max_len: int=5000):\n",
    "    super(PositionalEncoding, self).__init__()  # old syntax\n",
    "    self.dropout = T.nn.Dropout(p=dropout)\n",
    "    pe = T.zeros(max_len, d_model)  # like 10x4\n",
    "    position = \\\n",
    "      T.arange(0, max_len, dtype=T.float).unsqueeze(1)\n",
    "    div_term = T.exp(T.arange(0, d_model, 2).float() * \\\n",
    "      (-np.log(10_000.0) / d_model))\n",
    "    pe[:, 0::2] = T.sin(position * div_term)\n",
    "    pe[:, 1::2] = T.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    self.register_buffer('pe', pe)  # allows state-save\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:x.size(0), :]\n",
    "    return self.dropout(x)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class TransformerNet(T.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TransformerNet, self).__init__()\n",
    "    self.embed = SkipLinear(8, 32)  # 8 inputs, each goes to 4 \n",
    "    self.pos_enc = \\\n",
    "      PositionalEncoding(4, dropout=0.20)  # positional\n",
    "    self.enc_layer = T.nn.TransformerEncoderLayer(d_model=4,\n",
    "      nhead=2, dim_feedforward=10, \n",
    "      batch_first=True)  # d_model divisible by nhead\n",
    "    self.trans_enc = T.nn.TransformerEncoder(self.enc_layer,\n",
    "      num_layers=2)  # 6 layers default\n",
    "\n",
    "    self.fc1 = T.nn.Linear(32, 10)  # 8--32-T-10-1\n",
    "    self.fc2 = T.nn.Linear(10, 1)\n",
    "\n",
    "    # default weight and bias initialization\n",
    "\n",
    "  def forward(self, x):\n",
    "    z = self.embed(x)  # 8 inpts to 32 embed\n",
    "    z = z.reshape(-1, 8, 4)  # bat seq embed\n",
    "    z = self.pos_enc(z) \n",
    "    z = self.trans_enc(z) \n",
    "    z = z.reshape(-1, 32)  # torch.Size([bs, xxx])\n",
    "    z = T.tanh(self.fc1(z))\n",
    "    z = self.fc2(z)  # regression: no activation\n",
    "    return z\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def accuracy(model, ds, pct_close):\n",
    "  # assumes model.eval()\n",
    "  # correct within pct of true income\n",
    "  n_correct = 0; n_wrong = 0\n",
    "\n",
    "  for i in range(len(ds)):\n",
    "    X = ds[i][0].reshape(1,-1)  # make it a batch\n",
    "    Y = ds[i][1].reshape(1)\n",
    "    with T.no_grad():\n",
    "      oupt = model(X)         # computed income\n",
    "\n",
    "    if T.abs(oupt - Y) <= T.abs(pct_close * Y):\n",
    "      n_correct += 1\n",
    "    else:\n",
    "      n_wrong += 1\n",
    "  acc = (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "  return acc\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def accuracy_x(model, ds, pct_close):\n",
    "  # all-at-once (quick)\n",
    "  # assumes model.eval()\n",
    "  X = ds.x_data  # all inputs\n",
    "  Y = ds.y_data  # all targets\n",
    "  n_items = len(X)\n",
    "  with T.no_grad():\n",
    "    pred = model(X)  # all predicted incomes\n",
    " \n",
    "  n_correct = T.sum((T.abs(pred - Y) <= \\\n",
    "    T.abs(pct_close * Y)))\n",
    "  result = (n_correct.item() / n_items)  # scalar\n",
    "  return result  \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def train(model, ds, bs, lr, me, le, test_ds):\n",
    "  # dataset, bat_size, lrn_rate, max_epochs, log interval\n",
    "  train_ldr = T.utils.data.DataLoader(ds, batch_size=bs,\n",
    "    shuffle=True)\n",
    "  loss_func = T.nn.MSELoss()\n",
    "  optimizer = T.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "  for epoch in range(0, me):\n",
    "    epoch_loss = 0.0  # for one full epoch\n",
    "    for (b_idx, batch) in enumerate(train_ldr):\n",
    "      X = batch[0]  # predictors\n",
    "      y = batch[1]  # target income\n",
    "      optimizer.zero_grad()\n",
    "      oupt = model(X)\n",
    "      loss_val = loss_func(oupt, y)  # a tensor\n",
    "      epoch_loss += loss_val.item()  # accumulate\n",
    "      loss_val.backward()  # compute gradients\n",
    "      optimizer.step()     # update weights\n",
    "\n",
    "    if epoch % le == 0:\n",
    "      print(\"epoch = %4d  |  loss = %0.4f\" % \\\n",
    "        (epoch, epoch_loss))\n",
    "      # model.eval()\n",
    "      # print(\"-------------\")\n",
    "      # acc_train = accuracy(model, ds, 0.10)\n",
    "      # print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
    "      # acc_test = accuracy(model, test_ds, 0.10) \n",
    "      # print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
    "      # model.train()\n",
    "      # print(\"-------------\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "  # 0. get started\n",
    "  print(\"\\nBegin People predict income using Transformer \")\n",
    "  T.manual_seed(0)\n",
    "  np.random.seed(0)\n",
    "  \n",
    "\n",
    "\n",
    "  # 1. create Dataset objects\n",
    "  print(\"\\nCreating People Dataset objects \")\n",
    "  train_file = \"../src/spotpython/data/people_train.csv\"\n",
    "  train_ds = PeopleDataset(train_file)  # 200 rows\n",
    "\n",
    "  test_file = \"../src/spotpython/data/people_test.csv\"\n",
    "  test_ds = PeopleDataset(test_file)  # 40 rows\n",
    "\n",
    "  # 2. create network\n",
    "  print(\"\\nCreating (8--32)-T-10-1 neural network \")\n",
    "  net = TransformerNet().to(device)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 3. train model\n",
    "  print(\"\\nbat_size = 10 \")\n",
    "  print(\"loss = MSELoss() \")\n",
    "  print(\"optimizer = Adam \")\n",
    "  print(\"lrn_rate = 0.01 \")\n",
    "\n",
    "  print(\"\\nStarting training\")\n",
    "  net.train()\n",
    "  train(net, train_ds, bs=10, lr=0.01, me=300,\n",
    "    le=50, test_ds=test_ds)\n",
    "  print(\"Done \")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 4. evaluate model accuracy\n",
    "  print(\"\\nComputing model accuracy (within 0.10 of true) \")\n",
    "  net.eval()\n",
    "  acc_train = accuracy(net, train_ds, 0.10)  # item-by-item\n",
    "  print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
    "\n",
    "  acc_test = accuracy_x(net, test_ds, 0.10)  # all-at-once\n",
    "  print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 5. make a prediction\n",
    "  print(\"\\nPredicting income for M 34 Oklahoma moderate: \")\n",
    "  x = np.array([[-1, 0.34, 0,0,1,  0,1,0]],\n",
    "    dtype=np.float32)\n",
    "  x = T.tensor(x, dtype=T.float32).to(device) \n",
    "\n",
    "  with T.no_grad():\n",
    "    pred_inc = net(x)\n",
    "  pred_inc = pred_inc.item()  # scalar\n",
    "  print(\"$%0.2f\" % (pred_inc * 100_000))  # un-normalized\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 6. save model (state_dict approach)\n",
    "  print(\"\\nSaving trained model state\")\n",
    "  fn = \".\\\\Models\\\\people_income_model.pt\"\n",
    "  T.save(net.state_dict(), fn)\n",
    "\n",
    "  # model = Net()\n",
    "  # model.load_state_dict(T.load(fn))\n",
    "  # use model to make prediction(s)\n",
    "\n",
    "  print(\"\\nEnd People income demo \")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SkipLinear(torch.nn.Module):\n",
    "    class Core(torch.nn.Module):\n",
    "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
    "\n",
    "        def __init__(self, n):\n",
    "            \"\"\"\n",
    "            Initialize the layer.\n",
    "\n",
    "            Args:\n",
    "                n (int): The number of output nodes.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
    "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
    "            lim = 0.01\n",
    "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
    "\n",
    "        def forward(self, x)->torch.Tensor:\n",
    "            \"\"\"\n",
    "            Forward pass through the layer.\n",
    "\n",
    "            Args:\n",
    "                x (torch.Tensor): The input tensor.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The output of the layer.\n",
    "            \"\"\"\n",
    "            return x @ self.weights.t() + self.biases\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        if n_out % n_in != 0:\n",
    "            raise ValueError(\"n_out % n_in != 0\")\n",
    "        n = n_out // n_in  # num nodes per input\n",
    "\n",
    "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        lst_nodes = []\n",
    "        for i in range(self.n_in):\n",
    "            xi = x[:, i].reshape(-1, 1)\n",
    "            oupt = self.lst_modules[i](xi)\n",
    "            lst_nodes.append(oupt)\n",
    "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "        for i in range(2, self.n_in):\n",
    "            result = torch.cat((result, lst_nodes[i]), 1)\n",
    "        result = result.reshape(-1, self.n_out)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipLinear(torch.nn.Module):\n",
    "\n",
    "    class Core(torch.nn.Module):\n",
    "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
    "\n",
    "        def __init__(self, n):\n",
    "            \"\"\"\n",
    "            Initialize the layer.\n",
    "\n",
    "            Args:\n",
    "                n (int): The number of output nodes.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
    "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
    "            lim = 0.01\n",
    "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
    "\n",
    "        def forward(self, x) -> torch.Tensor:\n",
    "            \"\"\"\n",
    "            Forward pass through the layer.\n",
    "\n",
    "            Args:\n",
    "                x (torch.Tensor): The input tensor.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The output of the layer.\n",
    "            \"\"\"\n",
    "            return x @ self.weights.t() + self.biases\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        if n_out % n_in != 0:\n",
    "            raise ValueError(\"n_out % n_in != 0\")\n",
    "        n = n_out // n_in  # num nodes per input\n",
    "\n",
    "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        lst_nodes = []\n",
    "        for i in range(self.n_in):\n",
    "            xi = x[:, i].reshape(-1, 1)\n",
    "            oupt = self.lst_modules[i](xi)\n",
    "            lst_nodes.append(oupt)\n",
    "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "        for i in range(2, self.n_in):\n",
    "            result = torch.cat((result, lst_nodes[i]), 1)\n",
    "        result = result.reshape(-1, self.n_out)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spotGUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import get_default_values, get_bound_values\n",
    "lhd = LightHyperDict()\n",
    "# generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
    "fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
    "get_default_values(fun_control)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import json\n",
    "from spotpython.hyperparameters.values import get_default_values, get_bound_values\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "\n",
    "\n",
    "def create_gui(model):\n",
    "    lhd = LightHyperDict()\n",
    "    # generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
    "    fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
    "\n",
    "    # Apply the functions to the dictionary\n",
    "    default_values = get_default_values(fun_control)\n",
    "    lower_bound_values = get_bound_values(fun_control, \"lower\")\n",
    "    upper_bound_values = get_bound_values(fun_control, \"upper\")\n",
    "\n",
    "    # Create a tkinter window\n",
    "    root = tk.Tk()\n",
    "\n",
    "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
    "    for i, (key, value) in enumerate(lhd.hyper_dict['NetLightRegression'].items()):\n",
    "            # Create a label with the key as text\n",
    "            label = tk.Label(root, text=key)\n",
    "            label.grid(row=i, column=0, sticky=\"W\")\n",
    "\n",
    "            # Create an entry with the default value as the default text\n",
    "            default_entry = tk.Entry(root)\n",
    "            default_entry.insert(0, value)\n",
    "            default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "        # add the lower bound values in column 2\n",
    "            lower_bound_entry = tk.Entry(root)\n",
    "            lower_bound_entry.insert(0, lower_bound_values[i])\n",
    "            lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
    "        # add the upper bound values in column 3\n",
    "            upper_bound_entry = tk.Entry(root)\n",
    "            upper_bound_entry.insert(0, upper_bound_values[i])\n",
    "            upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
    "\n",
    "    # Run the tkinter main loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Call the function to create the GUI\n",
    "create_gui(model = 'NetLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "\n",
    "\n",
    "def create_gui(model):\n",
    "    lhd = LightHyperDict()\n",
    "    dict =  lhd.hyper_dict[model]\n",
    "\n",
    "    \n",
    "    # Create a tkinter window\n",
    "    root = tk.Tk()\n",
    "\n",
    "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
    "    # TODO: Add labels to the column headers\n",
    "    for i, (key, value) in enumerate(dict.items()):            \n",
    "            if dict[key][\"type\"] == \"int\" or dict[key][\"type\"] == \"float\":\n",
    "                # Create a label with the key as text\n",
    "                label = tk.Label(root, text=key)\n",
    "                label.grid(row=i, column=0, sticky=\"W\")\n",
    "                # Create an entry with the default value as the default text\n",
    "                default_entry = tk.Entry(root)\n",
    "                default_entry.insert(0, dict[key][\"default\"])\n",
    "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "                # add the lower bound values in column 2\n",
    "                lower_bound_entry = tk.Entry(root)                \n",
    "                lower_bound_entry.insert(0, dict[key][\"lower\"])\n",
    "                lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
    "                # add the upper bound values in column 3\n",
    "                upper_bound_entry = tk.Entry(root)\n",
    "                upper_bound_entry.insert(0, dict[key][\"upper\"])\n",
    "                upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
    "            if dict[key][\"type\"] == \"factor\":        \n",
    "                # Create a label with the key as text\n",
    "                label = tk.Label(root, text=key)\n",
    "                label.grid(row=i, column=0, sticky=\"W\")\n",
    "                # Create an entry with the default value as the default text\n",
    "                default_entry = tk.Entry(root)\n",
    "                default_entry.insert(0, dict[key][\"default\"])\n",
    "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "                # add the lower bound values in column 2\n",
    "                factor_level_entry = tk.Entry(root)\n",
    "                # add a comma to each level\n",
    "                dict[key][\"levels\"] = \", \".join(dict[key][\"levels\"])                                \n",
    "                factor_level_entry.insert(0, dict[key][\"levels\"])\n",
    "                # TODO: Fix columnspan\n",
    "                factor_level_entry.grid(row=i, column=2, columnspan=2, sticky=\"W\")\n",
    "\n",
    "    # Run the tkinter main loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Call the function to create the GUI\n",
    "create_gui(model = 'NetLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gui(model = 'TransformerLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save Load Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spotpython.utils.file import save_experiment, load_experiment\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "\n",
    "def test_file_save_load():\n",
    "    fun = analytical().fun_branin\n",
    "\n",
    "    fun_control = fun_control_init(\n",
    "                PREFIX=\"branin\",\n",
    "                SUMMARY_WRITER=False,\n",
    "                lower = np.array([0, 0]),\n",
    "                upper = np.array([10, 10]),\n",
    "                fun_evals=8,\n",
    "                fun_repeats=1,\n",
    "                max_time=inf,\n",
    "                noise=False,\n",
    "                tolerance_x=0,\n",
    "                ocba_delta=0,\n",
    "                var_type=[\"num\", \"num\"],\n",
    "                infill_criterion=\"ei\",\n",
    "                n_points=1,\n",
    "                seed=123,\n",
    "                log_level=20,\n",
    "                show_models=False,\n",
    "                show_progress=True)\n",
    "    design_control = design_control_init(\n",
    "                init_size=5,\n",
    "                repeats=1)\n",
    "    surrogate_control = surrogate_control_init(\n",
    "                model_fun_evals=10000,\n",
    "                min_theta=-3,\n",
    "                max_theta=3,\n",
    "                n_theta=2,\n",
    "                theta_init_zero=True,\n",
    "                n_p=1,\n",
    "                optim_p=False,\n",
    "                var_type=[\"num\", \"num\"],\n",
    "                seed=124)\n",
    "    optimizer_control = optimizer_control_init(\n",
    "                max_iter=1000,\n",
    "                seed=125)\n",
    "    spot_tuner = spot.Spot(fun=fun,\n",
    "                fun_control=fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control,\n",
    "                optimizer_control=optimizer_control)\n",
    "    # Call the save_experiment function\n",
    "    filename = save_experiment(\n",
    "        spot_tuner=spot_tuner,\n",
    "        fun_control=fun_control,\n",
    "        design_control=None,\n",
    "        surrogate_control=None,\n",
    "        optimizer_control=None\n",
    "    )\n",
    "\n",
    "    # Verify that the pickle file is created\n",
    "    assert os.path.exists(filename)\n",
    "\n",
    "    # Call the load_experiment function\n",
    "    spot_tuner_1, fun_control_1, design_control_1, surrogate_control_1, optimizer_control_1 = load_experiment(filename)\n",
    "\n",
    "    # Verify the name of the pickle file\n",
    "    assert filename == f\"spot_{fun_control['PREFIX']}experiment.pickle\"\n",
    "\n",
    "    # Clean up the temporary directory\n",
    "    os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_save_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netlightregression2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression2 import NetLightRegression2\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "BATCH_SIZE = 8\n",
    "dataset = Diabetes()\n",
    "train1_set, test_set = torch.utils.data.random_split(dataset, [0.6, 0.4])\n",
    "train_set, val_set = torch.utils.data.random_split(train1_set, [0.6, 0.4])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(f\"batch_x.shape: {batch_x.shape}\")\n",
    "print(f\"batch_y.shape: {batch_y.shape}\")\n",
    "net_light_base = NetLightRegression2(l1=128,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='Default',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=10,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=10,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "# data.csv is simple csv file with 11 samples\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from spotpython.utils.file import save_experiment, load_experiment\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "fun = analytical().fun_branin\n",
    "fun_control = fun_control_init(\n",
    "            PREFIX=\"branin\",\n",
    "            SUMMARY_WRITER=False,\n",
    "            lower = np.array([0, 0]),\n",
    "            upper = np.array([10, 10]),\n",
    "            fun_evals=8,\n",
    "            fun_repeats=1,\n",
    "            max_time=inf,\n",
    "            noise=False,\n",
    "            tolerance_x=0,\n",
    "            ocba_delta=0,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            infill_criterion=\"ei\",\n",
    "            n_points=1,\n",
    "            seed=123,\n",
    "            log_level=20,\n",
    "            show_models=False,\n",
    "            show_progress=True)\n",
    "design_control = design_control_init(\n",
    "            init_size=5,\n",
    "            repeats=1)\n",
    "surrogate_control = surrogate_control_init(\n",
    "            model_fun_evals=10000,\n",
    "            min_theta=-3,\n",
    "            max_theta=3,\n",
    "            n_theta=2,\n",
    "            theta_init_zero=True,\n",
    "            n_p=1,\n",
    "            optim_p=False,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            seed=124)\n",
    "optimizer_control = optimizer_control_init(\n",
    "            max_iter=1000,\n",
    "            seed=125)\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,\n",
    "            optimizer_control=optimizer_control)\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters(fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tuned Hyperparameters from a Machine/Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.device import getDevice\n",
    "from math import inf\n",
    "from spotpython.utils.init import fun_control_init\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "\n",
    "MAX_TIME = 1\n",
    "FUN_EVALS = 10\n",
    "INIT_SIZE = 5\n",
    "WORKERS = 0\n",
    "PREFIX=\"037\"\n",
    "DEVICE = getDevice()\n",
    "DEVICES = 1\n",
    "TEST_SIZE = 0.4\n",
    "TORCH_METRIC = \"mean_squared_error\"\n",
    "dataset = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    _torchmetric=TORCH_METRIC,\n",
    "    PREFIX=PREFIX,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    data_set=dataset,\n",
    "    device=DEVICE,\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=FUN_EVALS,\n",
    "    log_level=50,\n",
    "    max_time=MAX_TIME,\n",
    "    num_workers=WORKERS,\n",
    "    show_progress=True,\n",
    "    test_size=TEST_SIZE,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "add_core_model_to_fun_control(fun_control=fun_control,\n",
    "                              core_model=NetLightRegression,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n",
    "set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n",
    "set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\n",
    "                \"Adam\",\n",
    "                \"RAdam\",\n",
    "            ])\n",
    "set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n",
    "set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n",
    "set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n",
    "set_control_hyperparameter_value(fun_control, \"act_fn\",[\n",
    "                \"ReLU\",\n",
    "                \"LeakyReLU\"\n",
    "            ] )\n",
    "from spotpython.utils.init import design_control_init, surrogate_control_init\n",
    "design_control = design_control_init(init_size=INIT_SIZE)\n",
    "\n",
    "surrogate_control = surrogate_control_init(noise=True,\n",
    "                                            n_theta=2)\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "fun = HyperLight(log_level=50).fun\n",
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       fun_control=fun_control,\n",
    "                       design_control=design_control,\n",
    "                       surrogate_control=surrogate_control)\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors\n",
    "\n",
    "* Example from https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/012_num_spot_ei.html#factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.design.spacefilling import spacefilling\n",
    "from spotpython.build.kriging import Kriging\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = spacefilling(2)\n",
    "n = 30\n",
    "rng = np.random.RandomState(1)\n",
    "lower = np.array([-5,-0])\n",
    "upper = np.array([10,15])\n",
    "fun = analytical().fun_branin_factor\n",
    "#fun = analytical(sigma=0).fun_sphere\n",
    "\n",
    "X0 = gen.scipy_lhd(n, lower=lower, upper = upper)\n",
    "X1 = np.random.randint(low=0, high=3, size=(n,))\n",
    "X = np.c_[X0, X1]\n",
    "y = fun(X)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "S = Kriging(name='kriging',  seed=123, log_level=10, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\n",
    "S.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sf = Kriging(name='kriging',  seed=123, log_level=10, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"factor\"])\n",
    "# Sf = Kriging(name='kriging',  seed=123, log_level=50, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\n",
    "Sf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0\n",
    "for _ in range(100):\n",
    "    n = 100\n",
    "    X0 = gen.scipy_lhd(n, lower=lower, upper = upper)\n",
    "    X1 = np.random.randint(low=0, high=3, size=(n,))\n",
    "    X = np.c_[X0, X1]\n",
    "    y = fun(X)\n",
    "    s=np.sum(np.abs(S.predict(X) - y))\n",
    "    sf=np.sum(np.abs(Sf.predict(X) - y))\n",
    "    res = res + (sf - s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def select_distant_points(X, y, k):\n",
    "    \"\"\"\n",
    "    Selects k points that are distant from each other using a clustering approach.\n",
    "    \n",
    "    :param X: np.array of shape (n, k), with n points in k-dimensional space.\n",
    "    :param y: np.array of length n, with values corresponding to each point in X.\n",
    "    :param k: The number of distant points to select.\n",
    "    :return: Selected k points from X and their corresponding y values.\n",
    "    \"\"\"\n",
    "    # Perform k-means clustering to find k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n",
    "    \n",
    "    # Find the closest point in X to each cluster center\n",
    "    selected_points = np.array([X[np.argmin(np.linalg.norm(X - center, axis=1))] for center in kmeans.cluster_centers_])\n",
    "    \n",
    "    # Find indices of the selected points in the original X array\n",
    "    indices = np.array([np.where(np.all(X==point, axis=1))[0][0] for point in selected_points])\n",
    "    \n",
    "    # Select the corresponding y values\n",
    "    selected_y = y[indices]\n",
    "    \n",
    "    return selected_points, selected_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 2)  # Generate some random points\n",
    "y = np.random.rand(100)     # Random corresponding y values\n",
    "k = 5\n",
    "\n",
    "selected_points, selected_y = select_distant_points(X, y, k)\n",
    "print(\"Selected Points:\", selected_points)\n",
    "print(\"Corresponding y values:\", selected_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 5\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1, -1]),\n",
    "    upper = np.array([1, 1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "S.plot_important_hyperparameter_contour(max_imp=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n",
    "\n",
    "# Sorting the array in descending order by the second element of each sub-list\n",
    "sorted_array = sorted(array, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(sorted_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_second_and_return_indices(array):\n",
    "    \"\"\"\n",
    "    Sorts an array of arrays based on the second values in descending order and returns\n",
    "    the indices of the original array entries.\n",
    "\n",
    "    :param array: List of lists, where each inner list has at least two elements.\n",
    "    :return: Indices of the original array entries after sorting by the second value.\n",
    "             Returns an empty list if the input is empty or None.\n",
    "    :raises ValueError: If any sub-array is improperly structured.\n",
    "    \"\"\"\n",
    "    if not array:\n",
    "        return []\n",
    "\n",
    "    # Check for improperly structured sub-arrays\n",
    "    for item in array:\n",
    "        if not isinstance(item, list) or len(item) < 2:\n",
    "            raise ValueError(\"All sub-arrays must be lists with at least two elements.\")\n",
    "\n",
    "    # Enumerate the array to keep track of original indices, then sort by the second item\n",
    "    sorted_indices = [index for index, value in sorted(enumerate(array), key=lambda x: x[1][1], reverse=True)]\n",
    "\n",
    "    return sorted_indices\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n",
    "    indices = sort_by_second_and_return_indices(array)\n",
    "    print(\"Indices of the sorted elements:\", indices)\n",
    "except ValueError as error:\n",
    "    print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Core Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree import HoeffdingAdaptiveTreeRegressor\n",
    "from spotriver.data.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_for_core_model, get_default_values\n",
    "fun_control = {}\n",
    "add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=RiverHyperDict,\n",
    "    filename=None)\n",
    "values = get_default_values(fun_control)\n",
    "print(values)\n",
    "# get_default_hyperparameters_for_core_model(fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytest\n",
    "import pprint\n",
    "from spotpython.plot.xai import (get_activations, get_gradients, get_weights, plot_nn_values_hist, plot_nn_values_scatter, visualize_weights, visualize_gradients, visualize_activations, visualize_activations_distributions, visualize_gradient_distributions, visualize_weights_distributions)\n",
    "\n",
    "def test_plot_nn_values_scatter_reshaped_values():\n",
    "    # Mock data for testing\n",
    "    nn_values = {\n",
    "        'layer0': np.random.rand(10),  # 10 values suggesting padding for a 4x4\n",
    "        'layer1': np.random.rand(64),  # 64 values suggesting a perfect square (8x8)\n",
    "        'layer2': np.random.rand(32),  # 32 values suggesting  padding for a 6x6\n",
    "        'layer3': np.random.rand(16),  # 16 values suggesting a perfect square (4x4)\n",
    "    }\n",
    "\n",
    "    # Use the modified function that returns reshaped_values for testing\n",
    "    reshaped_values = plot_nn_values_scatter(nn_values, 'Test NN', return_reshaped=True)    \n",
    "\n",
    "    pprint.pprint(nn_values)\n",
    "    pprint.pprint(reshaped_values)\n",
    "    \n",
    "\n",
    "    # Assert for layer0: Checks if reshaping is correct for perfect square\n",
    "    assert reshaped_values['layer0'].shape == (4, 4)\n",
    "    # Assert for layer1: Checks if reshaping is correct for non-square\n",
    "    assert reshaped_values['layer1'].shape == (8, 8)\n",
    "    assert reshaped_values['layer2'].shape == (6, 6)\n",
    "    assert reshaped_values['layer3'].shape == (4, 4)\n",
    "\n",
    "\n",
    "\n",
    "test_plot_nn_values_scatter_reshaped_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.convert import set_dataset_target_type\n",
    "import pandas as pd\n",
    "dataset = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9], \"y\": [True, False, True]})\n",
    "print(dataset)\n",
    "dataset = set_dataset_target_type(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import river.tree\n",
    "core_model_name = \"tree.HoeffdingTreeRegressor\"\n",
    "core_model_module = core_model_name.split(\".\")[0]\n",
    "coremodel = core_model_name.split(\".\")[1]\n",
    "core_model_instance = getattr(getattr(river, core_model_module), coremodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.friedman import FriedmanDriftDataset\n",
    "import matplotlib.pyplot as plt\n",
    "data_generator = FriedmanDriftDataset(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\n",
    "data = [data for data in data_generator]\n",
    "indices = [i for _, _, i in data]\n",
    "values = {f\"x{i}\": [] for i in range(5)}\n",
    "values[\"y\"] = []\n",
    "for x, y, _ in data:\n",
    "    for i in range(5):\n",
    "        values[f\"x{i}\"].append(x[i])\n",
    "    values[\"y\"].append(y)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for label, series in values.items():\n",
    "    plt.plot(indices, series, label=label)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('')\n",
    "plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n",
    "plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scaler for Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from spotpython.utils.scaler import TorchStandardScaler\n",
    "import torch\n",
    "\n",
    "scaler=TorchStandardScaler()\n",
    "\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.float64)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "import numpy as np\n",
    "np.max(diabetes.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "dataset = Diabetes()\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 1\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "dataset = CaliforniaHousing()\n",
    "print(dataset.get_names())\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "dataset = CaliforniaHousing()\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "import torch\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "import torch\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=2, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n",
    "# print the first batch of the training set from data_module.data_train\n",
    "print(next(iter(data_module.train_dataloader())))\n",
    "# print the first batch of the training set from data_module.data_train as a numpy array\n",
    "print(next(iter(data_module.train_dataloader()))[0].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.utils.scaler import TorchStandardScaler, TorchMinMaxScaler\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "\n",
    "\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "scaler = TorchMinMaxScaler()\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "\n",
    "loader = data_module.train_dataloader\n",
    "\n",
    "total_sum = None\n",
    "total_count = 0\n",
    "\n",
    "# Iterate over batches in the DataLoader\n",
    "for batch in loader():\n",
    "    inputs, targets = batch\n",
    "    \n",
    "\n",
    "total_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "def test_net_light_regression_class():\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "    dataset = Diabetes()\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    net_light_regression = NetLightRegression(\n",
    "        l1=128,\n",
    "        epochs=10,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        initialization=\"Default\",\n",
    "        act_fn=nn.ReLU(),\n",
    "        optimizer=\"Adam\",\n",
    "        dropout_prob=0.1,\n",
    "        lr_mult=0.1,\n",
    "        patience=5,\n",
    "        _L_in=10,\n",
    "        _L_out=1,\n",
    "        _torchmetric=\"mean_squared_error\",\n",
    "    )\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=2,\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer.fit(net_light_regression, train_loader, val_loader)\n",
    "    res = trainer.test(net_light_regression, test_loader)\n",
    "    # test if the entry 'hp_metric' is in the res dict\n",
    "    assert \"hp_metric\" in res[0].keys()\n",
    "\n",
    "test_net_light_regression_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_int_hyperparameter_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_int_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"forest.AMFRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_int_hyperparameter_values(fun_control, \"n_estimators\", 2, 5)\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_factor_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_factor_hyperparameter_values(fun_control, \"leaf_model\", ['LinearRegression',\n",
    "                                                     'Perceptron'])\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "\n",
    "set_factor_hyperparameter_values(fun_control, \"leaf_model\", [\"LinearRegression\",\n",
    "                                                                \"Perceptron\"])\n",
    "\n",
    "# Access updated hyperparameters\n",
    "updated_hyperparameters = fun_control[\"core_model_hyper_dict\"]\n",
    "print(updated_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_boolean_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"forest.AMFRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_boolean_hyperparameter_values(fun_control, \"use_aggregation\", 0, 0)\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, full_dataset, train_size=0.8, batch_size=32, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.dataset = full_dataset\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Split the dataset\n",
    "        train_len = int(len(self.dataset) * self.train_size)\n",
    "        val_len = len(self.dataset) - train_len\n",
    "        self.train_set, self.val_set = random_split(self.dataset, [train_len, val_len])\n",
    "        \n",
    "        # Fit scaler on training data\n",
    "        train_data = torch.stack([item[0] for item in self.train_set])\n",
    "        print(f\"train_data before scaling\\n: {train_data}\")  \n",
    "        self.scaler.fit(train_data)\n",
    "       \n",
    "        # Transform training data\n",
    "        scaled_train_data = self.scaler.transform(train_data)\n",
    "        self.train_set = self._update_dataset(self.train_set, scaled_train_data)\n",
    "        print(f\"train_data after scaling\\n: {self.train_set}\")  \n",
    "        \n",
    "        # Transform validation data\n",
    "        val_data = torch.stack([item[0] for item in self.val_set])\n",
    "        scaled_val_data = self.scaler.transform(val_data)\n",
    "        self.val_set = self._update_dataset(self.val_set, scaled_val_data)\n",
    "\n",
    "    def _update_dataset(self, original_dataset, scaled_data):\n",
    "        updated_dataset = []\n",
    "        for i, (data, label) in enumerate(original_dataset):\n",
    "            updated_dataset.append((torch.tensor(scaled_data[i]), label))\n",
    "        return updated_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_data = torch.stack([item[0] for item in self.test_set])\n",
    "        scaled_test_data = self.scaler.transform(test_data)\n",
    "        self.test_set = self._update_dataset(self.test_set, scaled_test_data)\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Here you can download datasets if needed\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a 3-dimensional tensor with 1000 samples\n",
    "n = 10\n",
    "data = torch.rand((n, 3))\n",
    "print(f\"data: {data}\")\n",
    "labels = torch.tensor([i % 2 for i in range(n)], dtype=torch.float32)\n",
    "print(f\"labels: {labels}\")\n",
    "full_dataset = MyDataset(data, labels)\n",
    "\n",
    "# Creating DataModule instance\n",
    "data_module = MyDataModule(full_dataset)\n",
    "\n",
    "# Setup the data module\n",
    "data_module.setup()\n",
    "\n",
    "# Example of fetching a single batch\n",
    "train_loader = data_module.train_dataloader()\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch data shape: {batch[0].shape}\")\n",
    "    x, y = batch\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, do not delete the following imports, they are needed for the function add_core_model_to_fun_control\n",
    "import river\n",
    "from river import forest, tree, linear_model, rules\n",
    "from river import preprocessing\n",
    "import sklearn.metrics\n",
    "import spotpython\n",
    "from spotpython.light import regression\n",
    "\n",
    "def get_core_model_from_name(core_model_name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns the river core model name and instance from a core model name.\n",
    "\n",
    "    Args:\n",
    "        core_model_name (str): The full name of the core model in the format 'module.Model'.\n",
    "\n",
    "    Returns:\n",
    "        (str, object): A tuple containing the core model name and an instance of the core model.\n",
    "    \"\"\"\n",
    "    # Split the model name into its components\n",
    "    name_parts = core_model_name.split(\".\")\n",
    "    \n",
    "    if len(name_parts) < 2:\n",
    "        raise ValueError(f\"Invalid core model name: {core_model_name}. Expected format: 'module.ModelName'.\")\n",
    "\n",
    "    module_name = name_parts[0]\n",
    "    model_name = name_parts[1]\n",
    "    \n",
    "    try:\n",
    "        # Try to get the model from the river library\n",
    "        core_model_instance = getattr(getattr(river, module_name), model_name)\n",
    "        return model_name, core_model_instance\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            # Try to get the model from the spotpython library\n",
    "            submodule_name = name_parts[1]\n",
    "            model_name = name_parts[2] if len(name_parts) == 3 else model_name\n",
    "            print(f\"module_name: {module_name}\")\n",
    "            print(f\"submodule_name: {submodule_name}\")\n",
    "            print(f\"model_name: {model_name}\")\n",
    "            core_model_instance = getattr(getattr(getattr(spotpython, module_name), submodule_name), model_name)\n",
    "            return model_name, core_model_instance\n",
    "        except AttributeError:\n",
    "            raise ValueError(f\"Model '{core_model_name}' not found in either 'river' or 'spotpython' libraries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of usage\n",
    "model_name, model_instance = get_core_model_from_name('tree.HoeffdingTreeRegressor')\n",
    "print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, model_instance = get_core_model_from_name(\"light.regression.NNLinearRegressor\")\n",
    "print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 8\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=10,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes(return_X_y=False, as_frame=True)\n",
    "# svaing the data to a csv file\n",
    "data.frame.to_csv('~/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "n_features = 2\n",
    "n_samples = 500\n",
    "target_column = \"y\"\n",
    "ds =  make_moons(n_samples, noise=0.5, random_state=0)\n",
    "X, y = ds\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1))))\n",
    "test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1))))\n",
    "train.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "test.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "train.head()\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.concat([train, test])\n",
    "data.to_csv('moon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_classification(n_samples=1000, n_features=20,  n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('binary_classification.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_classification(n_samples=1000, n_features=20,  n_informative=9, n_redundant=2, n_repeated=0, n_classes=10, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('multiple_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('regression.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "data = load_iris(as_frame=True)\n",
    "data.frame.to_csv('iris.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotGUI.tuner.spotRun import get_report_file_name\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(PREFIX=\"test\")\n",
    "get_report_file_name(fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotGUI.tuner.spotRun import get_scenario_dict\n",
    "import pprint\n",
    "dic = get_scenario_dict(\"sklearn\")\n",
    "pprint.pprint(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.utils.io import load_hcf_dataframe, hcf_df2tensor\n",
    "from pyhcf.utils.names import load_all_features_N_regression_list\n",
    "from torch.utils.data import DataLoader\n",
    "df = load_hcf_dataframe(A=True,\n",
    "    H=True,\n",
    "    param_list=load_all_features_N_regression_list(),\n",
    "    target='N',\n",
    "    rmNA=True,\n",
    "    rmMF=True,\n",
    "    rmV=4,\n",
    "    min_freq=1000,\n",
    "    incl_drossel=False)\n",
    "dataset = hcf_df2tensor(df, target='N', return_X_y=False)\n",
    "print(type(dataset))\n",
    "print(len(dataset))\n",
    "# save the 'TensorDataset' object to a pkl file\n",
    "# import pickle\n",
    "# with open('hcf_dataset.pkl', 'wb') as f:\n",
    "#     pickle.dump(dataset, f)\n",
    "# load the 'TensorDataset' object from the pkl file\n",
    "# with open('hcf_dataset.pkl', 'rb') as f:\n",
    "#     dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0)\n",
    "# get the dimensions of the first sample\n",
    "dataset.__getitem__(0)[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "X0, y0 = S.generate_random_point()\n",
    "print(f\"X0: {X0}\")\n",
    "print(f\"y0: {y0}\")\n",
    "assert X0.size == 2\n",
    "assert y0.size == 1\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "assert y0 >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import get_spot_tensorboard_path\n",
    "get_spot_tensorboard_path(\"00_ubuntu_2021-08-31_14-30-00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import get_experiment_name\n",
    "get_experiment_name(prefix=\"00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 5\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1, -1]),\n",
    "    upper = np.array([1, 1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "S.plot_important_hyperparameter_contour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import design_control_init\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "design_control = design_control_init(init_size=3)\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "S = spot.Spot(fun = analytical().fun_sphere,\n",
    "              fun_control = fun_control,\n",
    "              design_control = design_control)\n",
    "X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n",
    "assert X.shape[0] == 3\n",
    "assert X.shape[1] == 2\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import inf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from spotriver.utils.data_conversion import convert_to_df\n",
    "from river.datasets import synth\n",
    "import warnings\n",
    "if not os.path.exists('./figures'):\n",
    "    os.makedirs('./figures')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "PREFIX=\"TEST_SAVE\"\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from river.datasets import synth\n",
    "from spotriver.utils.data_conversion import convert_to_df\n",
    "from math import inf\n",
    "import numpy as np\n",
    "from spotriver.fun.hyperriver import HyperRiver\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "\n",
    "\n",
    "target_column = \"y\"\n",
    "metric = mean_absolute_error\n",
    "horizon = 7*24\n",
    "n_train = horizon\n",
    "p_1 = int(n_train/4)\n",
    "p_2 = int(n_train/2)\n",
    "position=(p_1, p_2)\n",
    "dataset_train = synth.FriedmanDrift(\n",
    "   drift_type='gra',\n",
    "   position=position,\n",
    "   seed=123\n",
    ")\n",
    "\n",
    "train = convert_to_df(dataset_train, n_total=n_train)\n",
    "train.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n",
    "\n",
    "\n",
    "n_val = 10_000\n",
    "p_1 = int(n_val/4)\n",
    "p_2 = int(n_val/2)\n",
    "position=(p_1, p_2)\n",
    "dataset_val = synth.FriedmanDrift(\n",
    "   drift_type='gra',\n",
    "   position=position,\n",
    "   seed=124\n",
    ")\n",
    "val = convert_to_df(dataset_val, n_total=n_val)\n",
    "val.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n",
    "\n",
    "from math import inf\n",
    "import numpy as np\n",
    "from spotriver.fun.hyperriver import HyperRiver\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "\n",
    "fun = HyperRiver().fun_oml_horizon\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    TENSORBOARD_CLEAN=False,\n",
    "    tensorboard_start=False,\n",
    "    tensorboard_stop=False,\n",
    "    fun_evals=inf,\n",
    "    max_time=0.1,\n",
    "\n",
    "    prep_model_name=\"StandardScaler\",\n",
    "    test=val, # tuner uses the validation set as test set\n",
    "    train=train,\n",
    "    target_column=target_column,\n",
    "\n",
    "    metric_sklearn_name=\"mean_absolute_error\",\n",
    "    horizon=7*24,\n",
    "    oml_grace_period=7*24,\n",
    "    weight_coeff=0.0,\n",
    "    weights=np.array([100, 0.1, 0.1]),\n",
    "\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    "   )\n",
    "\n",
    "\n",
    "design_control = design_control_init(\n",
    "    init_size=3,\n",
    ")\n",
    "\n",
    "surrogate_control = surrogate_control_init(\n",
    "    noise=True,\n",
    "    n_theta=2,\n",
    "    min_Lambda=0.001,\n",
    "    max_Lambda=100,\n",
    ")\n",
    "\n",
    "optimizer_control = optimizer_control_init()\n",
    "\n",
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control,\n",
    "    surrogate_control=surrogate_control,\n",
    "    optimizer_control=optimizer_control,\n",
    ")\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.file import load_and_run_spot_python_experiment\n",
    "spot_tuner = load_and_run_spot_python_experiment(\"spot_000_experiment.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_div2_list(n, n_min) -> list:\n",
    "    \"\"\"\n",
    "    Generate a list of numbers from n to n_min (inclusive) by dividing n by 2\n",
    "    until the result is less than n_min.\n",
    "    This function starts with n and keeps dividing it by 2 until n_min is reached.\n",
    "    The number of times each value is added to the list is determined by n // current.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number to start with.\n",
    "        n_min (int): The minimum number to stop at.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of numbers from n to n_min (inclusive).\n",
    "\n",
    "    Examples:\n",
    "        _generate_div2_list(10, 1)\n",
    "        [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        _generate_div2_list(10, 2)\n",
    "        [10, 5, 5, 2, 2, 2, 2, 2]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    current = n\n",
    "    repeats = 1\n",
    "    max_repeats = 4\n",
    "    while current >= n_min:\n",
    "        result.extend([current] * min(repeats, max_repeats))\n",
    "        current = current // 2\n",
    "        repeats = repeats + 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_generate_div2_list(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_generate_div2_list(128, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_sizes(_L_in = 80, l1=2**9):\n",
    "    n_low = _L_in // 4\n",
    "    n_high = max(l1, 2 * n_low)\n",
    "    hidden_sizes = _generate_div2_list(n_high, n_low)\n",
    "    return hidden_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_hidden_sizes(l1=2**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.data.daten_lightv2 import DatenLightV2\n",
    "from pyhcf.utils.io import hcf_df2tensor\n",
    "df = DatenLightV2().load()\n",
    "print(f\"Datensatz der Gre {df.shape} erfolgreich geladen.\")\n",
    "print(df.columns.to_list())\n",
    "dataset = hcf_df2tensor(df, target='N', return_X_y=False)\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Targets Shape: {targets.shape}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Network Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we generate an 80-dim dataframe with 10000 samples, where the first two columns are random integers and the rest are random floats.\n",
    "* Then, we generate a target variable as the sum of the squared values.\n",
    "* The dataframe is converted to a tensor and split into a training, validation, and testing set. The corresponding data loaders are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "np.random.seed(42)\n",
    "n_samples = 10_000\n",
    "n_int = 2\n",
    "n_float = 76\n",
    "input_dim = n_int + n_float\n",
    "output_dim = 1\n",
    "data = np.random.rand(n_samples, n_float)\n",
    "data = np.hstack((np.random.randint(0, 10, (n_samples, n_int)), data))\n",
    "df = pd.DataFrame(data)\n",
    "df['y'] = np.sum(df.iloc[:, 2:]**2, axis=1)\n",
    "df.head()\n",
    "X = torch.tensor(df.iloc[:, :-1].values, dtype=torch.float32)\n",
    "y = torch.tensor(df.iloc[:, -1].values, dtype=torch.float32)\n",
    "dataset = TensorDataset(X, y)\n",
    "print(f\"Dataset with input tensor shape: {dataset.tensors[0].shape}\")\n",
    "print(f\"Dataset with target tensor shape: {dataset.tensors[1].shape}\")\n",
    "# print(dataset[0][0])\n",
    "# print(dataset[0][1])\n",
    "train_size_0 = int(0.8 * len(dataset))\n",
    "train_size = int(0.8 * train_size_0)\n",
    "val_size = train_size_0 - train_size\n",
    "test_size = len(dataset) - train_size_0\n",
    "train_dataset_0, test_dataset = random_split(dataset, [train_size_0, test_size])\n",
    "train_dataset, val_dataset = random_split(train_dataset_0, [train_size, val_size])\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NNLinearRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                    batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=input_dim,\n",
    "                                    _L_out=output_dim,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from spotpython.hyperparameters.optimizer import optimizer_handler\n",
    "import torchmetrics.functional.regression\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "\n",
    "class SettingsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset to handle settings-based data.\"\"\"\n",
    "    def __init__(self, dataframe, settings_columns, target_column):\n",
    "        self.dataframe = dataframe\n",
    "        self.settings_columns = settings_columns\n",
    "        self.target_column = target_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        settings = tuple(self.dataframe.iloc[idx][self.settings_columns])\n",
    "        features = self.dataframe.iloc[idx].drop(self.settings_columns + [self.target_column]).values\n",
    "        target = self.dataframe.iloc[idx][self.target_column]\n",
    "        return settings, torch.tensor(features, dtype=torch.float32), torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "class FilteredNNLinearRegressor:\n",
    "    def __init__(self, settings_columns, data, target_column='target', **nn_kwargs):\n",
    "        self.settings_columns = settings_columns\n",
    "        self.models = {}\n",
    "        self.nn_kwargs = nn_kwargs\n",
    "        self.data = data\n",
    "        self.target_column = target_column\n",
    "        self.prepare_networks()\n",
    "\n",
    "    def prepare_networks(self):\n",
    "        settings_combinations = self.data[self.settings_columns].drop_duplicates().to_records(index=False)\n",
    "        i = 0\n",
    "        for combination in settings_combinations:\n",
    "            print(f\"Combination {i}: {combination}\")\n",
    "            i += 1\n",
    "            self.models[combination] = NNLinearRegressor(**self.nn_kwargs)\n",
    "\n",
    "    def feature_filter(self, settings):\n",
    "        \"\"\"Filter the data based on given settings tuple.\"\"\"\n",
    "        df_filtered = self.data[(self.data[self.settings_columns] == pd.Series(settings)).all(axis=1)]\n",
    "        print(f\"df_filtered: {df_filtered}\")\n",
    "        return df_filtered\n",
    "\n",
    "    def train(self, trainer_kwargs):\n",
    "        # Split data and train each model separately\n",
    "        for settings, model in self.models.items():\n",
    "            filtered_data = self.feature_filter(settings)\n",
    "            dataset = SettingsDataset(filtered_data, self.settings_columns, self.target_column)\n",
    "            train_loader = DataLoader(dataset, batch_size=self.nn_kwargs['batch_size'])\n",
    "            trainer = L.Trainer(**trainer_kwargs)\n",
    "            trainer.fit(model, train_loader)\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        predictions = {}\n",
    "        for settings, model in self.models.items():\n",
    "            filtered_data = self.feature_filter(settings)\n",
    "            if not filtered_data.empty:\n",
    "                dataset = SettingsDataset(filtered_data, self.settings_columns, self.target_column)\n",
    "                test_loader = DataLoader(dataset, batch_size=self.nn_kwargs['batch_size'])\n",
    "                trainer = L.Trainer()\n",
    "                preds = trainer.predict(model, test_loader)\n",
    "                predictions[settings] = preds\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'setting1': [-1, -1, 1, 1],\n",
    "    'setting2': ['A', 'B', 'A', 'B'],\n",
    "    'feature1': [0.1, 0.2, 0.3, 0.4],\n",
    "    'feature2': [0.5, 0.6, 0.7, 0.8],\n",
    "    'target': [1, 2, 3, 4]\n",
    "})\n",
    "\n",
    "settings_columns = ['setting1', 'setting2']\n",
    "target_column = 'target'\n",
    "nn_kwargs = {\n",
    "    'l1': 16,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 2,\n",
    "    'initialization': 'xavier',\n",
    "    'act_fn': nn.ReLU(),\n",
    "    'optimizer': 'Adam',\n",
    "    'dropout_prob': 0.1,\n",
    "    'lr_mult': 0.1,\n",
    "    'patience': 2,\n",
    "    'batch_norm': True,\n",
    "    '_L_in': 2,  # For this example, 2 features besides settings\n",
    "    '_L_out': 1,\n",
    "    '_torchmetric': \"mean_squared_error\",\n",
    "}\n",
    "\n",
    "multi_network = FilteredNNLinearRegressor(settings_columns, data, target_column, **nn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = {'max_epochs': 2,  'enable_progress_bar': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_network.train(trainer_kwargs)\n",
    "predictions = multi_network.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explanation:\n",
    "1. SettingsDataset: A custom dataset class that includes settings as part of the data. Each row has a tuple of settings, feature values, and the target value.\n",
    "\n",
    "2. FilteredNNLinearRegressor: An umbrella class that handles setting combinations and assigns each its own `NNLinearRegressor` model instance. It trains these models using only relevant data filtered by the `feature_filter()` function.\n",
    "\n",
    "3. Feature Filtering: The `feature_filter()` function uses Pandas to filter rows based on their relevant setting information before creating dataset and loader instances for each unique settings combination.\n",
    "\n",
    "4. Training and Prediction: We generate and train separate models for each settings combination and then predict using test data filtered similarly using the defined `feature_filter()`.\n",
    "\n",
    "This approach provides modularity as each model is logically separated based on settings, while utilizing your existing class structure to individually specify training processes and handle data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.plot.xai import get_gradients\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "gradients = get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "# assert that the gradients are a dictionary with keys that contain the string 'layers' and values that are arrays\n",
    "assert all([key in gradients.keys() for key in gradients.keys()])\n",
    "assert all([isinstance(value, np.ndarray) for value in gradients.values()])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.plot.xai import get_gradients\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CondNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConditionalLayer(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, output_dim):\n",
    "        super(ConditionalLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.condition_fc = nn.Linear(condition_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # Basic linear transformation\n",
    "        base_output = self.fc(x)\n",
    "        # Compute a condition-dependent transformation\n",
    "        condition_output = self.condition_fc(condition)\n",
    "        # Modulate the output by adding the condition-dependent transformation\n",
    "        output = base_output + condition_output\n",
    "        return F.relu(output)\n",
    "\n",
    "class ConditionalNet(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, hidden_dim, output_dim):\n",
    "        super(ConditionalNet, self).__init__()\n",
    "        self.cond_layer1 = ConditionalLayer(input_dim, condition_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        x = self.cond_layer1(x, condition)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_dim = 10\n",
    "condition_dim = 2  # For instance, if you have two conditional features like region and season\n",
    "hidden_dim = 20\n",
    "output_dim = 1\n",
    "\n",
    "model = ConditionalNet(input_dim, condition_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Example data\n",
    "x = torch.randn(5, input_dim)\n",
    "condition = torch.randn(5, condition_dim)\n",
    "\n",
    "output = model(x, condition)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.light.regression import NNCondNetRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 64\n",
    "# generate data\n",
    "num_samples = 1_000\n",
    "input_dim = 10\n",
    "cond_dim = 2\n",
    "X = torch.randn(num_samples, input_dim)  # random data for example\n",
    "Y = torch.randn(num_samples, 1)  # random target for example\n",
    "data_set = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNCondNetRegressor(l1=128,\n",
    "                                batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_cond=cond_dim,\n",
    "                                    _L_in=input_dim - cond_dim,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# validation and test should give the same result, because the data is the same\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CondNet Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.file import get_experiment_filename\n",
    "from math import inf\n",
    "from spotpython.hyperparameters.values import set_hyperparameter\n",
    "\n",
    "PREFIX=\"4000\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "input_dim = 10\n",
    "output_dim = 1\n",
    "cond_dim = 2\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    fun_evals=inf,\n",
    "    max_time=1,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNCondNetRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=input_dim - cond_dim,\n",
    "    _L_out=1,\n",
    "    _L_cond=cond_dim,)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "\n",
    "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
    "set_hyperparameter(fun_control, \"l1\", [3,4])\n",
    "set_hyperparameter(fun_control, \"epochs\", [3,7])\n",
    "set_hyperparameter(fun_control, \"batch_size\", [4,5])\n",
    "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
    "set_hyperparameter(fun_control, \"patience\", [2,3])\n",
    "set_hyperparameter(fun_control, \"lr_mult\", [0.1, 20.0])\n",
    "\n",
    "design_control = design_control_init(init_size=10)\n",
    "\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_weights(net, return_index=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the weights of a neural network and the size of each layer.\n",
    "\n",
    "    Args:\n",
    "        net (object):\n",
    "            A neural network.\n",
    "        return_index (bool, optional):\n",
    "            Whether to return the index. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            A tuple containing:\n",
    "            - weights: A dictionary with the weights of the neural network.\n",
    "            - index: The layer index list (only if return_index is True).\n",
    "            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "\n",
    "    Examples:\n",
    "        # Example usage (as described in the original function's docstring)\n",
    "    \"\"\"\n",
    "    weights = {}\n",
    "    index = []\n",
    "    layer_sizes = {}\n",
    "    \n",
    "    for name, param in net.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            continue\n",
    "        \n",
    "        # Extract layer number\n",
    "        layer_number = int(name.split(\".\")[1])\n",
    "        index.append(layer_number)\n",
    "        \n",
    "        # Create dictionary key for this layer\n",
    "        key_name = f\"Layer {layer_number}\"\n",
    "        \n",
    "        # Store weight information\n",
    "        weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
    "        \n",
    "        # Store layer size as a NumPy array\n",
    "        layer_sizes[key_name] = np.array(param.size())\n",
    "    \n",
    "    if return_index:\n",
    "        return weights, index, layer_sizes\n",
    "    else:\n",
    "        return weights, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.plot.xai import get_weights\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "weights, layer_sizes = get_weights(net=model)\n",
    "weights, layer_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import math\n",
    "\n",
    "def plot_nn_values_scatter(\n",
    "    nn_values,\n",
    "    layer_sizes,\n",
    "    nn_values_names=\"\",\n",
    "    absolute=True,\n",
    "    cmap=\"gray\",\n",
    "    figsize=(6, 6),\n",
    "    return_reshaped=False,\n",
    "    show=True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Plot the values of a neural network including a marker for padding values.\n",
    "    For simplicity, this example will annotate 'P' directly on the plot for padding values\n",
    "    using a unique marker value approach.\n",
    "\n",
    "    Args:\n",
    "        nn_values (dict):\n",
    "            A dictionary with the values of the neural network. For example,\n",
    "            the weights, gradients, or activations.\n",
    "        layer_sizes (dict):\n",
    "            A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "        nn_values_names (str, optional):\n",
    "            The name of the values. Defaults to \"\".\n",
    "        absolute (bool, optional):\n",
    "            Whether to use the absolute values. Defaults to True.\n",
    "        cmap (str, optional):\n",
    "            The colormap to use. Defaults to \"gray\".\n",
    "        figsize (tuple, optional):\n",
    "            The figure size. Defaults to (6, 6).\n",
    "        return_reshaped (bool, optional):\n",
    "            Whether to return the reshaped values. Defaults to False.\n",
    "        show (bool, optional):\n",
    "            Whether to show the plot. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the reshaped values.\n",
    "    \"\"\"\n",
    "    if cmap == \"gray\":\n",
    "        cmap = \"gray\"\n",
    "    elif cmap == \"BlueWhiteRed\":\n",
    "        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"blue\", \"white\", \"red\"])\n",
    "    elif cmap == \"GreenYellowRed\":\n",
    "        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"green\", \"yellow\", \"red\"])\n",
    "    else:\n",
    "        cmap = \"viridis\"\n",
    "\n",
    "    res = {}\n",
    "    padding_marker = np.nan  # Use NaN as a special marker for padding\n",
    "    for layer, values in nn_values.items():\n",
    "        if layer not in layer_sizes:\n",
    "            print(f\"Layer {layer} size not defined, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        layer_shape = layer_sizes[layer]\n",
    "        height, width = layer_shape if len(layer_shape) == 2 else (layer_shape[0], 1)  # Support linear layers\n",
    "        \n",
    "        print(f\"{len(values)} values in Layer {layer}. Geometry: ({height}, {width})\")\n",
    "        \n",
    "        total_size = height * width\n",
    "        if len(values) < total_size:\n",
    "            padding_needed = total_size - len(values)\n",
    "            print(f\"{padding_needed} padding values added to Layer {layer}.\")\n",
    "            values = np.append(values, [padding_marker] * padding_needed)  # Append padding values\n",
    "\n",
    "        if absolute:\n",
    "            reshaped_values = np.abs(values).reshape((height, width))\n",
    "            # Mark padding values distinctly by setting them back to NaN\n",
    "            reshaped_values[reshaped_values == np.abs(padding_marker)] = np.nan\n",
    "        else:\n",
    "            reshaped_values = values.reshape((height, width))\n",
    "\n",
    "        _, ax = plt.subplots(figsize=figsize)\n",
    "        cax = ax.imshow(reshaped_values, cmap=cmap, interpolation=\"nearest\")\n",
    "\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                if np.isnan(reshaped_values[i, j]):\n",
    "                    ax.text(j, i, \"P\", ha=\"center\", va=\"center\", color=\"red\")\n",
    "        \n",
    "        plt.colorbar(cax, label=\"Value\")\n",
    "        plt.title(f\"{nn_values_names} Plot for {layer}\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "        # Add reshaped_values to the dictionary res\n",
    "        res[layer] = reshaped_values\n",
    "    if return_reshaped:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, layer_sizes = get_weights(net=model)\n",
    "plot_nn_values_scatter(nn_values=weights, layer_sizes=layer_sizes, nn_values_names=\"Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "\n",
    "def get_gradients(net, fun_control, batch_size, device=\"cpu\", normalize=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the gradients of a neural network and the size of each layer.\n",
    "\n",
    "    Args:\n",
    "        net (object):\n",
    "            A neural network.\n",
    "        fun_control (dict):\n",
    "            A dictionary with the function control.\n",
    "        batch_size (int, optional):\n",
    "            The batch size.\n",
    "        device (str, optional):\n",
    "            The device to use. Defaults to \"cpu\".\n",
    "        normalize (bool, optional):\n",
    "            Whether to normalize the input data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - grads: A dictionary with the gradients of the neural network.\n",
    "            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "\n",
    "    Examples:\n",
    "        # Example usage to compute gradients\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    dataset = fun_control[\"data_set\"]\n",
    "    data_module = LightDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        test_size=fun_control[\"test_size\"],\n",
    "        scaler=fun_control[\"scaler\"],\n",
    "        verbosity=10,\n",
    "    )\n",
    "    data_module.setup(stage=\"fit\")\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    inputs, targets = next(iter(train_loader))\n",
    "    if normalize:\n",
    "        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    net.zero_grad()\n",
    "    preds = net(inputs)\n",
    "    preds = preds.squeeze(-1)  # Remove the last dimension if it's 1\n",
    "    loss = F.mse_loss(preds, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = {}\n",
    "    layer_sizes = {}\n",
    "    for name, params in net.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            # Collect gradient information\n",
    "            grads[name] = params.grad.view(-1).cpu().clone().numpy()\n",
    "            # Collect size information\n",
    "            layer_sizes[name] = np.array(params.size())\n",
    "\n",
    "    net.zero_grad()\n",
    "    return grads, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients, layer_sizes = get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "plot_nn_values_scatter(nn_values=gradients, layer_sizes=layer_sizes, nn_values_names=\"Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(data, layer_index) -> bool:\n",
    "    \"\"\"Checks for NaN values in the tensor data.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The tensor to check for NaN values.\n",
    "        layer_index (int): The index of the layer for logging purposes.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if NaNs are found, False otherwise.\n",
    "    \"\"\"\n",
    "    if torch.isnan(data).any():\n",
    "        print(f\"NaN detected after layer {layer_index}\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(net, fun_control, batch_size, device=\"cpu\", normalize=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Computes the activations for each layer of the network, the mean activations,\n",
    "    and the sizes of the activations for each layer.\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The neural network model.\n",
    "        fun_control (dict): A dictionary containing the dataset.\n",
    "        batch_size (int): The batch size for the data loader.\n",
    "        device (str): The device to run the model on. Defaults to \"cpu\".\n",
    "        normalize (bool): Whether to normalize the input data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the activations, mean activations, and layer sizes for each layer.\n",
    "\n",
    "    Examples:\n",
    "        from spotpython.plot.xai import get_activations\n",
    "            activations, mean_activations, layer_sizes = get_activations(net, fun_control)\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    mean_activations = {}\n",
    "    layer_sizes = {}\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    dataset = fun_control[\"data_set\"]\n",
    "    data_module = LightDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        test_size=fun_control[\"test_size\"],\n",
    "        scaler=fun_control[\"scaler\"],\n",
    "        verbosity=10,\n",
    "    )\n",
    "    data_module.setup(stage=\"fit\")\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    inputs, _ = next(iter(train_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    if normalize:\n",
    "        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        # Loop through all layers\n",
    "        for layer_index, layer in enumerate(net.layers[:-1]):\n",
    "            inputs = layer(inputs)  # Forward pass through the layer\n",
    "\n",
    "            # Check for NaNs\n",
    "            if check_for_nans(inputs, layer_index):\n",
    "                break\n",
    "\n",
    "            # Collect activations for Linear layers\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                activations[layer_index] = inputs.view(-1).cpu().numpy()\n",
    "                mean_activations[layer_index] = inputs.mean(dim=0).cpu().numpy()\n",
    "                # Record the size of the activations\n",
    "                layer_sizes[layer_index] = np.array(inputs.size())\n",
    "\n",
    "    return activations, mean_activations, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations, mean_activations, layer_sizes = get_activations(net=model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "plot_nn_values_scatter(nn_values=activations, layer_sizes=layer_sizes, nn_values_names=\"Activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def extract_linear_dims(model):\n",
    "    dims = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Append input and output features of the Linear layer\n",
    "            dims.append(layer.in_features)\n",
    "            dims.append(layer.out_features)\n",
    "    return np.array(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_linear_dims(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "def viz_net(net,\n",
    "            device=\"cpu\",\n",
    "            show_attrs=False,\n",
    "            show_saved=False,\n",
    "            filename=\"model_architecture\",\n",
    "            format=\"png\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the architecture of a linear neural network.\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The neural network model.\n",
    "        device (str, optional): The device to use. Defaults to \"cpu\".\n",
    "        show_attrs (bool, optional): Whether to show the attributes. Defaults to False.\n",
    "        show_saved (bool, optional): Whether to show the saved. Defaults to False.\n",
    "        filename (str, optional): The filename. Defaults to \"model_architecture\".\n",
    "        format (str, optional): The format. Defaults to \"png\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the model does not have a linear layer.\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "       dim=extract_linear_dims(net)\n",
    "    except:\n",
    "        error_message = \"The model does not have a linear layer.\"\n",
    "        raise ValueError(error_message)\n",
    "    x = torch.randn(1, dim[0]).requires_grad_(True)\n",
    "    x = x.to(device)\n",
    "    output = net(x)\n",
    "    dot = make_dot(output, params=dict(net.named_parameters()), show_attrs=show_attrs , show_saved=show_saved)\n",
    "    dot.render(filename, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_net(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.plot.xai import viz_net\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "_L_in=10\n",
    "_L_out=1\n",
    "_torchmetric=\"mean_squared_error\"\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=_L_in,\n",
    "    _L_out=_L_out,\n",
    "    _torchmetric=_torchmetric,\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "viz_net(net=model, device=\"cpu\", show_attrs=True, show_saved=True, filename=\"model_architecture\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "S.exp_imp(1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "# assert S.exp_imp(0.0, 1.0) == 1/np.sqrt(2*np.pi)\n",
    "# which is approx. 0.3989422804014327\n",
    "S.exp_imp(0.0, 1.0)\n",
    "0.3989422804014327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_de_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract_from_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "\n",
    "# Define the number of theta and p parameters\n",
    "num_theta = 2\n",
    "num_p = 3\n",
    "\n",
    "# Initialize the Kriging model\n",
    "kriging_model = Kriging(\n",
    "    name='kriging',\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=False\n",
    ")\n",
    "\n",
    "# Extract parameters from given bounds\n",
    "# Assumes 'extract_from_bounds' will split the array into `theta` and `p` based on `n_theta`.\n",
    "bounds_array = np.array([1, 2, 3, 4, 5])\n",
    "kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "\n",
    "# Validate the expected values for theta and p\n",
    "# Convert theta and p to lists if they are numpy arrays\n",
    "theta_list = list(kriging_model.theta)\n",
    "p_list = list(kriging_model.p)\n",
    "\n",
    "assert theta_list == [1, 2], f\"Expected theta to be [1, 2] but got {theta_list}\"\n",
    "assert p_list == [3, 4, 5], f\"Expected p to be [3] but got {p_list}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "# Define the number of theta and p parameters\n",
    "num_theta = 2\n",
    "num_p = 3\n",
    "# Initialize the Kriging model\n",
    "kriging_model = Kriging(\n",
    "    name='kriging',\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=True\n",
    ")\n",
    "# Create bounds array\n",
    "bounds_array = np.array([1, 2, 3, 4, 5, 6])\n",
    "# Extract parameters from given bounds\n",
    "kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "# Assertions to check if parameters are correctly extracted\n",
    "assert np.array_equal(kriging_model.theta, [1, 2]), f\"Expected theta to be [1, 2] but got {kriging_model.theta}\"\n",
    "assert np.array_equal(kriging_model.p, [3, 4, 5]), f\"Expected p to be [3, 4, 5] but got {kriging_model.p}\"\n",
    "assert kriging_model.Lambda == 6, f\"Expected Lambda to be 6 but got {kriging_model.Lambda}\"\n",
    "print(\"All assertions passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "print(new_theta_p_Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.update_log()\n",
    "print(S.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 0], [1, 0]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.fit(nat_X, nat_y)\n",
    "print(S.Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "print(f\"S.nat_X: {S.nat_X}\")\n",
    "print(f\"S.nat_y: {S.nat_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "assert S.var_type == ['num', 'num']\n",
    "assert S.var_type == ['num', 'num']\n",
    "assert S.num_mask.all() == True\n",
    "assert S.factor_mask.all() == False\n",
    "assert S.int_mask.all() == False\n",
    "assert S.ordered_mask.all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "assert S.theta.all() == array([0., 0.]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import log, var\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "nat_y = np.array([1, 2, 3])\n",
    "n=3\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "# if var(self.nat_y) is > 0, then self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n",
    "# else self.pen_val = self.n * var(self.nat_y) + 1e4\n",
    "assert S.pen_val == nat_X.shape[0] * log(var(S.nat_y)) + 1e4\n",
    "assert S.Psi.shape == (n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")\n",
    "S.build_U()\n",
    "print(f\"S.U:{S.U}\")\n",
    "S.likelihood()\n",
    "S.negLnLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "from numpy import power\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "print(S.__is_any__(power(10.0, S.theta), 0))\n",
    "print(S.__is_any__(S.theta, 0))\n",
    "S.theta: [0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")\n",
    "S.build_U()\n",
    "print(f\"S.U:{S.U}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1], [2]])\n",
    "nat_y = np.array([5, 10])\n",
    "n=2\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.build_Psi()\n",
    "S.build_U()\n",
    "S.likelihood()\n",
    "# assert S.mu is close to 7.5 with a tolerance of 1e-6\n",
    "assert np.allclose(S.mu, 7.5, atol=1e-6)\n",
    "E = np.exp(1)\n",
    "sigma2 = E/(E**2 -1) * (25/4 + 25/4*E)\n",
    "# asssert S.SigmaSqr is close to sigma2 with a tolerance of 1e-6\n",
    "assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)\n",
    "print(f\"S.LnDetPsi:{S.LnDetPsi}\")\n",
    "print(f\"S.self.negLnLike:{S.negLnLike}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# 1-dimensional example\n",
    "fun = analytical().fun_sphere\n",
    "fun_control=fun_control_init(lower = np.array([-1]),\n",
    "                            upper = np.array([1]),\n",
    "                            noise=False)\n",
    "design_control=design_control_init(init_size=10)\n",
    "S = spot.Spot(fun=fun,\n",
    "              fun_control=fun_control,\n",
    "              design_control=design_control)\n",
    "S.initialize_design()\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.surrogate.plot()\n",
    "# 2-dimensional example\n",
    "fun = analytical().fun_sphere\n",
    "fun_control=fun_control_init(lower = np.array([-1, -1]),\n",
    "                            upper = np.array([1, 1]),\n",
    "                            noise=False)\n",
    "design_control=design_control_init(init_size=10)\n",
    "S = spot.Spot(fun=fun,\n",
    "              fun_control=fun_control,\n",
    "              design_control=design_control)\n",
    "S.initialize_design()\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.surrogate.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_0).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "print(f\"mean_prediction: {mean_prediction}\")\n",
    "print(f\"std_prediction: {std_prediction}\")\n",
    "print(f\"s_ei: {s_ei}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "    from numpy import linspace, arange, empty\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=10).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "n = X.shape[0]\n",
    "y = empty(n, dtype=float)\n",
    "s = empty(n, dtype=float)\n",
    "ei = empty(n, dtype=float)\n",
    "for i in range(n):\n",
    "    y_coded, s_coded, ei_coded = S.predict_coded(X[i, :])\n",
    "    y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n",
    "    s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n",
    "    ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n",
    "print(f\"y: {y}\")\n",
    "print(f\"s: {s}\")\n",
    "print(f\"ei: {-1.0*ei}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_psi_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "X_train = np.array([[1., 2.],\n",
    "                    [2., 4.],\n",
    "                    [3., 6.]])\n",
    "y_train = np.array([1., 2., 3.])\n",
    "S = Kriging(name='kriging',\n",
    "            seed=123,\n",
    "            log_level=50,\n",
    "            n_theta=1,\n",
    "            noise=False,\n",
    "            cod_type=\"norm\")\n",
    "S.fit(X_train, y_train)\n",
    "# force theta to simple values:\n",
    "S.theta = np.array([0.0])\n",
    "nat_X = np.array([1., 0.])\n",
    "S.psi = np.zeros((S.n, 1))\n",
    "S.build_psi_vec(nat_X)\n",
    "res = np.array([[np.exp(-4)],\n",
    "    [np.exp(-17)],\n",
    "    [np.exp(-40)]])\n",
    "assert np.array_equal(S.psi, res)\n",
    "print(f\"S.psi: {S.psi}\")\n",
    "print(f\"Control value res: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Log Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import inf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.file import get_experiment_filename\n",
    "\n",
    "PREFIX=\"00_TEST\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    save_experiment=True,\n",
    "    fun_evals=inf,\n",
    "    max_time=10,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    tensorboard_log=True,\n",
    "    show_config=True,)\n",
    "\n",
    "design_control = design_control_init(init_size=5)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNLinearRegressor Teest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "torch.manual_seed(0)\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 64\n",
    "# generate data\n",
    "num_samples = 1_000\n",
    "input_dim = 10\n",
    "X = torch.randn(num_samples, input_dim)  # random data for example\n",
    "Y = torch.randn(num_samples, 1)  # random target for example\n",
    "data_set = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=input_dim,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# validation and test should give the same result, because the data is the same\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Objective Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [-1, -1, -1]])\n",
    "fun = analytical()\n",
    "fun.fun_cubed(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([np.zeros(10), np.ones(10)])\n",
    "fun = analytical()\n",
    "fun.fun_wingwt(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.architecture import generate_div2_list\n",
    "# call the function with all integer values between 5 and 10\n",
    "for n in range(5, 21):\n",
    "    print(generate_div2_list(n, n_min=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.architecture import get_hidden_sizes\n",
    "_L_in = 10\n",
    "max_n = 10\n",
    "for l1 in range(5, 20):    \n",
    "    print(get_hidden_sizes(_L_in, l1, max_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def get_three_layers(_L_in, l1) -> list:\n",
    "    \"\"\"\n",
    "    Calculate three layers based on input values.\n",
    "\n",
    "    Args:\n",
    "        _L_in (float): The input value to be multiplied.\n",
    "        l1 (float): The multiplier for the layers.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing three calculated layers [a, b, c] where:\n",
    "            - a = 3 * l1 * _L_in\n",
    "            - b = 2 * l1 * _L_in\n",
    "            - c = l1 * _L_in\n",
    "\n",
    "    Examples:\n",
    "        from spotpython.hyperparameters.architecture import get_three_layers\n",
    "            _L_in = 10\n",
    "            l1 = 20\n",
    "            get_three_layers(_L_in, l1)\n",
    "            [600, 400, 200]\n",
    "    \"\"\"\n",
    "    a = 3 * l1 * _L_in\n",
    "    b = 2 * l1 * _L_in\n",
    "    c = ceil(l1/2) * _L_in\n",
    "    return [a, b, a, b, b, c, c] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_L_in = 20\n",
    "l1 = 4\n",
    "get_three_layers(_L_in, l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for 0.20.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_spot_attributes_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1]),\n",
    "    upper = np.array([1]),\n",
    "    fun_evals=n)\n",
    "design_control=design_control_init(init_size=ni)\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n",
    "df = spot_1.get_spot_attributes_as_df()\n",
    "df[\"Attribute Name\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_red_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "# number of points\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals = n)\n",
    "design_control=design_control_init(init_size=ni)\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n",
    "assert spot_1.lower.size == 2\n",
    "assert spot_1.upper.size == 2\n",
    "assert len(spot_1.var_type) == 2\n",
    "assert spot_1.red_dim == False\n",
    "spot_1.lower = np.array([-1, -1])\n",
    "spot_1.upper = np.array([-1, -1])\n",
    "spot_1.to_red_dim()\n",
    "assert spot_1.lower.size == 0\n",
    "assert spot_1.upper.size == 0\n",
    "assert len(spot_1.var_type) == 0\n",
    "assert spot_1.red_dim == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_all_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n",
    "lower = np.array([-1, -1, 0, 0])\n",
    "upper = np.array([1, -1, 0, 5])  # Second and third dimensions are fixed\n",
    "fun_evals = 10\n",
    "var_type = ['float', 'int', 'float', 'int']\n",
    "var_name = ['x1', 'x2', 'x3', 'x4']\n",
    "spot_instance = spot.Spot(\n",
    "    fun = Analytical().fun_sphere, \n",
    "    fun_control=fun_control_init(lower=lower, upper=upper, fun_evals=fun_evals)\n",
    ")\n",
    "X0 = np.array([[2.5, 3.5], [4.5, 5.5]])\n",
    "X_full_dim = spot_instance.to_all_dim(X0)\n",
    "print(X_full_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_new_X0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "            n_points=10,\n",
    "            ocba_delta=0,\n",
    "            lower = np.array([-1, -1]),\n",
    "            upper = np.array([1, 1])\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "             fun_control=fun_control,\n",
    "             design_control=design_control,\n",
    ")\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "X0 = S.get_new_X0()\n",
    "assert X0.shape[0] == S.n_points\n",
    "assert X0.shape[1] == S.lower.size\n",
    "# assert new points are in the interval [lower, upper]\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "# print using 20 digits precision\n",
    "np.set_printoptions(precision=20)\n",
    "print(f\"X0: {X0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython import Analytical\n",
    "from spotpython import Spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write_tensorboard_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    tensorboard_log=True,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    lower = np.array([-1]),\n",
    "    upper = np.array([1])\n",
    "    )\n",
    "fun = Analytical().fun_sphere\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "S.initialize_design()\n",
    "S.write_tensorboard_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize_design_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "fun = Analytical().fun_sphere\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "X_start = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "S.initialize_design_matrix(X_start)\n",
    "print(f\"Design matrix: {S.X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate_initial_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    lower=np.array([-1, -1]),\n",
    "    upper=np.array([1, 1])\n",
    ")\n",
    "fun = Analytical().fun_sphere\n",
    "S = Spot(fun=fun, fun_control=fun_control)\n",
    "X0 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "S.initialize_design_matrix(X_start=X0)\n",
    "S.evaluate_initial_design()\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Experiment:\n",
    "    def save_experiment(self, filename=None, path=None, overwrite=True) -> None:\n",
    "        \"\"\"\n",
    "        Save the experiment to a file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename of the experiment file.\n",
    "            path (str): The path to the experiment file.\n",
    "            overwrite (bool): If `True`, the file will be overwritten if it already exists. Default is `True`.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Ensure we don't accidentally try to pickle unpicklable components\n",
    "        self.close_and_del_spot_writer()\n",
    "        self.remove_logger_handlers()\n",
    "\n",
    "        # Create deep copies of control dictionaries\n",
    "        fun_control = copy.deepcopy(self.fun_control)\n",
    "        optimizer_control = copy.deepcopy(self.optimizer_control)\n",
    "        surrogate_control = copy.deepcopy(self.surrogate_control)\n",
    "        design_control = copy.deepcopy(self.design_control)\n",
    "\n",
    "        # Prepare an experiment dictionary excluding any explicitly unpickable components\n",
    "        experiment = {\n",
    "            \"design_control\": design_control,\n",
    "            \"fun_control\": fun_control,\n",
    "            \"optimizer_control\": optimizer_control,\n",
    "            \"spot_tuner\": self._get_pickle_safe_spot_tuner(),\n",
    "            \"surrogate_control\": surrogate_control,\n",
    "        }\n",
    "\n",
    "        # Determine the filename based on PREFIX if not provided\n",
    "        PREFIX = fun_control.get(\"PREFIX\", \"experiment\")\n",
    "        if filename is None:\n",
    "            filename = self.get_experiment_filename(PREFIX)\n",
    "\n",
    "        if path is not None:\n",
    "            filename = os.path.join(path, filename)\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if filename is not None and os.path.exists(filename) and not overwrite:\n",
    "            print(f\"Error: File {filename} already exists. Use overwrite=True to overwrite the file.\")\n",
    "            return\n",
    "\n",
    "        # Serialize the experiment dictionary to the pickle file\n",
    "        if filename is not None:\n",
    "            with open(filename, \"wb\") as handle:\n",
    "                try:\n",
    "                    pickle.dump(experiment, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during pickling: {e}\")\n",
    "                    raise e\n",
    "            print(f\"Experiment saved to {filename}\")\n",
    "\n",
    "    def remove_logger_handlers(self) -> None:\n",
    "        \"\"\"\n",
    "        Remove handlers from the logger to avoid pickling issues.\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        for handler in list(logger.handlers):  # Copy the list to avoid modification during iteration\n",
    "            logger.removeHandler(handler)\n",
    "\n",
    "    def close_and_del_spot_writer(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete the spot_writer attribute from the object\n",
    "        if it exists and close the writer.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n",
    "            self.spot_writer.flush()\n",
    "            self.spot_writer.close()\n",
    "            del self.spot_writer\n",
    "\n",
    "    def _get_pickle_safe_spot_tuner(self):\n",
    "        \"\"\"\n",
    "        Create a copy of self excluding unpickleable components for safe pickling.\n",
    "        This ensures no unpicklable components are passed to pickle.dump().\n",
    "        \"\"\"\n",
    "        # Make a deepcopy and manually remove unpickleable components\n",
    "        spot_tuner = copy.deepcopy(self)\n",
    "        for attr in ['spot_writer']:\n",
    "            if hasattr(spot_tuner, attr):\n",
    "                delattr(spot_tuner, attr)\n",
    "        return spot_tuner\n",
    "\n",
    "    def get_experiment_filename(self, prefix):\n",
    "        \"\"\"\n",
    "        Generate a filename based on a given prefix with additional unique identifiers or timestamps.\n",
    "        \"\"\"\n",
    "        # Implement the logic to generate a filename\n",
    "        return f\"{prefix}_experiment.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform_hyper_parameter_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.transform import transform_hyper_parameter_values\n",
    "fun_control = {\n",
    "    \"core_model_hyper_dict\": {\n",
    "        \"leaf_prediction\": {\n",
    "                \"type\": \"factor\",\n",
    "                \"transform\": \"None\",\n",
    "                \"default\": \"mean\",\n",
    "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
    "                \"core_model_parameter_type\": \"str\"\n",
    "                            },\n",
    "        \"max_depth\": {\n",
    "                \"type\": \"int\",\n",
    "                \"default\": 20,\n",
    "                \"transform\": \"transform_power_2\",\n",
    "                \"lower\": 2,\n",
    "                \"upper\": 20}\n",
    "            }\n",
    "    }\n",
    "hyper_parameter_values = {\n",
    "        'max_depth': 2,\n",
    "        'leaf_prediction': 'mean'}\n",
    "transform_hyper_parameter_values(fun_control, hyper_parameter_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.transform import transform_hyper_parameter_values\n",
    "fun_control = {\n",
    "    \"core_model_hyper_dict\": {\n",
    "        \"l1\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 3,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 3,\n",
    "            \"upper\": 8\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 4,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 4,\n",
    "            \"upper\": 9\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 4,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 1,\n",
    "            \"upper\": 4\n",
    "        },\n",
    "        \"act_fn\": {\n",
    "            \"levels\": [\n",
    "                \"Sigmoid\",\n",
    "                \"Tanh\",\n",
    "                \"ReLU\",\n",
    "                \"LeakyReLU\",\n",
    "                \"ELU\",\n",
    "                \"Swish\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"ReLU\",\n",
    "            \"transform\": \"None\",\n",
    "            \"class_name\": \"spotpython.torch.activation\",\n",
    "            \"core_model_parameter_type\": \"instance()\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 5\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"levels\": [\n",
    "                \"Adadelta\",\n",
    "                \"Adagrad\",\n",
    "                \"Adam\",\n",
    "                \"AdamW\",\n",
    "                \"SparseAdam\",\n",
    "                \"Adamax\",\n",
    "                \"ASGD\",\n",
    "                \"NAdam\",\n",
    "                \"RAdam\",\n",
    "                \"RMSprop\",\n",
    "                \"Rprop\",\n",
    "                \"SGD\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"SGD\",\n",
    "            \"transform\": \"None\",\n",
    "            \"class_name\": \"torch.optim\",\n",
    "            \"core_model_parameter_type\": \"str\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 11\n",
    "        },\n",
    "        \"dropout_prob\": {\n",
    "            \"type\": \"float\",\n",
    "            \"default\": 0.01,\n",
    "            \"transform\": \"None\",\n",
    "            \"lower\": 0.0,\n",
    "            \"upper\": 0.25\n",
    "        },\n",
    "        \"lr_mult\": {\n",
    "            \"type\": \"float\",\n",
    "            \"default\": 1.0,\n",
    "            \"transform\": \"None\",\n",
    "            \"lower\": 0.1,\n",
    "            \"upper\": 10.0\n",
    "        },\n",
    "        \"patience\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 2,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 2,\n",
    "            \"upper\": 6\n",
    "        },\n",
    "        \"batch_norm\": {\n",
    "            \"levels\": [\n",
    "                0,\n",
    "                1\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": 0,\n",
    "            \"transform\": \"None\",\n",
    "            \"core_model_parameter_type\": \"bool\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 1\n",
    "        },\n",
    "        \"initialization\": {\n",
    "            \"levels\": [\n",
    "                \"Default\",\n",
    "                \"kaiming_uniform\",\n",
    "                \"kaiming_normal\",\n",
    "                \"xavier_uniform\",\n",
    "                \"xavier_normal\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"Default\",\n",
    "            \"transform\": \"None\",\n",
    "            \"core_model_parameter_type\": \"str\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 4\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "hyper_parameter_values = {\n",
    "        'l1': 2,\n",
    "        'epochs': 3,\n",
    "        'batch_size': 4,\n",
    "        'act_fn': 'ReLU',\n",
    "        'optimizer': 'SGD',\n",
    "        'dropout_prob': 0.01,\n",
    "        'lr_mult': 1.0,\n",
    "        'patience': 3,\n",
    "        'batch_norm': 0,\n",
    "        'initialization': 'Default',        \n",
    "    }\n",
    "transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import assign_values\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "var_list = ['a', 'b']\n",
    "result = assign_values(X, var_list)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from spotpythonspot import Spot\n",
    "from spotpythonfunobjectivefunctions import Analytical\n",
    "from spotpythonutilsinit import fun_control_init, design_control_init\n",
    "from spotpythonutilsfile import load_result\n",
    "import pprint\n",
    "\n",
    "def _compare_dicts(dict1, dict2, ignore_keys=None):\n",
    "    \"\"\"\n",
    "    Compare two dictionaries, including element-wise comparison for numpy arrays\n",
    "    Print missing elements (keys) if the dictionaries do not match\n",
    "\n",
    "    Args:\n",
    "        dict1 (dict): First dictionary to compare\n",
    "        dict2 (dict): Second dictionary to compare\n",
    "        ignore_keys (list, optional): List of keys to ignore during comparison Default is None\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the dictionaries match, False otherwise\n",
    "    \"\"\"\n",
    "    if ignore_keys is None:\n",
    "        ignore_keys = []\n",
    "    # ensure that ignore_keys is a list\n",
    "    if not isinstance(ignore_keys, list):\n",
    "        ignore_keys = [ignore_keys]\n",
    "\n",
    "    keys1 = set(dict1keys()) - set(ignore_keys)\n",
    "    keys2 = set(dict2keys()) - set(ignore_keys)\n",
    "\n",
    "    if keys1 != keys2:\n",
    "        missing_in_dict1 = keys2 - keys1\n",
    "        missing_in_dict2 = keys1 - keys2\n",
    "        print(f\"Missing in dict1: {missing_in_dict1}\")\n",
    "        print(f\"Missing in dict2: {missing_in_dict2}\")\n",
    "        return False\n",
    "\n",
    "    for key in keys1:\n",
    "        if isinstance(dict1[key], npndarray) and isinstance(dict2[key], npndarray):\n",
    "            if not nparray_equal(dict1[key], dict2[key]):\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "        else:\n",
    "            if dict1[key] != dict2[key]:\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_save_and_load_experiment(tmp_path):\n",
    "    PREFIX = \"test_02\"\n",
    "    # Initialize function control\n",
    "    fun_control = fun_control_init(\n",
    "        PREFIX=PREFIX,\n",
    "        lower=nparray([-1, -1]),\n",
    "        upper=nparray([1, 1]),\n",
    "        verbosity=1\n",
    "    )\n",
    "    \n",
    "    design_control = design_control_init(init_size=7)\n",
    "\n",
    "    fun = Analytical()fun_sphere\n",
    "        \n",
    "    S = Spot(\n",
    "        fun=fun,\n",
    "        fun_control=fun_control,\n",
    "        design_control=design_control,\n",
    "    )\n",
    "    \n",
    "    X_start = nparray([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    S.run(X_start=X_start)\n",
    "\n",
    "    # Load the experiment\n",
    "    S_loaded = load_result(PREFIX)\n",
    "    print(f\"S: {S}\")    \n",
    "    print(f\"S_loaded: {S_loaded}\")\n",
    "    pprint.pprint(S_loaded)\n",
    "    loaded_fun_control = S_loaded.fun_control\n",
    "    loaded_design_control = S_loaded.design_control\n",
    "    loaded_surrogate_control = S_loaded.surrogate_control\n",
    "    loaded_optimizer_control = S_loaded.optimizer_control\n",
    "    \n",
    "    # Check if the loaded data matches the original data\n",
    "    # It is ok if the counter is different, because it is increased during the run\n",
    "    assert _compare_dicts(loaded_fun_control, fun_control, ignore_keys=\"counter\"), \"Loaded fun_control should match the original fun_control.\"\n",
    "    assert _compare_dicts(loaded_design_control, design_control), \"Loaded design_control should match the original design_control.\"\n",
    "    assert _compare_dicts(loaded_surrogate_control, S.surrogate_control), \"Loaded surrogate_control should match the original surrogate_control.\"\n",
    "    assert _compare_dicts(loaded_optimizer_control, S.optimizer_control), \"Loaded optimizer_control should match the original optimizer_control.\"\n",
    "\n",
    "    # Check if the S_loaded is an instance of Spot\n",
    "    assert isinstance(S_loaded, Spot), \"Loaded S_loaded should be an instance of Spot.\"\n",
    "\n",
    "    # Check if the design matrix and response vector are equal\n",
    "    # if there are differences, print the differences\n",
    "    # Differences are OK\n",
    "    # if not np.array_equal(S_loaded.X, S.X):\n",
    "    #     print(f\"Design matrix mismatch: {S_loaded.X} != {S.X}\")\n",
    "    # if not np.array_equal(S_loaded.y, S.y):\n",
    "    #     print(f\"Response vector mismatch: {S_loaded.y} != {S.y}\")\n",
    "\n",
    "\n",
    "test_save_and_load_experiment(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n",
    "\n",
    "ni = 7\n",
    "PREFIX = \"test_plot_progress_05\"\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "PREFIX=PREFIX,\n",
    "lower=np.array([-1, -1]),\n",
    "upper=np.array([1, 1]),\n",
    "fun_evals=fun_evals,\n",
    "tolerance_x=np.sqrt(np.spacing(1))\n",
    ")\n",
    "design_control = design_control_init(init_size=ni)\n",
    "surrogate_control = surrogate_control_init(n_theta=3)\n",
    "S = Spot(\n",
    "fun=fun,\n",
    "fun_control=fun_control,\n",
    "design_control=design_control,\n",
    "surrogate_control=surrogate_control,\n",
    ")\n",
    "S = S.run()\n",
    "\n",
    "# Test plot_progress with different parameters\n",
    "S.plot_progress(show=False)  # Test with show=False\n",
    "S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename\n",
    "# add NaN to S.y at position 2\n",
    "S.y[2] = np.nan\n",
    "S.plot_progress(show=False)  # Test with NaN in S.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot.spot import Spot\n",
    "from spotpython.utils.repair import repair_non_numeric\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    ")\n",
    "\n",
    "fun = Analytical().fun_branin_factor\n",
    "ni = 12\n",
    "spot_test = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control_init(\n",
    "        lower=np.array([-5, -0, 1]), upper=np.array([10, 15, 3]), var_type=[\"num\", \"num\", \"factor\"]\n",
    "    ),\n",
    "    design_control=design_control_init(init_size=ni),\n",
    ")\n",
    "spot_test.run()\n",
    "# 3rd variable should be a rounded float, because it was labeled as a factor\n",
    "assert spot_test.min_X[2] == round(spot_test.min_X[2])\n",
    "\n",
    "spot_test.X = spot_test.generate_design(\n",
    "    size=spot_test.design_control[\"init_size\"],\n",
    "    repeats=spot_test.design_control[\"repeats\"],\n",
    "    lower=spot_test.lower,\n",
    "    upper=spot_test.upper,\n",
    ")\n",
    "spot_test.X = repair_non_numeric(spot_test.X, spot_test.var_type)\n",
    "assert spot_test.X.ndim == 2\n",
    "assert spot_test.X.shape[0] == ni\n",
    "assert spot_test.X.shape[1] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spot_test.get_spot_attributes_as_df()\n",
    "list(df['Attribute Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "    \n",
    "# Setup: Configure initial parameters\n",
    "ni = 7\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX= \"test_get_spot_attributes_as_df\",\n",
    "    lower=np.array([-1]),\n",
    "    upper=np.array([1]),\n",
    "    fun_evals=n\n",
    ")\n",
    "design_control = design_control_init(init_size=ni)\n",
    "\n",
    "# Create instance of the Spot class\n",
    "S = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control\n",
    ")\n",
    "\n",
    "# Run the optimization\n",
    "S.run()\n",
    "\n",
    "# Get the attributes as a DataFrame\n",
    "df = S.get_spot_attributes_as_df()\n",
    "\n",
    "# Define expected attribute names (ensure these match your Spot class' attributes)\n",
    "expected_attributes = ['X',\n",
    "                        'all_lower',\n",
    "                        'all_upper',\n",
    "                        'all_var_name',\n",
    "                        'all_var_type',\n",
    "                        'counter',\n",
    "                        'de_bounds',\n",
    "                        'design',\n",
    "                        'design_control',\n",
    "                        'eps',\n",
    "                        'fun_control',\n",
    "                        'fun_evals',\n",
    "                        'fun_repeats',\n",
    "                        'ident',\n",
    "                        'infill_criterion',\n",
    "                        'k',\n",
    "                        'log_level',\n",
    "                        'lower',\n",
    "                        'max_surrogate_points',\n",
    "                        'max_time',\n",
    "                        'mean_X',\n",
    "                        'mean_y',\n",
    "                        'min_X',\n",
    "                        'min_mean_X',\n",
    "                        'min_mean_y',\n",
    "                        'min_y',\n",
    "                        'n_points',\n",
    "                        'noise',\n",
    "                        'ocba_delta',\n",
    "                        'optimizer_control',\n",
    "                        'progress_file',\n",
    "                        'red_dim',\n",
    "                        'rng',\n",
    "                        'show_models',\n",
    "                        'show_progress',\n",
    "                        'spot_writer',\n",
    "                        'surrogate',\n",
    "                        'surrogate_control',\n",
    "                        'tkagg',\n",
    "                        'tolerance_x',\n",
    "                        'upper',\n",
    "                        'var_name',\n",
    "                        'var_type',\n",
    "                        'var_y',\n",
    "                        'verbosity',\n",
    "                        'y']\n",
    "\n",
    "# Check that the DataFrame has the correct attributes\n",
    "assert list(df['Attribute Name']) == expected_attributes\n",
    "\n",
    "# Further checks can be done for specific attribute values\n",
    "# Example: Check that 'fun_evals' has the expected value\n",
    "fun_evals_row = df.query(\"`Attribute Name` == 'fun_evals'\")\n",
    "assert not fun_evals_row.empty and fun_evals_row['Attribute Value'].values[0] == n\n",
    "\n",
    "# Example: Check that 'lower' has the expected value\n",
    "lower_row = df.query(\"`Attribute Name` == 'lower'\")\n",
    "assert not lower_row.empty and lower_row['Attribute Value'].values[0] == [-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate_dic_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import iterate_dict_values\n",
    "var_dict = {'a': np.array([1, 3, 5]), 'b': np.array([2, 4, 6])}\n",
    "print(var_dict)\n",
    "list(iterate_dict_values(var_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.values import convert_keys\n",
    "d = {'a': 1, 'b': 2.1, 'c': 3}\n",
    "var_type = [\"int\", \"num\", \"int\"]\n",
    "convert_keys(d, var_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_dict_with_levels_and_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "from spotpython.hyperparameters.values import assign_values, get_var_name, iterate_dict_values, convert_keys, get_dict_with_levels_and_types, get_core_model_from_name, add_core_model_to_fun_control\n",
    "import pprint\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "coremodel, core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "for values in iterate_dict_values(var_dict):\n",
    "    values = convert_keys(values, fun_control[\"var_type\"])\n",
    "    pprint.pprint(values)\n",
    "    # pprint.pprint(fun_control)\n",
    "    values = get_dict_with_levels_and_types(fun_control=fun_control, v=values)\n",
    "    pprint.pprint(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_one_config_from_var_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import  get_core_model_from_name, add_core_model_to_fun_control, generate_one_config_from_var_dict\n",
    "import pprint\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "_ , core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "var_dict = {'l1': np.array([3.]),\n",
    "            'epochs': np.array([4.]),\n",
    "            'batch_size': np.array([4.]),\n",
    "            'act_fn': np.array([2.]),\n",
    "            'optimizer': np.array([11.]),\n",
    "            'dropout_prob': np.array([0.01]),\n",
    "            'lr_mult': np.array([1.]),\n",
    "            'patience': np.array([2.]),\n",
    "            'batch_norm': np.array([0.]),\n",
    "            'initialization': np.array([0.])}\n",
    "g = generate_one_config_from_var_dict(var_dict=var_dict, fun_control=fun_control)\n",
    "# Since g is an iterator, we need to call next to get the values\n",
    "values = next(g)\n",
    "pprint.pprint(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return_conf_list_from_var_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import  get_core_model_from_name, add_core_model_to_fun_control, return_conf_list_from_var_dict\n",
    "import pprint\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "_ , core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "var_dict = {'l1': np.array([3., 4.]),\n",
    "            'epochs': np.array([4., 3.]),\n",
    "            'batch_size': np.array([4., 4.]),\n",
    "            'act_fn': np.array([2., 1.]),\n",
    "            'optimizer': np.array([11., 10.]),\n",
    "            'dropout_prob': np.array([0.01, 0.]),\n",
    "            'lr_mult': np.array([1., 1.1]),\n",
    "            'patience': np.array([2., 3.]),\n",
    "            'batch_norm': np.array([0., 1.]),\n",
    "            'initialization': np.array([0., 1.])}\n",
    "return_conf_list_from_var_dict(var_dict=var_dict, fun_control=fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_one_config_from_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import get_core_model_from_name, add_core_model_to_fun_control, get_one_config_from_X\n",
    "core_model_name=\"light.regression.NNLinearRegressor\"\n",
    "hyperdict=LightHyperDict\n",
    "fun_control = {}\n",
    "coremodel, core_model_instance = get_core_model_from_name(core_model_name)\n",
    "add_core_model_to_fun_control(\n",
    "    core_model=core_model_instance,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=hyperdict,\n",
    "    filename=None,\n",
    ")\n",
    "X = np.array([[3.0e+00, 4.0e+00, 4.0e+00, 2.0e+00, 1.1e+01, 1.0e-02, 1.0e+00, 2.0e+00, 0.0e+00,\n",
    " 0.0e+00]])\n",
    "print(X)\n",
    "get_one_config_from_X(X, fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_tuned_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.spot import Spot\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_hyperparameter, get_tuned_architecture\n",
    "\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    force_run=False,\n",
    "    PREFIX=\"get_one_config_from_X\",\n",
    "    save_experiment=True,\n",
    "    fun_evals=10,\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "set_hyperparameter(fun_control, \"epochs\", [2,2])\n",
    "set_hyperparameter(fun_control, \"patience\", [1,2])\n",
    "design_control = design_control_init(init_size=5)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "S = Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "S.run()\n",
    "get_tuned_architecture(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_and_save_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import pprint\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.utils.file import load_experiment, load_result\n",
    "\n",
    "def _compare_dicts(dict1, dict2, ignore_keys=None):\n",
    "    \"\"\"\n",
    "    Compare two dictionaries, including element-wise comparison for numpy arrays.\n",
    "    Print missing elements (keys) if the dictionaries do not match.\n",
    "\n",
    "    Args:\n",
    "        dict1 (dict): First dictionary to compare.\n",
    "        dict2 (dict): Second dictionary to compare.\n",
    "        ignore_keys (list, optional): List of keys to ignore during comparison. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the dictionaries match, False otherwise.\n",
    "    \"\"\"\n",
    "    if ignore_keys is None:\n",
    "        ignore_keys = []\n",
    "    # ensure that ignore_keys is a list\n",
    "    if not isinstance(ignore_keys, list):\n",
    "        ignore_keys = [ignore_keys]\n",
    "\n",
    "    keys1 = set(dict1.keys()) - set(ignore_keys)\n",
    "    keys2 = set(dict2.keys()) - set(ignore_keys)\n",
    "\n",
    "    if keys1 != keys2:\n",
    "        missing_in_dict1 = keys2 - keys1\n",
    "        missing_in_dict2 = keys1 - keys2\n",
    "        print(f\"Missing in dict1: {missing_in_dict1}\")\n",
    "        print(f\"Missing in dict2: {missing_in_dict2}\")\n",
    "        return False\n",
    "\n",
    "    for key in keys1:\n",
    "        if isinstance(dict1[key], np.ndarray) and isinstance(dict2[key], np.ndarray):\n",
    "            if not np.array_equal(dict1[key], dict2[key]):\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "        else:\n",
    "            if dict1[key] != dict2[key]:\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "PREFIX = \"test_save_and_load_experiment_04\"\n",
    "# Initialize function control\n",
    "fun_control = fun_control_init(\n",
    "    save_experiment=True,\n",
    "    PREFIX=PREFIX,\n",
    "    lower=np.array([-1, -1]),\n",
    "    upper=np.array([1, 1]),\n",
    "    verbosity=2,\n",
    "    log_level=50\n",
    ")\n",
    "\n",
    "design_control = design_control_init(init_size=7)\n",
    "\n",
    "fun = Analytical().fun_sphere\n",
    "    \n",
    "S = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control\n",
    ")\n",
    "\n",
    "# Load the experiment\n",
    "S_loaded = load_experiment(PREFIX)\n",
    "print(f\"S: {S}\")    \n",
    "print(f\"S_loaded: {S_loaded}\")\n",
    "pprint.pprint(S_loaded)\n",
    "loaded_fun_control = S_loaded.fun_control\n",
    "# pprint.pprint(loaded_fun_control)\n",
    "loaded_design_control = S_loaded.design_control\n",
    "loaded_surrogate_control = S_loaded.surrogate_control\n",
    "loaded_optimizer_control = S_loaded.optimizer_control\n",
    "\n",
    "# Check if the loaded data matches the original data\n",
    "# It is ok if the counter is different, because it is increased during the run\n",
    "assert _compare_dicts(loaded_fun_control, fun_control, ignore_keys=\"counter\"), \"Loaded fun_control should match the original fun_control.\"\n",
    "assert _compare_dicts(loaded_design_control, design_control), \"Loaded design_control should match the original design_control.\"\n",
    "assert _compare_dicts(loaded_surrogate_control, S.surrogate_control), \"Loaded surrogate_control should match the original surrogate_control.\"\n",
    "assert _compare_dicts(loaded_optimizer_control, S.optimizer_control), \"Loaded optimizer_control should match the original optimizer_control.\"\n",
    "\n",
    "# Check if the S_loaded is an instance of Spot\n",
    "assert isinstance(S_loaded, Spot), \"Loaded S_loaded should be an instance of Spot.\"\n",
    "\n",
    "# Check if the design matrix and response vector are equal\n",
    "# if there are differences, print the differences\n",
    "# Differences are OK\n",
    "if not np.array_equal(S_loaded.X, S.X):\n",
    "    print(f\"Design matrix mismatch: {S_loaded.X} != {S.X}\")\n",
    "if not np.array_equal(S_loaded.y, S.y):\n",
    "    print(f\"Response vector mismatch: {S_loaded.y} != {S.y}\")\n",
    "\n",
    "S_loaded.run()\n",
    "\n",
    "S.run()\n",
    "S_loaded_2 = load_result(PREFIX)\n",
    "S_loaded_2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_exp_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.eda import print_exp_table, print_res_table\n",
    "from spotpython.hyperparameters.values import set_hyperparameter\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=\"show_exp_table\",\n",
    "    fun_evals=5,\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
    "set_hyperparameter(fun_control, \"l1\", [1,2])\n",
    "set_hyperparameter(fun_control, \"epochs\", [2,2])\n",
    "set_hyperparameter(fun_control, \"batch_size\", [4,11])\n",
    "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
    "set_hyperparameter(fun_control, \"patience\", [1,2])\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "print_exp_table(fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_res_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.eda import print_res_table\n",
    "from spotpython.hyperparameters.values import set_hyperparameter\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=\"show_exp_table\",\n",
    "    fun_evals=5,\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
    "set_hyperparameter(fun_control, \"l1\", [1,2])\n",
    "set_hyperparameter(fun_control, \"epochs\", [2,2])\n",
    "set_hyperparameter(fun_control, \"batch_size\", [4,11])\n",
    "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
    "set_hyperparameter(fun_control, \"patience\", [1,2])\n",
    "design_control = design_control_init(init_size=3)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "S = Spot(fun=fun, fun_control=fun_control, design_control=design_control)\n",
    "\n",
    "S.run()\n",
    "\n",
    "print_res_table(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_new_X0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "            seed=1,\n",
    "            n_points=10,\n",
    "            ocba_delta=0,\n",
    "            lower = np.array([-1, -1]),\n",
    "            upper = np.array([1, 1])\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    ")\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "X0 = S.get_new_X0()\n",
    "assert X0.shape[0] == S.n_points\n",
    "assert X0.shape[1] == S.lower.size\n",
    "# assert new points are in the interval [lower, upper]\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "# print using 20 digits precision\n",
    "np.set_printoptions(precision=20)\n",
    "print(f\"X0: {X0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init\n",
    "nn = 10\n",
    "fun_sphere = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        n_points=nn,\n",
    "        )\n",
    "spot_1 = spot.Spot(\n",
    "    fun=fun_sphere,\n",
    "    fun_control=fun_control,\n",
    "    )\n",
    "# (S-2) Initial Design:\n",
    "spot_1.X = spot_1.design.scipy_lhd(\n",
    "    spot_1.design_control[\"init_size\"], lower=spot_1.lower, upper=spot_1.upper\n",
    ")\n",
    "print(f\"spot_1.X: {spot_1.X}\")\n",
    "# (S-3): Eval initial design:\n",
    "spot_1.y = spot_1.fun(spot_1.X)\n",
    "print(f\"spot_1.y: {spot_1.y}\")\n",
    "spot_1.fit_surrogate()\n",
    "spot_1.suggest_X0()\n",
    "X0 = spot_1.X0\n",
    "print(f\"X0: {X0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate_mean_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.aggregate import aggregate_mean_var\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4], [1, 2]])\n",
    "y = np.array([1, 2, 3])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1,2]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.ones((2, 3))\n",
    "y_1 = np.sum(X_1, axis=1)\n",
    "y_2 = 2 * y_1\n",
    "X_2 = np.append(X_1, 2 * X_1, axis=0)\n",
    "X = np.append(X_2, X_1, axis=0)\n",
    "y = np.append(y_1, y_2, axis=0)\n",
    "y = np.append(y, y_2, axis=0)\n",
    "print(X)\n",
    "print(y)\n",
    "Z = aggregate_mean_var(X, y, var_empirical=True)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.ones((2, 3))\n",
    "y_1 = np.sum(X_1, axis=1)\n",
    "y_2 = 2 * y_1\n",
    "X_2 = np.append(X_1, 2 * X_1, axis=0)\n",
    "X = np.append(X_2, X_1, axis=0)\n",
    "y = np.append(y_1, y_2, axis=0)\n",
    "y = np.append(y, y_2, axis=0)\n",
    "print(X)\n",
    "print(y)\n",
    "Z = aggregate_mean_var(X, y, var_empirical=False)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.aggregate import aggregate_mean_var\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4], [1, 2]])\n",
    "y = np.array([1, 2, 3])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y, var_empirical=True)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1,2]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "X_agg, y_mean, y_var = aggregate_mean_var(X, y, var_empirical=True)\n",
    "print(X_agg)\n",
    "print(y_mean)\n",
    "print(y_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit_surrogate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# number of initial points:\n",
    "ni = 0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select_distant_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.aggregate import select_distant_points\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "selected_points, selected_y = select_distant_points(X, y, 3)\n",
    "print(selected_points)\n",
    "print(selected_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# number of initial points:\n",
    "ni = 0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.surrogate.predict(np.array([[0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.spot import spot\n",
    "# number of initial points:\n",
    "ni = 0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "X_shape_before = S.X.shape\n",
    "print(f\"X_shape_before: {X_shape_before}\")\n",
    "print(f\"y_size_before: {S.y.size}\")\n",
    "y_size_before = S.y.size\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.update_design()\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "print(f\"S.n_points: {S.n_points}\")\n",
    "print(f\"X_shape_after: {S.X.shape}\")\n",
    "print(f\"y_size_after: {S.y.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "        sigma=0.02,\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        noise=True,\n",
    "        ocba_delta=1,\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni, repeats=2)\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            design_control=design_control,\n",
    "            fun_control=fun_control\n",
    ")\n",
    "S.initialize_design(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "X_shape_before = S.X.shape\n",
    "print(f\"X_shape_before: {X_shape_before}\")\n",
    "print(f\"y_size_before: {S.y.size}\")\n",
    "y_size_before = S.y.size\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.update_design()\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")\n",
    "print(f\"S.n_points: {S.n_points}\")\n",
    "print(f\"S.ocba_delta: {S.ocba_delta}\")\n",
    "print(f\"X_shape_after: {S.X.shape}\")\n",
    "print(f\"y_size_after: {S.y.size}\")\n",
    "# compare the shapes of the X and y values before and after the update_design method\n",
    "assert X_shape_before[0] + S.ocba_delta == S.X.shape[0]\n",
    "assert X_shape_before[1] == S.X.shape[1]\n",
    "assert y_size_before + S.ocba_delta == S.y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_ocba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.budget.ocba import get_ocba\n",
    "from spotpython.utils import fun_control_init, design_control_init, surrogate_control_init\n",
    "# Example is based on the example from the book:\n",
    "# Chun-Hung Chen and Loo Hay Lee:\n",
    "#     Stochastic Simulation Optimization: An Optimal Computer Budget Allocation,\n",
    "#     pp. 49 and pp. 215\n",
    "#     p. 49:\n",
    "#     mean_y = np.array([1,2,3,4,5])\n",
    "#     var_y = np.array([1,1,9,9,4])\n",
    "#     get_ocba(mean_y, var_y, 50)\n",
    "#     [11  9 19  9  2]\n",
    "fun = Analytical().fun_linear\n",
    "fun_control = fun_control_init(\n",
    "                lower = np.array([-1]),\n",
    "                upper = np.array([1]),\n",
    "                fun_evals = 20,\n",
    "                fun_repeats = 2,\n",
    "                noise = True,\n",
    "                ocba_delta=1,\n",
    "                seed=123,\n",
    "                show_models=False,\n",
    "                sigma=0.001,\n",
    "                )\n",
    "design_control = design_control_init(init_size=3, repeats=2)\n",
    "surrogate_control = surrogate_control_init(noise=True)\n",
    "spot_1_noisy = Spot(fun=fun,                \n",
    "                fun_control = fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control)\n",
    "spot_1_noisy.run()\n",
    "spot_2 = copy.deepcopy(spot_1_noisy)\n",
    "spot_2.mean_y = np.array([1,2,3,4,5])\n",
    "spot_2.var_y = np.array([1,1,9,9,4])\n",
    "n = 50\n",
    "o = get_ocba(spot_2.mean_y, spot_2.var_y, n)\n",
    "assert sum(o) == 50\n",
    "assert (o == np.array([[11, 9, 19, 9, 2]])).all()\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_ocba_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.budget.ocba import get_ocba_X\n",
    "from spotpython.utils.aggregate import aggregate_mean_var\n",
    "import numpy as np\n",
    "X = np.array([[1,2,3],\n",
    "            [1,2,3],\n",
    "            [4,5,6],\n",
    "            [4,5,6],\n",
    "            [4,5,6],\n",
    "            [7,8,9],\n",
    "            [7,8,9],])\n",
    "y = np.array([1,2,30,40, 40, 500, 600  ])\n",
    "Z = aggregate_mean_var(X=X, y=y)\n",
    "mean_X = Z[0]\n",
    "mean_y = Z[1]\n",
    "var_y = Z[2]\n",
    "print(f\"X: {X}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"mean_X: {mean_X}\")\n",
    "print(f\"mean_y: {mean_y}\")\n",
    "print(f\"var_y: {var_y}\")\n",
    "delta = 5\n",
    "X_new = get_ocba_X(X=mean_X, means=mean_y, vars=var_y, delta=delta,verbose=True)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## suggest_new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init\n",
    "nn = 3\n",
    "fun_sphere = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        n_points=nn,\n",
    "        )\n",
    "S = Spot(\n",
    "    fun=fun_sphere,\n",
    "    fun_control=fun_control,\n",
    "    )\n",
    "S.X = S.design.scipy_lhd(\n",
    "    S.design_control[\"init_size\"], lower=S.lower, upper=S.upper\n",
    ")\n",
    "print(f\"S.X: {S.X}\")\n",
    "S.y = S.fun(S.X)\n",
    "print(f\"S.y: {S.y}\")\n",
    "S.fit_surrogate()\n",
    "X0 = S.suggest_new_X()\n",
    "print(f\"X0: {X0}\")\n",
    "assert X0.size == S.n_points * S.k\n",
    "assert X0.ndim == 2\n",
    "assert X0.shape[0] == nn\n",
    "assert X0.shape[1] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.optimize import dual_annealing\n",
    "from scipy.optimize import basinhopping\n",
    "    \n",
    "nn = 2\n",
    "fun_sphere = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower=np.array([-1, -1]),\n",
    "    upper=np.array([1, 1]),\n",
    "    n_points=nn,\n",
    ")\n",
    "design_control = design_control_init(init_size=10)\n",
    "surrogate_control = surrogate_control_init()\n",
    "\n",
    "# optimizers = [dual_annealing, differential_evolution, direct, shgo, basinhopping]\n",
    "optimizers = [differential_evolution, dual_annealing, direct, shgo]\n",
    "\n",
    "for optimizer_name in optimizers:\n",
    "    optimizer_control = optimizer_control_init()\n",
    "\n",
    "    S = Spot(\n",
    "        fun=fun_sphere,\n",
    "        fun_control=fun_control,\n",
    "        design_control=design_control,\n",
    "        optimizer_control=optimizer_control,\n",
    "        surrogate_control=surrogate_control,\n",
    "        optimizer=optimizer_name\n",
    "    )\n",
    "    \n",
    "    S.X = S.design.scipy_lhd(\n",
    "        S.design_control[\"init_size\"], lower=S.lower, upper=S.upper\n",
    "    )\n",
    "    S.y = S.fun(S.X)\n",
    "    S.fit_surrogate()\n",
    "    X0 = S.suggest_new_X()\n",
    "    print(f\"X0: {X0}\")\n",
    "\n",
    "    assert X0.size <= S.n_points * S.k\n",
    "    assert X0.ndim == 2\n",
    "    assert X0.shape[0] <= nn\n",
    "    assert X0.shape[1] == 2\n",
    "    assert np.all(X0 >= S.lower)\n",
    "    assert np.all(X0 <= S.upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=100, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.plot(X, s_ei, label=\"Expected Improvement\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, -s_ei, label=\"Negative Expected Improvement\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 0, 0], [1, 0, 0]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.fit(nat_X, nat_y)\n",
    "print(S.Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4], [1,2]])\n",
    "nat_y = np.array([1, 2, 11])\n",
    "S = Kriging()\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "print(f\"S.nat_X: {S.nat_X}\")\n",
    "print(f\"S.nat_y: {S.nat_y}\")\n",
    "print(f\"S.aggregated_mean_y: {S.aggregated_mean_y}\")\n",
    "print(f\"S.min_X: {S.min_X}\")\n",
    "print(f\"S.max_X: {S.max_X}\")\n",
    "print(f\"S.n: {S.n}\")\n",
    "print(f\"S.k: {S.k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "assert S.var_type == ['num', 'num']\n",
    "assert S.num_mask.all() == True\n",
    "assert S.factor_mask.all() == False\n",
    "assert S.int_mask.all() == False\n",
    "assert S.ordered_mask.all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "nat_y = np.array([1, 2, 3])\n",
    "var_type = [\"num\", \"int\", \"float\"]\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(var_type=var_type, seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "assert S.var_type == [\"num\", \"int\", \"float\"]\n",
    "assert S.num_mask.all() == False\n",
    "assert S.factor_mask.all() == False\n",
    "assert S.int_mask.all() == False\n",
    "assert S.ordered_mask.all() == True\n",
    "assert np.all(S.num_mask == np.array([True, False, False]))\n",
    "assert np.all(S.int_mask == np.array([False, True, False]))\n",
    "assert np.all(S.ordered_mask == np.array([True, True, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "# Create a Kriging instance with var_type shorter than k\n",
    "var_type = [\"num\"]\n",
    "S = Kriging(var_type=var_type, seed=124, n_theta=2, n_p=2, optim_p=True, noise=True)\n",
    "\n",
    "# Initialize variables\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "nat_y = np.array([1, 2, 3])\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "\n",
    "# Set variable types\n",
    "S._set_variable_types()\n",
    "\n",
    "print(f\"S.var_type: {S.var_type}\")\n",
    "\n",
    "# Check if the variable types are defaulted to 'num' and masks are set correctly\n",
    "assert S.var_type == [\"num\", \"num\"]\n",
    "assert np.all(S.num_mask == np.array([True, True]))\n",
    "assert np.all(S.int_mask == np.array([False, False]))\n",
    "assert np.all(S.ordered_mask == np.array([True, True]))\n",
    "assert np.all(S.factor_mask == np.array([False, False]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "n = nat_X.shape[0]\n",
    "k = nat_X.shape[1]\n",
    "nat_y = np.array([1, 2])\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "assert S.theta.all() == array([0., 0.]).all()\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "t = np.ones(n_theta, dtype=float) * n / (100 * k)\n",
    "assert S.theta.all() == t.all()\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "n = nat_X.shape[0]\n",
    "k = nat_X.shape[1]\n",
    "nat_y = np.array([1, 2, 3])\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "assert S.theta.all() == array([0., 0.]).all()\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True, theta_init_zero=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "t = np.ones(n_theta, dtype=float) * n / (100 * k)\n",
    "assert S.theta.all() == t.all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "from numpy import log, var\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "nat_y = np.array([1, 2, 3])\n",
    "n = nat_X.shape[0]\n",
    "k = nat_X.shape[1]\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "S._initialize_matrices()\n",
    "assert np.all(S.p == 2.0 * np.ones(n_p))\n",
    "# if var(self.nat_y) is > 0, then self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n",
    "# else self.pen_val = self.n * var(self.nat_y) + 1e4\n",
    "assert S.pen_val == nat_X.shape[0] * log(var(S.nat_y)) + 1e4\n",
    "assert S.Psi.shape == (n, n)\n",
    "assert S.psi.shape == (n, 1)\n",
    "assert S.one.shape == (n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "S = Kriging()\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "S = Kriging(n_theta=2, n_p=2, optim_p=True)\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "S = Kriging(n_theta=2, n_p=2, optim_p=True, noise=True)\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "S = Kriging(n_theta=2, n_p=2, noise=True)\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=True)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "S._initialize_matrices()\n",
    "S._set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "assert  len(new_theta_p_Lambda) == n_theta + n_p + 1\n",
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n_theta=2\n",
    "n_p=2\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "S._initialize_matrices()\n",
    "S._set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "assert len(new_theta_p_Lambda) == n_theta + n_p\n",
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n_theta=2\n",
    "n_p=1\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=True, noise=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "S._initialize_matrices()\n",
    "S._set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "assert  len(new_theta_p_Lambda) == n_theta + n_p\n",
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n_theta=1\n",
    "n_p=1\n",
    "S=Kriging(seed=124, n_theta=n_theta, n_p=n_p, optim_p=False, noise=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "S._set_theta_values()\n",
    "S._initialize_matrices()\n",
    "S._set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "assert  len(new_theta_p_Lambda) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract_from_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build import Kriging\n",
    "num_theta = 2\n",
    "num_p = 3\n",
    "S = Kriging(\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=True\n",
    ")\n",
    "bounds_array = np.array([1, 2, 3, 4, 5, 6])\n",
    "S.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "assert np.array_equal(S.theta,\n",
    "    [1, 2]), f\"Expected theta to be [1, 2] but got {S.theta}\"\n",
    "assert np.array_equal(S.p,\n",
    "    [3, 4, 5]), f\"Expected p to be [3, 4, 5] but got {S.p}\"\n",
    "assert S.Lambda == 6, f\"Expected Lambda to be 6 but got {S.Lambda}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build import Kriging\n",
    "num_theta = 1\n",
    "num_p = 1\n",
    "S = Kriging(\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=False,\n",
    "    noise=False\n",
    ")\n",
    "bounds_array = np.array([1])\n",
    "S.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "assert np.array_equal(S.theta,\n",
    "    [1]), f\"Expected theta to be [1] but got {S.theta}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build import Kriging\n",
    "num_theta = 1\n",
    "num_p = 2\n",
    "S = Kriging(\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=True\n",
    ")\n",
    "bounds_array = np.array([1, 2, 3, 4])\n",
    "S.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "assert np.array_equal(S.theta,\n",
    "    [1]), f\"Expected theta to be [1, 2] but got {S.theta}\"\n",
    "assert np.array_equal(S.p,\n",
    "    [2, 3]), f\"Expected p to be [3, 4, 5] but got {S.p}\"\n",
    "assert S.Lambda == 4, f\"Expected Lambda to be 6 but got {S.Lambda}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build_Psi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S._initialize_variables(nat_X, nat_y)\n",
    "S._set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S._set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S._initialize_matrices()\n",
    "S._set_de_bounds()\n",
    "new_theta_p_Lambda = S._optimize_model()\n",
    "S._extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")\n",
    "print(f\"S.cnd_Psi: {S.cnd_Psi}\")\n",
    "print(f\"S.inf_Psi: {S.inf_Psi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all_estimates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"/Users/bartz/workspace/ki-for-hcf/KI-basierte_Verdichterauslegung/\"\n",
    "import pandas as pd\n",
    "\n",
    "def load_pkl_files(dirname=\"\"):\n",
    "    df_x1 = pd.read_pickle(dirname+'x_1.pkl')\n",
    "    df_x2 = pd.read_pickle(dirname+'x_2.pkl')\n",
    "    df_y = pd.read_pickle(dirname+'y.pkl')\n",
    "    # combine x_1 and x_2 and  y\n",
    "    df = pd.concat([df_x1, df_x2, df_y], axis=1)\n",
    "    df_x1x2 = pd.concat([df_x1, df_x2], axis=1)\n",
    "    df_x1y = pd.concat([df_x1, df_y], axis=1)\n",
    "    df_x2y = pd.concat([df_x2, df_y], axis=1)\n",
    "    return df, df_x1, df_x2, df_x1x2, df_x1y, df_x2y, df_y\n",
    "df, df_x1, df_x2, df_x1x2, df_x1y, df_x2y, df_y = load_pkl_files(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_vars(crude):\n",
    "    \"\"\"Utility function to extract variables from a formula.\n",
    "    \n",
    "    Args:\n",
    "        crude (str): A formula.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of variables.\n",
    "        \n",
    "    Examples:\n",
    "        all_vars(\"y ~ x1 + x2\")\n",
    "        ['y', 'x1', 'x2']\n",
    "        all_vars(\"y ~ x1 + x2 + x3\")\n",
    "        ['y', 'x1', 'x2', 'x3']\n",
    "    \"\"\"    \n",
    "    # Split the formula into the dependent and independent variables\n",
    "    dependent, independent = crude.split(\"~\")\n",
    "    # Strip whitespace and split the independent variables by '+'\n",
    "    independent_vars = independent.strip().split(\"+\")\n",
    "    # Combine the dependent variable with the independent variables\n",
    "    return [dependent.strip()] + [var.strip() for var in independent_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df_y.iloc[:, 0], df_x2], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars(\"AR ~ AK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import norm\n",
    "from statsmodels.tools.eval_measures import aic\n",
    "\n",
    "def all_lm(crude, xlist, data, remove_na=True):\n",
    "    # Prepare the data frame\n",
    "    data = copy.deepcopy(data)\n",
    "    data = data[all_vars(crude) + xlist]\n",
    "    if remove_na:\n",
    "        data = data.dropna()\n",
    "    print(data.head())\n",
    "    # Crude model\n",
    "    mod_0 = ols(crude, data=data).fit()\n",
    "    p = mod_0.pvalues.iloc[1]\n",
    "    print(f\"p-values: {p}\")\n",
    "    estimate = mod_0.params.iloc[1]\n",
    "    print(f\"estimate: {estimate}\")  \n",
    "    conf_int = mod_0.conf_int().iloc[1]\n",
    "    print(f\"conf_int: {conf_int}\")\n",
    "    aic_value = mod_0.aic\n",
    "    print(f\"aic: {aic_value}\")\n",
    "    n = len(mod_0.resid)\n",
    "    df_0 = pd.DataFrame([[\"Crude\", estimate, conf_int[0], conf_int[1], p, aic_value, n]], \n",
    "                        columns=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\"])\n",
    "    \n",
    "    # All combinations model\n",
    "    comb_lst = list(itertools.chain.from_iterable(itertools.combinations(xlist, r) for r in range(1, len(xlist)+1)))\n",
    "    models = [ols(f\"{crude} + {' + '.join(comb)}\", data=data).fit() for comb in comb_lst]\n",
    "\n",
    "    df_list = []\n",
    "    for i, model in enumerate(models):\n",
    "        p = model.pvalues.iloc[1]\n",
    "        estimate = model.params.iloc[1]\n",
    "        conf_int = model.conf_int().iloc[1]\n",
    "        aic_value = model.aic\n",
    "        n = len(model.resid)\n",
    "        comb_str = \", \".join(comb_lst[i])\n",
    "        df_list.append([comb_str, estimate, conf_int[0], conf_int[1], p, aic_value, n])\n",
    "    \n",
    "    df_coef = pd.DataFrame(df_list, columns=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\"])\n",
    "    estimates = pd.concat([df_0, df_coef], ignore_index=True)\n",
    "    return {\"estimate\": estimates, \"xlist\": xlist, \"fun\": \"all_lm\", \"crude\": crude, \"family\": \"lm\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlist2 = [\"AL\", \"AM\", \"AN\", \"AO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = all_lm(crude = \"AR ~ AK\", xlist = vlist2, data = df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_plot(data, xlabels=None, xlim=(0, 1), xlab=\"P value\", ylim=None, ylab=None, xscale_log=True, yscale_log=False, title=None):\n",
    "    data = copy.deepcopy(data)\n",
    "    if xlabels is None:\n",
    "        xlabels = [0, 0.001, 0.01, 0.05, 0.2, 0.5, 1]\n",
    "    xbreaks = np.power(xlabels, np.log(0.5) / np.log(0.05))\n",
    "    \n",
    "    result_df = data[\"estimate\"]\n",
    "    if ylab is None:\n",
    "        ylab = \"Coefficient\" if data[\"fun\"] == \"all_lm\" else \"Effect estimates\"\n",
    "    hline = 0 if data[\"fun\"] == \"all_lm\" else 1\n",
    "    \n",
    "    result_df['p_value'] = np.power(result_df['p'], np.log(0.5) / np.log(0.05))\n",
    "    if ylim is None:\n",
    "        maxv = max(result_df['estimate'].max(), abs(result_df['estimate'].min()))\n",
    "        ylim = (-maxv, maxv) if data[\"fun\"] == \"all_lm\" else (1/maxv, maxv)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=result_df, x=\"p_value\", y=\"estimate\")\n",
    "    if xscale_log:\n",
    "        plt.xscale('log')\n",
    "    if yscale_log:\n",
    "        plt.yscale('log')\n",
    "    plt.xticks(ticks=xbreaks, labels=xlabels)\n",
    "    plt.axvline(x=0.5, linestyle='--')\n",
    "    plt.axhline(y=hline, linestyle='--')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def all_plot2(data, xlabels=None, xlim=(0, 1), xlab=\"P value\", ylim=None, ylab=None, yscale_log=False, title=None, grid=True, ncol=2):\n",
    "    \"\"\"\n",
    "    Generates a panel of scatter plots with effect estimates of all possible models against p-values.\n",
    "    Each plot includes effect estimates from all models including a specific variable.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary containing the following keys:\n",
    "            - estimate (pd.DataFrame): A DataFrame containing the estimates.\n",
    "            - xlist (list): A list of variables.\n",
    "            - fun (str): The function name.\n",
    "            - family (str): The family of the model.\n",
    "        xlabels (list): A list of x-axis labels.\n",
    "        xlim (tuple): The x-axis limits.\n",
    "        xlab (str): The x-axis label.\n",
    "        ylim (tuple): The y-axis limits.\n",
    "        ylab (str): The y-axis label.\n",
    "        yscale_log (bool): Whether to scale y-axis to log10. Default is False.\n",
    "        title (str): The title of the plot.\n",
    "        grid (bool): Whether to display gridlines. Default is True.\n",
    "        ncol (int): Number of columns in the plot grid. Default is 2.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "               \n",
    "    Examples:\n",
    "        data = {\n",
    "            \"estimate\": pd.DataFrame({\n",
    "                \"variables\": [\"Crude\", \"AL\", \"AM\", \"AN\", \"AO\"],\n",
    "                \"estimate\": [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                \"conf_low\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                \"conf_high\": [0.9, 1.0, 1.1, 1.2, 1.3],\n",
    "                \"p\": [0.01, 0.02, 0.03, 0.04, 0.05],\n",
    "                \"aic\": [100, 200, 300, 400, 500],\n",
    "                \"n\": [10, 20, 30, 40, 50]\n",
    "            }),\n",
    "            \"xlist\": [\"AL\", \"AM\", \"AN\", \"AO\"],\n",
    "            \"fun\": \"all_lm\"\n",
    "        }\n",
    "        all_plot2(data)\n",
    "    \"\"\"\n",
    "    if xlabels is None:\n",
    "        xlabels = [0, 0.001, 0.01, 0.05, 0.2, 0.5, 1]\n",
    "    xbreaks = np.power(xlabels, np.log(0.5) / np.log(0.05))\n",
    "    \n",
    "    result_df = data[\"estimate\"]\n",
    "    if ylab is None:\n",
    "        ylab = {\n",
    "            \"all_lm\": \"Coefficient\",\n",
    "            \"poisson\": \"Rate ratio\",\n",
    "            \"binomial\": \"Odds ratio\"\n",
    "        }.get(data.get(\"fun\"), \"Effect estimates\")\n",
    "    \n",
    "    hline = 0 if data[\"fun\"] == \"all_lm\" else 1\n",
    "    \n",
    "    result_df[\"p_value\"] = np.power(result_df[\"p\"], np.log(0.5) / np.log(0.05))\n",
    "    if ylim is None:\n",
    "        maxv = max(result_df[\"estimate\"].max(), abs(result_df[\"estimate\"].min()))\n",
    "        if data[\"fun\"] == \"all_lm\":\n",
    "            ylim = (-maxv, maxv)\n",
    "        else:\n",
    "            ylim = (1 / maxv, maxv)\n",
    "\n",
    "    # Create a DataFrame to mark inclusion of variables\n",
    "    mark_df = pd.DataFrame({x: result_df[\"variables\"].str.contains(x).astype(int) for x in data[\"xlist\"]})\n",
    "    df_scatter = pd.concat([result_df, mark_df], axis=1)\n",
    "    \n",
    "    # Melt the DataFrame for plotting\n",
    "    df_long = df_scatter.melt(id_vars=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\", \"p_value\"], \n",
    "                              value_vars=data[\"xlist\"],\n",
    "                              var_name=\"variable\", value_name=\"inclusion\")\n",
    "    df_long[\"inclusion\"] = df_long[\"inclusion\"].apply(lambda x: \"Included\" if x > 0 else \"Not included\")\n",
    "\n",
    "    # Calculate number of rows based on number of columns\n",
    "    nrow = int(np.ceil(len(data[\"xlist\"]) / ncol))\n",
    "\n",
    "    # Plotting\n",
    "    g = sns.FacetGrid(df_long, col=\"variable\", hue=\"inclusion\", palette={\"Included\": \"blue\", \"Not included\": \"orange\"},\n",
    "                      col_wrap=ncol, height=4, sharex=False, sharey=False)\n",
    "    g.map(sns.scatterplot, \"p_value\", \"estimate\")\n",
    "    g.add_legend()\n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_xticks(xbreaks)\n",
    "        ax.set_xticklabels(xlabels)\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.axvline(x=0.5, linestyle='--', linewidth=1.5, color='black')  # Black dashed vertical line\n",
    "        ax.axhline(y=hline, linestyle='--', linewidth=1.5, color='black')  # Black dashed horizontal line\n",
    "        if grid:\n",
    "            ax.grid(True)\n",
    "    if yscale_log:\n",
    "        g.set(yscale=\"log\")\n",
    "    g.set_axis_labels(xlab, ylab)\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    if title:\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        g.figure.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot2(res, ylim=(-0.2,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def all_plot2_old(data, xlabels=None, xlim=(0, 1), xlab=\"P value\", ylim=None, ylab=None, title=None, grid=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict): A dictionary containing the following keys:\n",
    "            - estimate (pd.DataFrame): A DataFrame containing the estimates.\n",
    "            - xlist (list): A list of variables.\n",
    "            - fun (str): The function name.\n",
    "            - family (str): The family of the model.\n",
    "        xlabels (list): A list of x-axis labels.\n",
    "        xlim (tuple): The x-axis limits.\n",
    "        xlab (str): The x-axis label.\n",
    "        ylim (tuple): The y-axis limits.\n",
    "        ylab (str): The y-axis label.\n",
    "        title (str): The title of the plot.\n",
    "        grid (bool): Whether to display gridlines. Default is True.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "               \n",
    "    Examples:\n",
    "        data = {\n",
    "            \"estimate\": pd.DataFrame({\n",
    "                \"variables\": [\"Crude\", \"AL\", \"AM\", \"AN\", \"AO\"],\n",
    "                \"estimate\": [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                \"conf_low\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                \"conf_high\": [0.9, 1.0, 1.1, 1.2, 1.3],\n",
    "                \"p\": [0.01, 0.02, 0.03, 0.04, 0.05],\n",
    "                \"aic\": [100, 200, 300, 400, 500],\n",
    "                \"n\": [10, 20, 30, 40, 50]\n",
    "            }),\n",
    "            \"xlist\": [\"AL\", \"AM\", \"AN\", \"AO\"],\n",
    "            \"fun\": \"all_lm\"\n",
    "        }\n",
    "    \n",
    "    \"\"\"\n",
    "    if xlabels is None:\n",
    "        xlabels = [0, 0.001, 0.01, 0.05, 0.2, 0.5, 1]\n",
    "    xbreaks = np.power(xlabels, np.log(0.5) / np.log(0.05))\n",
    "    \n",
    "    result_df = data[\"estimate\"]\n",
    "    if ylab is None:\n",
    "        if data[\"fun\"] == \"all_lm\":\n",
    "            ylab = \"Coefficient\"\n",
    "        elif data[\"family\"] == \"poisson\":\n",
    "            ylab = \"Rate ratio\"\n",
    "        elif data[\"family\"] == \"binomial\":\n",
    "            ylab = \"Odds ratio\"\n",
    "        else:\n",
    "            ylab = \"Effect estimates\"\n",
    "    hline = 0 if data[\"fun\"] == \"all_lm\" else 1\n",
    "    \n",
    "    result_df['p_value'] = np.power(result_df['p'], np.log(0.5) / np.log(0.05))\n",
    "    if ylim is None:\n",
    "        if data[\"fun\"] == \"all_lm\":\n",
    "            maxv = max(result_df['estimate'].max(), abs(result_df['estimate'].min()))\n",
    "            ylim = (-maxv, maxv)\n",
    "        else:\n",
    "            maxv = max(max(1 / result_df['estimate']), max(result_df['estimate']))\n",
    "            ylim = (1 / maxv, maxv)\n",
    "\n",
    "    # Create a DataFrame to mark inclusion of variables\n",
    "    mark_df = pd.DataFrame({x: result_df['variables'].str.contains(x).astype(int) for x in data['xlist']})\n",
    "    df_scatter = pd.concat([result_df, mark_df], axis=1)\n",
    "    \n",
    "    # Melt the DataFrame for plotting\n",
    "    df_long = df_scatter.melt(id_vars=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\", \"p_value\"], \n",
    "                              value_vars=data['xlist'],\n",
    "                              var_name=\"variable\", value_name=\"inclusion\")\n",
    "    df_long['inclusion'] = df_long['inclusion'].apply(lambda x: \"Included\" if x > 0 else \"Not included\")\n",
    "\n",
    "    # Plotting\n",
    "    g = sns.FacetGrid(df_long, col=\"variable\", hue=\"inclusion\", height=4, sharex=False, sharey=False)\n",
    "    g.map(sns.scatterplot, \"p_value\", \"estimate\")\n",
    "    g.add_legend()\n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_xticks(xbreaks)\n",
    "        ax.set_xticklabels(xlabels)\n",
    "        if grid:\n",
    "            ax.grid(True)\n",
    "    g.set_axis_labels(xlab, ylab)\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    if title:\n",
    "        plt.subplots_adjust(top=0.8)\n",
    "        g.fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests for fit_all_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_data(n_samples=100, b0=1, b1=2, b2=3, b3=4, b12=5, b13=6, b23=7, b123=8, noise_std=1):\n",
    "    \"\"\"\n",
    "    Generate data for the linear formula y ~ b0 + b1*x1 + b2*x2 + b3*x3 + b12*x1*x2 + b13*x1*x3 + b23*x2*x3 + b123*x1*x2*x3.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): Number of samples to generate.\n",
    "        b0 (float): Coefficient for the intercept.\n",
    "        b1 (float): Coefficient for x1.\n",
    "        b2 (float): Coefficient for x2.\n",
    "        b3 (float): Coefficient for x3.\n",
    "        b12 (float): Coefficient for the interaction term x1*x2.\n",
    "        b13 (float): Coefficient for the interaction term x1*x3.\n",
    "        b23 (float): Coefficient for the interaction term x2*x3.\n",
    "        b123 (float): Coefficient for the interaction term x1*x2*x3.\n",
    "        noise_std (float): Standard deviation of the Gaussian noise added to y.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the generated data with columns ['x1', 'x2', 'x3', 'y'].\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    x1 = np.random.uniform(0, 1, n_samples)\n",
    "    x2 = np.random.uniform(0, 1, n_samples)\n",
    "    x3 = np.random.uniform(0, 1, n_samples)\n",
    "    \n",
    "    y = (b0 + b1*x1 + b2*x2 + b3*x3 + b12*x1*x2 + b13*x1*x3 + b23*x2*x3 + b123*x1*x2*x3 +\n",
    "         np.random.normal(0, noise_std, n_samples))\n",
    "    \n",
    "    data = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "data = generate_data()\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_data(b0=0, b1=1, b2=0, b3=10, b12=0, b13=0, b23=0, b123=0, noise_std=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def fit_ols_model(formula, data):\n",
    "    \"\"\"\n",
    "    Fit an OLS model using the given formula and data, and print the results.\n",
    "\n",
    "    Args:\n",
    "        formula (str): The formula for the OLS model.\n",
    "        data (pd.DataFrame): The data frame containing the variables.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the p-values, estimates, confidence intervals, and AIC value.\n",
    "    \"\"\"\n",
    "    mod_0 = smf.ols(formula=formula, data=data).fit()\n",
    "    p = mod_0.pvalues.iloc[1]\n",
    "    estimate = mod_0.params.iloc[1]\n",
    "    conf_int = mod_0.conf_int().iloc[1]\n",
    "    aic_value = mod_0.aic\n",
    "\n",
    "    print(f\"p-values: {p}\")\n",
    "    print(f\"estimate: {estimate}\")\n",
    "    print(f\"conf_int: {conf_int}\")\n",
    "    print(f\"aic: {aic_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_ols_model(\"y ~ x1\", data)\n",
    "fit_ols_model(\"y ~ x1 + x2\", data)\n",
    "fit_ols_model(\"y ~ x1 + x3\", data)\n",
    "fit_ols_model(\"y ~ x1 + x2 + x3\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.stats import fit_all_lm, plot_coeff_vs_pvals, plot_coeff_vs_pvals_by_included\n",
    "res = fit_all_lm(\"y ~ x1\", [\"x2\", \"x3\"], data)\n",
    "print(res[\"estimate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coeff_vs_pvals_by_included(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a 5x5 correlation matrix, generate a DataFrame with samples drawn from a multivariate normal distribution with the given correlation matrix. Since the correlation matrix is symmetric, we only need to specify the upper triangular part of the matrix. Write a function that accepts the upper triangular part of the correlation matrix and the number of samples to generate. The function should return a DataFrame with the generated samples.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_samples_from_correlation_matrix(upper_tri, n_samples):\n",
    "    \"\"\"\n",
    "    Generate samples from a multivariate normal distribution with a given correlation matrix.\n",
    "\n",
    "    Args:\n",
    "        upper_tri (list): A list containing the upper triangular part of the 5x5 correlation matrix.\n",
    "        n_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the generated samples.\n",
    "    \"\"\"\n",
    "    # Create an empty 5x5 correlation matrix\n",
    "    corr_matrix = np.zeros((5, 5))\n",
    "\n",
    "    # Fill the upper triangular part of the correlation matrix\n",
    "    upper_tri_indices = np.triu_indices(5, k=1)\n",
    "    corr_matrix[upper_tri_indices] = upper_tri\n",
    "\n",
    "    # Make the correlation matrix symmetric\n",
    "    corr_matrix = corr_matrix + corr_matrix.T\n",
    "\n",
    "    # Set the diagonal elements to 1\n",
    "    np.fill_diagonal(corr_matrix, 1)\n",
    "\n",
    "    # Generate samples from a multivariate normal distribution\n",
    "    mean = np.zeros(5)\n",
    "    samples = np.random.multivariate_normal(mean, corr_matrix, size=n_samples)\n",
    "\n",
    "    # Create a DataFrame with the generated samples\n",
    "    columns = [f'x{i+1}' for i in range(4)]\n",
    "    columns = ['y'] + columns\n",
    "    df_samples = pd.DataFrame(samples, columns=columns)\n",
    "\n",
    "    return df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_tri = [1, 0.5, 0.5, 0.5,\n",
    "                .5, .0, .0,\n",
    "                    .0, .0,\n",
    "                        .5]\n",
    "n_samples = 10\n",
    "df_samples = generate_samples_from_correlation_matrix(upper_tri, n_samples)\n",
    "print(df_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.stats import fit_all_lm, plot_coeff_vs_pvals, plot_coeff_vs_pvals_by_included\n",
    "res = fit_all_lm(\"y ~ x1\", [\"x2\", \"x3\", \"x4\"], df_samples)\n",
    "print(res[\"estimate\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coeff_vs_pvals_by_included(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from spotpython.gp.functions import fried\n",
    "from spotpython.gp.distances import dist\n",
    "from spotpython.gp.likelihood import nl, gradnl\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Parameters\n",
    "n = 100\n",
    "m = 7\n",
    "eps = np.sqrt(np.finfo(float).eps)\n",
    "\n",
    "# Train-test partition and application of f(x) on both\n",
    "data = fried(2 * n, m)\n",
    "train = data.iloc[:n]\n",
    "test = data.iloc[n:]\n",
    "\n",
    "# Extract data elements from both train and test\n",
    "X = train.iloc[:, :m].values\n",
    "y = train['Y'].values\n",
    "XX = test.iloc[:, :m].values\n",
    "yy = test['Y'].values  # for score\n",
    "yytrue = test['Ytrue'].values  # for RMSE\n",
    "\n",
    "# Compute distance matrices\n",
    "D = dist(X)\n",
    "out = minimize(lambda par: nl(par, D, y), x0=[0.1, 0.1 * np.var(y)], jac=lambda par: gradnl(par, D, y),\n",
    "               method=\"L-BFGS-B\", bounds=[(eps, 10), (eps, np.var(y))])\n",
    "\n",
    "# Compute covariance matrices\n",
    "K = np.exp(-D / out.x[0]) + np.diag([out.x[1]] * n)\n",
    "Ki = inv(K)\n",
    "tau2hat = (y.T @ Ki @ y) / n\n",
    "\n",
    "DXX = dist(XX)\n",
    "KXX = np.exp(-DXX / out.x[0]) + np.diag([out.x[1]] * len(XX))\n",
    "DX = dist(XX, X)\n",
    "KX = np.exp(-DX / out.x[0])\n",
    "\n",
    "# Predictions\n",
    "mup = KX @ Ki @ y\n",
    "Sigmap = tau2hat * (KXX - KX @ Ki @ KX.T)\n",
    "\n",
    "# Calculation of metrics for GP by hand\n",
    "rmse = np.sqrt(np.mean((yytrue - mup) ** 2))\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from spotpython.gp.functions import fried\n",
    "from spotpython.gp.likelihood import nlsep, gradnlsep\n",
    "from spotpython.gp.distances import covar_anisotropic\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Parameters\n",
    "n = 100\n",
    "m = 7\n",
    "eps = np.sqrt(np.finfo(float).eps)\n",
    "\n",
    "# Train-test partition and application of f(x) on both\n",
    "data = fried(2 * n, m)\n",
    "train = data.iloc[:n]\n",
    "test = data.iloc[n:]\n",
    "\n",
    "# Extract data elements from both train and test\n",
    "X = train.iloc[:, :m].values\n",
    "y = train['Y'].values\n",
    "XX = test.iloc[:, :m].values\n",
    "yy = test['Y'].values  # for score\n",
    "yytrue = test['Ytrue'].values  # for RMSE\n",
    "\n",
    "# Optimize parameters\n",
    "outg = minimize(lambda par: nlsep(par, X, y), x0=np.concatenate([np.repeat(0.1, m), [0.1 * np.var(y)]]),\n",
    "                jac=lambda par: gradnlsep(par, X, y), method=\"L-BFGS-B\",\n",
    "                bounds=[(eps, 10)] * m + [(eps, np.var(y))])\n",
    "\n",
    "# Compute covariance matrices\n",
    "K = covar_anisotropic(X, d=outg.x[:m], g=outg.x[m])\n",
    "Ki = inv(K)\n",
    "tau2hat = (y.T @ Ki @ y) / len(X)\n",
    "\n",
    "KXX = covar_anisotropic(XX, d=outg.x[:m], g=outg.x[m])\n",
    "KX = covar_anisotropic(XX, X, d=outg.x[:m], g=0.0)\n",
    "mup2 = KX @ Ki @ y\n",
    "Sigmap2 = tau2hat * (KXX - KX @ Ki @ KX.T)\n",
    "\n",
    "# Calculation of metrics for separable GP by hand\n",
    "rmse = np.sqrt(np.mean((yytrue - mup2) ** 2))\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from numpy.linalg import inv\n",
    "from spotpython.gp.functions import fried\n",
    "from spotpython.gp.likelihood import nlsep, gradnlsep\n",
    "from spotpython.gp.distances import covar_anisotropic\n",
    "\n",
    "class GP:\n",
    "    def __init__(self, n=100, m=7):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.eps = np.sqrt(np.finfo(float).eps)\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Process model.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Training input matrix of shape (n, m).\n",
    "            y (np.ndarray): Training response vector of shape (n,).\n",
    "\n",
    "        Returns:\n",
    "            self: Fitted model.\n",
    "        \"\"\"\n",
    "        # Optimize parameters\n",
    "        outg = minimize(lambda par: nlsep(par, X, y), x0=np.concatenate([np.repeat(0.1, self.m), [0.1 * np.var(y)]]),\n",
    "                        jac=lambda par: gradnlsep(par, X, y), method=\"L-BFGS-B\",\n",
    "                        bounds=[(self.eps, 10)] * self.m + [(self.eps, np.var(y))])\n",
    "\n",
    "        # Compute covariance matrices\n",
    "        K = covar_anisotropic(X, d=outg.x[:self.m], g=outg.x[self.m])\n",
    "        Ki = inv(K)\n",
    "        tau2hat = (y.T @ Ki @ y) / len(X)\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.outg = outg\n",
    "        self.Ki = Ki\n",
    "        self.tau2hat = tau2hat\n",
    "        self.fitted = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, XX):\n",
    "        \"\"\"\n",
    "        Predict using the Gaussian Process model.\n",
    "\n",
    "        Args:\n",
    "            XX (np.ndarray): Test input matrix of shape (n_test, m).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Predicted mean (mup2) and covariance (Sigmap2).\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"The model must be fitted before calling predict.\")\n",
    "\n",
    "        KXX = covar_anisotropic(XX, d=self.outg.x[:self.m], g=self.outg.x[self.m])\n",
    "        KX = covar_anisotropic(XX, self.X, d=self.outg.x[:self.m], g=0.0)\n",
    "        mup2 = KX @ self.Ki @ self.y\n",
    "        Sigmap2 = self.tau2hat * (KXX - KX @ self.Ki @ KX.T)\n",
    "\n",
    "        return mup2, Sigmap2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# Parameters\n",
    "n = 100\n",
    "m = 7\n",
    "\n",
    "# Train-test partition and application of f(x) on both\n",
    "data = fried(2 * n, m)\n",
    "train = data.iloc[:n]\n",
    "test = data.iloc[n:]\n",
    "\n",
    "# Extract data elements from both train and test\n",
    "X = train.iloc[:, :m].values\n",
    "y = train['Y'].values\n",
    "XX = test.iloc[:, :m].values\n",
    "yy = test['Y'].values  # for score\n",
    "yytrue = test['Ytrue'].values  # for RMSE\n",
    "\n",
    "# Fit the model\n",
    "gp_model = GP(n=n, m=m)\n",
    "gp_model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "mup2, Sigmap2 = gp_model.predict(XX)\n",
    "\n",
    "# Calculation of metrics for separable GP by hand\n",
    "rmse = np.sqrt(np.mean((yytrue - mup2) ** 2))\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.gp.regressor import GPRegressor\n",
    "from spotpython.gp.functions import fried\n",
    "import numpy as np\n",
    "n = 100\n",
    "m = 7\n",
    "# Train-test partition and application of f(x) on both\n",
    "data = fried(2 * n, m)\n",
    "train = data.iloc[:n]\n",
    "test = data.iloc[n:]\n",
    "# Extract data elements from both train and test\n",
    "X = train.iloc[:, :m].values\n",
    "y = train['Y'].values\n",
    "XX = test.iloc[:, :m].values\n",
    "yy = test['Y'].values  # for score\n",
    "yytrue = test['Ytrue'].values  # for RMSE\n",
    "# Fit the model\n",
    "gp_model = GPRegressor()\n",
    "gp_model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "mup2, Sigmap2 = gp_model.predict(XX)\n",
    "\n",
    "# Calculation of metrics for separable GP by hand\n",
    "rmse = np.sqrt(np.mean((yytrue - mup2) ** 2))\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.design.spacefilling import SpaceFilling\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.build.kriging import Kriging\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "rng = np.random.RandomState(1)\n",
    "training_indices = rng.choice(np.arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gaussian_process.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"sk-learn Version: Gaussian process regression on noise-free dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = GPRegressor()\n",
    "S.fit(X_train, y_train)\n",
    "S_mean_prediction, S_std_prediction = S.predict(X)\n",
    "# compute the square root of the S_std_prediction diagonal to get the standard deviation\n",
    "S_std_prediction = np.sqrt(np.diag(S_std_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, S_mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    S_mean_prediction - 1.96 * S_std_prediction,\n",
    "    S_mean_prediction + 1.96 * S_std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"spotpython Version: Gaussian process regression on noise-free dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, S_mean_prediction, label=\"spotpython Mean prediction\")\n",
    "plt.plot(X, mean_prediction, label=\"Sklearn Mean Prediction\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Comparing Mean Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.gp.gp_sep import newGPsep\n",
    "G = newGPsep(X_train, y_train, d=2.0, g=1/10, dK=True)\n",
    "p = G.predict(X)\n",
    "mu = p[\"mean\"]\n",
    "Sigma = p[\"Sigma\"]\n",
    "std = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mu, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mu - 1.96 * std,\n",
    "    mu + 1.96 * std,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"spotpython Version: Gaussian process regression on noise-free dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import linalg_dposv, covar_sep_symm\n",
    "n = 3\n",
    "Mutil = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]], dtype=float)\n",
    "Mi = np.eye(n)\n",
    "info = linalg_dposv(n, Mutil, Mi)\n",
    "print(\"Info:\", info)\n",
    "print(\"Mi:\", Mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 2\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "n = 3\n",
    "d = np.array([1.0, 1.0])\n",
    "g = 0.1\n",
    "K = covar_sep_symm(col, X, n, d, g)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import newGPsep\n",
    "\n",
    "# Create a matrix X with values from 0 to 2*pi, length 7, and 1 column\n",
    "X = np.linspace(0, 2 * np.pi, 7).reshape(-1, 1)\n",
    "# Calculate the sine of each element in X\n",
    "Z = np.sin(X)\n",
    "gpsep = newGPsep(X, Z, 1, 0.001)\n",
    "print(gpsep.K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import newGPsep\n",
    "\n",
    "def f2d(x, y=None):\n",
    "    \"\"\"\n",
    "    Simple 2-d test function used in Gramacy & Apley (2015).\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): The x-coordinates.\n",
    "        y (ndarray, optional): The y-coordinates. If None, x is assumed to be a 2D array.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The calculated z-values.\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        if not isinstance(x, np.ndarray) or x.ndim != 2:\n",
    "            x = np.array(x).reshape(-1, 2)\n",
    "        y = x[:, 1]\n",
    "        x = x[:, 0]\n",
    "\n",
    "    def g(z):\n",
    "        return np.exp(-(z - 1) ** 2) + np.exp(-0.8 * (z + 1) ** 2) - 0.05 * np.sin(8 * (z + 0.1))\n",
    "\n",
    "    z = -g(x) * g(y)\n",
    "    return z\n",
    "\n",
    "# Design with N=441\n",
    "x = np.linspace(-2, 2, 11)\n",
    "X = np.array(np.meshgrid(x, x)).T.reshape(-1, 2)\n",
    "Z = f2d(X)\n",
    "\n",
    "# Fit a GP\n",
    "gpsep = newGPsep(X, Z, d=0.35, g=1/1000)\n",
    "print(gpsep.K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import newGPsep, predictGPsep\n",
    "\n",
    "def f2d(x, y=None):\n",
    "    \"\"\"\n",
    "    Simple 2-d test function used in Gramacy & Apley (2015).\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): The x-coordinates.\n",
    "        y (ndarray, optional): The y-coordinates. If None, x is assumed to be a 2D array.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The calculated z-values.\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        if not isinstance(x, np.ndarray) or x.ndim != 2:\n",
    "            x = np.array(x).reshape(-1, 2)\n",
    "        y = x[:, 1]\n",
    "        x = x[:, 0]\n",
    "\n",
    "    def g(z):\n",
    "        return np.exp(-(z - 1) ** 2) + np.exp(-0.8 * (z + 1) ** 2) - 0.05 * np.sin(8 * (z + 0.1))\n",
    "\n",
    "    z = -g(x) * g(y)\n",
    "    return z\n",
    "\n",
    "# Design with N=441\n",
    "x = np.linspace(-2, 2, 11)\n",
    "X = np.array(np.meshgrid(x, x)).T.reshape(-1, 2)\n",
    "Z = f2d(X)\n",
    "\n",
    "# Fit a GP\n",
    "gpsep = newGPsep(X, Z, d=0.35, g=1/1000)\n",
    "\n",
    "# Predictive grid with NN=400\n",
    "xx = np.linspace(-1.9, 1.9, 20)\n",
    "XX = np.array(np.meshgrid(xx, xx)).T.reshape(-1, 2)\n",
    "ZZ = f2d(XX)\n",
    "\n",
    "# Predict\n",
    "p = predictGPsep(gpsep, XX)\n",
    "\n",
    "# RMSE: compare to similar experiment in aGP docs\n",
    "rmse = np.sqrt(np.mean((p[\"mean\"] - ZZ) ** 2))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Visualize the result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(p[\"mean\"].reshape(len(xx), len(xx)), extent=(xx.min(), xx.max(), xx.min(), xx.max()), origin='lower', cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Predictive Mean\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow((p[\"mean\"] - ZZ).reshape(len(xx), len(xx)), extent=(xx.min(), xx.max(), xx.min(), xx.max()), origin='lower', cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Residuals\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ktKi <- t(k) %*% Ki\n",
    "ktKik <- ktKi %*% k\n",
    "mean <- ktKi %*% Z\n",
    "Sigma <- phi*(Sigma - ktKik)/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import newGPsep\n",
    "from spotpython.gp.functions import f2d\n",
    "import matplotlib.pyplot as plt\n",
    "# Design with N=441\n",
    "x = np.linspace(-2, 2, 11)\n",
    "X = np.array(np.meshgrid(x, x)).T.reshape(-1, 2)\n",
    "Z = f2d(X)\n",
    "# Fit a GP\n",
    "gpsep = newGPsep(X, Z, d=0.35, g=1/1000, dK=True)\n",
    "#\n",
    "gpsep.mleGPsep_main(tmin=[0.0, 0.0, 1/10000], tmax=[0.6, 0.6, 1/100], maxit=1000)\n",
    "# Predictive grid with NN=400\n",
    "xx = np.linspace(-1.9, 1.9, 20)\n",
    "XX = np.array(np.meshgrid(xx, xx)).T.reshape(-1, 2)\n",
    "ZZ = f2d(XX)\n",
    "# Predict\n",
    "p = gpsep.predict(XX)\n",
    "# RMSE: compare to similar experiment in aGP docs\n",
    "rmse = np.sqrt(np.mean((p[\"mean\"] - ZZ) ** 2))\n",
    "print(\"RMSE:\", rmse)\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(p[\"mean\"].reshape(len(xx), len(xx)), extent=(xx.min(), xx.max(), xx.min(), xx.max()), origin='lower', cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Predictive Mean\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow((p[\"mean\"] - ZZ).reshape(len(xx), len(xx)), extent=(xx.min(), xx.max(), xx.min(), xx.max()), origin='lower', cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import newGPsep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple sine data\n",
    "X = np.linspace(0, 2 * np.pi, 7).reshape(-1, 1)\n",
    "Z = np.sin(X)\n",
    "\n",
    "# New GP fit\n",
    "gpsep = newGPsep(X, Z, d=2, g=0.000001)\n",
    "\n",
    "# Make predictions\n",
    "XX = np.linspace(-1, 2 * np.pi + 1, 499).reshape(-1, 1)\n",
    "p = gpsep.predict(XX, lite=False)\n",
    "\n",
    "# Sample from the predictive distribution\n",
    "N = 100\n",
    "mean = p[\"mean\"]\n",
    "Sigma = p[\"Sigma\"]\n",
    "df = p[\"df\"]\n",
    "\n",
    "# Generate samples from the multivariate t-distribution\n",
    "ZZ = np.random.multivariate_normal(mean, Sigma, N)\n",
    "ZZ = ZZ.T\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(N):\n",
    "    plt.plot(XX, ZZ[:, i], color=\"gray\", linewidth=0.5)\n",
    "plt.scatter(X, Z, color=\"black\", s=50, zorder=5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f-hat(x)\")\n",
    "plt.title(\"Predictive Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import newGPsep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple sine data\n",
    "X = np.linspace(0, 2 * np.pi, 7).reshape(-1, 1)\n",
    "Z = np.sin(X)\n",
    "\n",
    "# New GP fit\n",
    "gpsep = newGPsep(X, Z, d=2, g=0.000001)\n",
    "\n",
    "# Make predictions\n",
    "XX = np.linspace(-1, 2 * np.pi + 1, 499).reshape(-1, 1)\n",
    "p = gpsep.predict(XX, lite=False)\n",
    "\n",
    "# Sample from the predictive distribution\n",
    "N = 100\n",
    "mean = p[\"mean\"]\n",
    "Sigma = p[\"Sigma\"]\n",
    "df = p[\"df\"]\n",
    "\n",
    "# Generate samples from the multivariate t-distribution\n",
    "ZZ = np.random.multivariate_normal(mean, Sigma, N)\n",
    "ZZ = ZZ.T\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(N):\n",
    "    plt.plot(XX, ZZ[:, i], color=\"gray\", linewidth=0.5)\n",
    "plt.scatter(X, Z, color=\"black\", s=50, zorder=5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f-hat(x)\")\n",
    "plt.title(\"Predictive Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import newGPsep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple sine data\n",
    "X = np.linspace(0, 2 * np.pi, 7).reshape(-1, 1)\n",
    "Z = np.sin(X)\n",
    "\n",
    "# New GP fit\n",
    "gpsep = newGPsep(X, Z, d=1, g=0.000001, dK=True)\n",
    "\n",
    "gpsep.mleGPsep_main(tmin=[0.9, 0.0000001], tmax=[1.1,  0.000002], maxit=1000)\n",
    "\n",
    "# Make predictions\n",
    "XX = np.linspace(-1, 2 * np.pi + 1, 499).reshape(-1, 1)\n",
    "p = gpsep.predict(XX, lite=False)\n",
    "\n",
    "# Sample from the predictive distribution\n",
    "N = 100\n",
    "mean = p[\"mean\"]\n",
    "Sigma = p[\"Sigma\"]\n",
    "df = p[\"df\"]\n",
    "\n",
    "# Generate samples from the multivariate t-distribution\n",
    "ZZ = np.random.multivariate_normal(mean, Sigma, N)\n",
    "ZZ = ZZ.T\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(N):\n",
    "    plt.plot(XX, ZZ[:, i], color=\"gray\", linewidth=0.5)\n",
    "plt.scatter(X, Z, color=\"black\", s=50, zorder=5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f-hat(x)\")\n",
    "plt.title(\"Predictive Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.likelihood import gradnlsep\n",
    "X = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "Y = np.array([1.0, 2.0, 3.0])\n",
    "par = np.array([0.5, 0.5, 0.1])\n",
    "grad = gradnlsep(par, X, Y)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spotpython.gp.gp_sep import GPsep\n",
    "\n",
    "# Get motorcycle dataset - using statsmodels or loading from CSV\n",
    "try:\n",
    "    # Try to access via statsmodels\n",
    "    import statsmodels.api as sm\n",
    "    mcycle = sm.datasets.get_rdataset(\"mcycle\", \"MASS\").data\n",
    "    X = mcycle.values[:, 0].reshape(-1, 1)  # First column as matrix\n",
    "    Z = mcycle.values[:, 1]                 # Second column as vector\n",
    "except (ImportError, ValueError):\n",
    "    # Fallback to a small sample of the motorcycle data\n",
    "    X = np.array([2.4, 5.0, 7.5, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0]).reshape(-1, 1)\n",
    "    Z = np.array([-50.0, -30.0, -37.5, -30.0, -20.0, -15.0, -12.5, -7.5, -11.0, -10.0])\n",
    "\n",
    "# Create GPsep instance and call methods\n",
    "gp = GPsep(X=X, Z=Z, m=X.shape[1], n=X.shape[0])\n",
    "\n",
    "# Process length-scale parameters equivalent to darg(NULL, X)\n",
    "d_result = gp.darg(None, X)\n",
    "print(\"Length-scale parameters:\")\n",
    "print(d_result)\n",
    "\n",
    "# Process nugget parameters equivalent to garg(list(mle=TRUE), Z)\n",
    "g_result = gp.garg({\"mle\": True}, Z)\n",
    "print(\"\\nNugget parameters:\")\n",
    "print(g_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.gp.gp_sep import GPsep\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "gp = GPsep(m=2, n=3, X=X)\n",
    "result = gp.getDs(p=0.1, samp_size=10000)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motorcycle Example with Fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from spotpython.gp.gp_sep import GPsep\n",
    "\n",
    "# Load the motorcycle dataset\n",
    "try:\n",
    "    # Try to access via statsmodels\n",
    "    import statsmodels.api as sm\n",
    "    mcycle = sm.datasets.get_rdataset(\"mcycle\", \"MASS\").data\n",
    "    X = mcycle.values[:, 0].reshape(-1, 1)  # First column as matrix\n",
    "    Z = mcycle.values[:, 1]                 # Second column as vector\n",
    "except (ImportError, ValueError):\n",
    "    # Fallback to a small sample of the motorcycle data\n",
    "    print(\"MASS package data not available, using approximation\")\n",
    "    X = np.array([2.4, 5.0, 7.5, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0]).reshape(-1, 1)\n",
    "    Z = np.array([-50.0, -30.0, -37.5, -30.0, -20.0, -15.0, -12.5, -7.5, -11.0, -10.0])\n",
    "\n",
    "gpsep = GPsep(nlsep_method=\"chol\", gradnlsep_method=\"chol\", n_restarts_optimizer=10,  auto_optimize=True, verbosity=0, max_points=None, g=1/1000)\n",
    "gpsep.fit(X, Z)\n",
    "\n",
    "# Optional: Plot the fitted model\n",
    "XX = np.linspace(min(X), max(X), 200).reshape(-1, 1)\n",
    "pred = gpsep.predict(XX, return_full=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, Z, color='black', s=20, label='Data')\n",
    "plt.plot(XX.flatten(), pred['mean'], color='blue', lw=2, label='GP mean')\n",
    "\n",
    "# Add uncertainty bands (2 standard deviations)\n",
    "std = np.sqrt(np.diag(pred['Sigma']))\n",
    "plt.fill_between(XX.flatten(), \n",
    "                 pred['mean'] - 2*std,\n",
    "                 pred['mean'] + 2*std, \n",
    "                 alpha=0.3, color='blue', label='95% CI')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Acceleration (g)')\n",
    "plt.title('Motorcycle Data - Gaussian Process Regression')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sine Example with Fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import GPsep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple sine data\n",
    "X = np.linspace(0, 2 * np.pi, 7).reshape(-1, 1)\n",
    "Z = np.sin(X)\n",
    "\n",
    "# New GP fit\n",
    "gpsep = GPsep(nlsep_method=\"chol\", gradnlsep_method=\"inv\", auto_optimize=True, verbosity=0)\n",
    "# gpsep.fit(X, Z, dK=True, auto_optimize=False, verbosity=1, d=3.5, g=1e-6)\n",
    "gpsep.fit(X, Z, dK=True)\n",
    "\n",
    "# Make predictions\n",
    "XX = np.linspace(-1, 2 * np.pi + 1, 499).reshape(-1, 1)\n",
    "p = gpsep.predict(XX, lite=False, return_full=True)\n",
    "\n",
    "# Sample from the predictive distribution\n",
    "N = 100\n",
    "mean = p[\"mean\"]\n",
    "Sigma = p[\"Sigma\"]\n",
    "df = p[\"df\"]\n",
    "\n",
    "\n",
    "# Generate samples from the multivariate t-distribution\n",
    "ZZ = np.random.multivariate_normal(mean, Sigma, N)\n",
    "ZZ = ZZ.T\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(N):\n",
    "    plt.plot(XX, ZZ[:, i], color=\"gray\", linewidth=0.5)\n",
    "plt.scatter(X, Z, color=\"black\", s=50, zorder=5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f-hat(x)\")\n",
    "plt.title(\"Predictive Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gramacy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import GPsep\n",
    "from spotpython.gp.functions import f2d\n",
    "import matplotlib.pyplot as plt\n",
    "# Design with N=441\n",
    "x = np.linspace(-2, 2, 11)\n",
    "X = np.array(np.meshgrid(x, x)).T.reshape(-1, 2)\n",
    "Z = f2d(X)\n",
    "# Fit a GP\n",
    "# gpsep = newGPsep(X, Z, d=0.35, g=1/1000, dK=True)\n",
    "#\n",
    "# gpsep.mleGPsep_main(tmin=[0.0, 0.0, 1/10000], tmax=[0.6, 0.6, 1/100], maxit=1000)\n",
    "gpsep = GPsep(nlsep_method=\"chol\", gradnlsep_method=\"chol\", n_restarts_optimizer=25, auto_optimize=True, verbosity=0)\n",
    "# gpsep.fit(X, Z, dK=True, auto_optimize=True, verbosity=1, d=3.5, g=1e-6)\n",
    "gpsep.fit(X, Z, dK=True)\n",
    "\n",
    "# Predictive grid with NN=400\n",
    "xx = np.linspace(-1.9, 1.9, 20)\n",
    "XX = np.array(np.meshgrid(xx, xx)).T.reshape(-1, 2)\n",
    "ZZ = f2d(XX)\n",
    "# Predict\n",
    "p = gpsep.predict(XX, return_full=True)\n",
    "# RMSE: compare to similar experiment in aGP docs\n",
    "rmse = np.sqrt(np.mean((p[\"mean\"] - ZZ) ** 2))\n",
    "print(\"RMSE:\", rmse)\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(p[\"mean\"].reshape(len(xx), len(xx)), extent=(xx.min(), xx.max(), xx.min(), xx.max()), origin='lower', cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Predictive Mean\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow((p[\"mean\"] - ZZ).reshape(len(xx), len(xx)), extent=(xx.min(), xx.max(), xx.min(), xx.max()), origin='lower', cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TGP Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import GPsep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create synthetic data with different behaviors in different regions\n",
    "# np.random.seed(42)\n",
    "# X = np.sort(np.random.uniform(-10, 10, 200)).reshape(-1, 1)\n",
    "# y = np.sin(X[:, 0]) * np.exp(-0.1 * np.abs(X[:, 0]))\n",
    "# y += 0.1 * np.random.randn(len(y))  # Add noise\n",
    "\n",
    "import statsmodels.api as sm\n",
    "mcycle = sm.datasets.get_rdataset(\"mcycle\", \"MASS\").data\n",
    "X = mcycle.values[:, 0].reshape(-1, 1)  # First column as matrix\n",
    "y = mcycle.values[:, 1]                 # Second column as vector\n",
    "\n",
    "# Fit TreeGP model with regression tree partitioning\n",
    "model = GPsep(nlsep_method=\"chol\", gradnlsep_method=\"chol\", n_restarts_optimizer=9, auto_optimize=True, verbosity=0, max_points=90)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "X_test = np.linspace(min(X), max(X), 200).reshape(-1, 1)\n",
    "\n",
    "y_pred, Sigma = model.predict(X_test, return_std=True)\n",
    "\n",
    "y_std = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='black', label='Training points')\n",
    "plt.plot(X_test, y_pred, 'r-', label='TreeGP prediction')\n",
    "plt.fill_between(\n",
    "    X_test.ravel(), \n",
    "    y_pred - 1.96 * y_std, \n",
    "    y_pred + 1.96 * y_std, \n",
    "    color='red', \n",
    "    alpha=0.2, \n",
    "    label='95% confidence interval'\n",
    ")\n",
    "    \n",
    "plt.legend()\n",
    "plt.title('GP Models')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "# plt.ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import GPsep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create synthetic data with different behaviors in different regions\n",
    "np.random.seed(42)\n",
    "X = np.sort(np.random.uniform(-10, 10, 200)).reshape(-1, 1)\n",
    "\n",
    "# Define the piecewise function:\n",
    "# - Linear for x in [-10, 0]\n",
    "# - sin(x) * exp(-0.1*|x|) for x in [0, 10]\n",
    "def piecewise_function(x):\n",
    "    result = np.zeros_like(x)\n",
    "    \n",
    "    # Linear part for x  0 (ax + b where a is the slope and b=0 since we pass through origin)\n",
    "    mask_negative = x <= 0\n",
    "    # Using slope of -0.5 (you can adjust this value as needed)\n",
    "    result[mask_negative] = 0.05 * x[mask_negative]\n",
    "    \n",
    "    # Original sin/exp part for x > 0\n",
    "    mask_positive = x > 0\n",
    "    result[mask_positive] = np.sin(x[mask_positive]) * np.exp(-0.1 * np.abs(x[mask_positive]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply the function to the data\n",
    "y = piecewise_function(X[:, 0])\n",
    "\n",
    "# Add some noise\n",
    "y += 0.01 * np.random.randn(len(y))\n",
    "\n",
    "\n",
    "# Fit TreeGP model with regression tree partitioning\n",
    "model = GPsep(nlsep_method=\"chol\", gradnlsep_method=\"chol\", n_restarts_optimizer=25, auto_optimize=True, verbosity=0, max_points=30, seed=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "X_test = np.linspace(-12, 12, 1000).reshape(-1, 1)\n",
    "y_pred, Sigma = model.predict(X_test, return_std=True)\n",
    "\n",
    "y_std = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='black', label='Training points')\n",
    "plt.plot(X_test, y_pred, 'r-', label='TreeGP prediction')\n",
    "plt.fill_between(\n",
    "    X_test.ravel(), \n",
    "    y_pred - 1.96 * y_std, \n",
    "    y_pred + 1.96 * y_std, \n",
    "    color='red', \n",
    "    alpha=0.2, \n",
    "    label='95% confidence interval'\n",
    ")\n",
    "    \n",
    "plt.legend()\n",
    "plt.title('Gaussian Process Regression With Piecewise Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.design.spacefilling import SpaceFilling\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.build.kriging import Kriging\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "rng = np.random.RandomState(1)\n",
    "training_indices = rng.choice(np.arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gaussian_process.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"sk-learn Version: Gaussian process regression on noise-free dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPSep on SKLearn Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.gp.gp_sep import GPsep\n",
    "gpsep = GPsep(nlsep_method=\"chol\", gradnlsep_method=\"chol\", n_restarts_optimizer=9, auto_optimize=True, verbosity=0, max_points=7, seed=1)\n",
    "gpsep.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction = gpsep.predict(X, return_std=True)\n",
    "# take the square root of the diagonal of the covariance matrix to get the standard deviation\n",
    "std_prediction = np.sqrt(np.diag(std_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"GPsep Version: Gaussian process regression on noise-free dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import GPsep\n",
    "from spotpython.plot.contour import plotModel\n",
    "\n",
    "# Create and fit a GPsep model\n",
    "X = np.random.rand(30, 3)\n",
    "y = np.sum(X**2, axis=1)\n",
    "model = GPsep().fit(X, y)\n",
    "\n",
    "# Plot the first two dimensions\n",
    "plotModel(\n",
    "    model=model,\n",
    "    lower=np.zeros(3),\n",
    "    upper=np.ones(3),\n",
    "    i=0,\n",
    "    j=1,\n",
    "    # var_name=[\"x\", \"y\", \"z\"],\n",
    "    title=\"Response Surface\",\n",
    "    aspect_equal=True,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.gp.gp_sep import GPsep\n",
    "from spotpython.plot.contour import plotCombinations\n",
    "\n",
    "# Create and fit a model\n",
    "X = np.random.rand(30, 5)\n",
    "y = np.sum(X**2, axis=1)\n",
    "model = GPsep().fit(X, y)\n",
    "\n",
    "# Plot with automatic bound detection\n",
    "plotCombinations(\n",
    "model=model,\n",
    "X=X,\n",
    "X_points=X,\n",
    "y_points=y,\n",
    "x_vars=[0],\n",
    "y_vars=[],\n",
    "use_max=False,\n",
    "use_min=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with explicit bounds\n",
    "plotCombinations(\n",
    "model=model,\n",
    "X_points=X,\n",
    "y_points=y,\n",
    "lower=np.zeros(5),\n",
    "upper=np.ones(5),\n",
    "use_max=False,\n",
    "use_min=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screening Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### python\n",
    "# filepath: /Users/bartz/workspace/spotPython/tests/test_screening_plot.py\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from spotpython.utils.effects import screening\n",
    "\n",
    "def sphere_fn(x: np.ndarray) -> float:\n",
    "    \"\"\"A simple test objective function: sum of squares.\"\"\"\n",
    "    return float(np.sum(x**2))\n",
    "\n",
    "# Small screening plan with 2 variables and 3 points (1 path, k=2 => (k+1)=3)\n",
    "X = np.array([\n",
    "[0,   0],\n",
    "[0.5, 0],\n",
    "[0.5, 0.5],\n",
    "])\n",
    "\n",
    "# Lower and upper bounds\n",
    "value_range = np.array([[0, 0], [1, 1]])\n",
    "xi = 1.0\n",
    "p = 5\n",
    "labels = [\"x1\", \"x2\"]\n",
    "\n",
    "# Run the screening plot function; if it finishes without error, the test passes.\n",
    "screening(X=X, fun=sphere_fn, xi=1, p=5, labels=labels, bounds=value_range, print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "\n",
    "\n",
    "def test_fun_wingwt():\n",
    "    \"\"\"Test the fun_wingwt method from the Analytical class.\"\"\"\n",
    "    # Create a small test input with shape (n, 10)\n",
    "    X_test = np.array([\n",
    "        [0.0]*10,\n",
    "        [1.0]*10\n",
    "    ])\n",
    "    fun = Analytical()\n",
    "    result = fun.fun_wingwt(X_test)\n",
    "\n",
    "    # Check shape of the output\n",
    "    assert result.shape == (2,), f\"Expected output shape (2,), got {result.shape}\"\n",
    "\n",
    "    # Simple check that values are not NaN or inf\n",
    "    assert np.all(np.isfinite(result)), \"Output contains non-finite values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.effects import screening, screeningplan\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "fun = Analytical()\n",
    "k = 10\n",
    "p = 10\n",
    "xi = 1\n",
    "r = 25\n",
    "X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n",
    "# Provide real-world bounds from the wing weight docs (2 x 10).\n",
    "value_range = np.array([\n",
    "    [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n",
    "    [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n",
    "])\n",
    "labels = [\n",
    "    \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n",
    "    \"q\",   \"lambda\", \"tc\", \"N_z\",\n",
    "    \"W_dg\", \"W_p\"\n",
    "]\n",
    "screening(\n",
    "    X=X,\n",
    "    fun=fun.fun_wingwt,\n",
    "    bounds=value_range,\n",
    "    xi=xi,\n",
    "    p=p,\n",
    "    labels=labels,\n",
    "    print=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.effects import screeningplan, screening\n",
    "from typing import Optional, Dict\n",
    "\n",
    "class UserAnalytical(Analytical):\n",
    "    \n",
    "        \n",
    "    def fun_function(self, X: np.ndarray, fun_control: Optional[Dict] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Custom new function: f(x) = x^4\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Input data as a 2D array.\n",
    "            fun_control (Optional[Dict]): Control parameters for the function.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Computed values with optional noise.\n",
    "        \n",
    "        Examples:\n",
    "            import numpy as np\n",
    "            X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "            fun = Analytical()\n",
    "            fun.fun_linear(X)\n",
    "        \"\"\"\n",
    "        X = self._prepare_input_data(X, fun_control)\n",
    "     \n",
    "        offset = np.ones(X.shape[1]) * self.offset\n",
    "        y = np.sum((X - offset) **4, axis=1) \n",
    "\n",
    "        # Add noise if specified in fun_control\n",
    "        return self._add_noise(y)\n",
    "\n",
    "# Without offset and without noise\n",
    "user_fun = Analytical(fun_control={\"alpha\": 2.0, \"beta\": [1.0, 2.0, 3.0]})\n",
    "X = np.array([[0, 0, 0], [1, 1, 1]])\n",
    "results = fun.fun_linear(X)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "# Without offset, noise, intercept (alpha) and weights (beta)\n",
    "fun = Analytical()\n",
    "X = np.array([[0, 0, 0], [1, 1, 1], [-1, -1, -1], [1, 2, 3]])\n",
    "results = fun.fun_linear(X)\n",
    "# We get the sum of the input values\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# With offset and without noise\n",
    "fun = Analytical(offset=1.0)\n",
    "X = np.array([[0, 0, 0], [1, 1, 1], [-1, -1, -1], [1, 2, 3]])\n",
    "results = fun.fun_linear(X)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With offset and noise\n",
    "fun = Analytical(offset=1.0, sigma=0.1, seed=1)\n",
    "X = np.array([[0, 0, 0], [1, 1, 1], [-1, -1, -1], [1, 2, 3]])\n",
    "results = fun.fun_linear(X)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide alpha (intercept), no beta\n",
    "fun_control = {\"alpha\": 10.0}\n",
    "fun = Analytical(fun_control=fun_control)\n",
    "X = np.array([[0, 0, 0], [1, 1, 1], [-1, -1, -1], [1, 2, 3]])\n",
    "results=fun.fun_linear(X)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide alpha (intercept) and beta (must be 3-dimensional)\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "fun_control = {\"alpha\": 10.0, \"beta\": [1.0, 2.0, 3.0]}\n",
    "fun = Analytical(fun_control=fun_control)\n",
    "X = np.array([[0, 0, 0], [1, 1, 1], [-1, -1, -1], [1, 2, 3]])\n",
    "results=fun.fun_linear(X)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a small test input with shape (n, 10)\n",
    "X_test = np.array([[0.0]*10,[1.0]*10])\n",
    "fun = Analytical()\n",
    "labels = [\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\", \"x10\"]\n",
    "result = screening(X_test, fun.fun_wingwt, np.array([[0]*10, [1]*10]), 0.1, 3, labels)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.sampling import bestlh\n",
    "bestlh(n=5, k=2, population=5, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.sampling import phisort\n",
    "X1 = bestlh(n=5, k=2, population=5, iterations=10)\n",
    "X2 = bestlh(n=5, k=2, population=15, iterations=20)\n",
    "X3 = bestlh(n=5, k=2, population=25, iterations=30)\n",
    "# Map X1 and X2 so that X3D has the two sampling plans in X3D[:, :, 0] and X3D[:, :, 1]\n",
    "X3D = np.array([X1, X2])\n",
    "print(phisort(X3D))\n",
    "X3D = np.array([X3, X2])\n",
    "print(phisort(X3D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.sampling import subset, bestlh\n",
    "X = bestlh(n=5, k=3, population=5, iterations=10)\n",
    "Xs, Xr = subset(X, ns=2)\n",
    "print(Xs)\n",
    "print(Xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.sampling import bestlh, mmlhs\n",
    "X_start = bestlh(n=10, k=5, population=2, iterations=3)\n",
    "print(f\"Initial plan:\\n{X_start}\")\n",
    "X_opt = mmlhs(X_start, population=5, iterations=5, q=2, plot=True)\n",
    "print(f\"Optimized plan:\\n{X_opt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.sampling import perturb\n",
    "# Create a simple 4x2 sampling plan\n",
    "X_original = np.array([\n",
    "[1, 3],\n",
    "[2, 4],\n",
    "[3, 1],\n",
    "[4, 2]])\n",
    "# Perturb it once\n",
    "X_perturbed = perturb(X_original, PertNum=1)\n",
    "print(X_perturbed)\n",
    "# The output may differ due to random swaps, but each column is still a permutation of [1,2,3,4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.sampling import mmsort\n",
    "# Suppose we have two 3-point sampling plans in 2D, stored in X3D:\n",
    "X1 = np.array([[0.0, 0.0],\n",
    "           [0.5, 0.5],\n",
    "           [1.0, 1.0]])\n",
    "X2 = np.array([[0.2, 0.2],\n",
    "           [0.25, 0.25],\n",
    "           [0.9, 0.9]])\n",
    "# Stack them along the third dimension: shape will be (3, 2, 2)\n",
    "X3D = np.stack([X1, X2], axis=2)\n",
    "# Sort them using the Morris-Mitchell criterion with p=2\n",
    "ranking = mmsort(X3D, p=2.0)\n",
    "print(ranking)\n",
    "# It might print [2 1] or [1 2], depending on which plan is more space-filling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.sampling import mmphi\n",
    "# Simple 3-point sampling plan in 2D\n",
    "X = np.array([\n",
    "     [0.0, 0.0],\n",
    "     [0.5, 0.5],\n",
    "     [1.0, 1.0]\n",
    " ])\n",
    "# Calculate the space-fillingness metric with q=2, using Euclidean distances (p=2)\n",
    "quality = mmphi(X, q=2, p=2)\n",
    "print(quality)\n",
    "# This value indicates how well points are spread out, with higher being better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.sampling import mm\n",
    "# Create two 3-point sampling plans in 2D\n",
    "X1 = np.array([[0.0, 0.0],\n",
    "[0.5, 0.5],\n",
    "[0.0, 1.0]])\n",
    "X2 = np.array([[0.1, 0.1],\n",
    "[0.4, 0.6],\n",
    "[0.1, 0.9]])\n",
    "# Compare which plan has better space-filling (Morris-Mitchell)\n",
    "better = mm(X1, X2, p=2.0)\n",
    "print(better)\n",
    "# Prints either 0, 1, or 2 depending on which plan is more space-filling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.sampling import jd\n",
    "# A small 3-point set in 2D\n",
    "X = np.array([[0.0, 0.0],\n",
    "[1.0, 1.0],\n",
    "[2.0, 2.0]])\n",
    "J, distinct_d = jd(X, p=2.0)\n",
    "print(\"Distinct distances:\", distinct_d)\n",
    "print(\"Occurrences:\", J)\n",
    "# Possible output (using Euclidean norm):\n",
    "# Distinct distances: [1.41421356 2.82842712]\n",
    "# Occurrences: [1 1]\n",
    "# Explanation: Distances are sqrt(2) between consecutive points and 2*sqrt(2) for the farthest pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.sampling import rlh\n",
    "# Generate a 2D Latin hypercube with 5 points and edges=0\n",
    "X = rlh(n=5, k=2, edges=0)\n",
    "print(X)\n",
    "# Example output (values vary due to randomness):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.sampling import fullfactorial\n",
    "q = [3, 2]\n",
    "X = fullfactorial(q, Edges=0)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kriging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.surrogate.kriging import Kriging\n",
    "# Training data\n",
    "X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n",
    "y_train = np.array([0.1, 0.2, 0.3])\n",
    "# Fit the Kriging model\n",
    "model = Kriging().fit(X_train, y_train)\n",
    "# Test data\n",
    "X_test = np.array([[0.25, 0.25], [0.75, 0.75]])\n",
    "# Predict responses\n",
    "y_pred,s  = model.predict(X_test, return_std=True)\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(\"Standard deviations:\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import Spot\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from spotpython.surrogate.kriging import Kriging\n",
    "X = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "rng = np.random.RandomState(1)\n",
    "training_indices = rng.choice(np.arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Kriging model\n",
    "model = Kriging(method=\"reinterpolation\")\n",
    "print(model.get_params())\n",
    "print(model.get_model_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "# show the model parameters\n",
    "print(model.get_params())\n",
    "print(model.get_model_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict responses\n",
    "mean_prediction, std_prediction = model.predict(X, return_std=True)\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"sk-learn Version: Gaussian process regression on noise-free dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict responses\n",
    "mean_prediction, std_prediction, ei = model.predict(X, return_std=True, return_ei=True)\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.plot(X, 10**(-ei), label=\"Expected improvement\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"sk-learn Version: Gaussian process regression on noise-free dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict responses\n",
    "mean_prediction, std_prediction, ei = model.predict(X, return_std=True, return_ei=True)\n",
    "plt.plot(X, 10**(-ei), label=\"Expected improvement\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"sk-learn Version: Gaussian process regression on noise-free dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airfoilcd Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.surrogate.functions.forr08a import aerofoilcd\n",
    "import numpy as np\n",
    "from spotpython.spot import Spot\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from spotpython.surrogate.kriging import Kriging\n",
    "X = np.linspace(start=0.0, stop=1, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(aerofoilcd(X))\n",
    "X_train = np.array([[0.0], [0.5], [0.7], [0.9], [1.0]])\n",
    "y_train = aerofoilcd(X_train)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Kriging model\n",
    "model = Kriging(method=\"interpolation\")\n",
    "print(model.get_params())\n",
    "print(model.get_model_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "# show the model parameters\n",
    "print(model.get_params())\n",
    "print(model.get_model_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict responses\n",
    "mean_prediction, std_prediction = model.predict(X, return_std=True)\n",
    "plt.plot(X, y, label=r\"$f(x) = aerofoilcd(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"sk-learn Version: Gaussian process regression on noisy dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict responses\n",
    "mean_prediction, std_prediction, ei = model.predict(X, return_std=True, return_ei=True)\n",
    "plt.plot(X, 10**(-ei), label=\"Expected improvement\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"sk-learn Version: Gaussian process regression on noisy dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d plots predictions and error using branin function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.surrogate.functions.forr08a import branin\n",
    "# generate a 2D grid\n",
    "X = np.linspace(0, 1, 100)\n",
    "Y = np.linspace(0, 1, 100)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "# compute the function values\n",
    "Z = branin(np.array([X.ravel(), Y.ravel()]).T)\n",
    "# plot the function\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z.reshape(X.shape), cmap='viridis')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.title('Branin Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.surrogate.kriging import Kriging, plot1d, plot2d\n",
    "from spotpython.surrogate.functions.forr08a import branin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x1 = np.linspace(0, 1, 5)\n",
    "x2 = np.linspace(0, 1, 5)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "# compute the function values\n",
    "Y = branin(np.array([X1.ravel(), X2.ravel()]).T)\n",
    "print(Y.shape)\n",
    "\n",
    "x1_min = x1.min()\n",
    "x1_max = x1.max()\n",
    "x2_min = x2.min()\n",
    "x2_max = x2.max()\n",
    "\n",
    "# create a 2-d np.array with the input values\n",
    "X = np.array([X1.ravel(), X2.ravel()]).T\n",
    "# create a 1-d np.array with the output values\n",
    "y = Y.ravel()\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Initialize and train the Kriging model\n",
    "S = Kriging()\n",
    "S.fit(X, y)\n",
    "\n",
    "# Predict on the original input data\n",
    "mean_prediction, std_prediction = S.predict(X, return_std=True)\n",
    "plot2d(model=S, X=X, y=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.sampling import bestlh\n",
    "from spotpython.surrogate.kriging import Kriging,plotkd\n",
    "from spotpython.surrogate.functions.forr08a import branin\n",
    "X = bestlh(n=25, k=2, population=5, iterations=10)\n",
    "y = branin(X)\n",
    "S = Kriging()\n",
    "S.fit(X, y)\n",
    "\n",
    "# Predict on the original input data\n",
    "mean_prediction, std_prediction = S.predict(X, return_std=True)\n",
    "plotkd(model=S, X=X, y=y, i=0, j=1, eps=1e-3, var_names=[\"X1\", \"X2\", \"Z\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.surrogate.kriging import Kriging, plotkd\n",
    "from spotpython.surrogate.functions.forr08a import branin\n",
    "import numpy as np\n",
    "# Example: 3D input data\n",
    "X_train = np.random.rand(10, 3)*10  # 3D input data\n",
    "y_train = np.sin(X_train[:, 0]) + np.cos(X_train[:, 1]) + X_train[:, 2]\n",
    "\n",
    "# Fit the Kriging model\n",
    "model = Kriging()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot the Kriging surrogate for dimensions 0 and 1\n",
    "plotkd(model, X_train, y_train, i=0, j=2, eps=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot predicted vs true values\n",
    "plt.figure()\n",
    "plt.plot(mean_prediction, Y, 'o')\n",
    "plt.plot([min(Y), max(Y)], [min(Y), max(Y)] , 'r--')\n",
    "plt.ylabel('Actual values')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.title('Kriging predictions. 2d input')\n",
    "plt.show()\n",
    "\n",
    "# generate a 3D plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], Y, c='r', marker='o')\n",
    "ax.scatter(X[:, 0], X[:, 1], mean_prediction, c='b', marker='x')\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_zlabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| echo: true\n",
    "#| eval: false\n",
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace, arange\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "df = df_original.copy()\n",
    "\n",
    "x1 = df[get_x_names()[0]]\n",
    "x2 = df[get_x_names()[1]]\n",
    "X = np.array([x1, x2]).T\n",
    "y = df[get_z_names()[0]]\n",
    "# convert to numpy array\n",
    "# X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "x1_min = x1.min()\n",
    "x1_max = x1.max()\n",
    "x2_min = x2.min()\n",
    "x2_max = x2.max()\n",
    "\n",
    "\n",
    "# Select training indices\n",
    "training_indices = rng.choice(arange(y.size), size=250, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "\n",
    "# selct test indices\n",
    "test_indices = np.setdiff1d(arange(y.size), training_indices)\n",
    "X_test, y_test = X[test_indices], y[test_indices]\n",
    "\n",
    "# Initialize and train the Kriging model\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the original input data\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X_test, return_val=\"all\")\n",
    "\n",
    "# plot predicted vs true values\n",
    "plt.figure()\n",
    "plt.plot(mean_prediction, y_test, 'o')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)] , 'r--')\n",
    "plt.ylabel('Actual values')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.title('Kriging predictions. 2d input')\n",
    "plt.show()\n",
    "\n",
    "# generate a 3D plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], y_test, c='r', marker='o')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], mean_prediction, c='b', marker='x')\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_zlabel('Y')\n",
    "plt.show()\n",
    "\n",
    "# generate a 3D surface plot\n",
    "S.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desirability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.desirability import DOverall, DMax, DCategorical\n",
    "import numpy as np\n",
    "dMax_obj = DMax(low=0, high=10, missing=0.5)\n",
    "dMax_obj.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for DCategorical\n",
    "dCategorical_obj = DCategorical(missing=0.2, values={\"A\": 0.8, \"B\": 0.6, \"C\": 0.4})\n",
    "dCategorical_obj.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from spotpython.utils.desirability import DMax, DMin, DTarget\n",
    "\n",
    "\n",
    "# Create a 1x3 grid of subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# (a) Plot for DMax\n",
    "axes[0].set_title(\"(a)\")\n",
    "dmax1 = DMax(1, 3)\n",
    "\n",
    "\n",
    "dmax2 = DMax(1, 3, scale=5)\n",
    "dmax3 = DMax(1, 3, scale=1/5)\n",
    "\n",
    "# Plot DMax with scale=1\n",
    "x = np.linspace(dmax1.low, dmax1.high, 100)\n",
    "y = dmax1.predict(x)\n",
    "axes[0].plot(x, y, label=\"scale = 1\", color=\"black\")\n",
    "\n",
    "# Plot DMax with scale=5\n",
    "y2 = dmax2.predict(x)\n",
    "axes[0].plot(x, y2, label=\"scale = 5\", color=\"red\")\n",
    "axes[0].text(2.73, 0.3, \"scale = 5\", color=\"red\")\n",
    "\n",
    "# Plot DMax with scale=0.2\n",
    "y3 = dmax3.predict(x)\n",
    "axes[0].plot(x, y3, label=\"scale = 0.2\", color=\"blue\")\n",
    "axes[0].text(1.3, 0.8, \"scale = 0.2\", color=\"blue\")\n",
    "\n",
    "axes[0].set_xlabel(\"Input\")\n",
    "axes[0].set_ylabel(\"Desirability\")\n",
    "axes[0].legend()\n",
    "\n",
    "# (b) Plot for DMin\n",
    "axes[1].set_title(\"(b)\")\n",
    "dmin1 = DMin(1, 3)\n",
    "dmin2 = DMin(1, 3, scale=5)\n",
    "dmin3 = DMin(1, 3, scale=1/5)\n",
    "\n",
    "# Plot DMin with scale=1\n",
    "y = dmin1.predict(x)\n",
    "axes[1].plot(x, y, label=\"scale = 1\", color=\"black\")\n",
    "\n",
    "# Plot DMin with scale=5\n",
    "y2 = dmin2.predict(x)\n",
    "axes[1].plot(x, y2, label=\"scale = 5\", color=\"red\")\n",
    "axes[1].text(1.5, 0.1, \"scale = 5\", color=\"red\")\n",
    "\n",
    "# Plot DMin with scale=0.2\n",
    "y3 = dmin3.predict(x)\n",
    "axes[1].plot(x, y3, label=\"scale = 0.2\", color=\"blue\")\n",
    "axes[1].text(1.5, 1, \"scale = 0.2\", color=\"blue\")\n",
    "\n",
    "axes[1].set_xlabel(\"Input\")\n",
    "axes[1].set_ylabel(\"Desirability\")\n",
    "axes[1].legend()\n",
    "\n",
    "# (c) Plot for DTarget\n",
    "axes[2].set_title(\"(c)\")\n",
    "dtarget1 = DTarget(1, 2, 3)\n",
    "dtarget2 = DTarget(1, 2, 3, low_scale=5)\n",
    "dtarget3 = DTarget(1, 2, 3, low_scale=1/5)\n",
    "\n",
    "# Plot DTarget with low_scale=1\n",
    "y = dtarget1.predict(x)\n",
    "axes[2].plot(x, y, label=\"lowScale = 1\", color=\"black\")\n",
    "\n",
    "# Plot DTarget with low_scale=5\n",
    "y2 = dtarget2.predict(x)\n",
    "axes[2].plot(x, y2, label=\"lowScale = 5\", color=\"red\")\n",
    "axes[2].text(1.9, 0.1, \"lowScale = 5\", color=\"red\")\n",
    "\n",
    "# Plot DTarget with low_scale=0.2\n",
    "y3 = dtarget3.predict(x)\n",
    "axes[2].plot(x, y3, label=\"lowScale = 0.2\", color=\"blue\")\n",
    "axes[2].text(1.3, 0.9, \"lowScale = 0.2\", color=\"blue\")\n",
    "\n",
    "axes[2].set_xlabel(\"Input\")\n",
    "axes[2].set_ylabel(\"Desirability\")\n",
    "axes[2].legend()\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def conversion_pred(x):\n",
    "    return (\n",
    "        81.09\n",
    "        + 1.0284 * x[0]\n",
    "        + 4.043 * x[1]\n",
    "        + 6.2037 * x[2]\n",
    "        - 1.8366 * x[0]**2\n",
    "        + 2.9382 * x[1]**2\n",
    "        - 5.1915 * x[2]**2\n",
    "        + 2.2150 * x[0] * x[1]\n",
    "        + 11.375 * x[0] * x[2]\n",
    "        - 3.875 * x[1] * x[2]\n",
    "    )\n",
    "\n",
    "def activity_pred(x):\n",
    "    return (\n",
    "        59.85\n",
    "        + 3.583 * x[0]\n",
    "        + 0.2546 * x[1]\n",
    "        + 2.2298 * x[2]\n",
    "        + 0.83479 * x[0]**2\n",
    "        + 0.07484 * x[1]**2\n",
    "        + 0.05716 * x[2]**2\n",
    "        - 0.3875 * x[0] * x[1]\n",
    "        - 0.375 * x[0] * x[2]\n",
    "        + 0.3125 * x[1] * x[2]\n",
    "    )\n",
    "\n",
    "# Create the grid\n",
    "time = np.linspace(-1.7, 1.7, 50)\n",
    "temperature = np.linspace(-1.7, 1.7, 4)\n",
    "catalyst = np.linspace(-1.7, 1.7, 50)\n",
    "\n",
    "# Create a DataFrame with all combinations of time, temperature, and catalyst\n",
    "plot_grid = pd.DataFrame(\n",
    "    np.array(np.meshgrid(time, temperature, catalyst)).T.reshape(-1, 3),\n",
    "    columns=[\"time\", \"temperature\", \"catalyst\"]\n",
    ")\n",
    "\n",
    "# Apply the conversion_pred and activity_pred functions to each row\n",
    "plot_grid[\"conversionPred\"] = plot_grid.apply(\n",
    "    lambda row: conversion_pred([row[\"time\"], row[\"temperature\"], row[\"catalyst\"]]), axis=1\n",
    ")\n",
    "plot_grid[\"activityPred\"] = plot_grid.apply(\n",
    "    lambda row: activity_pred([row[\"time\"], row[\"temperature\"], row[\"catalyst\"]]), axis=1\n",
    ")\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "print(plot_grid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Assuming `plot_grid` DataFrame is already created with columns:\n",
    "# \"time\", \"temperature\", \"catalyst\", \"conversionPred\"\n",
    "\n",
    "# Create a contour plot for each unique temperature\n",
    "unique_temperatures = plot_grid[\"temperature\"].unique()\n",
    "n_temps = len(unique_temperatures)\n",
    "\n",
    "# Set up a 2x2 grid of subplots with space for the colorbar\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "gs = gridspec.GridSpec(2, 3, width_ratios=[1, 1, 0.05])  # Add a narrow column for the colorbar\n",
    "\n",
    "axes = [fig.add_subplot(gs[i // 2, i % 2]) for i in range(n_temps)]\n",
    "\n",
    "for i, temp in enumerate(unique_temperatures):\n",
    "    # Filter data for the current temperature\n",
    "    temp_data = plot_grid[plot_grid[\"temperature\"] == temp]\n",
    "    \n",
    "    # Pivot the data for contour plotting\n",
    "    pivot_table = temp_data.pivot(index=\"catalyst\", columns=\"time\", values=\"conversionPred\")\n",
    "    \n",
    "    # Create the contour plot\n",
    "    ax = axes[i]\n",
    "    contour = ax.contourf(\n",
    "        pivot_table.columns,  # x-axis (time)\n",
    "        pivot_table.index,    # y-axis (catalyst)\n",
    "        pivot_table.values,   # z-axis (conversionPred)\n",
    "        cmap=\"viridis\",\n",
    "        levels=20  # Number of contour levels\n",
    "    )\n",
    "    contour_lines = ax.contour(\n",
    "        pivot_table.columns,\n",
    "        pivot_table.index,\n",
    "        pivot_table.values,\n",
    "        colors=\"black\",\n",
    "        linewidths=0.5,\n",
    "        levels=20\n",
    "    )\n",
    "    ax.clabel(contour_lines, inline=True, fontsize=8)  # Add labels to contour lines\n",
    "    \n",
    "    # Set plot labels and title\n",
    "    ax.set_title(f\"Temperature = {temp:.2f}\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Catalyst\")\n",
    "\n",
    "# Add a colorbar to the right of the plots\n",
    "cbar_ax = fig.add_subplot(gs[:, 2])  # Use the third column for the colorbar\n",
    "fig.colorbar(contour, cax=cbar_ax, orientation=\"vertical\", label=\"Conversion Prediction\")\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the overall desirability functions, objects must be created for the individual functions. For example, the following code chunk creates the appropriate objects and uses the `predict` method to estimate desirability at the center point of the design:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the overall score for these settings of the experimental factors, the `dOverall` function is used to combine the objects and \\code{predict} is used to get the final score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create desirability objects\n",
    "conversionD = DMax(80, 97)\n",
    "activityD = DTarget(55, 57.5, 60)\n",
    "\n",
    "# Predict outcomes\n",
    "pred_outcomes = [\n",
    "    conversion_pred([0, 0, 0]),\n",
    "    activity_pred([0, 0, 0])\n",
    "]\n",
    "print(\"Predicted Outcomes:\", pred_outcomes)\n",
    "\n",
    "# Predict desirability for each outcome\n",
    "conversion_desirability = conversionD.predict(pred_outcomes[0])\n",
    "activity_desirability = activityD.predict(pred_outcomes[1])\n",
    "\n",
    "print(\"Conversion Desirability:\", conversion_desirability)\n",
    "print(\"Activity Desirability:\", activity_desirability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `conversionD` and `activityD` are already created\n",
    "overallD = DOverall(conversionD, activityD)\n",
    "\n",
    "# Print the overall desirability object (for debugging purposes)\n",
    "# print(\"Overall Desirability Object:\", overallD)\n",
    "print(overallD.d_objs[0].low)\n",
    "print(overallD.d_objs[0].high)\n",
    "print(overallD.d_objs[0].missing)\n",
    "print(overallD.d_objs[1].low)\n",
    "print(overallD.d_objs[1].target)\n",
    "print(overallD.d_objs[1].high)\n",
    "print(overallD.d_objs[1].missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attributes of DOverall:\")\n",
    "overallD.print_class_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict the overall desirability for the given outcomes\n",
    "overall_desirability = overallD.predict(pred_outcomes, all=True)\n",
    "print(\"Overall Desirability:\", overall_desirability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `overallD` is an instance of DOverall and `plotGrid` is a pandas DataFrame\n",
    "# The columns 4 and 5 in R correspond to zero-based indices 3 and 4 in Python\n",
    "\n",
    "# Predict desirability values\n",
    "d_values = overallD.predict(plot_grid.iloc[:, [3, 4]].values, all=True)\n",
    "overallD.predict(plot_grid.iloc[:, [3, 4]].values, all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract individual and overall desirabilities\n",
    "individual_desirabilities = d_values[0]\n",
    "overall_desirability = d_values[1]\n",
    "\n",
    "# Create a DataFrame from the individual desirabilities\n",
    "d_values_df = pd.DataFrame(individual_desirabilities).T  # Transpose to have correct orientation\n",
    "d_values_df.columns = [\"D1\", \"D2\"]  # Adjust column names as needed\n",
    "\n",
    "# Add the overall desirability to the DataFrame\n",
    "d_values_df[\"Overall\"] = overall_desirability\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "plot_grid = pd.concat([plot_grid, d_values_df], axis=1)\n",
    "\n",
    "# Print the updated DataFrame (optional)\n",
    "print(plot_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_contour_plot(data, x_col, y_col, z_col, facet_col=None, aspect=1, as_table=True):\n",
    "    \"\"\"\n",
    "    Creates contour plots similar to R's contourplot function using matplotlib.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        x_col (str): The name of the column to use for the x-axis.\n",
    "        y_col (str): The name of the column to use for the y-axis.\n",
    "        z_col (str): The name of the column to use for the z-axis (contour values).\n",
    "        facet_col (str, optional): The name of the column to use for faceting (creating subplots). Defaults to None.\n",
    "        aspect (float, optional): The aspect ratio of the plot. Defaults to 1.\n",
    "        as_table (bool, optional): Whether to arrange facets as a table. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    if facet_col:\n",
    "        facet_values = data[facet_col].unique()\n",
    "        num_facets = len(facet_values)\n",
    "\n",
    "        # Determine subplot layout\n",
    "        if as_table:\n",
    "            num_cols = int(np.ceil(np.sqrt(num_facets)))\n",
    "            num_rows = int(np.ceil(num_facets / num_cols))\n",
    "        else:\n",
    "            num_cols = num_facets\n",
    "            num_rows = 1\n",
    "\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 6, num_rows * 6))  # Adjust figsize as needed\n",
    "        axes = np.array(axes).flatten()  # Flatten the axes array for easy indexing\n",
    "\n",
    "        for i, facet_value in enumerate(facet_values):\n",
    "            ax = axes[i]\n",
    "            facet_data = data[data[facet_col] == facet_value]\n",
    "\n",
    "            # Create grid for contour plot\n",
    "            x = np.unique(facet_data[x_col])\n",
    "            y = np.unique(facet_data[y_col])\n",
    "            X, Y = np.meshgrid(x, y)\n",
    "            Z = facet_data.pivot_table(index=y_col, columns=x_col, values=z_col).values\n",
    "\n",
    "            # Plot contour\n",
    "            contour = ax.contour(X, Y, Z, levels=10, cmap='viridis')  # Adjust levels and cmap as needed\n",
    "            ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "            # Set labels and title\n",
    "            ax.set_xlabel(x_col)\n",
    "            ax.set_ylabel(y_col)\n",
    "            ax.set_title(f\"{facet_col} = {facet_value}\")\n",
    "            ax.set_aspect(aspect)\n",
    "\n",
    "        # Remove empty subplots\n",
    "        for i in range(num_facets, len(axes)):\n",
    "            fig.delaxes(axes[i])\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        # Create grid for contour plot\n",
    "        x = np.unique(data[x_col])\n",
    "        y = np.unique(data[y_col])\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = data.pivot_table(index=y_col, columns=x_col, values=z_col).values\n",
    "\n",
    "        # Plot contour\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        contour = ax.contour(X, Y, Z, levels=10, cmap='viridis')  # Adjust levels and cmap as needed\n",
    "        ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel(x_col)\n",
    "        ax.set_ylabel(y_col)\n",
    "        ax.set_title(f\"Contour Plot of {z_col}\")\n",
    "        ax.set_aspect(aspect)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage (assuming plotGrid is a pandas DataFrame)\n",
    "# and 'time', 'catalyst', 'temperature', and 'D1' are column names\n",
    "create_contour_plot(\n",
    "    data=plot_grid,\n",
    "    x_col='time',\n",
    "    y_col='catalyst',\n",
    "    z_col='D1',\n",
    "    facet_col='temperature',\n",
    "    aspect=1,\n",
    "    as_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming plotGrid is a pandas DataFrame)\n",
    "# and 'time', 'catalyst', 'temperature', and 'Overall' are column names\n",
    "create_contour_plot(\n",
    "    data=plot_grid,\n",
    "    x_col='time',\n",
    "    y_col='catalyst',\n",
    "    z_col='D2',\n",
    "    facet_col='temperature',\n",
    "    aspect=1,\n",
    "    as_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming plotGrid is a pandas DataFrame)\n",
    "# and 'time', 'catalyst', 'temperature', and 'Overall' are column names\n",
    "create_contour_plot(\n",
    "    data=plot_grid,\n",
    "    x_col='time',\n",
    "    y_col='catalyst',\n",
    "    z_col='Overall',\n",
    "    facet_col='temperature',\n",
    "    aspect=1,\n",
    "    as_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the search grid\n",
    "time = np.linspace(-1.5, 1.5, 5)\n",
    "temperature = np.linspace(-1.5, 1.5, 5)\n",
    "catalyst = np.linspace(-1.5, 1.5, 5)\n",
    "\n",
    "search_grid = pd.DataFrame(\n",
    "    np.array(np.meshgrid(time, temperature, catalyst)).T.reshape(-1, 3),\n",
    "    columns=[\"time\", \"temperature\", \"catalyst\"]\n",
    ")\n",
    "\n",
    "def rsm_opt(x, d_object, space=\"square\"):\n",
    "    \"\"\"\n",
    "    Optimization function to calculate desirability.\n",
    "\n",
    "    Args:\n",
    "        x (list or np.ndarray): Input parameters (time, temperature, catalyst).\n",
    "        d_object (DOverall): Overall desirability object.\n",
    "        space (str): Design space (\"square\" or \"circular\").\n",
    "\n",
    "    Returns:\n",
    "        float: Negative desirability (for maximization).\n",
    "    \"\"\"\n",
    "    # Calculate conversion and activity predictions\n",
    "    conv = conversion_pred(x)\n",
    "    acty = activity_pred(x)\n",
    "\n",
    "    # Predict desirability using the overallD object\n",
    "    desirability = d_object.predict(np.array([[conv, acty]]))\n",
    "\n",
    "    # Apply space constraints\n",
    "    if space == \"circular\":\n",
    "        if np.sqrt(np.sum(np.array(x) ** 2)) > 1.682:\n",
    "            return 0.0\n",
    "    elif space == \"square\":\n",
    "        if np.any(np.abs(np.array(x)) > 1.682):\n",
    "            return 0.0\n",
    "\n",
    "    return -desirability  # Return negative desirability for maximization\n",
    "\n",
    "# Initialize the best result\n",
    "best = None\n",
    "\n",
    "# Perform optimization for each point in the search grid\n",
    "for i, row in search_grid.iterrows():\n",
    "    initial_guess = row.values  # Initial guess for optimization\n",
    "\n",
    "    # Perform optimization using scipy's minimize function\n",
    "    result = minimize(\n",
    "        rsm_opt,\n",
    "        initial_guess,\n",
    "        args=(overallD, \"square\"),  # Pass additional arguments to rsmOpt\n",
    "        method=\"Nelder-Mead\",\n",
    "        options={\"maxiter\": 1000, \"disp\": False}\n",
    "    )\n",
    "\n",
    "    # Update the best result if necessary\n",
    "    if best is None or result.fun < best.fun:  # Compare based on the negative desirability\n",
    "        best = result\n",
    "\n",
    "# Print the best result\n",
    "print(\"Best Parameters:\", best.x)\n",
    "print(\"Best Desirability:\", -best.fun)  # Convert back to positive desirability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the search grid\n",
    "time = np.linspace(-1.5, 1.5, 5)\n",
    "temperature = np.linspace(-1.5, 1.5, 5)\n",
    "catalyst = np.linspace(-1.5, 1.5, 5)\n",
    "\n",
    "search_grid = pd.DataFrame(\n",
    "    np.array(np.meshgrid(time, temperature, catalyst)).T.reshape(-1, 3),\n",
    "    columns=[\"time\", \"temperature\", \"catalyst\"]\n",
    ")\n",
    "\n",
    "# Define the optimization function (rsmOpt must be implemented separately)\n",
    "def rsm_opt(x, d_object, space=\"circular\"):\n",
    "    \"\"\"\n",
    "    Optimization function to calculate desirability.\n",
    "\n",
    "    Args:\n",
    "        x (list or np.ndarray): Input parameters (time, temperature, catalyst).\n",
    "        d_object (DOverall): Overall desirability object.\n",
    "        space (str): Design space (\"circular\" or \"square\").\n",
    "\n",
    "    Returns:\n",
    "        float: Negative desirability (for maximization).\n",
    "    \"\"\"\n",
    "    # Calculate conversion and activity predictions\n",
    "    conv = conversion_pred(x)\n",
    "    acty = activity_pred(x)\n",
    "\n",
    "    # Predict desirability using the overallD object\n",
    "    desirability = d_object.predict(np.array([[conv, acty]]))\n",
    "\n",
    "    # Apply space constraints\n",
    "    if space == \"circular\":\n",
    "        if np.sqrt(np.sum(np.array(x) ** 2)) > 1.682:\n",
    "            return 0.0\n",
    "    elif space == \"square\":\n",
    "        if np.any(np.abs(np.array(x)) > 1.682):\n",
    "            return 0.0\n",
    "\n",
    "    return -desirability  # Return negative desirability for maximization\n",
    "\n",
    "# Initialize the best result\n",
    "best = None\n",
    "\n",
    "# Perform optimization for each point in the search grid\n",
    "for i, row in search_grid.iterrows():\n",
    "    initial_guess = row.values  # Initial guess for optimization\n",
    "\n",
    "    # Perform optimization using scipy's minimize function\n",
    "    result = minimize(\n",
    "        rsm_opt,\n",
    "        initial_guess,\n",
    "        args=(overallD, \"circular\"),  # Pass additional arguments to rsmOpt\n",
    "        method=\"Nelder-Mead\",\n",
    "        options={\"maxiter\": 1000, \"disp\": False}\n",
    "    )\n",
    "\n",
    "    # Update the best result if necessary\n",
    "    if best is None or result.fun < best.fun:  # Compare based on the negative desirability\n",
    "        best = result\n",
    "\n",
    "# Print the best result\n",
    "print(\"Best Parameters:\", best.x)\n",
    "print(\"Best Desirability:\", -best.fun)  # Convert back to positive desirability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.utils.parallel import evaluate_row\n",
    "\n",
    "def sample_objective(row, control):\n",
    "    return row + control.get('offset', 0)\n",
    "\n",
    "def test_evaluate_row_with_list():\n",
    "    row = [1, 2, 3]\n",
    "    fun_control = {'offset': 10}\n",
    "    result = evaluate_row(row, sample_objective, fun_control)\n",
    "    result = np.squeeze(result)  # Remove the extra dimension\n",
    "    expected_result = np.array([11, 12, 13])\n",
    "    assert np.array_equal(result, expected_result), f\"Expected {expected_result}, but got {result}\"\n",
    "\n",
    "def test_evaluate_row_with_ndarray():\n",
    "    row = np.array([1, 2, 3])\n",
    "    fun_control = {'offset': 10}\n",
    "    result = evaluate_row(row, sample_objective, fun_control)\n",
    "    result = np.squeeze(result)  # Remove the extra dimension\n",
    "    expected_result = np.array([11, 12, 13])\n",
    "    assert np.array_equal(result, expected_result), f\"Expected {expected_result}, but got {result}\"\n",
    "\n",
    "def test_evaluate_row_without_offset():\n",
    "    row = np.array([4, 5, 6])\n",
    "    fun_control = {}\n",
    "    result = evaluate_row(row, sample_objective, fun_control)\n",
    "    result = np.squeeze(result)  # Remove the extra dimension\n",
    "    expected_result = np.array([4, 5, 6])\n",
    "    assert np.array_equal(result, expected_result), f\"Expected {expected_result}, but got {result}\"\n",
    "\n",
    "def test_evaluate_row_with_different_offset():\n",
    "    row = np.array([7, 8, 9])\n",
    "    fun_control = {'offset': -5}\n",
    "    result = evaluate_row(row, sample_objective, fun_control)\n",
    "    result = np.squeeze(result)  # Remove the extra dimension\n",
    "    expected_result = np.array([2, 3, 4])\n",
    "    assert np.array_equal(result, expected_result), f\"Expected {expected_result}, but got {result}\"\n",
    "\n",
    "def test_evaluate_row_with_float_values():\n",
    "    row = np.array([1.5, 2.5, 3.5])\n",
    "    fun_control = {'offset': 10}\n",
    "    result = evaluate_row(row, sample_objective, fun_control)\n",
    "    result = np.squeeze(result)  # Remove the extra dimension\n",
    "    expected_result = np.array([11.5, 12.5, 13.5])\n",
    "    assert np.array_equal(result, expected_result), f\"Expected {expected_result}, but got {result}\"\n",
    "\n",
    "def test_evaluate_row_with_negative_values():\n",
    "    row = np.array([-1, -2, -3])\n",
    "    fun_control = {'offset': 10}\n",
    "    result = evaluate_row(row, sample_objective, fun_control)\n",
    "    result = np.squeeze(result)  # Remove the extra dimension\n",
    "    expected_result = np.array([9, 8, 7])\n",
    "    assert np.array_equal(result, expected_result), f\"Expected {expected_result}, but got {result}\"\n",
    "\n",
    "def test_evaluate_row_with_zero_values():\n",
    "    row = np.array([0, 0, 0])\n",
    "    fun_control = {'offset': 10}\n",
    "    result = evaluate_row(row, sample_objective, fun_control)\n",
    "    result = np.squeeze(result)  # Remove the extra dimension\n",
    "    expected_result = np.array([10, 10, 10])\n",
    "    assert np.array_equal(result, expected_result), f\"Expected {expected_result}, but got {result}\"\n",
    "\n",
    "def test_evaluate_row_with_empty_row():\n",
    "    row = np.array([])\n",
    "    fun_control = {'offset': 10}\n",
    "    result = evaluate_row(row, sample_objective, fun_control)\n",
    "    result = np.squeeze(result)  # Remove the extra dimension\n",
    "    expected_result = np.array([])\n",
    "    assert np.array_equal(result, expected_result), f\"Expected {expected_result}, but got {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPOT Dimension (Shapes) Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf\n",
    "import numpy as np\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.eda import print_exp_table\n",
    "from spotpython.spot import spot\n",
    "from spotpython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "PREFIX=\"000\"\n",
    "data_set = Diabetes()\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    save_experiment=True,\n",
    "    fun_evals=inf,\n",
    "    max_time=1,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    tensorboard_log=True,\n",
    "    seed=42,)\n",
    "print_exp_table(fun_control)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# set epochs to 2^8:\n",
    "X[0, 1] = 8\n",
    "# set patience to 2^10:\n",
    "X[0, 7] = 10\n",
    "print(f\"X: {X}\")\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "# so that two values are returned\n",
    "X = np.vstack((X, X))\n",
    "print(f\"X: {X}\")\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "hyper_light = HyperLight(seed=125, log_level=50)\n",
    "res= hyper_light.fun(X, fun_control)\n",
    "print(f\"res.shape: {res.shape}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.multiobjectivefunctions import MultiAnalytical\n",
    "import numpy as np\n",
    "fun = MultiAnalytical(m=1)\n",
    "# Input data\n",
    "X = np.array([[0, 0, 0], [1, 1, 1]])\n",
    "# Single objective\n",
    "print(fun.fun_mo_linear(X))\n",
    "# Output: [[0.]\n",
    "#          [3.]]\n",
    "# Two objectives\n",
    "fun = MultiAnalytical(m=2)\n",
    "print(fun.fun_mo_linear(X))\n",
    "# Output: [[ 0. -0.]\n",
    "#          [ 3. -3.]]\n",
    "# Three objectives\n",
    "fun = MultiAnalytical(m=3)\n",
    "print(fun.fun_mo_linear(X))\n",
    "# Output: [[ 0. -0.  0.]\n",
    "#          [ 3. -3.  3.]]\n",
    "# Four objectives\n",
    "fun = MultiAnalytical(m=4)\n",
    "print(fun.fun_mo_linear(X))\n",
    "# Output: [[ 0. -0.  0. -0.]\n",
    "#          [ 3. -3.  3. -3.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[10]]).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Objective Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.multiobjectivefunctions import MultiAnalytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n",
    "\n",
    "\n",
    "def test_repeats_1_1_0():\n",
    "    \"\"\"\n",
    "    Test repeats. 1 repeat of initial design points, 1 repeat of function evaluations, and 0 repeats of OCBA.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "# Objective function with 4 objectives\n",
    "fun = MultiAnalytical(m=4).fun_mo_linear\n",
    "surrogate_control = surrogate_control_init(noise=True)\n",
    "design_control = design_control_init(init_size=3, repeats=1)\n",
    "fun_control = fun_control_init(\n",
    "    lower=np.array([-1]),\n",
    "    upper=np.array([1]),\n",
    "    fun_evals=20,\n",
    "    fun_repeats=1,\n",
    "    noise=True,\n",
    "    ocba_delta=0,\n",
    "    seed=123,\n",
    "    show_models=False,\n",
    "    show_progress=True,\n",
    "    sigma=0.001,\n",
    ")\n",
    "S = Spot(\n",
    "    fun=fun,\n",
    "    surrogate_control=surrogate_control,\n",
    "    design_control=design_control,\n",
    "    fun_control=fun_control,\n",
    ")\n",
    "S.run()\n",
    "S.y_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.multiobjectivefunctions import MultiAnalytical\n",
    "from spotpython.spot import Spot\n",
    "\n",
    "from spotpython.utils.init import fun_control_init\n",
    "# Objective function with m=5 objectives\n",
    "fun = MultiAnalytical(m=5).fun_mo_linear\n",
    "fun_control = fun_control_init(\n",
    "    lower=np.array([1, 0, 5]),\n",
    "    upper=np.array([10, 10, 100])\n",
    ")\n",
    "S = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    ")\n",
    "S.run()\n",
    "S.y_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.multiobjectivefunctions import MultiAnalytical\n",
    "from spotpython.spot import Spot\n",
    "\n",
    "from spotpython.utils.init import fun_control_init\n",
    "\n",
    "# Sum of squares aggregation function\n",
    "def aggregate(y):\n",
    "    return np.sum(y*y, axis=1)\n",
    "\n",
    "# 3 objectives\n",
    "fun = MultiAnalytical(m=3).fun_mo_linear\n",
    "\n",
    "fun_control = fun_control_init(    \n",
    "    lower = np.array([1, 0]),\n",
    "    upper = np.array([10, 10])\n",
    ")\n",
    "fun_control[\"fun_mo2so\"] = aggregate\n",
    "\n",
    "S = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    ")\n",
    "S.run()\n",
    "S.y_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.multiobjectivefunctions import MultiAnalytical\n",
    "from spotpython.spot import Spot\n",
    "\n",
    "from spotpython.utils.init import fun_control_init\n",
    "\n",
    "# first objective by 2, second multiplied by 3, third  multiplied by 4\n",
    "def aggregate(y):\n",
    "    return np.sum(y*np.array([2, 3, 4]), axis=1)\n",
    "\n",
    "# 3 objectives\n",
    "fun = MultiAnalytical(m=3).fun_mo_linear\n",
    "\n",
    "fun_control = fun_control_init(    \n",
    "    lower = np.array([1, 0]),\n",
    "    upper = np.array([10, 10])\n",
    ")\n",
    "fun_control[\"fun_mo2so\"] = aggregate\n",
    "\n",
    "S = Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    ")\n",
    "S.run()\n",
    "S.y_mo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.plot.contour import create_contour_plot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a grid of x and y values\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "x_grid, y_grid = np.meshgrid(x, y)\n",
    "\n",
    "# Calculate z = x^2 + y^2\n",
    "z = x_grid**2 + y_grid**2\n",
    "\n",
    "# Flatten the grid and create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'x': x_grid.flatten(),\n",
    "    'y': y_grid.flatten(),\n",
    "    'z': z.flatten()\n",
    "})\n",
    "\n",
    "# Create the contour plot\n",
    "create_contour_plot(data, 'x', 'y', 'z', facet_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.plot.contour import create_contour_plot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a grid of x and y values\n",
    "x = np.linspace(-5, 5, 50)\n",
    "y = np.linspace(-5, 5, 50)\n",
    "x_grid, y_grid = np.meshgrid(x, y)\n",
    "\n",
    "# Calculate z = x^2 + y^2 for two different facets\n",
    "z1 = x_grid**2 + y_grid**2\n",
    "z2 = (x_grid - 2)**2 + (y_grid - 2)**2\n",
    "\n",
    "# Flatten the grids and create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'x': np.tile(x, len(y) * 2),  # Repeat x values for both facets\n",
    "    'y': np.repeat(y, len(x) * 2),  # Repeat y values for both facets\n",
    "    'z': np.concatenate([z1.flatten(), z2.flatten()]),  # Combine z values for both facets\n",
    "    'facet': ['Facet A'] * len(z1.flatten()) + ['Facet B'] * len(z2.flatten())  # Create facet column\n",
    "})\n",
    "\n",
    "# Create the contour plot with facets\n",
    "create_contour_plot(data, 'x', 'y', 'z', facet_col='facet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.plot.contour import mo_create_contour_plots, mo_generate_plot_grid\n",
    "\n",
    "# Define the variables and their ranges\n",
    "variables = {\n",
    "    \"time\": (-1.7, 1.7),\n",
    "    \"temperature\": (-1.7, 1.7),\n",
    "    \"catalyst\": (-1.7, 1.7)\n",
    "}\n",
    "resolutions = {\n",
    "    \"time\": 50,\n",
    "    \"temperature\": 4,\n",
    "    \"catalyst\": 50\n",
    "}\n",
    "\n",
    "# Define the objective functions\n",
    "def conversion_pred(x):\n",
    "    return (\n",
    "        81.09\n",
    "        + 1.0284 * x[0]\n",
    "        + 4.043 * x[1]\n",
    "        + 6.2037 * x[2]\n",
    "        - 1.8366 * x[0]**2\n",
    "        + 2.9382 * x[1]**2\n",
    "        - 5.1915 * x[2]**2\n",
    "        + 2.2150 * x[0] * x[1]\n",
    "        + 11.375 * x[0] * x[2]\n",
    "        - 3.875 * x[1] * x[2]\n",
    "    )\n",
    "\n",
    "def activity_pred(x):\n",
    "    return (\n",
    "        59.85\n",
    "        + 3.583 * x[0]\n",
    "        + 0.2546 * x[1]\n",
    "        + 2.2298 * x[2]\n",
    "        + 0.83479 * x[0]**2\n",
    "        + 0.07484 * x[1]**2\n",
    "        + 0.05716 * x[2]**2\n",
    "        - 0.3875 * x[0] * x[1]\n",
    "        - 0.375 * x[0] * x[2]\n",
    "        + 0.3125 * x[1] * x[2]\n",
    "    )\n",
    "\n",
    "functions = {\n",
    "    \"conversionPred\": conversion_pred,\n",
    "    \"activityPred\": activity_pred\n",
    "}\n",
    "\n",
    "# Generate the plot grid\n",
    "plot_grid = mo_generate_plot_grid(variables, resolutions, functions)\n",
    "\n",
    "# Create contour plots for \"conversionPred\"\n",
    "mo_create_contour_plots(\n",
    "    plot_grid,\n",
    "    x_col=\"time\",\n",
    "    y_col=\"catalyst\",\n",
    "    z_col=\"conversionPred\",\n",
    "    facet_col=\"temperature\",\n",
    "    z_label=\"Conversion Prediction\"\n",
    ")\n",
    "\n",
    "# Create contour plots for \"activityPred\"\n",
    "mo_create_contour_plots(\n",
    "    plot_grid,\n",
    "    x_col=\"time\",\n",
    "    y_col=\"catalyst\",\n",
    "    z_col=\"activityPred\",\n",
    "    facet_col=\"temperature\",\n",
    "    z_label=\"Activity Prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.desirability import DOverall, rsm_opt, DTarget, conversion_pred, activity_pred\n",
    "\n",
    "# List of prediction functions\n",
    "prediction_funcs = [conversion_pred, activity_pred]\n",
    "\n",
    "# Example desirability object (DOverall)\n",
    "from spotpython.utils.desirability import DOverall, DMax, DTarget\n",
    "conversionD = DMax(80, 97)\n",
    "activityD = DTarget(55, 57.5, 60)\n",
    "overallD = DOverall(conversionD, activityD)\n",
    "\n",
    "# Example input parameters\n",
    "x = [0.5, 0.5, 0.5]\n",
    "\n",
    "# Calculate desirability\n",
    "result = rsm_opt(x, overallD, prediction_funcs, space=\"square\")\n",
    "print(\"Negative Desirability:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randint from Forr08a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.effects import randorient\n",
    "k = 2\n",
    "p = 3\n",
    "xi = 1\n",
    "randorient(k, p, xi, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morris-Mitchell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experimental Setup ---\n",
    "from spotpython.utils.sampling import jd, mmphi, mmphi_intensive\n",
    "from scipy.stats import qmc\n",
    "import numpy as np\n",
    "\n",
    "N_DIM = 2\n",
    "Q_EXP = 2.0\n",
    "P_NORM = 2.0 # Euclidean distance\n",
    "\n",
    "# We will compare two high-quality LHS designs of different sizes\n",
    "# with two lower-quality random designs of the same sizes.\n",
    "lhs_design_20 = qmc.LatinHypercube(d=N_DIM, seed=1).random(n=20)\n",
    "random_design_20 = np.random.default_rng(1).random(size=(20, N_DIM))\n",
    "\n",
    "lhs_design_50 = qmc.LatinHypercube(d=N_DIM, seed=2).random(n=50)\n",
    "random_design_50 = np.random.default_rng(2).random(size=(50, N_DIM))\n",
    "\n",
    "designs_to_test = {\n",
    "    \"LHS (N=20)\": lhs_design_20,\n",
    "    \"Random (N=20)\": random_design_20,\n",
    "    \"LHS (N=50)\": lhs_design_50,\n",
    "    \"Random (N=50)\": random_design_50,\n",
    "}\n",
    "\n",
    "results = []\n",
    "for label, design in designs_to_test.items():\n",
    "    raw_score = mmphi(design, q=Q_EXP, p=P_NORM)\n",
    "    intensive_score = mmphi_intensive(design, q=Q_EXP, p=P_NORM)\n",
    "    results.append({\n",
    "        \"Design\": label,\n",
    "        \"Original mmphi (Extensive)\": f\"{raw_score:.4f}\",\n",
    "        \"New mmphi_intensive (Intensive)\": f\"{intensive_score:.4f}\",\n",
    "    })\n",
    "    \n",
    "# --- Print Results Table ---\n",
    "headers = list(results[0].keys())\n",
    "widths = {h: max(len(h), max(len(row[h]) for row in results)) for h in headers}\n",
    "\n",
    "header_line = \" | \".join(h.ljust(widths[h]) for h in headers)\n",
    "print(header_line)\n",
    "print(\"-\" * len(header_line))\n",
    "\n",
    "for row in results:\n",
    "    row_line = \" | \".join(row[h].ljust(widths[h]) for h in headers)\n",
    "    print(row_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function for design generation ---\n",
    "def generate_design(n_points: int, n_dim: int, seed: int, design_type: str) -> np.ndarray:\n",
    "    if design_type == 'lhs':\n",
    "        sampler = qmc.LatinHypercube(d=n_dim, seed=seed)\n",
    "        return sampler.random(n=n_points)\n",
    "    elif design_type == 'random':\n",
    "        rng = np.random.default_rng(seed)\n",
    "        return rng.random(size=(n_points, n_dim))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown design type\")\n",
    "\n",
    "# --- Analysis Parameters ---\n",
    "N_DIM = 2\n",
    "Q_EXP = 2.0\n",
    "P_NORM = 2.0 # Euclidean distance\n",
    "\n",
    "designs_to_compare = {\n",
    "    \"Good Design (LHS, N=20)\":   generate_design(150, N_DIM, 42, 'lhs'),\n",
    "    \"Poor Design (Random, N=20)\": generate_design(150, N_DIM, 42, 'random'),\n",
    "    \"Good Design (LHS, N=50)\":   generate_design(151, N_DIM, 42, 'lhs'),\n",
    "    \"Poor Design (Random, N=50)\": generate_design(151, N_DIM, 42, 'random')\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Running comparative analysis\\n\")\n",
    "for label, design in designs_to_compare.items():\n",
    "    # Calculate original (extensive) metric\n",
    "    original_score = mmphi(design, q=Q_EXP, p=P_NORM)\n",
    "    \n",
    "    # Calculate new (intensive) metric\n",
    "    intensive_score = mmphi_intensive(design, q=Q_EXP, p=P_NORM)\n",
    "    \n",
    "    results.append({\n",
    "        'Design': label,\n",
    "        'Original mmphi': f\"{original_score:.4f}\",\n",
    "        'Intensive mmphi': f\"{intensive_score:.4f}\"\n",
    "    })\n",
    "\n",
    "# --- Print Results Table ---\n",
    "headers = list(results[0].keys())\n",
    "widths = {h: max(len(h), max(len(row[h]) for row in results)) for h in headers}\n",
    "\n",
    "header_line = \" | \".join(h.ljust(widths[h]) for h in headers)\n",
    "print(header_line)\n",
    "print(\"-\" * len(header_line))\n",
    "\n",
    "for row in results:\n",
    "    row_line = \" | \".join(row[h].ljust(widths[h]) for h in headers)\n",
    "    print(row_line)\n",
    "\n",
    "print(\"\\n--- Interpretation ---\")\n",
    "print(\"Notice how 'Original mmphi' scores are not comparable between N=20 and N=50 designs.\")\n",
    "print(\"The 'Intensive mmphi' scores, however, are comparable. The two 'Good' LHS designs have\")\n",
    "print(\"similar low scores, which are clearly better (lower) than the scores for the 'Poor' random designs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, \n",
    "    surrogate_control_init, \n",
    "    design_control_init\n",
    ")\n",
    "\n",
    "# Create test function (3D sphere function)\n",
    "fun = Analytical().fun_sphere\n",
    "\n",
    "# Setup control parameters\n",
    "fun_control = fun_control_init(\n",
    "    lower=np.array([-1, -1, -1]),\n",
    "    upper=np.array([1, 1, 1]),\n",
    "    fun_evals=20,\n",
    "    var_name=[\"x1\", \"x2\", \"x3\"],  # Give names to variables\n",
    ")\n",
    "\n",
    "# Setup design with initial size\n",
    "design_control = design_control_init(init_size=10)\n",
    "\n",
    "# Setup surrogate model with multiple theta values\n",
    "surrogate_control = surrogate_control_init(\n",
    "    n_theta=\"anisotropic\",  # Use different theta for each dimension\n",
    "    method=\"interpolation\"\n",
    ")\n",
    "\n",
    "# Initialize and run spot\n",
    "S = spot.Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control,\n",
    "    surrogate_control=surrogate_control\n",
    ")\n",
    "S.run()\n",
    "\n",
    "# Get importance values\n",
    "importance = S.get_importance()\n",
    "print(\"\\nVariable importance values:\")\n",
    "for var, imp in zip(S.all_var_name, importance):\n",
    "    print(f\"{var}: {imp:.2f}%\")\n",
    "\n",
    "# Example output:\n",
    "# Variable importance values:\n",
    "# x1: 100.00%\n",
    "# x2: 85.32%\n",
    "# x3: 76.15%\n",
    "\n",
    "# Try with single theta (should return zeros)\n",
    "surrogate_control = surrogate_control_init(\n",
    "    n_theta=1,  # Single theta for all dimensions\n",
    "    method=\"interpolation\"\n",
    ")\n",
    "\n",
    "S2 = spot.Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control,\n",
    "    surrogate_control=surrogate_control\n",
    ")\n",
    "S2.run()\n",
    "\n",
    "importance2 = S2.get_importance()\n",
    "print(\"\\nImportance values with single theta:\")\n",
    "print(importance2)  # Will print [0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.spot import Spot\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_hyperparameter, get_tuned_architecture\n",
    "\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    force_run=True,\n",
    "    PREFIX=\"get_one_config_from_X\",\n",
    "    fun_evals=5,\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "set_hyperparameter(fun_control, \"epochs\", [2,2])\n",
    "set_hyperparameter(fun_control, \"patience\", [1,1])\n",
    "design_control = design_control_init(init_size=4)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "S = Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "S.run()\n",
    "conf = get_tuned_architecture(S)\n",
    "    \n",
    "# Consider transformation of the values\n",
    "assert conf['epochs'] == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.fun_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kriging Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "     fun_control_init, surrogate_control_init, design_control_init\n",
    " )\n",
    "fun_control = fun_control_init(\n",
    "     lower=np.array([-5, 0]),\n",
    "     upper=np.array([10, 15]),\n",
    "     fun_evals=100\n",
    " )\n",
    "design_control = design_control_init(init_size=100)\n",
    "surrogate_control = surrogate_control_init(n_theta=\"anisotropic\", method=\"interpolation\")\n",
    "S = spot.Spot(\n",
    "     fun=Analytical().fun_branin,\n",
    "     fun_control=fun_control,\n",
    "     design_control=design_control,\n",
    "     surrogate_control=surrogate_control\n",
    " )\n",
    "S.run()\n",
    "S.plot_contour(i=0, j=1, title=\"\", contour_levels=25, n_grid=100, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.plot_contour(i=0, j=1, title=\"\", contour_levels=25, n_grid=100, show=True, add_points=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "     fun_control_init, surrogate_control_init, design_control_init\n",
    " )\n",
    "fun_control = fun_control_init(\n",
    "     lower=np.array([-5, 0]),\n",
    "     upper=np.array([10, 15]),\n",
    "     fun_evals=100\n",
    " )\n",
    "design_control = design_control_init(init_size=100)\n",
    "surrogate_control = surrogate_control_init(n_theta=\"anisotropic\", method=\"interpolation\")\n",
    "S = spot.Spot(\n",
    "     fun=Analytical().fun_branin,\n",
    "     fun_control=fun_control,\n",
    "     design_control=design_control,\n",
    "     surrogate_control=surrogate_control\n",
    " )\n",
    "S.run()\n",
    "S.get_spot_attributes_as_df()\n",
    "S.plot_contour(i=0, j=1, title=\"\", contour_levels=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_fun(X, **kwargs):\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    a = 1\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5 / np.pi\n",
    "    r = 6\n",
    "    s = 10\n",
    "    t = 1 / (8 * np.pi)\n",
    "    y = a * (x2 - b * x1**2 + c * x1 - r)**2 + s * (1 - t) * np.cos(x1) + s\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.surrogate.kriging import Kriging\n",
    "from spotpython.surrogate.plot import plotkd\n",
    "# Training data\n",
    "x1 = np.linspace(-5, 10, 10)\n",
    "x2 = np.linspace(0, 15, 10)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "X_train = np.array([X1.ravel(), X2.ravel()]).T\n",
    "# Apply the user-defined function to generate Z values\n",
    "Z = user_fun(np.array([X1.ravel(), X2.ravel()]).T)\n",
    "Z = Z.reshape(X1.shape)\n",
    "y_train = Z.ravel()\n",
    "# Initialize and fit the Kriging model\n",
    "model = Kriging().fit(X_train, y_train)\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 3D Kriging surrogate\n",
    "plotkd(model, X_train, y_train, 0, 1, eps=1e-10, max_error=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 3D Kriging surrogate\n",
    "plotkd(model, X_train, y_train, 0, 1, eps=1e-10, max_error=1e-9, add_points=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "     fun_control_init, surrogate_control_init, design_control_init\n",
    " )\n",
    "fun_control = fun_control_init(\n",
    "     lower=np.array([-5, 0]),\n",
    "     upper=np.array([10, 15]),\n",
    "     fun_evals=100\n",
    " )\n",
    "design_control = design_control_init(init_size=100)\n",
    "surrogate_control = surrogate_control_init(n_theta=\"anisotropic\", method=\"interpolation\")\n",
    "S = spot.Spot(\n",
    "     fun=Analytical().fun_branin,\n",
    "     fun_control=fun_control,\n",
    "     design_control=design_control,\n",
    "     surrogate_control=surrogate_control\n",
    " )\n",
    "S.run()\n",
    "S.plot_contour(i=0, j=1, title=\"\", contour_levels=25, n_grid=100, show=True)\n",
    "S.var_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Using output from Spot\n",
    "# Assume S is a Spot object with a fitted surrogate\n",
    "X, Y, Z = S.prepare_plot(i=0, j=1, n_grid=100)\n",
    "from spotpython.surrogate.plot import plot_3d_contour\n",
    "plot_3d_contour(\n",
    "     X,\n",
    "     Y,\n",
    "     Z,\n",
    "     vmin=Z.min(),\n",
    "     vmax=Z.max(),\n",
    "     var_name=S.var_name,\n",
    "     i=0,\n",
    "     j=1,\n",
    "     title=\"Surrogate Model Contour\",\n",
    "     contour_levels=25,\n",
    "     show=True\n",
    " )\n",
    "# Example 2: Using plot_3d_contour from scratch\n",
    "import numpy as np\n",
    "from spotpython.surrogate.plot import plot_3d_contour\n",
    "# Create a grid\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "# Define a function over the grid\n",
    "Z = np.sin(np.sqrt(X**2 + Y**2))\n",
    "\n",
    "plot_3d_contour(\n",
    "     X,\n",
    "     Y,\n",
    "     Z,\n",
    "     vmin=Z.min(),\n",
    "     vmax=Z.max(),\n",
    "     var_name=[\"x\", \"y\"],\n",
    "     i=0,\n",
    "     j=1,\n",
    "     title=\"Sine Surface\",\n",
    "     contour_levels=20,\n",
    "     show=True\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotkd(model, X_train, y_train, eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.surrogate.kriging import Kriging\n",
    "from spotpython.surrogate.plot import plotkd\n",
    "# Training data\n",
    "X_train = 2*(np.random.rand(10, 2) - 0.5)  # 10 samples with 3 dimensions\n",
    "y_train = np.sin(X_train[:, 0])**2 + np.cos(X_train[:, 1])**2  # Example target function  \n",
    "# Initialize and fit the Kriging model\n",
    "model = Kriging(method=\"interpolation\").fit(X_train, y_train,)\n",
    "# Plot the Kriging surrogate for dimensions 0 and 1\n",
    "plotkd(model, X_train, y_train, i=0, j=1, show=True, max_error=1e-6, eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n",
    "fun = Analytical()\n",
    "fun.fun_branin_factor(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "     fun_control_init, surrogate_control_init, design_control_init\n",
    " )\n",
    "fun_control = fun_control_init(\n",
    "     lower=np.array([-2, 0, 0]),\n",
    "     upper=np.array([10, 15, 2]),\n",
    "     fun_evals=100,\n",
    "     var_type=[\"num\", \"num\", \"factor\"],  # Specify variable types\n",
    " )\n",
    "design_control = design_control_init(init_size=90)\n",
    "surrogate_control = surrogate_control_init(n_theta=\"anisotropic\", method=\"interpolation\")\n",
    "S = spot.Spot(\n",
    "     fun=Analytical().fun_branin_factor,\n",
    "     fun_control=fun_control,\n",
    "     design_control=design_control,\n",
    "     surrogate_control=surrogate_control\n",
    " )\n",
    "S.run()\n",
    "S.plot_contour(i=0, j=1, title=\"\", contour_levels=25, n_grid=100, show=True, add_points=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.plot_contour(i=0, j=2, title=\"\", contour_levels=25, n_grid=100, show=True, add_points=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.plot_contour(i=1, j=2, title=\"\", contour_levels=25, n_grid=100, show=True, add_points=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.surrogate.plot import generate_mesh_grid\n",
    "# Example 1: Using input data\n",
    "X = np.random.rand(4, 3)  # 5 samples with 3 dimensions\n",
    "print(f\"X:\\n{X}\")\n",
    "X_i, X_j, grid_points = generate_mesh_grid(X, i=0, j=1, num=5,  var_type=[\"num\", \"num\", \"factor\"])\n",
    "print(f\"X_i:\\n{X_i},\\nX_j:\\n{X_j},\\ngrid_points:\\n{grid_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Using lower and upper bounds\n",
    "lower = np.array([-5, 0, 0])\n",
    "upper = np.array([10, 15, 2])\n",
    "X_i, X_j, grid_points = generate_mesh_grid(lower=lower, upper=upper, i=0, j=1, num=5, var_type=[\"num\", \"num\", \"factor\"])\n",
    "print(f\"X_i:\\n{X_i},\\nX_j:\\n{X_j},\\ngrid_points:\\n{grid_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = np.array([-5, 0, 0])\n",
    "upper = np.array([10, 15, 2])\n",
    "X_i, X_j, grid_points = generate_mesh_grid(lower=lower, upper=upper, i=0, j=2, num=10, var_type=[\"num\", \"num\", \"factor\"])\n",
    "print(f\"X_i:\\n{X_i},\\nX_j:\\n{X_j},\\ngrid_points:\\n{grid_points}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test n_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import (fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init)\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.eda import print_exp_table, print_res_table\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start _surrogate_control_setup: n_theta is anisotropic\n",
      "Leaving _surrogate_control_setup: n_theta is 1\n",
      "spotpython tuning: 1.0155104003206277e-09 [#######---] 73.33% \n",
      "spotpython tuning: 1.0155104003206277e-09 [########--] 80.00% \n",
      "spotpython tuning: 1.0155104003206277e-09 [#########-] 86.67% \n",
      "spotpython tuning: 1.0155104003206277e-09 [#########-] 93.33% \n",
      "spotpython tuning: 6.271448121765632e-11 [##########] 100.00% Done...\n",
      "\n",
      "Experiment saved to 000_res.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Attribute Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Attribute Value",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "6fbf471f-5977-4989-b8f7-c3b915b7dc9b",
       "rows": [
        [
         "0",
         "X",
         "[[ 6.35296274e-02]\n [-4.10764204e-01]\n [-4.40719746e-02]\n [ 9.63125638e-01]\n [-8.35181180e-01]\n [-3.62418901e-01]\n [ 4.15331000e-01]\n [ 3.44685120e-01]\n [ 6.36049088e-01]\n [-7.77978539e-01]\n [-3.18670739e-05]\n [-4.98884500e-05]\n [-5.37213964e-05]\n [-6.14242049e-05]\n [ 7.91924752e-06]]"
        ],
        [
         "1",
         "all_lower",
         "[-1]"
        ],
        [
         "2",
         "all_upper",
         "[1]"
        ],
        [
         "3",
         "all_var_name",
         "['x0']"
        ],
        [
         "4",
         "all_var_type",
         "['num']"
        ],
        [
         "5",
         "counter",
         "15"
        ],
        [
         "6",
         "de_bounds",
         "[[np.int64(-1), np.int64(1)]]"
        ],
        [
         "7",
         "design",
         "<spotpython.design.spacefilling.SpaceFilling object at 0x30cd7b1d0>"
        ],
        [
         "8",
         "design_control",
         "{'init_size': 10, 'repeats': 1}"
        ],
        [
         "9",
         "eps",
         "1.4901161193847656e-08"
        ],
        [
         "10",
         "fun_control",
         "{'PREFIX': '000', 'CHECKPOINT_PATH': 'runs/saved_models/', 'DATASET_PATH': 'data/', 'RESULTS_PATH': 'results/', 'TENSORBOARD_PATH': 'runs/', 'TENSORBOARD_CLEAN': False, '_L_in': None, '_L_out': None, '_L_cond': None, '_torchmetric': None, 'accelerator': 'auto', 'check_finite': True, 'collate_fn_name': None, 'converters': None, 'core_model': None, 'core_model_name': None, 'counter': 15, 'data': None, 'data_dir': './data', 'data_full_train': None, 'divergence_threshold': None, 'hacky': False, 'data_module': None, 'data_set': None, 'data_set_name': None, 'data_test': None, 'data_val': None, 'db_dict_name': None, 'design': None, 'device': None, 'devices': 'auto', 'enable_progress_bar': False, 'eval': None, 'force_run': True, 'fun_evals': 15, 'fun_mo2so': None, 'fun_repeats': 1, 'horizon': None, 'hyperdict': None, 'infill_criterion': 'y', 'k_folds': 3, 'log_every_n_steps': 50, 'log_graph': False, 'log_level': 50, 'loss_function': None, 'lower': array([-1]), 'max_time': 1, 'max_surrogate_points': 30, 'metric_river': None, 'metric_sklearn': None, 'metric_sklearn_name': None, 'metric_torch': None, 'metric_params': {}, 'model_dict': {}, 'noise': False, 'n_points': 1, 'n_samples': None, 'n_total': None, 'num_nodes': 1, 'num_sanity_val_steps': 2, 'num_workers': 0, 'ocba_delta': 0, 'oml_grace_period': None, 'optimizer': None, 'path': None, 'penalty_NA': None, 'precision': '32', 'prep_model': None, 'prep_model_name': None, 'progress_file': None, 'save_experiment': False, 'save_result': True, 'scaler': None, 'scaler_name': None, 'scenario': None, 'seed': 123, 'show_batch_interval': 1000000, 'show_config': False, 'show_models': False, 'show_progress': True, 'shuffle': None, 'shuffle_train': True, 'shuffle_val': False, 'shuffle_test': False, 'sigma': 0.0, 'stopping_threshold': None, 'spot_tensorboard_path': None, 'strategy': 'auto', 'target_column': None, 'target_type': None, 'task': None, 'tensorboard_log': False, 'tensorboard_start': False, 'tensorboard_stop': False, 'test': None, 'test_seed': 1234, 'test_size': 0.4, 'tkagg': False, 'tolerance_x': 0, 'train': None, 'upper': array([1]), 'var_name': None, 'var_type': ['num'], 'verbosity': 0, 'weights': 1.0, 'weight_coeff': 0.0, 'weights_entry': None}"
        ],
        [
         "11",
         "fun_evals",
         "15"
        ],
        [
         "12",
         "fun_repeats",
         "1"
        ],
        [
         "13",
         "ident",
         "[False]"
        ],
        [
         "14",
         "infill_criterion",
         "y"
        ],
        [
         "15",
         "k",
         "1"
        ],
        [
         "16",
         "log_level",
         "50"
        ],
        [
         "17",
         "lower",
         "[-1]"
        ],
        [
         "18",
         "max_surrogate_points",
         "30"
        ],
        [
         "19",
         "max_time",
         "1"
        ],
        [
         "20",
         "mean_X",
         null
        ],
        [
         "21",
         "mean_y",
         null
        ],
        [
         "22",
         "min_X",
         "[7.91924752e-06]"
        ],
        [
         "23",
         "min_mean_X",
         null
        ],
        [
         "24",
         "min_mean_y",
         null
        ],
        [
         "25",
         "min_y",
         "6.271448121765632e-11"
        ],
        [
         "26",
         "n_points",
         "1"
        ],
        [
         "27",
         "noise",
         "False"
        ],
        [
         "28",
         "ocba_delta",
         "0"
        ],
        [
         "29",
         "optimizer_control",
         "{'max_iter': 1000, 'seed': 125}"
        ],
        [
         "30",
         "progress_file",
         null
        ],
        [
         "31",
         "red_dim",
         "False"
        ],
        [
         "32",
         "rng",
         "Generator(PCG64)"
        ],
        [
         "33",
         "show_models",
         "False"
        ],
        [
         "34",
         "show_progress",
         "True"
        ],
        [
         "35",
         "spot_writer",
         null
        ],
        [
         "36",
         "surrogate",
         "Kriging(counter=9, eps=np.float64(1.4901161193847656e-08), max_Lambda=0,\n        min_Lambda=-9, model_fun_evals=10000,\n        model_optimizer=<function differential_evolution at 0x16a88c860>,\n        n_theta=1)"
        ],
        [
         "37",
         "surrogate_control",
         "{'log_level': 50, 'method': 'regression', 'model_optimizer': <function differential_evolution at 0x16a88c860>, 'model_fun_evals': 10000, 'min_theta': -3.0, 'max_theta': 2.0, 'n_theta': 1, 'p_val': 2.0, 'n_p': 1, 'optim_p': False, 'min_Lambda': -9, 'max_Lambda': 0, 'seed': 124, 'theta_init_zero': False, 'var_type': ['num'], 'metric_factorial': 'canberra'}"
        ],
        [
         "38",
         "tkagg",
         "False"
        ],
        [
         "39",
         "tolerance_x",
         "0"
        ],
        [
         "40",
         "upper",
         "[1]"
        ],
        [
         "41",
         "var_name",
         "['x0']"
        ],
        [
         "42",
         "var_type",
         "['num']"
        ],
        [
         "43",
         "var_y",
         null
        ],
        [
         "44",
         "verbosity",
         "0"
        ],
        [
         "45",
         "y",
         "[4.03601355e-03 1.68727231e-01 1.94233894e-03 9.27610994e-01\n 6.97527604e-01 1.31347460e-01 1.72499840e-01 1.18807832e-01\n 4.04558442e-01 6.05250607e-01 1.01551040e-09 2.48885744e-09\n 2.88598843e-09 3.77293295e-09 6.27144812e-11]"
        ],
        [
         "46",
         "y_mo",
         null
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 47
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute Name</th>\n",
       "      <th>Attribute Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X</td>\n",
       "      <td>[[0.06352962735037115], [-0.41076420376044454]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_lower</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_upper</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all_var_name</td>\n",
       "      <td>[x0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all_var_type</td>\n",
       "      <td>[num]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>counter</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>de_bounds</td>\n",
       "      <td>[[-1, 1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>design</td>\n",
       "      <td>&lt;spotpython.design.spacefilling.SpaceFilling o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>design_control</td>\n",
       "      <td>{'init_size': 10, 'repeats': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eps</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fun_control</td>\n",
       "      <td>{'PREFIX': '000', 'CHECKPOINT_PATH': 'runs/sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fun_evals</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fun_repeats</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ident</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>infill_criterion</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>k</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>log_level</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lower</td>\n",
       "      <td>[-1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>max_surrogate_points</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>max_time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean_X</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mean_y</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min_X</td>\n",
       "      <td>[7.919247515872725e-06]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>min_mean_X</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>min_mean_y</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>min_y</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>n_points</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>noise</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ocba_delta</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>optimizer_control</td>\n",
       "      <td>{'max_iter': 1000, 'seed': 125}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>progress_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>red_dim</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rng</td>\n",
       "      <td>Generator(PCG64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>show_models</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>show_progress</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>spot_writer</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>surrogate</td>\n",
       "      <td>Kriging(counter=9, eps=np.float64(1.4901161193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>surrogate_control</td>\n",
       "      <td>{'log_level': 50, 'method': 'regression', 'mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tkagg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tolerance_x</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>upper</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>var_name</td>\n",
       "      <td>[x0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>var_type</td>\n",
       "      <td>[num]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>var_y</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>verbosity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>y</td>\n",
       "      <td>[0.004036013551277026, 0.168727231090952, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>y_mo</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Attribute Name                                    Attribute Value\n",
       "0                      X  [[0.06352962735037115], [-0.41076420376044454]...\n",
       "1              all_lower                                               [-1]\n",
       "2              all_upper                                                [1]\n",
       "3           all_var_name                                               [x0]\n",
       "4           all_var_type                                              [num]\n",
       "5                counter                                                 15\n",
       "6              de_bounds                                          [[-1, 1]]\n",
       "7                 design  <spotpython.design.spacefilling.SpaceFilling o...\n",
       "8         design_control                    {'init_size': 10, 'repeats': 1}\n",
       "9                    eps                                                0.0\n",
       "10           fun_control  {'PREFIX': '000', 'CHECKPOINT_PATH': 'runs/sav...\n",
       "11             fun_evals                                                 15\n",
       "12           fun_repeats                                                  1\n",
       "13                 ident                                            [False]\n",
       "14      infill_criterion                                                  y\n",
       "15                     k                                                  1\n",
       "16             log_level                                                 50\n",
       "17                 lower                                               [-1]\n",
       "18  max_surrogate_points                                                 30\n",
       "19              max_time                                                  1\n",
       "20                mean_X                                               None\n",
       "21                mean_y                                               None\n",
       "22                 min_X                            [7.919247515872725e-06]\n",
       "23            min_mean_X                                               None\n",
       "24            min_mean_y                                               None\n",
       "25                 min_y                                                0.0\n",
       "26              n_points                                                  1\n",
       "27                 noise                                              False\n",
       "28            ocba_delta                                                  0\n",
       "29     optimizer_control                    {'max_iter': 1000, 'seed': 125}\n",
       "30         progress_file                                               None\n",
       "31               red_dim                                              False\n",
       "32                   rng                                   Generator(PCG64)\n",
       "33           show_models                                              False\n",
       "34         show_progress                                               True\n",
       "35           spot_writer                                               None\n",
       "36             surrogate  Kriging(counter=9, eps=np.float64(1.4901161193...\n",
       "37     surrogate_control  {'log_level': 50, 'method': 'regression', 'mod...\n",
       "38                 tkagg                                              False\n",
       "39           tolerance_x                                                  0\n",
       "40                 upper                                                [1]\n",
       "41              var_name                                               [x0]\n",
       "42              var_type                                              [num]\n",
       "43                 var_y                                               None\n",
       "44             verbosity                                                  0\n",
       "45                     y  [0.004036013551277026, 0.168727231090952, 0.00...\n",
       "46                  y_mo                                               None"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun = Analytical().fun_sphere\n",
    "fun_control=fun_control_init(lower = np.array([-1]),\n",
    "                             upper = np.array([1]))\n",
    "S = Spot(fun=fun, fun_control=fun_control)\n",
    "S.run()\n",
    "S.get_spot_attributes_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_level': 50,\n",
       " 'method': 'regression',\n",
       " 'model_optimizer': <function scipy.optimize._differentialevolution.differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=1000, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0, updating='immediate', workers=1, constraints=(), x0=None, *, integrality=None, vectorized=False)>,\n",
       " 'model_fun_evals': 10000,\n",
       " 'min_theta': -3.0,\n",
       " 'max_theta': 2.0,\n",
       " 'n_theta': 1,\n",
       " 'p_val': 2.0,\n",
       " 'n_p': 1,\n",
       " 'optim_p': False,\n",
       " 'min_Lambda': -9,\n",
       " 'max_Lambda': 0,\n",
       " 'seed': 124,\n",
       " 'theta_init_zero': False,\n",
       " 'var_type': ['num'],\n",
       " 'metric_factorial': 'canberra'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.surrogate_control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_name: light\n",
      "submodule_name: regression\n",
      "model_name: NNLinearRegressor\n",
      "Start _surrogate_control_setup: n_theta is 1\n",
      "Leaving _surrogate_control_setup: n_theta is 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'log_level': 50,\n",
       " 'method': 'regression',\n",
       " 'model_optimizer': <function scipy.optimize._differentialevolution.differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=1000, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0, updating='immediate', workers=1, constraints=(), x0=None, *, integrality=None, vectorized=False)>,\n",
       " 'model_fun_evals': 10000,\n",
       " 'min_theta': -3.0,\n",
       " 'max_theta': 2.0,\n",
       " 'n_theta': 1,\n",
       " 'p_val': 2.0,\n",
       " 'n_p': 1,\n",
       " 'optim_p': False,\n",
       " 'min_Lambda': -9,\n",
       " 'max_Lambda': 0,\n",
       " 'seed': 124,\n",
       " 'theta_init_zero': False,\n",
       " 'var_type': ['int',\n",
       "  'int',\n",
       "  'int',\n",
       "  'factor',\n",
       "  'factor',\n",
       "  'float',\n",
       "  'float',\n",
       "  'int',\n",
       "  'factor',\n",
       "  'factor'],\n",
       " 'metric_factorial': 'canberra'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun_control = fun_control_init(\n",
    "    PREFIX=\"S_03\",\n",
    "    max_time=1,\n",
    "    data_set = Diabetes(),\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    divergence_threshold=5_000,\n",
    "    _L_in=10,\n",
    "    _L_out=1)\n",
    "\n",
    "S_3 = Spot(fun=HyperLight().fun,\n",
    "         fun_control=fun_control)\n",
    "S_3.surrogate_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_3.get_spot_attributes_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spot312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
