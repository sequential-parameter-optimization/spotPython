{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "title: 'spotPython Tests'\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# fun_control_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n",
        "fun_control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def class_attributes_to_dataframe(class_obj):\n",
        "    # Get the attributes and their values of the class object\n",
        "    attributes = [attr for attr in dir(class_obj) if not callable(getattr(class_obj, attr)) and not attr.startswith(\"__\")]\n",
        "    values = [getattr(class_obj, attr) for attr in attributes]\n",
        "    \n",
        "    # Create a DataFrame from the attributes and values\n",
        "    df = pd.DataFrame({'Attribute Name': attributes, 'Attribute Value': values})\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "class MyClass:\n",
        "    def __init__(self):\n",
        "        self.name = \"John\"\n",
        "        self.age = 30\n",
        "        self.salary = 50000\n",
        "\n",
        "my_instance = MyClass()\n",
        "df = class_attributes_to_dataframe(my_instance)\n",
        "print(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import inf\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "from spotPython.spot import spot\n",
        "# number of initial points:\n",
        "ni = 7\n",
        "# number of points\n",
        "n = 10\n",
        "\n",
        "fun = analytical().fun_sphere\n",
        "lower = np.array([-1])\n",
        "upper = np.array([1])\n",
        "design_control={\"init_size\": ni}\n",
        "\n",
        "spot_1 = spot.Spot(fun=fun,\n",
        "            lower = lower,\n",
        "            upper= upper,\n",
        "            fun_evals = n,\n",
        "            show_progress=True,\n",
        "            design_control=design_control,)\n",
        "spot_1.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sys import stdout\n",
        "df = spot_1.class_attributes_to_dataframe()\n",
        "stdout.write(df.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from river import datasets\n",
        "from river import evaluate\n",
        "from river.linear_model import LogisticRegression\n",
        "from river import metrics\n",
        "from river import optim\n",
        "from river import preprocessing\n",
        "\n",
        "dataset = datasets.Phishing()\n",
        "\n",
        "model = (\n",
        "    preprocessing.StandardScaler() |\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "metric = metrics.Accuracy()\n",
        "\n",
        "evaluate.progressive_val_score(dataset, model, metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.csvdataset import CSVDataset\n",
        "# dataset = CSVDataset(csv_file='./data/spotPython/data.csv', target_column='prognosis')\n",
        "dataset = CSVDataset(target_column='prognosis')\n",
        "print(dataset.data.shape)\n",
        "print(dataset.targets.shape)            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.extra_repr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 3\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSV Data set VBDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the csv_file='./data/spotPython/data.csv' as a pandas df and save it as a pickle file\n",
        "import pandas as pd\n",
        "df = pd.read_csv('./data/spotPython/data.csv')\n",
        "df.to_pickle('./data/spotPython/data.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyHcf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyhcf.data.daten_sensitive import DatenSensitive\n",
        "from pyhcf.utils.names import get_short_parameter_names\n",
        "daten = DatenSensitive()\n",
        "df = daten.load()\n",
        "names =  df.columns\n",
        "names = get_short_parameter_names(names)\n",
        "# rename columns with short names\n",
        "df.columns = names\n",
        "df.head()\n",
        "# save the df as a csv file\n",
        "df.to_csv('./data/spotPython/data_sensitive.csv', index=False)\n",
        "# save the df as a pickle file\n",
        "df.to_pickle('./data/spotPython/data_sensitive.pkl')\n",
        "# remove all rows with NaN values\n",
        "df = df.dropna()\n",
        "# save the df as a csv file\n",
        "df.to_csv('./data/spotPython/data_sensitive_rmNA.csv', index=False)\n",
        "# save the df as a pickle file\n",
        "df.to_pickle('./data/spotPython/data_sensitive_rmNA.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyHcf data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from spotPython.light.csvdataset import CSVDataset\n",
        "# import torch\n",
        "# dataset = CSVDataset(csv_file='./data/spotPython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# # Set batch size for DataLoader\n",
        "# batch_size = 5000\n",
        "# # Create DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Iterate over the data in the DataLoader\n",
        "# for batch in dataloader:\n",
        "#     inputs, targets = batch\n",
        "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
        "#     print(\"---------------\")\n",
        "#     # print(f\"Inputs: {inputs}\")\n",
        "#     print(f\"Targets: {targets}\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from spotPython.light.csvdataset import CSVDataset\n",
        "# import torch\n",
        "# dataset = CSVDataset(csv_file='./data/spotPython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# # Set batch size for DataLoader\n",
        "# batch_size = 5000\n",
        "# # Create DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Iterate over the data in the DataLoader\n",
        "# for batch in dataloader:\n",
        "#     inputs, targets = batch\n",
        "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
        "#     print(\"---------------\")\n",
        "#     # print(f\"Inputs: {inputs}\")\n",
        "#     print(f\"Targets: {targets}\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pickle data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
        "dataset.feature_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Sensitive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = PKLDataset(pkl_file='./data/spotPython/data_sensitive.pkl', target_column='A', feature_type=torch.long, rmNA=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# # Set batch size for DataLoader\n",
        "# batch_size = 5\n",
        "# # Create DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Iterate over the data in the DataLoader\n",
        "# for batch in dataloader:\n",
        "#     inputs, targets = batch\n",
        "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
        "#     print(\"---------------\")\n",
        "#     print(f\"Inputs: {inputs}\")\n",
        "#     print(f\"Targets: {targets}\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/spotPython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test lightdatamodule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "from spotPython.data.csvdataset import CSVDataset\n",
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "# dataset = PKLDataset(directory=\"./data/spotPython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_module.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Training set size: {len(data_module.data_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Validation set size: {len(data_module.data_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Test set size: {len(data_module.data_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set the DataModule in fun_control "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "from spotPython.data.csvdataset import CSVDataset\n",
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "fun_control = fun_control_init()\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=7)\n",
        "dm.setup()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                key=\"data_module\",\n",
        "                value=dm, replace=True)\n",
        "data_module = fun_control[\"data_module\"]\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## same with the sensitive data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "fun_control = fun_control_init()\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/spotPython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
        "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
        "dm.setup()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                key=\"data_module\",\n",
        "                value=dm, replace=True)\n",
        "data_module = fun_control[\"data_module\"]\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## same, but VBDO data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "from spotPython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "fun_control = fun_control_init()\n",
        "dataset = CSVDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/VBDP/\", filename=\"train.csv\",target_column='prognosis', feature_type=torch.long)\n",
        "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
        "dm.setup()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                key=\"data_module\",\n",
        "                value=dm, replace=True)\n",
        "data_module = fun_control[\"data_module\"]\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# load Hyperdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "lhd = LightHyperDict()\n",
        "lhd.hyper_dict\n",
        "user_lhd = LightHyperDict(filename=\"user_hyper_dict.json\", directory=\"./hyperdict/\")\n",
        "user_lhd.hyper_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diabetes data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes  \n",
        "import torch\n",
        "\n",
        "# Load the diabetes dataset\n",
        "feature_df, target_df = load_diabetes(return_X_y=True, as_frame=True)\n",
        "feature_tensor = torch.tensor(feature_df.values, dtype=torch.float32)\n",
        "target_tensor = torch.tensor(target_df.values, dtype=torch.float32)\n",
        "feature_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.diabetes import Diabetes\n",
        "dataset = Diabetes()\n",
        "print(dataset.data.shape)\n",
        "print(dataset.targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# add core model to fun control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.netlightregressione import NetLightRegression\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "fun_control[\"core_model\"].__name__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if the fun_control[\"core_model_hyper_dict\"] is a LightHyperDict\n",
        "isinstance(fun_control[\"core_model_hyper_dict\"], dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test check_X_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.light.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotPython.fun.hyperlight import HyperLight\n",
        "from spotPython.hyperparameters.values import get_var_name\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "hyper_light = HyperLight(seed=126, log_level=50)\n",
        "n_hyperparams = len(get_var_name(fun_control))\n",
        "# generate a random np.array X with shape (2, n_hyperparams)\n",
        "X = np.random.rand(2, n_hyperparams)\n",
        "X == hyper_light.check_X_shape(X, fun_control)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test hyperlight fun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.light.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
        "from spotPython.fun.hyperlight import HyperLight\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "import numpy as np\n",
        "fun_control = fun_control_init(\n",
        "    _L_in=10,\n",
        "    _L_out=1,)\n",
        "\n",
        "dataset = Diabetes()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                    key=\"data_set\",\n",
        "                    value=dataset)\n",
        "\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "hyper_light = HyperLight(seed=126, log_level=50)\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
        "X = np.vstack((X, X))\n",
        "y = hyper_light.fun(X, fun_control)\n",
        "y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test  NetLightRegression Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "from spotPython.light.netlightregression import NetLightRegression\n",
        "from torch import nn\n",
        "import lightning as L\n",
        "PATH_DATASETS = './data'\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "dataset = Diabetes()\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "batch_x, batch_y = next(iter(train_loader)) \n",
        "print(batch_x.shape)\n",
        "print(batch_y.shape)\n",
        "\n",
        "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
        "                                initialization='xavier', act_fn=nn.ReLU(),\n",
        "                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n",
        "                                patience=5, _L_in=10, _L_out=1)\n",
        "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
        "trainer.fit(net_light_base, train_loader)\n",
        "trainer.validate(net_light_base, val_loader)\n",
        "trainer.test(net_light_base, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# tests optimizer_handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "from spotPython.light.netlightregression import NetLightRegression\n",
        "from torch import nn\n",
        "import lightning as L\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "lr_mult=0.1\n",
        "\n",
        "dataset = Diabetes()\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
        "                                initialization='xavier', act_fn=nn.ReLU(),\n",
        "                                optimizer='Adam', dropout_prob=0.1, lr_mult=lr_mult,\n",
        "                                patience=5, _L_in=10, _L_out=1)\n",
        "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
        "trainer.fit(net_light_base, train_loader)\n",
        "# Adam uses a lr which is calculated as lr=lr_mult * 0.001, so this value\n",
        "# should be 0.1 * 0.001 = 0.0001 \n",
        "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*0.001\n",
        "\n",
        "\n",
        "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
        "                                initialization='xavier', act_fn=nn.ReLU(),\n",
        "                                optimizer='Adadelta', dropout_prob=0.1, lr_mult=lr_mult,\n",
        "                                patience=5, _L_in=10, _L_out=1)\n",
        "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
        "trainer.fit(net_light_base, train_loader)\n",
        "# Adadelta uses a lr which is calculated as lr=lr_mult * 1.0, so this value\n",
        "# should be 1.0 * 0.1 = 0.1 \n",
        "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test train_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.light.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "from spotPython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
        "from spotPython.light.traintest import train_model, test_model\n",
        "fun_control = fun_control_init(\n",
        "    _L_in=10,\n",
        "    _L_out=1,)\n",
        "\n",
        "dataset = Diabetes()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset)\n",
        "\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "var_dict = assign_values(X, get_var_name(fun_control))\n",
        "for config in generate_one_config_from_var_dict(var_dict, fun_control):\n",
        "    y_train = train_model(config, fun_control)\n",
        "    y_test = test_model(config, fun_control)\n",
        "    break\n",
        "print(y_train)\n",
        "print(y_test[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.light.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "from spotPython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
        "from spotPython.light.traintest import test_model\n",
        "\n",
        "\n",
        "def test_traintest_test_model():\n",
        "    fun_control = fun_control_init(\n",
        "        _L_in=10,\n",
        "        _L_out=1,)\n",
        "\n",
        "    dataset = Diabetes()\n",
        "    set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset)\n",
        "\n",
        "    add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                                fun_control=fun_control,\n",
        "                                hyper_dict=LightHyperDict)\n",
        "    X = get_default_hyperparameters_as_array(fun_control)\n",
        "    var_dict = assign_values(X, get_var_name(fun_control))\n",
        "    for vals in generate_one_config_from_var_dict(var_dict, fun_control):\n",
        "        y_test = test_model(test_config=vals,\n",
        "                            fun_control=fun_control)\n",
        "        break\n",
        "    # check if y is a float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# test getVarName()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperparameters.values import get_var_name\n",
        "fun_control = {\"core_model_hyper_dict\":{\n",
        "            \"leaf_prediction\": {\n",
        "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": \"mean\",\n",
        "                \"core_model_parameter_type\": \"str\"},\n",
        "            \"leaf_model\": {\n",
        "                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": \"LinearRegression\",\n",
        "                \"core_model_parameter_type\": \"instance\"},\n",
        "            \"splitter\": {\n",
        "                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": \"EBSTSplitter\",\n",
        "                \"core_model_parameter_type\": \"instance()\"},\n",
        "            \"binary_split\": {\n",
        "                \"levels\": [0, 1],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": 0,\n",
        "                \"core_model_parameter_type\": \"bool\"},\n",
        "            \"stop_mem_management\": {\n",
        "                \"levels\": [0, 1],\n",
        "                \"type\": \"factor\",\n",
        "                \"default\": 0,\n",
        "                \"core_model_parameter_type\": \"bool\"}}}\n",
        "len(get_var_name(fun_control))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test netlightregression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from spotPython.spot import spot\n",
        "from math import inf\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
        "from spotPython.utils.device import getDevice\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "from spotPython.data.pkldataset import PKLDataset\n",
        "from spotPython.light.regression.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n",
        "from spotPython.hyperparameters.values import modify_hyper_parameter_levels\n",
        "from spotPython.fun.hyperlight import HyperLight\n",
        "from spotPython.hyperparameters.values import (get_bound_values,\n",
        "    get_var_name,\n",
        "    get_var_type,)\n",
        "from spotPython.utils.eda import gen_design_table\n",
        "from spotPython.hyperparameters.values import get_tuned_architecture\n",
        "from spotPython.light.testmodel import test_model\n",
        "from spotPython.light.loadmodel import load_light_from_checkpoint\n",
        "\n",
        "MAX_TIME = 1\n",
        "INIT_SIZE = 5\n",
        "WORKERS = 0\n",
        "PREFIX=\"031\"\n",
        "\n",
        "experiment_name = get_experiment_name(prefix=PREFIX)\n",
        "fun_control = fun_control_init(\n",
        "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
        "    num_workers=WORKERS,\n",
        "    device=getDevice(),\n",
        "    _L_in=133,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True)\n",
        "\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/spotPython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset)\n",
        "\n",
        "\n",
        "\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[5,8])\n",
        "modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[3,5])\n",
        "modify_hyper_parameter_bounds(fun_control, \"batch_size\", bounds=[2, 8])\n",
        "modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
        "\n",
        "print(gen_design_table(fun_control))\n",
        "\n",
        "var_type = get_var_type(fun_control)\n",
        "var_name = get_var_name(fun_control)\n",
        "lower = get_bound_values(fun_control, \"lower\")\n",
        "upper = get_bound_values(fun_control, \"upper\")\n",
        "fun = HyperLight(log_level=50).fun\n",
        "spot_tuner = spot.Spot(fun=fun,\n",
        "                       log_level=50,\n",
        "                   lower = lower,\n",
        "                   upper = upper,\n",
        "                   fun_evals = inf,\n",
        "                   max_time = MAX_TIME,\n",
        "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                   var_type = var_type,\n",
        "                   var_name = var_name,\n",
        "                   show_progress= True,\n",
        "                   fun_control = fun_control,\n",
        "                   design_control={\"init_size\": INIT_SIZE},\n",
        "                   surrogate_control={\"noise\": True,\n",
        "                                      \"min_theta\": -4,\n",
        "                                      \"max_theta\": 3,\n",
        "                                      \"n_theta\": len(var_name),\n",
        "                                      \"model_fun_evals\": 10_000,\n",
        "                                      })\n",
        "spot_tuner.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_progress(log_y=False, filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_importance(threshold=0.025, filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_tuned_architecture(spot_tuner, fun_control)\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_model(config, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_loaded = load_light_from_checkpoint(config, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.plot_important_hyperparameter_contour(filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.parallel_plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.cvmodel import cv_model\n",
        "# set the number of folds to 10\n",
        "fun_control[\"k_folds\"] = 10\n",
        "cv_model(config, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# fun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.fun.objectivefunctions import analytical\n",
        "import numpy as np\n",
        "y = np.array([1, 2, 3, 4, 5])\n",
        "fun = analytical(sigma=1.0, seed=123)\n",
        "fun.add_noise(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.fun.objectivefunctions import analytical\n",
        "import numpy as np\n",
        "print(np.array([1, 2, 3, 4, 5]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import inf\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "from spotPython.spot import spot\n",
        "from scipy.optimize import shgo\n",
        "from scipy.optimize import direct\n",
        "from scipy.optimize import differential_evolution\n",
        "import matplotlib.pyplot as plt\n",
        "from spotPython.utils.init import fun_control_init\n",
        "fun_control = fun_control_init(seed=4321, sigma=0.1)\n",
        "fun = analytical(seed=222, sigma=0.0).fun_sphere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_1 = spot.Spot(fun=fun,\n",
        "                   lower = np.array([-10]),\n",
        "                   upper = np.array([100]),\n",
        "                   fun_evals = 100,\n",
        "                   fun_repeats = 3,\n",
        "                   max_time = inf,\n",
        "                   noise = True,\n",
        "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                   var_type=[\"num\"],\n",
        "                   infill_criterion = \"y\",\n",
        "                   n_points = 1,\n",
        "                   seed=111,\n",
        "                   log_level = 10,\n",
        "                   show_models=False,\n",
        "                   fun_control = fun_control,\n",
        "                   design_control={\"init_size\": 5,\n",
        "                                   \"repeats\": 1},\n",
        "                   surrogate_control={\"noise\": True,\n",
        "                                      \"cod_type\": \"norm\",\n",
        "                                      \"min_theta\": -4,\n",
        "                                      \"max_theta\": 3,\n",
        "                                      \"n_theta\": 1,\n",
        "                                      \"model_optimizer\": differential_evolution,\n",
        "                                      \"model_fun_evals\": 1000,\n",
        "                                      })\n",
        "spot_1.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def squared_euclidean_distance(X_0, X, theta):\n",
        "    return np.sum(theta*(X_0 - X)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
        "from numpy.linalg import cholesky, solve\n",
        "from numpy.random import multivariate_normal\n",
        "def build_Psi(X, theta):\n",
        "    n = X.shape[0]\n",
        "    k = X.shape[1]\n",
        "    D = zeros((k, n, n))\n",
        "    for l in range(k):\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
        "    D = sum(D)\n",
        "    D = D + D.T\n",
        "    return exp(-D)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "theta = np.array([1.0, 1.0])\n",
        "X = np.array([[1.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n",
        "print(X.shape)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "build_Psi(X, theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.fun.objectivefunctions import analytical\n",
        "import numpy as np\n",
        "X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n",
        "fun = analytical()\n",
        "fun.fun_branin_factor(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "pi = np.pi\n",
        "X = np.array([[0,0], [-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
        "fun = analytical()\n",
        "fun.fun_branin(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.fun.objectivefunctions import analytical\n",
        "import numpy as np\n",
        "pi = np.pi\n",
        "X_0 = np.array([[0, 0]])\n",
        "X_1 = np.array([[-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
        "X_2 = np.array([[0,0,0], [0,0,1], [0,0,2]])\n",
        "fun = analytical()\n",
        "y_0 = fun.fun_branin(X_0)\n",
        "y_1 = fun.fun_branin(X_1)\n",
        "y_2 = fun.fun_branin_factor(X_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "round(y_1[0], 2) == round(y_1[1],2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "round(y_1[0], 2) == round(y_1[2],2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_2[0] == y_0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_2[1] == y_0 + 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_2[2] == y_0 - 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy.random import multivariate_normal\n",
        "import numpy as np\n",
        "n = 100\n",
        "X = np.linspace(0, 10, n, endpoint=False).reshape(-1,1)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
        "from numpy.linalg import cholesky, solve\n",
        "from numpy.random import multivariate_normal\n",
        "def build_Psi(X, theta):\n",
        "    n = X.shape[0]\n",
        "    k = X.shape[1]\n",
        "    D = zeros((k, n, n))\n",
        "    for l in range(k):\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
        "    D = sum(D)\n",
        "    D = D + D.T\n",
        "    return exp(-D)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "theta = np.array([1.0])\n",
        "Psi = build_Psi(X, theta)\n",
        "np.round(Psi[:3,:], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = (3, 1, 1), check_valid=\"raise\")\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert Y to a 3 x 100 array\n",
        "Y = np.squeeze(Y)\n",
        "Y.shape\n",
        "Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot 3 samples from the GP as a function of X\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X, Y.T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = 3, check_valid=\"raise\")\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot 3 samples from the GP as a function of X\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X, Y.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/spotPython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(f\"Inputs Shape: {inputs.shape}\")\n",
        "    print(f\"Targets Shape: {targets.shape}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test HyperLight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.light.regression.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotPython.fun.hyperlight import HyperLight\n",
        "from spotPython.hyperparameters.values import get_var_name\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                            fun_control=fun_control,\n",
        "                            hyper_dict=LightHyperDict)\n",
        "hyper_light = HyperLight(seed=126, log_level=50)\n",
        "n_hyperparams = len(get_var_name(fun_control))\n",
        "# generate a random np.array X with shape (2, n_hyperparams)\n",
        "X = np.random.rand(2, n_hyperparams)\n",
        "X == hyper_light.check_X_shape(X, fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.light.regression.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import (add_core_model_to_fun_control,\n",
        "    get_default_hyperparameters_as_array)\n",
        "from spotPython.fun.hyperlight import HyperLight\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "import numpy as np\n",
        "fun_control = fun_control_init(\n",
        "    _L_in=10,\n",
        "    _L_out=1,)\n",
        "dataset = Diabetes()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset)\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                            fun_control=fun_control,\n",
        "                            hyper_dict=LightHyperDict)\n",
        "hyper_light = HyperLight(seed=126, log_level=50)\n",
        "X = get_default_hyperparameters_as_array(fun_control)\n",
        "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
        "# so that two values are returned\n",
        "X = np.vstack((X, X))\n",
        "hyper_light.fun(X, fun_control)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## test pkldataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.pkldataset import PKLDataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/spotPython/\",\n",
        "                    filename=\"data_sensitive.pkl\",\n",
        "                    target_column='N',\n",
        "                    feature_type=torch.float32,\n",
        "                    target_type=torch.float32,\n",
        "                    rmNA=True)\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytest\n",
        "import numpy as np\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperparameters.values import get_bound_values\n",
        "from spotPython.hyperparameters.values import get_control_key_value, set_control_key_value\n",
        "from spotPython.hyperparameters.values import get_var_type_from_var_name\n",
        "\n",
        "fun_control = fun_control_init()\n",
        "set_control_key_value(control_dict=fun_control, key=\"var_type\", value=[\"int\", \"float\", \"str\"], replace=True)\n",
        "set_control_key_value(control_dict=fun_control, key=\"var_name\", value=[\"max_depth\", \"learning_rate\", \"model_type\"], replace=True)\n",
        "\n",
        "print(fun_control)\n",
        "\n",
        "# Test with existing var_name\n",
        "assert get_var_type_from_var_name(var_name=\"max_depth\", fun_control=fun_control) == \"int\"\n",
        "assert get_var_type_from_var_name(var_name=\"learning_rate\", fun_control=fun_control) == \"float\"\n",
        "assert get_var_type_from_var_name(var_name=\"model_type\", fun_control=fun_control) == \"str\"\n",
        "\n",
        "# Test with non-existing var_name\n",
        "with pytest.raises(ValueError):\n",
        "    get_var_type_from_var_name(var_name=\"non_existing\", fun_control=fun_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperparameters.values import get_control_key_value\n",
        "from spotPython.light.regression.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotPython.hyperparameters.values import get_var_type_from_var_name\n",
        "\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                    fun_control=fun_control,\n",
        "                    hyper_dict=LightHyperDict)\n",
        "var_type = get_control_key_value(control_dict=fun_control, key=\"var_type\")\n",
        "var_name = get_control_key_value(control_dict=fun_control, key=\"var_name\")\n",
        "print(var_type)\n",
        "print(var_name)\n",
        "vn = \"l1\"\n",
        "get_var_type_from_var_name(fun_control=fun_control, var_name=vn)\n",
        "\n",
        "assert var_type[var_name.index(vn)] == \"int\"\n",
        "assert get_var_type_from_var_name(fun_control, vn) == \"int\"\n",
        "vn = \"initialization\"\n",
        "assert var_type[var_name.index(vn)] == \"factor\"\n",
        "assert var_type[var_name.index(vn)] == \"factor\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperparameters.values import get_control_key_value\n",
        "from spotPython.light.regression.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotPython.hyperparameters.values import set_control_hyperparameter_value\n",
        "\n",
        "fun_control = fun_control_init()\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                    fun_control=fun_control,\n",
        "                    hyper_dict=LightHyperDict)\n",
        "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"l1\", value=[1,7])\n",
        "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"initialization\", value=[\"xavier2\", \"kaiming2\"])\n",
        "print(fun_control)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## get names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_entry(dictionary, key, i):\n",
        "    if key in dictionary:\n",
        "        if 'levels' in dictionary[key]:\n",
        "            if i < len(dictionary[key]['levels']):\n",
        "                return dictionary[key]['levels'][i]\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from spotPython.data.pkldataset_intern import PKLDataset\n",
        "from spotPython.utils.device import getDevice\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
        "import numpy as np\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "from spotPython.light.regression.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotPython.utils.eda import gen_design_table\n",
        "from math import inf\n",
        "\n",
        "MAX_TIME = 60\n",
        "FUN_EVALS = inf\n",
        "INIT_SIZE = 25\n",
        "WORKERS = 0\n",
        "PREFIX=\"031\"\n",
        "DEVICE = getDevice()\n",
        "\n",
        "\n",
        "experiment_name = get_experiment_name(prefix=PREFIX)\n",
        "fun_control = fun_control_init(\n",
        "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    device=DEVICE,\n",
        "    enable_progress_bar=False,\n",
        "    fun_evals=FUN_EVALS,\n",
        "    log_level=10,\n",
        "    max_time=MAX_TIME,\n",
        "    num_workers=WORKERS,\n",
        "    show_progress=True,\n",
        "    tolerance_x=np.sqrt(np.spacing(1)),\n",
        "    )\n",
        "\n",
        "dataset = Diabetes()\n",
        "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotPython/notebooks/data/spotPython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True, rmMF=True)\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset,\n",
        "                        replace=True)\n",
        "\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"_L_in\",\n",
        "                        value=133,\n",
        "                        replace=True)\n",
        "\n",
        "\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=LightHyperDict)\n",
        "# from spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n",
        "\n",
        "from spotPython.hyperparameters.values import set_control_hyperparameter_value\n",
        "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
        "set_control_hyperparameter_value(fun_control, \"epochs\", [4,9])\n",
        "set_control_hyperparameter_value(fun_control, \"batch_size\", [1, 4])\n",
        "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fun_control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_entry(dictionary, key, i):\n",
        "    if 'core_model_hyper_dict' in dictionary:\n",
        "        if key in dictionary['core_model_hyper_dict']:\n",
        "            if 'levels' in dictionary['core_model_hyper_dict'][key]:\n",
        "                if i < len(dictionary['core_model_hyper_dict'][key]['levels']):\n",
        "                    return dictionary['core_model_hyper_dict'][key]['levels'][i]\n",
        "    return None\n",
        "print(get_entry(fun_control, \"optimizer\", 0)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.device import getDevice\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
        "import numpy as np\n",
        "from spotPython.data.diabetes import Diabetes\n",
        "from spotPython.light.regression.netlightregression import NetLightRegression\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "from spotPython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n",
        "from spotPython.hyperparameters.values import set_control_key_value\n",
        "from spotPython.hyperparameters.values import set_control_hyperparameter_value\n",
        "experiment_name = get_experiment_name(prefix=\"000\")\n",
        "fun_control = fun_control_init(\n",
        "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
        "    _L_in=10,\n",
        "    _L_out=1,\n",
        "    TENSORBOARD_CLEAN=True,\n",
        "    device=getDevice(),\n",
        "    enable_progress_bar=False,\n",
        "    fun_evals=15,\n",
        "    log_level=10,\n",
        "    max_time=1,\n",
        "    num_workers=0,\n",
        "    show_progress=True,\n",
        "    tolerance_x=np.sqrt(np.spacing(1)),\n",
        "    )\n",
        "dataset = Diabetes()\n",
        "set_control_key_value(control_dict=fun_control,\n",
        "                        key=\"data_set\",\n",
        "                        value=dataset,\n",
        "                        replace=True)\n",
        "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
        "                            fun_control=fun_control,\n",
        "                            hyper_dict=LightHyperDict)\n",
        "\n",
        "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
        "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
        "assert get_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0) == \"Adam\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def get_timestamp(only_int=True):\n",
        "    dt = datetime.datetime.now().isoformat(sep=\" \", timespec=\"microseconds\")\n",
        "    if only_int:\n",
        "        # remove - . : and space\n",
        "        dt = dt.replace(\"-\", \"\")\n",
        "        dt = dt.replace(\".\", \"\")\n",
        "        dt = dt.replace(\":\", \"\")\n",
        "        dt = dt.replace(\" \", \"\")\n",
        "    return dt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytest\n",
        "import numpy as np\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "from spotPython.spot import spot\n",
        "from spotPython.utils.init import (\n",
        "    fun_control_init, surrogate_control_init, design_control_init\n",
        ")\n",
        "\n",
        "def test_plot_progress():\n",
        "    # number of initial points:\n",
        "    ni = 7\n",
        "    # number of points\n",
        "    fun_evals = 10\n",
        "    fun = analytical().fun_sphere\n",
        "    fun_control = fun_control_init(\n",
        "        lower = np.array([-1, -1]),\n",
        "        upper = np.array([1, 1]),\n",
        "        fun_evals=fun_evals,\n",
        "        tolerance_x = np.sqrt(np.spacing(1))\n",
        "    )\n",
        "    design_control=design_control_init(init_size=ni)\n",
        "    surrogate_control=surrogate_control_init(n_theta=3)\n",
        "    S = spot.Spot(fun=fun,\n",
        "                    fun_control=fun_control,\n",
        "                    design_control=design_control,\n",
        "                    surrogate_control=surrogate_control,)\n",
        "    S.run()\n",
        "\n",
        "    # Test plot_progress with different parameters\n",
        "    S.plot_progress(show=False)  # Test with show=False\n",
        "    S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
        "    S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
        "    S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename\n",
        "    # add NaN to S.y at position 2\n",
        "    S.y[2] = np.nan\n",
        "    S.plot_progress(show=False)  # Test with show=False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytest\n",
        "import numpy as np\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "from spotPython.spot import spot\n",
        "from spotPython.utils.init import (\n",
        "    fun_control_init, surrogate_control_init, design_control_init\n",
        ")\n",
        "\n",
        "\n",
        "# number of initial points:\n",
        "ni = 7\n",
        "# number of points\n",
        "fun_evals = 10\n",
        "fun = analytical().fun_sphere\n",
        "fun_control = fun_control_init(\n",
        "    lower = np.array([-1, -1]),\n",
        "    upper = np.array([1, 1]),\n",
        "    fun_evals=fun_evals,\n",
        "    tolerance_x = np.sqrt(np.spacing(1))\n",
        ")\n",
        "design_control=design_control_init(init_size=ni)\n",
        "surrogate_control=surrogate_control_init(n_theta=3)\n",
        "S = spot.Spot(fun=fun,\n",
        "                fun_control=fun_control,\n",
        "                design_control=design_control,\n",
        "                surrogate_control=surrogate_control,)\n",
        "S.run()\n",
        "\n",
        "# remove points from S.y so that there are less than ni points\n",
        "S.y = S.y[:3]\n",
        "# Test plot_progress with different parameters\n",
        "S.plot_progress(show=False)  # Test with show=False\n",
        "S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
        "S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
        "S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import inf\n",
        "from spotPython.spot import spot\n",
        "from scipy.optimize import differential_evolution\n",
        "from spotPython.utils.init import (\n",
        "    fun_control_init,\n",
        "    design_control_init,\n",
        "    surrogate_control_init,\n",
        "    optimizer_control_init)\n",
        "def objective_function(X, fun_control=None):\n",
        "    if not isinstance(X, np.ndarray):\n",
        "        X = np.array(X)\n",
        "    if X.shape[1] != 2:\n",
        "        raise Exception\n",
        "    x0 = X[:, 0]\n",
        "    x1 = X[:, 1]\n",
        "    y = x0**2 + 10*x1**2\n",
        "    return y\n",
        "fun_control = fun_control_init(\n",
        "            lower = np.array([0, 0]),\n",
        "            upper = np.array([10, 10]),\n",
        "            fun_evals=8,\n",
        "            fun_repeats=1,\n",
        "            max_time=inf,\n",
        "            noise=True,\n",
        "            tolerance_x=0,\n",
        "            ocba_delta=0,\n",
        "            var_type=[\"num\", \"num\"],\n",
        "            infill_criterion=\"ei\",\n",
        "            n_points=1,\n",
        "            seed=123,\n",
        "            log_level=10,\n",
        "            show_models=False,\n",
        "            show_progress=True)\n",
        "design_control = design_control_init(\n",
        "            init_size=5,\n",
        "            repeats=1)\n",
        "surrogate_control = surrogate_control_init(\n",
        "            log_level=10,\n",
        "            model_optimizer=differential_evolution,\n",
        "            model_fun_evals=10000,\n",
        "            min_theta=-3,\n",
        "            max_theta=3,\n",
        "            n_theta=2,\n",
        "            theta_init_zero=True,\n",
        "            n_p=1,\n",
        "            optim_p=False,\n",
        "            noise=True,\n",
        "            var_type=[\"num\", \"num\"],\n",
        "            seed=124, \n",
        "            min_Lambda=1,\n",
        "            max_Lambda=10)\n",
        "optimizer_control = optimizer_control_init(\n",
        "            max_iter=1000,\n",
        "            seed=125)\n",
        "spot = spot.Spot(fun=objective_function,\n",
        "            fun_control=fun_control,\n",
        "            design_control=design_control,\n",
        "            surrogate_control=surrogate_control,\n",
        "            optimizer_control=optimizer_control\n",
        "            )\n",
        "spot.run()\n",
        "spot.plot_progress()\n",
        "spot.plot_contour(i=0, j=1)\n",
        "spot.plot_importance()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from spotPython.spot import spot\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "from spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n",
        "\n",
        "fun = analytical().fun_branin\n",
        "fun_control = fun_control_init(lower = np.array([-5, 0]),\n",
        "                               upper = np.array([10, 15]),\n",
        "                               fun_evals=20)\n",
        "design_control = design_control_init(init_size=10)\n",
        "surrogate_control = surrogate_control_init(n_theta=2)\n",
        "S = spot.Spot(fun=fun, fun_control=fun_control, design_control=design_control)\n",
        "S.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "S.print_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "S.plot_progress(log_y=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "S.surrogate.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import inf\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "from spotPython.spot import spot\n",
        "from scipy.optimize import shgo\n",
        "from scipy.optimize import direct\n",
        "from scipy.optimize import differential_evolution\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fun = analytical().fun_sphere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.linspace(-1,1,100).reshape(-1,1)\n",
        "y = fun(x)\n",
        "plt.figure()\n",
        "plt.plot(x,y, \"k\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
        "spot_1 = spot.Spot(fun=fun,\n",
        "                   fun_control=fun_control_init(\n",
        "                        lower = np.array([-10]),\n",
        "                        upper = np.array([100]),\n",
        "                        fun_evals = 7,\n",
        "                        fun_repeats = 1,\n",
        "                        max_time = inf,\n",
        "                        noise = False,\n",
        "                        tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                        var_type=[\"num\"],\n",
        "                        infill_criterion = \"y\",\n",
        "                        n_points = 1,\n",
        "                        seed=123,\n",
        "                        log_level = 50),\n",
        "                   design_control=design_control_init(\n",
        "                        init_size=5,\n",
        "                        repeats=1),\n",
        "                   surrogate_control=surrogate_control_init(\n",
        "                        noise=False,\n",
        "                        min_theta=-4,\n",
        "                        max_theta=3,\n",
        "                        n_theta=1,\n",
        "                        model_optimizer=differential_evolution,\n",
        "                        model_fun_evals=10000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_1.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.fun.objectivefunctions import analytical\n",
        "fun = analytical().fun_sphere\n",
        "from spotPython.design.spacefilling import spacefilling\n",
        "design = spacefilling(2)\n",
        "from scipy.optimize import differential_evolution\n",
        "optimizer = differential_evolution\n",
        "from spotPython.build.kriging import Kriging\n",
        "surrogate = Kriging()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\n",
        "fun_control=fun_control_init(lower=np.array([-1, -1]),\n",
        "                            upper=np.array([1, 1]))\n",
        "design_control=design_control_init()\n",
        "optimizer_control=optimizer_control_init()\n",
        "surrogate_control=surrogate_control_init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.spot import spot\n",
        "spot_tuner = spot.Spot(fun=fun,\n",
        "                       fun_control=fun_control,\n",
        "                       design_control=design_control,\n",
        "                       optimizer_control=optimizer_control,\n",
        "                       surrogate_control=surrogate_control)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spot_tuner.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pytest\n",
        "import torch\n",
        "from pyhcf.data.loadHcfData import build_df, load_hcf_data\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_list=[\"L\", \"AQ\", \"AS\"]\n",
        "dataset = load_hcf_data(param_list=p_list, target=\"T\",\n",
        "                        rmNA=True, rmMF=True,\n",
        "                        load_all_features=False,\n",
        "                        load_thermo_features=False,\n",
        "                        scale_data=True,\n",
        "                        return_X_y=False)\n",
        "assert isinstance(dataset, torch.utils.data.TensorDataset)\n",
        "assert len(dataset) > 0\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 5\n",
        "# Create DataLoader    \n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    assert inputs.size(0) == batch_size\n",
        "    print(f\"Inputs Shape: {inputs.shape[1]}\")\n",
        "    print(f\"P List: {p_list}\")\n",
        "    print(f\"P List Length: {len(p_list)}\")\n",
        "    # input is p_list + 1 (for target)\n",
        "    # p_list = [\"L\", \"AQ\", \"AS\"] plus target \"N\"\n",
        "    assert inputs.shape[1] + 1 == len(p_list)\n",
        "    print(f\"Targets Shape: {targets.shape[0]}\")\n",
        "    assert targets.shape[0] == batch_size\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "from spotPython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "# data.csv is simple csv file with 11 samples\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "data_module.setup()\n",
        "print(f\"Training set size: {len(data_module.data_train)}\")\n",
        "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
        "print(f\"Test set size: {len(data_module.data_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.data.lightdatamodule import LightDataModule\n",
        "from spotPython.data.csvdataset import CSVDataset\n",
        "import torch\n",
        "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
        "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
        "data_module.setup(stage=\"predict\")\n",
        "print(f\"Predict set size: {len(data_module.data_predict)}\")\n",
        "for batch in data_module.predict_dataloader():\n",
        "    inputs, targets = batch\n",
        "    print(f\"inputs: {inputs}\")\n",
        "    print(f\"targets: {targets}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(data_module.data_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_div2_list(n, n_min):\n",
        "    result = []\n",
        "    current = n\n",
        "    while current >= n_min:\n",
        "        result.extend([current] * (n // current))\n",
        "        current = current // 2\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_L_in = 128\n",
        "l1 = \n",
        "\n",
        "n_low = _L_in // 4\n",
        "# ensure that n_high is larger than n_low\n",
        "n_high = max(l1, 2 * n_low)\n",
        "generate_div2_list(n_high, n_low)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.math import generate_div2_list\n",
        "generate_div2_list(64, 63)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.transformer.positionalEncoding import PositionalEncoding\n",
        "import torch\n",
        "# number of tensors\n",
        "n = 3\n",
        "# dimension of each tensor\n",
        "k = 32\n",
        "pe = PositionalEncoding(d_model=k, dropout_prob=0, verbose=False)\n",
        "input = torch.zeros(1, n, k)\n",
        "# Generate a tensor of size (1, 10, 4) with values from 1 to 10\n",
        "for i in range(n):\n",
        "    input[0, i, :] = i\n",
        "print(f\"Input shape: {input.shape}\")\n",
        "print(f\"Input: {input}\")\n",
        "output = pe(input)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output: {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.transformer.skiplinear import SkipLinear\n",
        "import torch\n",
        "n_in = 2\n",
        "n_out = 4\n",
        "sl = SkipLinear(n_in, n_out)\n",
        "input = torch.zeros(1, n_in)\n",
        "for i in range(n_in):\n",
        "    input[0, i] = i\n",
        "print(f\"Input shape: {input.shape}\")\n",
        "print(f\"Input: {input}\")\n",
        "output = sl(input)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output: {output}\")\n",
        "print(sl.lst_modules)\n",
        "for i in sl.lst_modules:\n",
        "    print(f\"weights: {i.weights}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Example from J. Caffrey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# people_income_transformer.py\n",
        "# predict income from sex, age, city, politics\n",
        "# PyTorch 2.0.0-CPU Anaconda3-2022.10  Python 3.9.13\n",
        "# Windows 10/11 \n",
        "\n",
        "# Transformer component for regression\n",
        "\n",
        "import numpy as np\n",
        "import torch as T\n",
        "\n",
        "device = T.device('cpu')  # apply to Tensor or Module\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class PeopleDataset(T.utils.data.Dataset):\n",
        "  def __init__(self, src_file):\n",
        "    # sex age   state   income   politics\n",
        "    # -1  0.27  0 1 0   0.7610   0 0 1\n",
        "    # +1  0.19  0 0 1   0.6550   1 0 0\n",
        "\n",
        "    tmp_x = np.loadtxt(src_file, usecols=[0,1,2,3,4,6,7,8],\n",
        "      delimiter=\",\", comments=\"#\", dtype=np.float32)\n",
        "    tmp_y = np.loadtxt(src_file, usecols=5, delimiter=\",\",\n",
        "      comments=\"#\", dtype=np.float32)\n",
        "    tmp_y = tmp_y.reshape(-1,1)  # 2D required\n",
        "\n",
        "    self.x_data = T.tensor(tmp_x, dtype=T.float32).to(device)\n",
        "    self.y_data = T.tensor(tmp_y, dtype=T.float32).to(device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    preds = self.x_data[idx]\n",
        "    incom = self.y_data[idx] \n",
        "    return (preds, incom)  # as a tuple\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class SkipLinear(T.nn.Module):\n",
        "\n",
        "  # -----\n",
        "\n",
        "  class Core(T.nn.Module):\n",
        "    def __init__(self, n):\n",
        "      super().__init__()\n",
        "      # 1 node to n nodes, n gte 2\n",
        "      self.weights = T.nn.Parameter(T.zeros((n,1),\n",
        "        dtype=T.float32))\n",
        "      self.biases = T.nn.Parameter(T.tensor(n,\n",
        "        dtype=T.float32))\n",
        "      lim = 0.01\n",
        "      T.nn.init.uniform_(self.weights, -lim, lim)\n",
        "      T.nn.init.zeros_(self.biases)\n",
        "\n",
        "    def forward(self, x):\n",
        "      wx= T.mm(x, self.weights.t())\n",
        "      v = T.add(wx, self.biases)\n",
        "      return v\n",
        "\n",
        "  # -----\n",
        "\n",
        "  def __init__(self, n_in, n_out):\n",
        "    super().__init__()\n",
        "    self.n_in = n_in; self.n_out = n_out\n",
        "    if n_out  % n_in != 0:\n",
        "      print(\"FATAL: n_out must be divisible by n_in\")\n",
        "    n = n_out // n_in  # num nodes per input\n",
        "\n",
        "    self.lst_modules = \\\n",
        "      T.nn.ModuleList([SkipLinear.Core(n) for \\\n",
        "        i in range(n_in)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    lst_nodes = []\n",
        "    for i in range(self.n_in):\n",
        "      xi = x[:,i].reshape(-1,1)\n",
        "      oupt = self.lst_modules[i](xi)\n",
        "      lst_nodes.append(oupt)\n",
        "    result = T.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
        "    for i in range(2,self.n_in):\n",
        "      result = T.cat((result, lst_nodes[i]), 1)\n",
        "    result = result.reshape(-1, self.n_out)\n",
        "    return result\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class PositionalEncoding(T.nn.Module):  # documentation code\n",
        "  def __init__(self, d_model: int, dropout: float=0.1,\n",
        "   max_len: int=5000):\n",
        "    super(PositionalEncoding, self).__init__()  # old syntax\n",
        "    self.dropout = T.nn.Dropout(p=dropout)\n",
        "    pe = T.zeros(max_len, d_model)  # like 10x4\n",
        "    position = \\\n",
        "      T.arange(0, max_len, dtype=T.float).unsqueeze(1)\n",
        "    div_term = T.exp(T.arange(0, d_model, 2).float() * \\\n",
        "      (-np.log(10_000.0) / d_model))\n",
        "    pe[:, 0::2] = T.sin(position * div_term)\n",
        "    pe[:, 1::2] = T.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "    self.register_buffer('pe', pe)  # allows state-save\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0), :]\n",
        "    return self.dropout(x)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class TransformerNet(T.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TransformerNet, self).__init__()\n",
        "    self.embed = SkipLinear(8, 32)  # 8 inputs, each goes to 4 \n",
        "    self.pos_enc = \\\n",
        "      PositionalEncoding(4, dropout=0.20)  # positional\n",
        "    self.enc_layer = T.nn.TransformerEncoderLayer(d_model=4,\n",
        "      nhead=2, dim_feedforward=10, \n",
        "      batch_first=True)  # d_model divisible by nhead\n",
        "    self.trans_enc = T.nn.TransformerEncoder(self.enc_layer,\n",
        "      num_layers=2)  # 6 layers default\n",
        "\n",
        "    self.fc1 = T.nn.Linear(32, 10)  # 8--32-T-10-1\n",
        "    self.fc2 = T.nn.Linear(10, 1)\n",
        "\n",
        "    # default weight and bias initialization\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.embed(x)  # 8 inpts to 32 embed\n",
        "    z = z.reshape(-1, 8, 4)  # bat seq embed\n",
        "    z = self.pos_enc(z) \n",
        "    z = self.trans_enc(z) \n",
        "    z = z.reshape(-1, 32)  # torch.Size([bs, xxx])\n",
        "    z = T.tanh(self.fc1(z))\n",
        "    z = self.fc2(z)  # regression: no activation\n",
        "    return z\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def accuracy(model, ds, pct_close):\n",
        "  # assumes model.eval()\n",
        "  # correct within pct of true income\n",
        "  n_correct = 0; n_wrong = 0\n",
        "\n",
        "  for i in range(len(ds)):\n",
        "    X = ds[i][0].reshape(1,-1)  # make it a batch\n",
        "    Y = ds[i][1].reshape(1)\n",
        "    with T.no_grad():\n",
        "      oupt = model(X)         # computed income\n",
        "\n",
        "    if T.abs(oupt - Y) <= T.abs(pct_close * Y):\n",
        "      n_correct += 1\n",
        "    else:\n",
        "      n_wrong += 1\n",
        "  acc = (n_correct * 1.0) / (n_correct + n_wrong)\n",
        "  return acc\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def accuracy_x(model, ds, pct_close):\n",
        "  # all-at-once (quick)\n",
        "  # assumes model.eval()\n",
        "  X = ds.x_data  # all inputs\n",
        "  Y = ds.y_data  # all targets\n",
        "  n_items = len(X)\n",
        "  with T.no_grad():\n",
        "    pred = model(X)  # all predicted incomes\n",
        " \n",
        "  n_correct = T.sum((T.abs(pred - Y) <= \\\n",
        "    T.abs(pct_close * Y)))\n",
        "  result = (n_correct.item() / n_items)  # scalar\n",
        "  return result  \n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def train(model, ds, bs, lr, me, le, test_ds):\n",
        "  # dataset, bat_size, lrn_rate, max_epochs, log interval\n",
        "  train_ldr = T.utils.data.DataLoader(ds, batch_size=bs,\n",
        "    shuffle=True)\n",
        "  loss_func = T.nn.MSELoss()\n",
        "  optimizer = T.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(0, me):\n",
        "    epoch_loss = 0.0  # for one full epoch\n",
        "    for (b_idx, batch) in enumerate(train_ldr):\n",
        "      X = batch[0]  # predictors\n",
        "      y = batch[1]  # target income\n",
        "      optimizer.zero_grad()\n",
        "      oupt = model(X)\n",
        "      loss_val = loss_func(oupt, y)  # a tensor\n",
        "      epoch_loss += loss_val.item()  # accumulate\n",
        "      loss_val.backward()  # compute gradients\n",
        "      optimizer.step()     # update weights\n",
        "\n",
        "    if epoch % le == 0:\n",
        "      print(\"epoch = %4d  |  loss = %0.4f\" % \\\n",
        "        (epoch, epoch_loss))\n",
        "      # model.eval()\n",
        "      # print(\"-------------\")\n",
        "      # acc_train = accuracy(model, ds, 0.10)\n",
        "      # print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
        "      # acc_test = accuracy(model, test_ds, 0.10) \n",
        "      # print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
        "      # model.train()\n",
        "      # print(\"-------------\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def main():\n",
        "  # 0. get started\n",
        "  print(\"\\nBegin People predict income using Transformer \")\n",
        "  T.manual_seed(0)\n",
        "  np.random.seed(0)\n",
        "  \n",
        "\n",
        "\n",
        "  # 1. create Dataset objects\n",
        "  print(\"\\nCreating People Dataset objects \")\n",
        "  train_file = \"../src/spotPython/data/people_train.csv\"\n",
        "  train_ds = PeopleDataset(train_file)  # 200 rows\n",
        "\n",
        "  test_file = \"../src/spotPython/data/people_test.csv\"\n",
        "  test_ds = PeopleDataset(test_file)  # 40 rows\n",
        "\n",
        "  # 2. create network\n",
        "  print(\"\\nCreating (8--32)-T-10-1 neural network \")\n",
        "  net = TransformerNet().to(device)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "  # 3. train model\n",
        "  print(\"\\nbat_size = 10 \")\n",
        "  print(\"loss = MSELoss() \")\n",
        "  print(\"optimizer = Adam \")\n",
        "  print(\"lrn_rate = 0.01 \")\n",
        "\n",
        "  print(\"\\nStarting training\")\n",
        "  net.train()\n",
        "  train(net, train_ds, bs=10, lr=0.01, me=300,\n",
        "    le=50, test_ds=test_ds)\n",
        "  print(\"Done \")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "  # 4. evaluate model accuracy\n",
        "  print(\"\\nComputing model accuracy (within 0.10 of true) \")\n",
        "  net.eval()\n",
        "  acc_train = accuracy(net, train_ds, 0.10)  # item-by-item\n",
        "  print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
        "\n",
        "  acc_test = accuracy_x(net, test_ds, 0.10)  # all-at-once\n",
        "  print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "  # 5. make a prediction\n",
        "  print(\"\\nPredicting income for M 34 Oklahoma moderate: \")\n",
        "  x = np.array([[-1, 0.34, 0,0,1,  0,1,0]],\n",
        "    dtype=np.float32)\n",
        "  x = T.tensor(x, dtype=T.float32).to(device) \n",
        "\n",
        "  with T.no_grad():\n",
        "    pred_inc = net(x)\n",
        "  pred_inc = pred_inc.item()  # scalar\n",
        "  print(\"$%0.2f\" % (pred_inc * 100_000))  # un-normalized\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "  # 6. save model (state_dict approach)\n",
        "  print(\"\\nSaving trained model state\")\n",
        "  fn = \".\\\\Models\\\\people_income_model.pt\"\n",
        "  T.save(net.state_dict(), fn)\n",
        "\n",
        "  # model = Net()\n",
        "  # model.load_state_dict(T.load(fn))\n",
        "  # use model to make prediction(s)\n",
        "\n",
        "  print(\"\\nEnd People income demo \")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skip Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class SkipLinear(torch.nn.Module):\n",
        "    class Core(torch.nn.Module):\n",
        "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
        "\n",
        "        def __init__(self, n):\n",
        "            \"\"\"\n",
        "            Initialize the layer.\n",
        "\n",
        "            Args:\n",
        "                n (int): The number of output nodes.\n",
        "            \"\"\"\n",
        "            super().__init__()\n",
        "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
        "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
        "            lim = 0.01\n",
        "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
        "\n",
        "        def forward(self, x)->torch.Tensor:\n",
        "            \"\"\"\n",
        "            Forward pass through the layer.\n",
        "\n",
        "            Args:\n",
        "                x (torch.Tensor): The input tensor.\n",
        "\n",
        "            Returns:\n",
        "                torch.Tensor: The output of the layer.\n",
        "            \"\"\"\n",
        "            return x @ self.weights.t() + self.biases\n",
        "\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super().__init__()\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        if n_out % n_in != 0:\n",
        "            raise ValueError(\"n_out % n_in != 0\")\n",
        "        n = n_out // n_in  # num nodes per input\n",
        "\n",
        "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        lst_nodes = []\n",
        "        for i in range(self.n_in):\n",
        "            xi = x[:, i].reshape(-1, 1)\n",
        "            oupt = self.lst_modules[i](xi)\n",
        "            lst_nodes.append(oupt)\n",
        "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
        "        for i in range(2, self.n_in):\n",
        "            result = torch.cat((result, lst_nodes[i]), 1)\n",
        "        result = result.reshape(-1, self.n_out)\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SkipLinear(torch.nn.Module):\n",
        "\n",
        "    class Core(torch.nn.Module):\n",
        "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
        "\n",
        "        def __init__(self, n):\n",
        "            \"\"\"\n",
        "            Initialize the layer.\n",
        "\n",
        "            Args:\n",
        "                n (int): The number of output nodes.\n",
        "            \"\"\"\n",
        "            super().__init__()\n",
        "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
        "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
        "            lim = 0.01\n",
        "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
        "\n",
        "        def forward(self, x) -> torch.Tensor:\n",
        "            \"\"\"\n",
        "            Forward pass through the layer.\n",
        "\n",
        "            Args:\n",
        "                x (torch.Tensor): The input tensor.\n",
        "\n",
        "            Returns:\n",
        "                torch.Tensor: The output of the layer.\n",
        "            \"\"\"\n",
        "            return x @ self.weights.t() + self.biases\n",
        "\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super().__init__()\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        if n_out % n_in != 0:\n",
        "            raise ValueError(\"n_out % n_in != 0\")\n",
        "        n = n_out // n_in  # num nodes per input\n",
        "\n",
        "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        lst_nodes = []\n",
        "        for i in range(self.n_in):\n",
        "            xi = x[:, i].reshape(-1, 1)\n",
        "            oupt = self.lst_modules[i](xi)\n",
        "            lst_nodes.append(oupt)\n",
        "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
        "        for i in range(2, self.n_in):\n",
        "            result = torch.cat((result, lst_nodes[i]), 1)\n",
        "        result = result.reshape(-1, self.n_out)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# spotGUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import get_default_values, get_bound_values\n",
        "lhd = LightHyperDict()\n",
        "# generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
        "fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
        "get_default_values(fun_control)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tkinter as tk\n",
        "import json\n",
        "from spotPython.hyperparameters.values import get_default_values, get_bound_values\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "\n",
        "\n",
        "def create_gui(model):\n",
        "    lhd = LightHyperDict()\n",
        "    # generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
        "    fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
        "\n",
        "    # Apply the functions to the dictionary\n",
        "    default_values = get_default_values(fun_control)\n",
        "    lower_bound_values = get_bound_values(fun_control, \"lower\")\n",
        "    upper_bound_values = get_bound_values(fun_control, \"upper\")\n",
        "\n",
        "    # Create a tkinter window\n",
        "    root = tk.Tk()\n",
        "\n",
        "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
        "    for i, (key, value) in enumerate(lhd.hyper_dict['NetLightRegression'].items()):\n",
        "            # Create a label with the key as text\n",
        "            label = tk.Label(root, text=key)\n",
        "            label.grid(row=i, column=0, sticky=\"W\")\n",
        "\n",
        "            # Create an entry with the default value as the default text\n",
        "            default_entry = tk.Entry(root)\n",
        "            default_entry.insert(0, value)\n",
        "            default_entry.grid(row=i, column=1, sticky=\"W\")\n",
        "        # add the lower bound values in column 2\n",
        "            lower_bound_entry = tk.Entry(root)\n",
        "            lower_bound_entry.insert(0, lower_bound_values[i])\n",
        "            lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
        "        # add the upper bound values in column 3\n",
        "            upper_bound_entry = tk.Entry(root)\n",
        "            upper_bound_entry.insert(0, upper_bound_values[i])\n",
        "            upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
        "\n",
        "    # Run the tkinter main loop\n",
        "    root.mainloop()\n",
        "\n",
        "# Call the function to create the GUI\n",
        "create_gui(model = 'NetLightRegression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tkinter as tk\n",
        "from spotPython.hyperdict.light_hyper_dict import LightHyperDict\n",
        "\n",
        "\n",
        "def create_gui(model):\n",
        "    lhd = LightHyperDict()\n",
        "    dict =  lhd.hyper_dict[model]\n",
        "\n",
        "    \n",
        "    # Create a tkinter window\n",
        "    root = tk.Tk()\n",
        "\n",
        "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
        "    # TODO: Add labels to the column headers\n",
        "    for i, (key, value) in enumerate(dict.items()):            \n",
        "            if dict[key][\"type\"] == \"int\" or dict[key][\"type\"] == \"float\":\n",
        "                # Create a label with the key as text\n",
        "                label = tk.Label(root, text=key)\n",
        "                label.grid(row=i, column=0, sticky=\"W\")\n",
        "                # Create an entry with the default value as the default text\n",
        "                default_entry = tk.Entry(root)\n",
        "                default_entry.insert(0, dict[key][\"default\"])\n",
        "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
        "                # add the lower bound values in column 2\n",
        "                lower_bound_entry = tk.Entry(root)                \n",
        "                lower_bound_entry.insert(0, dict[key][\"lower\"])\n",
        "                lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
        "                # add the upper bound values in column 3\n",
        "                upper_bound_entry = tk.Entry(root)\n",
        "                upper_bound_entry.insert(0, dict[key][\"upper\"])\n",
        "                upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
        "            if dict[key][\"type\"] == \"factor\":        \n",
        "                # Create a label with the key as text\n",
        "                label = tk.Label(root, text=key)\n",
        "                label.grid(row=i, column=0, sticky=\"W\")\n",
        "                # Create an entry with the default value as the default text\n",
        "                default_entry = tk.Entry(root)\n",
        "                default_entry.insert(0, dict[key][\"default\"])\n",
        "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
        "                # add the lower bound values in column 2\n",
        "                factor_level_entry = tk.Entry(root)\n",
        "                # add a comma to each level\n",
        "                dict[key][\"levels\"] = \", \".join(dict[key][\"levels\"])                                \n",
        "                factor_level_entry.insert(0, dict[key][\"levels\"])\n",
        "                # TODO: Fix columnspan\n",
        "                factor_level_entry.grid(row=i, column=2, columnspan=2, sticky=\"W\")\n",
        "\n",
        "    # Run the tkinter main loop\n",
        "    root.mainloop()\n",
        "\n",
        "# Call the function to create the GUI\n",
        "create_gui(model = 'NetLightRegression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_gui(model = 'TransformerLightRegression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# save Load Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from spotPython.utils.file import save_experiment, load_experiment\n",
        "import numpy as np\n",
        "from math import inf\n",
        "from spotPython.spot import spot\n",
        "from spotPython.utils.init import (\n",
        "    fun_control_init,\n",
        "    design_control_init,\n",
        "    surrogate_control_init,\n",
        "    optimizer_control_init)\n",
        "from spotPython.fun.objectivefunctions import analytical\n",
        "\n",
        "def test_file_save_load():\n",
        "    fun = analytical().fun_branin\n",
        "\n",
        "    fun_control = fun_control_init(\n",
        "                PREFIX=\"branin\",\n",
        "                SUMMARY_WRITER=False,\n",
        "                lower = np.array([0, 0]),\n",
        "                upper = np.array([10, 10]),\n",
        "                fun_evals=8,\n",
        "                fun_repeats=1,\n",
        "                max_time=inf,\n",
        "                noise=False,\n",
        "                tolerance_x=0,\n",
        "                ocba_delta=0,\n",
        "                var_type=[\"num\", \"num\"],\n",
        "                infill_criterion=\"ei\",\n",
        "                n_points=1,\n",
        "                seed=123,\n",
        "                log_level=20,\n",
        "                show_models=False,\n",
        "                show_progress=True)\n",
        "    design_control = design_control_init(\n",
        "                init_size=5,\n",
        "                repeats=1)\n",
        "    surrogate_control = surrogate_control_init(\n",
        "                model_fun_evals=10000,\n",
        "                min_theta=-3,\n",
        "                max_theta=3,\n",
        "                n_theta=2,\n",
        "                theta_init_zero=True,\n",
        "                n_p=1,\n",
        "                optim_p=False,\n",
        "                var_type=[\"num\", \"num\"],\n",
        "                seed=124)\n",
        "    optimizer_control = optimizer_control_init(\n",
        "                max_iter=1000,\n",
        "                seed=125)\n",
        "    spot_tuner = spot.Spot(fun=fun,\n",
        "                fun_control=fun_control,\n",
        "                design_control=design_control,\n",
        "                surrogate_control=surrogate_control,\n",
        "                optimizer_control=optimizer_control)\n",
        "    # Call the save_experiment function\n",
        "    pkl_name = save_experiment(\n",
        "        spot_tuner=spot_tuner,\n",
        "        fun_control=fun_control,\n",
        "        design_control=None,\n",
        "        surrogate_control=None,\n",
        "        optimizer_control=None\n",
        "    )\n",
        "\n",
        "    # Verify that the pickle file is created\n",
        "    assert os.path.exists(pkl_name)\n",
        "\n",
        "    # Call the load_experiment function\n",
        "    spot_tuner_1, fun_control_1, design_control_1, surrogate_control_1, optimizer_control_1 = load_experiment(pkl_name)\n",
        "\n",
        "    # Verify the name of the pickle file\n",
        "    assert pkl_name == f\"spot_{fun_control['PREFIX']}experiment.pickle\"\n",
        "\n",
        "    # Clean up the temporary directory\n",
        "    os.remove(pkl_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 123\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment saved as spot_braninexperiment.pickle\n"
          ]
        }
      ],
      "source": [
        "test_file_save_load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
