{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  cache: false\n",
    "  eval: true\n",
    "  echo: true\n",
    "  warning: false\n",
    "title: 'spotpython Tests'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fun_control_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(_L_in=64, _L_out=11, num_workers=0, device=None)\n",
    "fun_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def class_attributes_to_dataframe(class_obj):\n",
    "    # Get the attributes and their values of the class object\n",
    "    attributes = [attr for attr in dir(class_obj) if not callable(getattr(class_obj, attr)) and not attr.startswith(\"__\")]\n",
    "    values = [getattr(class_obj, attr) for attr in attributes]\n",
    "    \n",
    "    # Create a DataFrame from the attributes and values\n",
    "    df = pd.DataFrame({'Attribute Name': attributes, 'Attribute Value': values})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.name = \"John\"\n",
    "        self.age = 30\n",
    "        self.salary = 50000\n",
    "\n",
    "my_instance = MyClass()\n",
    "df = class_attributes_to_dataframe(my_instance)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "n = 10\n",
    "\n",
    "fun = analytical().fun_sphere\n",
    "lower = np.array([-1])\n",
    "upper = np.array([1])\n",
    "design_control={\"init_size\": ni}\n",
    "\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            lower = lower,\n",
    "            upper= upper,\n",
    "            fun_evals = n,\n",
    "            show_progress=True,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "df = spot_1.class_attributes_to_dataframe()\n",
    "stdout.write(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river.linear_model import LogisticRegression\n",
    "from river import metrics\n",
    "from river import optim\n",
    "from river import preprocessing\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset, model, metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.csvdataset import CSVDataset\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data.csv', target_column='prognosis')\n",
    "dataset = CSVDataset(target_column='prognosis')\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.extra_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 3\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Data set VBDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv_file='./data/spotpython/data.csv' as a pandas df and save it as a pickle file\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/spotpython/data.csv')\n",
    "df.to_pickle('./data/spotpython/data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyHcf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.data.daten_sensitive import DatenSensitive\n",
    "from pyhcf.utils.names import get_short_parameter_names\n",
    "daten = DatenSensitive()\n",
    "df = daten.load()\n",
    "names =  df.columns\n",
    "names = get_short_parameter_names(names)\n",
    "# rename columns with short names\n",
    "df.columns = names\n",
    "df.head()\n",
    "# save the df as a csv file\n",
    "df.to_csv('./data/spotpython/data_sensitive.csv', index=False)\n",
    "# save the df as a pickle file\n",
    "df.to_pickle('./data/spotpython/data_sensitive.pkl')\n",
    "# remove all rows with NaN values\n",
    "df = df.dropna()\n",
    "# save the df as a csv file\n",
    "df.to_csv('./data/spotpython/data_sensitive_rmNA.csv', index=False)\n",
    "# save the df as a pickle file\n",
    "df.to_pickle('./data/spotpython/data_sensitive_rmNA.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyHcf data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.light.csvdataset import CSVDataset\n",
    "# import torch\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5000\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     # print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.light.csvdataset import CSVDataset\n",
    "# import torch\n",
    "# dataset = CSVDataset(csv_file='./data/spotpython/data_sensitive.csv', target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5000\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     # print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset.feature_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(pkl_file='./data/spotpython/data_sensitive.pkl', target_column='A', feature_type=torch.long, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # Set batch size for DataLoader\n",
    "# batch_size = 5\n",
    "# # Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Iterate over the data in the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     inputs, targets = batch\n",
    "#     print(f\"Batch Size: {inputs.size(0)}\")\n",
    "#     print(\"---------------\")\n",
    "#     print(f\"Inputs: {inputs}\")\n",
    "#     print(f\"Targets: {targets}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test lightdatamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "# dataset = PKLDataset(directory=\"./data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set size: {len(data_module.data_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation set size: {len(data_module.data_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the DataModule in fun_control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=7)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same with the sensitive data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float64, rmNA=False)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same, but VBDO data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "fun_control = fun_control_init()\n",
    "dataset = CSVDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/VBDP/\", filename=\"train.csv\",target_column='prognosis', feature_type=torch.long)\n",
    "dm = LightDataModule(dataset=dataset, batch_size=5, test_size=77)\n",
    "dm.setup()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                key=\"data_module\",\n",
    "                value=dm, replace=True)\n",
    "data_module = fun_control[\"data_module\"]\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Hyperdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "lhd = LightHyperDict()\n",
    "lhd.hyper_dict\n",
    "user_lhd = LightHyperDict(filename=\"user_hyper_dict.json\", directory=\"./hyperdict/\")\n",
    "user_lhd.hyper_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes  \n",
    "import torch\n",
    "\n",
    "# Load the diabetes dataset\n",
    "feature_df, target_df = load_diabetes(return_X_y=True, as_frame=True)\n",
    "feature_tensor = torch.tensor(feature_df.values, dtype=torch.float32)\n",
    "target_tensor = torch.tensor(target_df.values, dtype=torch.float32)\n",
    "feature_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "dataset = Diabetes()\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add core model to fun control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.netlightregressione import NetLightRegression\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "fun_control[\"core_model\"].__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the fun_control[\"core_model_hyper_dict\"] is a LightHyperDict\n",
    "isinstance(fun_control[\"core_model_hyper_dict\"], dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test check_X_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "n_hyperparams = len(get_var_name(fun_control))\n",
    "# generate a random np.array X with shape (2, n_hyperparams)\n",
    "X = np.random.rand(2, n_hyperparams)\n",
    "X == hyper_light.check_X_shape(X, fun_control)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test hyperlight fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                    key=\"data_set\",\n",
    "                    value=dataset)\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "X = np.vstack((X, X))\n",
    "y = hyper_light.fun(X, fun_control)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test  NetLightRegression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader)) \n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adam', dropout_prob=0.1, lr_mult=0.1,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests optimizer_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "lr_mult=0.1\n",
    "\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adam', dropout_prob=0.1, lr_mult=lr_mult,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# Adam uses a lr which is calculated as lr=lr_mult * 0.001, so this value\n",
    "# should be 0.1 * 0.001 = 0.0001 \n",
    "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*0.001\n",
    "\n",
    "\n",
    "net_light_base = NetLightRegression(l1=128, epochs=10, batch_size=BATCH_SIZE,\n",
    "                                initialization='xavier', act_fn=nn.ReLU(),\n",
    "                                optimizer='Adadelta', dropout_prob=0.1, lr_mult=lr_mult,\n",
    "                                patience=5, _L_in=10, _L_out=1)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# Adadelta uses a lr which is calculated as lr=lr_mult * 1.0, so this value\n",
    "# should be 1.0 * 0.1 = 0.1 \n",
    "trainer.optimizers[0].param_groups[0][\"lr\"] == lr_mult*1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
    "from spotpython.light.traintest import train_model, test_model\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "var_dict = assign_values(X, get_var_name(fun_control))\n",
    "for config in generate_one_config_from_var_dict(var_dict, fun_control):\n",
    "    y_train = train_model(config, fun_control)\n",
    "    y_test = test_model(config, fun_control)\n",
    "    break\n",
    "print(y_train)\n",
    "print(y_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_as_array\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_name, assign_values, generate_one_config_from_var_dict\n",
    "from spotpython.light.traintest import test_model\n",
    "\n",
    "\n",
    "def test_traintest_test_model():\n",
    "    fun_control = fun_control_init(\n",
    "        _L_in=10,\n",
    "        _L_out=1,)\n",
    "\n",
    "    dataset = Diabetes()\n",
    "    set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "    add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                                fun_control=fun_control,\n",
    "                                hyper_dict=LightHyperDict)\n",
    "    X = get_default_hyperparameters_as_array(fun_control)\n",
    "    var_dict = assign_values(X, get_var_name(fun_control))\n",
    "    for vals in generate_one_config_from_var_dict(var_dict, fun_control):\n",
    "        y_test = test_model(test_config=vals,\n",
    "                            fun_control=fun_control)\n",
    "        break\n",
    "    # check if y is a float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test getVarName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = {\"core_model_hyper_dict\":{\n",
    "            \"leaf_prediction\": {\n",
    "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"mean\",\n",
    "                \"core_model_parameter_type\": \"str\"},\n",
    "            \"leaf_model\": {\n",
    "                \"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"LinearRegression\",\n",
    "                \"core_model_parameter_type\": \"instance\"},\n",
    "            \"splitter\": {\n",
    "                \"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": \"EBSTSplitter\",\n",
    "                \"core_model_parameter_type\": \"instance()\"},\n",
    "            \"binary_split\": {\n",
    "                \"levels\": [0, 1],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": 0,\n",
    "                \"core_model_parameter_type\": \"bool\"},\n",
    "            \"stop_mem_management\": {\n",
    "                \"levels\": [0, 1],\n",
    "                \"type\": \"factor\",\n",
    "                \"default\": 0,\n",
    "                \"core_model_parameter_type\": \"bool\"}}}\n",
    "len(get_var_name(fun_control))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test netlightregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from math import inf\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import modify_hyper_parameter_bounds\n",
    "from spotpython.hyperparameters.values import modify_hyper_parameter_levels\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import (get_bound_values,\n",
    "    get_var_name,\n",
    "    get_var_type,)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.hyperparameters.values import get_tuned_architecture\n",
    "from spotpython.light.testmodel import test_model\n",
    "from spotpython.light.loadmodel import load_light_from_checkpoint\n",
    "\n",
    "MAX_TIME = 1\n",
    "INIT_SIZE = 5\n",
    "WORKERS = 0\n",
    "PREFIX=\"031\"\n",
    "\n",
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    num_workers=WORKERS,\n",
    "    device=getDevice(),\n",
    "    _L_in=133,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True)\n",
    "\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "\n",
    "\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[5,8])\n",
    "modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[3,5])\n",
    "modify_hyper_parameter_bounds(fun_control, \"batch_size\", bounds=[2, 8])\n",
    "modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
    "\n",
    "print(gen_design_table(fun_control))\n",
    "\n",
    "var_type = get_var_type(fun_control)\n",
    "var_name = get_var_name(fun_control)\n",
    "lower = get_bound_values(fun_control, \"lower\")\n",
    "upper = get_bound_values(fun_control, \"upper\")\n",
    "fun = HyperLight(log_level=50).fun\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       log_level=50,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   max_time = MAX_TIME,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type = var_type,\n",
    "                   var_name = var_name,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": INIT_SIZE},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": len(var_name),\n",
    "                                      \"model_fun_evals\": 10_000,\n",
    "                                      })\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_progress(log_y=False, filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_importance(threshold=0.025, filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_tuned_architecture(spot_tuner, fun_control)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = load_light_from_checkpoint(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_important_hyperparameter_contour(filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.parallel_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.cvmodel import cv_model\n",
    "# set the number of folds to 10\n",
    "fun_control[\"k_folds\"] = 10\n",
    "cv_model(config, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "fun = analytical(sigma=1.0, seed=123)\n",
    "fun.add_noise(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "print(np.array([1, 2, 3, 4, 5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(seed=4321, sigma=0.1)\n",
    "fun = analytical(seed=222, sigma=0.0).fun_sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_1 = spot.Spot(fun=fun,\n",
    "                   lower = np.array([-10]),\n",
    "                   upper = np.array([100]),\n",
    "                   fun_evals = 100,\n",
    "                   fun_repeats = 3,\n",
    "                   max_time = inf,\n",
    "                   noise = True,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type=[\"num\"],\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=111,\n",
    "                   log_level = 10,\n",
    "                   show_models=False,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": 5,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": 1,\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 1000,\n",
    "                                      })\n",
    "spot_1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def squared_euclidean_distance(X_0, X, theta):\n",
    "    return np.sum(theta*(X_0 - X)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
    "from numpy.linalg import cholesky, solve\n",
    "from numpy.random import multivariate_normal\n",
    "def build_Psi(X, theta):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    D = zeros((k, n, n))\n",
    "    for l in range(k):\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
    "    D = sum(D)\n",
    "    D = D + D.T\n",
    "    return exp(-D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0, 1.0])\n",
    "X = np.array([[1.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_Psi(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 2]])\n",
    "fun = analytical()\n",
    "fun.fun_branin_factor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pi = np.pi\n",
    "X = np.array([[0,0], [-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
    "fun = analytical()\n",
    "fun.fun_branin(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "pi = np.pi\n",
    "X_0 = np.array([[0, 0]])\n",
    "X_1 = np.array([[-pi, 12.275], [pi, 2.275], [9.42478, 2.475]])\n",
    "X_2 = np.array([[0,0,0], [0,0,1], [0,0,2]])\n",
    "fun = analytical()\n",
    "y_0 = fun.fun_branin(X_0)\n",
    "y_1 = fun.fun_branin(X_1)\n",
    "y_2 = fun.fun_branin_factor(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(y_1[0], 2) == round(y_1[1],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "round(y_1[0], 2) == round(y_1[2],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[0] == y_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[1] == y_0 + 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_2[2] == y_0 - 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "import numpy as np\n",
    "n = 100\n",
    "X = np.linspace(0, 10, n, endpoint=False).reshape(-1,1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array, zeros, power, ones, exp, multiply, eye, linspace, mat, spacing, sqrt, arange, append, ravel\n",
    "from numpy.linalg import cholesky, solve\n",
    "from numpy.random import multivariate_normal\n",
    "def build_Psi(X, theta):\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    D = zeros((k, n, n))\n",
    "    for l in range(k):\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                D[l, i, j] = theta[l]*(X[i,l] - X[j,l])**2\n",
    "    D = sum(D)\n",
    "    D = D + D.T\n",
    "    return exp(-D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0])\n",
    "Psi = build_Psi(X, theta)\n",
    "np.round(Psi[:3,:], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = (3, 1, 1), check_valid=\"raise\")\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Y to a 3 x 100 array\n",
    "Y = np.squeeze(Y)\n",
    "Y.shape\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 samples from the GP as a function of X\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, Y.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = multivariate_normal(zeros(Psi.shape[0]), Psi, size = 3, check_valid=\"raise\")\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 samples from the GP as a function of X\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, Y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Targets Shape: {targets.shape}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test HyperLight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.hyperparameters.values import get_var_name\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "n_hyperparams = len(get_var_name(fun_control))\n",
    "# generate a random np.array X with shape (2, n_hyperparams)\n",
    "X = np.random.rand(2, n_hyperparams)\n",
    "X == hyper_light.check_X_shape(X, fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (add_core_model_to_fun_control,\n",
    "    get_default_hyperparameters_as_array)\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,)\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset)\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "hyper_light = HyperLight(seed=126, log_level=50)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "# combine X and X to a np.array with shape (2, n_hyperparams)\n",
    "# so that two values are returned\n",
    "X = np.vstack((X, X))\n",
    "hyper_light.fun(X, fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test pkldataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.pkldataset import PKLDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# dataset = PKLDataset(target_column='prognosis', feature_type=torch.long)\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\",\n",
    "                    filename=\"data_sensitive.pkl\",\n",
    "                    target_column='N',\n",
    "                    feature_type=torch.float32,\n",
    "                    target_type=torch.float32,\n",
    "                    rmNA=True)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_bound_values\n",
    "from spotpython.hyperparameters.values import get_control_key_value, set_control_key_value\n",
    "from spotpython.hyperparameters.values import get_var_type_from_var_name\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "set_control_key_value(control_dict=fun_control, key=\"var_type\", value=[\"int\", \"float\", \"str\"], replace=True)\n",
    "set_control_key_value(control_dict=fun_control, key=\"var_name\", value=[\"max_depth\", \"learning_rate\", \"model_type\"], replace=True)\n",
    "\n",
    "print(fun_control)\n",
    "\n",
    "# Test with existing var_name\n",
    "assert get_var_type_from_var_name(var_name=\"max_depth\", fun_control=fun_control) == \"int\"\n",
    "assert get_var_type_from_var_name(var_name=\"learning_rate\", fun_control=fun_control) == \"float\"\n",
    "assert get_var_type_from_var_name(var_name=\"model_type\", fun_control=fun_control) == \"str\"\n",
    "\n",
    "# Test with non-existing var_name\n",
    "with pytest.raises(ValueError):\n",
    "    get_var_type_from_var_name(var_name=\"non_existing\", fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_control_key_value\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import get_var_type_from_var_name\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                    fun_control=fun_control,\n",
    "                    hyper_dict=LightHyperDict)\n",
    "var_type = get_control_key_value(control_dict=fun_control, key=\"var_type\")\n",
    "var_name = get_control_key_value(control_dict=fun_control, key=\"var_name\")\n",
    "print(var_type)\n",
    "print(var_name)\n",
    "vn = \"l1\"\n",
    "get_var_type_from_var_name(fun_control=fun_control, var_name=vn)\n",
    "\n",
    "assert var_type[var_name.index(vn)] == \"int\"\n",
    "assert get_var_type_from_var_name(fun_control, vn) == \"int\"\n",
    "vn = \"initialization\"\n",
    "assert var_type[var_name.index(vn)] == \"factor\"\n",
    "assert var_type[var_name.index(vn)] == \"factor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import get_control_key_value\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "\n",
    "fun_control = fun_control_init()\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                    fun_control=fun_control,\n",
    "                    hyper_dict=LightHyperDict)\n",
    "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"l1\", value=[1,7])\n",
    "set_control_hyperparameter_value(control_dict=fun_control, hyperparameter=\"initialization\", value=[\"xavier2\", \"kaiming2\"])\n",
    "print(fun_control)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry(dictionary, key, i):\n",
    "    if key in dictionary:\n",
    "        if 'levels' in dictionary[key]:\n",
    "            if i < len(dictionary[key]['levels']):\n",
    "                return dictionary[key]['levels'][i]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from spotpython.data.pkldataset_intern import PKLDataset\n",
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from math import inf\n",
    "\n",
    "MAX_TIME = 60\n",
    "FUN_EVALS = inf\n",
    "INIT_SIZE = 25\n",
    "WORKERS = 0\n",
    "PREFIX=\"031\"\n",
    "DEVICE = getDevice()\n",
    "\n",
    "\n",
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    device=DEVICE,\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=FUN_EVALS,\n",
    "    log_level=10,\n",
    "    max_time=MAX_TIME,\n",
    "    num_workers=WORKERS,\n",
    "    show_progress=True,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "\n",
    "dataset = Diabetes()\n",
    "dataset = PKLDataset(directory=\"/Users/bartz/workspace/spotpython/notebooks/data/spotpython/\", filename=\"data_sensitive.pkl\", target_column='N', feature_type=torch.float32, target_type=torch.float32, rmNA=True, rmMF=True)\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"_L_in\",\n",
    "                        value=133,\n",
    "                        replace=True)\n",
    "\n",
    "\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "# from spotpython.hyperparameters.values import modify_hyper_parameter_bounds\n",
    "\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
    "set_control_hyperparameter_value(fun_control, \"epochs\", [4,9])\n",
    "set_control_hyperparameter_value(fun_control, \"batch_size\", [1, 4])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry(dictionary, key, i):\n",
    "    if 'core_model_hyper_dict' in dictionary:\n",
    "        if key in dictionary['core_model_hyper_dict']:\n",
    "            if 'levels' in dictionary['core_model_hyper_dict'][key]:\n",
    "                if i < len(dictionary['core_model_hyper_dict'][key]['levels']):\n",
    "                    return dictionary['core_model_hyper_dict'][key]['levels'][i]\n",
    "    return None\n",
    "print(get_entry(fun_control, \"optimizer\", 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.device import getDevice\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "import numpy as np\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotpython.hyperparameters.values import get_ith_hyperparameter_name_from_fun_control\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "experiment_name = get_experiment_name(prefix=\"000\")\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    device=getDevice(),\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=15,\n",
    "    log_level=10,\n",
    "    max_time=1,\n",
    "    num_workers=0,\n",
    "    show_progress=True,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "dataset = Diabetes()\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                        key=\"data_set\",\n",
    "                        value=dataset,\n",
    "                        replace=True)\n",
    "add_core_model_to_fun_control(core_model=NetLightRegression,\n",
    "                            fun_control=fun_control,\n",
    "                            hyper_dict=LightHyperDict)\n",
    "\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [3,8])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
    "assert get_ith_hyperparameter_name_from_fun_control(fun_control, key=\"optimizer\", i=0) == \"Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_timestamp(only_int=True):\n",
    "    dt = datetime.datetime.now().isoformat(sep=\" \", timespec=\"microseconds\")\n",
    "    if only_int:\n",
    "        # remove - . : and space\n",
    "        dt = dt.replace(\"-\", \"\")\n",
    "        dt = dt.replace(\".\", \"\")\n",
    "        dt = dt.replace(\":\", \"\")\n",
    "        dt = dt.replace(\" \", \"\")\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, surrogate_control_init, design_control_init\n",
    ")\n",
    "\n",
    "def test_plot_progress():\n",
    "    # number of initial points:\n",
    "    ni = 7\n",
    "    # number of points\n",
    "    fun_evals = 10\n",
    "    fun = analytical().fun_sphere\n",
    "    fun_control = fun_control_init(\n",
    "        lower = np.array([-1, -1]),\n",
    "        upper = np.array([1, 1]),\n",
    "        fun_evals=fun_evals,\n",
    "        tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "    design_control=design_control_init(init_size=ni)\n",
    "    surrogate_control=surrogate_control_init(n_theta=3)\n",
    "    S = spot.Spot(fun=fun,\n",
    "                    fun_control=fun_control,\n",
    "                    design_control=design_control,\n",
    "                    surrogate_control=surrogate_control,)\n",
    "    S.run()\n",
    "\n",
    "    # Test plot_progress with different parameters\n",
    "    S.plot_progress(show=False)  # Test with show=False\n",
    "    S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "    S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "    S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename\n",
    "    # add NaN to S.y at position 2\n",
    "    S.y[2] = np.nan\n",
    "    S.plot_progress(show=False)  # Test with show=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, surrogate_control_init, design_control_init\n",
    ")\n",
    "\n",
    "\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "                fun_control=fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "\n",
    "# remove points from S.y so that there are less than ni points\n",
    "S.y = S.y[:3]\n",
    "# Test plot_progress with different parameters\n",
    "S.plot_progress(show=False)  # Test with show=False\n",
    "S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import differential_evolution\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "def objective_function(X, fun_control=None):\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X)\n",
    "    if X.shape[1] != 2:\n",
    "        raise Exception\n",
    "    x0 = X[:, 0]\n",
    "    x1 = X[:, 1]\n",
    "    y = x0**2 + 10*x1**2\n",
    "    return y\n",
    "fun_control = fun_control_init(\n",
    "            lower = np.array([0, 0]),\n",
    "            upper = np.array([10, 10]),\n",
    "            fun_evals=8,\n",
    "            fun_repeats=1,\n",
    "            max_time=inf,\n",
    "            noise=True,\n",
    "            tolerance_x=0,\n",
    "            ocba_delta=0,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            infill_criterion=\"ei\",\n",
    "            n_points=1,\n",
    "            seed=123,\n",
    "            log_level=10,\n",
    "            show_models=False,\n",
    "            show_progress=True)\n",
    "design_control = design_control_init(\n",
    "            init_size=5,\n",
    "            repeats=1)\n",
    "surrogate_control = surrogate_control_init(\n",
    "            log_level=10,\n",
    "            model_optimizer=differential_evolution,\n",
    "            model_fun_evals=10000,\n",
    "            min_theta=-3,\n",
    "            max_theta=3,\n",
    "            n_theta=2,\n",
    "            theta_init_zero=True,\n",
    "            n_p=1,\n",
    "            optim_p=False,\n",
    "            noise=True,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            seed=124, \n",
    "            min_Lambda=1,\n",
    "            max_Lambda=10)\n",
    "optimizer_control = optimizer_control_init(\n",
    "            max_iter=1000,\n",
    "            seed=125)\n",
    "spot = spot.Spot(fun=objective_function,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,\n",
    "            optimizer_control=optimizer_control\n",
    "            )\n",
    "spot.run()\n",
    "spot.plot_progress()\n",
    "spot.plot_contour(i=0, j=1)\n",
    "spot.plot_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n",
    "\n",
    "fun = analytical().fun_branin\n",
    "fun_control = fun_control_init(lower = np.array([-5, 0]),\n",
    "                               upper = np.array([10, 15]),\n",
    "                               fun_evals=20)\n",
    "design_control = design_control_init(init_size=10)\n",
    "surrogate_control = surrogate_control_init(n_theta=2)\n",
    "S = spot.Spot(fun=fun, fun_control=fun_control, design_control=design_control)\n",
    "S.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.plot_progress(log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.surrogate.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = analytical().fun_sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,100).reshape(-1,1)\n",
    "y = fun(x)\n",
    "plt.figure()\n",
    "plt.plot(x,y, \"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "                   fun_control=fun_control_init(\n",
    "                        lower = np.array([-10]),\n",
    "                        upper = np.array([100]),\n",
    "                        fun_evals = 7,\n",
    "                        fun_repeats = 1,\n",
    "                        max_time = inf,\n",
    "                        noise = False,\n",
    "                        tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                        var_type=[\"num\"],\n",
    "                        infill_criterion = \"y\",\n",
    "                        n_points = 1,\n",
    "                        seed=123,\n",
    "                        log_level = 50),\n",
    "                   design_control=design_control_init(\n",
    "                        init_size=5,\n",
    "                        repeats=1),\n",
    "                   surrogate_control=surrogate_control_init(\n",
    "                        noise=False,\n",
    "                        min_theta=-4,\n",
    "                        max_theta=3,\n",
    "                        n_theta=1,\n",
    "                        model_optimizer=differential_evolution,\n",
    "                        model_fun_evals=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "fun = analytical().fun_sphere\n",
    "from spotpython.design.spacefilling import spacefilling\n",
    "design = spacefilling(2)\n",
    "from scipy.optimize import differential_evolution\n",
    "optimizer = differential_evolution\n",
    "from spotpython.build.kriging import Kriging\n",
    "surrogate = Kriging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init, design_control_init, optimizer_control_init, surrogate_control_init\n",
    "fun_control=fun_control_init(lower=np.array([-1, -1]),\n",
    "                            upper=np.array([1, 1]))\n",
    "design_control=design_control_init()\n",
    "optimizer_control=optimizer_control_init()\n",
    "surrogate_control=surrogate_control_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       fun_control=fun_control,\n",
    "                       design_control=design_control,\n",
    "                       optimizer_control=optimizer_control,\n",
    "                       surrogate_control=surrogate_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytest\n",
    "import torch\n",
    "from pyhcf.data.loadHcfData import build_df, load_hcf_data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list=[\"L\", \"AQ\", \"AS\"]\n",
    "dataset = load_hcf_data(param_list=p_list, target=\"T\",\n",
    "                        rmNA=True, rmMF=True,\n",
    "                        load_all_features=False,\n",
    "                        load_thermo_features=False,\n",
    "                        scale_data=True,\n",
    "                        return_X_y=False)\n",
    "assert isinstance(dataset, torch.utils.data.TensorDataset)\n",
    "assert len(dataset) > 0\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader    \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    assert inputs.size(0) == batch_size\n",
    "    print(f\"Inputs Shape: {inputs.shape[1]}\")\n",
    "    print(f\"P List: {p_list}\")\n",
    "    print(f\"P List Length: {len(p_list)}\")\n",
    "    # input is p_list + 1 (for target)\n",
    "    # p_list = [\"L\", \"AQ\", \"AS\"] plus target \"N\"\n",
    "    assert inputs.shape[1] + 1 == len(p_list)\n",
    "    print(f\"Targets Shape: {targets.shape[0]}\")\n",
    "    assert targets.shape[0] == batch_size\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "# data.csv is simple csv file with 11 samples\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup(stage=\"predict\")\n",
    "print(f\"Predict set size: {len(data_module.data_predict)}\")\n",
    "for batch in data_module.predict_dataloader():\n",
    "    inputs, targets = batch\n",
    "    print(f\"inputs: {inputs}\")\n",
    "    print(f\"targets: {targets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_module.data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_div2_list(n, n_min):\n",
    "    result = []\n",
    "    current = n\n",
    "    while current >= n_min:\n",
    "        result.extend([current] * (n // current))\n",
    "        current = current // 2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_L_in = 128\n",
    "l1 = \n",
    "\n",
    "n_low = _L_in // 4\n",
    "# ensure that n_high is larger than n_low\n",
    "n_high = max(l1, 2 * n_low)\n",
    "generate_div2_list(n_high, n_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.math import generate_div2_list\n",
    "generate_div2_list(64, 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.transformer.positionalEncoding import PositionalEncoding\n",
    "import torch\n",
    "# number of tensors\n",
    "n = 3\n",
    "# dimension of each tensor\n",
    "k = 32\n",
    "pe = PositionalEncoding(d_model=k, dropout_prob=0, verbose=False)\n",
    "input = torch.zeros(1, n, k)\n",
    "# Generate a tensor of size (1, 10, 4) with values from 1 to 10\n",
    "for i in range(n):\n",
    "    input[0, i, :] = i\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Input: {input}\")\n",
    "output = pe(input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.transformer.skiplinear import SkipLinear\n",
    "import torch\n",
    "n_in = 2\n",
    "n_out = 4\n",
    "sl = SkipLinear(n_in, n_out)\n",
    "input = torch.zeros(1, n_in)\n",
    "for i in range(n_in):\n",
    "    input[0, i] = i\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Input: {input}\")\n",
    "output = sl(input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(sl.lst_modules)\n",
    "for i in sl.lst_modules:\n",
    "    print(f\"weights: {i.weights}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Example from J. Caffrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_income_transformer.py\n",
    "# predict income from sex, age, city, politics\n",
    "# PyTorch 2.0.0-CPU Anaconda3-2022.10  Python 3.9.13\n",
    "# Windows 10/11 \n",
    "\n",
    "# Transformer component for regression\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "\n",
    "device = T.device('cpu')  # apply to Tensor or Module\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class PeopleDataset(T.utils.data.Dataset):\n",
    "  def __init__(self, src_file):\n",
    "    # sex age   state   income   politics\n",
    "    # -1  0.27  0 1 0   0.7610   0 0 1\n",
    "    # +1  0.19  0 0 1   0.6550   1 0 0\n",
    "\n",
    "    tmp_x = np.loadtxt(src_file, usecols=[0,1,2,3,4,6,7,8],\n",
    "      delimiter=\",\", comments=\"#\", dtype=np.float32)\n",
    "    tmp_y = np.loadtxt(src_file, usecols=5, delimiter=\",\",\n",
    "      comments=\"#\", dtype=np.float32)\n",
    "    tmp_y = tmp_y.reshape(-1,1)  # 2D required\n",
    "\n",
    "    self.x_data = T.tensor(tmp_x, dtype=T.float32).to(device)\n",
    "    self.y_data = T.tensor(tmp_y, dtype=T.float32).to(device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    preds = self.x_data[idx]\n",
    "    incom = self.y_data[idx] \n",
    "    return (preds, incom)  # as a tuple\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class SkipLinear(T.nn.Module):\n",
    "\n",
    "  # -----\n",
    "\n",
    "  class Core(T.nn.Module):\n",
    "    def __init__(self, n):\n",
    "      super().__init__()\n",
    "      # 1 node to n nodes, n gte 2\n",
    "      self.weights = T.nn.Parameter(T.zeros((n,1),\n",
    "        dtype=T.float32))\n",
    "      self.biases = T.nn.Parameter(T.tensor(n,\n",
    "        dtype=T.float32))\n",
    "      lim = 0.01\n",
    "      T.nn.init.uniform_(self.weights, -lim, lim)\n",
    "      T.nn.init.zeros_(self.biases)\n",
    "\n",
    "    def forward(self, x):\n",
    "      wx= T.mm(x, self.weights.t())\n",
    "      v = T.add(wx, self.biases)\n",
    "      return v\n",
    "\n",
    "  # -----\n",
    "\n",
    "  def __init__(self, n_in, n_out):\n",
    "    super().__init__()\n",
    "    self.n_in = n_in; self.n_out = n_out\n",
    "    if n_out  % n_in != 0:\n",
    "      print(\"FATAL: n_out must be divisible by n_in\")\n",
    "    n = n_out // n_in  # num nodes per input\n",
    "\n",
    "    self.lst_modules = \\\n",
    "      T.nn.ModuleList([SkipLinear.Core(n) for \\\n",
    "        i in range(n_in)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    lst_nodes = []\n",
    "    for i in range(self.n_in):\n",
    "      xi = x[:,i].reshape(-1,1)\n",
    "      oupt = self.lst_modules[i](xi)\n",
    "      lst_nodes.append(oupt)\n",
    "    result = T.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "    for i in range(2,self.n_in):\n",
    "      result = T.cat((result, lst_nodes[i]), 1)\n",
    "    result = result.reshape(-1, self.n_out)\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class PositionalEncoding(T.nn.Module):  # documentation code\n",
    "  def __init__(self, d_model: int, dropout: float=0.1,\n",
    "   max_len: int=5000):\n",
    "    super(PositionalEncoding, self).__init__()  # old syntax\n",
    "    self.dropout = T.nn.Dropout(p=dropout)\n",
    "    pe = T.zeros(max_len, d_model)  # like 10x4\n",
    "    position = \\\n",
    "      T.arange(0, max_len, dtype=T.float).unsqueeze(1)\n",
    "    div_term = T.exp(T.arange(0, d_model, 2).float() * \\\n",
    "      (-np.log(10_000.0) / d_model))\n",
    "    pe[:, 0::2] = T.sin(position * div_term)\n",
    "    pe[:, 1::2] = T.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    self.register_buffer('pe', pe)  # allows state-save\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:x.size(0), :]\n",
    "    return self.dropout(x)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class TransformerNet(T.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TransformerNet, self).__init__()\n",
    "    self.embed = SkipLinear(8, 32)  # 8 inputs, each goes to 4 \n",
    "    self.pos_enc = \\\n",
    "      PositionalEncoding(4, dropout=0.20)  # positional\n",
    "    self.enc_layer = T.nn.TransformerEncoderLayer(d_model=4,\n",
    "      nhead=2, dim_feedforward=10, \n",
    "      batch_first=True)  # d_model divisible by nhead\n",
    "    self.trans_enc = T.nn.TransformerEncoder(self.enc_layer,\n",
    "      num_layers=2)  # 6 layers default\n",
    "\n",
    "    self.fc1 = T.nn.Linear(32, 10)  # 8--32-T-10-1\n",
    "    self.fc2 = T.nn.Linear(10, 1)\n",
    "\n",
    "    # default weight and bias initialization\n",
    "\n",
    "  def forward(self, x):\n",
    "    z = self.embed(x)  # 8 inpts to 32 embed\n",
    "    z = z.reshape(-1, 8, 4)  # bat seq embed\n",
    "    z = self.pos_enc(z) \n",
    "    z = self.trans_enc(z) \n",
    "    z = z.reshape(-1, 32)  # torch.Size([bs, xxx])\n",
    "    z = T.tanh(self.fc1(z))\n",
    "    z = self.fc2(z)  # regression: no activation\n",
    "    return z\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def accuracy(model, ds, pct_close):\n",
    "  # assumes model.eval()\n",
    "  # correct within pct of true income\n",
    "  n_correct = 0; n_wrong = 0\n",
    "\n",
    "  for i in range(len(ds)):\n",
    "    X = ds[i][0].reshape(1,-1)  # make it a batch\n",
    "    Y = ds[i][1].reshape(1)\n",
    "    with T.no_grad():\n",
    "      oupt = model(X)         # computed income\n",
    "\n",
    "    if T.abs(oupt - Y) <= T.abs(pct_close * Y):\n",
    "      n_correct += 1\n",
    "    else:\n",
    "      n_wrong += 1\n",
    "  acc = (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "  return acc\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def accuracy_x(model, ds, pct_close):\n",
    "  # all-at-once (quick)\n",
    "  # assumes model.eval()\n",
    "  X = ds.x_data  # all inputs\n",
    "  Y = ds.y_data  # all targets\n",
    "  n_items = len(X)\n",
    "  with T.no_grad():\n",
    "    pred = model(X)  # all predicted incomes\n",
    " \n",
    "  n_correct = T.sum((T.abs(pred - Y) <= \\\n",
    "    T.abs(pct_close * Y)))\n",
    "  result = (n_correct.item() / n_items)  # scalar\n",
    "  return result  \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def train(model, ds, bs, lr, me, le, test_ds):\n",
    "  # dataset, bat_size, lrn_rate, max_epochs, log interval\n",
    "  train_ldr = T.utils.data.DataLoader(ds, batch_size=bs,\n",
    "    shuffle=True)\n",
    "  loss_func = T.nn.MSELoss()\n",
    "  optimizer = T.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "  for epoch in range(0, me):\n",
    "    epoch_loss = 0.0  # for one full epoch\n",
    "    for (b_idx, batch) in enumerate(train_ldr):\n",
    "      X = batch[0]  # predictors\n",
    "      y = batch[1]  # target income\n",
    "      optimizer.zero_grad()\n",
    "      oupt = model(X)\n",
    "      loss_val = loss_func(oupt, y)  # a tensor\n",
    "      epoch_loss += loss_val.item()  # accumulate\n",
    "      loss_val.backward()  # compute gradients\n",
    "      optimizer.step()     # update weights\n",
    "\n",
    "    if epoch % le == 0:\n",
    "      print(\"epoch = %4d  |  loss = %0.4f\" % \\\n",
    "        (epoch, epoch_loss))\n",
    "      # model.eval()\n",
    "      # print(\"-------------\")\n",
    "      # acc_train = accuracy(model, ds, 0.10)\n",
    "      # print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
    "      # acc_test = accuracy(model, test_ds, 0.10) \n",
    "      # print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
    "      # model.train()\n",
    "      # print(\"-------------\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "  # 0. get started\n",
    "  print(\"\\nBegin People predict income using Transformer \")\n",
    "  T.manual_seed(0)\n",
    "  np.random.seed(0)\n",
    "  \n",
    "\n",
    "\n",
    "  # 1. create Dataset objects\n",
    "  print(\"\\nCreating People Dataset objects \")\n",
    "  train_file = \"../src/spotpython/data/people_train.csv\"\n",
    "  train_ds = PeopleDataset(train_file)  # 200 rows\n",
    "\n",
    "  test_file = \"../src/spotpython/data/people_test.csv\"\n",
    "  test_ds = PeopleDataset(test_file)  # 40 rows\n",
    "\n",
    "  # 2. create network\n",
    "  print(\"\\nCreating (8--32)-T-10-1 neural network \")\n",
    "  net = TransformerNet().to(device)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 3. train model\n",
    "  print(\"\\nbat_size = 10 \")\n",
    "  print(\"loss = MSELoss() \")\n",
    "  print(\"optimizer = Adam \")\n",
    "  print(\"lrn_rate = 0.01 \")\n",
    "\n",
    "  print(\"\\nStarting training\")\n",
    "  net.train()\n",
    "  train(net, train_ds, bs=10, lr=0.01, me=300,\n",
    "    le=50, test_ds=test_ds)\n",
    "  print(\"Done \")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 4. evaluate model accuracy\n",
    "  print(\"\\nComputing model accuracy (within 0.10 of true) \")\n",
    "  net.eval()\n",
    "  acc_train = accuracy(net, train_ds, 0.10)  # item-by-item\n",
    "  print(\"Accuracy on train data = %0.4f\" % acc_train)\n",
    "\n",
    "  acc_test = accuracy_x(net, test_ds, 0.10)  # all-at-once\n",
    "  print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 5. make a prediction\n",
    "  print(\"\\nPredicting income for M 34 Oklahoma moderate: \")\n",
    "  x = np.array([[-1, 0.34, 0,0,1,  0,1,0]],\n",
    "    dtype=np.float32)\n",
    "  x = T.tensor(x, dtype=T.float32).to(device) \n",
    "\n",
    "  with T.no_grad():\n",
    "    pred_inc = net(x)\n",
    "  pred_inc = pred_inc.item()  # scalar\n",
    "  print(\"$%0.2f\" % (pred_inc * 100_000))  # un-normalized\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "  # 6. save model (state_dict approach)\n",
    "  print(\"\\nSaving trained model state\")\n",
    "  fn = \".\\\\Models\\\\people_income_model.pt\"\n",
    "  T.save(net.state_dict(), fn)\n",
    "\n",
    "  # model = Net()\n",
    "  # model.load_state_dict(T.load(fn))\n",
    "  # use model to make prediction(s)\n",
    "\n",
    "  print(\"\\nEnd People income demo \")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SkipLinear(torch.nn.Module):\n",
    "    class Core(torch.nn.Module):\n",
    "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
    "\n",
    "        def __init__(self, n):\n",
    "            \"\"\"\n",
    "            Initialize the layer.\n",
    "\n",
    "            Args:\n",
    "                n (int): The number of output nodes.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
    "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
    "            lim = 0.01\n",
    "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
    "\n",
    "        def forward(self, x)->torch.Tensor:\n",
    "            \"\"\"\n",
    "            Forward pass through the layer.\n",
    "\n",
    "            Args:\n",
    "                x (torch.Tensor): The input tensor.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The output of the layer.\n",
    "            \"\"\"\n",
    "            return x @ self.weights.t() + self.biases\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        if n_out % n_in != 0:\n",
    "            raise ValueError(\"n_out % n_in != 0\")\n",
    "        n = n_out // n_in  # num nodes per input\n",
    "\n",
    "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        lst_nodes = []\n",
    "        for i in range(self.n_in):\n",
    "            xi = x[:, i].reshape(-1, 1)\n",
    "            oupt = self.lst_modules[i](xi)\n",
    "            lst_nodes.append(oupt)\n",
    "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "        for i in range(2, self.n_in):\n",
    "            result = torch.cat((result, lst_nodes[i]), 1)\n",
    "        result = result.reshape(-1, self.n_out)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipLinear(torch.nn.Module):\n",
    "\n",
    "    class Core(torch.nn.Module):\n",
    "        \"\"\"A simple linear layer with n outputs.\"\"\"\n",
    "\n",
    "        def __init__(self, n):\n",
    "            \"\"\"\n",
    "            Initialize the layer.\n",
    "\n",
    "            Args:\n",
    "                n (int): The number of output nodes.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.weights = torch.nn.Parameter(torch.zeros((n, 1), dtype=torch.float32))\n",
    "            self.biases = torch.nn.Parameter(torch.zeros(n, dtype=torch.float32))\n",
    "            lim = 0.01\n",
    "            torch.nn.init.uniform_(self.weights, -lim, lim)\n",
    "\n",
    "        def forward(self, x) -> torch.Tensor:\n",
    "            \"\"\"\n",
    "            Forward pass through the layer.\n",
    "\n",
    "            Args:\n",
    "                x (torch.Tensor): The input tensor.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The output of the layer.\n",
    "            \"\"\"\n",
    "            return x @ self.weights.t() + self.biases\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        if n_out % n_in != 0:\n",
    "            raise ValueError(\"n_out % n_in != 0\")\n",
    "        n = n_out // n_in  # num nodes per input\n",
    "\n",
    "        self.lst_modules = torch.nn.ModuleList([SkipLinear.Core(n) for i in range(n_in)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        lst_nodes = []\n",
    "        for i in range(self.n_in):\n",
    "            xi = x[:, i].reshape(-1, 1)\n",
    "            oupt = self.lst_modules[i](xi)\n",
    "            lst_nodes.append(oupt)\n",
    "        result = torch.cat((lst_nodes[0], lst_nodes[1]), 1)\n",
    "        for i in range(2, self.n_in):\n",
    "            result = torch.cat((result, lst_nodes[i]), 1)\n",
    "        result = result.reshape(-1, self.n_out)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spotGUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import get_default_values, get_bound_values\n",
    "lhd = LightHyperDict()\n",
    "# generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
    "fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
    "get_default_values(fun_control)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import json\n",
    "from spotpython.hyperparameters.values import get_default_values, get_bound_values\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "\n",
    "\n",
    "def create_gui(model):\n",
    "    lhd = LightHyperDict()\n",
    "    # generate a dictionary fun_control with the key \"core_model_hyper_dict\" and the value lhd.hyper_dict['NetLightRegression']\n",
    "    fun_control = {\"core_model_hyper_dict\": lhd.hyper_dict['NetLightRegression']}\n",
    "\n",
    "    # Apply the functions to the dictionary\n",
    "    default_values = get_default_values(fun_control)\n",
    "    lower_bound_values = get_bound_values(fun_control, \"lower\")\n",
    "    upper_bound_values = get_bound_values(fun_control, \"upper\")\n",
    "\n",
    "    # Create a tkinter window\n",
    "    root = tk.Tk()\n",
    "\n",
    "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
    "    for i, (key, value) in enumerate(lhd.hyper_dict['NetLightRegression'].items()):\n",
    "            # Create a label with the key as text\n",
    "            label = tk.Label(root, text=key)\n",
    "            label.grid(row=i, column=0, sticky=\"W\")\n",
    "\n",
    "            # Create an entry with the default value as the default text\n",
    "            default_entry = tk.Entry(root)\n",
    "            default_entry.insert(0, value)\n",
    "            default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "        # add the lower bound values in column 2\n",
    "            lower_bound_entry = tk.Entry(root)\n",
    "            lower_bound_entry.insert(0, lower_bound_values[i])\n",
    "            lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
    "        # add the upper bound values in column 3\n",
    "            upper_bound_entry = tk.Entry(root)\n",
    "            upper_bound_entry.insert(0, upper_bound_values[i])\n",
    "            upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
    "\n",
    "    # Run the tkinter main loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Call the function to create the GUI\n",
    "create_gui(model = 'NetLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "\n",
    "\n",
    "def create_gui(model):\n",
    "    lhd = LightHyperDict()\n",
    "    dict =  lhd.hyper_dict[model]\n",
    "\n",
    "    \n",
    "    # Create a tkinter window\n",
    "    root = tk.Tk()\n",
    "\n",
    "    # Loop over the dictionary and create labels and entries for each key-value pair\n",
    "    # TODO: Add labels to the column headers\n",
    "    for i, (key, value) in enumerate(dict.items()):            \n",
    "            if dict[key][\"type\"] == \"int\" or dict[key][\"type\"] == \"float\":\n",
    "                # Create a label with the key as text\n",
    "                label = tk.Label(root, text=key)\n",
    "                label.grid(row=i, column=0, sticky=\"W\")\n",
    "                # Create an entry with the default value as the default text\n",
    "                default_entry = tk.Entry(root)\n",
    "                default_entry.insert(0, dict[key][\"default\"])\n",
    "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "                # add the lower bound values in column 2\n",
    "                lower_bound_entry = tk.Entry(root)                \n",
    "                lower_bound_entry.insert(0, dict[key][\"lower\"])\n",
    "                lower_bound_entry.grid(row=i, column=2, sticky=\"W\")\n",
    "                # add the upper bound values in column 3\n",
    "                upper_bound_entry = tk.Entry(root)\n",
    "                upper_bound_entry.insert(0, dict[key][\"upper\"])\n",
    "                upper_bound_entry.grid(row=i, column=3, sticky=\"W\")\n",
    "            if dict[key][\"type\"] == \"factor\":        \n",
    "                # Create a label with the key as text\n",
    "                label = tk.Label(root, text=key)\n",
    "                label.grid(row=i, column=0, sticky=\"W\")\n",
    "                # Create an entry with the default value as the default text\n",
    "                default_entry = tk.Entry(root)\n",
    "                default_entry.insert(0, dict[key][\"default\"])\n",
    "                default_entry.grid(row=i, column=1, sticky=\"W\")\n",
    "                # add the lower bound values in column 2\n",
    "                factor_level_entry = tk.Entry(root)\n",
    "                # add a comma to each level\n",
    "                dict[key][\"levels\"] = \", \".join(dict[key][\"levels\"])                                \n",
    "                factor_level_entry.insert(0, dict[key][\"levels\"])\n",
    "                # TODO: Fix columnspan\n",
    "                factor_level_entry.grid(row=i, column=2, columnspan=2, sticky=\"W\")\n",
    "\n",
    "    # Run the tkinter main loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Call the function to create the GUI\n",
    "create_gui(model = 'NetLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gui(model = 'TransformerLightRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save Load Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spotpython.utils.file import save_experiment, load_experiment\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "\n",
    "def test_file_save_load():\n",
    "    fun = analytical().fun_branin\n",
    "\n",
    "    fun_control = fun_control_init(\n",
    "                PREFIX=\"branin\",\n",
    "                SUMMARY_WRITER=False,\n",
    "                lower = np.array([0, 0]),\n",
    "                upper = np.array([10, 10]),\n",
    "                fun_evals=8,\n",
    "                fun_repeats=1,\n",
    "                max_time=inf,\n",
    "                noise=False,\n",
    "                tolerance_x=0,\n",
    "                ocba_delta=0,\n",
    "                var_type=[\"num\", \"num\"],\n",
    "                infill_criterion=\"ei\",\n",
    "                n_points=1,\n",
    "                seed=123,\n",
    "                log_level=20,\n",
    "                show_models=False,\n",
    "                show_progress=True)\n",
    "    design_control = design_control_init(\n",
    "                init_size=5,\n",
    "                repeats=1)\n",
    "    surrogate_control = surrogate_control_init(\n",
    "                model_fun_evals=10000,\n",
    "                min_theta=-3,\n",
    "                max_theta=3,\n",
    "                n_theta=2,\n",
    "                theta_init_zero=True,\n",
    "                n_p=1,\n",
    "                optim_p=False,\n",
    "                var_type=[\"num\", \"num\"],\n",
    "                seed=124)\n",
    "    optimizer_control = optimizer_control_init(\n",
    "                max_iter=1000,\n",
    "                seed=125)\n",
    "    spot_tuner = spot.Spot(fun=fun,\n",
    "                fun_control=fun_control,\n",
    "                design_control=design_control,\n",
    "                surrogate_control=surrogate_control,\n",
    "                optimizer_control=optimizer_control)\n",
    "    # Call the save_experiment function\n",
    "    filename = save_experiment(\n",
    "        spot_tuner=spot_tuner,\n",
    "        fun_control=fun_control,\n",
    "        design_control=None,\n",
    "        surrogate_control=None,\n",
    "        optimizer_control=None\n",
    "    )\n",
    "\n",
    "    # Verify that the pickle file is created\n",
    "    assert os.path.exists(filename)\n",
    "\n",
    "    # Call the load_experiment function\n",
    "    spot_tuner_1, fun_control_1, design_control_1, surrogate_control_1, optimizer_control_1 = load_experiment(filename)\n",
    "\n",
    "    # Verify the name of the pickle file\n",
    "    assert filename == f\"spot_{fun_control['PREFIX']}experiment.pickle\"\n",
    "\n",
    "    # Clean up the temporary directory\n",
    "    os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_save_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netlightregression2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression2 import NetLightRegression2\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "BATCH_SIZE = 8\n",
    "dataset = Diabetes()\n",
    "train1_set, test_set = torch.utils.data.random_split(dataset, [0.6, 0.4])\n",
    "train_set, val_set = torch.utils.data.random_split(train1_set, [0.6, 0.4])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(f\"batch_x.shape: {batch_x.shape}\")\n",
    "print(f\"batch_y.shape: {batch_y.shape}\")\n",
    "net_light_base = NetLightRegression2(l1=128,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='Default',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=10,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=10,  enable_progress_bar=False)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "import torch\n",
    "# data.csv is simple csv file with 11 samples\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.long)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from spotpython.utils.file import save_experiment, load_experiment\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,\n",
    "    design_control_init,\n",
    "    surrogate_control_init,\n",
    "    optimizer_control_init)\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "fun = analytical().fun_branin\n",
    "fun_control = fun_control_init(\n",
    "            PREFIX=\"branin\",\n",
    "            SUMMARY_WRITER=False,\n",
    "            lower = np.array([0, 0]),\n",
    "            upper = np.array([10, 10]),\n",
    "            fun_evals=8,\n",
    "            fun_repeats=1,\n",
    "            max_time=inf,\n",
    "            noise=False,\n",
    "            tolerance_x=0,\n",
    "            ocba_delta=0,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            infill_criterion=\"ei\",\n",
    "            n_points=1,\n",
    "            seed=123,\n",
    "            log_level=20,\n",
    "            show_models=False,\n",
    "            show_progress=True)\n",
    "design_control = design_control_init(\n",
    "            init_size=5,\n",
    "            repeats=1)\n",
    "surrogate_control = surrogate_control_init(\n",
    "            model_fun_evals=10000,\n",
    "            min_theta=-3,\n",
    "            max_theta=3,\n",
    "            n_theta=2,\n",
    "            theta_init_zero=True,\n",
    "            n_p=1,\n",
    "            optim_p=False,\n",
    "            var_type=[\"num\", \"num\"],\n",
    "            seed=124)\n",
    "optimizer_control = optimizer_control_init(\n",
    "            max_iter=1000,\n",
    "            seed=125)\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,\n",
    "            optimizer_control=optimizer_control)\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters(fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tuned Hyperparameters from a Machine/Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.device import getDevice\n",
    "from math import inf\n",
    "from spotpython.utils.init import fun_control_init\n",
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import set_control_key_value\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "\n",
    "MAX_TIME = 1\n",
    "FUN_EVALS = 10\n",
    "INIT_SIZE = 5\n",
    "WORKERS = 0\n",
    "PREFIX=\"037\"\n",
    "DEVICE = getDevice()\n",
    "DEVICES = 1\n",
    "TEST_SIZE = 0.4\n",
    "TORCH_METRIC = \"mean_squared_error\"\n",
    "dataset = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    _torchmetric=TORCH_METRIC,\n",
    "    PREFIX=PREFIX,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    data_set=dataset,\n",
    "    device=DEVICE,\n",
    "    enable_progress_bar=False,\n",
    "    fun_evals=FUN_EVALS,\n",
    "    log_level=50,\n",
    "    max_time=MAX_TIME,\n",
    "    num_workers=WORKERS,\n",
    "    show_progress=True,\n",
    "    test_size=TEST_SIZE,\n",
    "    tolerance_x=np.sqrt(np.spacing(1)),\n",
    "    )\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control\n",
    "add_core_model_to_fun_control(fun_control=fun_control,\n",
    "                              core_model=NetLightRegression,\n",
    "                              hyper_dict=LightHyperDict)\n",
    "from spotpython.hyperparameters.values import set_control_hyperparameter_value\n",
    "\n",
    "set_control_hyperparameter_value(fun_control, \"l1\", [7, 8])\n",
    "set_control_hyperparameter_value(fun_control, \"epochs\", [3, 5])\n",
    "set_control_hyperparameter_value(fun_control, \"batch_size\", [4, 5])\n",
    "set_control_hyperparameter_value(fun_control, \"optimizer\", [\n",
    "                \"Adam\",\n",
    "                \"RAdam\",\n",
    "            ])\n",
    "set_control_hyperparameter_value(fun_control, \"dropout_prob\", [0.01, 0.1])\n",
    "set_control_hyperparameter_value(fun_control, \"lr_mult\", [0.5, 5.0])\n",
    "set_control_hyperparameter_value(fun_control, \"patience\", [2, 3])\n",
    "set_control_hyperparameter_value(fun_control, \"act_fn\",[\n",
    "                \"ReLU\",\n",
    "                \"LeakyReLU\"\n",
    "            ] )\n",
    "from spotpython.utils.init import design_control_init, surrogate_control_init\n",
    "design_control = design_control_init(init_size=INIT_SIZE)\n",
    "\n",
    "surrogate_control = surrogate_control_init(noise=True,\n",
    "                                            n_theta=2)\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "fun = HyperLight(log_level=50).fun\n",
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                       fun_control=fun_control,\n",
    "                       design_control=design_control,\n",
    "                       surrogate_control=surrogate_control)\n",
    "spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.get_tuned_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors\n",
    "\n",
    "* Example from https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/012_num_spot_ei.html#factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.design.spacefilling import spacefilling\n",
    "from spotpython.build.kriging import Kriging\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = spacefilling(2)\n",
    "n = 30\n",
    "rng = np.random.RandomState(1)\n",
    "lower = np.array([-5,-0])\n",
    "upper = np.array([10,15])\n",
    "fun = analytical().fun_branin_factor\n",
    "#fun = analytical(sigma=0).fun_sphere\n",
    "\n",
    "X0 = gen.scipy_lhd(n, lower=lower, upper = upper)\n",
    "X1 = np.random.randint(low=0, high=3, size=(n,))\n",
    "X = np.c_[X0, X1]\n",
    "y = fun(X)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "S = Kriging(name='kriging',  seed=123, log_level=10, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\n",
    "S.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sf = Kriging(name='kriging',  seed=123, log_level=10, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"factor\"])\n",
    "# Sf = Kriging(name='kriging',  seed=123, log_level=50, n_theta=3, noise=False, var_type=[\"num\", \"num\", \"num\"])\n",
    "Sf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0\n",
    "for _ in range(100):\n",
    "    n = 100\n",
    "    X0 = gen.scipy_lhd(n, lower=lower, upper = upper)\n",
    "    X1 = np.random.randint(low=0, high=3, size=(n,))\n",
    "    X = np.c_[X0, X1]\n",
    "    y = fun(X)\n",
    "    s=np.sum(np.abs(S.predict(X) - y))\n",
    "    sf=np.sum(np.abs(Sf.predict(X) - y))\n",
    "    res = res + (sf - s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def select_distant_points(X, y, k):\n",
    "    \"\"\"\n",
    "    Selects k points that are distant from each other using a clustering approach.\n",
    "    \n",
    "    :param X: np.array of shape (n, k), with n points in k-dimensional space.\n",
    "    :param y: np.array of length n, with values corresponding to each point in X.\n",
    "    :param k: The number of distant points to select.\n",
    "    :return: Selected k points from X and their corresponding y values.\n",
    "    \"\"\"\n",
    "    # Perform k-means clustering to find k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n",
    "    \n",
    "    # Find the closest point in X to each cluster center\n",
    "    selected_points = np.array([X[np.argmin(np.linalg.norm(X - center, axis=1))] for center in kmeans.cluster_centers_])\n",
    "    \n",
    "    # Find indices of the selected points in the original X array\n",
    "    indices = np.array([np.where(np.all(X==point, axis=1))[0][0] for point in selected_points])\n",
    "    \n",
    "    # Select the corresponding y values\n",
    "    selected_y = y[indices]\n",
    "    \n",
    "    return selected_points, selected_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 2)  # Generate some random points\n",
    "y = np.random.rand(100)     # Random corresponding y values\n",
    "k = 5\n",
    "\n",
    "selected_points, selected_y = select_distant_points(X, y, k)\n",
    "print(\"Selected Points:\", selected_points)\n",
    "print(\"Corresponding y values:\", selected_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 5\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1, -1]),\n",
    "    upper = np.array([1, 1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "S.plot_important_hyperparameter_contour(max_imp=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n",
    "\n",
    "# Sorting the array in descending order by the second element of each sub-list\n",
    "sorted_array = sorted(array, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(sorted_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_second_and_return_indices(array):\n",
    "    \"\"\"\n",
    "    Sorts an array of arrays based on the second values in descending order and returns\n",
    "    the indices of the original array entries.\n",
    "\n",
    "    :param array: List of lists, where each inner list has at least two elements.\n",
    "    :return: Indices of the original array entries after sorting by the second value.\n",
    "             Returns an empty list if the input is empty or None.\n",
    "    :raises ValueError: If any sub-array is improperly structured.\n",
    "    \"\"\"\n",
    "    if not array:\n",
    "        return []\n",
    "\n",
    "    # Check for improperly structured sub-arrays\n",
    "    for item in array:\n",
    "        if not isinstance(item, list) or len(item) < 2:\n",
    "            raise ValueError(\"All sub-arrays must be lists with at least two elements.\")\n",
    "\n",
    "    # Enumerate the array to keep track of original indices, then sort by the second item\n",
    "    sorted_indices = [index for index, value in sorted(enumerate(array), key=lambda x: x[1][1], reverse=True)]\n",
    "\n",
    "    return sorted_indices\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    array = [['x0', 85.50983192204619], ['x1', 100.0], ['x2', 81.35712613549178]]\n",
    "    indices = sort_by_second_and_return_indices(array)\n",
    "    print(\"Indices of the sorted elements:\", indices)\n",
    "except ValueError as error:\n",
    "    print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Core Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree import HoeffdingAdaptiveTreeRegressor\n",
    "from spotriver.data.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.hyperparameters.values import add_core_model_to_fun_control, get_default_hyperparameters_for_core_model, get_default_values\n",
    "fun_control = {}\n",
    "add_core_model_to_fun_control(core_model=HoeffdingAdaptiveTreeRegressor,\n",
    "    fun_control=fun_control,\n",
    "    hyper_dict=RiverHyperDict,\n",
    "    filename=None)\n",
    "values = get_default_values(fun_control)\n",
    "print(values)\n",
    "# get_default_hyperparameters_for_core_model(fun_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytest\n",
    "import pprint\n",
    "from spotpython.plot.xai import (get_activations, get_gradients, get_weights, plot_nn_values_hist, plot_nn_values_scatter, visualize_weights, visualize_gradients, visualize_activations, visualize_activations_distributions, visualize_gradient_distributions, visualize_weights_distributions)\n",
    "\n",
    "def test_plot_nn_values_scatter_reshaped_values():\n",
    "    # Mock data for testing\n",
    "    nn_values = {\n",
    "        'layer0': np.random.rand(10),  # 10 values suggesting padding for a 4x4\n",
    "        'layer1': np.random.rand(64),  # 64 values suggesting a perfect square (8x8)\n",
    "        'layer2': np.random.rand(32),  # 32 values suggesting  padding for a 6x6\n",
    "        'layer3': np.random.rand(16),  # 16 values suggesting a perfect square (4x4)\n",
    "    }\n",
    "\n",
    "    # Use the modified function that returns reshaped_values for testing\n",
    "    reshaped_values = plot_nn_values_scatter(nn_values, 'Test NN', return_reshaped=True)    \n",
    "\n",
    "    pprint.pprint(nn_values)\n",
    "    pprint.pprint(reshaped_values)\n",
    "    \n",
    "\n",
    "    # Assert for layer0: Checks if reshaping is correct for perfect square\n",
    "    assert reshaped_values['layer0'].shape == (4, 4)\n",
    "    # Assert for layer1: Checks if reshaping is correct for non-square\n",
    "    assert reshaped_values['layer1'].shape == (8, 8)\n",
    "    assert reshaped_values['layer2'].shape == (6, 6)\n",
    "    assert reshaped_values['layer3'].shape == (4, 4)\n",
    "\n",
    "\n",
    "\n",
    "test_plot_nn_values_scatter_reshaped_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.convert import set_dataset_target_type\n",
    "import pandas as pd\n",
    "dataset = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9], \"y\": [True, False, True]})\n",
    "print(dataset)\n",
    "dataset = set_dataset_target_type(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import river.tree\n",
    "core_model_name = \"tree.HoeffdingTreeRegressor\"\n",
    "core_model_module = core_model_name.split(\".\")[0]\n",
    "coremodel = core_model_name.split(\".\")[1]\n",
    "core_model_instance = getattr(getattr(river, core_model_module), coremodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.friedman import FriedmanDriftDataset\n",
    "import matplotlib.pyplot as plt\n",
    "data_generator = FriedmanDriftDataset(n_samples=100, seed=42, change_point1=50, change_point2=75, constant=False)\n",
    "data = [data for data in data_generator]\n",
    "indices = [i for _, _, i in data]\n",
    "values = {f\"x{i}\": [] for i in range(5)}\n",
    "values[\"y\"] = []\n",
    "for x, y, _ in data:\n",
    "    for i in range(5):\n",
    "        values[f\"x{i}\"].append(x[i])\n",
    "    values[\"y\"].append(y)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for label, series in values.items():\n",
    "    plt.plot(indices, series, label=label)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('')\n",
    "plt.axvline(x=50, color='k', linestyle='--', label='Drift Point 1')\n",
    "plt.axvline(x=75, color='r', linestyle='--', label='Drift Point 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scaler for Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from spotpython.utils.scaler import TorchStandardScaler\n",
    "import torch\n",
    "\n",
    "scaler=TorchStandardScaler()\n",
    "\n",
    "dataset = CSVDataset(csv_file='data.csv', target_column='prognosis', feature_type=torch.float64)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "import numpy as np\n",
    "np.max(diabetes.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.data.pkldataset import PKLDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "dataset = Diabetes()\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 1\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "dataset = CaliforniaHousing()\n",
    "print(dataset.get_names())\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "dataset = CaliforniaHousing()\n",
    "print(dataset.data.shape)\n",
    "print(dataset.targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "import torch\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "# Set batch size for DataLoader\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "import torch\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=2, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "print(f\"Training set size: {len(data_module.data_train)}\")\n",
    "print(f\"Validation set size: {len(data_module.data_val)}\")\n",
    "print(f\"Test set size: {len(data_module.data_test)}\")\n",
    "# print the first batch of the training set from data_module.data_train\n",
    "print(next(iter(data_module.train_dataloader())))\n",
    "# print the first batch of the training set from data_module.data_train as a numpy array\n",
    "print(next(iter(data_module.train_dataloader()))[0].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "from spotpython.data.csvdataset import CSVDataset\n",
    "from spotpython.utils.scaler import TorchStandardScaler, TorchMinMaxScaler\n",
    "from spotpython.data.california_housing import CaliforniaHousing\n",
    "\n",
    "\n",
    "dataset = CaliforniaHousing(feature_type=torch.float32, target_type=torch.float32)\n",
    "scaler = TorchMinMaxScaler()\n",
    "data_module = LightDataModule(dataset=dataset, batch_size=5, test_size=0.5, scaler=scaler)\n",
    "data_module.setup()\n",
    "\n",
    "loader = data_module.train_dataloader\n",
    "\n",
    "total_sum = None\n",
    "total_count = 0\n",
    "\n",
    "# Iterate over batches in the DataLoader\n",
    "for batch in loader():\n",
    "    inputs, targets = batch\n",
    "    \n",
    "\n",
    "total_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.netlightregression import NetLightRegression\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "def test_net_light_regression_class():\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "    dataset = Diabetes()\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    net_light_regression = NetLightRegression(\n",
    "        l1=128,\n",
    "        epochs=10,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        initialization=\"Default\",\n",
    "        act_fn=nn.ReLU(),\n",
    "        optimizer=\"Adam\",\n",
    "        dropout_prob=0.1,\n",
    "        lr_mult=0.1,\n",
    "        patience=5,\n",
    "        _L_in=10,\n",
    "        _L_out=1,\n",
    "        _torchmetric=\"mean_squared_error\",\n",
    "    )\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=2,\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer.fit(net_light_regression, train_loader, val_loader)\n",
    "    res = trainer.test(net_light_regression, test_loader)\n",
    "    # test if the entry 'hp_metric' is in the res dict\n",
    "    assert \"hp_metric\" in res[0].keys()\n",
    "\n",
    "test_net_light_regression_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_int_hyperparameter_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_int_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"forest.AMFRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_int_hyperparameter_values(fun_control, \"n_estimators\", 2, 5)\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_factor_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_factor_hyperparameter_values(fun_control, \"leaf_model\", ['LinearRegression',\n",
    "                                                     'Perceptron'])\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "\n",
    "set_factor_hyperparameter_values(fun_control, \"leaf_model\", [\"LinearRegression\",\n",
    "                                                                \"Perceptron\"])\n",
    "\n",
    "# Access updated hyperparameters\n",
    "updated_hyperparameters = fun_control[\"core_model_hyper_dict\"]\n",
    "print(updated_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.hyperparameters.values import set_boolean_hyperparameter_values\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "fun_control = fun_control_init(\n",
    "    core_model_name=\"forest.AMFRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    ")\n",
    "print(\"Before modification:\")\n",
    "print(gen_design_table(fun_control))\n",
    "set_boolean_hyperparameter_values(fun_control, \"use_aggregation\", 0, 0)\n",
    "print(\"After modification:\")\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, full_dataset, train_size=0.8, batch_size=32, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.dataset = full_dataset\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Split the dataset\n",
    "        train_len = int(len(self.dataset) * self.train_size)\n",
    "        val_len = len(self.dataset) - train_len\n",
    "        self.train_set, self.val_set = random_split(self.dataset, [train_len, val_len])\n",
    "        \n",
    "        # Fit scaler on training data\n",
    "        train_data = torch.stack([item[0] for item in self.train_set])\n",
    "        print(f\"train_data before scaling\\n: {train_data}\")  \n",
    "        self.scaler.fit(train_data)\n",
    "       \n",
    "        # Transform training data\n",
    "        scaled_train_data = self.scaler.transform(train_data)\n",
    "        self.train_set = self._update_dataset(self.train_set, scaled_train_data)\n",
    "        print(f\"train_data after scaling\\n: {self.train_set}\")  \n",
    "        \n",
    "        # Transform validation data\n",
    "        val_data = torch.stack([item[0] for item in self.val_set])\n",
    "        scaled_val_data = self.scaler.transform(val_data)\n",
    "        self.val_set = self._update_dataset(self.val_set, scaled_val_data)\n",
    "\n",
    "    def _update_dataset(self, original_dataset, scaled_data):\n",
    "        updated_dataset = []\n",
    "        for i, (data, label) in enumerate(original_dataset):\n",
    "            updated_dataset.append((torch.tensor(scaled_data[i]), label))\n",
    "        return updated_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_data = torch.stack([item[0] for item in self.test_set])\n",
    "        scaled_test_data = self.scaler.transform(test_data)\n",
    "        self.test_set = self._update_dataset(self.test_set, scaled_test_data)\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Here you can download datasets if needed\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a 3-dimensional tensor with 1000 samples\n",
    "n = 10\n",
    "data = torch.rand((n, 3))\n",
    "print(f\"data: {data}\")\n",
    "labels = torch.tensor([i % 2 for i in range(n)], dtype=torch.float32)\n",
    "print(f\"labels: {labels}\")\n",
    "full_dataset = MyDataset(data, labels)\n",
    "\n",
    "# Creating DataModule instance\n",
    "data_module = MyDataModule(full_dataset)\n",
    "\n",
    "# Setup the data module\n",
    "data_module.setup()\n",
    "\n",
    "# Example of fetching a single batch\n",
    "train_loader = data_module.train_dataloader()\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch data shape: {batch[0].shape}\")\n",
    "    x, y = batch\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, do not delete the following imports, they are needed for the function add_core_model_to_fun_control\n",
    "import river\n",
    "from river import forest, tree, linear_model, rules\n",
    "from river import preprocessing\n",
    "import sklearn.metrics\n",
    "import spotpython\n",
    "from spotpython.light import regression\n",
    "\n",
    "def get_core_model_from_name(core_model_name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns the river core model name and instance from a core model name.\n",
    "\n",
    "    Args:\n",
    "        core_model_name (str): The full name of the core model in the format 'module.Model'.\n",
    "\n",
    "    Returns:\n",
    "        (str, object): A tuple containing the core model name and an instance of the core model.\n",
    "    \"\"\"\n",
    "    # Split the model name into its components\n",
    "    name_parts = core_model_name.split(\".\")\n",
    "    \n",
    "    if len(name_parts) < 2:\n",
    "        raise ValueError(f\"Invalid core model name: {core_model_name}. Expected format: 'module.ModelName'.\")\n",
    "\n",
    "    module_name = name_parts[0]\n",
    "    model_name = name_parts[1]\n",
    "    \n",
    "    try:\n",
    "        # Try to get the model from the river library\n",
    "        core_model_instance = getattr(getattr(river, module_name), model_name)\n",
    "        return model_name, core_model_instance\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            # Try to get the model from the spotpython library\n",
    "            submodule_name = name_parts[1]\n",
    "            model_name = name_parts[2] if len(name_parts) == 3 else model_name\n",
    "            print(f\"module_name: {module_name}\")\n",
    "            print(f\"submodule_name: {submodule_name}\")\n",
    "            print(f\"model_name: {model_name}\")\n",
    "            core_model_instance = getattr(getattr(getattr(spotpython, module_name), submodule_name), model_name)\n",
    "            return model_name, core_model_instance\n",
    "        except AttributeError:\n",
    "            raise ValueError(f\"Model '{core_model_name}' not found in either 'river' or 'spotpython' libraries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of usage\n",
    "model_name, model_instance = get_core_model_from_name('tree.HoeffdingTreeRegressor')\n",
    "print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, model_instance = get_core_model_from_name(\"light.regression.NNLinearRegressor\")\n",
    "print(f\"Model Name: {model_name}, Model Instance: {model_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 8\n",
    "dataset = Diabetes()\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=10,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes(return_X_y=False, as_frame=True)\n",
    "# svaing the data to a csv file\n",
    "data.frame.to_csv('~/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "n_features = 2\n",
    "n_samples = 500\n",
    "target_column = \"y\"\n",
    "ds =  make_moons(n_samples, noise=0.5, random_state=0)\n",
    "X, y = ds\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1, 1))))\n",
    "test = pd.DataFrame(np.hstack((X_test, y_test.reshape(-1, 1))))\n",
    "train.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "test.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "train.head()\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.concat([train, test])\n",
    "data.to_csv('moon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_classification(n_samples=1000, n_features=20,  n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('binary_classification.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_classification(n_samples=1000, n_features=20,  n_informative=9, n_redundant=2, n_repeated=0, n_classes=10, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('multiple_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
    "# combine the training and test data and save to a csv file\n",
    "data = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))))\n",
    "data.columns = [f\"x{i}\" for i in range(1, 21)] + [\"y\"]\n",
    "data.to_csv('regression.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "data = load_iris(as_frame=True)\n",
    "data.frame.to_csv('iris.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotGUI.tuner.spotRun import get_report_file_name\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(PREFIX=\"test\")\n",
    "get_report_file_name(fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotGUI.tuner.spotRun import get_scenario_dict\n",
    "import pprint\n",
    "dic = get_scenario_dict(\"sklearn\")\n",
    "pprint.pprint(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.utils.io import load_hcf_dataframe, hcf_df2tensor\n",
    "from pyhcf.utils.names import load_all_features_N_regression_list\n",
    "from torch.utils.data import DataLoader\n",
    "df = load_hcf_dataframe(A=True,\n",
    "    H=True,\n",
    "    param_list=load_all_features_N_regression_list(),\n",
    "    target='N',\n",
    "    rmNA=True,\n",
    "    rmMF=True,\n",
    "    rmV=4,\n",
    "    min_freq=1000,\n",
    "    incl_drossel=False)\n",
    "dataset = hcf_df2tensor(df, target='N', return_X_y=False)\n",
    "print(type(dataset))\n",
    "print(len(dataset))\n",
    "# save the 'TensorDataset' object to a pkl file\n",
    "# import pickle\n",
    "# with open('hcf_dataset.pkl', 'wb') as f:\n",
    "#     pickle.dump(dataset, f)\n",
    "# load the 'TensorDataset' object from the pkl file\n",
    "# with open('hcf_dataset.pkl', 'rb') as f:\n",
    "#     dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0)\n",
    "# get the dimensions of the first sample\n",
    "dataset.__getitem__(0)[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "X0, y0 = S.generate_random_point()\n",
    "print(f\"X0: {X0}\")\n",
    "print(f\"y0: {y0}\")\n",
    "assert X0.size == 2\n",
    "assert y0.size == 1\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "assert y0 >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import get_spot_tensorboard_path\n",
    "get_spot_tensorboard_path(\"00_ubuntu_2021-08-31_14-30-00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import get_experiment_name\n",
    "get_experiment_name(prefix=\"00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 5\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1, -1]),\n",
    "    upper = np.array([1, 1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "design_control=design_control_init(init_size=ni)\n",
    "surrogate_control=surrogate_control_init(n_theta=3)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,\n",
    "            surrogate_control=surrogate_control,)\n",
    "S.run()\n",
    "S.plot_important_hyperparameter_contour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import design_control_init\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "design_control = design_control_init(init_size=3)\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals=fun_evals,\n",
    "    tolerance_x = np.sqrt(np.spacing(1))\n",
    "    )\n",
    "S = spot.Spot(fun = analytical().fun_sphere,\n",
    "              fun_control = fun_control,\n",
    "              design_control = design_control)\n",
    "X = S.generate_design(size=3, repeats=1, lower=np.array([0, 0]), upper=np.array([100, 1]))\n",
    "assert X.shape[0] == 3\n",
    "assert X.shape[1] == 2\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import inf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from spotriver.utils.data_conversion import convert_to_df\n",
    "from river.datasets import synth\n",
    "import warnings\n",
    "if not os.path.exists('./figures'):\n",
    "    os.makedirs('./figures')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "PREFIX=\"TEST_SAVE\"\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from river.datasets import synth\n",
    "from spotriver.utils.data_conversion import convert_to_df\n",
    "from math import inf\n",
    "import numpy as np\n",
    "from spotriver.fun.hyperriver import HyperRiver\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "\n",
    "\n",
    "target_column = \"y\"\n",
    "metric = mean_absolute_error\n",
    "horizon = 7*24\n",
    "n_train = horizon\n",
    "p_1 = int(n_train/4)\n",
    "p_2 = int(n_train/2)\n",
    "position=(p_1, p_2)\n",
    "dataset_train = synth.FriedmanDrift(\n",
    "   drift_type='gra',\n",
    "   position=position,\n",
    "   seed=123\n",
    ")\n",
    "\n",
    "train = convert_to_df(dataset_train, n_total=n_train)\n",
    "train.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n",
    "\n",
    "\n",
    "n_val = 10_000\n",
    "p_1 = int(n_val/4)\n",
    "p_2 = int(n_val/2)\n",
    "position=(p_1, p_2)\n",
    "dataset_val = synth.FriedmanDrift(\n",
    "   drift_type='gra',\n",
    "   position=position,\n",
    "   seed=124\n",
    ")\n",
    "val = convert_to_df(dataset_val, n_total=n_val)\n",
    "val.columns = [f\"x{i}\" for i in range(1, 11)] + [target_column]\n",
    "\n",
    "from math import inf\n",
    "import numpy as np\n",
    "from spotriver.fun.hyperriver import HyperRiver\n",
    "from spotriver.hyperdict.river_hyper_dict import RiverHyperDict\n",
    "from spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init, optimizer_control_init\n",
    "\n",
    "fun = HyperRiver().fun_oml_horizon\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    TENSORBOARD_CLEAN=False,\n",
    "    tensorboard_start=False,\n",
    "    tensorboard_stop=False,\n",
    "    fun_evals=inf,\n",
    "    max_time=0.1,\n",
    "\n",
    "    prep_model_name=\"StandardScaler\",\n",
    "    test=val, # tuner uses the validation set as test set\n",
    "    train=train,\n",
    "    target_column=target_column,\n",
    "\n",
    "    metric_sklearn_name=\"mean_absolute_error\",\n",
    "    horizon=7*24,\n",
    "    oml_grace_period=7*24,\n",
    "    weight_coeff=0.0,\n",
    "    weights=np.array([100, 0.1, 0.1]),\n",
    "\n",
    "    core_model_name=\"tree.HoeffdingTreeRegressor\",\n",
    "    hyperdict=RiverHyperDict,\n",
    "   )\n",
    "\n",
    "\n",
    "design_control = design_control_init(\n",
    "    init_size=3,\n",
    ")\n",
    "\n",
    "surrogate_control = surrogate_control_init(\n",
    "    noise=True,\n",
    "    n_theta=2,\n",
    "    min_Lambda=0.001,\n",
    "    max_Lambda=100,\n",
    ")\n",
    "\n",
    "optimizer_control = optimizer_control_init()\n",
    "\n",
    "from spotpython.spot import spot\n",
    "spot_tuner = spot.Spot(\n",
    "    fun=fun,\n",
    "    fun_control=fun_control,\n",
    "    design_control=design_control,\n",
    "    surrogate_control=surrogate_control,\n",
    "    optimizer_control=optimizer_control,\n",
    ")\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.file import load_and_run_spot_python_experiment\n",
    "spot_tuner = load_and_run_spot_python_experiment(\"spot_000_experiment.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_div2_list(n, n_min) -> list:\n",
    "    \"\"\"\n",
    "    Generate a list of numbers from n to n_min (inclusive) by dividing n by 2\n",
    "    until the result is less than n_min.\n",
    "    This function starts with n and keeps dividing it by 2 until n_min is reached.\n",
    "    The number of times each value is added to the list is determined by n // current.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number to start with.\n",
    "        n_min (int): The minimum number to stop at.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of numbers from n to n_min (inclusive).\n",
    "\n",
    "    Examples:\n",
    "        _generate_div2_list(10, 1)\n",
    "        [10, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        _generate_div2_list(10, 2)\n",
    "        [10, 5, 5, 2, 2, 2, 2, 2]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    current = n\n",
    "    repeats = 1\n",
    "    max_repeats = 4\n",
    "    while current >= n_min:\n",
    "        result.extend([current] * min(repeats, max_repeats))\n",
    "        current = current // 2\n",
    "        repeats = repeats + 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_generate_div2_list(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_generate_div2_list(128, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_sizes(_L_in = 80, l1=2**9):\n",
    "    n_low = _L_in // 4\n",
    "    n_high = max(l1, 2 * n_low)\n",
    "    hidden_sizes = _generate_div2_list(n_high, n_low)\n",
    "    return hidden_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_hidden_sizes(l1=2**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhcf.data.daten_lightv2 import DatenLightV2\n",
    "from pyhcf.utils.io import hcf_df2tensor\n",
    "df = DatenLightV2().load()\n",
    "print(f\"Datensatz der Größe {df.shape} erfolgreich geladen.\")\n",
    "print(df.columns.to_list())\n",
    "dataset = hcf_df2tensor(df, target='N', return_X_y=False)\n",
    "batch_size = 5\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# Iterate over the data in the DataLoader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Batch Size: {inputs.size(0)}\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Targets Shape: {targets.shape}\")\n",
    "    print(\"---------------\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Network Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we generate an 80-dim dataframe with 10000 samples, where the first two columns are random integers and the rest are random floats.\n",
    "* Then, we generate a target variable as the sum of the squared values.\n",
    "* The dataframe is converted to a tensor and split into a training, validation, and testing set. The corresponding data loaders are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "np.random.seed(42)\n",
    "n_samples = 10_000\n",
    "n_int = 2\n",
    "n_float = 76\n",
    "input_dim = n_int + n_float\n",
    "output_dim = 1\n",
    "data = np.random.rand(n_samples, n_float)\n",
    "data = np.hstack((np.random.randint(0, 10, (n_samples, n_int)), data))\n",
    "df = pd.DataFrame(data)\n",
    "df['y'] = np.sum(df.iloc[:, 2:]**2, axis=1)\n",
    "df.head()\n",
    "X = torch.tensor(df.iloc[:, :-1].values, dtype=torch.float32)\n",
    "y = torch.tensor(df.iloc[:, -1].values, dtype=torch.float32)\n",
    "dataset = TensorDataset(X, y)\n",
    "print(f\"Dataset with input tensor shape: {dataset.tensors[0].shape}\")\n",
    "print(f\"Dataset with target tensor shape: {dataset.tensors[1].shape}\")\n",
    "# print(dataset[0][0])\n",
    "# print(dataset[0][1])\n",
    "train_size_0 = int(0.8 * len(dataset))\n",
    "train_size = int(0.8 * train_size_0)\n",
    "val_size = train_size_0 - train_size\n",
    "test_size = len(dataset) - train_size_0\n",
    "train_dataset_0, test_dataset = random_split(dataset, [train_size_0, test_size])\n",
    "train_dataset, val_dataset = random_split(train_dataset_0, [train_size, val_size])\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NNLinearRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                    batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=input_dim,\n",
    "                                    _L_out=output_dim,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from spotpython.hyperparameters.optimizer import optimizer_handler\n",
    "import torchmetrics.functional.regression\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "\n",
    "class SettingsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset to handle settings-based data.\"\"\"\n",
    "    def __init__(self, dataframe, settings_columns, target_column):\n",
    "        self.dataframe = dataframe\n",
    "        self.settings_columns = settings_columns\n",
    "        self.target_column = target_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        settings = tuple(self.dataframe.iloc[idx][self.settings_columns])\n",
    "        features = self.dataframe.iloc[idx].drop(self.settings_columns + [self.target_column]).values\n",
    "        target = self.dataframe.iloc[idx][self.target_column]\n",
    "        return settings, torch.tensor(features, dtype=torch.float32), torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "class FilteredNNLinearRegressor:\n",
    "    def __init__(self, settings_columns, data, target_column='target', **nn_kwargs):\n",
    "        self.settings_columns = settings_columns\n",
    "        self.models = {}\n",
    "        self.nn_kwargs = nn_kwargs\n",
    "        self.data = data\n",
    "        self.target_column = target_column\n",
    "        self.prepare_networks()\n",
    "\n",
    "    def prepare_networks(self):\n",
    "        settings_combinations = self.data[self.settings_columns].drop_duplicates().to_records(index=False)\n",
    "        i = 0\n",
    "        for combination in settings_combinations:\n",
    "            print(f\"Combination {i}: {combination}\")\n",
    "            i += 1\n",
    "            self.models[combination] = NNLinearRegressor(**self.nn_kwargs)\n",
    "\n",
    "    def feature_filter(self, settings):\n",
    "        \"\"\"Filter the data based on given settings tuple.\"\"\"\n",
    "        df_filtered = self.data[(self.data[self.settings_columns] == pd.Series(settings)).all(axis=1)]\n",
    "        print(f\"df_filtered: {df_filtered}\")\n",
    "        return df_filtered\n",
    "\n",
    "    def train(self, trainer_kwargs):\n",
    "        # Split data and train each model separately\n",
    "        for settings, model in self.models.items():\n",
    "            filtered_data = self.feature_filter(settings)\n",
    "            dataset = SettingsDataset(filtered_data, self.settings_columns, self.target_column)\n",
    "            train_loader = DataLoader(dataset, batch_size=self.nn_kwargs['batch_size'])\n",
    "            trainer = L.Trainer(**trainer_kwargs)\n",
    "            trainer.fit(model, train_loader)\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        predictions = {}\n",
    "        for settings, model in self.models.items():\n",
    "            filtered_data = self.feature_filter(settings)\n",
    "            if not filtered_data.empty:\n",
    "                dataset = SettingsDataset(filtered_data, self.settings_columns, self.target_column)\n",
    "                test_loader = DataLoader(dataset, batch_size=self.nn_kwargs['batch_size'])\n",
    "                trainer = L.Trainer()\n",
    "                preds = trainer.predict(model, test_loader)\n",
    "                predictions[settings] = preds\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'setting1': [-1, -1, 1, 1],\n",
    "    'setting2': ['A', 'B', 'A', 'B'],\n",
    "    'feature1': [0.1, 0.2, 0.3, 0.4],\n",
    "    'feature2': [0.5, 0.6, 0.7, 0.8],\n",
    "    'target': [1, 2, 3, 4]\n",
    "})\n",
    "\n",
    "settings_columns = ['setting1', 'setting2']\n",
    "target_column = 'target'\n",
    "nn_kwargs = {\n",
    "    'l1': 16,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 2,\n",
    "    'initialization': 'xavier',\n",
    "    'act_fn': nn.ReLU(),\n",
    "    'optimizer': 'Adam',\n",
    "    'dropout_prob': 0.1,\n",
    "    'lr_mult': 0.1,\n",
    "    'patience': 2,\n",
    "    'batch_norm': True,\n",
    "    '_L_in': 2,  # For this example, 2 features besides settings\n",
    "    '_L_out': 1,\n",
    "    '_torchmetric': \"mean_squared_error\",\n",
    "}\n",
    "\n",
    "multi_network = FilteredNNLinearRegressor(settings_columns, data, target_column, **nn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = {'max_epochs': 2,  'enable_progress_bar': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_network.train(trainer_kwargs)\n",
    "predictions = multi_network.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explanation:\n",
    "1. SettingsDataset: A custom dataset class that includes settings as part of the data. Each row has a tuple of settings, feature values, and the target value.\n",
    "\n",
    "2. FilteredNNLinearRegressor: An umbrella class that handles setting combinations and assigns each its own `NNLinearRegressor` model instance. It trains these models using only relevant data filtered by the `feature_filter()` function.\n",
    "\n",
    "3. Feature Filtering: The `feature_filter()` function uses Pandas to filter rows based on their relevant setting information before creating dataset and loader instances for each unique settings combination.\n",
    "\n",
    "4. Training and Prediction: We generate and train separate models for each settings combination and then predict using test data filtered similarly using the defined `feature_filter()`.\n",
    "\n",
    "This approach provides modularity as each model is logically separated based on settings, while utilizing your existing class structure to individually specify training processes and handle data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.plot.xai import get_gradients\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "gradients = get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "# assert that the gradients are a dictionary with keys that contain the string 'layers' and values that are arrays\n",
    "assert all([key in gradients.keys() for key in gradients.keys()])\n",
    "assert all([isinstance(value, np.ndarray) for value in gradients.values()])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.plot.xai import get_gradients\n",
    "import numpy as np\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CondNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConditionalLayer(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, output_dim):\n",
    "        super(ConditionalLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.condition_fc = nn.Linear(condition_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # Basic linear transformation\n",
    "        base_output = self.fc(x)\n",
    "        # Compute a condition-dependent transformation\n",
    "        condition_output = self.condition_fc(condition)\n",
    "        # Modulate the output by adding the condition-dependent transformation\n",
    "        output = base_output + condition_output\n",
    "        return F.relu(output)\n",
    "\n",
    "class ConditionalNet(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, hidden_dim, output_dim):\n",
    "        super(ConditionalNet, self).__init__()\n",
    "        self.cond_layer1 = ConditionalLayer(input_dim, condition_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        x = self.cond_layer1(x, condition)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_dim = 10\n",
    "condition_dim = 2  # For instance, if you have two conditional features like region and season\n",
    "hidden_dim = 20\n",
    "output_dim = 1\n",
    "\n",
    "model = ConditionalNet(input_dim, condition_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Example data\n",
    "x = torch.randn(5, input_dim)\n",
    "condition = torch.randn(5, condition_dim)\n",
    "\n",
    "output = model(x, condition)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.light.regression import NNCondNetRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 64\n",
    "# generate data\n",
    "num_samples = 1_000\n",
    "input_dim = 10\n",
    "cond_dim = 2\n",
    "X = torch.randn(num_samples, input_dim)  # random data for example\n",
    "Y = torch.randn(num_samples, 1)  # random target for example\n",
    "data_set = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNCondNetRegressor(l1=128,\n",
    "                                batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_cond=cond_dim,\n",
    "                                    _L_in=input_dim - cond_dim,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# validation and test should give the same result, because the data is the same\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CondNet Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.file import get_experiment_filename\n",
    "from math import inf\n",
    "from spotpython.hyperparameters.values import set_hyperparameter\n",
    "\n",
    "PREFIX=\"4000\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "input_dim = 10\n",
    "output_dim = 1\n",
    "cond_dim = 2\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    fun_evals=inf,\n",
    "    max_time=1,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNCondNetRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=input_dim - cond_dim,\n",
    "    _L_out=1,\n",
    "    _L_cond=cond_dim,)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "\n",
    "set_hyperparameter(fun_control, \"optimizer\", [ \"Adadelta\", \"Adam\", \"Adamax\"])\n",
    "set_hyperparameter(fun_control, \"l1\", [3,4])\n",
    "set_hyperparameter(fun_control, \"epochs\", [3,7])\n",
    "set_hyperparameter(fun_control, \"batch_size\", [4,5])\n",
    "set_hyperparameter(fun_control, \"dropout_prob\", [0.0, 0.025])\n",
    "set_hyperparameter(fun_control, \"patience\", [2,3])\n",
    "set_hyperparameter(fun_control, \"lr_mult\", [0.1, 20.0])\n",
    "\n",
    "design_control = design_control_init(init_size=10)\n",
    "\n",
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_weights(net, return_index=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the weights of a neural network and the size of each layer.\n",
    "\n",
    "    Args:\n",
    "        net (object):\n",
    "            A neural network.\n",
    "        return_index (bool, optional):\n",
    "            Whether to return the index. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            A tuple containing:\n",
    "            - weights: A dictionary with the weights of the neural network.\n",
    "            - index: The layer index list (only if return_index is True).\n",
    "            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "\n",
    "    Examples:\n",
    "        # Example usage (as described in the original function's docstring)\n",
    "    \"\"\"\n",
    "    weights = {}\n",
    "    index = []\n",
    "    layer_sizes = {}\n",
    "    \n",
    "    for name, param in net.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            continue\n",
    "        \n",
    "        # Extract layer number\n",
    "        layer_number = int(name.split(\".\")[1])\n",
    "        index.append(layer_number)\n",
    "        \n",
    "        # Create dictionary key for this layer\n",
    "        key_name = f\"Layer {layer_number}\"\n",
    "        \n",
    "        # Store weight information\n",
    "        weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
    "        \n",
    "        # Store layer size as a NumPy array\n",
    "        layer_sizes[key_name] = np.array(param.size())\n",
    "    \n",
    "    if return_index:\n",
    "        return weights, index, layer_sizes\n",
    "    else:\n",
    "        return weights, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotpython.plot.xai import get_weights\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "weights, layer_sizes = get_weights(net=model)\n",
    "weights, layer_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import math\n",
    "\n",
    "def plot_nn_values_scatter(\n",
    "    nn_values,\n",
    "    layer_sizes,\n",
    "    nn_values_names=\"\",\n",
    "    absolute=True,\n",
    "    cmap=\"gray\",\n",
    "    figsize=(6, 6),\n",
    "    return_reshaped=False,\n",
    "    show=True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Plot the values of a neural network including a marker for padding values.\n",
    "    For simplicity, this example will annotate 'P' directly on the plot for padding values\n",
    "    using a unique marker value approach.\n",
    "\n",
    "    Args:\n",
    "        nn_values (dict):\n",
    "            A dictionary with the values of the neural network. For example,\n",
    "            the weights, gradients, or activations.\n",
    "        layer_sizes (dict):\n",
    "            A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "        nn_values_names (str, optional):\n",
    "            The name of the values. Defaults to \"\".\n",
    "        absolute (bool, optional):\n",
    "            Whether to use the absolute values. Defaults to True.\n",
    "        cmap (str, optional):\n",
    "            The colormap to use. Defaults to \"gray\".\n",
    "        figsize (tuple, optional):\n",
    "            The figure size. Defaults to (6, 6).\n",
    "        return_reshaped (bool, optional):\n",
    "            Whether to return the reshaped values. Defaults to False.\n",
    "        show (bool, optional):\n",
    "            Whether to show the plot. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the reshaped values.\n",
    "    \"\"\"\n",
    "    if cmap == \"gray\":\n",
    "        cmap = \"gray\"\n",
    "    elif cmap == \"BlueWhiteRed\":\n",
    "        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"blue\", \"white\", \"red\"])\n",
    "    elif cmap == \"GreenYellowRed\":\n",
    "        cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"green\", \"yellow\", \"red\"])\n",
    "    else:\n",
    "        cmap = \"viridis\"\n",
    "\n",
    "    res = {}\n",
    "    padding_marker = np.nan  # Use NaN as a special marker for padding\n",
    "    for layer, values in nn_values.items():\n",
    "        if layer not in layer_sizes:\n",
    "            print(f\"Layer {layer} size not defined, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        layer_shape = layer_sizes[layer]\n",
    "        height, width = layer_shape if len(layer_shape) == 2 else (layer_shape[0], 1)  # Support linear layers\n",
    "        \n",
    "        print(f\"{len(values)} values in Layer {layer}. Geometry: ({height}, {width})\")\n",
    "        \n",
    "        total_size = height * width\n",
    "        if len(values) < total_size:\n",
    "            padding_needed = total_size - len(values)\n",
    "            print(f\"{padding_needed} padding values added to Layer {layer}.\")\n",
    "            values = np.append(values, [padding_marker] * padding_needed)  # Append padding values\n",
    "\n",
    "        if absolute:\n",
    "            reshaped_values = np.abs(values).reshape((height, width))\n",
    "            # Mark padding values distinctly by setting them back to NaN\n",
    "            reshaped_values[reshaped_values == np.abs(padding_marker)] = np.nan\n",
    "        else:\n",
    "            reshaped_values = values.reshape((height, width))\n",
    "\n",
    "        _, ax = plt.subplots(figsize=figsize)\n",
    "        cax = ax.imshow(reshaped_values, cmap=cmap, interpolation=\"nearest\")\n",
    "\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                if np.isnan(reshaped_values[i, j]):\n",
    "                    ax.text(j, i, \"P\", ha=\"center\", va=\"center\", color=\"red\")\n",
    "        \n",
    "        plt.colorbar(cax, label=\"Value\")\n",
    "        plt.title(f\"{nn_values_names} Plot for {layer}\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "        # Add reshaped_values to the dictionary res\n",
    "        res[layer] = reshaped_values\n",
    "    if return_reshaped:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, layer_sizes = get_weights(net=model)\n",
    "plot_nn_values_scatter(nn_values=weights, layer_sizes=layer_sizes, nn_values_names=\"Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "\n",
    "def get_gradients(net, fun_control, batch_size, device=\"cpu\", normalize=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the gradients of a neural network and the size of each layer.\n",
    "\n",
    "    Args:\n",
    "        net (object):\n",
    "            A neural network.\n",
    "        fun_control (dict):\n",
    "            A dictionary with the function control.\n",
    "        batch_size (int, optional):\n",
    "            The batch size.\n",
    "        device (str, optional):\n",
    "            The device to use. Defaults to \"cpu\".\n",
    "        normalize (bool, optional):\n",
    "            Whether to normalize the input data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - grads: A dictionary with the gradients of the neural network.\n",
    "            - layer_sizes: A dictionary with layer names as keys and their sizes as entries in NumPy array format.\n",
    "\n",
    "    Examples:\n",
    "        # Example usage to compute gradients\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    dataset = fun_control[\"data_set\"]\n",
    "    data_module = LightDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        test_size=fun_control[\"test_size\"],\n",
    "        scaler=fun_control[\"scaler\"],\n",
    "        verbosity=10,\n",
    "    )\n",
    "    data_module.setup(stage=\"fit\")\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    inputs, targets = next(iter(train_loader))\n",
    "    if normalize:\n",
    "        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    net.zero_grad()\n",
    "    preds = net(inputs)\n",
    "    preds = preds.squeeze(-1)  # Remove the last dimension if it's 1\n",
    "    loss = F.mse_loss(preds, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = {}\n",
    "    layer_sizes = {}\n",
    "    for name, params in net.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            # Collect gradient information\n",
    "            grads[name] = params.grad.view(-1).cpu().clone().numpy()\n",
    "            # Collect size information\n",
    "            layer_sizes[name] = np.array(params.size())\n",
    "\n",
    "    net.zero_grad()\n",
    "    return grads, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients, layer_sizes = get_gradients(model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "plot_nn_values_scatter(nn_values=gradients, layer_sizes=layer_sizes, nn_values_names=\"Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(data, layer_index) -> bool:\n",
    "    \"\"\"Checks for NaN values in the tensor data.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The tensor to check for NaN values.\n",
    "        layer_index (int): The index of the layer for logging purposes.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if NaNs are found, False otherwise.\n",
    "    \"\"\"\n",
    "    if torch.isnan(data).any():\n",
    "        print(f\"NaN detected after layer {layer_index}\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(net, fun_control, batch_size, device=\"cpu\", normalize=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Computes the activations for each layer of the network, the mean activations,\n",
    "    and the sizes of the activations for each layer.\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The neural network model.\n",
    "        fun_control (dict): A dictionary containing the dataset.\n",
    "        batch_size (int): The batch size for the data loader.\n",
    "        device (str): The device to run the model on. Defaults to \"cpu\".\n",
    "        normalize (bool): Whether to normalize the input data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the activations, mean activations, and layer sizes for each layer.\n",
    "\n",
    "    Examples:\n",
    "        from spotpython.plot.xai import get_activations\n",
    "            activations, mean_activations, layer_sizes = get_activations(net, fun_control)\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    mean_activations = {}\n",
    "    layer_sizes = {}\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    dataset = fun_control[\"data_set\"]\n",
    "    data_module = LightDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        test_size=fun_control[\"test_size\"],\n",
    "        scaler=fun_control[\"scaler\"],\n",
    "        verbosity=10,\n",
    "    )\n",
    "    data_module.setup(stage=\"fit\")\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    inputs, _ = next(iter(train_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    if normalize:\n",
    "        inputs = (inputs - inputs.mean(dim=0, keepdim=True)) / inputs.std(dim=0, keepdim=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        # Loop through all layers\n",
    "        for layer_index, layer in enumerate(net.layers[:-1]):\n",
    "            inputs = layer(inputs)  # Forward pass through the layer\n",
    "\n",
    "            # Check for NaNs\n",
    "            if check_for_nans(inputs, layer_index):\n",
    "                break\n",
    "\n",
    "            # Collect activations for Linear layers\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                activations[layer_index] = inputs.view(-1).cpu().numpy()\n",
    "                mean_activations[layer_index] = inputs.mean(dim=0).cpu().numpy()\n",
    "                # Record the size of the activations\n",
    "                layer_sizes[layer_index] = np.array(inputs.size())\n",
    "\n",
    "    return activations, mean_activations, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations, mean_activations, layer_sizes = get_activations(net=model, fun_control=fun_control, batch_size=batch_size, device = \"cpu\")\n",
    "plot_nn_values_scatter(nn_values=activations, layer_sizes=layer_sizes, nn_values_names=\"Activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.data.lightdatamodule import LightDataModule\n",
    "# from spotpython.plot.xai import get_gradients\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=10, # 10: diabetes\n",
    "    _L_out=1,\n",
    "    _torchmetric=\"mean_squared_error\",\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "_L_in = fun_control[\"_L_in\"]\n",
    "_L_out = fun_control[\"_L_out\"]\n",
    "_torchmetric = fun_control[\"_torchmetric\"]\n",
    "batch_size = 16\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def extract_linear_dims(model):\n",
    "    dims = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Append input and output features of the Linear layer\n",
    "            dims.append(layer.in_features)\n",
    "            dims.append(layer.out_features)\n",
    "    return np.array(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_linear_dims(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "def viz_net(net,\n",
    "            device=\"cpu\",\n",
    "            show_attrs=False,\n",
    "            show_saved=False,\n",
    "            filename=\"model_architecture\",\n",
    "            format=\"png\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the architecture of a linear neural network.\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The neural network model.\n",
    "        device (str, optional): The device to use. Defaults to \"cpu\".\n",
    "        show_attrs (bool, optional): Whether to show the attributes. Defaults to False.\n",
    "        show_saved (bool, optional): Whether to show the saved. Defaults to False.\n",
    "        filename (str, optional): The filename. Defaults to \"model_architecture\".\n",
    "        format (str, optional): The format. Defaults to \"png\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the model does not have a linear layer.\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "       dim=extract_linear_dims(net)\n",
    "    except:\n",
    "        error_message = \"The model does not have a linear layer.\"\n",
    "        raise ValueError(error_message)\n",
    "    x = torch.randn(1, dim[0]).requires_grad_(True)\n",
    "    x = x.to(device)\n",
    "    output = net(x)\n",
    "    dot = make_dot(output, params=dict(net.named_parameters()), show_attrs=show_attrs , show_saved=show_saved)\n",
    "    dot.render(filename, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_net(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.plot.xai import viz_net\n",
    "from spotpython.utils.init import fun_control_init\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.light.regression.nn_linear_regressor import NNLinearRegressor\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.hyperparameters.values import (\n",
    "        get_default_hyperparameters_as_array, get_one_config_from_X)\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "_L_in=10\n",
    "_L_out=1\n",
    "_torchmetric=\"mean_squared_error\"\n",
    "fun_control = fun_control_init(\n",
    "    _L_in=_L_in,\n",
    "    _L_out=_L_out,\n",
    "    _torchmetric=_torchmetric,\n",
    "    data_set=Diabetes(),\n",
    "    core_model=NNLinearRegressor,\n",
    "    hyperdict=LightHyperDict)\n",
    "X = get_default_hyperparameters_as_array(fun_control)\n",
    "config = get_one_config_from_X(X, fun_control)\n",
    "model = fun_control[\"core_model\"](**config, _L_in=_L_in, _L_out=_L_out, _torchmetric=_torchmetric)\n",
    "viz_net(net=model, device=\"cpu\", show_attrs=True, show_saved=True, filename=\"model_architecture\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "plt.plot(X, y, label=r\"$f(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "S.exp_imp(1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.aggregated_mean_y = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "# assert S.exp_imp(0.0, 1.0) == 1/np.sqrt(2*np.pi)\n",
    "# which is approx. 0.3989422804014327\n",
    "S.exp_imp(0.0, 1.0)\n",
    "0.3989422804014327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_de_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.set_de_bounds()\n",
    "print(S.de_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract_from_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "\n",
    "# Define the number of theta and p parameters\n",
    "num_theta = 2\n",
    "num_p = 3\n",
    "\n",
    "# Initialize the Kriging model\n",
    "kriging_model = Kriging(\n",
    "    name='kriging',\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=False\n",
    ")\n",
    "\n",
    "# Extract parameters from given bounds\n",
    "# Assumes 'extract_from_bounds' will split the array into `theta` and `p` based on `n_theta`.\n",
    "bounds_array = np.array([1, 2, 3, 4, 5])\n",
    "kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "\n",
    "# Validate the expected values for theta and p\n",
    "# Convert theta and p to lists if they are numpy arrays\n",
    "theta_list = list(kriging_model.theta)\n",
    "p_list = list(kriging_model.p)\n",
    "\n",
    "assert theta_list == [1, 2], f\"Expected theta to be [1, 2] but got {theta_list}\"\n",
    "assert p_list == [3, 4, 5], f\"Expected p to be [3] but got {p_list}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "# Define the number of theta and p parameters\n",
    "num_theta = 2\n",
    "num_p = 3\n",
    "# Initialize the Kriging model\n",
    "kriging_model = Kriging(\n",
    "    name='kriging',\n",
    "    seed=124,\n",
    "    n_theta=num_theta,\n",
    "    n_p=num_p,\n",
    "    optim_p=True,\n",
    "    noise=True\n",
    ")\n",
    "# Create bounds array\n",
    "bounds_array = np.array([1, 2, 3, 4, 5, 6])\n",
    "# Extract parameters from given bounds\n",
    "kriging_model.extract_from_bounds(new_theta_p_Lambda=bounds_array)\n",
    "# Assertions to check if parameters are correctly extracted\n",
    "assert np.array_equal(kriging_model.theta, [1, 2]), f\"Expected theta to be [1, 2] but got {kriging_model.theta}\"\n",
    "assert np.array_equal(kriging_model.p, [3, 4, 5]), f\"Expected p to be [3, 4, 5] but got {kriging_model.p}\"\n",
    "assert kriging_model.Lambda == 6, f\"Expected Lambda to be 6 but got {kriging_model.Lambda}\"\n",
    "print(\"All assertions passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "print(new_theta_p_Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.update_log()\n",
    "print(S.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 0], [1, 0]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.fit(nat_X, nat_y)\n",
    "print(S.Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "S = Kriging()\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "print(f\"S.nat_X: {S.nat_X}\")\n",
    "print(f\"S.nat_y: {S.nat_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "assert S.var_type == ['num', 'num']\n",
    "assert S.var_type == ['num', 'num']\n",
    "assert S.num_mask.all() == True\n",
    "assert S.factor_mask.all() == False\n",
    "assert S.int_mask.all() == False\n",
    "assert S.ordered_mask.all() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "nat_X = np.array([[1, 2], [3, 4]])\n",
    "nat_y = np.array([1, 2])\n",
    "n=2\n",
    "p=2\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "assert S.theta.all() == array([0., 0.]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import log, var\n",
    "nat_X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "nat_y = np.array([1, 2, 3])\n",
    "n=3\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "# if var(self.nat_y) is > 0, then self.pen_val = self.n * log(var(self.nat_y)) + 1e4\n",
    "# else self.pen_val = self.n * var(self.nat_y) + 1e4\n",
    "assert S.pen_val == nat_X.shape[0] * log(var(S.nat_y)) + 1e4\n",
    "assert S.Psi.shape == (n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")\n",
    "S.build_U()\n",
    "print(f\"S.U:{S.U}\")\n",
    "S.likelihood()\n",
    "S.negLnLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "from numpy import power\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "print(S.__is_any__(power(10.0, S.theta), 0))\n",
    "print(S.__is_any__(S.theta, 0))\n",
    "S.theta: [0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[0], [1]])\n",
    "nat_y = np.array([0, 1])\n",
    "n=1\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "print(S.nat_X)\n",
    "print(S.nat_y)\n",
    "S.set_theta_values()\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.initialize_matrices()\n",
    "S.set_de_bounds()\n",
    "new_theta_p_Lambda = S.optimize_model()\n",
    "S.extract_from_bounds(new_theta_p_Lambda)\n",
    "print(f\"S.theta: {S.theta}\")\n",
    "S.build_Psi()\n",
    "print(f\"S.Psi: {S.Psi}\")\n",
    "S.build_U()\n",
    "print(f\"S.U:{S.U}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "nat_X = np.array([[1], [2]])\n",
    "nat_y = np.array([5, 10])\n",
    "n=2\n",
    "p=1\n",
    "S=Kriging(name='kriging', seed=124, n_theta=n, n_p=p, optim_p=True, noise=False, theta_init_zero=True)\n",
    "S.initialize_variables(nat_X, nat_y)\n",
    "S.set_variable_types()\n",
    "S.set_theta_values()\n",
    "S.initialize_matrices()\n",
    "S.build_Psi()\n",
    "S.build_U()\n",
    "S.likelihood()\n",
    "# assert S.mu is close to 7.5 with a tolerance of 1e-6\n",
    "assert np.allclose(S.mu, 7.5, atol=1e-6)\n",
    "E = np.exp(1)\n",
    "sigma2 = E/(E**2 -1) * (25/4 + 25/4*E)\n",
    "# asssert S.SigmaSqr is close to sigma2 with a tolerance of 1e-6\n",
    "assert np.allclose(S.SigmaSqr, sigma2, atol=1e-6)\n",
    "print(f\"S.LnDetPsi:{S.LnDetPsi}\")\n",
    "print(f\"S.self.negLnLike:{S.negLnLike}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "# 1-dimensional example\n",
    "fun = analytical().fun_sphere\n",
    "fun_control=fun_control_init(lower = np.array([-1]),\n",
    "                            upper = np.array([1]),\n",
    "                            noise=False)\n",
    "design_control=design_control_init(init_size=10)\n",
    "S = spot.Spot(fun=fun,\n",
    "              fun_control=fun_control,\n",
    "              design_control=design_control)\n",
    "S.initialize_design()\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.surrogate.plot()\n",
    "# 2-dimensional example\n",
    "fun = analytical().fun_sphere\n",
    "fun_control=fun_control_init(lower = np.array([-1, -1]),\n",
    "                            upper = np.array([1, 1]),\n",
    "                            noise=False)\n",
    "design_control=design_control_init(init_size=10)\n",
    "S = spot.Spot(fun=fun,\n",
    "              fun_control=fun_control,\n",
    "              design_control=design_control)\n",
    "S.initialize_design()\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "S.surrogate.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "from numpy import linspace, arange\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=1_0).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "mean_prediction, std_prediction, s_ei = S.predict(X, return_val=\"all\")\n",
    "print(f\"mean_prediction: {mean_prediction}\")\n",
    "print(f\"std_prediction: {std_prediction}\")\n",
    "print(f\"s_ei: {s_ei}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.build.kriging import Kriging\n",
    "import numpy as np\n",
    "    from numpy import linspace, arange, empty\n",
    "rng = np.random.RandomState(1)\n",
    "X = linspace(start=0, stop=10, num=10).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))\n",
    "training_indices = rng.choice(arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "S = Kriging(name='kriging', seed=124)\n",
    "S.fit(X_train, y_train)\n",
    "n = X.shape[0]\n",
    "y = empty(n, dtype=float)\n",
    "s = empty(n, dtype=float)\n",
    "ei = empty(n, dtype=float)\n",
    "for i in range(n):\n",
    "    y_coded, s_coded, ei_coded = S.predict_coded(X[i, :])\n",
    "    y[i] = y_coded if np.isscalar(y_coded) else y_coded.item()\n",
    "    s[i] = s_coded if np.isscalar(s_coded) else s_coded.item()\n",
    "    ei[i] = ei_coded if np.isscalar(ei_coded) else ei_coded.item()\n",
    "print(f\"y: {y}\")\n",
    "print(f\"s: {s}\")\n",
    "print(f\"ei: {-1.0*ei}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_psi_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.build.kriging import Kriging\n",
    "X_train = np.array([[1., 2.],\n",
    "                    [2., 4.],\n",
    "                    [3., 6.]])\n",
    "y_train = np.array([1., 2., 3.])\n",
    "S = Kriging(name='kriging',\n",
    "            seed=123,\n",
    "            log_level=50,\n",
    "            n_theta=1,\n",
    "            noise=False,\n",
    "            cod_type=\"norm\")\n",
    "S.fit(X_train, y_train)\n",
    "# force theta to simple values:\n",
    "S.theta = np.array([0.0])\n",
    "nat_X = np.array([1., 0.])\n",
    "S.psi = np.zeros((S.n, 1))\n",
    "S.build_psi_vec(nat_X)\n",
    "res = np.array([[np.exp(-4)],\n",
    "    [np.exp(-17)],\n",
    "    [np.exp(-40)]])\n",
    "assert np.array_equal(S.psi, res)\n",
    "print(f\"S.psi: {S.psi}\")\n",
    "print(f\"Control value res: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, optimizer_control_init, surrogate_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Log Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import inf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from spotpython.data.diabetes import Diabetes\n",
    "from spotpython.hyperdict.light_hyper_dict import LightHyperDict\n",
    "from spotpython.fun.hyperlight import HyperLight\n",
    "from spotpython.utils.init import (fun_control_init, surrogate_control_init, design_control_init)\n",
    "from spotpython.utils.eda import gen_design_table\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.file import get_experiment_filename\n",
    "\n",
    "PREFIX=\"00_TEST\"\n",
    "\n",
    "data_set = Diabetes()\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    PREFIX=PREFIX,\n",
    "    save_experiment=True,\n",
    "    fun_evals=inf,\n",
    "    max_time=10,\n",
    "    data_set = data_set,\n",
    "    core_model_name=\"light.regression.NNLinearRegressor\",\n",
    "    hyperdict=LightHyperDict,\n",
    "    _L_in=10,\n",
    "    _L_out=1,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    tensorboard_log=True,\n",
    "    show_config=True,)\n",
    "\n",
    "design_control = design_control_init(init_size=5)\n",
    "\n",
    "fun = HyperLight().fun\n",
    "\n",
    "spot_tuner = spot.Spot(fun=fun,fun_control=fun_control, design_control=design_control)\n",
    "res = spot_tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNLinearRegressor Teest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from spotpython.light.regression import NNLinearRegressor\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "torch.manual_seed(0)\n",
    "PATH_DATASETS = './data'\n",
    "BATCH_SIZE = 64\n",
    "# generate data\n",
    "num_samples = 1_000\n",
    "input_dim = 10\n",
    "X = torch.randn(num_samples, input_dim)  # random data for example\n",
    "Y = torch.randn(num_samples, 1)  # random target for example\n",
    "data_set = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=BATCH_SIZE)\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "net_light_base = NNLinearRegressor(l1=128,\n",
    "                                batch_norm=True,\n",
    "                                    epochs=10,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    initialization='xavier',\n",
    "                                    act_fn=nn.ReLU(),\n",
    "                                    optimizer='Adam',\n",
    "                                    dropout_prob=0.1,\n",
    "                                    lr_mult=0.1,\n",
    "                                    patience=5,\n",
    "                                    _L_in=input_dim,\n",
    "                                    _L_out=1,\n",
    "                                    _torchmetric=\"mean_squared_error\",)\n",
    "trainer = L.Trainer(max_epochs=2,  enable_progress_bar=True)\n",
    "trainer.fit(net_light_base, train_loader)\n",
    "# validation and test should give the same result, because the data is the same\n",
    "trainer.validate(net_light_base, val_loader)\n",
    "trainer.test(net_light_base, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Objective Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [-1, -1, -1]])\n",
    "fun = analytical()\n",
    "fun.fun_cubed(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "import numpy as np\n",
    "X = np.array([np.zeros(10), np.ones(10)])\n",
    "fun = analytical()\n",
    "fun.fun_wingwt(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.architecture import generate_div2_list\n",
    "# call the function with all integer values between 5 and 10\n",
    "for n in range(5, 21):\n",
    "    print(generate_div2_list(n, n_min=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.hyperparameters.architecture import get_hidden_sizes\n",
    "_L_in = 10\n",
    "max_n = 10\n",
    "for l1 in range(5, 20):    \n",
    "    print(get_hidden_sizes(_L_in, l1, max_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def get_three_layers(_L_in, l1) -> list:\n",
    "    \"\"\"\n",
    "    Calculate three layers based on input values.\n",
    "\n",
    "    Args:\n",
    "        _L_in (float): The input value to be multiplied.\n",
    "        l1 (float): The multiplier for the layers.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing three calculated layers [a, b, c] where:\n",
    "            - a = 3 * l1 * _L_in\n",
    "            - b = 2 * l1 * _L_in\n",
    "            - c = l1 * _L_in\n",
    "\n",
    "    Examples:\n",
    "        from spotpython.hyperparameters.architecture import get_three_layers\n",
    "            _L_in = 10\n",
    "            l1 = 20\n",
    "            get_three_layers(_L_in, l1)\n",
    "            [600, 400, 200]\n",
    "    \"\"\"\n",
    "    a = 3 * l1 * _L_in\n",
    "    b = 2 * l1 * _L_in\n",
    "    c = ceil(l1/2) * _L_in\n",
    "    return [a, b, a, b, b, c, c] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_L_in = 20\n",
    "l1 = 4\n",
    "get_three_layers(_L_in, l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for 0.20.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_spot_attributes_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1]),\n",
    "    upper = np.array([1]),\n",
    "    fun_evals=n)\n",
    "design_control=design_control_init(init_size=ni)\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n",
    "df = spot_1.get_spot_attributes_as_df()\n",
    "df[\"Attribute Name\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_red_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "# number of points\n",
    "n = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]),\n",
    "    fun_evals = n)\n",
    "design_control=design_control_init(init_size=ni)\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n",
    "assert spot_1.lower.size == 2\n",
    "assert spot_1.upper.size == 2\n",
    "assert len(spot_1.var_type) == 2\n",
    "assert spot_1.red_dim == False\n",
    "spot_1.lower = np.array([-1, -1])\n",
    "spot_1.upper = np.array([-1, -1])\n",
    "spot_1.to_red_dim()\n",
    "assert spot_1.lower.size == 0\n",
    "assert spot_1.upper.size == 0\n",
    "assert len(spot_1.var_type) == 0\n",
    "assert spot_1.red_dim == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_all_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n",
    "lower = np.array([-1, -1, 0, 0])\n",
    "upper = np.array([1, -1, 0, 5])  # Second and third dimensions are fixed\n",
    "fun_evals = 10\n",
    "var_type = ['float', 'int', 'float', 'int']\n",
    "var_name = ['x1', 'x2', 'x3', 'x4']\n",
    "spot_instance = spot.Spot(\n",
    "    fun = Analytical().fun_sphere, \n",
    "    fun_control=fun_control_init(lower=lower, upper=upper, fun_evals=fun_evals)\n",
    ")\n",
    "X0 = np.array([[2.5, 3.5], [4.5, 5.5]])\n",
    "X_full_dim = spot_instance.to_all_dim(X0)\n",
    "print(X_full_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_new_X0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "# number of initial points:\n",
    "ni = 3\n",
    "X_start = np.array([[0, 1], [1, 0], [1, 1], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "            n_points=10,\n",
    "            ocba_delta=0,\n",
    "            lower = np.array([-1, -1]),\n",
    "            upper = np.array([1, 1])\n",
    ")\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "             fun_control=fun_control,\n",
    "             design_control=design_control,\n",
    ")\n",
    "S.initialize_design(X_start=X_start)\n",
    "S.update_stats()\n",
    "S.fit_surrogate()\n",
    "X0 = S.get_new_X0()\n",
    "assert X0.shape[0] == S.n_points\n",
    "assert X0.shape[1] == S.lower.size\n",
    "# assert new points are in the interval [lower, upper]\n",
    "assert np.all(X0 >= S.lower)\n",
    "assert np.all(X0 <= S.upper)\n",
    "# print using 20 digits precision\n",
    "np.set_printoptions(precision=20)\n",
    "print(f\"X0: {X0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython import Analytical\n",
    "from spotpython import Spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init, design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.run(X_start=X_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import spot\n",
    "from spotpython.utils.init import (\n",
    "    fun_control_init,  design_control_init\n",
    "    )\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# start point X_0\n",
    "X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1]))\n",
    "design_control=design_control_init(init_size=ni)\n",
    "S = spot.Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            design_control=design_control,)\n",
    "S.initialize_design(X_start=X_start)\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write_tensorboard_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    tensorboard_log=True,\n",
    "    TENSORBOARD_CLEAN=True,\n",
    "    lower = np.array([-1]),\n",
    "    upper = np.array([1])\n",
    "    )\n",
    "fun = Analytical().fun_sphere\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "S.initialize_design()\n",
    "S.write_tensorboard_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize_design_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    lower = np.array([-1, -1]),\n",
    "    upper = np.array([1, 1])\n",
    "    )\n",
    "fun = Analytical().fun_sphere\n",
    "\n",
    "S = Spot(fun=fun,\n",
    "            fun_control=fun_control,\n",
    "            )\n",
    "X_start = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "S.initialize_design_matrix(X_start)\n",
    "print(f\"Design matrix: {S.X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate_initial_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init\n",
    "fun_control = fun_control_init(\n",
    "    lower=np.array([-1, -1]),\n",
    "    upper=np.array([1, 1])\n",
    ")\n",
    "fun = Analytical().fun_sphere\n",
    "S = Spot(fun=fun, fun_control=fun_control)\n",
    "X0 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "S.initialize_design_matrix(X_start=X0)\n",
    "S.evaluate_initial_design()\n",
    "print(f\"S.X: {S.X}\")\n",
    "print(f\"S.y: {S.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Experiment:\n",
    "    def save_experiment(self, filename=None, path=None, overwrite=True) -> None:\n",
    "        \"\"\"\n",
    "        Save the experiment to a file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename of the experiment file.\n",
    "            path (str): The path to the experiment file.\n",
    "            overwrite (bool): If `True`, the file will be overwritten if it already exists. Default is `True`.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Ensure we don't accidentally try to pickle unpicklable components\n",
    "        self.close_and_del_spot_writer()\n",
    "        self.remove_logger_handlers()\n",
    "\n",
    "        # Create deep copies of control dictionaries\n",
    "        fun_control = copy.deepcopy(self.fun_control)\n",
    "        optimizer_control = copy.deepcopy(self.optimizer_control)\n",
    "        surrogate_control = copy.deepcopy(self.surrogate_control)\n",
    "        design_control = copy.deepcopy(self.design_control)\n",
    "\n",
    "        # Prepare an experiment dictionary excluding any explicitly unpickable components\n",
    "        experiment = {\n",
    "            \"design_control\": design_control,\n",
    "            \"fun_control\": fun_control,\n",
    "            \"optimizer_control\": optimizer_control,\n",
    "            \"spot_tuner\": self._get_pickle_safe_spot_tuner(),\n",
    "            \"surrogate_control\": surrogate_control,\n",
    "        }\n",
    "\n",
    "        # Determine the filename based on PREFIX if not provided\n",
    "        PREFIX = fun_control.get(\"PREFIX\", \"experiment\")\n",
    "        if filename is None:\n",
    "            filename = self.get_experiment_filename(PREFIX)\n",
    "\n",
    "        if path is not None:\n",
    "            filename = os.path.join(path, filename)\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if filename is not None and os.path.exists(filename) and not overwrite:\n",
    "            print(f\"Error: File {filename} already exists. Use overwrite=True to overwrite the file.\")\n",
    "            return\n",
    "\n",
    "        # Serialize the experiment dictionary to the pickle file\n",
    "        if filename is not None:\n",
    "            with open(filename, \"wb\") as handle:\n",
    "                try:\n",
    "                    pickle.dump(experiment, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during pickling: {e}\")\n",
    "                    raise e\n",
    "            print(f\"Experiment saved to {filename}\")\n",
    "\n",
    "    def remove_logger_handlers(self) -> None:\n",
    "        \"\"\"\n",
    "        Remove handlers from the logger to avoid pickling issues.\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        for handler in list(logger.handlers):  # Copy the list to avoid modification during iteration\n",
    "            logger.removeHandler(handler)\n",
    "\n",
    "    def close_and_del_spot_writer(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete the spot_writer attribute from the object\n",
    "        if it exists and close the writer.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"spot_writer\") and self.spot_writer is not None:\n",
    "            self.spot_writer.flush()\n",
    "            self.spot_writer.close()\n",
    "            del self.spot_writer\n",
    "\n",
    "    def _get_pickle_safe_spot_tuner(self):\n",
    "        \"\"\"\n",
    "        Create a copy of self excluding unpickleable components for safe pickling.\n",
    "        This ensures no unpicklable components are passed to pickle.dump().\n",
    "        \"\"\"\n",
    "        # Make a deepcopy and manually remove unpickleable components\n",
    "        spot_tuner = copy.deepcopy(self)\n",
    "        for attr in ['spot_writer']:\n",
    "            if hasattr(spot_tuner, attr):\n",
    "                delattr(spot_tuner, attr)\n",
    "        return spot_tuner\n",
    "\n",
    "    def get_experiment_filename(self, prefix):\n",
    "        \"\"\"\n",
    "        Generate a filename based on a given prefix with additional unique identifiers or timestamps.\n",
    "        \"\"\"\n",
    "        # Implement the logic to generate a filename\n",
    "        return f\"{prefix}_experiment.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform_hyper_parameter_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.transform import transform_hyper_parameter_values\n",
    "fun_control = {\n",
    "    \"core_model_hyper_dict\": {\n",
    "        \"leaf_prediction\": {\n",
    "                \"type\": \"factor\",\n",
    "                \"transform\": \"None\",\n",
    "                \"default\": \"mean\",\n",
    "                \"levels\": [\"mean\", \"model\", \"adaptive\"],\n",
    "                \"core_model_parameter_type\": \"str\"\n",
    "                            },\n",
    "        \"max_depth\": {\n",
    "                \"type\": \"int\",\n",
    "                \"default\": 20,\n",
    "                \"transform\": \"transform_power_2\",\n",
    "                \"lower\": 2,\n",
    "                \"upper\": 20}\n",
    "            }\n",
    "    }\n",
    "hyper_parameter_values = {\n",
    "        'max_depth': 2,\n",
    "        'leaf_prediction': 'mean'}\n",
    "transform_hyper_parameter_values(fun_control, hyper_parameter_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotpython.utils.transform import transform_hyper_parameter_values\n",
    "fun_control = {\n",
    "    \"core_model_hyper_dict\": {\n",
    "        \"l1\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 3,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 3,\n",
    "            \"upper\": 8\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 4,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 4,\n",
    "            \"upper\": 9\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 4,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 1,\n",
    "            \"upper\": 4\n",
    "        },\n",
    "        \"act_fn\": {\n",
    "            \"levels\": [\n",
    "                \"Sigmoid\",\n",
    "                \"Tanh\",\n",
    "                \"ReLU\",\n",
    "                \"LeakyReLU\",\n",
    "                \"ELU\",\n",
    "                \"Swish\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"ReLU\",\n",
    "            \"transform\": \"None\",\n",
    "            \"class_name\": \"spotpython.torch.activation\",\n",
    "            \"core_model_parameter_type\": \"instance()\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 5\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"levels\": [\n",
    "                \"Adadelta\",\n",
    "                \"Adagrad\",\n",
    "                \"Adam\",\n",
    "                \"AdamW\",\n",
    "                \"SparseAdam\",\n",
    "                \"Adamax\",\n",
    "                \"ASGD\",\n",
    "                \"NAdam\",\n",
    "                \"RAdam\",\n",
    "                \"RMSprop\",\n",
    "                \"Rprop\",\n",
    "                \"SGD\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"SGD\",\n",
    "            \"transform\": \"None\",\n",
    "            \"class_name\": \"torch.optim\",\n",
    "            \"core_model_parameter_type\": \"str\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 11\n",
    "        },\n",
    "        \"dropout_prob\": {\n",
    "            \"type\": \"float\",\n",
    "            \"default\": 0.01,\n",
    "            \"transform\": \"None\",\n",
    "            \"lower\": 0.0,\n",
    "            \"upper\": 0.25\n",
    "        },\n",
    "        \"lr_mult\": {\n",
    "            \"type\": \"float\",\n",
    "            \"default\": 1.0,\n",
    "            \"transform\": \"None\",\n",
    "            \"lower\": 0.1,\n",
    "            \"upper\": 10.0\n",
    "        },\n",
    "        \"patience\": {\n",
    "            \"type\": \"int\",\n",
    "            \"default\": 2,\n",
    "            \"transform\": \"transform_power_2_int\",\n",
    "            \"lower\": 2,\n",
    "            \"upper\": 6\n",
    "        },\n",
    "        \"batch_norm\": {\n",
    "            \"levels\": [\n",
    "                0,\n",
    "                1\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": 0,\n",
    "            \"transform\": \"None\",\n",
    "            \"core_model_parameter_type\": \"bool\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 1\n",
    "        },\n",
    "        \"initialization\": {\n",
    "            \"levels\": [\n",
    "                \"Default\",\n",
    "                \"kaiming_uniform\",\n",
    "                \"kaiming_normal\",\n",
    "                \"xavier_uniform\",\n",
    "                \"xavier_normal\"\n",
    "            ],\n",
    "            \"type\": \"factor\",\n",
    "            \"default\": \"Default\",\n",
    "            \"transform\": \"None\",\n",
    "            \"core_model_parameter_type\": \"str\",\n",
    "            \"lower\": 0,\n",
    "            \"upper\": 4\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "hyper_parameter_values = {\n",
    "        'l1': 2,\n",
    "        'epochs': 3,\n",
    "        'batch_size': 4,\n",
    "        'act_fn': 'ReLU',\n",
    "        'optimizer': 'SGD',\n",
    "        'dropout_prob': 0.01,\n",
    "        'lr_mult': 1.0,\n",
    "        'patience': 3,\n",
    "        'batch_norm': 0,\n",
    "        'initialization': 'Default',        \n",
    "    }\n",
    "transform_hyper_parameter_values(fun_control, hyper_parameter_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotpython.hyperparameters.values import assign_values\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "var_list = ['a', 'b']\n",
    "result = assign_values(X, var_list)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.utils.init import fun_control_init, design_control_init\n",
    "from spotpython.utils.file import load_result\n",
    "import pprint\n",
    "\n",
    "def _compare_dicts(dict1, dict2, ignore_keys=None):\n",
    "    \"\"\"\n",
    "    Compare two dictionaries, including element-wise comparison for numpy arrays.\n",
    "    Print missing elements (keys) if the dictionaries do not match.\n",
    "\n",
    "    Args:\n",
    "        dict1 (dict): First dictionary to compare.\n",
    "        dict2 (dict): Second dictionary to compare.\n",
    "        ignore_keys (list, optional): List of keys to ignore during comparison. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the dictionaries match, False otherwise.\n",
    "    \"\"\"\n",
    "    if ignore_keys is None:\n",
    "        ignore_keys = []\n",
    "    # ensure that ignore_keys is a list\n",
    "    if not isinstance(ignore_keys, list):\n",
    "        ignore_keys = [ignore_keys]\n",
    "\n",
    "    keys1 = set(dict1.keys()) - set(ignore_keys)\n",
    "    keys2 = set(dict2.keys()) - set(ignore_keys)\n",
    "\n",
    "    if keys1 != keys2:\n",
    "        missing_in_dict1 = keys2 - keys1\n",
    "        missing_in_dict2 = keys1 - keys2\n",
    "        print(f\"Missing in dict1: {missing_in_dict1}\")\n",
    "        print(f\"Missing in dict2: {missing_in_dict2}\")\n",
    "        return False\n",
    "\n",
    "    for key in keys1:\n",
    "        if isinstance(dict1[key], np.ndarray) and isinstance(dict2[key], np.ndarray):\n",
    "            if not np.array_equal(dict1[key], dict2[key]):\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "        else:\n",
    "            if dict1[key] != dict2[key]:\n",
    "                print(f\"Mismatch in key '{key}': {dict1[key]} != {dict2[key]}\")\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_save_and_load_experiment(tmp_path):\n",
    "    PREFIX = \"test_02\"\n",
    "    # Initialize function control\n",
    "    fun_control = fun_control_init(\n",
    "        PREFIX=PREFIX,\n",
    "        lower=np.array([-1, -1]),\n",
    "        upper=np.array([1, 1]),\n",
    "        verbosity=1\n",
    "    )\n",
    "    \n",
    "    design_control = design_control_init(init_size=7)\n",
    "\n",
    "    fun = Analytical().fun_sphere\n",
    "        \n",
    "    S = Spot(\n",
    "        fun=fun,\n",
    "        fun_control=fun_control,\n",
    "        design_control=design_control,\n",
    "    )\n",
    "    \n",
    "    X_start = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    S.run(X_start=X_start)\n",
    "\n",
    "    # Load the experiment\n",
    "    S_loaded = load_result(PREFIX)\n",
    "    print(f\"S: {S}\")    \n",
    "    print(f\"S_loaded: {S_loaded}\")\n",
    "    pprint.pprint(S_loaded)\n",
    "    loaded_fun_control = S_loaded.fun_control\n",
    "    loaded_design_control = S_loaded.design_control\n",
    "    loaded_surrogate_control = S_loaded.surrogate_control\n",
    "    loaded_optimizer_control = S_loaded.optimizer_control\n",
    "    \n",
    "    # Check if the loaded data matches the original data\n",
    "    # It is ok if the counter is different, because it is increased during the run\n",
    "    assert _compare_dicts(loaded_fun_control, fun_control, ignore_keys=\"counter\"), \"Loaded fun_control should match the original fun_control.\"\n",
    "    assert _compare_dicts(loaded_design_control, design_control), \"Loaded design_control should match the original design_control.\"\n",
    "    assert _compare_dicts(loaded_surrogate_control, S.surrogate_control), \"Loaded surrogate_control should match the original surrogate_control.\"\n",
    "    assert _compare_dicts(loaded_optimizer_control, S.optimizer_control), \"Loaded optimizer_control should match the original optimizer_control.\"\n",
    "\n",
    "    # Check if the S_loaded is an instance of Spot\n",
    "    assert isinstance(S_loaded, Spot), \"Loaded S_loaded should be an instance of Spot.\"\n",
    "\n",
    "    # Check if the design matrix and response vector are equal\n",
    "    # if there are differences, print the differences\n",
    "    # Differences are OK\n",
    "    # if not np.array_equal(S_loaded.X, S.X):\n",
    "    #     print(f\"Design matrix mismatch: {S_loaded.X} != {S.X}\")\n",
    "    # if not np.array_equal(S_loaded.y, S.y):\n",
    "    #     print(f\"Response vector mismatch: {S_loaded.y} != {S.y}\")\n",
    "\n",
    "\n",
    "test_save_and_load_experiment(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment saved to test_plot_progress_05_exp.pkl\n",
      "Result file test_plot_progress_05_res.pkl exists. Loading the result.\n",
      "Loaded experiment from test_plot_progress_05_res.pkl\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from spotpython.fun.objectivefunctions import Analytical\n",
    "from spotpython.spot import Spot\n",
    "from spotpython.utils.init import fun_control_init, surrogate_control_init, design_control_init\n",
    "\n",
    "ni = 7\n",
    "PREFIX = \"test_plot_progress_05\"\n",
    "# number of points\n",
    "fun_evals = 10\n",
    "fun = Analytical().fun_sphere\n",
    "fun_control = fun_control_init(\n",
    "PREFIX=PREFIX,\n",
    "lower=np.array([-1, -1]),\n",
    "upper=np.array([1, 1]),\n",
    "fun_evals=fun_evals,\n",
    "tolerance_x=np.sqrt(np.spacing(1))\n",
    ")\n",
    "design_control = design_control_init(init_size=ni)\n",
    "surrogate_control = surrogate_control_init(n_theta=3)\n",
    "S = Spot(\n",
    "fun=fun,\n",
    "fun_control=fun_control,\n",
    "design_control=design_control,\n",
    "surrogate_control=surrogate_control,\n",
    ")\n",
    "S = S.run()\n",
    "\n",
    "# Test plot_progress with different parameters\n",
    "S.plot_progress(show=False)  # Test with show=False\n",
    "S.plot_progress(log_x=True, show=False)  # Test with log_x=True\n",
    "S.plot_progress(log_y=True, show=False)  # Test with log_y=True\n",
    "S.plot_progress(filename=\"test_plot.png\", show=False)  # Test with a different filename\n",
    "# add NaN to S.y at position 2\n",
    "S.y[2] = np.nan\n",
    "S.plot_progress(show=False)  # Test with NaN in S.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spot312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
